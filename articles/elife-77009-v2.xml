<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">77009</article-id><article-id pub-id-type="doi">10.7554/eLife.77009</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Signal denoising through topographic modularity of neural circuits</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-266751"><name><surname>Zajzon</surname><given-names>Barna</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3458-103X</contrib-id><email>b.zajzon@fz-juelich.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-44888"><name><surname>Dahmen</surname><given-names>David</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7664-916X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-271236"><name><surname>Morrison</surname><given-names>Abigail</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6933-797X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-271237"><name><surname>Duarte</surname><given-names>Renato</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6099-667X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02nv7yv05</institution-id><institution>Institute of Neuroscience and Medicine (INM-6) and Institute for Advanced Simulation (IAS-6) and JARA-BRAIN Institute I, Jülich Research Centre</institution></institution-wrap><addr-line><named-content content-type="city">Jülich</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xfq0f34</institution-id><institution>Department of Psychiatry, Psychotherapy and Psychosomatics, RWTH Aachen University</institution></institution-wrap><addr-line><named-content content-type="city">Aachen</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xfq0f34</institution-id><institution>Department of Computer Science 3 - Software Engineering, RWTH Aachen University</institution></institution-wrap><addr-line><named-content content-type="city">Aachen</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Donders Institute for Brain, Cognition and Behavior, Radboud University Nijmegen</institution></institution-wrap><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xez1567</institution-id><institution>Salk Institute for Biological Studies</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>26</day><month>01</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e77009</elocation-id><history><date date-type="received" iso-8601-date="2022-01-12"><day>12</day><month>01</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-01-25"><day>25</day><month>01</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-01-12"><day>12</day><month>01</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.01.10.475681"/></event></pub-history><permissions><copyright-statement>© 2023, Zajzon et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Zajzon et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-77009-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-77009-figures-v2.pdf"/><abstract><p>Information from the sensory periphery is conveyed to the cortex via structured projection pathways that spatially segregate stimulus features, providing a robust and efficient encoding strategy. Beyond sensory encoding, this prominent anatomical feature extends throughout the neocortex. However, the extent to which it influences cortical processing is unclear. In this study, we combine cortical circuit modeling with network theory to demonstrate that the sharpness of topographic projections acts as a bifurcation parameter, controlling the macroscopic dynamics and representational precision across a modular network. By shifting the balance of excitation and inhibition, topographic modularity gradually increases task performance and improves the signal-to-noise ratio across the system. We demonstrate that in biologically constrained networks, such a denoising behavior is contingent on recurrent inhibition. We show that this is a robust and generic structural feature that enables a broad range of behaviorally relevant operating regimes, and provide an in-depth theoretical analysis unraveling the dynamical principles underlying the mechanism.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>network dynamics</kwd><kwd>neural circuits</kwd><kwd>theoretical neuroscience</kwd><kwd>topographic modularity</kwd><kwd>signal denoising</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Initiative and Networking Fund of the Helmholtz Association</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Zajzon</surname><given-names>Barna</given-names></name><name><surname>Morrison</surname><given-names>Abigail</given-names></name><name><surname>Duarte</surname><given-names>Renato</given-names></name><name><surname>Dahmen</surname><given-names>David</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Helmholtz Portfolio theme Supercomputing and Modeling for the Human Brain</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Zajzon</surname><given-names>Barna</given-names></name><name><surname>Morrison</surname><given-names>Abigail</given-names></name><name><surname>Duarte</surname><given-names>Renato</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Excellence Initiative of the German federal and state governments</institution></institution-wrap></funding-source><award-id>G:(DE-82)EXS-SF-neuroIC002</award-id><principal-award-recipient><name><surname>Zajzon</surname><given-names>Barna</given-names></name><name><surname>Morrison</surname><given-names>Abigail</given-names></name><name><surname>Duarte</surname><given-names>Renato</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001656</institution-id><institution>Helmholtz Association</institution></institution-wrap></funding-source><award-id>VH-NG-1028</award-id><principal-award-recipient><name><surname>Dahmen</surname><given-names>David</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>European Commission HBP</institution></institution-wrap></funding-source><award-id>945539</award-id><principal-award-recipient><name><surname>Dahmen</surname><given-names>David</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Topographic maps can gradually increase the fidelity of sensory representations and improve signal-to-noise ratio across multiple cortical circuits, a generic architectural feature that depends solely on the modularity of topographic projections and recurrent inhibition.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Sensory inputs are often ambiguous, noisy, and imprecise. Due to volatility in the environment and inaccurate peripheral representations, the sensory signals that arrive at the neocortical circuitry are often incomplete or corrupt (<xref ref-type="bibr" rid="bib19">Faisal et al., 2008</xref>; <xref ref-type="bibr" rid="bib67">Renart and Machens, 2014</xref>). However, from these noisy input streams, the system is able to acquire reliable internal representations and extract relevant computable features at various degrees of abstraction (<xref ref-type="bibr" rid="bib23">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib56">Okada et al., 2010</xref>; <xref ref-type="bibr" rid="bib13">DiCarlo et al., 2012</xref>). Sensory perception in the mammalian neocortex thus relies on efficiently detecting the relevant input signals while minimizing the impact of noise.</p><p>Making sense of the environment also requires the estimation of features not explicitly represented by low-level sensory inputs. These inferential processes (<xref ref-type="bibr" rid="bib51">Młynarski and Hermundstad, 2018</xref>; <xref ref-type="bibr" rid="bib58">Parr et al., 2019</xref>) rely on the propagation of internal signals such as expectations and predictions, the accuracy of which must be evaluated against the ground truth, that is the sensory input stream. In a highly dynamic environment, this translates to a continuous process whose precision hinges on the fidelity with which external stimuli are encoded in the neural substrate. Additionally, as the system is modular and hierarchical (strikingly so in the sensory and motor components; <xref ref-type="bibr" rid="bib50">Meunier et al., 2010</xref>; <xref ref-type="bibr" rid="bib57">Park and Friston, 2013</xref>), it is critical that the external signal permeates the different processing modules despite the increasing distance from the sensory periphery (the input source) and the various transformations it is exposed to along the way, which degrade the signal via the interference of task-irrelevant and intrinsic, ongoing activity.</p><p>Accurate signal propagation can be achieved in a number of ways. One obvious solution is the direct routing and distribution of the signal, such that direct sensory input can be fed to different processing modules, which may be partially achieved through thalamocortical projections (<xref ref-type="bibr" rid="bib77">Sherman and Guillery, 2002</xref>; <xref ref-type="bibr" rid="bib54">Nakajima and Halassa, 2017</xref>). Another possibility, which we explore in this study, is to propagate the input signal through tailored pathways that route the information throughout the system, allowing different processing stages to retrieve it without incurring much representational loss. Throughout the mammalian neocortex, the existence and characteristics of structured projections (topographic maps) present a possible substrate for such signal routing. By preserving the relative organization of tuned neuronal populations, such maps imprint spatiotemporal features of (noisy) sensory inputs onto the cortex (<xref ref-type="bibr" rid="bib32">Kaas, 1997</xref>; <xref ref-type="bibr" rid="bib5">Bednar and Wilson, 2016</xref>; <xref ref-type="bibr" rid="bib91">Wandell and Winawer, 2011</xref>). In a previous study (<xref ref-type="bibr" rid="bib93">Zajzon et al., 2019</xref>), we discovered that structured projections can create feature-specific pathways that allow the external inputs to be faithfully represented and propagated throughout the system, but it remains unclear which connectivity properties are critical and what the underlying mechanism is. Moreover, beyond mere sensory representation, there is evidence that such structure-preserving mappings are also involved in more complex cognitive processes in associative and frontal areas (<xref ref-type="bibr" rid="bib25">Hagler and Sereno, 2006</xref>; <xref ref-type="bibr" rid="bib78">Silver and Kastner, 2009</xref>; <xref ref-type="bibr" rid="bib59">Patel et al., 2014</xref>), suggesting that topographic maps are a prominent structural feature of cortical organization.</p><p>In this study, we hypothesize that structured projection pathways allow sensory stimuli to be accurately reconstructed as they permeate multiple processing modules. We demonstrate that, by modulating effective connectivity and regional E/I balance, topographic projections additionally serve a <italic>denoising</italic> function, not merely allowing the faithful propagation of input signals, but systematically improving the system’s internal representations and increasing signal-to-noise ratio. We identify a critical threshold in the degree of modularity in topographic projections, beyond which the system behaves effectively as a denoising autoencoder (note that the parallel is established here on conceptual, not formal, grounds as the system is capable of retrieving the original, uncorrupted input from a noisy source, but bears no formal similarity to denoising autoencoder algorithms). Additionally, we demonstrate that this phenomenon is robust, with the qualitative behavior persisting across very different models. Theoretical considerations and network simulations show that it hinges solely on the modularity of topographic projections and the presence of recurrent inhibition, with the external input and single-neuron properties influencing where/when, but not if, denoising occurs. Our results suggest that modular structure in feedforward projection pathways can have a significant effect on the system’s qualitative behavior, enabling a wide range of behaviorally relevant and empirically supported dynamic regimes. This allows the system to: (1) maintain stable representations of multiple stimulus features (<xref ref-type="bibr" rid="bib1">Andersen et al., 2008</xref>); (2) amplify features of interest while suppressing others through winner-takes-all (WTA) mechanisms (<xref ref-type="bibr" rid="bib15">Douglas and Martin, 2004</xref>; <xref ref-type="bibr" rid="bib10">Carandini and Heeger, 2011</xref>); and (3) dynamically represent different stimulus features as stable and metastable states and stochastically switch among active representations through a winnerless competition (WLC) effect (<xref ref-type="bibr" rid="bib49">McCormick, 2005</xref>; <xref ref-type="bibr" rid="bib63">Rabinovich et al., 2008</xref>; <xref ref-type="bibr" rid="bib69">Rost et al., 2018</xref>).</p><p>Our key finding, that the modulation of information processing dynamics and the fidelity of stimulus/feature representations results from the structure of topographic feedforward projections, provides new meaning and functional relevance to the pervasiveness of these projection maps throughout the mammalian neocortex. Beyond routing feature-specific information from sensory transducers through brainstem, thalamus, and into primary sensory cortices (notably tonotopic, retinotopic, and somatotopic maps), their maintenance within the neocortex (<xref ref-type="bibr" rid="bib59">Patel et al., 2014</xref>) ensures that even cortical regions that are not directly engaged with the sensory input (higher-order cortex), can receive faithful representations of it, and that these internal signals, emanating from lower-order cortical areas, can dramatically skew and modulate the circuit’s E/I balance and local functional connectivity, resulting in fundamental differences in the systems’ responsiveness.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>To investigate the role of structured pathways between processing modules in modulating the fidelity of stimulus representations, we study a network comprising up to six sequentially connected sub-networks (SSNs, see Materials and methods and <xref ref-type="fig" rid="fig1">Figure 1a</xref>). Each SSN is a <italic>balanced random network</italic> (see e.g. <xref ref-type="bibr" rid="bib9">Brunel, 2000</xref>) of 10,000, sparsely and randomly coupled leaky integrate-and-fire (LIF) neurons (80% excitatory and 20% inhibitory). In each SSN, neurons are assigned to sub-populations associated with a particular stimulus. Excitatory neurons belonging to such stimulus-specific sub-populations then project to the subsequent SSN with a varying degree of specificity. We refer to a set of stimulus-specific sub-populations across the network and the structured feedforward projections among them as a <italic>topographic map</italic>. The specificity of the map is determined by the degree of <italic>modularity</italic> of the corresponding projections matrices (see e.g. <xref ref-type="fig" rid="fig1">Figure 1a</xref>). Modularity is thus defined as the relative density of connections within a stimulus-specific pathway (i.e., connecting sub-populations associated to the <italic>same</italic> stimulus; see Materials and methods and <xref ref-type="fig" rid="fig1">Figure 1a</xref>). In the following, we study the role of topographic specificity in modulating the system’s functional and representational dynamics and its ability to cope with noise-corrupted input signals.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Sequential denoising spiking architecture.</title><p>(<bold>a</bold>) A continuous step signal is used to drive the network. The input is spatially encoded in the first sub-network (SSN<sub>0</sub>), whereby each input channel is mapped exclusively onto a sub-population of stimulus-specific excitatory and inhibitory neurons (schematically illustrated by the colors; see also inset, top left). This exclusive encoding is retained to variable degrees across the network, through topographically structured feedforward projections (inset, top right) controlled by the modularity parameter <inline-formula><mml:math id="inf1"><mml:mi>m</mml:mi></mml:math></inline-formula> (see Materials and methods). This is illustrated explicitly for both topographic maps (purple and cyan arrows). Projections between SSNs are purely excitatory and target both excitatory and inhibitory neurons. (<bold>b</bold>) Signal reconstruction across the network. Single-trial illustration of target signal (black step function) and readout output (red curves) in three different SSNs, for <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math></inline-formula> and no added noise (<inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). For simplicity, only two out of ten input channels are shown. (<bold>c</bold>) Signal reconstruction error in the different SSNs for the no-noise scenario shown in (<bold>b</bold>). Color shade denotes network depth, from SSN<sub>0</sub> (lightest) to SSN<sub>5</sub> (darkest). The horizontal red line represents chance level, while the gray vertical line marks the transition (switching) point <inline-formula><mml:math id="inf4"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>switch</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mn>0.83</mml:mn></mml:mrow></mml:math></inline-formula> (see main text). <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> shows the task performance for a broader range of parameters. (<bold>d</bold>) Performance gain across the network, relative to SSN<sub>0</sub>, for the setup illustrated in (<bold>b</bold>). (<bold>e</bold>) as in (<bold>b</bold>) but for <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>f</bold>) Reconstruction error in SSN<sub>5</sub> for the different noise intensities. Horizontal and vertical dashed lines as in (<bold>c</bold>). (<bold>g</bold>) Performance gain in SSN<sub>5</sub>, relative to SSN<sub>0</sub>.</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Code and data for <xref ref-type="fig" rid="fig1">Figure 1</xref> and related figure supplements.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-77009-fig1-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Sequential denoising effect.</title><p>(<bold>a</bold>) Reconstruction error (NRMSE) in three different sub-networks as a function of modularity (<inline-formula><mml:math id="inf6"><mml:mi>m</mml:mi></mml:math></inline-formula>) and noise amplitude (<inline-formula><mml:math id="inf7"><mml:msub><mml:mi>σ</mml:mi><mml:mi>ϵ</mml:mi></mml:msub></mml:math></inline-formula>). The points marked in the rightmost panel correspond to chance-level reconstruction accuracy. (<bold>b</bold>) Relative reconstruction performance gain in SSN<sub>5</sub> compared to SSN<sub>0</sub>, expressed as percentage of error decrease.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig1-figsupp1-v2.tif"/></fig></fig-group><sec id="s2-1"><title>Sequential denoising through structured projections</title><p>By systematically varying the degree of modular specialization in the feedforward projections (modularity parameter, <inline-formula><mml:math id="inf8"><mml:mi>m</mml:mi></mml:math></inline-formula>, see Materials and methods and <xref ref-type="fig" rid="fig1">Figure 1</xref>), we can control the segregation of stimulus-specific pathways across the network and investigate how it influences the characteristics of neural representations as the signal propagates. If the feedforward projections are unstructured or moderately structured (<inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≲</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>), information about the input fails to permeate the network, resulting in a chance-level reconstruction accuracy in the last sub-network, SSN<sub>5</sub>, even in the absence of noise (see <xref ref-type="fig" rid="fig1">Figure 1b, c</xref>). However, as <inline-formula><mml:math id="inf10"><mml:mi>m</mml:mi></mml:math></inline-formula> approaches a switching value <inline-formula><mml:math id="inf11"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>switch</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mn>0.83</mml:mn></mml:mrow></mml:math></inline-formula>, there is a qualitative transition in the system’s behavior, leading to a consistently higher reconstruction accuracy across the sub-networks (<xref ref-type="fig" rid="fig1">Figure 1b–e</xref>), regardless of the amount of noise added to the signal (<xref ref-type="fig" rid="fig1">Figure 1f, g</xref>).</p><p>Beyond this transition point, reconstruction accuracy improves with depth, that is the signal is more accurately represented in SSN<sub>5</sub> than in the initial sub-network, SSN<sub>0</sub>, with an effective accuracy gain of over 40% (<xref ref-type="fig" rid="fig1">Figure 1d, g</xref>). While the addition of noise does impair the absolute reconstruction accuracy in all cases (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), the denoising effect persists even if the input is severely corrupted (<inline-formula><mml:math id="inf12"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, see <xref ref-type="fig" rid="fig1">Figure 1f, g</xref>). This is a counter-intuitive result, suggesting that topographic modularity is not only necessary for reliable communication across multiple populations (see <xref ref-type="bibr" rid="bib93">Zajzon et al., 2019</xref>), but also supports an effective denoising effect, whereby representational precision increases with depth, even if the signal is profoundly distorted by noise.</p></sec><sec id="s2-2"><title>Noise suppression and response amplification</title><p>The sequential denoising effect observed beyond the transition point <inline-formula><mml:math id="inf13"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>switch</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mn>0.83</mml:mn></mml:mrow></mml:math></inline-formula> results in an increasingly accurate input encoding through progressively more precise internal representations. In general, such a phenomenon could be achieved either through noise suppression, stimulus-specific response amplification or both. In this section, we examine these possibilities by analyzing and comparing the input-driven dynamics of the different sub-networks. The strict segregation of stimulus-specific sub-populations in SSN<sub>0</sub> is only fully preserved across the system if <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, in which case signal encoding and transmission primarily rely on this spatial segregation. Spiking activity across the different SSNs (<xref ref-type="fig" rid="fig2">Figure 2a</xref>) demonstrates that the system gradually sharpens the segregation of stimulus-specific sub-populations; indeed, in systems with fully modular feedforward projections, activity in the last sub-network is concentrated predominantly in the stimulated sub-populations. This effect can be observed in both excitatory (E) and inhibitory (I) populations, as both are equally targeted by the feedforward excitatory projections. The sharpening effect consists of both <italic>noise suppression</italic> and <italic>response amplification</italic> (<xref ref-type="fig" rid="fig2">Figure 2b</xref>), measured as the relative firing rates of the non-stimulated <inline-formula><mml:math id="inf15"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mn>5</mml:mn><mml:mi>NS</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mn>0</mml:mn><mml:mi>NS</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and stimulated sub-populations <inline-formula><mml:math id="inf16"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mn>5</mml:mn><mml:mi mathvariant="normal">S</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mn>0</mml:mn><mml:mi mathvariant="normal">S</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, respectively. For ,<inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. noise suppression is only marginal and responses within the stimulated pathways are not amplified (<inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Activity modulation and representational precision.</title><p>(<bold>a</bold>) One second of spiking activity observed across 1000 randomly chosen excitatory (blue) and inhibitory (red) neurons in SSN<sub>0</sub>, SSN<sub>2</sub> and SSN<sub>5</sub>, for <inline-formula><mml:math id="inf19"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math></inline-formula> (top) and <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (bottom). (<bold>b</bold>) Mean quotient of firing rates in SSN<sub>5</sub> and SSN<sub>0</sub> <inline-formula><mml:math id="inf22"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for stimulated (S, left) and non-stimulated (NS, right) sub-populations for different input noise levels, describing response amplification and noise suppression, respectively. (<bold>c</bold>) Mean firing rates of the stimulated (top) and non-stimulated (bottom) excitatory sub-populations in the different SSNs (color shade as in <xref ref-type="fig" rid="fig1">Figure 1</xref>), for <inline-formula><mml:math id="inf23"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. For modularity values facilitating an asynchronous irregular regime across the network, the firing rates predicted by mean-field theory (left) closely match the simulation data (right). (<bold>d</bold>) Mean-field predictions for the stationary firing rates of the stimulated (top) and non-stimulated (bottom) sub-populations, in a system with 50 sub-networks and <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Note that all reported simulation data correspond to the mean firing rates acquired over a period of 10 s and averaged across 5 trials per condition. <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> shows the firing rates as a function of the input intensity <inline-formula><mml:math id="inf25"><mml:mi>λ</mml:mi></mml:math></inline-formula>.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Code and data for <xref ref-type="fig" rid="fig2">Figure 2</xref> and related figure supplements.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-77009-fig2-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Mean-field predictions for the gain in the firing rates of stimulated sub-populations.</title><p>(<bold>a</bold>) <inline-formula><mml:math id="inf26"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mn>3</mml:mn><mml:mi mathvariant="normal">S</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">S</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and (<bold>b</bold>) <inline-formula><mml:math id="inf27"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mn>5</mml:mn><mml:mi mathvariant="normal">S</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mn>4</mml:mn><mml:mi mathvariant="normal">S</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, as a function of modularity <inline-formula><mml:math id="inf28"><mml:mi>m</mml:mi></mml:math></inline-formula> and input intensity, scaled by <inline-formula><mml:math id="inf29"><mml:mi>λ</mml:mi></mml:math></inline-formula> (see Materials and methods). Dashed lines demarcate the transition to positive gain.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig2-figsupp1-v2.tif"/></fig></fig-group><p>Mean-field analysis of the stationary network activity (see Materials and methods and Appendix B) predicts that the firing rates of the stimulus-specific sub-populations increase systematically with modularity, whereas the untuned neurons are gradually silenced (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, left). At the transition point <inline-formula><mml:math id="inf30"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>switch</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mn>0.83</mml:mn></mml:mrow></mml:math></inline-formula>, mean firing rates across the different sub-networks converge, which translates into a globally uniform signal encoding capacity, corresponding to the zero-gain convergence point in <xref ref-type="fig" rid="fig1">Figure 1d, g</xref>. As the degree of modularity increases beyond this point, the self-consistent state is lost again as the functional dynamics across the network shifts toward a gradual response sharpening, whereby the activity of stimulus-tuned neurons become increasingly dominant (<xref ref-type="fig" rid="fig2">Figure 2a–c</xref>). The effect is more pronounced for the deeper sub-networks. Note that the analytical results match well with those obtained by numerical simulation (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, right).</p><p>In the limit of very deep networks (up to 50 SSNs, <xref ref-type="fig" rid="fig2">Figure 2d</xref>) the system becomes bistable, with rates converging to either a high-activity state associated with signal amplification or a low-activity state driven by the background input. The transition point is observed at a modularity value of <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.83</mml:mn></mml:mrow></mml:math></inline-formula>, matching the results reported so far. Below this value, elevated activity in the stimulated sub-populations can be maintained across the initial sub-networks (&lt;10), but eventually dies out; the rate of all neurons decays and information about the input cannot reach the deeper populations. Importantly, for <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.83</mml:mn></mml:mrow></mml:math></inline-formula>, the transition toward the high-activity state is slower. This allows the input signal to faithfully propagate across a large number of sub-networks (<inline-formula><mml:math id="inf33"><mml:mrow><mml:mi/><mml:mo>≈</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula>), without being driven into implausible activity states.</p></sec><sec id="s2-3"><title>E/I balance and asymmetric effective couplings</title><p>The departure from the balanced activity in the initial sub-networks can be better understood by zooming in at the synaptic level and analyzing how topography influences the synaptic input currents. The segregation of feedforward projections into stimulus-specific pathways breaks the symmetry between excitation and inhibition (see <xref ref-type="fig" rid="fig3">Figure 3a</xref>) that characterizes the balanced state (<xref ref-type="bibr" rid="bib26">Haider et al., 2006</xref>; <xref ref-type="bibr" rid="bib75">Shadlen and Newsome, 1994</xref>), for which the first two sub-networks were tuned (see Materials and methods). E/I balance is thus systematically shifted toward excitation in the stimulated populations and inhibition in the non-stimulated ones. Neurons belonging to sub-populations associated with the active stimulus receive significantly more net overall excitation, whereas the other neurons become gradually more inhibited. This disparity grows not only with modularity but also with network depth. Overall, across the whole system, increasing modularity results in an increasingly inhibition-dominated dynamical regime (inset in <xref ref-type="fig" rid="fig3">Figure 3a</xref>), whereby stronger effective inhibition silences non-stimulated populations, thus sharpening stimulus/feature representations by concentrating activity in the stimulus-driven sub-populations.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Asymmetric effective couplings modulate the E/I balance and support sequential denoising.</title><p>(<bold>a</bold>) Mean synaptic input currents for neurons in the stimulated (solid curves) and non-stimulated (dashed curves) excitatory sub-populations in the different SSNs. To avoid clutter, data for SSN<sub>0</sub> are only shown by markers (independent of <inline-formula><mml:math id="inf34"><mml:mi>m</mml:mi></mml:math></inline-formula>). Inset shows the currents (in pA) averaged over all excitatory neurons in the different sub-networks; increasing modularity leads to a dominance of inhibition in the deeper sub-networks. Color shade represents depth, from SSN<sub>1</sub> (light) to SSN<sub>5</sub> (dark). (<bold>b</bold>) Mean-field approximation of the effective recurrent weights in SSN<sub>5</sub>. Curve shade and style as in (<bold>a</bold>). (<bold>c</bold>) Spectral radius of the effective connectivity matrices <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as a function of modularity. (<bold>d</bold>) Eigenvalue spectra for the effective coupling matrices in SSN<sub>5</sub>, for <inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula> (top) and <inline-formula><mml:math id="inf37"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula> (bottom). The largest negative eigenvalue (outlier, see Materials and methods), characteristic of inhibition-dominated networks, is omitted for clarity.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Code and data for <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-77009-fig3-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig3-v2.tif"/></fig><p>To gain an intuitive understanding of these effects from a dynamical systems perspective, we linearize the network dynamics around the stationary working points of the individual populations (<xref ref-type="bibr" rid="bib81">Tetzlaff et al., 2012</xref>) in order to obtain the effective connectivity <inline-formula><mml:math id="inf38"><mml:mi>W</mml:mi></mml:math></inline-formula> of the system (see Materials and methods and Appendix B). The effective impact of a single spike from a presynaptic neuron <inline-formula><mml:math id="inf39"><mml:mi>j</mml:mi></mml:math></inline-formula> on the firing rate of a postsynaptic neuron <inline-formula><mml:math id="inf40"><mml:mi>i</mml:mi></mml:math></inline-formula> (the effective weight <inline-formula><mml:math id="inf41"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>) is determined not only by the synaptic efficacies <inline-formula><mml:math id="inf42"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, but also by the statistics of the synaptic input fluctuations to the target cell <inline-formula><mml:math id="inf43"><mml:mi>i</mml:mi></mml:math></inline-formula> that determine its excitability (see Materials and methods, <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>). This analysis reveals that there is an increase in the effective synaptic input onto neurons in the stimulated sub-populations as a function of modularity (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). Conversely, non-stimulated neurons effectively receive weaker excitatory (and stronger inhibitory) drive and become increasingly less responsive (see <xref ref-type="fig" rid="fig3">Figure 3a, b</xref>). The role of topographic modularity in denoising can thus be understood as a transient, stimulus-specific change in effective connectivity.</p><p>For low and moderate topographic precision (<inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≲</mml:mo><mml:mn>0.83</mml:mn></mml:mrow></mml:math></inline-formula>), denoising does not occur as the effective weights are sufficiently similar to maintain a stable E/I balance across all populations and sub-networks (<xref ref-type="fig" rid="fig3">Figure 3a, b</xref>), resulting in a relatively uniform global dynamical state (indicated in <xref ref-type="fig" rid="fig3">Figure 3c</xref> by a constant spectral radius for <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≲</mml:mo><mml:mn>0.83</mml:mn></mml:mrow></mml:math></inline-formula>, see also Materials and methods) and stable linearized dynamics (<inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>However, as the feedforward projections become more structured, the system undergoes qualitative changes: after a weak transient (<inline-formula><mml:math id="inf47"><mml:mrow><mml:mn>0.83</mml:mn><mml:mo>≲</mml:mo><mml:mi>m</mml:mi><mml:mo>≲</mml:mo><mml:mn>0.85</mml:mn></mml:mrow></mml:math></inline-formula>) the spectral radius <inline-formula><mml:math id="inf48"><mml:mi>ρ</mml:mi></mml:math></inline-formula> in the deep SSNs expands due to the increased effective coupling to the stimulated sub-population (<xref ref-type="fig" rid="fig3">Figure 3b</xref>); the spectral radius eventually (<inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≳</mml:mo><mml:mn>0.85</mml:mn></mml:mrow></mml:math></inline-formula>) contracts with increasing modularity (<xref ref-type="fig" rid="fig3">Figure 3c, d</xref>). Given that <inline-formula><mml:math id="inf50"><mml:mi>ρ</mml:mi></mml:math></inline-formula> is determined by the variance of <inline-formula><mml:math id="inf51"><mml:mi>W</mml:mi></mml:math></inline-formula>, that is heterogeneity across connections (<xref ref-type="bibr" rid="bib64">Rajan and Abbott, 2006</xref>), this behavior is expected: most weights are in the non-stimulated pathways, which decrease with larger <inline-formula><mml:math id="inf52"><mml:mi>m</mml:mi></mml:math></inline-formula> and network depth (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). Strong inhibitory currents (<xref ref-type="fig" rid="fig3">Figure 3a</xref>) suppress the majority of neurons, thereby reducing noise, as demonstrated by the collapse of the bulk of the eigenvalues toward the center for larger <inline-formula><mml:math id="inf53"><mml:mi>m</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). Indicative of a more constrained state space, this contractive effect suggests that population activity becomes gradually entrained by the spatially encoded input along the stimulated pathway, whereas the responses of the non-stimulated neurons have a diminishing influence on the overall behavior.</p><p>By biasing the effective connectivity of the system, precise topography can thus modulate the balance of excitation and inhibition in the different sub-networks, concentrating the activity along specific pathways. This results in both a systematic amplification of stimulus-specific responses and a systematic suppression of noise (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). The sharpness/precision of topographic specificity along these pathways thus acts as a critical control parameter that largely determines the qualitative behavior of the system and can dramatically alter its responsiveness to external inputs.</p></sec><sec id="s2-4"><title>Modulating inhibition</title><p>How can the system generate and maintain the elevated inhibition underlying such a noise-suppressing regime? On the one hand, feedforward excitatory input may increase the activity of certain excitatory neurons in <inline-formula><mml:math id="inf54"><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula> of sub-network <inline-formula><mml:math id="inf55"><mml:msub><mml:mi>SSN</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula>, which, in turn, can lead to increased mean inhibition through local recurrent connections. On the other hand, denoising could depend strongly on the concerted topographic projections onto <inline-formula><mml:math id="inf56"><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula>. Such structured feedforward inhibition is known to play important functional roles in, for example, sharpening the spatial contrast of somatosensory stimuli (<xref ref-type="bibr" rid="bib53">Mountcastle and Powell, 1959</xref>) or enhancing coding precision throughout the ascending auditory pathways (<xref ref-type="bibr" rid="bib68">Roberts et al., 2013</xref>).</p><p>To investigate whether recurrent activity alone can generate sufficiently strong inhibition for signal transmission and denoising, we maintained the modular structure between the excitatory populations and randomized the feedforward projections onto the inhibitory ones (<inline-formula><mml:math id="inf57"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf58"><mml:mrow><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, compare top panels of <xref ref-type="fig" rid="fig4">Figure 4a, b</xref>). This leads to unstable firing patterns in the downstream sub-networks, characterized by significant accumulation of synchrony and increased firing rates (see bottom panels of <xref ref-type="fig" rid="fig4">Figure 4a, b</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1a, b</xref>). These effects, known to result from shared pre-synaptic excitatory inputs (see e.g. <xref ref-type="bibr" rid="bib76">Shadlen and Newsome, 1998</xref>; <xref ref-type="bibr" rid="bib80">Tetzlaff et al., 2003</xref>; <xref ref-type="bibr" rid="bib37">Kumar et al., 2008a</xref>), are more pronounced for larger <inline-formula><mml:math id="inf59"><mml:mi>m</mml:mi></mml:math></inline-formula> and network depth (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Compared with the baseline network, whose activity shows clear spatially encoded stimuli (sequential activation of stimulus-specific sub-populations [<xref ref-type="fig" rid="fig4">Figure 4a</xref>, bottom]), removing structure from the projections onto inhibitory neurons abolishes the effect and prevents accurate signal transmission.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Modular projections to inhibitory populations stabilize network dynamics.</title><p>Raster plots show 1 s of spiking activity of 1000 randomly chosen neurons in SSN<sub>5</sub>, for different network configurations. (<bold>a</bold>) Baseline network with <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.88</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>b</bold>) Unstructured feedforward projections to the inhibitory sub-populations lead to highly synchronized network activity, hindering signal representation. (<bold>c</bold>) Same as the baseline network in (<bold>a</bold>), but with random projections for <inline-formula><mml:math id="inf61"><mml:mrow><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and additional but unspecific (Poissonian) excitatory input to <inline-formula><mml:math id="inf62"><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:math></inline-formula> controlled via <inline-formula><mml:math id="inf63"><mml:msubsup><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">X</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula>. Without such input (<inline-formula><mml:math id="inf64"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">X</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, left), the activity is strongly synchronous, but this is compensated for by the additional excitation, reducing synchrony and restoring the denoising property (<inline-formula><mml:math id="inf65"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">X</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> spikes/s, right). <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> depicts the activity statistics in the last two modules, for the different scenarios.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Code and data for <xref ref-type="fig" rid="fig4">Figure 4</xref> and related figure supplements.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-77009-fig4-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Spiking statistics for different feedforward wiring to inhibitory neurons.</title><p>(<bold>a</bold>) Mean firing rates (top panel) and synchrony (Pearson’s correlation coefficient, computed pairwise over spikes binned into 2 ms bins and averaged across 500 pairs, lower panel) in SSN<sub>4</sub> and SSN<sub>5</sub>, as a function of modularity. (<bold>b</bold>) Same as (<bold>a</bold>), except with random feedforward projections to the inhibitory pools, that is, <inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> connections, <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0.4</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>c</bold>) Same as the baseline network in (<bold>a</bold>), with <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> only for <inline-formula><mml:math id="inf70"><mml:mrow><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. In addition, each neuron in <inline-formula><mml:math id="inf71"><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:math></inline-formula> receives further excitatory background input with intensity <inline-formula><mml:math id="inf72"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">X</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">X</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">X</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>. Statistics are computed as a function of the additional rate <inline-formula><mml:math id="inf73"><mml:msubsup><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">X</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig4-figsupp1-v2.tif"/></fig></fig-group><p>These effects of unstructured inhibitory projections are so marked that they can be observed even if a single set of projections is modified: this can be seen in <xref ref-type="fig" rid="fig4">Figure 4c</xref>, where only the <inline-formula><mml:math id="inf74"><mml:mrow><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> connections are randomized. It is worth noting, however, that the excessive synchronization that results from unstructured inhibitory projections (<xref ref-type="fig" rid="fig4">Figure 4c</xref>, bottom left, no additional input condition) can be easily counteracted by driving <inline-formula><mml:math id="inf75"><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:math></inline-formula> (the inhibitory population that receives only unstructured projections) with additional uncorrelated external input. If strong enough (<inline-formula><mml:math id="inf76"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">X</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>≈</mml:mo><mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>sec</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), this additional external drive pushes the inhibitory population into an asynchronous regime that restores the sharp, stimulus-specific responses in the excitatory population of the corresponding sub-network (see <xref ref-type="fig" rid="fig4">Figure 4c</xref>, bottom right, and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1c</xref>).</p><p>These results emphasize the control of inhibitory neurons’ responsiveness as the main causal mechanism behind the effects reported. Elevated local inhibition is strictly required, but whether this is achieved by tailored, stimulus-specific activation of inhibitory sub-populations, or by uncorrelated excitatory drive onto all inhibitory neurons appears to be irrelevant and both conditions result in sharp, stimulus-tuned responses in the excitatory populations.</p></sec><sec id="s2-5"><title>A generalizable structural effect</title><p>We have demonstrated that, by controlling the different sub-networks’ operating point, the sharpness of feedforward projections allows the architecture to systematically improve the quality of internal representations and retrieve the input structure, even if profoundly corrupted by noise. In this section, we investigate the robustness of the phenomenon in order to determine whether it can be entirely ascribed to the topographic projections (a structural/architectural feature) or if the particular choices of models and model parameters for neuronal and synaptic dynamics contribute to the effect.</p><p>To do so, we study two alternative model systems on the signal denoising task. These are structured similar to the baseline system explored so far, comprising separate sequential sub-networks with modular feedforward projections among them (see <xref ref-type="fig" rid="fig1">Figure 1</xref> and Materials and methods), but vary in total size, neuronal and synaptic dynamics. In the first test case, only the models of synaptic transmission and corresponding parameters are altered. To increase biological verisimilitude and following <xref ref-type="bibr" rid="bib93">Zajzon et al., 2019</xref>, synaptic transmission is modeled as a conductance-based process, with different kinetics for excitatory and inhibitory transmission, corresponding to the responses of <inline-formula><mml:math id="inf77"><mml:mi>AMPA</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>GABA</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub></mml:math></inline-formula> receptors, respectively, see Materials and methods and <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref> for details. The results, illustrated in <xref ref-type="fig" rid="fig5">Figure 5a</xref>, demonstrate that task performance and population activity across the network follow a similar trend to the baseline model (<xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2a, b</xref>). Despite severe noise corruption, the system is able to generate a clear, discernible representation of the input as early as SSN<sub>2</sub> and can accurately reconstruct the signal. Importantly, the relative improvement with increasing modularity and network depth is retained. In comparison to the baseline model, the transition occurs for a slightly different topographic configuration, <inline-formula><mml:math id="inf79"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>switch</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mn>0.85</mml:mn></mml:mrow></mml:math></inline-formula>, at which point the network dynamics converges toward a low-rate, stable asynchronous irregular regime across all populations, facilitating a linear firing rate propagation along the topographic maps (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Denoising through modular topography is a robust structural effect.</title><p>(<bold>a</bold>) Signal reconstruction (top) and corresponding network activity (bottom) for a network with leaky integrate-and-fire (LIF) neurons and conductance-based synapses (see Materials and methods). Single-trial illustration of target signal (black step function) and readout output (red curves) in three different SSNs, for <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula> and strong noise corruption (<inline-formula><mml:math id="inf81"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>). For simplicity, only two out of ten input channels are shown. <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> shows additional activity statistics. (<bold>b</bold>) As in (<bold>a</bold>) for a rate-based model with <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf83"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (see Materials and methods for details).</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Code and data for <xref ref-type="fig" rid="fig5">Figure 5</xref> and related figure supplements.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-77009-fig5-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Spiking statistics for the conductance-based model.</title><p>(<bold>a</bold>) Synchrony (Pearson’s correlation coefficient, computed pairwise over spikes binned into 2 ms bins and averaged across 500 pairs); (<bold>b</bold>) Irregularity measured as the coefficient of variation (CV); (<bold>c</bold>) Mean firing rate across the excitatory populations. All depicted statistics were averaged over five simulations, each lasting 5 s, with 10 input stimuli.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig5-figsupp1-v2.tif"/></fig></fig-group><p>The second test case is a smaller and simpler network of nonlinear rate neuron models (see <xref ref-type="fig" rid="fig5">Figure 5b</xref> and Materials and methods) which interact via continuous signals (rates) rather than discontinuities (spikes). Despite these profound differences in the neuronal and synaptic dynamics, the same behavior is observed, demonstrating that sequential denoising is a structural effect, dependent on the population firing rates and thus less sensitive to fluctuations in the precise spike times. Moreover, the robustness with respect to the network size suggests that denoising could also be performed in smaller, localized circuits, possibly operating in parallel on different features of the input stimuli.</p></sec><sec id="s2-6"><title>Variable map sizes</title><p>Despite their ubiquity throughout the neocortex, the characteristics of structured projection pathways is far from uniform (<xref ref-type="bibr" rid="bib5">Bednar and Wilson, 2016</xref>), exhibiting marked differences in spatial precision and specificity, aligned with macroscopic gradients of cortical organization. This non-uniformity may play an important functional role supporting feature aggregation (<xref ref-type="bibr" rid="bib25">Hagler and Sereno, 2006</xref>) and the development of mixed representations (<xref ref-type="bibr" rid="bib59">Patel et al., 2014</xref>) in higher (more anterior) cortical areas. Here, we consider two scenarios in the baseline (current-based) model to examine the robustness of our findings to more complex topographic configurations.</p><p>First, we varied the size of stimulus-tuned sub-populations (parametrized by <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>d</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula>, see Materials and methods) but kept them fixed across the network. For small sub-populations and intermediate degrees of topographic modularity, the activity along the stimulated pathway decays with network depth, suggesting that input information does not reach the deeper SSNs (see <xref ref-type="fig" rid="fig6">Figure 6a</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). These results place a lower bound on the size of stimulus-tuned sub-populations below which no signal propagation can occur, as reflected by the negative gain in performance for <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). Whereas denoising is robust to variation around the baseline value of <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> that yielded perfect partitioning of the feedforward projections (see Supplementary Materials), an upper bound may emerge due to increasing overlap between the maps (<inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> in <xref ref-type="fig" rid="fig6">Figure 6b</xref>). In this case, the activity may ‘spill over’ to other pathways than the stimulated one, corrupting the input representations and hindering accurate transmission and decoding. This can be alleviated by reduced or no overlap (as in <xref ref-type="fig" rid="fig6">Figure 6a</xref>), in which case signal propagation and denoising is successful for larger map sizes (<inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> also for <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). We thus observe a trade-off between map size, overlap and the degree of topographic precision that is required to accurately propagate stimulus representations (see Discussion).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Variation in the map sizes.</title><p>(<bold>a</bold>) Ratio of the firing rates of the stimulated sub-populations in the first and last sub-networks, <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, as a function of modularity and map size (parameterized by <inline-formula><mml:math id="inf91"><mml:mi>d</mml:mi></mml:math></inline-formula> and constant throughout the network, that is <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, see Materials and methods). Depicted values correspond to stationary firing rates predicted by mean-field theory, smoothed using a Lanczos filter. Note that, in order to ensure that every neuron was uniquely tuned, that is there is no overlap between stimulus-specific sub-populations, the number of sub-populations was igen</p><p>chosen to be proportional to the map size (<inline-formula><mml:math id="inf93"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). (<bold>b, c</bold>) Performance gain in SSN<sub>5</sub> relative to SSN<sub>0</sub> (ten stimuli, as in <xref ref-type="fig" rid="fig1">Figure 1d, g</xref>), for varying properties of structural mappings: (<bold>b</bold>) fixed map size (<inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) with color shade denoting map size, and (<bold>c</bold>) linearly increasing map size (<inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) and a smaller initial map size <inline-formula><mml:math id="inf96"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn></mml:mrow></mml:math></inline-formula>. The results depict the average performance gains measured across five trials, using the current-based model illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref> (ten stimuli) and no input noise (<inline-formula><mml:math id="inf97"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> further illustrates how the activity varies across the modules as a function of the map size.</p><p><supplementary-material id="fig6sdata1"><label>Figure 6—source data 1.</label><caption><title>Code and data for <xref ref-type="fig" rid="fig6">Figure 6</xref> and related figure supplements.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-77009-fig6-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Transition point in modularity decreases with larger map sizes.</title><p>(<bold>a</bold>) Mean-field predictions for the stationary firing rates of the stimulated (left) and non-stimulated sub-populations (right) in SSN<sub>5</sub>, as a function of modularity <inline-formula><mml:math id="inf98"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and fixed map size (parametrized by <inline-formula><mml:math id="inf99"><mml:mi>d</mml:mi></mml:math></inline-formula>, see Materials and methods) across the modules <inline-formula><mml:math id="inf100"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). To limit the impact of additional parameters when varying the map sizes (e.g., overlap), the number of stimulus-specific sub-populations and <inline-formula><mml:math id="inf101"><mml:mi>d</mml:mi></mml:math></inline-formula> where chosen such that every neuron in each population belonged to exactly one stimulus-specific sub-population (see main text for more details). (<bold>b</bold>) Predicted firing rates in the stimulated sub-populations of the different sub-networks, for <inline-formula><mml:math id="inf102"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> (left) and <inline-formula><mml:math id="inf103"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> (right), with <inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig6-figsupp1-v2.tif"/></fig></fig-group><p>Second, we took into account the fact that these structural features are known to vary with hierarchical depth resulting in increasingly larger sub-populations and, consequently, increasingly overlapping stimulus selectivity (<xref ref-type="bibr" rid="bib79">Smith et al., 2001</xref>; <xref ref-type="bibr" rid="bib59">Patel et al., 2014</xref>; <xref ref-type="bibr" rid="bib5">Bednar and Wilson, 2016</xref>). To capture this effect, we introduce a linear scaling of map size with depth (<inline-formula><mml:math id="inf105"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi>i</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, see Materials and methods). The ability of the circuit to gradually clean the signal’s representation is fully preserved, as illustrated in <xref ref-type="fig" rid="fig6">Figure 6c</xref>. In fact, for intermediate modularity (<inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) broadening the projections can further sharpen the reconstruction precision (compare curves for <inline-formula><mml:math id="inf108"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>).</p><p>Taken together, these observations demonstrate that a gradual denoising of stimulus inputs can occur entirely as a consequence of the modular wiring between the subsequent processing circuits. Importantly, this effect generalizes well across diverse neuron and synapse models, as well as key system properties, making modular topography a potentially universal circuit feature for handling noisy data streams.</p></sec><sec id="s2-7"><title>Modularity as a bifurcation parameter</title><p>The results so far indicate that the modular topographic projections, more so than the individual characteristics of neurons and synapses, lead to a sequential denoising effect through a joint process of signal amplification and noise suppression. To better understand how the system transitions to such an operating regime, it is helpful to examine its macroscopic dynamics in the limit of many sub-networks (<xref ref-type="bibr" rid="bib84">Toyoizumi, 2012</xref>; <xref ref-type="bibr" rid="bib11">Cayco-Gajic and Shea-Brown, 2013</xref>; <xref ref-type="bibr" rid="bib33">Kadmon and Sompolinsky, 2016</xref>). We apply standard mean-field techniques (<xref ref-type="bibr" rid="bib22">Fourcaud and Brunel, 2002</xref>; <xref ref-type="bibr" rid="bib28">Helias et al., 2013</xref>; <xref ref-type="bibr" rid="bib72">Schuecker et al., 2015</xref>) to find the asymptotic firing rates (fixed points across sub-networks) of the stimulated and non-stimulated sub-populations as a function of topography (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). For this, we can approximate the input <italic>μ</italic> to a group of neurons as a linear function of its firing rate <inline-formula><mml:math id="inf110"><mml:mi>ν</mml:mi></mml:math></inline-formula> with a slope <inline-formula><mml:math id="inf111"><mml:mi>κ</mml:mi></mml:math></inline-formula> that is determined by the coupling within the group and an offset given by inputs from other groups of neurons (orange line in <xref ref-type="fig" rid="fig7">Figure 7a</xref>). With an approximately sigmoidal rate transfer function, the self-consistent solutions are at the intersections marked in <xref ref-type="fig" rid="fig7">Figure 7a</xref>.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Modularity changes the fixed point structure of the system.</title><p>(<bold>a</bold>) Sketch for self-consistent solution (for the full derivation, see Appendix B) for the firing rate of the stimulated sub-population (blue curves) and the linear relation <inline-formula><mml:math id="inf112"><mml:mrow><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>-</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (orange lines), in the limit of infinitely deep networks. Squares denote stable (black) and unstable (red) fixed points where input and output rates are the same. (<bold>b</bold>) Bifurcation diagram obtained from numerical evaluation of the mean-field self-consistency equations, <xref ref-type="disp-formula" rid="equ9 equ10">Equations 9 and 10</xref> showing a single stable fixed point in the fading regime, and multiple stable (black) and unstable (red) fixed points in the active regime where denoising occurs. (<bold>c</bold>) Potential energy of the mean activity (see Materials and methods and <xref ref-type="disp-formula" rid="equ24">Equation 22</xref> in Appendix B) for increasing topographic modularity. A stable state, corresponding to local minimum in the potential, exists at a low non-zero rate in every case, including for <inline-formula><mml:math id="inf113"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math></inline-formula> (gray dashed curves, inset). For <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≥</mml:mo><mml:mn>0.76</mml:mn></mml:mrow></mml:math></inline-formula> (colored solid curves), a second fixed point appears at progressively larger firing rates. (<bold>d</bold>) Theoretical predictions for the stationary firing rates of the stimulated and non-stimulated sub-populations in SSN<sub>0</sub>, as a function of stimulus intensity (<inline-formula><mml:math id="inf115"><mml:mi>λ</mml:mi></mml:math></inline-formula>, see Materials and methods). Low, standard, and high denote <inline-formula><mml:math id="inf116"><mml:mi>λ</mml:mi></mml:math></inline-formula> values of 0.01, 0.05 (baseline value used in <xref ref-type="fig" rid="fig1">Figure 1</xref>), and 0.25, respectively. (<bold>e</bold>) Sketch of attractor basins in the potential for different values of <inline-formula><mml:math id="inf117"><mml:mi>m</mml:mi></mml:math></inline-formula>. Markers correspond to the highlighted initial states in (<bold>d</bold>), with solid and dashed arrows indicating attraction toward the high- and low-activity state, respectively. (<bold>f</bold>) Firing rates of the stimulated sub-population as a function of modularity in the limit of infinite sub-networks, for the three different <inline-formula><mml:math id="inf118"><mml:mi>λ</mml:mi></mml:math></inline-formula> marked in (<bold>d</bold>). (<bold>g</bold>) Modularity threshold for the active regime shifts with increasing noise in the input, modeled as additional input to the non-stimulated sub-populations in SSN<sub>0</sub>. <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref> show the dependency of the effective feedforward couplings on different parameters. Note that all panels (except (<bold>a</bold>)) show theoretical predictions obtained from numerical evaluation of the mean-field self-consistency equations.</p><p><supplementary-material id="fig7sdata1"><label>Figure 7—source data 1.</label><caption><title>Code and data for <xref ref-type="fig" rid="fig7">Figure 7</xref> and related figure supplements.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-77009-fig7-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Mean-field predictions for the gain in the firing rates of stimulated sub-populations.</title><p>(<bold>a</bold>) <inline-formula><mml:math id="inf119"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mn>3</mml:mn><mml:mi mathvariant="normal">S</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">S</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and (<bold>b</bold>) <inline-formula><mml:math id="inf120"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mn>5</mml:mn><mml:mi mathvariant="normal">S</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mn>4</mml:mn><mml:mi mathvariant="normal">S</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, as a function of modularity <inline-formula><mml:math id="inf121"><mml:mi>m</mml:mi></mml:math></inline-formula> and input intensity, scaled by <inline-formula><mml:math id="inf122"><mml:mi>λ</mml:mi></mml:math></inline-formula> (see Materials and methods). Dashed lines demarcate the transition to positive gain.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig7-figsupp1-v2.tif"/></fig></fig-group><p>Formally, all neurons in the deep sub-networks of one topographic map form such a group as they share the same firing rate (asymptotic value). The coupling <inline-formula><mml:math id="inf123"><mml:mi>κ</mml:mi></mml:math></inline-formula> within this group comprises not only recurrent connections of one sub-network but also modular feedforward projections across sub-networks. For small modularity, the group is in an inhibition-dominated regime (<inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) and we obtain only one fixed point at low activity (<xref ref-type="fig" rid="fig7">Figure 7a</xref>, left). Importantly, the firing rate of this fixed point is the same for stimulated and non-stimulated topographic maps. Any influence of input signals applied to SSN<sub>0</sub> therefore vanishes in the deeper sub-networks and the signal cannot be reconstructed (<italic>fading</italic> regime). As topographic projections become more concentrated (larger <inline-formula><mml:math id="inf125"><mml:mi>m</mml:mi></mml:math></inline-formula>), <inline-formula><mml:math id="inf126"><mml:mi>κ</mml:mi></mml:math></inline-formula> changes sign and gradually leads to two additional fixed points (as conceptually illustrated in <xref ref-type="fig" rid="fig7">Figure 7a</xref> and quantified in <xref ref-type="fig" rid="fig7">Figure 7b</xref> by numerically solving the self-consistent mean-field equations, see also Appendix B): an unstable one (red) that eventually vanishes with increasing <inline-formula><mml:math id="inf127"><mml:mi>m</mml:mi></mml:math></inline-formula> and a stable high-activity fixed point (black). The bistability opens the possibility to distinguish between stimulated and non-stimulated topographic maps and thereby reconstruct the signal in deep sub-networks: in the <italic>active</italic> regime beyond the <italic>critical modularity threshold</italic> (here <inline-formula><mml:math id="inf128"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≥</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>crit</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.76</mml:mn></mml:mrow></mml:math></inline-formula>), a sufficiently strong input signal can drive the activity along the stimulated map to the high-activity fixed point, such that it can permeate the system, while the non-stimulated sub-populations still converge to the low-activity fixed point. Note that this critical modularity represents the minimum modularity value for which bistability emerges. It typically differs from the actual switching point <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, which additionally depends on the input intensity.</p><p>In the potential energy landscape <inline-formula><mml:math id="inf130"><mml:mi>U</mml:mi></mml:math></inline-formula> (see Materials and methods), where stable fixed points correspond to minima, the bistability that emerges for more structured topography <inline-formula><mml:math id="inf131"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≥</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>crit</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.76</mml:mn></mml:mrow></mml:math></inline-formula> can be understood as a transition from a single minimum at low rates (<xref ref-type="fig" rid="fig7">Figure 7c</xref>, inset) to a second minimum associated with the high-activity state (<xref ref-type="fig" rid="fig7">Figure 7c</xref>). Even though the full dynamics of the spiking network away from the fixed point cannot be entirely understood in this simplified potential picture (see Appendix B), qualitatively, more strongly modular networks cause deeper potential wells, corresponding to more attractive dynamical states and higher firing rates (see <xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>).</p><p>Because the intensity of the input signal dictates the rate of different populations in the initial sub-network SSN<sub>0</sub> (<xref ref-type="fig" rid="fig7">Figure 7d</xref>), it also determines, for any given modularity, whether the rate of the stimulated sub-population is in the basin of attraction of the high-activity (see <xref ref-type="fig" rid="fig7">Figure 7e</xref>, solid markers and arrows) or low-activity (dashed, blue marker and arrow) fixed point. Denoising, and therefore increasing signal reconstruction, is thus achieved by successively (across sub-networks) pushing the population states toward the self-consistent firing rates.</p><p>As reported above, for the baseline network and (standard) input (<inline-formula><mml:math id="inf132"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>) used in <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref>, the switching point between low and high activity is at <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.83</mml:mn></mml:mrow></mml:math></inline-formula> (blue markers in <xref ref-type="fig" rid="fig7">Figure 7d, f</xref>). Stronger input signals move the switching point toward the minimal modularity <inline-formula><mml:math id="inf134"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.76</mml:mn></mml:mrow></mml:math></inline-formula> of the active regime (black markers in <xref ref-type="fig" rid="fig7">Figure 7d, f</xref>), while weaker inputs only induce a switch at larger modularities (gray markers in <xref ref-type="fig" rid="fig7">Figure 7d, f</xref>).</p><p>Noise in the input simply shifts the transition point to the high-activity state in a similar manner, with more modular connectivity required to compensate for stronger jitter (<xref ref-type="fig" rid="fig7">Figure 7g</xref>). However, as long as the mean firing rate of the stimulated sub-population in SSN<sub>0</sub> is slightly higher than that of the non-stimulated ones (up to 0.5 spks/sec), it is sufficient to position the system in the attracting basin of the high-rate fixed point and the system is able to clean the signal representation. This indicates a remarkably robust denoising mechanism.</p></sec><sec id="s2-8"><title>Critical modularity for denoising</title><p>In addition to properties of the input, the critical modularity marking the onset of the active regime is also influenced by neuronal and connectivity features. To build some intuition, it is helpful to consider the sigmoidal activation function of spiking neurons (<xref ref-type="fig" rid="fig8">Figure 8a</xref>). The nonlinearity of this function prohibits us from obtaining quantitative, closed-form analytical expressions for the critical modularity and requires a numerical solution of the self-consistency equations (<xref ref-type="fig" rid="fig7">Figure 7b</xref>). However, since the continuous rate model shows a qualitatively similar behavior to the spiking baseline model (see Section ‘A generalizable structural effect’), we can study a fully analytically tractable model with piecewise linear activation function (<xref ref-type="fig" rid="fig8">Figure 8a, b</xref>) to expose the dependence of the critical modularity on both neuron and network properties (see detailed derivations in Appendix B).</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Dependence of critical modularity on neuron and connectivity features.</title><p>(<bold>a</bold>) Activation function <inline-formula><mml:math id="inf135"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for leaky integrate-and-fire model as a function of the mean input <italic>μ</italic> for <inline-formula><mml:math id="inf136"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (black to gray) and piecewise linear approximation with qualitatively similar shape (red). (<bold>b</bold>) Bifurcation diagram as in <xref ref-type="fig" rid="fig7">Figure 7b</xref>, but for piecewise linear activation function shown in inset. Low-activity fixed points at zero rate are not shown, which is the case throughout for the non-stimulated sub-populations. This panel corresponds to the cross-section marked by the gray dashed lines in (<bold>c</bold>), at <inline-formula><mml:math id="inf137"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">X</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math></inline-formula>. Likewise, the vertical cyan bar corresponds to the lower bound on modularity depicted by the cyan curve in (<bold>c</bold>) for the same value <inline-formula><mml:math id="inf138"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">X</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>c</bold>) Analytically derived bounds on modularity (purple line corresponds to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, cyan curve to <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) as a function of external input for the baseline model with inhibition-dominated recurrent connectivity (<inline-formula><mml:math id="inf139"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). Shaded regions denote positions of stable (black) and unstable (red) fixed points with <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf141"><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mi>NS</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Hatched area represents region with stable fixed points at saturated rates. Denoising occurs in all areas with stable fixed points (hatched and black shaded regions). Negative values on the <italic>x</italic>-axis correspond to inhibitory external background input with rate <inline-formula><mml:math id="inf142"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>ν</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula>. (<bold>d</bold>) Same as panel (<bold>c</bold>) for networks with no recurrent connectivity within the SSNs (green curve defined by <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>). (<bold>e</bold>) Same as panel (<bold>c</bold>) for networks with excitation-dominated connectivity within SSNs (<inline-formula><mml:math id="inf143"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). (<bold>f</bold>) Same as <xref ref-type="fig" rid="fig7">Figure 7b</xref>, obtained through numerical evaluation of the mean-field self-consistent equations for the spiking model. All non-zero fixed points are stable, with points representing stimulated (circle) and non-stimulated (cross) populations overlapping. (<bold>g</bold>) Mean firing rates across the SSNs in the current-based (baseline) model with no recurrent connections, obtained from 5 s of network simulations and averaged over five trials. (<bold>h, i</bold>) Same as (<bold>f, g</bold>) for networks with excitation-dominated connectivity.</p><p><supplementary-material id="fig8sdata1"><label>Figure 8—source data 1.</label><caption><title>Code and data for <xref ref-type="fig" rid="fig8">Figure 8</xref> and related figure supplements.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-77009-fig8-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig8-v2.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Influence of the activation function’s dynamic range on the bifurcation behavior in excitation-dominated networks (<inline-formula><mml:math id="inf144"><mml:mrow><mml:mi>g</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mrow><mml:mo mathvariant="normal">-</mml:mo><mml:mn mathvariant="normal">3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, see also <xref ref-type="fig" rid="fig8">Figure 8e</xref>).</title><p>(<bold>a</bold>) The baseline dynamic range <inline-formula><mml:math id="inf145"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>15</mml:mn><mml:mo>,</mml:mo><mml:mn>150</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> is extended to <inline-formula><mml:math id="inf146"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>15</mml:mn><mml:mo>,</mml:mo><mml:mn>210</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> or shifted to <inline-formula><mml:math id="inf147"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>75</mml:mn><mml:mo>,</mml:mo><mml:mn>210</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. (<bold>b</bold>) Given that <inline-formula><mml:math id="inf148"><mml:msub><mml:mi>μ</mml:mi><mml:mi>max</mml:mi></mml:msub></mml:math></inline-formula> does not enter the lower bound on the modularity determined by <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> (green curve), extending the dynamic range (see panel (<bold>a</bold>)) does not affect the region of stable fixed points in the parameter space. For positive background input, there are no stable fixed points, only unstable ones at non-saturated rates for low values of <inline-formula><mml:math id="inf149"><mml:msub><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">X</mml:mi></mml:msub></mml:math></inline-formula>, due to excitatory recurrent fluctuations in the activity. For stronger background input, no fixed points exists where <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> . In this case, the activity of the non-stimulated populations (non-zero) dominates the recurrent dynamics and denoising can not be achieved. (<bold>c</bold>) Shifting the dynamic range altogether (see panel (<bold>a</bold>)) leads to the emergence of stable fixed points at saturated rates also for positive external input, but the region in which denoising occurs is still significantly smaller than for networks with recurrent inhibition (see <xref ref-type="fig" rid="fig8">Figure 8c</xref>). For these values, <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is ensured because the total input to the non-stimulated populations remains below the shifted dynamical range, in contrast to just extending the dynamic range where even low inputs can lead to non-zero activity. Moreover, the shifted activation function requires a biologically implausible strength of input for activation. The firing threshold of biological neurons is typically <inline-formula><mml:math id="inf152"><mml:mrow><mml:mn>15</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> above the resting membrane potential, which is much less than the shifted <inline-formula><mml:math id="inf153"><mml:msub><mml:mi>μ</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:math></inline-formula>. Note that similar to <xref ref-type="fig" rid="fig8">Figure 8</xref>, here we plot only the fixed points for <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig8-figsupp1-v2.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>Firing rates in <inline-formula><mml:math id="inf156"><mml:msub><mml:mi mathvariant="normal">SSN</mml:mi><mml:mn mathvariant="normal">5</mml:mn></mml:msub></mml:math></inline-formula> in the absence of external background noise <inline-formula><mml:math id="inf157"><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">X</mml:mi></mml:msub><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">0</mml:mn></mml:mrow><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</title><p>(<bold>a</bold>) Firing rates of stimulated (top) and non-stimulated (bottom) populations obtained from simulations of a network without any recurrent connections, for different input intensities <inline-formula><mml:math id="inf158"><mml:mi>λ</mml:mi></mml:math></inline-formula>. Successful denoising only occurs for the extreme case of <inline-formula><mml:math id="inf159"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, in which case the pathways are completely segregated. Note that the input rate was unchanged, <inline-formula><mml:math id="inf160"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>in</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>12</mml:mn><mml:mo>⁢</mml:mo><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>b</bold>) Same as (<bold>a</bold>), but for excitatory recurrence (<inline-formula><mml:math id="inf161"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). Recurrent excitation spreads the input from the stimulated pathway to non-stimulated neurons. Results shown only for <inline-formula><mml:math id="inf162"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, with larger values leading to similar results.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig8-figsupp2-v2.tif"/></fig></fig-group><p>In this simple model, the output is zero for inputs below <inline-formula><mml:math id="inf163"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>min</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula> and at maximum rate <inline-formula><mml:math id="inf164"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>150</mml:mn></mml:mrow></mml:math></inline-formula> for inputs above <inline-formula><mml:math id="inf165"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>400</mml:mn></mml:mrow></mml:math></inline-formula>. In between these two bounds, the output is linearly interpolated <inline-formula><mml:math id="inf166"><mml:mrow><mml:mrow><mml:mi>ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. As discussed before, successful denoising is achieved if the non-stimulated sub-populations are silent, <inline-formula><mml:math id="inf167"><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mi>NS</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and the stimulated sub-populations are active, <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Note that in the following we focus on this ideal scenario representing perfect denoising, but, in principle, intermediate solutions with <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>≫</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> may also occur and could still be considered as successful denoising. Analyzing for which neuron, network and input properties this scenario is achieved, we obtain multiple conditions for the modularity that need to be fulfilled.</p><p>The first condition illustrates the dependence of the critical modularity on the neuron model (<xref ref-type="fig" rid="fig8">Figure 8c</xref>, purple horizontal line)<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>m</mml:mi><mml:mo>≥</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf170"><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:msub></mml:math></inline-formula> is the number of stimulus-specific sub-populations and <inline-formula><mml:math id="inf171"><mml:mrow><mml:mi>α</mml:mi><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (typically with a value of 0.25) represents the (reduced) noise ratio in the deeper sub-networks, with <inline-formula><mml:math id="inf172"><mml:mi>α</mml:mi></mml:math></inline-formula> scaling the noise and <inline-formula><mml:math id="inf173"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:math></inline-formula> scaling the feedforward connections (see Materials and methods). This is necessary to ensure that the total excitatory input to each neuron is consistent across the network. In particular, the critical modularity depends on the dynamic range of input <inline-formula><mml:math id="inf174"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and output <inline-formula><mml:math id="inf175"><mml:msub><mml:mi>ν</mml:mi><mml:mi>max</mml:mi></mml:msub></mml:math></inline-formula>. The condition represents a lower bound on the modularity required for denoising. Importantly, while it depends on the effective coupling strength <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, the noise ratio <inline-formula><mml:math id="inf177"><mml:mi>α</mml:mi></mml:math></inline-formula> and the number of maps <inline-formula><mml:math id="inf178"><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:msub></mml:math></inline-formula> (see Materials and methods), it does not depend on the nature of the recurrent interactions (E/I ratio) and the strength of the external background input. In addition, we find two additional critical values of the modularity (cyan and green curves in <xref ref-type="fig" rid="fig8">Figure 8c–e</xref>), both of which do depend on the strength of the external background input <inline-formula><mml:math id="inf179"><mml:msub><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">X</mml:mi></mml:msub></mml:math></inline-formula> and the recurrent connectivity (E/I ratio <inline-formula><mml:math id="inf180"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:math></inline-formula>):<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Depending on the external input strength <inline-formula><mml:math id="inf181"><mml:msub><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">X</mml:mi></mml:msub></mml:math></inline-formula>, these are either upper or lower bounds. In the denominator of these expressions, the total input (recurrent and external) is compared to the limits of the dynamic range of the neuron model. The cancellation between recurrent and external inputs in the inhibition-dominated baseline model typically yields a total input within the dynamic range of the neuron, such that modularity in feedforward connections can decrease the input of the non-stimulated sub-populations to silence them, and increase the input of the stimulated sub-populations to support their activity. The competition between the excitatory and inhibitory contributions ensures that the total input does not lead to a saturating output activity. Thus, for inhibitory recurrence, denoising can be achieved at a moderate level of modularity over a large range of external background inputs (shaded black and hatched regions in <xref ref-type="fig" rid="fig8">Figure 8c</xref>), which demonstrates a robust denoising mechanism even in the presence of changes in the input environment.</p><p>In contrast, if recurrent connections are absent, strong inhibitory external background input is required to counteract the excitatory feedforward input and achieve a denoising scenario (<xref ref-type="fig" rid="fig8">Figure 8d</xref>). Fixed points at non-saturated activity <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> are also present for low excitatory external input, but unstable due to the positive recurrent feedback. This is because in networks without recurrence, there is no competition between the recurrent input and the external and feedforward inputs. As a result, the input to both the stimulated and non-stimulated sub-populations is typically high, such that modulation of the feedforward input via topography cannot lead to a strong distinction between the pathways as required for denoising. In these networks, one typically observes high activity in all populations. A similar behavior can be observed in excitation-dominated networks (<xref ref-type="fig" rid="fig8">Figure 8e</xref>), where the inhibitory external background input must be even stronger to compensate the excitatory feedforward and recurrent connectivity and reach a stable denoising regime.</p><p>Note that inhibitory external input is not in line with the excitatory nature of external inputs to local circuits in the brain and is therefore biologically implausible. One way to achieve denoising in excitation-dominated networks for excitatory background inputs would be to shift the dynamic range of the activation function (see <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>), which is, however, not consistent with the biophysical properties of real neurons (distance between threshold and rest as compared to typical strengths of postsynaptic potentials). In summary, we find that recurrent inhibition is crucial to achieve denoising in biologically plausible settings.</p><p>These results on the role of recurrence and external input can be transferred to the behavior of the spiking model. While details of the fixed point behavior depend on the specific choice of the activation function, <xref ref-type="fig" rid="fig8">Figure 8f, h</xref> shows that there is also no denoising regime for the spiking model in case of no or excitation-dominated recurrence and a biologically plausible level of external input. Instead, one finds high activity in both stimulated and non-stimulated sub-populations, as confirmed by network simulations (<xref ref-type="fig" rid="fig8">Figure 8g, i</xref>). <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref> further confirms that even reducing the external input to zero does not avoid this high-activity state in both stimulated and non-stimulated sub-populations for <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s2-9"><title>Input integration and multi-stability</title><p>The analysis considered in the sections above is restricted to a system driven with a single external stimulus. However, to adequately understand the system’s dynamics, we need to account for the fact that it can be concurrently driven by multiple input streams. If two simultaneously active stimuli drive the system (see illustration in <xref ref-type="fig" rid="fig9">Figure 9a</xref>), the qualitative behavior where the responses along the stimulated (non-stimulated) maps are enhanced (silenced) is retained if the strength of the two input channels is sufficiently different (<xref ref-type="fig" rid="fig9">Figure 9b</xref>, top panel). In this case, the weaker stimulus is not strong enough to drive the sub-population it stimulates toward the basin of attraction of the high-activity fixed point. Consequently, the sub-population driven by this second stimulus behaves as a non-stimulated sub-population and the system remains responsive to only one of the two inputs, acting as a WTA circuit. If, however, the ratio of stimulus intensities varies, two active sub-populations may co-exist (<xref ref-type="fig" rid="fig9">Figure 9b</xref>, center) and/or compete (bottom panel), depending also on the degree of topographic modularity.</p><fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>For multiple input streams, topography may elicit a wide range of dynamical regimes.</title><p>(<bold>a</bold>) Two active input channels with corresponding stimulus intensities <inline-formula><mml:math id="inf184"><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf185"><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, mapped onto non-overlapping sub-populations, drive the network simultaneously. Throughout this section, <inline-formula><mml:math id="inf186"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> is fixed to the previous baseline value. (<bold>b</bold>) Mean firing rates of the two stimulated sub-populations (purple and cyan), as well as the non-stimulated sub-populations (black) for three different combinations of <inline-formula><mml:math id="inf187"><mml:mi>m</mml:mi></mml:math></inline-formula> and ratios <inline-formula><mml:math id="inf188"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (as marked in (<bold>c</bold>)). (<bold>c</bold>) Correlation-based similarity score shows three distinct dynamical regimes in SSN<sub>5</sub> when considering the firing rates of two, simultaneously stimulated sub-populations associated with <italic>S</italic><sub>1</sub> and <italic>S</italic><sub>2</sub>, respectively: coexisting (Co-Ex, red area), winner-takes-all (WTA, gray), and winnerless competition (WLC, blue). Curves mark the boundaries between the different regimes (see Materials and methods). Activity for marked parameter combinations shown in (<bold>b</bold>). (<bold>d</bold>) Evolution of the similarity score with increasing network depth, for <inline-formula><mml:math id="inf189"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.83</mml:mn></mml:mrow></mml:math></inline-formula> and input ratio of 0.86. For deep networks, the Co-Ex region vanishes and the system converges to either WLC or WTA dynamics. (<bold>e</bold>) Schematic showing the influence of modularity and input intensity on the system’s potential energy landscape (see Materials and methods): (1) in the fading regime there is a single low-activity fixed point (minimum in the potential); (2) increasing modularity creates two high-activity fixed points associated with S1 and S2, with the dynamics always converging to the same minimum due to <inline-formula><mml:math id="inf190"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>≫</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>; (3) strengthening S2 balances the initial conditions, resulting in frequent, fluctuation-driven switching between the two states; (4) for larger <inline-formula><mml:math id="inf191"><mml:mi>m</mml:mi></mml:math></inline-formula> values, switching speed decreases as the wells become deeper and the barrier between the wells wider. (<bold>f</bold>) Switching frequency between the dominating sub-populations in SSN<sub>5</sub> decays with increasing modularity. Data computed over 10 s, for <inline-formula><mml:math id="inf192"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>. <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref> and <xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref> show the evolution of the Co-Ex region over 12 modules and the potential landscape, respectively.</p><p><supplementary-material id="fig9sdata1"><label>Figure 9—source data 1.</label><caption><title>Code and data for <xref ref-type="fig" rid="fig9">Figure 9</xref> and related figure supplements.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-77009-fig9-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig9-v2.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 1.</label><caption><title>Evolution of similarity score for 12 sub-networks.</title><p>Correlation-based similarity score illustrates the three dynamical regimes observed across the different sub-networks, for two input streams: coexisting (CoEx, red area and positive score), winner-take-all behavior (gray, score near 0), and winnerless competition (WLC, blue and negative score). As predicted by the mean-field analysis, the CoEx region vanishes with increasing network depth. The calculation of the similarity score is detailed in Materials and methods. If either stimuli could not be decoded, we set the score to 0. In SSN<sub>11</sub>, ‘X’ indicates parameter combinations where none of the stimuli could be decoded.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig9-figsupp1-v2.tif"/></fig><fig id="fig9s2" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 2.</label><caption><title>Potential landscape for two input streams.</title><p>For intermediate modularity (<inline-formula><mml:math id="inf193"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.85</mml:mn></mml:mrow></mml:math></inline-formula>, left and <inline-formula><mml:math id="inf194"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>, right), there are two high-activity fixed points (circled cross markers) in addition to the low-activity one near zero (marker added manually here, as it is not observable due to the larger integration step of 5 spks/sec used here). If the projections are almost fully modular (<inline-formula><mml:math id="inf195"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>), an additional high-activity fixed point can be observed for identical <inline-formula><mml:math id="inf196"><mml:msup><mml:mi>ν</mml:mi><mml:mi>S1</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf197"><mml:msup><mml:mi>ν</mml:mi><mml:mi>S2</mml:mi></mml:msup></mml:math></inline-formula>. In this case, the two stimulated sub-populations can be considered as one larger population, for which the common <inline-formula><mml:math id="inf198"><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> becomes positive, as in the case of a single input stream (see <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>), just for larger <inline-formula><mml:math id="inf199"><mml:mi>m</mml:mi></mml:math></inline-formula>. Gray, anti-diagonal lines represent the one-dimensional sections illustrated in <xref ref-type="fig" rid="fig9">Figure 9e</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig9-figsupp2-v2.tif"/></fig></fig-group><p>To quantify these variations in macroscopic behavior, we focus on the dynamics of SSN<sub>5</sub> and measure the similarity (correlation coefficient) between the firing rates of the two stimulus-specific sub-populations as a function of modularity and ratio of input intensities <inline-formula><mml:math id="inf200"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (see Materials and methods and <xref ref-type="fig" rid="fig9">Figure 9c</xref>). In the case that both inputs have similar intensities but the feedforward projections are not sufficiently modular, both sub-populations are activated simultaneously (Co-Ex, red area in <xref ref-type="fig" rid="fig9">Figure 9c</xref>). This is the dynamical regime that dominates the earlier sub-networks. However, this is a transient state, and the Co-Ex region gradually shrinks with network depth until it vanishes completely after approximately 9–10 SSNs (see <xref ref-type="fig" rid="fig9">Figure 9d</xref>).</p><p>For low modularity, the system settles in the single stable state associated with near-zero firing rates, as illustrated schematically in the energy landscape in <xref ref-type="fig" rid="fig9">Figure 9e</xref>, (1) (see Materials and methods, Appendix B, and Supplementary Materials for derivations and numerical simulations). Above the critical modularity value, the system enters one of two different regimes. For <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.84</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and an input ratio below 0.7 (<xref ref-type="fig" rid="fig9">Figure 9c</xref>, gray area), one stimulus dominates (WTA) and the responses in the two populations are uncorrelated (<xref ref-type="fig" rid="fig9">Figure 9b</xref>, top panel). Although the potential landscape contains two minima corresponding to either population being active, the system always settles in the high-activity attractor state corresponding to the dominating input (<xref ref-type="fig" rid="fig9">Figure 9e</xref>, (2)).</p><p>If, however, the two inputs have comparable intensities and the topographic projections are sharp enough (<inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.84</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), the system transitions into a different dynamical state where neither stimulus-specific sub-population can maintain an elevated firing rate for extended periods of time. In the extreme case of nearly identical intensities (<inline-formula><mml:math id="inf203"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>≥</mml:mo><mml:mn>0.9</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and high modularity, the responses become anti-correlated (<xref ref-type="fig" rid="fig9">Figure 9b</xref>, bottom panel), that is the activation of the two stimulus-specific sub-populations switches, as they engage in a dynamic behavior reminiscent of WLC between multiple neuronal groups (<xref ref-type="bibr" rid="bib40">Lagzi and Rotter, 2015</xref>; <xref ref-type="bibr" rid="bib69">Rost et al., 2018</xref>). The switching between the two states is driven by stochastic fluctuations (<xref ref-type="fig" rid="fig9">Figure 9e</xref>, (3)). The depth of the wells and width of barrier (distance between fixed points) increase with modularity (see <xref ref-type="fig" rid="fig9">Figure 9e</xref>, (4) and <xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>), suggesting a greater difficulty in moving between the two attractors and consequently fewer state changes. Numerical simulations confirm this slowdown in switching (<xref ref-type="fig" rid="fig9">Figure 9f</xref>).</p><p>We wish to emphasize that the different dynamical states arise primarily from the feedforward connectivity profile. Nevertheless, even though the synaptic weights are not directly modified, varying the topographic modularity does translate to a modification of the effective connectivity weights (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). The ratio of stimulus intensities also plays a role in determining the dynamics, but there is a (narrow) range (approximately between 0.75 and 0.8) for which all 3 regions can be reached through sole modification of the modularity. Together, these results demonstrate that topography can not only lead to spatial denoising but also enable various, functionally important network operating points.</p></sec><sec id="s2-10"><title>Reconstruction and denoising of dynamical inputs</title><p>Until now, we have considered continuous but piecewise constant, step signals, with each step lasting for a relatively long and fixed period of <inline-formula><mml:math id="inf204"><mml:mrow><mml:mpadded width="+5pt"><mml:mn>200</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:math></inline-formula>. This may give the impression that the denoising effects we report only works for static or slowly changing inputs, whereas naturalistic stimuli are continuously varying. Nevertheless, sensory perception across modalities relies on varying degrees of temporal and spatial discretization (<xref ref-type="bibr" rid="bib86">VanRullen and Koch, 2003</xref>), with individual (sub-)features of the input encoded by specific (sub-)populations of neurons in the early stages of the sensory hierarchy. In this section, we will demonstrate that denoising is robust to the temporal properties of the input and its encoding, as we relax many of the assumptions made in previous sections.</p><p>We consider a sinusoidal input signal, which we discretize and map onto the network according to the depiction in <xref ref-type="fig" rid="fig10">Figure 10a</xref>. This approach is similar to previous works, for instance it can mimic the movement of a light spot across the retina (<xref ref-type="bibr" rid="bib35">Klos et al., 2018</xref>). By varying the sampling interval <inline-formula><mml:math id="inf205"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> and number of channels <inline-formula><mml:math id="inf206"><mml:mi>k</mml:mi></mml:math></inline-formula>, we can change the coarseness of the discretization from step-like signals to more continuous approximations of the input. If we choose a high sampling rate (<inline-formula><mml:math id="inf207"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+5pt"><mml:mn>1</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) and sufficient channels (<inline-formula><mml:math id="inf208"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math></inline-formula>), we can accurately encode even fast changing signals (<xref ref-type="fig" rid="fig10">Figure 10b</xref>). Given that each input-driven  SSN is inhibition-dominated and therefore close to the balanced state, the network exhibits a fast tracking property (<xref ref-type="bibr" rid="bib87">van Vreeswijk and Sompolinsky, 1996</xref>) and can accurately represent and denoise the underlying continuous signal in the spiking activity (<xref ref-type="fig" rid="fig10">Figure 10c</xref>, top). This is also captured by the readout, with the tracking precision increasing with network depth (<xref ref-type="fig" rid="fig10">Figure 10c</xref>, bottom). In this condition, there is a performance gain of up to 50% in the noiseless case (<xref ref-type="fig" rid="fig10">Figure 10d</xref>, top) and similar values for varying levels of noise (<xref ref-type="fig" rid="fig10">Figure 10d</xref>, bottom).</p><fig-group><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Reconstruction of a dynamic, continuous input signal.</title><p>(<bold>a</bold>) Sketch of the encoding and mapping of a sinusoidal input <inline-formula><mml:math id="inf209"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> onto the current-based network model. The signal is sampled at regular time intervals <inline-formula><mml:math id="inf210"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, with each sample binned into one of <inline-formula><mml:math id="inf211"><mml:mi>k</mml:mi></mml:math></inline-formula> channels (which is then active for a duration of <inline-formula><mml:math id="inf212"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>). This yields a temporally and spatially discretized <inline-formula><mml:math id="inf213"><mml:mi>k</mml:mi></mml:math></inline-formula>-dimensional binary signal <inline-formula><mml:math id="inf214"><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, from which we obtain the final noisy input <inline-formula><mml:math id="inf215"><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> similar to the baseline network (see <xref ref-type="fig" rid="fig1">Figure 1</xref> and Materials and methods). Unlike the one-to-one mapping in <xref ref-type="fig" rid="fig1">Figure 1</xref>, here we decouple the number of channels <inline-formula><mml:math id="inf216"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math></inline-formula> from that of topographic maps, <inline-formula><mml:math id="inf217"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> (map size is unchanged, <inline-formula><mml:math id="inf218"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>800</mml:mn></mml:mrow></mml:math></inline-formula>). Because <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the channels project to evenly spaced but overlapping sub-populations in SSN<sub>0</sub>, while the maps themselves overlap significantly. (<bold>b</bold>) Discretized signal <inline-formula><mml:math id="inf220"><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and rate encoding for input <inline-formula><mml:math id="inf221"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf222"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+5pt"><mml:mn>1</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and no noise (<inline-formula><mml:math id="inf223"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>c</bold>) Top panel shows the spiking activity of 500 randomly chosen excitatory (blue) and inhibitory (red) neurons in SSN<sub>0</sub>, SSN<sub>2</sub>, and SSN<sub>5</sub>, for <inline-formula><mml:math id="inf224"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>. Corresponding target signal <inline-formula><mml:math id="inf225"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (black) and readout output (red) are shown in bottom panel. (<bold>d</bold>) Relative gain in performance in SSN<sub>2</sub> and SSN<sub>5</sub> for <inline-formula><mml:math id="inf226"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (top). Color shade denotes network depth. Bottom panel shows relative gain in SSN<sub>5</sub> for different levels of noise <inline-formula><mml:math id="inf227"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>e–g</bold>) Same as (<bold>b–d</bold>), but for a slowly varying signal (sampled at <inline-formula><mml:math id="inf228"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+5pt"><mml:mn>20</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), <inline-formula><mml:math id="inf229"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf230"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Performance results are averaged across five trials. We used 20 s of data for training and 10 s for testing (activity sampled every 1 ms, irrespective of input discretization <inline-formula><mml:math id="inf231"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>).</p><p><supplementary-material id="fig10sdata1"><label>Figure 10—source data 1.</label><caption><title>Code and data for <xref ref-type="fig" rid="fig10">Figure 10</xref> and related figure supplements.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-77009-fig10-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig10-v2.tif"/></fig><fig id="fig10s1" position="float" specific-use="child-fig"><label>Figure 10—figure supplement 1.</label><caption><title>Limits of denoising for rapidly changing and noisy dynamical inputs.</title><p>(<bold>a</bold>) A fast changing input signal <inline-formula><mml:math id="inf232"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>24</mml:mn><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>12</mml:mn><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> sampled at <inline-formula><mml:math id="inf233"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, with no additional noise (<inline-formula><mml:math id="inf234"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>b</bold>) While portions of the signal can be successfully transmitted and denoised in SSN<sub>5</sub>, there are significant periods (steep slopes) where the signal representation is lost. (<bold>c</bold>) Slower signal <inline-formula><mml:math id="inf235"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> with significant noise corruption (<inline-formula><mml:math id="inf236"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>in</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>). Continuous red curve denotes the input signal <inline-formula><mml:math id="inf237"><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>d</bold>) Strong noise in the input leads to heavy fluctuations in the activity of the deeper populations, corrupting the signal representations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77009-fig10-figsupp1-v2.tif"/></fig></fig-group><p>Note that due to the increased number of input channels (40 compared to 10) projecting to the same number of neurons in SSN<sub>0</sub> as before <inline-formula><mml:math id="inf238"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>800</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, for the same <inline-formula><mml:math id="inf239"><mml:msub><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi></mml:msub></mml:math></inline-formula> the effective amount of noise each neuron receives is, on average, four times larger than in the baseline network. Moreover, the task was made more difficult by the significant overlap between the maps (<inline-formula><mml:math id="inf240"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>) as well as the resulting decrease in neuronal input selectivity. Nevertheless, similar results were obtained for slower and more coarsely sampled signals (<xref ref-type="fig" rid="fig10">Figure 10e–g</xref>).</p><p>We found comparable denoising dynamics for a large range of parameter combinations involving the map size, number of maps, number of channels, and signal complexity. Although there are limits with respect to the frequencies (and noise intensity) the network can track (see <xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1</xref>), these findings indicate a very robust and flexible phenomenon for denoising spatially encoded sensory stimuli.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The presence of stimulus- or feature-tuned sub-populations of neurons in primary sensory cortices (as well as in downstream areas) provides an efficient spatial encoding strategy (<xref ref-type="bibr" rid="bib61">Pouget et al., 1999</xref>; <xref ref-type="bibr" rid="bib74">Seriès et al., 2004</xref>; <xref ref-type="bibr" rid="bib82">Tkacik et al., 2010</xref>) that ensures the relevant computable features are accurately represented. Here, we propose that beyond primary sensory areas, modular topographic projections play a key role in preserving accurate representations of sensory inputs across many processing modules. Acting as a structural scaffold for a sequential denoising mechanism, we show how they simultaneously enhance relevant stimulus features and remove noisy interference. We demonstrate this phenomenon in a variety of network models and provide a theoretical analysis that indicates its robustness and generality.</p><p>When reconstructing a spatially encoded input signal corrupted by noise in a network of sequentially connected populations, we find that a convergent structure in the feedforward projections is not only critical for successfully solving the task, but that the performance increases significantly with network depth beyond a certain modularity (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Through this mechanism, the response selectivity of the stimulated sub-populations is sharpened within each subsequent sub-network, while others are silenced (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Such wiring may support efficient and robust information transmission from the thalamus to deeper cortical centers, retaining faithful representations even in the presence of strong noise. We demonstrate that this holds for a variety of signals, from approximately static (stepwise) to smoothly and rapidly changing dynamic inputs (<xref ref-type="fig" rid="fig10">Figure 10</xref>). Thanks to the balance of excitation and inhibition, the network is able to track spatially encoded signals on very short timescales, and is flexible with respect to the level of spatial and temporal discretization. Accurate tracking and denoising requires that the encoding is locally static/semi-stationary for only a few tens of milliseconds, which is roughly in line with psychophysics studies on the limits of sensory perception (<xref ref-type="bibr" rid="bib7">Borghuis et al., 2019</xref>).</p><p>More generally, topographic modularity, in conjunction with other top-down processes (<xref ref-type="bibr" rid="bib36">Kok et al., 2012</xref>), could provide the anatomical substrate for the implementation of a number of behaviorally relevant processes. For example, feedforward topographic projections on the visual pathway could contribute, together with various attentional control processes, to the widely observed <italic>pop-out effect</italic> in the later stages of the visual hierarchy (<xref ref-type="bibr" rid="bib8">Brefczynski-Lewis et al., 2009</xref>; <xref ref-type="bibr" rid="bib30">Itti et al., 1998</xref>). The pop-out effect, at its core, assumes that in a given context some neurons exhibit sharper selectivity to their preferred stimulus feature than the neighboring regions, which can be achieved through a winner-take-all (WTA) mechanism (see <xref ref-type="fig" rid="fig9">Figure 9</xref> and <xref ref-type="bibr" rid="bib29">Himberger et al., 2018</xref>).</p><p>The WTA behavior underlying the denoising is caused by a re-shaping of the E/I balance across the network (see <xref ref-type="fig" rid="fig3">Figure 3</xref>). As the excitatory feedforward projections become more focused, they modulate the system’s effective connectivity and thereby the gain on the stimulus-specific pathways, gating or allowing (and even enhancing) signal propagation. This change renders the stimulated pathway excitatory in the active regime (see <xref ref-type="fig" rid="fig7">Figure 7</xref>), leading to multiple fixed points such as those observed in networks with local recurrent excitation (<xref ref-type="bibr" rid="bib65">Renart et al., 2007</xref>; <xref ref-type="bibr" rid="bib44">Litwin-Kumar and Doiron, 2012</xref>). While the high-activity fixed point of such clustered networks is reached over time, in our model it unfolds progressively in space, across multiple populations. Importantly, in the range of biologically plausible numbers of cortical areas relevant for signal transmission (up to 10 for some visual stimuli, see <xref ref-type="bibr" rid="bib20">Felleman and Van Essen, 1991</xref>; <xref ref-type="bibr" rid="bib27">Hegdé and Felleman, 2007</xref>) and intermediate modularity, the firing rates remain within experimentally observed limits and do not saturate. The basic principle is similar to other approaches that alter the gain on specific pathways to facilitate stimulus propagation, for example through stronger synaptic weights (<xref ref-type="bibr" rid="bib89">Vogels and Abbott, 2005</xref>), stronger nonlinearity (<xref ref-type="bibr" rid="bib84">Toyoizumi, 2012</xref>), tuning of connectivity strength, and neuronal thresholds (<xref ref-type="bibr" rid="bib11">Cayco-Gajic and Shea-Brown, 2013</xref>), via detailed balance of local excitation and inhibition (amplitude gating; <xref ref-type="bibr" rid="bib90">Vogels and Abbott, 2009</xref>) or with additional subcortical structures (<xref ref-type="bibr" rid="bib12">Cortes and van Vreeswijk, 2015</xref>). Additionally, our model also displays some activity characteristics reported previously, such as the response sharpening observed for synfire chains (<xref ref-type="bibr" rid="bib14">Diesmann et al., 1999</xref>) or (almost) linear firing rate propagation (<xref ref-type="bibr" rid="bib39">Kumar et al., 2010</xref>) (for intermediate modularity).</p><p>However, due to the reliance on increasing inhibitory activity at every stage, we speculate that denoising, as studied here, would not occur in such a system containing a single, shared inhibitory pool with homogeneous connectivity. In this case, inhibition would affect all excitatory populations uniformly, with stronger activity potentially preventing accurate stimulus transmission from the initial sub-networks. Nevertheless, this problem could be alleviated using a more realistic, localized spatial connectivity profile as in <xref ref-type="bibr" rid="bib37">Kumar et al., 2008a</xref>, or by adding shadow pools (groups of inhibitory neurons) for each layer of the network, carefully wired in a recurrent or feedforward manner (<xref ref-type="bibr" rid="bib2">Aviel et al., 2003</xref>; <xref ref-type="bibr" rid="bib3">Aviel et al., 2005</xref>; <xref ref-type="bibr" rid="bib90">Vogels and Abbott, 2009</xref>). In such networks with non-random or spatially dependent connectivity, structured (modular) topographic projections onto the inhibitory populations will likely be necessary to maintain stable dynamics and attain the appropriate inhibition-dominated regimes (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Alternatively, these could be achieved through additional, targeted inputs from other areas (<xref ref-type="fig" rid="fig4">Figure 4</xref>), with feedforward inhibition known to provide a possible mechanism for context-dependent gating or selective enhancement of certain stimulus features (<xref ref-type="bibr" rid="bib21">Ferrante et al., 2009</xref>; <xref ref-type="bibr" rid="bib68">Roberts et al., 2013</xref>).</p><p>While our findings build on the above results, we here show that the experimentally observed topographic maps may serve as a structural denoising mechanism for sensory stimuli. In contrast to most works on signal propagation where noise mainly serves to stabilize the dynamics and is typically avoided in the input, here the system is driven by a continuous signal severely corrupted by noise. Taking a more functional approach, this input is reconstructed using linear combinations of the full network responses, rather than evaluating the correlation structure of the activity or relying on precise firing rates. Focusing on the modularity of such maps in recurrent spiking networks, our model also differs from previous studies exploring optimal connectivity profiles for minimizing information loss in purely feedforward networks (<xref ref-type="bibr" rid="bib66">Renart and van Rossum, 2012</xref>; <xref ref-type="bibr" rid="bib94">Zylberberg et al., 2017</xref>), also in the context of sequential denoising autoencoders (<xref ref-type="bibr" rid="bib33">Kadmon and Sompolinsky, 2016</xref>) and stimulus classification (<xref ref-type="bibr" rid="bib4">Babadi and Sompolinsky, 2014</xref>), which used simplified neuron models or shallow networks, made no distinction between excitatory and inhibitory connections, or relied on specific, trained connection patterns (e.g., chosen by the pseudo-inverse model). Although the bistability underlying denoising can, in principle, also be achieved in such feedforward or networks without inhibition, our theoretical predictions and network simulations indicate that for biologically constrained circuits (i.e., where the background and long-range feedforward input is excitatory), inhibitory recurrence is indispensable for the spatial denoising studied here (see Section ‘Critical modularity for denoising’). Recurrent inhibition compensates for the feedforward and external excitation, generating competition between the topographic pathways and allowing the populations to rapidly track their input.</p><p>Moreover, our findings provide an explanation for how low-intensity stimuli (1–2 spks/sec above background activity, see <xref ref-type="fig" rid="fig2">Figure 2</xref> and Supplementary Materials) could be amplified across the cortex despite significant noise corruption, and relies on a generic principle that persists across different network models (<xref ref-type="fig" rid="fig5">Figure 5</xref>) while also being robust to variations in the map size (<xref ref-type="fig" rid="fig6">Figure 6</xref>). We demonstrated both the existence of a lower and upper (due to increased overlap) bound on their spatial extent for signal transmission, as well as an optimal region for which denoising was most pronounced. These results indicate a trade-off between modularity and map size, with larger maps sustaining stimulus propagation at lower modularity values, whereas smaller maps must compensate through increased topographic density (see <xref ref-type="fig" rid="fig6">Figure 6a</xref> and Supplementary Materials). In the case of smaller maps, progressively enlarging the receptive fields enhanced the denoising effect and improved task performance (<xref ref-type="fig" rid="fig6">Figure 6c</xref>), suggesting a functional benefit for the anatomically observed decrease in topographic specificity with hierarchical depth (<xref ref-type="bibr" rid="bib5">Bednar and Wilson, 2016</xref>; <xref ref-type="bibr" rid="bib79">Smith et al., 2001</xref>). One advantage of such a wiring could be spatial efficiency in the initial stages of the sensory hierarchy due to anatomical constraints, for instance the retina or the lateral geniculate nucleus. While we get a good qualitative description of how the spatial variation of topographic maps influences the system’s computational properties, the numerical values in general are not necessarily representative. Cortical maps are highly dynamic and exhibit more complex patterning, making (currently scarce) precise anatomical data a prerequisite for more detailed investigations. For instance, despite abundant information on the size of receptive fields (<xref ref-type="bibr" rid="bib79">Smith et al., 2001</xref>; <xref ref-type="bibr" rid="bib45">Liu et al., 2016</xref>; <xref ref-type="bibr" rid="bib34">Keliris et al., 2019</xref>), there is relatively little data on the connectivity between neurons tuned to related or different stimulus features across distinct cortical circuits. Should such experiments become feasible in the future, our model provides a testable prediction: the projections must be denser (or stronger) between smaller maps to allow robust communication whereas for larger maps fewer connections may be sufficient.</p><p>Finally, our model relates topographic connectivity to competition-based network dynamics. For two input signals of comparable intensities, moderately structured projections allow both representations to coexist in a decodable manner up to a certain network depth, whereas strongly modular connections elicit WLC like behavior characterized by stochastic switching between the two stimuli (see <xref ref-type="fig" rid="fig9">Figure 9</xref>). Computation by switching is a functionally relevant principle (<xref ref-type="bibr" rid="bib49">McCormick, 2005</xref>; <xref ref-type="bibr" rid="bib71">Schittler Neves and Timme, 2012</xref>), which relies on fluctuation- or input-driven competition between different metastable (unstable) or stable attractor states. In the model studied here, modular topography induced multi-stability (uncertainty) in representations, alternating between two stable fixed points corresponding to the two input signals. Structured projections may thus partially explain the experimentally observed competition between multiple stimulus representations across the visual pathway (<xref ref-type="bibr" rid="bib43">Li et al., 2016</xref>), and is conceptually similar to an attractor-based model of perceptual bistability (<xref ref-type="bibr" rid="bib52">Moreno-Bote et al., 2007</xref>). Moreover, this multi-stability across sub-networks can be ‘exploited’ at any stage by control signals, that is additional modulation (inihibitory) could suppress one and amplify (bias) another.</p><p>Importantly, all these different dynamical regimes emerge progressively through the hierarchy and are not discernible in the initial modules. Previous studies reporting on similar dynamical states have usually considered either the synaptic weights as the main control parameter (<xref ref-type="bibr" rid="bib40">Lagzi and Rotter, 2015</xref>; <xref ref-type="bibr" rid="bib41">Lagzi et al., 2019</xref>; <xref ref-type="bibr" rid="bib89">Vogels and Abbott, 2005</xref>) or studied specific architectures with clustered connectivity (<xref ref-type="bibr" rid="bib70">Schaub et al., 2015</xref>; <xref ref-type="bibr" rid="bib44">Litwin-Kumar and Doiron, 2012</xref>; <xref ref-type="bibr" rid="bib69">Rost et al., 2018</xref>). Our findings suggest that in a hierarchical circuit a similar palette of behaviors can be also obtained given appropriate effective connectivity patterns modulated exclusively through modular topography. Although we used fixed projections throughout this study, these could also be learned and shaped continuously through various forms of synaptic plasticity (see e.g. <xref ref-type="bibr" rid="bib83">Tomasello et al., 2018</xref>). To achieve such a variety of dynamics, cortical circuits most likely rely on a combination of all these mechanisms, that is, pre-wired modular connections (within and between distant modules) and heterogeneous gain adaptation through plasticity, along with more complex processes such as targeted inhibitory gating.</p><p>Overall, our results highlight a novel functional role for topographically structured projection pathways in constructing reliable representations from noisy sensory signals, and accurately routing them across the cortical circuitry despite the plethora of noise sources along each processing stage.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Network architecture</title><p>We consider a feedforward network architecture where each sub-network (SSN) is a balanced random network (<xref ref-type="bibr" rid="bib9">Brunel, 2000</xref>) composed of <inline-formula><mml:math id="inf241"><mml:mrow><mml:mi mathsize="90%">N</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">10000</mml:mn></mml:mrow></mml:math></inline-formula> homogeneous LIF neurons, grouped into a population of <inline-formula><mml:math id="inf242"><mml:mrow><mml:msup><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%" mathvariant="normal">E</mml:mi></mml:msup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="90%">0.8</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> excitatory and <inline-formula><mml:math id="inf243"><mml:mrow><mml:msup><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%" mathvariant="normal">I</mml:mi></mml:msup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="90%">0.2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> inhibitory units. Within each sub-network, neurons are connected randomly and sparsely, with a fixed number of <inline-formula><mml:math id="inf244"><mml:mrow><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">E</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mi mathsize="90%">ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%" mathvariant="normal">E</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> local excitatory and <inline-formula><mml:math id="inf245"><mml:mrow><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">I</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mi mathsize="90%">ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%" mathvariant="normal">I</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> local inhibitory inputs per neuron. The sub-networks are arranged sequentially, that is the excitatory neurons <inline-formula><mml:math id="inf246"><mml:msub><mml:mi mathsize="90%" mathvariant="normal">E</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula> in <inline-formula><mml:math id="inf247"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula> project to both <inline-formula><mml:math id="inf248"><mml:msub><mml:mi mathsize="90%" mathvariant="normal">E</mml:mi><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf249"><mml:msub><mml:mi mathsize="90%" mathvariant="normal">I</mml:mi><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> populations in the subsequent sub-network <inline-formula><mml:math id="inf250"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> (for an illustrative example, see <xref ref-type="fig" rid="fig1">Figure 1a</xref>). There are no inhibitory feedforward projections. Although projections between sub-networks have a specific, non-uniform structure (see next section), each neuron in <inline-formula><mml:math id="inf251"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> receives the same total number of synapses from the previous SSN, <inline-formula><mml:math id="inf252"><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%">FF</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>In addition, all neurons receive <inline-formula><mml:math id="inf253"><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">X</mml:mi></mml:msub></mml:math></inline-formula> inputs from an external source representing stochastic background noise. For the first sub-network, we set <inline-formula><mml:math id="inf254"><mml:mrow><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">X</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, as it is commonly assumed that the number of background input synapses modeling local and distant cortical input is in the same range as the number of recurrent excitatory connections (see e.g. <xref ref-type="bibr" rid="bib9">Brunel, 2000</xref>; <xref ref-type="bibr" rid="bib38">Kumar et al., 2008b</xref>; <xref ref-type="bibr" rid="bib16">Duarte and Morrison, 2014</xref>). To ensure that the total excitatory input to each neuron is consistent across the network, we scale <inline-formula><mml:math id="inf255"><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">X</mml:mi></mml:msub></mml:math></inline-formula> by a factor of <inline-formula><mml:math id="inf256"><mml:mrow><mml:mi mathsize="90%">α</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0.25</mml:mn></mml:mrow></mml:math></inline-formula> for the deeper SSNs and set <inline-formula><mml:math id="inf257"><mml:mrow><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%">FF</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mi mathsize="90%">α</mml:mi></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">E</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, resulting in a ratio of 3:1 between the number of feedforward and background synapses.</p></sec><sec id="s4-2"><title>Modular feedforward projections</title><p>Within each SSN, each neuron is assigned to one or more of <inline-formula><mml:math id="inf258"><mml:msub><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%" mathvariant="normal">C</mml:mi></mml:msub></mml:math></inline-formula> sub-populations <italic>SP</italic> associated with a specific stimulus (<inline-formula><mml:math id="inf259"><mml:mrow><mml:msub><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%" mathvariant="normal">C</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">10</mml:mn></mml:mrow></mml:math></inline-formula> unless otherwise stated). This is illustrated in <xref ref-type="fig" rid="fig1">Figure 1a</xref> for <inline-formula><mml:math id="inf260"><mml:mrow><mml:msub><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%" mathvariant="normal">C</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow></mml:math></inline-formula>. We choose these sub-populations so as to minimize their overlap within each <inline-formula><mml:math id="inf261"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula>, and control their effective size <inline-formula><mml:math id="inf262"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathsize="90%">C</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi><mml:mi mathsize="90%">β</mml:mi></mml:msubsup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%">d</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%">β</mml:mi></mml:msup></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="90%">β</mml:mi><mml:mo mathsize="90%" stretchy="false">∈</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">[</mml:mo><mml:mi mathsize="90%" mathvariant="normal">E</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%" mathvariant="normal">I</mml:mi><mml:mo maxsize="90%" minsize="90%">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, through the scaling parameter <inline-formula><mml:math id="inf263"><mml:mrow><mml:msub><mml:mi mathsize="90%">d</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">∈</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">[</mml:mo><mml:mn mathsize="90%">0</mml:mn><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mn mathsize="90%">1</mml:mn><mml:mo maxsize="90%" minsize="90%">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Depending on the size and number of sub-populations, it is possible that some neurons are not part of any or that some neurons belong to multiple such sub-populations (overlap).</p><sec id="s4-2-1"><title>Map size</title><p>In what follows, a topographic map refers to the sequence of sub-populations in the different sub-networks associated with the same stimulus. To enable a flexible manipulation of the map sizes, we constrain the scaling factor <inline-formula><mml:math id="inf264"><mml:msub><mml:mi mathsize="90%">d</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula> by introducing a step-wise linear increment <inline-formula><mml:math id="inf265"><mml:mi mathsize="90%">δ</mml:mi></mml:math></inline-formula>, such that <inline-formula><mml:math id="inf266"><mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="90%">d</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%">d</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="90%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">δ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="90%">i</mml:mi><mml:mo mathsize="90%" stretchy="false">≥</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Unless otherwise stated, we set <inline-formula><mml:math id="inf267"><mml:mrow><mml:msub><mml:mi mathsize="90%">d</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0.1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf268"><mml:mrow><mml:mi mathsize="90%">δ</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0</mml:mn></mml:mrow></mml:math></inline-formula>. Note that all SPs within a given SSN have the same size. In this study, we will only explore values in the range <inline-formula><mml:math id="inf269"><mml:mrow><mml:mn mathsize="90%">0</mml:mn><mml:mo mathsize="90%" stretchy="false">≤</mml:mo><mml:mi mathsize="90%">δ</mml:mi><mml:mo mathsize="90%" stretchy="false">≤</mml:mo><mml:mn mathsize="90%">0.02</mml:mn></mml:mrow></mml:math></inline-formula> to ensure consistent map sizes across the system, that is, <inline-formula><mml:math id="inf270"><mml:mrow><mml:mn mathsize="90%">0</mml:mn><mml:mo mathsize="90%" stretchy="false">≤</mml:mo><mml:msub><mml:mi mathsize="90%">d</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">≤</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf271"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula> (see constraints in Appendix A).</p></sec><sec id="s4-2-2"><title>Modularity</title><p>To systematically modify the degree of modular segregation in the topographic projections, we define a modularity parameter that determines the relative probability for feedforward connections from a given SP in <inline-formula><mml:math id="inf272"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula> to target the corresponding SP in <inline-formula><mml:math id="inf273"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. Specifically, we follow (<xref ref-type="bibr" rid="bib55">Newman, 2009</xref>; <xref ref-type="bibr" rid="bib62">Pradhan et al., 2011</xref>) and define <inline-formula><mml:math id="inf274"><mml:mrow><mml:mi mathsize="90%">m</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mfrac><mml:msub><mml:mi mathsize="90%">p</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub><mml:msub><mml:mi mathsize="90%">p</mml:mi><mml:mi mathsize="90%" mathvariant="normal">c</mml:mi></mml:msub></mml:mfrac></mml:mrow><mml:mo mathsize="90%" stretchy="false">∈</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">[</mml:mo><mml:mn mathsize="90%">0</mml:mn><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mn mathsize="90%">1</mml:mn><mml:mo maxsize="90%" minsize="90%">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the ratio of the feedforward projection probabilities between neurons belonging to different SPs <inline-formula><mml:math id="inf275"><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msub><mml:mi mathsize="90%">p</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:math></inline-formula> and between neurons on the same topographic map <inline-formula><mml:math id="inf276"><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msub><mml:mi mathsize="90%">p</mml:mi><mml:mi mathsize="90%" mathvariant="normal">c</mml:mi></mml:msub><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:math></inline-formula>. According to the above definition, the feedforward connectivity matrix is random and homogeneous (Erdős-Rényi graph) if <inline-formula><mml:math id="inf277"><mml:mrow><mml:mi mathsize="90%">m</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0</mml:mn></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf278"><mml:mrow><mml:msub><mml:mi mathsize="90%">d</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig1">Figure 1a</xref>). For <inline-formula><mml:math id="inf279"><mml:mrow><mml:mi mathsize="90%">m</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:math></inline-formula> it is a block-diagonal matrix, where the individual SPs overlap only when <inline-formula><mml:math id="inf280"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. In order to isolate the effects on the network dynamics and computational performance attributable exclusively to the topographic structure, the overall density of the feedforward connectivity matrix is kept constant at <inline-formula><mml:math id="inf281"><mml:mrow><mml:mrow><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mi mathsize="90%">α</mml:mi></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mo mathsize="90%" stretchy="false">*</mml:mo><mml:mi mathsize="90%">ϵ</mml:mi></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0.075</mml:mn></mml:mrow></mml:math></inline-formula> (see also previous section). We note that, while providing the flexibility to implement the variations studied in this manuscript, this formalism has limitations (see Appendix A).</p></sec></sec><sec id="s4-3"><title>Neuron and synapse model</title><p>We study networks composed of LIF neurons with fixed voltage threshold and static synapses with exponentially decaying postsynaptic currents or conductances. The sub-threshold membrane potential dynamics of such a neuron evolves according to:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%">V</mml:mi><mml:mi mathsize="90%">rest</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="90%">V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="90%">R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathsize="90%">I</mml:mi><mml:mi mathsize="90%" mathvariant="normal">E</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="90%">I</mml:mi><mml:mi mathsize="90%" mathvariant="normal">I</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="90%">I</mml:mi><mml:mi mathsize="90%" mathvariant="normal">X</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf282"><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> is the membrane time constant, and <inline-formula><mml:math id="inf283"><mml:mrow><mml:mi mathsize="90%">R</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">I</mml:mi><mml:mi mathsize="90%">β</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the total synaptic input from population <inline-formula><mml:math id="inf284"><mml:mrow><mml:mi mathsize="90%">β</mml:mi><mml:mo mathsize="90%" stretchy="false">∈</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">[</mml:mo><mml:mi mathsize="90%">E</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%">I</mml:mi><mml:mo maxsize="90%" minsize="90%">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The background input <inline-formula><mml:math id="inf285"><mml:msup><mml:mi mathsize="90%">I</mml:mi><mml:mi mathsize="90%" mathvariant="normal">X</mml:mi></mml:msup></mml:math></inline-formula> is assumed to be excitatory and stochastic, modeled as a homogeneous Poisson process with constant rate <inline-formula><mml:math id="inf286"><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">X</mml:mi></mml:msub></mml:math></inline-formula>. Synaptic weights <inline-formula><mml:math id="inf287"><mml:msub><mml:mi mathsize="90%">J</mml:mi><mml:mi mathsize="90%">ij</mml:mi></mml:msub></mml:math></inline-formula>, representing the efficacy of interaction from presynaptic neuron <inline-formula><mml:math id="inf288"><mml:mi mathsize="90%">j</mml:mi></mml:math></inline-formula> to postsynaptic neuron <inline-formula><mml:math id="inf289"><mml:mi mathsize="90%">i</mml:mi></mml:math></inline-formula>, are equal for all realized connections of a given type, that is, <inline-formula><mml:math id="inf290"><mml:mrow><mml:msub><mml:mi mathsize="90%">J</mml:mi><mml:mi mathsize="90%">EE</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msub><mml:mi mathsize="90%">J</mml:mi><mml:mi mathsize="90%">IE</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mi mathsize="90%">J</mml:mi></mml:mrow></mml:math></inline-formula> for excitatory and <inline-formula><mml:math id="inf291"><mml:mrow><mml:msub><mml:mi mathsize="90%">J</mml:mi><mml:mi mathsize="90%">EI</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msub><mml:mi mathsize="90%">J</mml:mi><mml:mi mathsize="90%">II</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mi mathsize="90%">g</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">J</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for inhibitory synapses. All synaptic delays and time constants are equal in this setup. For a complete, tabular description of the models and model parameters used throughout this study, see <xref ref-type="supplementary-material" rid="supp1 supp2 supp3 supp4 supp5">Supplementary files 1–5</xref>.</p><p>Following previous works (<xref ref-type="bibr" rid="bib93">Zajzon et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Duarte and Morrison, 2014</xref>), we choose the intensity of the stochastic input <inline-formula><mml:math id="inf292"><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">X</mml:mi></mml:msub></mml:math></inline-formula> and the E–I ratio <inline-formula><mml:math id="inf293"><mml:mi mathsize="90%">g</mml:mi></mml:math></inline-formula> such that the first two sub-networks operate in a balanced, asynchronous irregular regime when driven solely by background input. This is achieved with <inline-formula><mml:math id="inf294"><mml:mrow><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">X</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+5pt"><mml:mn mathsize="90%">12</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">spikes</mml:mi></mml:mrow><mml:mo mathsize="90%" stretchy="false">/</mml:mo><mml:mi mathsize="90%" mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf295"><mml:mrow><mml:mi mathsize="90%">g</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mn mathsize="90%">12</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, resulting in average firing rates of <inline-formula><mml:math id="inf296"><mml:mrow><mml:mi/><mml:mo mathsize="90%" stretchy="false">∼</mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+5pt"><mml:mn mathsize="90%">3</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">spikes</mml:mi></mml:mrow><mml:mo mathsize="90%" stretchy="false">/</mml:mo><mml:mi mathsize="90%" mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, coefficient of variation (<inline-formula><mml:math id="inf297"><mml:mrow><mml:mi mathsize="90%">C</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">V</mml:mi><mml:mi mathsize="90%">ISI</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) in the interval <inline-formula><mml:math id="inf298"><mml:mrow><mml:mo maxsize="90%" minsize="90%">[</mml:mo><mml:mn mathsize="90%">1.0</mml:mn><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mn mathsize="90%">1.5</mml:mn><mml:mo maxsize="90%" minsize="90%">]</mml:mo></mml:mrow></mml:math></inline-formula> and Pearson cross-correlation (CC) ≤0.01 in <inline-formula><mml:math id="inf299"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf300"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mn mathsize="90%">1</mml:mn></mml:msub></mml:math></inline-formula>.</p><p>In Section ‘A generalizable structural effect’ we consider two additional systems, a network of LIF neurons with conductance-based synapses and a continuous firing rate model. The LIF network is described in detail in <xref ref-type="bibr" rid="bib93">Zajzon et al., 2019</xref>. Spike-triggered synaptic conductances are modeled as exponential functions, with fixed and equal conduction delays for all synapses. Key differences to the current-based model include, in addition to the biologically more plausible synapse model, longer synaptic time constants and stronger input (see also <xref ref-type="bibr" rid="bib93">Zajzon et al., 2019</xref> and <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref> for the numerical values of all parameters).</p><p>The continuous rate model contains <inline-formula><mml:math id="inf301"><mml:mrow><mml:mi mathsize="90%">N</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">3000</mml:mn></mml:mrow></mml:math></inline-formula> nonlinear units, the dynamics of which are governed by:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>+</mml:mo><mml:mi>J</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msqrt><mml:mn>2</mml:mn><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:msub></mml:msqrt><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf302"><mml:mi mathsize="90%" mathvariant="bold-italic">x</mml:mi></mml:math></inline-formula> represents the activation and <inline-formula><mml:math id="inf303"><mml:mi mathsize="90%" mathvariant="bold-italic">r</mml:mi></mml:math></inline-formula> the output of all units, commonly interpreted as the synaptic current variable and the firing rate estimate, respectively. The rates <inline-formula><mml:math id="inf304"><mml:msub><mml:mi mathsize="90%">r</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula> are obtained by applying the nonlinear transfer function <inline-formula><mml:math id="inf305"><mml:mrow><mml:mi mathsize="90%">tanh</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msub><mml:mi mathsize="90%">x</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, modified here to constrain the rates to the interval <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the neuronal time constant, <inline-formula><mml:math id="inf307"><mml:msup><mml:mi mathsize="90%" mathvariant="bold-italic">b</mml:mi><mml:mi mathsize="90%">rec</mml:mi></mml:msup></mml:math></inline-formula> is a vector of individual neuronal bias terms (i.e., a baseline activation), and <inline-formula><mml:math id="inf308"><mml:mi mathsize="90%">J</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf309"><mml:msup><mml:mi mathsize="90%">J</mml:mi><mml:mi mathsize="90%">in</mml:mi></mml:msup></mml:math></inline-formula> are the recurrent (including feedforward) and input weight matrices, respectively. These are constructed in the same manner as for the spiking networks, such that the overall connectivity, including the input mapping onto <inline-formula><mml:math id="inf310"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub></mml:math></inline-formula>, is identical for all three models. Input weights are drawn from a uniform distribution, while the rest follow a normal distribution. Finally, <inline-formula><mml:math id="inf311"><mml:mi mathsize="90%" mathvariant="bold-italic">ξ</mml:mi></mml:math></inline-formula> is a vector of <inline-formula><mml:math id="inf312"><mml:mi mathsize="90%">N</mml:mi></mml:math></inline-formula> independent realizations of Gaussian white noise with zero mean and variance scaled by <inline-formula><mml:math id="inf313"><mml:msub><mml:mi mathsize="90%">σ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">X</mml:mi></mml:msub></mml:math></inline-formula>. The differential equations are integrated numerically, using the Euler–Maruyama method with step <inline-formula><mml:math id="inf314"><mml:mrow><mml:mrow><mml:mi mathsize="90%">δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">t</mml:mi></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mpadded width="+5pt"><mml:mn mathsize="90%">1</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, with specific parameter values given in <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>.</p></sec><sec id="s4-4"><title>Signal reconstruction task</title><p>We evaluate the system’s ability to recover a simple, continuous step signal from a noisy variation using linear combinations of the population responses in the different SSNs (<xref ref-type="bibr" rid="bib47">Maass et al., 2002</xref>). This is equivalent to probing the network’s ability to function as a denoising autoencoder (<xref ref-type="bibr" rid="bib6">Bengio et al., 2013</xref>).</p><p>To generate the <inline-formula><mml:math id="inf315"><mml:msub><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%" mathvariant="normal">C</mml:mi></mml:msub></mml:math></inline-formula>-dimensional input signal <inline-formula><mml:math id="inf316"><mml:mrow><mml:mi mathsize="90%">u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we randomly draw stimuli from a predefined set <inline-formula><mml:math id="inf317"><mml:mrow><mml:mi mathsize="90%">S</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">{</mml:mo><mml:msub><mml:mi mathsize="90%">S</mml:mi><mml:mn mathsize="90%">1</mml:mn></mml:msub><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:msub><mml:mi mathsize="90%">S</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msub><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%" mathvariant="normal">…</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:msub><mml:mi mathsize="90%">S</mml:mi><mml:msub><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%" mathvariant="normal">C</mml:mi></mml:msub></mml:msub><mml:mo maxsize="90%" minsize="90%">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and set the corresponding channel to active for a fixed duration of 200 ms (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, left). This binary step signal <inline-formula><mml:math id="inf318"><mml:mrow><mml:mi mathsize="90%">u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is also the target signal to be reconstructed. The effective input is obtained by adding a Gaussian white noise process with zero mean and variance <inline-formula><mml:math id="inf319"><mml:msubsup><mml:mi mathsize="90%">σ</mml:mi><mml:mi mathsize="90%">ξ</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msubsup></mml:math></inline-formula> to <inline-formula><mml:math id="inf320"><mml:mrow><mml:mi mathsize="90%">u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and scaling the sum with the input rate <inline-formula><mml:math id="inf321"><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">in</mml:mi></mml:msub></mml:math></inline-formula>. Rectifying the resulting signal leads to the final form of the continuous input signal <inline-formula><mml:math id="inf322"><mml:mrow><mml:mrow><mml:mi mathsize="90%">z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msub><mml:mrow><mml:mo maxsize="90%" minsize="90%">[</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">in</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathsize="90%">u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="90%">ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo maxsize="90%" minsize="90%">]</mml:mo></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>. This allows us to control the amount of noise in the input, and thus the task difficulty, through a single parameter <inline-formula><mml:math id="inf323"><mml:msub><mml:mi mathsize="90%">σ</mml:mi><mml:mi mathsize="90%">ξ</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>To deliver the input to the circuit, the analog signal <inline-formula><mml:math id="inf324"><mml:mrow><mml:mi mathsize="90%">z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is converted into spike trains, with its amplitude serving as the rate of an inhomogeneous Poisson process generating independent spike trains. We set the scaling amplitude to <inline-formula><mml:math id="inf325"><mml:mrow><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">in</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%">E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">λ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">X</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, modeling stochastic input with fixed rate <inline-formula><mml:math id="inf326"><mml:mrow><mml:mi mathsize="90%">λ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">X</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="inf327"><mml:mrow><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%">E</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">800</mml:mn></mml:mrow></mml:math></inline-formula> neurons. If not otherwise specified, <inline-formula><mml:math id="inf328"><mml:mrow><mml:mi mathsize="90%">λ</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0.05</mml:mn></mml:mrow></mml:math></inline-formula> holds, resulting in a mean firing rate below 8 spks/sec in <inline-formula><mml:math id="inf329"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig2">Figure 2c</xref>).</p><p>Each input channel <inline-formula><mml:math id="inf330"><mml:mi mathsize="90%">k</mml:mi></mml:math></inline-formula> is mapped onto one of the <inline-formula><mml:math id="inf331"><mml:msub><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%" mathvariant="normal">C</mml:mi></mml:msub></mml:math></inline-formula> stimulus-specific sub-populations of excitatory and inhibitory neurons in the first (input) sub-network <inline-formula><mml:math id="inf332"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub></mml:math></inline-formula>, chosen according to the procedure described above (see also <xref ref-type="fig" rid="fig1">Figure 1a</xref>). This way, each stimulus <inline-formula><mml:math id="inf333"><mml:msub><mml:mi mathsize="90%">S</mml:mi><mml:mi mathsize="90%" mathvariant="normal">k</mml:mi></mml:msub></mml:math></inline-formula> is mapped onto a specific set of sub-populations in the different sub-networks, that is, the topographic map associated with <inline-formula><mml:math id="inf334"><mml:msub><mml:mi mathsize="90%">S</mml:mi><mml:mi mathsize="90%" mathvariant="normal">k</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>For each stimulus in the sequence, we sample the responses of the excitatory population in each <inline-formula><mml:math id="inf335"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula> at fixed time points (once every ms) relative to stimulus onset. We record from the membrane potentials <inline-formula><mml:math id="inf336"><mml:msub><mml:mi mathsize="90%">V</mml:mi><mml:mi mathsize="90%">m</mml:mi></mml:msub></mml:math></inline-formula> as they represent a parameter-free and direct measure of the population state (<xref ref-type="bibr" rid="bib18">Duarte et al., 2018</xref>; <xref ref-type="bibr" rid="bib85">Uhlmann et al., 2017</xref>). The activity vectors are then gathered in a state matrix <inline-formula><mml:math id="inf337"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is then used to train a linear readout to approximate the target output of the task (<xref ref-type="bibr" rid="bib46">Lukoševičius and Jaeger, 2009</xref>). We divide the input data, containing a total of 100 stimulus presentations (yielding <inline-formula><mml:math id="inf338"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> samples), into a training and a testing set (80/20%), and perform the training using ridge regression (L2 regularization), with the regularization parameter chosen by leave-one-out cross-validation on the training dataset.</p><p>Reconstruction performance is measured using the normalized root mean squared error (NRMSE). For this particular task, the effective delay in the build-up of optimal stimulus representations varies greatly across the sub-networks. In order to close in on the optimal delay for each <inline-formula><mml:math id="inf339"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula>, we train the state matrix <inline-formula><mml:math id="inf340"><mml:msub><mml:mi mathsize="90%">X</mml:mi><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:msub></mml:math></inline-formula> on a larger interval of delays and choose the one that minimizes the error, averaged across multiple trials.</p><p>In Section ‘Reconstruction and denoising of dynamical inputs’, we generalize the input to a sinusoidal signal <inline-formula><mml:math id="inf341"><mml:mrow><mml:mrow><mml:mi mathsize="90%">x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathsize="90%">sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mi mathsize="90%">a</mml:mi><mml:mo mathsize="90%" stretchy="false">⋅</mml:mo><mml:mi mathsize="90%">t</mml:mi></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="90%">cos</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mi mathsize="90%">b</mml:mi><mml:mo mathsize="90%" stretchy="false">⋅</mml:mo><mml:mi mathsize="90%">t</mml:mi></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, with parameters <inline-formula><mml:math id="inf342"><mml:mi mathsize="90%">a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf343"><mml:mi mathsize="90%">b</mml:mi></mml:math></inline-formula>. From this, we obtain <inline-formula><mml:math id="inf344"><mml:mrow><mml:mi mathsize="90%">u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> through the sampling and discretization process described in the respective section, and compute the final input <inline-formula><mml:math id="inf345"><mml:mrow><mml:mrow><mml:mi mathsize="90%">z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msub><mml:mrow><mml:mo maxsize="90%" minsize="90%">[</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">in</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathsize="90%">u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="90%">ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo maxsize="90%" minsize="90%">]</mml:mo></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> as above.</p></sec><sec id="s4-5"><title>Effective connectivity and stability analysis</title><p>To better understand the role of structural variations on the network’s dynamics, we determine the network’s effective connectivity matrix <inline-formula><mml:math id="inf346"><mml:mi mathsize="90%">W</mml:mi></mml:math></inline-formula> analytically by linear stability analysis around the system’s stationary working points (see Appendix B for the complete derivations). The elements <inline-formula><mml:math id="inf347"><mml:mrow><mml:msub><mml:mi mathsize="90%">w</mml:mi><mml:mrow><mml:mi mathsize="90%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">j</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="90%" stretchy="false">∈</mml:mo><mml:mi mathsize="90%">W</mml:mi></mml:mrow></mml:math></inline-formula> represent the integrated linear response of a target neuron <inline-formula><mml:math id="inf348"><mml:mi mathsize="90%">i</mml:mi></mml:math></inline-formula>, with stationary rate <inline-formula><mml:math id="inf349"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, to a small perturbation in the input rate <inline-formula><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> caused by a spike from presynaptic neuron <inline-formula><mml:math id="inf351"><mml:mi mathsize="90%">j</mml:mi></mml:math></inline-formula>. In other words, <inline-formula><mml:math id="inf352"><mml:msub><mml:mi mathsize="90%">w</mml:mi><mml:mrow><mml:mi mathsize="90%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> measures the average number of additional spikes emitted by a target neuron <inline-formula><mml:math id="inf353"><mml:mi mathsize="90%">i</mml:mi></mml:math></inline-formula> in response to a spike from the presynaptic neuron <inline-formula><mml:math id="inf354"><mml:mi mathsize="90%">j</mml:mi></mml:math></inline-formula>, and its relation to the synaptic weights is defined by <xref ref-type="bibr" rid="bib81">Tetzlaff et al., 2012</xref>; <xref ref-type="bibr" rid="bib28">Helias et al., 2013</xref>:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mspace width="32pt"/><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>α</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext> with </mml:mtext><mml:mspace width="12pt"/><mml:mrow><mml:mover><mml:mi>α</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext> and </mml:mtext><mml:mspace width="15pt"/><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Note that in <xref ref-type="fig" rid="fig3">Figure 3</xref> we ignore the contribution <inline-formula><mml:math id="inf355"><mml:mover accent="true"><mml:mi mathsize="90%">β</mml:mi><mml:mo mathsize="90%" stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> resulting from the modulation in the input variance <inline-formula><mml:math id="inf356"><mml:msubsup><mml:mi mathsize="90%">σ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">j</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msubsup></mml:math></inline-formula> which is significantly smaller due to the additional factor <inline-formula><mml:math id="inf357"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Importantly, the effective connectivity matrix <inline-formula><mml:math id="inf358"><mml:mi mathsize="90%">W</mml:mi></mml:math></inline-formula> allows us to gain insights into the stability of the system by eigenvalue decomposition. For large random coupling matrices, the effective weight matrix has a spectral radius <inline-formula><mml:math id="inf359"><mml:mrow><mml:mi mathsize="90%">ρ</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%">max</mml:mi><mml:mi mathsize="90%">k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathsize="90%">Re</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">{</mml:mo><mml:msub><mml:mi mathsize="90%">λ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">k</mml:mi></mml:msub><mml:mo maxsize="90%" minsize="90%">}</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> which is determined by the variances of <inline-formula><mml:math id="inf360"><mml:mi mathsize="90%">W</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib64">Rajan and Abbott, 2006</xref>). For inhibition-dominated systems, such as those we consider, there is a single negative outlier representing the mean effective weight, given the eigenvalue <inline-formula><mml:math id="inf361"><mml:msubsup><mml:mi mathsize="90%">λ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">k</mml:mi><mml:mo mathsize="90%" stretchy="false">*</mml:mo></mml:msubsup></mml:math></inline-formula> associated with the unit vector. The stability of the system is thus uniquely determined by the spectral radius <inline-formula><mml:math id="inf362"><mml:mi mathsize="90%">ρ</mml:mi></mml:math></inline-formula>: values smaller than unity indicate stable dynamics, whereas <inline-formula><mml:math id="inf363"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> lead to unstable linearized dynamics.</p></sec><sec id="s4-6"><title>Fixed point analysis</title><p>For the mean-field analysis, the <inline-formula><mml:math id="inf364"><mml:msub><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%" mathvariant="normal">C</mml:mi></mml:msub></mml:math></inline-formula> sub-populations in each sub-network can be reduced to only two groups of neurons, the first one comprising all neurons of the stimulated SPs and the second one comprising all neurons in all non-stimulated SPs. This is possible because (1) the firing rates of the excitatory and inhibitory neurons within one SP are identical, owing to homogeneous neuron parameters and matching incoming connection statistics, and (2) all neurons in non-stimulated SPs have the same rate <inline-formula><mml:math id="inf365"><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup></mml:math></inline-formula> that is in general different from the rate of the stimulated SP <inline-formula><mml:math id="inf366"><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:math></inline-formula>. Here we only sketch the main steps, with a detailed derivation given in Appendix B.</p><p>The mean inputs to the first sub-network can be obtained via<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mspace width=".5em"/><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mphantom><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mphantom></mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf367"><mml:mrow><mml:mi mathsize="90%">γ</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">I</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">/</mml:mo><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">E</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf368"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi>J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Both equations are of the form<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:mi mathsize="90%">κ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">ν</mml:mi></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mi mathsize="90%">μ</mml:mi><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mi mathsize="90%">I</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf369"><mml:mi mathsize="90%">κ</mml:mi></mml:math></inline-formula> is the effective self-coupling of a group of neurons with rate <inline-formula><mml:math id="inf370"><mml:mi mathsize="90%">ν</mml:mi></mml:math></inline-formula> and input <italic>μ</italic>, and <inline-formula><mml:math id="inf371"><mml:mi mathsize="90%">I</mml:mi></mml:math></inline-formula> denotes the external inputs from other groups. <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> describes a linear relationship between the rate <inline-formula><mml:math id="inf372"><mml:mi mathsize="90%">ν</mml:mi></mml:math></inline-formula> and the input <italic>μ</italic>. To find a self-consistent solution for the rates <inline-formula><mml:math id="inf373"><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf374"><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup></mml:math></inline-formula>, the above equations need to be solved numerically, taking into account in addition the <italic>f</italic>–<italic>I</italic> curve <inline-formula><mml:math id="inf375"><mml:mrow><mml:mi mathsize="90%">ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">μ</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the neurons that in the case of LIF model neurons also depends on the variance <inline-formula><mml:math id="inf376"><mml:msup><mml:mi mathsize="90%">σ</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msup></mml:math></inline-formula> of inputs. The latter can be obtained analogous to the mean input <italic>μ</italic> (see Appendix B). Note that for general nonlinearity <inline-formula><mml:math id="inf377"><mml:mrow><mml:mi mathsize="90%">ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">μ</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> there is no analytical closed-form solution for the fixed points.</p><p>Starting from <inline-formula><mml:math id="inf378"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mn mathsize="90%">1</mml:mn></mml:msub></mml:math></inline-formula>, networks are connected in a fixed pattern such that the rate <inline-formula><mml:math id="inf379"><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">i</mml:mi></mml:msub></mml:math></inline-formula> in <inline-formula><mml:math id="inf380"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mi mathsize="90%">i</mml:mi></mml:msub></mml:math></inline-formula> also depends on the excitatory input from the previous sub-network <inline-formula><mml:math id="inf381"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mrow><mml:mi mathsize="90%">i</mml:mi><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> with rate <inline-formula><mml:math id="inf382"><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mrow><mml:mi mathsize="90%">i</mml:mi><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. For a fixed point, we have <inline-formula><mml:math id="inf383"><mml:mrow><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">i</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mrow><mml:mi mathsize="90%">i</mml:mi><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib84">Toyoizumi, 2012</xref>). In this case, we can effectively group together stimulated/non-stimulated neurons in successive sub-networks and re-group equations for the mean input in the limit of many sub-networks, obtaining the simplified description (details see Appendix B)<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The scaling terms of the firing rates incorporate the recurrent and feedforward contributions from the stimulated and non-stimulated groups of neurons. They depend solely on some fixed parameters of the system, including modularity <inline-formula><mml:math id="inf384"><mml:mi mathsize="90%">m</mml:mi></mml:math></inline-formula> (see Appendix B). Importantly, <xref ref-type="disp-formula" rid="equ9 equ10">Equations 9 and 10</xref> and have the same linear form as (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>) <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> and can be solved numerically as described above. Again, for general nonlinear <inline-formula><mml:math id="inf385"><mml:mrow><mml:mi mathsize="90%">ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">μ</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> there is no closed-form analytical solution, but see below for a piecewise linear activation function <inline-formula><mml:math id="inf386"><mml:mrow><mml:mi mathsize="90%">ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">μ</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The numerical solutions for fixed points are obtained using the root finding algorithm root of the scipy.optimize package (<xref ref-type="bibr" rid="bib88">Virtanen et al., 2020</xref>). The stability of the fixed points is obtained by inserting the corresponding firing rates into the effective connectivity <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>. On the level of stimulated and non-stimulated sub-populations, the effective connectivity matrix reads<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi>α</mml:mi><mml:mo stretchy="false">˜</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi>α</mml:mi><mml:mo stretchy="false">˜</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi>α</mml:mi><mml:mo stretchy="false">˜</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi>α</mml:mi><mml:mo stretchy="false">˜</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>from which we obtain the maximum eigenvalue <inline-formula><mml:math id="inf387"><mml:mi mathsize="90%">ρ</mml:mi></mml:math></inline-formula>, which for stable fixed points must be smaller than 1.</p><p>The structure of fixed points for the stimulated sub-population (see discussion in ‘Modularity as a bifurcation parameter’) can furthermore be intuitively understood by studying the potential landscape of the system. The potential <inline-formula><mml:math id="inf388"><mml:mi mathsize="90%">U</mml:mi></mml:math></inline-formula> is thereby defined via the conservative force <inline-formula><mml:math id="inf389"><mml:mrow><mml:mi mathsize="90%">F</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mfrac><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">U</mml:mi></mml:mrow><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="90%">ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">μ</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:msup><mml:mi mathsize="90%">σ</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> that drives the system toward its fixed points via the equation of motion <inline-formula><mml:math id="inf390"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">t</mml:mi></mml:mrow></mml:mfrac><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mi mathsize="90%">F</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib92">Wong and Wang, 2006</xref>; <xref ref-type="bibr" rid="bib44">Litwin-Kumar and Doiron, 2012</xref>; <xref ref-type="bibr" rid="bib73">Schuecker et al., 2017</xref>). Note that <italic>μ</italic> and <inline-formula><mml:math id="inf391"><mml:msup><mml:mi mathsize="90%">σ</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msup></mml:math></inline-formula> are again functions of <inline-formula><mml:math id="inf392"><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf393"><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup></mml:math></inline-formula>, where the latter is the self-consistent rate of the non-stimulated sub-populations for given rate <inline-formula><mml:math id="inf394"><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S</mml:mi></mml:msup></mml:math></inline-formula> of the stimulated sub-population, <inline-formula><mml:math id="inf395"><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (details see Appendix B).</p></sec><sec id="s4-7"><title>Multiple inputs and correlation-based similarity score</title><p>In <xref ref-type="fig" rid="fig9">Figure 9</xref>, we consider two stimuli <inline-formula><mml:math id="inf396"><mml:msub><mml:mi mathsize="90%">S</mml:mi><mml:mn mathsize="90%">1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf397"><mml:msub><mml:mi mathsize="90%">S</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msub></mml:math></inline-formula> to be active simultaneously for 10 s. Let <inline-formula><mml:math id="inf398"><mml:mrow><mml:mi mathsize="90%">S</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">P</mml:mi><mml:mn mathsize="90%">1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf399"><mml:mrow><mml:mi mathsize="90%">S</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">P</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> be the two corresponding SPs in each sub-network. The firing rate of each SP is estimated from spike counts in time bins of 10 ms and smoothed with a Savitzky-Golay filter (length 21 and polynomial order 4). We compute a similarity score based on the correlation between these rates, scaled by the ratio of the input intensities <inline-formula><mml:math id="inf400"><mml:mrow><mml:msub><mml:mi mathsize="90%">λ</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msub><mml:mo mathsize="90%" stretchy="false">/</mml:mo><mml:msub><mml:mi mathsize="90%">λ</mml:mi><mml:mn mathsize="90%">1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (with <inline-formula><mml:math id="inf401"><mml:msub><mml:mi mathsize="90%">λ</mml:mi><mml:mn mathsize="90%">1</mml:mn></mml:msub></mml:math></inline-formula> fixed). This scaling is meant to introduce a gradient in the similarity score based on the firing rate differences, ensuring that high (absolute) scores require comparable activity levels in addition to strong correlations. To ensure that both stimuli are decodable where appropriate, we set the score to 0 when the difference between the rate of <inline-formula><mml:math id="inf402"><mml:mrow><mml:mi mathsize="90%">S</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">P</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and the non-stimulated SPs was &lt;1 spks/sec (<inline-formula><mml:math id="inf403"><mml:mrow><mml:mi mathsize="90%">S</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">P</mml:mi><mml:mn mathsize="90%">1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> had significantly higher rates). The curves in <xref ref-type="fig" rid="fig9">Figure 9c</xref> mark the regime boundaries: coexisting (Co-Ex) where score is &gt;0.1 (red curve); WLC where score is &lt;−0.1 (blue); WTA (gray) and where the score is in the interval (−0.1, 0.1), and either <inline-formula><mml:math id="inf404"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> holds or the score is 0. While the Co-Ex region is a dynamical regime that only occurs in the initial sub-networks (<xref ref-type="fig" rid="fig9">Figure 9d</xref>), the WTA and WLC regimes persist and can be understood again with the help of a potential <inline-formula><mml:math id="inf405"><mml:mi mathsize="90%">U</mml:mi></mml:math></inline-formula>, which is in this case a function of the rates of the two SPs (details see Appendix B).</p></sec><sec id="s4-8"><title>Numerical simulations and analysis</title><p>All numerical simulations were conducted using the Neural Microcircuit Simulation and Analysis Toolkit (NMSAT) v0.2 (<xref ref-type="bibr" rid="bib17">Duarte et al., 2017</xref>), a high-level Python framework for creating, simulating and evaluating complex, spiking neural microcircuits in a modular fashion. It builds on the PyNEST interface for NEST (<xref ref-type="bibr" rid="bib24">Gewaltig and Diesmann, 2007</xref>), which provides the core simulation engine. To ensure the reproduction of all the numerical experiments and figures presented in this study, and abide by the recommendations proposed in <xref ref-type="bibr" rid="bib60">Pauli et al., 2018</xref>, we provide a complete code package that implements project-specific functionality within NMSAT (see Data availability) using NEST 2.18.0 (<xref ref-type="bibr" rid="bib31">Jordan et al., 2019</xref>).</p></sec><sec id="s4-9"><title>Competing interests</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Software, Formal analysis, Supervision, Investigation, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Tabular description of current-based (baseline) network model.</title></caption><media xlink:href="elife-77009-supp1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Model parameters for the current-based network.</title></caption><media xlink:href="elife-77009-supp2-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Parameter values for the conductance-based model.</title></caption><media xlink:href="elife-77009-supp3-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>Description of the rate model.</title></caption><media xlink:href="elife-77009-supp4-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="supp5"><label>Supplementary file 5.</label><caption><title>Rate model parameters.</title></caption><media xlink:href="elife-77009-supp5-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-77009-transrepform1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. Modelling code can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6326496">https://doi.org/10.5281/zenodo.6326496</ext-link> (see also Supplementary Files). Source data and code files are also attached as zip folders to the individual main figures of this manuscript.</p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors gratefully acknowledge the computing time granted by the JARA-HPC Vergabegremium on the supercomputer JURECA at Forschungszentrum Jülich.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>SK</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name><name><surname>Müller</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Attention facilitates multiple stimulus features in parallel in human visual cortex</article-title><source>Current Biology</source><volume>18</volume><fpage>1006</fpage><lpage>1009</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.06.030</pub-id><pub-id pub-id-type="pmid">18595707</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aviel</surname><given-names>Y</given-names></name><name><surname>Mehring</surname><given-names>C</given-names></name><name><surname>Abeles</surname><given-names>M</given-names></name><name><surname>Horn</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>On embedding synfire chains in a balanced network</article-title><source>Neural Computation</source><volume>15</volume><fpage>1321</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.1162/089976603321780290</pub-id><pub-id pub-id-type="pmid">12816575</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aviel</surname><given-names>Y</given-names></name><name><surname>Horn</surname><given-names>D</given-names></name><name><surname>Abeles</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Memory capacity of balanced networks</article-title><source>Neural Computation</source><volume>17</volume><fpage>691</fpage><lpage>713</lpage><pub-id pub-id-type="doi">10.1162/0899766053019962</pub-id><pub-id pub-id-type="pmid">15802011</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Babadi</surname><given-names>B</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sparseness and expansion in sensory representations</article-title><source>Neuron</source><volume>83</volume><fpage>1213</fpage><lpage>1226</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.07.035</pub-id><pub-id pub-id-type="pmid">25155954</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bednar</surname><given-names>JA</given-names></name><name><surname>Wilson</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cortical maps</article-title><source>The Neuroscientist</source><volume>22</volume><fpage>604</fpage><lpage>617</lpage><pub-id pub-id-type="doi">10.1177/1073858415597645</pub-id><pub-id pub-id-type="pmid">26290447</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name><name><surname>Vincent</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representation learning: a review and new perspectives</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>35</volume><fpage>1798</fpage><lpage>1828</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2013.50</pub-id><pub-id pub-id-type="pmid">23787338</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borghuis</surname><given-names>BG</given-names></name><name><surname>Tadin</surname><given-names>D</given-names></name><name><surname>Lankheet</surname><given-names>MJM</given-names></name><name><surname>Lappin</surname><given-names>JS</given-names></name><name><surname>van de Grind</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Temporal limits of visual motion processing: psychophysics and neurophysiology</article-title><source>Vision</source><volume>3</volume><elocation-id>5</elocation-id><pub-id pub-id-type="doi">10.3390/vision3010005</pub-id><pub-id pub-id-type="pmid">31735806</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brefczynski-Lewis</surname><given-names>JA</given-names></name><name><surname>Datta</surname><given-names>R</given-names></name><name><surname>Lewis</surname><given-names>JW</given-names></name><name><surname>DeYoe</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The topography of visuospatial attention as revealed by a novel visual field mapping technique</article-title><source>Journal of Cognitive Neuroscience</source><volume>21</volume><fpage>1447</fpage><lpage>1460</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21005</pub-id><pub-id pub-id-type="pmid">18752412</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Dynamics of networks of randomly connected excitatory and inhibitory spiking neurons</article-title><source>Journal of Physiology, Paris</source><volume>94</volume><fpage>445</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/s0928-4257(00)01084-6</pub-id><pub-id pub-id-type="pmid">11165912</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Normalization as a canonical neural computation</article-title><source>Nature Reviews. Neuroscience</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id><pub-id pub-id-type="pmid">22108672</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cayco-Gajic</surname><given-names>NA</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neutral stability, rate propagation, and critical branching in feedforward networks</article-title><source>Neural Computation</source><volume>25</volume><fpage>1768</fpage><lpage>1806</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00461</pub-id><pub-id pub-id-type="pmid">23607560</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cortes</surname><given-names>N</given-names></name><name><surname>van Vreeswijk</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Pulvinar thalamic nucleus allows for asynchronous spike propagation through the cortex</article-title><source>Frontiers in Computational Neuroscience</source><volume>9</volume><elocation-id>60</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2015.00060</pub-id><pub-id pub-id-type="pmid">26042026</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diesmann</surname><given-names>M</given-names></name><name><surname>Gewaltig</surname><given-names>M-O</given-names></name><name><surname>Aertsen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Stable propagation of synchronous spiking in cortical neural networks</article-title><source>Nature</source><volume>402</volume><fpage>529</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1038/990101</pub-id><pub-id pub-id-type="pmid">10591212</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Douglas</surname><given-names>RJ</given-names></name><name><surname>Martin</surname><given-names>KAC</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Neuronal circuits of the neocortex</article-title><source>Annual Review of Neuroscience</source><volume>27</volume><fpage>419</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144152</pub-id><pub-id pub-id-type="pmid">15217339</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duarte</surname><given-names>RCF</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dynamic stability of sequential stimulus representations in adapting neuronal networks</article-title><source>Frontiers in Computational Neuroscience</source><volume>8</volume><elocation-id>124</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2014.00124</pub-id><pub-id pub-id-type="pmid">25374534</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Duarte</surname><given-names>R</given-names></name><name><surname>Zajzon</surname><given-names>B</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Neural microcircuit simulation and analysis toolkit</data-title><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.582645">https://doi.org/10.5281/zenodo.582645</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Duarte</surname><given-names>R</given-names></name><name><surname>Uhlmann</surname><given-names>M</given-names></name><name><surname>den van Broek</surname><given-names>D</given-names></name><name><surname>Fitz</surname><given-names>H</given-names></name><name><surname>Petersson</surname><given-names>KM</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Encoding symbolic sequences with spiking neural reservoirs</article-title><conf-name>2018 International Joint Conference on Neural Networks (IJCNN</conf-name><conf-loc>Rio de Janeiro</conf-loc><pub-id pub-id-type="doi">10.1109/IJCNN.2018.8489114</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faisal</surname><given-names>AA</given-names></name><name><surname>Selen</surname><given-names>LP</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Noise in the nervous system</article-title><source>Nature Reviews Neuroscience</source><volume>9</volume><fpage>292</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1038/nrn2258</pub-id><pub-id pub-id-type="pmid">18319728</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1-a</pub-id><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrante</surname><given-names>M</given-names></name><name><surname>Migliore</surname><given-names>M</given-names></name><name><surname>Ascoli</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Feed-forward inhibition as a buffer of the neuronal input-output relation</article-title><source>PNAS</source><volume>106</volume><fpage>18004</fpage><lpage>18009</lpage><pub-id pub-id-type="doi">10.1073/pnas.0904784106</pub-id><pub-id pub-id-type="pmid">19815518</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fourcaud</surname><given-names>N</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Dynamics of the firing probability of noisy integrate-and-fire neurons</article-title><source>Neural Computation</source><volume>14</volume><fpage>2057</fpage><lpage>2110</lpage><pub-id pub-id-type="doi">10.1162/089976602320264015</pub-id><pub-id pub-id-type="pmid">12184844</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id><pub-id pub-id-type="pmid">15937014</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gewaltig</surname><given-names>MO</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nest (neural simulation tool)</article-title><source>Scholarpedia</source><volume>2</volume><elocation-id>1430</elocation-id><pub-id pub-id-type="doi">10.4249/scholarpedia.1430</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagler</surname><given-names>DJ</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spatial maps in frontal and prefrontal cortex</article-title><source>NeuroImage</source><volume>29</volume><fpage>567</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.08.058</pub-id><pub-id pub-id-type="pmid">16289928</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haider</surname><given-names>B</given-names></name><name><surname>Duque</surname><given-names>A</given-names></name><name><surname>Hasenstaub</surname><given-names>AR</given-names></name><name><surname>McCormick</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neocortical network activity in vivo is generated through a dynamic balance of excitation and inhibition</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>4535</fpage><lpage>4545</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5297-05.2006</pub-id><pub-id pub-id-type="pmid">16641233</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hegdé</surname><given-names>J</given-names></name><name><surname>Felleman</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Reappraising the functional implications of the primate visual anatomical hierarchy</article-title><source>The Neuroscientist</source><volume>13</volume><fpage>416</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1177/1073858407305201</pub-id><pub-id pub-id-type="pmid">17901251</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helias</surname><given-names>M</given-names></name><name><surname>Tetzlaff</surname><given-names>T</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Echoes in correlated neural systems</article-title><source>New Journal of Physics</source><volume>15</volume><elocation-id>023002</elocation-id><pub-id pub-id-type="doi">10.1088/1367-2630/15/2/023002</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Himberger</surname><given-names>KD</given-names></name><name><surname>Chien</surname><given-names>H-Y</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Principles of temporal processing across the cortical hierarchy</article-title><source>Neuroscience</source><volume>389</volume><fpage>161</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2018.04.030</pub-id><pub-id pub-id-type="pmid">29729293</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itti</surname><given-names>L</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Niebur</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A model of saliency-based visual attention for rapid scene analysis</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>20</volume><fpage>1254</fpage><lpage>1259</lpage><pub-id pub-id-type="doi">10.1109/34.730558</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jordan</surname><given-names>J</given-names></name><name><surname>Mørk</surname><given-names>H</given-names></name><name><surname>Vennemo</surname><given-names>SB</given-names></name><name><surname>Terhorst</surname><given-names>D</given-names></name><name><surname>Peyser</surname><given-names>A</given-names></name><name><surname>Ippen</surname><given-names>T</given-names></name><name><surname>Deepu</surname><given-names>R</given-names></name><name><surname>Eppler</surname><given-names>JM</given-names></name><name><surname>Kunkel</surname><given-names>S</given-names></name><name><surname>Sinha</surname><given-names>A</given-names></name><name><surname>Fardet</surname><given-names>T</given-names></name><name><surname>Diaz</surname><given-names>S</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name><name><surname>Schenck</surname><given-names>W</given-names></name><name><surname>Dahmen</surname><given-names>D</given-names></name><name><surname>Pronold</surname><given-names>J</given-names></name><name><surname>Stapmanns</surname><given-names>J</given-names></name><name><surname>Trensch</surname><given-names>G</given-names></name><name><surname>Spreizer</surname><given-names>S</given-names></name><name><surname>Mitchell</surname><given-names>J</given-names></name><name><surname>Graber</surname><given-names>S</given-names></name><name><surname>Senk</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Nest 2.18.0</data-title><version designator="2.18.0">2.18.0</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.2605422">https://doi.org/10.5281/zenodo.2605422</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaas</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Topographic maps are fundamental to sensory processing</article-title><source>Brain Research Bulletin</source><volume>44</volume><fpage>107</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1016/s0361-9230(97)00094-4</pub-id><pub-id pub-id-type="pmid">9292198</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kadmon</surname><given-names>J</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>Optimal architectures in a solvable model of deep networks</chapter-title><person-group person-group-type="editor"><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Sugiyama</surname><given-names>M</given-names></name><name><surname>Luxburg</surname><given-names>U</given-names></name><name><surname>Guyon</surname><given-names>I</given-names></name><name><surname>Garnett</surname><given-names>R</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keliris</surname><given-names>GA</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Papanikolaou</surname><given-names>A</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Smirnakis</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Estimating average single-neuron visual receptive field sizes by fmri</article-title><source>PNAS</source><volume>116</volume><fpage>6425</fpage><lpage>6434</lpage><pub-id pub-id-type="doi">10.1073/pnas.1809612116</pub-id><pub-id pub-id-type="pmid">30867291</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klos</surname><given-names>C</given-names></name><name><surname>Miner</surname><given-names>D</given-names></name><name><surname>Triesch</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bridging structure and function: a model of sequence learning and prediction in primary visual cortex</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006187</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006187</pub-id><pub-id pub-id-type="pmid">29870532</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Jehee</surname><given-names>JFM</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>265</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.034</pub-id><pub-id pub-id-type="pmid">22841311</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>A</given-names></name><name><surname>Rotter</surname><given-names>S</given-names></name><name><surname>Aertsen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008a</year><article-title>Conditions for propagating synchronous spiking and asynchronous firing rates in a cortical network model</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>5268</fpage><lpage>5280</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2542-07.2008</pub-id><pub-id pub-id-type="pmid">18480283</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>A</given-names></name><name><surname>Schrader</surname><given-names>S</given-names></name><name><surname>Aertsen</surname><given-names>A</given-names></name><name><surname>Rotter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008b</year><article-title>The high-conductance state of cortical networks</article-title><source>Neural Computation</source><volume>20</volume><fpage>1</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.20.1.1</pub-id><pub-id pub-id-type="pmid">18044999</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>A</given-names></name><name><surname>Rotter</surname><given-names>S</given-names></name><name><surname>Aertsen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Spiking activity propagation in neuronal networks: reconciling different perspectives on neural coding</article-title><source>Nature Reviews. Neuroscience</source><volume>11</volume><fpage>615</fpage><lpage>627</lpage><pub-id pub-id-type="doi">10.1038/nrn2886</pub-id><pub-id pub-id-type="pmid">20725095</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lagzi</surname><given-names>F</given-names></name><name><surname>Rotter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dynamics of competition between subnetworks of spiking neuronal networks in the balanced state</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0138947</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0138947</pub-id><pub-id pub-id-type="pmid">26407178</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lagzi</surname><given-names>F</given-names></name><name><surname>Atay</surname><given-names>FM</given-names></name><name><surname>Rotter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Bifurcation analysis of the dynamics of interacting subnetworks of a spiking network</article-title><source>Scientific Reports</source><volume>9</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1038/s41598-019-47190-9</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Layer</surname><given-names>M</given-names></name><name><surname>Senk</surname><given-names>J</given-names></name><name><surname>Essink</surname><given-names>S</given-names></name><name><surname>Korvasová</surname><given-names>K</given-names></name><name><surname>van Meegen</surname><given-names>A</given-names></name><name><surname>Bos</surname><given-names>H</given-names></name><name><surname>Schuecker</surname><given-names>J</given-names></name><name><surname>Helias</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Lif meanfield tools</data-title><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3661413">https://doi.org/10.5281/zenodo.3661413</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Kozyrev</surname><given-names>V</given-names></name><name><surname>Kyllingsbæk</surname><given-names>S</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name><name><surname>Ditlevsen</surname><given-names>S</given-names></name><name><surname>Bundesen</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neurons in primate visual cortex alternate between responses to multiple stimuli in their receptive field</article-title><source>Frontiers in Computational Neuroscience</source><volume>10</volume><elocation-id>141</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2016.00141</pub-id><pub-id pub-id-type="pmid">28082892</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Slow dynamics and high variability in balanced cortical networks with clustered connections</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1498</fpage><lpage>1505</lpage><pub-id pub-id-type="doi">10.1038/nn.3220</pub-id><pub-id pub-id-type="pmid">23001062</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>L</given-names></name><name><surname>She</surname><given-names>L</given-names></name><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Lu</surname><given-names>HD</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name><name><surname>Poo</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Spatial structure of neuronal receptive field in awake monkey secondary visual cortex (V2)</article-title><source>PNAS</source><volume>113</volume><fpage>1913</fpage><lpage>1918</lpage><pub-id pub-id-type="doi">10.1073/pnas.1525505113</pub-id><pub-id pub-id-type="pmid">26839410</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lukoševičius</surname><given-names>M</given-names></name><name><surname>Jaeger</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reservoir computing approaches to recurrent neural network training</article-title><source>Computer Science Review</source><volume>3</volume><fpage>127</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/j.cosrev.2009.03.005</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Natschläger</surname><given-names>T</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Real-Time computing without stable states: a new framework for neural computation based on perturbations</article-title><source>Neural Computation</source><volume>14</volume><fpage>2531</fpage><lpage>2560</lpage><pub-id pub-id-type="doi">10.1162/089976602760407955</pub-id><pub-id pub-id-type="pmid">12433288</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mascaro</surname><given-names>M</given-names></name><name><surname>Amit</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effective neural response function for collective population states</article-title><source>Network</source><volume>10</volume><fpage>351</fpage><lpage>373</lpage><pub-id pub-id-type="pmid">10695764</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCormick</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neuronal networks: flip-flops in the brain</article-title><source>Current Biology</source><volume>15</volume><fpage>R294</fpage><lpage>R296</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2005.04.009</pub-id><pub-id pub-id-type="pmid">15854894</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meunier</surname><given-names>D</given-names></name><name><surname>Lambiotte</surname><given-names>R</given-names></name><name><surname>Bullmore</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modular and hierarchically modular organization of brain networks</article-title><source>Frontiers in Neuroscience</source><volume>4</volume><elocation-id>200</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2010.00200</pub-id><pub-id pub-id-type="pmid">21151783</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Młynarski</surname><given-names>WF</given-names></name><name><surname>Hermundstad</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Adaptive coding for dynamic sensory inference</article-title><source>eLife</source><volume>7</volume><elocation-id>e32055</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.32055</pub-id><pub-id pub-id-type="pmid">29988020</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Rinzel</surname><given-names>J</given-names></name><name><surname>Rubin</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Noise-Induced alternations in an attractor network model of perceptual bistability</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>1125</fpage><lpage>1139</lpage><pub-id pub-id-type="doi">10.1152/jn.00116.2007</pub-id><pub-id pub-id-type="pmid">17615138</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mountcastle</surname><given-names>VB</given-names></name><name><surname>Powell</surname><given-names>TP</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Neural mechanisms subserving cutaneous sensibility, with special reference to the role of afferent inhibition in sensory perception and discrimination</article-title><source>Bulletin of the Johns Hopkins Hospital</source><volume>105</volume><fpage>201</fpage><lpage>232</lpage><pub-id pub-id-type="pmid">14424738</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakajima</surname><given-names>M</given-names></name><name><surname>Halassa</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Thalamic control of functional cortical connectivity</article-title><source>Current Opinion in Neurobiology</source><volume>44</volume><fpage>127</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.04.001</pub-id><pub-id pub-id-type="pmid">28486176</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newman</surname><given-names>MEJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Random graphs with clustering</article-title><source>Physical Review Letters</source><volume>103</volume><elocation-id>058701</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.103.058701</pub-id><pub-id pub-id-type="pmid">19792540</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okada</surname><given-names>K</given-names></name><name><surname>Rong</surname><given-names>F</given-names></name><name><surname>Venezia</surname><given-names>J</given-names></name><name><surname>Matchin</surname><given-names>W</given-names></name><name><surname>Hsieh</surname><given-names>I-H</given-names></name><name><surname>Saberi</surname><given-names>K</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Hickok</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Hierarchical organization of human auditory cortex: evidence from acoustic invariance in the response to intelligible speech</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>2486</fpage><lpage>2495</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp318</pub-id><pub-id pub-id-type="pmid">20100898</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>HJ</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Structural and functional brain networks: from connections to cognition</article-title><source>Science</source><volume>342</volume><elocation-id>1238411</elocation-id><pub-id pub-id-type="doi">10.1126/science.1238411</pub-id><pub-id pub-id-type="pmid">24179229</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parr</surname><given-names>T</given-names></name><name><surname>Corcoran</surname><given-names>AW</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Hohwy</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Perceptual awareness and active inference</article-title><source>Neuroscience of Consciousness</source><volume>2019</volume><elocation-id>09</elocation-id><pub-id pub-id-type="doi">10.1093/nc/niz012</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>GH</given-names></name><name><surname>Kaplan</surname><given-names>DM</given-names></name><name><surname>Snyder</surname><given-names>LH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Topographic organization in the brain: searching for general principles</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>351</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.03.008</pub-id><pub-id pub-id-type="pmid">24862252</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pauli</surname><given-names>R</given-names></name><name><surname>Weidel</surname><given-names>P</given-names></name><name><surname>Kunkel</surname><given-names>S</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reproducing polychronization: a guide to maximizing the reproducibility of spiking network models</article-title><source>Frontiers in Neuroinformatics</source><volume>12</volume><elocation-id>46</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2018.00046</pub-id><pub-id pub-id-type="pmid">30123121</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Deneve</surname><given-names>S</given-names></name><name><surname>Ducom</surname><given-names>J-C</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Narrow versus wide tuning curves: what’s best for a population code?</article-title><source>Neural Computation</source><volume>11</volume><fpage>85</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1162/089976699300016818</pub-id><pub-id pub-id-type="pmid">9950723</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pradhan</surname><given-names>N</given-names></name><name><surname>Dasgupta</surname><given-names>S</given-names></name><name><surname>Sinha</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Modular organization enhances the robustness of attractor network dynamics</article-title><source>EPL</source><volume>94</volume><elocation-id>38004</elocation-id><pub-id pub-id-type="doi">10.1209/0295-5075/94/38004</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabinovich</surname><given-names>MI</given-names></name><name><surname>Huerta</surname><given-names>R</given-names></name><name><surname>Varona</surname><given-names>P</given-names></name><name><surname>Afraimovich</surname><given-names>VS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Transient cognitive dynamics, metastability, and decision making</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000072</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000072</pub-id><pub-id pub-id-type="pmid">18452000</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname><given-names>K</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Eigenvalue spectra of random matrices for neural networks</article-title><source>Physical Review Letters</source><volume>97</volume><elocation-id>188104</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.97.188104</pub-id><pub-id pub-id-type="pmid">17155583</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renart</surname><given-names>A</given-names></name><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name><name><surname>Parga</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Mean-driven and fluctuation-driven persistent activity in recurrent networks</article-title><source>Neural Computation</source><volume>19</volume><fpage>1</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1162/neco.2007.19.1.1</pub-id><pub-id pub-id-type="pmid">17134316</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renart</surname><given-names>A</given-names></name><name><surname>van Rossum</surname><given-names>MCW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Transmission of population-coded information</article-title><source>Neural Computation</source><volume>24</volume><fpage>391</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00227</pub-id><pub-id pub-id-type="pmid">22023200</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renart</surname><given-names>A</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Variability in neural activity and behavior</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><fpage>211</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.02.013</pub-id><pub-id pub-id-type="pmid">24632334</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roberts</surname><given-names>MT</given-names></name><name><surname>Seeman</surname><given-names>SC</given-names></name><name><surname>Golding</surname><given-names>NL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A mechanistic understanding of the role of feedforward inhibition in the mammalian sound localization circuitry</article-title><source>Neuron</source><volume>78</volume><fpage>923</fpage><lpage>935</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.04.022</pub-id><pub-id pub-id-type="pmid">23764291</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rost</surname><given-names>T</given-names></name><name><surname>Deger</surname><given-names>M</given-names></name><name><surname>Nawrot</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Winnerless competition in clustered balanced networks: inhibitory assemblies do the trick</article-title><source>Biol Cybern</source><volume>112</volume><fpage>81</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1007/s00422-017-0737-7</pub-id><pub-id pub-id-type="pmid">29075845</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaub</surname><given-names>MT</given-names></name><name><surname>Billeh</surname><given-names>YN</given-names></name><name><surname>Anastassiou</surname><given-names>CA</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Barahona</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Emergence of slow-switching assemblies in structured neuronal networks</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004196</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004196</pub-id><pub-id pub-id-type="pmid">26176664</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schittler Neves</surname><given-names>F</given-names></name><name><surname>Timme</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Computation by switching in complex networks of states</article-title><source>Physical Review Letters</source><volume>109</volume><elocation-id>018701</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.109.018701</pub-id><pub-id pub-id-type="pmid">23031136</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuecker</surname><given-names>J</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name><name><surname>Helias</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Modulated escape from a metastable state driven by colored noise</article-title><source>Physical Review. E, Statistical, Nonlinear, and Soft Matter Physics</source><volume>92</volume><elocation-id>052119</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.92.052119</pub-id><pub-id pub-id-type="pmid">26651659</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuecker</surname><given-names>J</given-names></name><name><surname>Schmidt</surname><given-names>M</given-names></name><name><surname>van Albada</surname><given-names>SJ</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name><name><surname>Helias</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fundamental activity constraints lead to specific interpretations of the connectome</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005179</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005179</pub-id><pub-id pub-id-type="pmid">28146554</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seriès</surname><given-names>P</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Tuning curve sharpening for orientation selectivity: coding efficiency and the impact of correlations</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>1129</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1038/nn1321</pub-id><pub-id pub-id-type="pmid">15452579</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Noise, neural codes and cortical organization</article-title><source>Current Opinion in Neurobiology</source><volume>4</volume><fpage>569</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1016/0959-4388(94)90059-0</pub-id><pub-id pub-id-type="pmid">7812147</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The variable discharge of cortical neurons: implications for connectivity, computation, and information coding</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>3870</fpage><lpage>3896</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-10-03870.1998</pub-id><pub-id pub-id-type="pmid">9570816</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherman</surname><given-names>SM</given-names></name><name><surname>Guillery</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The role of the thalamus in the flow of information to the cortex</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>357</volume><fpage>1695</fpage><lpage>1708</lpage><pub-id pub-id-type="doi">10.1098/rstb.2002.1161</pub-id><pub-id pub-id-type="pmid">12626004</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>MA</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Topographic maps in human frontal and parietal cortex</article-title><source>Trends in Cognitive Sciences</source><volume>13</volume><fpage>488</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.08.005</pub-id><pub-id pub-id-type="pmid">19758835</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>AT</given-names></name><name><surname>Singh</surname><given-names>KD</given-names></name><name><surname>Williams</surname><given-names>AL</given-names></name><name><surname>Greenlee</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Estimating receptive field size from fmri data in human striate and extrastriate visual cortex</article-title><source>Cerebral Cortex</source><volume>11</volume><fpage>1182</fpage><lpage>1190</lpage><pub-id pub-id-type="doi">10.1093/cercor/11.12.1182</pub-id><pub-id pub-id-type="pmid">11709489</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tetzlaff</surname><given-names>T</given-names></name><name><surname>Buschermöhle</surname><given-names>M</given-names></name><name><surname>Geisel</surname><given-names>T</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The spread of rate and correlation in stationary cortical networks</article-title><source>Neurocomputing</source><volume>52–54</volume><fpage>949</fpage><lpage>954</lpage><pub-id pub-id-type="doi">10.1016/S0925-2312(02)00854-8</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tetzlaff</surname><given-names>T</given-names></name><name><surname>Helias</surname><given-names>M</given-names></name><name><surname>Einevoll</surname><given-names>GT</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decorrelation of neural-network activity by inhibitory feedback</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002596</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002596</pub-id><pub-id pub-id-type="pmid">23133368</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tkacik</surname><given-names>G</given-names></name><name><surname>Prentice</surname><given-names>JS</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Optimal population coding by noisy spiking neurons</article-title><source>PNAS</source><volume>107</volume><fpage>14419</fpage><lpage>14424</lpage><pub-id pub-id-type="doi">10.1073/pnas.1004906107</pub-id><pub-id pub-id-type="pmid">20660781</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tomasello</surname><given-names>R</given-names></name><name><surname>Garagnani</surname><given-names>M</given-names></name><name><surname>Wennekers</surname><given-names>T</given-names></name><name><surname>Pulvermüller</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A neurobiologically constrained cortex model of semantic grounding with spiking neurons and brain-like connectivity</article-title><source>Frontiers in Computational Neuroscience</source><volume>12</volume><elocation-id>88</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2018.00088</pub-id><pub-id pub-id-type="pmid">30459584</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toyoizumi</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Nearly extensive sequential memory lifetime achieved by coupled nonlinear neurons</article-title><source>Neural Computation</source><volume>24</volume><fpage>2678</fpage><lpage>2699</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00324</pub-id><pub-id pub-id-type="pmid">22594828</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Uhlmann</surname><given-names>M</given-names></name><name><surname>Fitz</surname><given-names>H</given-names></name><name><surname>Duarte</surname><given-names>R</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>Petersson</surname><given-names>KM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The Best Spike Filter Kernel Is a Neuron</article-title><conf-name>Conference on Cognitive Computational Neuroscience</conf-name></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>VanRullen</surname><given-names>R</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Is perception discrete or continuous?</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>207</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(03)00095-0</pub-id><pub-id pub-id-type="pmid">12757822</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Vreeswijk</surname><given-names>C</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title><source>Science</source><volume>274</volume><fpage>1724</fpage><lpage>1726</lpage><pub-id pub-id-type="doi">10.1126/science.274.5293.1724</pub-id><pub-id pub-id-type="pmid">8939866</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>İ</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Signal propagation and logic gating in networks of integrate-and-fire neurons</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>10786</fpage><lpage>10795</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3508-05.2005</pub-id><pub-id pub-id-type="pmid">16291952</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Gating multiple signals through detailed balance of excitation and inhibition in spiking networks</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>483</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1038/nn.2276</pub-id><pub-id pub-id-type="pmid">19305402</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Imaging retinotopic maps in the human brain</article-title><source>Vision Research</source><volume>51</volume><fpage>718</fpage><lpage>737</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2010.08.004</pub-id><pub-id pub-id-type="pmid">20692278</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>K-F</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A recurrent network mechanism of time integration in perceptual decisions</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>1314</fpage><lpage>1328</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3733-05.2006</pub-id><pub-id pub-id-type="pmid">16436619</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zajzon</surname><given-names>B</given-names></name><name><surname>Mahmoudian</surname><given-names>S</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name><name><surname>Duarte</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Passing the message: representation transfer in modular balanced networks</article-title><source>Frontiers in Computational Neuroscience</source><volume>13</volume><elocation-id>79</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2019.00079</pub-id><pub-id pub-id-type="pmid">31920605</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Robust information propagation through noisy neural circuits</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005497</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005497</pub-id><pub-id pub-id-type="pmid">28419098</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Constraints on feedforward connectivity</title><p>This section expands on the limitations arising from the definitions of topographic modularity and map sizes used in this study. By imposing a fixed connection density on the feedforward connection matrices, the projection probabilities between neurons tuned to the same <inline-formula><mml:math id="inf406"><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msub><mml:mi mathsize="90%">p</mml:mi><mml:mi mathsize="90%" mathvariant="normal">c</mml:mi></mml:msub><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:math></inline-formula> and different <inline-formula><mml:math id="inf407"><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msub><mml:mi mathsize="90%">p</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:math></inline-formula> stimuli are uniquely determined by the modularity <inline-formula><mml:math id="inf408"><mml:mi mathsize="90%">m</mml:mi></mml:math></inline-formula> and the parameter <inline-formula><mml:math id="inf409"><mml:msub><mml:mi mathsize="90%">d</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf410"><mml:mi mathsize="90%">δ</mml:mi></mml:math></inline-formula>, which control the size of stimulus-specific sub-populations (see Materials and methods). For notational simplicity, here we consider the merged excitatory and inhibitory sub-populations tuned to a particular stimulus in a given sub-network <inline-formula><mml:math id="inf411"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, with a total size <inline-formula><mml:math id="inf412"><mml:mrow><mml:msub><mml:mi mathsize="90%">C</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:msubsup><mml:mi mathsize="90%">C</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi><mml:mi mathsize="90%" mathvariant="normal">E</mml:mi></mml:msubsup><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:msubsup><mml:mi mathsize="90%">C</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi><mml:mi mathsize="90%" mathvariant="normal">I</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Under the constraints applied in this work, the total density of a feedforward adjacency matrix between <inline-formula><mml:math id="inf413"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf414"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> can be computed as:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:msub><mml:mi mathsize="90%">σ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="90%">p</mml:mi><mml:mi mathsize="90%" mathvariant="normal">c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathsize="90%">U</mml:mi><mml:mi mathsize="90%" mathvariant="normal">c</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msubsup></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%">p</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathsize="90%">U</mml:mi><mml:mn mathsize="90%">0</mml:mn><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:msup><mml:mi mathsize="90%">N</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf415"><mml:msubsup><mml:mi mathsize="90%">U</mml:mi><mml:mn mathsize="90%">0</mml:mn><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf416"><mml:msubsup><mml:mi mathsize="90%">U</mml:mi><mml:mi mathsize="90%" mathvariant="normal">c</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msubsup></mml:math></inline-formula> are the number of <italic>realizable</italic> connections between similarly and differently tuned sub-populations, respectively. Since <inline-formula><mml:math id="inf417"><mml:mrow><mml:msubsup><mml:mi mathsize="90%">U</mml:mi><mml:mi mathsize="90%" mathvariant="normal">c</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msubsup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="90%">N</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msup><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:msubsup><mml:mi mathsize="90%">U</mml:mi><mml:mn mathsize="90%">0</mml:mn><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, we can simplify the notation and focus only on <inline-formula><mml:math id="inf418"><mml:msubsup><mml:mi mathsize="90%">U</mml:mi><mml:mn mathsize="90%">0</mml:mn><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msubsup></mml:math></inline-formula>. We distinguish between the cases of non-overlapping and overlapping stimulus-specific sub-populations:<disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>if </mml:mtext></mml:mstyle><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>if </mml:mtext></mml:mstyle><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where each potential synapse is counted only once, regardless of whether the involved neurons belong to any or multiple overlapping sub-populations. This ensures consistency with the definitions of the probabilities <inline-formula><mml:math id="inf419"><mml:msub><mml:mi mathsize="90%">p</mml:mi><mml:mi mathsize="90%" mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf420"><mml:msub><mml:mi mathsize="90%">p</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub></mml:math></inline-formula>. Alternatively, we can express <inline-formula><mml:math id="inf421"><mml:msubsup><mml:mi mathsize="90%">U</mml:mi><mml:mn mathsize="90%">0</mml:mn><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msubsup></mml:math></inline-formula> as:<disp-formula id="equ14"><mml:math id="m14"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi mathsize="90%">U</mml:mi><mml:mn mathsize="90%">0</mml:mn><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msubsup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:msup><mml:mi mathsize="90%">N</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%">stim</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%">stim</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="90%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">δ</mml:mi></mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:msub><mml:mi mathsize="90%">d</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mi mathsize="90%">i</mml:mi><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">δ</mml:mi></mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:msub><mml:mi mathsize="90%">d</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>For the case with no overlap, we can derive an additional constraint on the minimum sub-populations size <inline-formula><mml:math id="inf422"><mml:msub><mml:mi mathsize="90%">C</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula> for the required density <inline-formula><mml:math id="inf423"><mml:msub><mml:mi mathsize="90%">σ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula> to be satisfied, which we define in relation to the total number of sub-populations <inline-formula><mml:math id="inf424"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ15"><label>(13)</label><mml:math id="m15"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msqrt><mml:mfrac><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>The equality holds in the case of <inline-formula><mml:math id="inf425"><mml:mrow><mml:mi mathsize="90%">m</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:math></inline-formula> and all-to-all feedforward connectivity between similarly tuned sub-populations, that is, <inline-formula><mml:math id="inf426"><mml:mrow><mml:msub><mml:mi mathsize="90%">p</mml:mi><mml:mi mathsize="90%" mathvariant="normal">c</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>Mean-field analysis of network dynamics</title><p>For an analytical investigation of the role of topographic modularity on the network dynamics, we used mean-field theory (<xref ref-type="bibr" rid="bib22">Fourcaud and Brunel, 2002</xref>; <xref ref-type="bibr" rid="bib28">Helias et al., 2013</xref>; <xref ref-type="bibr" rid="bib72">Schuecker et al., 2015</xref>). Under the assumptions that each neuron receives a large number of small amplitude inputs at every time step, the synaptic time constants <inline-formula><mml:math id="inf427"><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">s</mml:mi></mml:msub></mml:math></inline-formula> are small compared to the membrane time constant <inline-formula><mml:math id="inf428"><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula>, and that the network activity is sufficiently asynchronous and irregular, we can make use of theoretical results obtained from the diffusion approximation of the LIF neuron model to determine the stationary population dynamics. The equations in this section were partially solved using a modified version of the LIF Meanfield Tools library (<xref ref-type="bibr" rid="bib42">Layer et al., 2020</xref>).</p><sec sec-type="appendix" id="s9-1"><title>Stationary firing rates and fixed points</title><p>In the circumstances described above, the total synaptic input to each neuron can be replaced by a Gaussian white noise process (independent across neurons) with mean <inline-formula><mml:math id="inf429"><mml:mrow><mml:mi mathsize="90%">μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and variance <inline-formula><mml:math id="inf430"><mml:mrow><mml:msup><mml:mi mathsize="90%">σ</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">t</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In the stationary state, these quantities, along with the firing rates of each afferent, can be well approximated by their constant time average. The stationary firing rate of the LIF neuron in response to such input is:<disp-formula id="equ16"><label>(14)</label><mml:math id="m16"><mml:mrow><mml:mi mathsize="90%">ν</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mtext mathsize="90%">ref</mml:mtext></mml:msub><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:msqrt><mml:mi mathsize="90%">π</mml:mi></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mi mathsize="90%">eff</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" mathsize="90%" stretchy="false" symmetric="true">∫</mml:mo><mml:msub><mml:mi mathsize="90%">y</mml:mi><mml:mi mathsize="90%" mathvariant="normal">r</mml:mi></mml:msub><mml:msub><mml:mi mathsize="90%">y</mml:mi><mml:mi mathsize="90%">θ</mml:mi></mml:msub></mml:msubsup><mml:mrow><mml:mrow><mml:mi mathsize="90%">exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">u</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:mtext mathsize="90%">erf</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi mathsize="90%">u</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo mathsize="90%" rspace="0pt" stretchy="false">d</mml:mo><mml:mi mathsize="90%">u</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>where erf is the error function and the integration limits are defined as <inline-formula><mml:math id="inf431"><mml:mrow><mml:msub><mml:mi mathsize="90%">y</mml:mi><mml:mi mathsize="90%" mathvariant="normal">r</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%">V</mml:mi><mml:mi mathsize="90%">reset</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mi mathsize="90%">μ</mml:mi></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mo mathsize="90%" stretchy="false">/</mml:mo><mml:mi mathsize="90%">σ</mml:mi></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:mfrac><mml:mi mathsize="90%">q</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">s</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">/</mml:mo><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mi mathsize="90%">eff</mml:mi></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf432"><mml:mrow><mml:msub><mml:mi mathsize="90%">y</mml:mi><mml:mi mathsize="90%">θ</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mi mathsize="90%">θ</mml:mi><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mi mathsize="90%">μ</mml:mi></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mo mathsize="90%" stretchy="false">/</mml:mo><mml:mi mathsize="90%">σ</mml:mi></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:mfrac><mml:mi mathsize="90%">q</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">s</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">/</mml:mo><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mi mathsize="90%">eff</mml:mi></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf433"><mml:mrow><mml:mi mathsize="90%">q</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:msqrt><mml:mn mathsize="90%">2</mml:mn></mml:msqrt><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">|</mml:mo><mml:mrow><mml:mi mathsize="90%">ζ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">/</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo maxsize="90%" minsize="90%">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and Riemann zeta function <inline-formula><mml:math id="inf434"><mml:mi mathsize="90%">ζ</mml:mi></mml:math></inline-formula> (see <xref ref-type="bibr" rid="bib22">Fourcaud and Brunel, 2002</xref>, Eq. 4.33). As we will see below, the mean μ and variance <inline-formula><mml:math id="inf435"><mml:msup><mml:mi mathsize="90%">σ</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msup></mml:math></inline-formula> of the input also depend on the stationary firing rate <inline-formula><mml:math id="inf436"><mml:mi mathsize="90%">ν</mml:mi></mml:math></inline-formula>, rendering <xref ref-type="disp-formula" rid="equ16">Equation 14</xref> an implicit equation that needs to be solved self-consistently using fixed-point iteration.</p><p>For simplicity, throughout the mean-field analyses we consider perfectly partitioned networks where each neuron belongs to exactly one topographic map, that is, to one of the <inline-formula><mml:math id="inf437"><mml:msub><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%" mathvariant="normal">C</mml:mi></mml:msub></mml:math></inline-formula> stimulus-specific, identically sized sub-populations <italic>SP</italic> (no overlap condition). We denote the firing rate of a neuron in the currently stimulated SP (receiving stimulus input in <inline-formula><mml:math id="inf438"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub></mml:math></inline-formula>) in sub-network <inline-formula><mml:math id="inf439"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula> by <inline-formula><mml:math id="inf440"><mml:msubsup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msubsup></mml:math></inline-formula>, and by <inline-formula><mml:math id="inf441"><mml:msubsup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msubsup></mml:math></inline-formula> that of neurons not associated with the stimulated pathway. Since the firing rates of excitatory and inhibitory neurons are equal (due to identical synaptic time constants and input statistics), we can write the constant mean synaptic input to neurons in the input sub-network as<disp-formula id="equ17"><label>(15)</label><mml:math id="m17"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>rec. stimulated</mml:mtext></mml:mrow></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>rec. non-stimulated</mml:mtext></mml:mrow></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>stimulus</mml:mtext></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>rec. stimulated</mml:mtext></mml:mrow></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>rec. non-stimulated</mml:mtext></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The variances <inline-formula><mml:math id="inf442"><mml:msup><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msubsup><mml:mi mathsize="90%">σ</mml:mi><mml:mn mathsize="90%">0</mml:mn><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msubsup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mn mathsize="90%">2</mml:mn></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf443"><mml:msup><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msubsup><mml:mi mathsize="90%">σ</mml:mi><mml:mn mathsize="90%">0</mml:mn><mml:mi mathsize="90%">NS</mml:mi></mml:msubsup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mn mathsize="90%">2</mml:mn></mml:msup></mml:math></inline-formula> can be obtained by squaring each weight <inline-formula><mml:math id="inf444"><mml:mi mathsize="90%">J</mml:mi></mml:math></inline-formula> in the above equation. To derive these equations for the deeper sub-networks <inline-formula><mml:math id="inf445"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, it is helpful to include auxiliary variables <inline-formula><mml:math id="inf446"><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf447"><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msub></mml:math></inline-formula>, representing the number of feedforward inputs to a neuron in <inline-formula><mml:math id="inf448"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi></mml:msub></mml:math></inline-formula> from its own SP in <inline-formula><mml:math id="inf449"><mml:msub><mml:mi mathsize="90%">SSN</mml:mi><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">i</mml:mi><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, and from one different SP (there are <inline-formula><mml:math id="inf450"><mml:mrow><mml:msub><mml:mi mathsize="90%">N</mml:mi><mml:mi mathsize="90%" mathvariant="normal">C</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:math></inline-formula> such sub-populations), respectively. Both <inline-formula><mml:math id="inf451"><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf452"><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msub></mml:math></inline-formula> are uniquely defined by the modularity <inline-formula><mml:math id="inf453"><mml:mi mathsize="90%">m</mml:mi></mml:math></inline-formula> and projection density <inline-formula><mml:math id="inf454"><mml:mi mathsize="90%">d</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf455"><mml:mrow><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mi mathsize="90%">m</mml:mi></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msub></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mi mathsize="90%">m</mml:mi></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mi mathsize="90%">α</mml:mi></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">E</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> holds as well. The mean synaptic inputs to the neurons in the deeper sub-networks can thus be written as:<disp-formula id="equ18"><label>(16)</label><mml:math id="m18"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>rec. stimulated</mml:mtext></mml:mrow></mml:mover></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>rec. non-stimulated</mml:mtext></mml:mrow></mml:mover></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>stimulated FF</mml:mtext></mml:mrow></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>non-stimulated FF</mml:mtext></mml:mrow></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>rec. stimulated</mml:mtext></mml:mrow></mml:mover></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>rec. non-stimulated</mml:mtext></mml:mrow></mml:mover></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Again, one can obtain the variances by squaring each weight <inline-formula><mml:math id="inf456"><mml:mi mathsize="90%">J</mml:mi></mml:math></inline-formula>. The stationary firing rates for the stimulated and non-stimulated sub-populations in all sub-networks are then found by first solving <xref ref-type="disp-formula" rid="equ16 equ17">Equations 14 and 15</xref> for the first sub-network and then (<xref ref-type="disp-formula" rid="equ18">Equation 16</xref>) <xref ref-type="disp-formula" rid="equ16 equ18">Equations 14 and 16</xref> successively for deeper sub-networks.</p><p>For very deep networks, one can ask the question, whether firing rates approach fixed points across sub-networks. If there are multiple fixed points, the initial condition, that is the externally stimulated activity of sub-populations in the first sub-network, decides in which of the fixed points the rates evolve, in a similar spirit as in recurrent networks after a start-up transient. For a fixed point, we have <inline-formula><mml:math id="inf457"><mml:mrow><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mrow><mml:mi mathsize="90%">i</mml:mi><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. In effect, we can re-group terms in <xref ref-type="disp-formula" rid="equ18">Equation 16</xref> that have the same rates such that formally we obtain an effective new group of neurons from the excitatory and inhibitory SPs of the current sub-network and the corresponding excitatory SPs of the previous sub-network, as indicated by the square brackets in the following formulas:<disp-formula id="equ19"><label>(17)</label><mml:math id="m19"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi>β</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ20"><label>(18)</label><mml:math id="m20"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi>β</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf458"><mml:mrow><mml:mi mathsize="90%">β</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">X</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">/</mml:mo><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">E</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf459"><mml:mrow><mml:mi mathsize="90%">γ</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">I</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">/</mml:mo><mml:msub><mml:mi mathsize="90%">K</mml:mi><mml:mi mathsize="90%" mathvariant="normal">E</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf460"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>τ</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi>J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>For the parameters <inline-formula><mml:math id="inf461"><mml:mi mathsize="90%">g</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf462"><mml:mi mathsize="90%">γ</mml:mi></mml:math></inline-formula> chosen here, <inline-formula><mml:math id="inf463"><mml:msub><mml:mi mathsize="90%">κ</mml:mi><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%">NS</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf464"><mml:msub><mml:mi mathsize="90%">κ</mml:mi><mml:mrow><mml:mi mathsize="90%">NS</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf465"><mml:msub><mml:mi mathsize="90%">κ</mml:mi><mml:mrow><mml:mi mathsize="90%">NS</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%">NS</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ19 equ20">Equations 17 and 18</xref> are always negative for any modularity <inline-formula><mml:math id="inf466"><mml:mi mathsize="90%">m</mml:mi></mml:math></inline-formula> due to the large recurrent inhibition. Therefore, for the non-stimulated group, <inline-formula><mml:math id="inf467"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> (see main text), such that one always finds a single fixed point, which, as desired, is at a low rate. Interestingly, the excitatory feedforward connections can switch the sign of <inline-formula><mml:math id="inf468"><mml:msub><mml:mi mathsize="90%">κ</mml:mi><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> from negative to positive for large values of <inline-formula><mml:math id="inf469"><mml:mi mathsize="90%">m</mml:mi></mml:math></inline-formula>, thereby rendering the active group effectively excitatory, leading to a saddle-node bifurcation and the emergence of a stable high-activity fixed point (see <xref ref-type="fig" rid="fig7">Figure 7b</xref> in the main text).</p><p>The structure of fixed points can also be understood by studying the potential landscape of the system: <xref ref-type="disp-formula" rid="equ16">Equation 14</xref> can be regarded as the fixed-point solution of the following evolution equations for the stimulated and non-stimulated sub-populations (<xref ref-type="bibr" rid="bib92">Wong and Wang, 2006</xref>; <xref ref-type="bibr" rid="bib73">Schuecker et al., 2017</xref>)<disp-formula id="equ21"><label>(19)</label><mml:math id="m21"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%" mathvariant="normal">Φ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%" rspace="4.2pt">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">,</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ22"><label>(20)</label><mml:math id="m22"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%" mathvariant="normal">Φ</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%" rspace="4.2pt">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">,</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf470"><mml:msub><mml:mi mathsize="90%" mathvariant="normal">Φ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf471"><mml:msub><mml:mi mathsize="90%" mathvariant="normal">Φ</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msub></mml:math></inline-formula> are defined via the right-hand side of <xref ref-type="disp-formula" rid="equ16">Equation 14</xref> with <inline-formula><mml:math id="inf472"><mml:msup><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf473"><mml:msup><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup></mml:math></inline-formula> inserted as defined in <xref ref-type="disp-formula" rid="equ19 equ20">Equations 17 and 18</xref> (and likewise for <inline-formula><mml:math id="inf474"><mml:msup><mml:mi mathsize="90%">σ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf475"><mml:msup><mml:mi mathsize="90%">σ</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup></mml:math></inline-formula>). Due to the asymmetry in connections between stimulated and non-stimulated sub-populations, the right-hand side of <xref ref-type="disp-formula" rid="equ21 equ22">Equations 19 and 20</xref> cannot be interpreted as a conservative force. Following the idea of effective response functions (<xref ref-type="bibr" rid="bib48">Mascaro and Amit, 1999</xref>), a potential <inline-formula><mml:math id="inf476"><mml:mrow><mml:mi mathsize="90%">U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the stimulated sub-population alone can, however, be defined by inserting the solution <inline-formula><mml:math id="inf477"><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mi mathsize="90%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> of <xref ref-type="disp-formula" rid="equ22">Equation 20</xref> into <xref ref-type="disp-formula" rid="equ21">Equation 19</xref><disp-formula id="equ23"><label>(21)</label><mml:math id="m23"><mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%" mathvariant="normal">Φ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="90%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>and interpreting the right-hand side as a conservative force <inline-formula><mml:math id="inf478"><mml:mrow><mml:mi mathsize="90%">F</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mfrac><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">U</mml:mi></mml:mrow><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib44">Litwin-Kumar and Doiron, 2012</xref>). The potential then follows from integration as<disp-formula id="equ24"><label>(22)</label><mml:math id="m24"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathsize="90%">U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="90%">U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mn mathsize="90%">0</mml:mn><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mn mathsize="90%">1</mml:mn><mml:mn mathsize="90%">2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mn mathsize="90%">2</mml:mn></mml:msup></mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" mathsize="90%" stretchy="false" symmetric="true">∫</mml:mo><mml:mn mathsize="90%">0</mml:mn><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:msubsup><mml:mrow><mml:msub><mml:mi mathsize="90%" mathvariant="normal">Φ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">ν</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="90%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">ν</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo mathsize="90%" rspace="0pt" stretchy="false">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi mathsize="90%">ν</mml:mi></mml:mpadded></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf479"><mml:mrow><mml:mi mathsize="90%">U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mn mathsize="90%">0</mml:mn><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is an inconsequential constant. We solved the latter integral numerically using the scipy.integrate.trapz function of SciPy (<xref ref-type="bibr" rid="bib88">Virtanen et al., 2020</xref>). The minima and maxima of the resulting potential correspond to locally stable and unstable fixed points, respectively. Note that while this single-population potential is useful to study the structure of fixed points, the full dynamics of all populations and global stability cannot be straight-forwardly infered from this reduced picture (<xref ref-type="bibr" rid="bib48">Mascaro and Amit, 1999</xref>; <xref ref-type="bibr" rid="bib69">Rost et al., 2018</xref>), here for two reasons: (1) For spiking networks, <xref ref-type="disp-formula" rid="equ21">Equation 19</xref> and <xref ref-type="disp-formula" rid="equ22">Equation 20</xref> do not describe the real dynamics of the mean activity. Their right-hand side only defines the stationary state solution. (2) The global stability of fixed points also depends on the time constants of all sub-populations’ mean activities (here <inline-formula><mml:math id="inf480"><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf481"><mml:msub><mml:mi mathsize="90%">τ</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msub></mml:math></inline-formula>), but the temporal dynamics of the non-stimulated sub-populations is neglected here.</p></sec><sec sec-type="appendix" id="s9-2"><title>Mean-field analysis for two input streams</title><p>In the case of two simultaneously active stimuli (see Section ‘Input integration and multi-stability’), if the stimulated group 1 is in the high-activity state with rate <inline-formula><mml:math id="inf482"><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S1</mml:mi></mml:msup></mml:math></inline-formula>, the second stimulated group 2 will receive an additional non-vanishing input of the form<disp-formula id="equ25"><label>(23)</label><mml:math id="m25"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>which is negative for all values of <inline-formula><mml:math id="inf483"><mml:mi mathsize="90%">m</mml:mi></mml:math></inline-formula> and can therefore lead to the silencing of group 2. If the stimuli are similarly strong, network fluctuations can dynamically switch the roles of the stimulated groups 1 and 2.</p><p>The dynamics and fixed-point structure in deep sub-networks can be studied using a two-dimensional potential landscape that is defined via the following evolution equation<disp-formula id="equ26"><label>(24)</label><mml:math id="m26"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">ϕ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ27"><label>(25)</label><mml:math id="m27"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">ϕ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf484"><mml:mrow><mml:mrow><mml:mi mathsize="90%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S1</mml:mi></mml:msup><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S2</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the fixed point of the non-stimulated sub-populations for given rates <inline-formula><mml:math id="inf485"><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S1</mml:mi></mml:msup><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S2</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> of the two stimulated sub-populations, respectively. The functions <inline-formula><mml:math id="inf486"><mml:msub><mml:mi mathsize="90%" mathvariant="normal">Φ</mml:mi><mml:mi mathsize="90%">S1</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf487"><mml:msub><mml:mi mathsize="90%" mathvariant="normal">Φ</mml:mi><mml:mi mathsize="90%">S2</mml:mi></mml:msub></mml:math></inline-formula> are again defined via the right-hand side of <xref ref-type="disp-formula" rid="equ16">Equation 14</xref> with inserted <inline-formula><mml:math id="inf488"><mml:msup><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">S1</mml:mi></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf489"><mml:msup><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">S2</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf490"><mml:msup><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup></mml:math></inline-formula> that are defined as follows (derivation analogous to the single-input case):<disp-formula id="equ28"><label>(26)</label><mml:math id="m28"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ29"><label>(27)</label><mml:math id="m29"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ30"><label>(28)</label><mml:math id="m30"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ31"><label>(29)</label><mml:math id="m31"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Due to the symmetry between the two stimulated sub-populations, the right-hand side of <xref ref-type="disp-formula" rid="equ26 equ27">Equations 24 and 25</xref> can be viewed as a conservative force <inline-formula><mml:math id="inf491"><mml:mi mathsize="90%" mathvariant="bold-italic">F</mml:mi></mml:math></inline-formula> of the potential <inline-formula><mml:math id="inf492"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="script">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where we parameterized the line integral along the path <inline-formula><mml:math id="inf493"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>:</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mi mathvariant="script">C</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">↦</mml:mo><mml:mi>t</mml:mi><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, which yields<disp-formula id="equ32"><label>(30)</label><mml:math id="m32"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathsize="90%">U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S1</mml:mi></mml:msup><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S2</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mn mathsize="90%">1</mml:mn><mml:mn mathsize="90%">2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S1</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mn mathsize="90%">2</mml:mn></mml:msup></mml:mrow><mml:mo mathsize="90%" stretchy="false">+</mml:mo><mml:mrow><mml:mfrac><mml:mn mathsize="90%">1</mml:mn><mml:mn mathsize="90%">2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S2</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mn mathsize="90%">2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" mathsize="90%" stretchy="false" symmetric="true">∫</mml:mo><mml:mn mathsize="90%">0</mml:mn><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S1</mml:mi></mml:msup></mml:msubsup><mml:mrow><mml:msub><mml:mi mathsize="90%" mathvariant="normal">Φ</mml:mi><mml:mi mathsize="90%">S1</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi mathsize="90%">ν</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="90%">ν</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S2</mml:mi></mml:msup><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S1</mml:mi></mml:msup></mml:mfrac></mml:mrow><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="90%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">ν</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="90%">ν</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S2</mml:mi></mml:msup><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S1</mml:mi></mml:msup></mml:mfrac></mml:mrow><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" mathsize="90%" stretchy="false" symmetric="true">∫</mml:mo><mml:mn mathsize="90%">0</mml:mn><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S2</mml:mi></mml:msup></mml:msubsup><mml:mrow><mml:msub><mml:mi mathsize="90%" mathvariant="normal">Φ</mml:mi><mml:mi mathsize="90%">S2</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathsize="90%">ν</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S1</mml:mi></mml:msup><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S2</mml:mi></mml:msup></mml:mfrac></mml:mrow><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%">ν</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="90%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mrow><mml:mi mathsize="90%">ν</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S1</mml:mi></mml:msup><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S2</mml:mi></mml:msup></mml:mfrac></mml:mrow><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%">ν</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="4.2pt">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The numerical evaluation of this two-dimensional potential is shown in <xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>, whereas sketches in <xref ref-type="fig" rid="fig9">Figure 9e</xref> show a one-dimensional section (gray lines in <xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>) that goes anti-diagonal through the two minima corresponding to one population being in the high-activity state and the other one being in the low-activity state.</p></sec><sec sec-type="appendix" id="s9-3"><title>Critical modularity for piecewise linear activation function</title><p>To obtain a closed-form analytic solution for the critical modularity, in the following we consider a neuron model with piecewise linear activation function<disp-formula id="equ33"><label>(31)</label><mml:math id="m33"><mml:mrow><mml:mrow><mml:mi mathsize="90%">ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">μ</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">max</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi mathsize="90%">μ</mml:mi><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:msub><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">min</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">max</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:msub><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">min</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>for <inline-formula><mml:math id="inf494"><mml:mrow><mml:mi mathsize="90%">μ</mml:mi><mml:mo mathsize="90%" stretchy="false">∈</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">[</mml:mo><mml:msub><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">min</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:msub><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">max</mml:mi></mml:msub><mml:mo maxsize="90%" minsize="90%">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf495"><mml:mrow><mml:mrow><mml:mi mathsize="90%">ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">μ</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf496"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf497"><mml:mrow><mml:mrow><mml:mi mathsize="90%">ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">μ</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">max</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf498"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig8">Figure 8a</xref>). Successful denoising requires the non-stimulated sub-populations to be silent, <inline-formula><mml:math id="inf499"><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mrow><mml:mi mathsize="90%">N</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">S</mml:mi></mml:mrow></mml:msup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0</mml:mn></mml:mrow></mml:math></inline-formula>, and the stimulated sub-populations to be active, <inline-formula><mml:math id="inf500"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. We first study solutions where <inline-formula><mml:math id="inf501"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msup><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and afterwards the case where <inline-formula><mml:math id="inf502"><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S</mml:mi></mml:msup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">max</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Inserting <xref ref-type="disp-formula" rid="equ33">Equation 31</xref> into <xref ref-type="disp-formula" rid="equ9 equ10">Equations 9 and 10</xref>, we obtain<disp-formula id="equ34"><mml:math id="m34"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The first equation can be solved for <inline-formula><mml:math id="inf503"><mml:msup><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:math></inline-formula><disp-formula id="equ35"><label>(32)</label><mml:math id="m35"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mfrac><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mfrac><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>which holds for<disp-formula id="equ36"><label>(33)</label><mml:math id="m36"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>≤</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ37"><label>(34)</label><mml:math id="m37"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>≤</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Requirement (<xref ref-type="disp-formula" rid="equ36">Equation 33</xref>) is equivalent to an inequality for <inline-formula><mml:math id="inf504"><mml:mi mathsize="90%">m</mml:mi></mml:math></inline-formula><disp-formula id="equ38"><mml:math id="m38"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>that, depending on the dynamic range of the neuron, the strength of the external background input and the recurrence, yields<disp-formula id="equ39"><label>(35)</label><mml:math id="m39"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>as an upper or lower bound for the modularity (<xref ref-type="fig" rid="fig8">Figure 8</xref>). Requirement (<xref ref-type="disp-formula" rid="equ37">Equation 34</xref>) with the solution (<xref ref-type="disp-formula" rid="equ35">Equation 32</xref>) for <inline-formula><mml:math id="inf505"><mml:msup><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup></mml:math></inline-formula> inserted yields a further lower bound<disp-formula id="equ40"><label>(36)</label><mml:math id="m40"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≥</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>for the modularity that is required for denoising. This criterion is independent of the external background input and the recurrence of the SSN.</p><p>Now we turn to the saturated scenario <inline-formula><mml:math id="inf506"><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S</mml:mi></mml:msup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">max</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf507"><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mrow><mml:mi mathsize="90%">N</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">S</mml:mi></mml:mrow></mml:msup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0</mml:mn></mml:mrow></mml:math></inline-formula> and obtain<disp-formula id="equ41"><mml:math id="m41"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>with the criteria<disp-formula id="equ42"><label>(37)</label><mml:math id="m42"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>≥</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ43"><label>(38)</label><mml:math id="m43"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>≤</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The first criterion (<xref ref-type="disp-formula" rid="equ42">Equation 37</xref>) yields the same critical value (<xref ref-type="disp-formula" rid="equ39">Equation 35</xref>) that for <inline-formula><mml:math id="inf508"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is a lower bound and otherwise an upper bound. The second criterion (<xref ref-type="disp-formula" rid="equ43">Equation 38</xref>) yields an additional lower bound for <inline-formula><mml:math id="inf509"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig8">Figure 8</xref>):<disp-formula id="equ44"><label>(39)</label><mml:math id="m44"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The above criteria yield necessary conditions for the existence of a fixed point with <inline-formula><mml:math id="inf510"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf511"><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mrow><mml:mi mathsize="90%">N</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">S</mml:mi></mml:mrow></mml:msup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0</mml:mn></mml:mrow></mml:math></inline-formula>. Next we study the stability of such solutions. This works analogous to the stability in the spiking models discussed in Section ‘Effective connectivity and stability analysis’ by studying the spectrum of the effective connectivity matrix. For the model <xref ref-type="disp-formula" rid="equ33">Equation 31</xref>, the effective connectivity is given by<disp-formula id="equ45"><label>(40)</label><mml:math id="m45"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf512"><mml:mrow><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mo mathsize="90%" stretchy="false">′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">μ</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">ν</mml:mi></mml:mrow><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">μ</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">μ</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf513"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. On the level of stimulated and non-stimulated sub-populations across layers, the effective connectivity becomes<disp-formula id="equ46"><label>(41)</label><mml:math id="m46"><mml:mrow><mml:mi mathsize="90%">W</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi mathsize="90%">κ</mml:mi><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">m</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mo mathsize="90%" stretchy="false">′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi mathsize="90%">κ</mml:mi><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%">NS</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">m</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mo mathsize="90%" stretchy="false">′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi mathsize="90%">κ</mml:mi><mml:mrow><mml:mi mathsize="90%">NS</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">m</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mo mathsize="90%" stretchy="false">′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi mathsize="90%">κ</mml:mi><mml:mrow><mml:mi mathsize="90%">NS</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%">NS</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">m</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mo mathsize="90%" stretchy="false">′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with eigenvalues<disp-formula id="equ47"><label>(42)</label><mml:math id="m47"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>±</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>ν</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The saturated fixed point <inline-formula><mml:math id="inf514"><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S</mml:mi></mml:msup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">max</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf515"><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mrow><mml:mi mathsize="90%">N</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="90%">S</mml:mi></mml:mrow></mml:msup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0</mml:mn></mml:mrow></mml:math></inline-formula> has <inline-formula><mml:math id="inf516"><mml:mrow><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mo mathsize="90%" stretchy="false">′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mo mathsize="90%" stretchy="false">′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0</mml:mn></mml:mrow></mml:math></inline-formula>, leading to <inline-formula><mml:math id="inf517"><mml:mrow><mml:msub><mml:mi mathsize="90%">λ</mml:mi><mml:mo mathsize="90%" stretchy="false">±</mml:mo></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0</mml:mn></mml:mrow></mml:math></inline-formula>. This fixed point is always stable. The non-saturated fixed point also has <inline-formula><mml:math id="inf518"><mml:mrow><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mo mathsize="90%" stretchy="false">′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:msup><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">NS</mml:mi></mml:msup><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0</mml:mn></mml:mrow></mml:math></inline-formula>. Consequently, <xref ref-type="disp-formula" rid="equ47">Equation 42</xref> simplifies to <inline-formula><mml:math id="inf519"><mml:mrow><mml:msub><mml:mi mathsize="90%">λ</mml:mi><mml:mo mathsize="90%" stretchy="false">-</mml:mo></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0</mml:mn></mml:mrow></mml:math></inline-formula> and<disp-formula id="equ48"><label>(43)</label><mml:math id="m48"><mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="90%">λ</mml:mi><mml:mo mathsize="90%" stretchy="false">+</mml:mo></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">max</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">max</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:msub><mml:mi mathsize="90%">μ</mml:mi><mml:mi mathsize="90%">min</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">κ</mml:mi><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">m</mml:mi><mml:mo maxsize="90%" minsize="90%" rspace="4.2pt">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For <inline-formula><mml:math id="inf520"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> fluctuations in the stimulated sub-population are being amplified. These fluctuations also drive fluctuations of the non-stimulated sub-population via the recurrent coupling. The fixed point thus becomes unstable and the necessary distinction between the stimulated and non-stimulated sub-populations vanishes. For inhibition-dominated recurrence, <inline-formula><mml:math id="inf521"><mml:mrow><mml:msub><mml:mi mathsize="90%">κ</mml:mi><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">m</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is small enough to obtain stable fixed points at non-saturated rates (<xref ref-type="fig" rid="fig8">Figure 8c</xref>). In the case of no recurrence or excitation-dominated recurrence, <inline-formula><mml:math id="inf522"><mml:mrow><mml:msub><mml:mi mathsize="90%">κ</mml:mi><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%" mathvariant="normal">S</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="90%" minsize="90%">(</mml:mo><mml:mi mathsize="90%">m</mml:mi><mml:mo maxsize="90%" minsize="90%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is much larger, typically driving <inline-formula><mml:math id="inf523"><mml:msub><mml:mi mathsize="90%">λ</mml:mi><mml:mo mathsize="90%" stretchy="false">+</mml:mo></mml:msub></mml:math></inline-formula> across the line of instability and preventing non-saturated fixed points to be stable. In such networks, only the saturated fixed point at <inline-formula><mml:math id="inf524"><mml:mrow><mml:msup><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">S</mml:mi></mml:msup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msub><mml:mi mathsize="90%">ν</mml:mi><mml:mi mathsize="90%">max</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is stable and reachable (<xref ref-type="fig" rid="fig8">Figure 8d and e</xref>).</p></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.77009.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xez1567</institution-id><institution>Salk Institute for Biological Studies</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.01.10.475681" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.01.10.475681"/></front-stub><body><p>This manuscript puts forward a new idea that topography in neural networks helps to remove noise from inputs. The authors show that there is a critical level of topography that is needed for network to denoise inputs.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.77009.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xez1567</institution-id><institution>Salk Institute for Biological Studies</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.01.10.475681">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.01.10.475681v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Signal denoising through topographic modularity of neural circuits&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Joshua Gold as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>To increase the impact of this work, it is necessary to</p><p>1) Clarify the properties of the critical point (see comments from Reviewer 1).</p><p>2) Consider how denoising could work for dynamic inputs (comments raised by both Reviewers).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>As I have stated in the main review section, my main issue with this work is that I fail to see the impact of the results. The authors provide a detailed analysis of a model for cortical connectivity with topographical connections. On the one side, they do not compare their analysis to neuronal recording or imaging, thus not providing evidence that their analysis of the dynamics is correct and justifies the model. On the other hand, the model does not offer deep theoretical insights and focuses on simplistic computational tasks, denoising, which could be achieved in different ways.</p><p>In the following, I give some recommendations to the authors on making the work more meaningful and robust, in my opinion. First, I will address the bigger question of what could be done to increase the impact of the work. Then, I will address some technical issues and lack of coherence.</p><p>– I would have liked to see a theoretical derivation of the critical modularity level. For example, the authors can attempt to derive bifurcation curves in Figure 7 and show how they depend on different parameters in the system. The authors show that the single neuron dynamics do not affect the result but other connectivity parameters (e.g., the mean and variance of the different weight matrices). If the authors think that m=0.83 is a universal critical value, they should argue for it.</p><p>– One possible way to better understand the dynamics in the network and the role of the recurrent connectivity is to extend the mean-field analysis to the fluctuations around the fixed points of each population. For example, one concern is that while the mean activity would be low in inactive channels, noise fluctuations could still propagate.</p><p>– Can the results be compared to a more simplified feedforward network with topographic connectivity? Is the recurrent connectivity needed to explain and interpret the result? Is the inhibitory network required, or can similar effects be achieved with a subcritical excitatory recurrent population in each layer?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The manuscript presents an interesting and novel idea. My main suggestions for improvement pertain to the clarity of the presentation. In many cases the results are presented out-of-order and it is difficult to understand the authors point until reading the next paragraph.</p><p>For example, the critical value is mentioned in the first paragraph on page 4, but at that point it is not explained that there is a transition. On page 5, the discussion of Figure 2 returns back to Figure 1. It might be better to re-order the panels within figures to ensure continuous sequential description.</p><p>An important typo in Figure 2 legend &quot;For&quot; should be &quot;Four&quot;.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.77009.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>As I have stated in the main review section, my main issue with this work is that I fail to see the impact of the results. The authors provide a detailed analysis of a model for cortical connectivity with topographical connections. On the one side, they do not compare their analysis to neuronal recording or imaging, thus not providing evidence that their analysis of the dynamics is correct and justifies the model. On the other hand, the model does not offer deep theoretical insights and focuses on simplistic computational tasks, denoising, which could be achieved in different ways.</p><p>In the following, I give some recommendations to the authors on making the work more meaningful and robust, in my opinion. First, I will address the bigger question of what could be done to increase the impact of the work. Then, I will address some technical issues and lack of coherence.</p></disp-quote><p>We thank the reviewer for the constructive feedback and valuable comments. We believe that they helped us to improve the quality and impact of our study significantly.</p><disp-quote content-type="editor-comment"><p>– I would have liked to see a theoretical derivation of the critical modularity level. For example, the authors can attempt to derive bifurcation curves in Figure 7 and show how they depend on different parameters in the system. The authors show that the single neuron dynamics do not affect the result but other connectivity parameters (e.g., the mean and variance of the different weight matrices). If the authors think that m=0.83 is a universal critical value, they should argue for it.</p></disp-quote><p>We apologize for the misunderstanding that the single-neuron dynamics does not affect the critical value of the modularity. The conductance-based and current-based spiking model are very similar in their dynamic behavior, leading to a similar value for the transition/switching modularity (<italic>m</italic> = 0<italic>.</italic>83 vs <italic>m</italic> = 0<italic>.</italic>85). We adapted the text to clarify this difference (see lines 208-211 and 274-276), and provided a more in-depth analysis of the influence of single-neuron properties, recurrent connectivity and inhibition on the critical modularity (see comment above, section ”Critical modularity for denoising” and Appendix B). Furthermore, we apologize for not having made clear in the previous version that the bifurcation curves in Figure 7 were indeed theoretically derived. The mean-field self-consistent equations, however, had to be solved numerically due to the complicated nonlinear activation function of spiking neurons. In the revised manuscript, we derived fully analytic closed-form expressions for the critical modularity for a qualitatively similar but more tractable piecewise linear activation function. These expressions are helpful to understand the role of recurrence, external input and single-neuron properties for denoising, see new section ”Critical modularity for denoising”.</p><disp-quote content-type="editor-comment"><p>– One possible way to better understand the dynamics in the network and the role of the recurrent connectivity is to extend the mean-field analysis to the fluctuations around the fixed points of each population. For example, one concern is that while the mean activity would be low in inactive channels, noise fluctuations could still propagate.</p></disp-quote><p>We thank the reviewer for this great suggestion and added a fluctuation analysis for the fully analytically tractable piecewise linear neuron model (Figure 8). The analysis shows that recurrent inhibition stabilizes both the high activity fixed point of the stimulated sub-population and the low activity fixed point of the non-stimulated sub-populations, yielding robust denoising behavior. This feature is lost for networks without recurrent connections or excitation-dominated networks, see new section ”Critical modularity for denoising”.</p><disp-quote content-type="editor-comment"><p>– Can the results be compared to a more simplified feedforward network with topographic connectivity? Is the recurrent connectivity needed to explain and interpret the result? Is the inhibitory network required, or can similar effects be achieved with a subcritical excitatory recurrent population in each layer?</p></disp-quote><p>In the revised manuscript, we address these questions in the new Figure 8 and section ”Critical modularity for denoising”. The results show that for biologically plausible settings (excitatory background and feedforward input) recurrent inhibition is crucial for denoising. We also study the case of no recurrence (purely feedforward network) and excitation-dominated recurrent connectivity and theoretically predict that no denoising can be achieved assuming biological constraints. We further validate these predictions with numerical simulations of corresponding spiking networks.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>The manuscript presents an interesting and novel idea. My main suggestions for improvement pertain to the clarity of the presentation. In many cases the results are presented out-of-order and it is difficult to understand the authors point until reading the next paragraph.</p></disp-quote><p>We thank the reviewer for the helpful suggestions and apologize for the lack of clarity in our initial submission. We have taken the reviewer’s suggestions into consideration and hope the manuscript is now clearer and more understandable.</p><disp-quote content-type="editor-comment"><p>For example, the critical value is mentioned in the first paragraph on page 4, but at that point it is not explained that there is a transition.</p></disp-quote><p>Please note that in the revised submission, we differentiate more explicitly between the transition point <italic>m</italic><sub>switch</sub> (dependent on the input) and the critical modularity threshold <italic>m</italic><sub>crit</sub> (independent of input). As the reviewer points out, the critical modularity which, in fact, referred to the switching point <italic>m</italic><sub>switch</sub>, was first mentioned in the initial paragraph of page 4. In this same paragraph of the revised text, we make it explicit that this represents a qualitative transition in the behavior of the system: ”However, as <italic>m</italic> approaches a switching value <italic>m</italic><sub>switch</sub> ≈ 0<italic>.</italic>83, there is a qualitative transition in the system’s behavior, leading to a consistently higher reconstruction accuracy across the sub-networks</p><p>The role of <italic>m</italic><sub>switch</sub> as a transition point is further clarified in the beginning of the next paragraph:</p><p>“Beyond this transition point, reconstruction accuracy improves with depth”</p><p>We believe this makes it clear that the switching value <italic>m</italic><sub>switch</sub> represents a qualitative transition point, but if the reviewer disagrees, we would welcome suggestions on how to improve the clarity and readability of this section. For a detailed discussion on the related critical modularity threshold <italic>m</italic><sub>crit</sub>, see new section ”Critical modularity for denoising”.</p><disp-quote content-type="editor-comment"><p>On page 5, the discussion of Figure 2 returns back to Figure 1. It might be better to re-order the panels within figures to ensure continuous sequential description.</p></disp-quote><p>We thank the reviewer for pointing this out. It is indeed a good practice to ensure sequential description and make sure the different sections refer to their corresponding figures / panels in a sequential order. After a careful revision, we have concluded that the reference to Figure 1 does not break the sequentiality of the results’ presentation. Figure 1 presents the model setup and emphasizes task performance whereas Figure 2 focuses on network dynamics. We concluded this order of presentation is clearest and the most aligned with the main text. When presenting the results of Figure 2, it is important to relate the system’s dynamics to the computational performance instead of treating them in isolation. For that reason, the reference to ”the zero-gain convergence point in Figure 1d, g” is pertinent. We do not believe this presentation of the results breaks the flow or constitutes a discontinuity, but if the reviewer disagrees, we welcome suggestions on how this could be improved.</p><disp-quote content-type="editor-comment"><p>An important typo in Figure 2 legend &quot;For&quot; should be &quot;Four&quot;.</p></disp-quote><p>We apologize if we misunderstood the reviewer, but we could not find any typo or discrepancy. We believe the reviewer refers to the following sentence:</p><p>”For modularity values facilitating an asynchronous irregular regime across the network, ”</p><p>The use of ”For” here is correct, and refers to all the modularity values depicted in panel (c) which facilitate an asynchronous irregular activity. We verified all other occurrences of ”for” and found no errors (none of the panels contain or illustrate exactly four values / elements). If we overlooked something, we would kindly ask the reviewer to direct our attention to what exactly is considered a typo in this legend.</p><p>References</p><p>Massimo Mascaro and Daniel J Amit. Effective neural response function for collective population states. <italic>Network: Computation in Neural Systems</italic>, 10(4):351–373, 1999.</p><p>Thomas Rost, Moritz Deger, and Martin P Nawrot. Winnerless competition in clustered balanced networks: inhibitory assemblies do the trick. <italic>Biological cybernetics</italic>, 112(1):81–98, 2018.</p></body></sub-article></article>