<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">70925</article-id><article-id pub-id-type="doi">10.7554/eLife.70925</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Spatial tuning of face part representations within face-selective areas revealed by high-field fMRI</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-243439"><name><surname>Zhang</surname><given-names>Jiedong</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4432-2752</contrib-id><email>zhangjiedong@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-243440"><name><surname>Jiang</surname><given-names>Yong</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-243441"><name><surname>Song</surname><given-names>Yunjie</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-158623"><name><surname>Zhang</surname><given-names>Peng</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-39918"><name><surname>He</surname><given-names>Sheng</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5547-923X</contrib-id><email>hes@ibp.ac.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution>Institute of Biophysics, Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution>University of Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution>Department of Psychology, University of Minnesota</institution><addr-line><named-content content-type="city">Minneapolis</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Meng</surname><given-names>Ming</given-names></name><role>Reviewing Editor</role><aff><institution>South China Normal University</institution><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Senior Editor</role><aff><institution>National Institute of Mental Health, National Institutes of Health</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>29</day><month>12</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e70925</elocation-id><history><date date-type="received" iso-8601-date="2021-06-02"><day>02</day><month>06</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-12-11"><day>11</day><month>12</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-06-23"><day>23</day><month>06</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.06.23.449598"/></event></pub-history><permissions><copyright-statement>© 2021, Zhang et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Zhang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-70925-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-70925-figures-v1.pdf"/><abstract><p>Regions sensitive to specific object categories as well as organized spatial patterns sensitive to different features have been found across the whole ventral temporal cortex (VTC). However, it is unclear that within each object category region, how specific feature representations are organized to support object identification. Would object features, such as object parts, be represented in fine-scale spatial tuning within object category-specific regions? Here, we used high-field 7T fMRI to examine the spatial tuning to different face parts within each face-selective region. Our results show consistent spatial tuning of face parts across individuals that within right posterior fusiform face area (pFFA) and right occipital face area (OFA), the posterior portion of each region was biased to eyes, while the anterior portion was biased to mouth and chin stimuli. Our results demonstrate that within the occipital and fusiform face processing regions, there exist systematic spatial tuning to different face parts that support further computation combining them.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>high-field fMRI</kwd><kwd>face</kwd><kwd>ventral temporal cortex</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>31800966</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Jiedong</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002367</institution-id><institution>Chinese Academy of Sciences</institution></institution-wrap></funding-source><award-id>CAS Pioneer Hundred Talents Program</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Jiedong</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002367</institution-id><institution>Chinese Academy of Sciences</institution></institution-wrap></funding-source><award-id>XDB32020200</award-id><principal-award-recipient><name><surname>He</surname><given-names>Sheng</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002367</institution-id><institution>Chinese Academy of Sciences</institution></institution-wrap></funding-source><award-id>KJZD-SW-L08</award-id><principal-award-recipient><name><surname>He</surname><given-names>Sheng</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Consistent spatial clustering and fine-scale neural tuning to different face parts were found within the face-processing regions, which may be the neural implementation of efficient neural computation for face identification.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The ventral temporal cortex (VTC) in the brain supports our remarkable ability to recognize objects rapidly and accurately from the visual input in everyday life. Identity information is extracted from visual input through multiple stages of representation. To fully understand the neural mechanism of object processing, it is critical to know how these representations are physically applied to anatomical neural structure in the VTC. Numerous studies have already revealed multiple levels of feature representation manifest at different scales of anatomical organizations which superimposed in the VTC. The superordinate category representations (e.g. animate/inanimate, real-world size) manifest at large scale organization covering the whole VTC. Meanwhile, the category-selective representations (e.g. face, body, and scene selective regions in the mid-fusiform gyrus) are revealed at finer spatial scale in the VTC (<xref ref-type="bibr" rid="bib16">Hasson et al., 2003</xref>; <xref ref-type="bibr" rid="bib38">Spiridon et al., 2006</xref>). Recent evidence suggested a general spatial organization of neural responses to dimensions in object feature space in monkey inferotemporal cortex (<xref ref-type="bibr" rid="bib2">Bao et al., 2020</xref>). Could such physical organization be further extended to even smaller scale, like object parts/features representations within each category-selective region? In other words, as part representations play a critical role in object processing, would there be consistent spatial tuning across individuals for different object parts within each category-selective region in VTC?</p><p>Fine-scale spatial organizations of low-level visual features have already been found in early visual cortex, such as ocular dominance columns and orientation pinwheels (<xref ref-type="bibr" rid="bib3">Blasdel and Salama, 1986</xref>; <xref ref-type="bibr" rid="bib4">Bonhoeffer and Grinvald, 1991</xref>; <xref ref-type="bibr" rid="bib18">Hubel et al., 1977</xref>; <xref ref-type="bibr" rid="bib44">Weliky et al., 1996</xref>). Among all the object-selective regions in the VTC, the face-selective regions, including FFA and OFA, are one of the most widely examined object-processing networks in the past decades in cognitive neuroscience. As faces have spatially separated yet organized features such as eyes and mouth which are easy to be defined, it is suitable to use face parts to examine whether there are spatial tunings for different object features in the VTC. Neurophysiology studies in non-human primates demonstrated face-selective neurons in face-selective regions showed different sensitivities to various of face feature or combination of dimensions in face feature space (<xref ref-type="bibr" rid="bib6">Chang and Tsao, 2017</xref>; <xref ref-type="bibr" rid="bib12">Freiwald et al., 2009</xref>). Human fMRI studies also found the neural response patterns in FFA or OFA could distinguish different face parts (<xref ref-type="bibr" rid="bib46">Zhang et al., 2015</xref>), suggesting voxels within same face-selective region may have different face feature tuning. In addition, previous study also suggests that the spatial distribution of a face feature may be relevant to the physical location of that feature in a face (<xref ref-type="bibr" rid="bib17">Henriksson et al., 2015</xref>).</p><p>The sizes of the face-selective regions in VTC are relatively small, spanning about 1 cm. To investigate the potential spatial tuning within each face region, high-resolution fMRI with sufficient sensitivity and spatial precision is necessary. With high-field fMRI, fine-scale patterns have been observed in early visual cortex, such as columnar-like structures in V1, V2, V3, V3a, and hMT (<xref ref-type="bibr" rid="bib7">Cheng et al., 2001</xref>; <xref ref-type="bibr" rid="bib14">Goncalves et al., 2015</xref>; <xref ref-type="bibr" rid="bib29">Nasr et al., 2016</xref>; <xref ref-type="bibr" rid="bib36">Schneider et al., 2019</xref>; <xref ref-type="bibr" rid="bib45">Yacoub et al., 2008</xref>; <xref ref-type="bibr" rid="bib48">Zimmermann et al., 2011</xref>). These findings validate the feasibility of using high-field fMRI to reveal fine-scale (several mm) structures in the visual cortex.</p><p>Here, we used 7T fMRI to examine whether category-specific feature information, such as object parts, would be represented in certain spatial pattern within object selective regions. With faces as stimuli, the high-field fMRI allowed for measuring detailed neural response patterns from multiple face-selective regions. Our results show that in the right pFFA and right OFA, different face parts elicited differential spatial patterns of fMRI responses. Specifically, eyes induced responses biased to the posterior portion of the ROIs while responses to mouth and chin were biased to the anterior portion of the ROIs. Similar spatial tuning was observed in both the pFFA and OFA, and the patterns are highly consistent across participants. Together, these results reveal robust fine-scale spatial tuning of face features within face-selective regions.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>One critical challenge to demonstrate the spatial tuning within single face-selective region is to find the anatomical landmark to align the function maps between different individuals, as the shape, size, and spatial location of FFA vary largely across individuals. Among all the anatomical structures in the VTC, the mid-fusiform sulcus (MFS) could potentially serve as landmark in the current study. MFS is relatively small structure in the VTC, but consistently present in most individuals (<xref ref-type="bibr" rid="bib43">Weiner et al., 2014</xref>). On the one hand, the structure of MFS could predict the coordinates of face-selective region around mid-fusiform, especially the anterior one (<xref ref-type="bibr" rid="bib43">Weiner et al., 2014</xref>). On the other hand, MFS is found to be highly consistent with many anatomical lateral-medial transitions in the VTC, such as cytoarchitecture and white-matter connectivity transitions (<xref ref-type="bibr" rid="bib43">Weiner et al., 2014</xref>; <xref ref-type="bibr" rid="bib5">Caspers et al., 2013</xref>; <xref ref-type="bibr" rid="bib15">Grill-Spector and Weiner, 2014</xref>; <xref ref-type="bibr" rid="bib26">Lorenz et al., 2017</xref>). In addition, it could also predict the transitions in many function organization, such as animacy/inanimacy and face/scene preference (<xref ref-type="bibr" rid="bib15">Grill-Spector and Weiner, 2014</xref>). Considering its anatomical and functional significance, in the current study, we used the direction of MFS to align the potential spatial tuning of face part across individuals.</p><p>Different face parts (i.e., eyes, nose, mouth, hair, and chin, see <xref ref-type="fig" rid="fig1">Figure 1A</xref>) and whole faces were presented to participants and they performed a one-back task in 7T MRI scanner. For each participant, five face-selective ROIs (i.e. right pFFA, right aFFA, right OFA, left FFA, and left OFA) were defined with independent localizer scans. Before comparing the spatial response patterns between the face parts, we assessed the overall neural response amplitudes they generated in each ROIs. All face selective regions showed a similar trend that eyes generated higher responses than nose, hair, and chin (ts &gt; 2.61, ps &lt;0.05; except for eyes vs. nose in the left FFA and for eyes vs. chin in left OFA, ts &lt; 2.40, ps &gt;0.06. See <xref ref-type="fig" rid="fig1">Figure 1B</xref>). However, mouth generated similar response amplitudes as eyes (ts &lt; 1.58, ps &gt;0.17).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Stimuli and fMRI response maps.</title><p>(<bold>A</bold>) Exemplars of face part stimuli used in the main experiment. The face parts were generated from 20 male faces. Each stimulus was presented around the fixation and participants performed a one-back task during the scan. (<bold>B</bold>) Average fMRI responses to different face parts in each face-selective region. Generally, eyes elicited higher responses than responses to nose, hair, and chin in most of the regions. No significant difference was observed between eyes and mouth responses. Error bars reflect ±1 SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70925-fig1-v1.tif"/></fig><p>Considering that eyes and mouth are two dominant features in face perception (<xref ref-type="bibr" rid="bib37">Schyns et al., 2002</xref>; <xref ref-type="bibr" rid="bib42">Wegrzyn et al., 2017</xref>), and their response amplitudes were similar in face-selective regions, in the initial step, we compared the spatial patterns of neural responses to eyes with that to mouth within each ROI. Each pattern was first normalized to remove any overall amplitude difference between conditions. Then we directly contrasted the two patterns and projected the difference onto the inflated brain surface. A spatial pattern was observed in the right pFFA consistently across all participants (<xref ref-type="fig" rid="fig2">Figure 2</xref>). In the dimension parallel to the mid-fusiform gyrus, the posterior portion of the right pFFA was biased to respond more to eyes, whereas the anterior portion was biased to respond more to mouth. Note that in participant S2, the direction of MFS was more lateral-medial near the position of the right pFFA, and interestingly, the eyes-mouth contrast map was oriented in the same direction, even though S2’s map may initially appear oriented differently from that of other participants. It suggests the anatomical orientation of MFS is highly correlated with such spatial tuning of face parts. To estimate the reliability of such spatial tuning, we split the eight runs data from each participant in the main experiment into two data sets (odd-runs and even-runs), and estimated the eyes-mouth biases within each data set. Then we calculated the correlation coefficient between such biases across different voxels between the two data sets to estimate the reliability of the results in the right pFFA. The results demonstrate strong reliability of the data within participants (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Contrast maps between normalized fMRI responses to eyes and mouth in the right pFFA illustrated in the volume (upper) or on inflated cortical surface (lower) of each participant.</title><p>On the surface, the mid-fusiform sulcus is shown in dark gray with orange outline. The blue line outlines the right pFFA identified with an independent localizer scan. Aligned with the direction of mid-fusiform sulcus, the posterior part of right pFFA shows response bias to eyes (warm colors), while the anterior part illustrates mouth bias (cool colors). The posterior to anterior pattern is generally consistent across participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70925-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Correlation of eyes-mouth bias across voxels between split half data sets in the main experiment.</title><p>Each panel shows data from one participant.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70925-fig2-figsupp1-v1.tif"/></fig></fig-group><p>To further demonstrate such relationship, and also to provide a quantitative description of the spatial tuning of face parts within right pFFA, the fMRI responses to different face parts were projected onto the brain surface of each individual participant. Then we grouped vertices based on their location along the direction parallel to the MFS, and averaged the fMRI responses at each location to generate the response profile on this posterior-anterior dimension (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, see details in Materials and methods). The group-averaged results clearly showed that the difference between eyes and mouth signals consistently changed along the posterior-anterior direction in the right pFFA (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). To quantify this trend, we further calculated the correlation coefficient between the eyes-mouth neural response differences and the position index along the posterior-anterior dimension (i.e. more posterior location was assigned with smaller value) in each participant. The group result revealed a significant negative correlation (t(5)=8.36, p = 0.0004, Cohen’s d = 3.41), confirming the consistency across participants that the posterior part of right pFFA was biased to eyes and anterior part was biased to mouth.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The spatial profiles of eyes and mouth responses along the posterior-anterior dimension.</title><p>(<bold>A</bold>) To obtain the one-dimensional spatial profile of fMRI responses, a line was drawn parallel to the direction of mid-fusiform sulcus. Response from each vertex in the right pFFA was projected to the closest point on the line and averaged to generate the response profile. (<bold>B</bold>) The response profile of eyes vs mouth on the anterior-posterior dimension in right pFFA. The shaded regions reflect ±1 SEM. (<bold>C</bold>) The spatial profiles of whole faces and everyday objects in the right pFFA. Both profiles showed similar patterns, though the whole face responses were generally higher than object responses. (<bold>D</bold>) The spatial profile of individual face part responses, after regressing out the general fMRI response patterns elicited by either the whole faces (upper) or everyday objects (lower). In both cases, distinct spatial profiles were observed between eyes and mouth in the right pFFA.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70925-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Stimuli and results of Control Experiment 1.</title><p>(<bold>A</bold>) Four conditions (i.e. single eye near peripheral, single eye near central, mouth near peripheral, mouth near central) were include in the Control Experiment 1. For the near peripheral conditions, eye or mouth were presented at 3.1° either to the left or to the right of the fixation. For the near central conditions, face parts were presented at 1.3° left or right to the fixation. (<bold>B</bold>) The contrast maps between eyes and mouth regardless of location in each participant, showing clear anterior-posterior trend. (<bold>C</bold>) The contrast maps of near peripheral vs. near central location regardless of face parts, no consistent pattern is observed.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70925-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Stimuli and results of Control Experiment 2.</title><p>(<bold>A</bold>) Stimulus examples from Control Experiment 2 that top or bottom face parts presented at upper or lower visual field. (<bold>B</bold>) The anterior-posterior neural response profiles of face parts in right pFFA in eight participants. (<bold>C</bold>) The maps (without spatial smoothing) of response biases to face parts presented above vs. below fixation in right pFFA.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70925-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Stimuli and results of pRF experiment.</title><p>(<bold>A</bold>) Wedge and ring stimuli used in the pRF experiment. (<bold>B</bold>) Visualization of receptive field center of each voxel in the right pFFA outlined in blue. The maps represent the locations of receptive field center on the horizontal (left column) or vertical (right column) axis. No consistent spatial pattern is found across different participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70925-fig3-figsupp3-v1.tif"/></fig></fig-group><p>The contrast map highlighted the differences between eyes and mouth responses. However, the original response patterns elicited by eyes and mouth share the same underlying general ‘face-related’ pattern, which was subtracted out when contrasting the two response patterns. To extract the response profile of individual face parts, we used independently obtained response patterns of whole faces as the general face-related pattern and regressed it out from the eyes and mouth response patterns. The fMRI responses could be influenced by multiple factors other than neural responses, such as the distribution of the vein, which means there is a shared factor driving the raw fMRI response patterns of different conditions. Thus, to eliminate such shared pattern from the patterns of different face parts, we regressed out the spatial patterns of the whole faces from patterns of each face part. With the general pattern regressed out, we observed distinct spatial profiles elicited by eyes and mouth in the right pFFA (<xref ref-type="fig" rid="fig3">Figure 3D</xref> top panel). The eye-biased voxels were more posterior than that of mouth-biased voxels, which is consistent with the contrast map shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p><p>Removing the general pattern helped to reveal the pattern of voxel biases for individual face parts. While removing the face-related general pattern achieved this goal, it is possible that removing the general face-related pattern distorted the parts generated response patterns since they share high-level visual information (i.e. face and eyes stimuli are both face-related). Therefore, it is important to check whether the parts specific patterns could be seen with removal of a common face-independent signal distribution. In five of the six participants, data were also obtained when they viewed everyday objects. Indeed, non-face objects generated significantly lower but spatially similar patterns of activation compared with faces across the right pFFA (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). This result suggests that there is a general intrinsic BOLD sensitivity profile in the pFFA regardless of the stimuli. Indeed, both face and non-face object patterns explained large part of the variation of the face part patterns (for faces average R<sup>2</sup> = 0.86, for objects average R<sup>2</sup> = 0.72). Thus we proceeded to use the response patterns of either faces or non-face everyday objects to regress out the intrinsic baseline profile from eyes and mouth response patterns, and plotted face part specific patterns along the posterior-anterior dimension. Consistently, results with object patterns removed showed clear posterior bias for eyes and anterior bias for mouth in the right pFFA (<xref ref-type="fig" rid="fig3">Figure 3D</xref> bottom panel).</p><p>To control for the potential contribution from retinotopic bias of the different face part conditions, in our experiment, all stimuli were presented at the fixation with a 1.3° horizontal jitter either to the left or to the right alternatively in different trials within a block. Even though the stimuli were centered on the fixation, because of the nature of the face parts (e.g. two eyes are apart, chin depicts the outline of the face), there were still small degrees (less than 3°) of retinotopic differences between the eyes and mouth conditions. To further rule out the retinotopic contribution, as well as to replicate our finding, we did two control experiments. In the first control experiment (Control Experiment 1), data were obtained with a single eye or mouth presented at either the near central (1.3°) or near peripheral (3.1°) location during the scan (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>). This 2 × 2 (face parts x location) design allowed us to contrasted fMRI response patterns between face parts (single eye vs. mouth) regardless the stimulus location, or between locations (near central vs. near peripheral) regardless the face parts presented. Data from six participants were collected in the Control Experiment 1 and two of them (S1 and S5) also participated main experiment. In all participants, the eye vs. mouth contrast revealed spatial patterns in the right pFFA very similar to that in the main experiment (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). However, contrasting fMRI responses between the near central and near peripheral location regardless the face parts failed to reveal consistent patterns across participants (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>). These results further support that the different fMRI response patterns we observed in the right pFFA were contributed by face feature differences rather than retinotopic bias. In the second control experiment (Control Experiment 2), we used top and bottom parts of the face as stimuli and counterbalance the stimulus location to verify the spatial tuning in the right pFFA. With a 2 × 2 design (eyes vs. nose &amp; mouth x present above vs. below fixation) (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>), consistent anterior-posterior spatial patterns in the right pFFA were observed in eight participants (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>), which further corroborated our main finding of spatially organized representation of face parts in the right pFFA.</p><p>In addition to the two control experiments, we also measured the population receptive field (pRF) of each voxel in the right pFFA in three participants from the main experiment (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3A</xref>) following established procedures (<xref ref-type="bibr" rid="bib11">Dumoulin and Wandell, 2008</xref>; <xref ref-type="bibr" rid="bib19">Kay et al., 2013</xref>; <xref ref-type="bibr" rid="bib22">Kriegeskorte et al., 2008</xref>). For each voxel, parameter x and y were calculated along with other parameters to represent the receptive field center on the horizontal (x) and vertical (y) axis in the visual field. Although generally more voxels in the right pFFA were bias to left visual field, which is consistent with previous report (<xref ref-type="bibr" rid="bib20">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Nichols et al., 2016</xref>), we observed no consistent spatial pattern in either x or y map of the right pFFA across participants (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3B</xref>).</p><p>To examine the spatial patterns of response from eyes and mouth in other face-selectivity regions, similar analyses as in pFFA were applied to the fMRI response patterns in the right OFA, right aFFA, left FFA, and left OFA. For the left and right OFA, the posterior-anterior dimension was defined as the direction parallel to the occipitotemporal sulcus (OTS), where the OFAs were located in most participants. Among these regions, the right OFA also had distinct response patterns for eyes and mouth along the posterior-anterior dimension (<xref ref-type="fig" rid="fig4">Figure 4</xref>), similar to what we observed in the right pFFA. Group negative correlation was observed between the eyes-mouth differences and the posterior-anterior location of the right OFA (t(5)=3.63, p = 0.015, Cohen’s d = 1.48). Such pattern was also observed in the Control Experiments. We also observed similar spatial patterns between eyes-mouth bias and visual field bias in vertical direction (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), which is consistent with previous findings in inferior occipital gyrus (<xref ref-type="bibr" rid="bib10">de Haas et al., 2021</xref>). While the right OFA and right pFFA have been considered as sensitive to facial components and whole faces respectively, in our data they showed similar spatial profiles of eyes and mouth responses along the posterior-anterior dimension. This is consistent with, but adds some constraints to, the idea that the right pFFA may receive face feature information from right OFA for further processing (<xref ref-type="bibr" rid="bib25">Liu et al., 2010</xref>; <xref ref-type="bibr" rid="bib47">Zhu et al., 2011</xref>). In other face-selective regions, no consistent pattern was observed, as the correlations between the eyes-mouth difference and posterior-anterior location were not significant (ts &lt;1.09, ps &gt;0.32, see <xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>The spatial tuning of face parts in other face-selective regions.</title><p>(<bold>A</bold>) The anterior-posterior neural response profiles of eyes and mouth in other face-selective regions. The contrast between normalized eyes and mouth response patterns in different regions are shown in the left column. The right column plots show the eyes and mouth response profiles with general response patterns regressed out. The right OFA (top panel) demonstrates different response profiles for eyes and mouth, similar to the observation in right pFFA. The shaded regions reflect ±1 SEM. (<bold>B</bold>) The eyes vs mouth contrast maps in right OFA in the control experiment 1. A consistent anterior-posterior pattern could be observed in each participant. Right OFA could not be identified in one of the six participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70925-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Maps of eyes-mouth bias (left column) and vertical visual field bias (right column) in the right OFA.</title><p>Results from three participants showed that two maps were spatially similar.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70925-fig4-figsupp1-v1.tif"/></fig></fig-group><p>Beside the anterior-posterior dimension, the spatial representation of parts could organize in other spatial dimensions, such as the lateral-medial dimension in the VTC, or even in more complex nonlinear patterns. However, since the right pFFA located within the sulcus (MFS) in most of our participants, such that voxels distant from each other on the surface along the lateral-medial dimension could be spatially adjacent in the volume space, making it difficult to accurately reconstruct the spatial pattern along the lateral-medial dimension within the sulcus. Nevertheless, the finding of anterior-posterior bias of face parts is sufficient to demonstrate the existence of fine-scale feature map within object-selective regions.</p><p>Our stimuli also included nose, hair, and chin images, thus gave us a chance to examine their spatial profiles in each face-selective ROI as we did for eyes and mouth, though their neural responses were generally lower than that from eyes and mouth. Chin and mouth elicited similar response patterns along the anterior-posterior dimension in the right pFFA and right OFA after regressing out general spatial patterns (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). By directly contrasting fMRI response patterns between eyes and chin, similar spatial profiles were revealed in the right pFFA and right OFA that the posterior part was biased to eyes and anterior part was biased to chin (ts &gt;5.30, ps &lt;0.01, see <xref ref-type="fig" rid="fig5">Figure 5B</xref>). We also observed a similar though less obvious profile in the left FFA (t(5)=2.68, p = 0.04, Cohen’s d = 1.09), but not in the right aFFA or left OFA (ts &lt;0.41, ps &gt;0.71).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>The anterior-posterior neural response profiles of all face parts (i.e. eyes, nose, mouth, hair, and chin) in five face-selective regions.</title><p>(<bold>A</bold>) With general pattern regressed out, the chin showed similar response profiles as mouth in right pFFA and right OFA. (<bold>B</bold>) Contrasting normalized eyes and chin response patterns revealed consistent changes along the posterior-anterior dimension in right pFFA and right OFA. The shaded regions reflect ±1 SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70925-fig5-v1.tif"/></fig></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Our results reveal that within certain face-selective regions in the human occipito-temporal cortex, the neural representations of different facial features have consistent spatial profiles. Such fine-scale spatial tuning is found similarly in the right pFFA and right OFA, but not in the more anterior right aFFA nor in the left hemisphere’s face-selective regions. In other words, fine-scale spatial tuning for face parts exists at the early to intermediate stages of face processing hierarchy in the right hemisphere.</p><p>In the current study, five face parts (i.e. eyes, nose, mouth, hair, and chin) were tested, with eyes and mouth showed most distinct spatial profiles in the right pFFA and right OFA. No obvious spatial pattern was observed for nose and hair in face-selective regions, but it would be premature to conclude that there is no fine-scale spatial profile for their neural representations. For one, the nose and hair stimuli elicited lower fMRI responses compared with eyes and mouth stimuli, making it more difficult to detect potential spatial patterns. The observation that eyes and mouth elicited most differential patterns is consistent with them providing more information about faces than other features in face processing (<xref ref-type="bibr" rid="bib37">Schyns et al., 2002</xref>; <xref ref-type="bibr" rid="bib42">Wegrzyn et al., 2017</xref>). The dominance of eyes and mouth in face-selective regions could be considered as a form of cortical magnification of more informative features, a common principle of functional organization in sensory cortex (<xref ref-type="bibr" rid="bib8">Cowey and Rolls, 1974</xref>; <xref ref-type="bibr" rid="bib9">Daniel and Whitteridge, 1961</xref>; <xref ref-type="bibr" rid="bib31">Penfield and Boldrey, 1937</xref>).</p><p>The discovery that some face parts are represented within the face-processing regions with fine-scale spatial tuning improve our understanding about how functional representations are physically applied to anatomical structures in the VTC. To further explore the neural models about object processing in the VTC, it is important to ask what kinds of constrain, functional or anatomical, result in such fine-scale spatial tuning? Many of the visual cortical areas have retinotopic maps, indeed having a retinotopic representation of the visual world is one of the key ways to define a visual cortical area. Along occipitotemporal processing stream, visual areas increasingly become more specialized in processing certain features and object categories. What is the relationship between a potential spatial organization of face part representations and the spatial relationship of face parts in a real face?</p><p>A recent study has revealed that in the inferior occipital gyrus, where the OFA located, both tunings for retinotopic location and face parts (<xref ref-type="bibr" rid="bib10">de Haas et al., 2021</xref>). Although the tuning peak maps were idiosyncratic across individuals, the two tuning maps were correlated within individuals, suggesting a relationship between face parts configuration and their typical retinotopic configuration. Our findings provide additional support for face part turning in the OFA, and further reveal that there exists a consistent spatial profile of face part tuning <italic>across</italic> individuals. More importantly, our finding of spatial tuning of face part in the pFFA indicates that although the organization of feature tuning could be constrained by the retinotopic tuning in occipital cortex, a more abstract feature tuning could still be spatially organized in cortical areas with absent or minimal retinotopic property in the later stages of VTC.</p><p>Another previous study also tested the idea of ‘faciotopy’, that there are cortical patches representing different face features within a face-selective region and the spatial organization of these feature patches on the cortical surface would reflect the physical relationships of face features (<xref ref-type="bibr" rid="bib17">Henriksson et al., 2015</xref>). Their results showed that in the OFA and FFA, the differences between neural response patterns of face parts were correlated with physical distances between face parts in a face. Our results support the existence of stable spatial profile of face features in the right OFA and right pFFA, especially for eyes and mouth. The possible mechanism underlying such faciotopy organization is the local-to-global computation, that physically adjacent face parts interact more than parts far apart from each other during the early stages of processing, thus it is more efficient for them to have neural representations near each other. However, in the current study, we did not find the posterior bias pattern for hair as we did for eyes, even though hair and eyes are spatially adjacent, which could be caused by the hair being generally less invariant and less informative in the face identification.</p><p>Another potential explanation could be that while both contribute to face identification, eyes and mouth are differentially involved in different neural networks and have distinct functional connectivity profiles with other brain regions. Specifically, the mouth region provides more information for language processing and audio-visual communication perception, thus it may be more connected to the language processing system. Meanwhile, the eyes are more important in face detection and eye gaze signifies interest, thus it may be more connected to the attention system. Previous studies have already found the connectivity profiles could predict the functional selectivity in the VTC, thus it would be interesting to examine whether the face part spatial tuning in the pFFA could be predicted using functional or anatomical connectivity to the other brain regions in the future studies.</p><p>The third explanation of our results is that the fine-scale pattern of face part sensitivity is driven by larger-scale organization of object-selective regions in the ventral pathway. As the FFA is overlapped with body-selective region fusiform-body area (FBA), it is possible that some face features (e.g. mouth) could be represented closer to the FBA, while face parts such as the eyes are more represented in the FFA proper. However, existing evidence does not support a consistent anterior-posterior relationship between FFA and FBA (<xref ref-type="bibr" rid="bib21">Kim et al., 2014</xref>). It remains important to directly compare the eyes-mouth pattern against the face-body pattern with high-resolution fMRI in future studies.</p><p>The spatial clustering of neurons with similar tuning is one of the organization principles in the brain. Such clustering may optimize the efficiency of the neural computation by reducing the total wire length (<xref ref-type="bibr" rid="bib24">Laughlin and Sejnowski, 2003</xref>), thus the clustering of neurons with similar feature tuning within face-selective regions could improve the processing efficiency to face stimuli. Our results provide evidence from neural imaging data to support the voxel level neuronal clustering driven by the tuning to different face parts. Previous fMRI and single unit recording studies in monkey face processing network have demonstrated strong correspondence between fMRI signal and neuronal responses (<xref ref-type="bibr" rid="bib40">Tsao et al., 2006</xref>), suggesting that the face part tuning in our results may be driven by neuronal response biases. In addition to neuronal response biases, the clustering could also reflect activity synchronization across neurons. Further neurophysiology studies are needed to delineate the specific mechanisms to the spatial clustering observed within face-selective regions.</p><p>Among five face-selective regions we examined, only the right pFFA and right OFA exhibited distinct fMRI response patterns for eyes and mouth. In the face processing network, face parts are believed to be represented in the posterior regions such as the OFA (<xref ref-type="bibr" rid="bib25">Liu et al., 2010</xref>; <xref ref-type="bibr" rid="bib1">Arcurio et al., 2012</xref>; <xref ref-type="bibr" rid="bib32">Pitcher et al., 2007</xref>). Part information is transmitted to anterior regions to be further integrated to form holistic face representations (<xref ref-type="bibr" rid="bib46">Zhang et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Rotshtein et al., 2005</xref>). In that sense, the more anterior regions in the face processing network are more responsible for representing integrated face information such as gender or identity rather than individual face parts (<xref ref-type="bibr" rid="bib13">Freiwald and Tsao, 2010</xref>; <xref ref-type="bibr" rid="bib23">Landi and Freiwald, 2017</xref>). Consistent with this idea, at the right aFFA, there is no obvious spatial tuning of face parts. Meanwhile, a clear hemispheric difference was found in our results that the distinct spatial response patterns for face parts were observed in the right but not the left hemisphere, which is consistent with previous findings that compared with left FFA, right FFA is more sensitive to face specific features (<xref ref-type="bibr" rid="bib27">Meng et al., 2012</xref>) and configural information (<xref ref-type="bibr" rid="bib34">Rossion et al., 2000</xref>). The neural clustering of face part tuning and consistent spatial patterns across individuals in the right rather than in the left face selective regions provides a potential computational advantage for right lateralization for face processing. The clustering of neurons with similar feature tuning have been found extensively in the ventral pathway, which may help to support a more efficient neural processing. Therefore, one of the neural mechanisms underlying the functional lateralization of face processing could be the existence of spatial clustering of face part tunings in the right hemisphere.</p><p>Much progress has been made in our understanding of object feature representation in the VTC during the past decade, especially with the view of feature space representation (<xref ref-type="bibr" rid="bib2">Bao et al., 2020</xref>; <xref ref-type="bibr" rid="bib6">Chang and Tsao, 2017</xref>). Consequently, we now believe that a large number of features are represented for object recognition, but how does our brain physically organize such complex feature representations in the VTC? One possible solution is that these feature representations manifest in different spatial scales. For more general features the representation manifests at large spatial scale across the whole VTC (e.g. large/small, animate/inanimate), and for more specific features such as face parts, it manifests at finer spatial scales within specific object processing regions. Under this view, we would expect more fine-scale feature organizations to be revealed with more advanced neural imaging tools, which are critical for fully understanding the neural algorithm of object processing in the VTC.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Six (3 females) human participants were recruited in the main experiment. Six (5 females) participants (two of them also participated main experiment) were recruited in the Control Experiment 1. Three participants (2 females) from main experiment finished the pRF experiment. Ten participants (one female) were recruited in the Control Experiment 2, but in two participants right pFFA was failed to be localized, thus we excluded these two participants from the analyses. All participants were between the ages of 21 and 27, right-handed, and had normal or corrected to normal visual acuity. They were recruited from the Chinese Academy of Sciences community with informed consent and received payment for their participation in the experiment. The experiment was approved by the Committee on the Use of Human Subjects at the Institute of Biophysics of Chinese Academy of Sciences (#2017-IRB-004).</p></sec><sec id="s4-2"><title>Stimuli and experimental design</title><p>In the main experiment, for face stimuli, 20 unique front-view Asian male face images were used. Each face image was gray-scaled and further divided into five parts (i.e. eyes, nose, mouth, hair, and chin. See <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Twenty unique gray-scaled everyday objects were used as comparison stimuli. The full face and object images on average subtended around 5° x 7°. For stimuli used in localizer scans, video clips of faces, objects, and scrambled objects were used (For detail see <xref ref-type="bibr" rid="bib33">Pitcher et al., 2011</xref>).</p><p>There were total of seven stimulus conditions (i.e. eyes, nose, mouth, hair, chin, whole face, and object condition). Each main experimental run contained two blocks of each stimulus condition. In the scan of participant S6, the object condition was not included. Each stimulus block lasted 16 s and contained 20 images of the same type. Each image was presented for 600 ms at fixation and followed by a 200 msec blank interval. There was a 16 s blank fixation block at the beginning, the middle, and the end of each run. Participants performed a 1-back task that they were asked to press a button when two successive images were the same. To balance the spatial property in the visual field of different images, each image was presented at a slightly shifted location, 1.3° either to the left or to the right of the fixation alternately in different trials within a block. Participants were instructed to maintain central fixation throughout the task.</p><p>Each localizer run contained four 18 s blocks of each of the three stimulus conditions (i.e. faces, everyday non-face objects, and scrambled objects) shown in a balanced block order. The 12 stimulus blocks were interleaved by three 18 s fixation blocks inserted at the beginning, middle and end of each run. Each block contained six video clips of a given stimulus category, each presented for 3 s. Participants were asked to watch the video without any task. No fixation point was presented during the scan.</p><p>The eight experimental runs and the two localizer runs were completed within the same scan session for each participant.</p><p>In the Control Experiment 1, we used a similar block design as that in the main experiment. There were six kinds of stimulus blocks (single eye near central, single eye near peripheral, mouth near central, mouth near peripheral, whole face, object) and each of them repeated three times in a single run. Each participant completed four runs and two localizer runs. In the eye near central condition, single left eye images were presented at 1.3° either to the left or to the right of the fixation alternately in different trials within a block. In the eye near peripheral condition, single left eye images were presented at 3.1° either to the left or to the right of the fixation. The central and peripheral locations were chosen to match the locations of eyes and mouth in the main experiment. Stimuli in mouth near central and mouth near peripheral conditions were presented in the same locations as in two eye conditions, respectively. Whole face and object conditions were the same as in the main experiment.</p><p>In the pRF experiment, we adopted stimuli and analysis code from analyzePRF package (<ext-link ext-link-type="uri" xlink:href="http://kendrickkay.net/analyzePRF/">http://kendrickkay.net/analyzePRF/</ext-link>). There were total of four conditions (i.e. clockwise wedges, counterclockwise wedges, expanding rings, contracting rings). The angular span of the wedges was 45°, and it revolved for 32 s per cycle. In the rings conditions, the rings swept 28 s per cycle with 4 s of rest followed. Colored object images were presented on the wedges or rings. The rings and wedges were presented within a radius of 10°. For each run, there was a 22 s blank fixation block at the beginning and the end. Participants performed a change detection task that they pressed a button whenever the fixation color changed. In each run, only one kind of PRF stimulus was presented and repeated eight cycles. Each participant finished four different pRF runs.</p><p>In the Control Experiment 2, similar block-design as in main experiment was used. Four face part conditions (top vs bottom part x present location) were included in the experiment (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>). The top part contained eyes (4.02° x 12.08°) and the bottom part contained nose and mouth (8.08° x 12.08°). To engage observers’ attention on the stimuli, a randomly selected four images in each block moved slightly either to the left or right during stimulus presentation. Observers were asked to judge the directions of these movements. Same localizer runs as in the main experiment were included for each participant.</p></sec><sec id="s4-3"><title>FMRI scanning</title><p>MRI data were collected on Siemens Magnetom 7 Tesla MRI system (passively shielded, 45mT/s slew rate) (Siemens, Erlangen, Germany), with a 32-channel receive 1-channel transmit head coil (NOVA Medical, Inc, Wilmington, MA, USA), at the Beijing MRI Center for Brain Research (BMCBR). High-resolution T1-weighted anatomical images (0.7 mm isotropic voxel size) were acquired with a MPRAGE sequence (256 sagittal slices, acquisition matrix = 320 × 320, Field of view = 223 × 223 mm, GRAPPA factor = 3, TR = 3100ms, TE = 3.56ms, TI = 1250ms, flip angle = 5°, pixel bandwidth = 182 Hz per pixel). Proton density (PD)-weighted images were acquired with same voxel size and FOV (256 sagittal slices, acquisition matrix = 320 × 320, Field of view = 223 × 223 mm, GRAPPA factor = 3, TR = 2340ms, TE = 3.56ms, flip angle = 5°, pixel bandwidth = 182 Hz). GE-EPI sequences was used to collect functional data in the main experiment (TR = 2000ms, TE = 18ms, 1.2 mm isotropic voxels, FOV = 168 × 168 mm, image matrix = 140 × 140, GRAPPA factor = 3, partial Fourier 6/8, 31 slices of 1.2 mm thinkness, flip angle is about 80, pixel bandwidth = 1276 Hz per pixel). During the scan, GE-EPI images with reversed phase encoding direction from experiment functional scan were collected to correct the spatial distortion of EPI images (<xref ref-type="bibr" rid="bib28">Morgan et al., 2004</xref>). Dielectric pads were placed on both sides of the head to improve B1 efficiency in the temporal cortex (<xref ref-type="bibr" rid="bib39">Teeuwisse et al., 2012</xref>), while bite-bar was used to reduce head movements for each participant. During the functional scan, respiration and pulse signals were recorded simultaneously. GE-EPI sequences with same resolution as in the main experiment was used in the control and pRF experiment (TR = 2000 ms, TE = 22ms, 1.2 mm isotropic voxels, FOV = 180 × 180 mm, image matrix = 150 × 150, GRAPPA factor = 2, partial Fourier 6/8, 31 slices of 1.2 mm thinkness, flip angle is about 80, pixel bandwidth = 1587 Hz per pixel). Dielectric pads were placed on the right side of the head.</p></sec><sec id="s4-4"><title>Data analysis</title><p>Anatomical data were analyzed with FreeSurfer (Cortechs Inc, Charlestown, MA) and custom MATLAB codes. To enhance the contrast between white and gray matter, T1-weighted images were divided by PD-weighted images (<xref ref-type="bibr" rid="bib41">Van de Moortele et al., 2009</xref>). Anatomical data were further processed with FreeSurfer to reconstruct the cortical surface models.</p><p>Functional data were analyzed with AFNI (<ext-link ext-link-type="uri" xlink:href="http://afni.nimh.nih.gov">http://afni.nimh.nih.gov</ext-link>), FreeSurfer, fROI (<ext-link ext-link-type="uri" xlink:href="http://froi.sourceforge.net">http://froi.sourceforge.net</ext-link>), and custom MATLAB codes. Data preprocessing included slice-timing correction, motion correction, removing physiological noise with respiration and pulse signals, distortion correction with reversed phase encoding EPI images, and intensity normalization. For the localizer runs only, spatial smoothing was applied (Gaussian kernel, 2 mm full width at half maximum). After preprocessing, function images were co-registered to anatomic images for each participant. To obtain the average response amplitude for each voxel in the specific stimulus condition for each individual observer, voxel time courses were fitted by a general linear model (GLM), whereby each condition was modeled by a boxcar regressor (matched in stimulus duration) and then convolved with a gamma function (delta = 2.25, tau = 1.25). The resulting beta weights were used to characterize the averaged response amplitudes.</p><p>The face-selective ROIs were identified by contrasting functional data between face and everyday-object conditions in the localizer runs. Specifically, FFA and OFA was defined as the set of continuous voxels in fusiform gyrus and inferior occipital gyrus, respectively, that showed significantly higher response to faces than to objects (p &lt; 0.01, uncorrected). We were able to identify right pFFA, right anterior FFA (right aFFA), right OFA, and left FFA in all six participants. The left OFA were successfully identified in five out of six participants. In each ROI, to remove the vein signal in the functional data, voxels of which signal changes to face stimuli were larger than 4% were excluded in further analysis.</p><p>For the main experimental data, to remove the general fMRI response pattern shared among different face parts, response patterns from whole faces or everyday objects were regressed out from response patterns of each individual face part. Whole face or object response in each voxel was used to predict the individual part response with linear regression algorithm, and the residuals across voxels were considered as the individual part response pattern with general response pattern removed. To extract the trend of the fMRI response pattern along anterior-posterior dimension in the FFA, we first drew a line along the mid-fusiform sulcus on the cortical surface of each participant. For all vertices within the FFA ROI, we calculated their shortest (orthogonal) distances to the line, and projected the neural response of all voxels in the FFA ROI to the line along the mid-fusiform sulcus, and obtained the averaged response on each point along the line to get the response profiles (see <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Similar analysis was done for OFA with the line drawn along the inferior occipital sulcus.</p><p>For the control experiments, same data processing steps as in the main experiment were applied to extract the spatial patterns of different conditions. For the pRF data, fMRI respond time course of each voxel was fit with compressive spatial summation (CSS) model (<ext-link ext-link-type="uri" xlink:href="http://kendrickkay.net/analyzePRF/">http://kendrickkay.net/analyzePRF/</ext-link>). To determine the center location (x, y) of each voxel’s population receptive field, CSS used an isotropic 2D Gaussian and a static power-low nonlinearity to model the fMRI response. In each voxel, model fitness can be quantified as the coefficient of determination between model and data (R<sup>2</sup>). We only included the pRF results of voxels with R<sup>2</sup> higher than 2%.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Supervision, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Methodology, Validation, Visualization</p></fn><fn fn-type="con" id="con3"><p>Data curation, Formal analysis, Validation, Visualization</p></fn><fn fn-type="con" id="con4"><p>Data curation, Investigation, Methodology, Validation, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Funding acquisition, Methodology, Project administration, Supervision, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All human participants were recruited from the Chinese Academy of Sciences community with informed consent and received payment for their participation in the experiment. The experiment was approved by the Committee on the Use of Human Subjects at the Institute of Biophysics of Chinese Academy of Sciences (#2017-IRB-004).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-70925-transrepform1-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>fMRI response data have been deposited in Dryad.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Song</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name><name><surname>He</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Dataset for Spatial tuning of face part representations within face-selective areas revealed by high-field fMRI</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.gmsbcc2nh</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by funds from NSFC grant (No. 31800966), CAS Pioneer Hundred Talents Program, Strategy Priority Research Program of Chinese Academy of Science (No. XDB32020200), and CAS Key Research Program of Frontier Sciences (No. KJZD-SW-L08). The authors would like to thank Dr. Chencan Qian and Dr. Zihao Zhang for their help during data collection and analysis.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcurio</surname><given-names>LR</given-names></name><name><surname>Gold</surname><given-names>JM</given-names></name><name><surname>James</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The response of face-selective cortex with single face parts and part combinations</article-title><source>Neuropsychologia</source><volume>50</volume><fpage>2454</fpage><lpage>2459</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.06.016</pub-id><pub-id pub-id-type="pmid">22750118</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>P</given-names></name><name><surname>She</surname><given-names>L</given-names></name><name><surname>McGill</surname><given-names>M</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A map of object space in primate inferotemporal cortex</article-title><source>Nature</source><volume>583</volume><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2350-5</pub-id><pub-id pub-id-type="pmid">32494012</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blasdel</surname><given-names>GG</given-names></name><name><surname>Salama</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Voltage-sensitive dyes reveal a modular organization in monkey striate cortex</article-title><source>Nature</source><volume>321</volume><fpage>579</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1038/321579a0</pub-id><pub-id pub-id-type="pmid">3713842</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Grinvald</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Iso-orientation domains in cat visual cortex are arranged in pinwheel-like patterns</article-title><source>Nature</source><volume>353</volume><fpage>429</fpage><lpage>431</lpage><pub-id pub-id-type="doi">10.1038/353429a0</pub-id><pub-id pub-id-type="pmid">1896085</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caspers</surname><given-names>J</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Schleicher</surname><given-names>A</given-names></name><name><surname>Mohlberg</surname><given-names>H</given-names></name><name><surname>Amunts</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cytoarchitectonical analysis and probabilistic mapping of two extrastriate areas of the human posterior fusiform gyrus</article-title><source>Brain Structure &amp; Function</source><volume>218</volume><fpage>511</fpage><lpage>526</lpage><pub-id pub-id-type="doi">10.1007/s00429-012-0411-8</pub-id><pub-id pub-id-type="pmid">22488096</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>L</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The Code for Facial Identity in the Primate Brain</article-title><source>Cell</source><volume>169</volume><fpage>1013</fpage><lpage>1028</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2017.05.011</pub-id><pub-id pub-id-type="pmid">28575666</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>K</given-names></name><name><surname>Waggoner</surname><given-names>RA</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Human ocular dominance columns as revealed by high-field functional magnetic resonance imaging</article-title><source>Neuron</source><volume>32</volume><fpage>359</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00477-9</pub-id><pub-id pub-id-type="pmid">11684004</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowey</surname><given-names>A</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Human cortical magnification factor and its relation to visual acuity</article-title><source>Experimental Brain Research</source><volume>21</volume><fpage>447</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1007/BF00237163</pub-id><pub-id pub-id-type="pmid">4442497</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daniel</surname><given-names>PM</given-names></name><name><surname>Whitteridge</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>The representation of the visual field on the cerebral cortex in monkeys</article-title><source>The Journal of Physiology</source><volume>159</volume><fpage>203</fpage><lpage>221</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1961.sp006803</pub-id><pub-id pub-id-type="pmid">13883391</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Haas</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Schwarzkopf</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Inferior Occipital Gyrus Is Organized along Common Gradients of Spatial and Face-Part Selectivity</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>5511</fpage><lpage>5521</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2415-20.2021</pub-id><pub-id pub-id-type="pmid">34016715</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Population receptive field estimates in human visual cortex</article-title><source>NeuroImage</source><volume>39</volume><fpage>647</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.034</pub-id><pub-id pub-id-type="pmid">17977024</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A face feature space in the macaque temporal lobe</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1187</fpage><lpage>1196</lpage><pub-id pub-id-type="doi">10.1038/nn.2363</pub-id><pub-id pub-id-type="pmid">19668199</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional compartmentalization and viewpoint generalization within the macaque face-processing system</article-title><source>Science</source><volume>330</volume><fpage>845</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1126/science.1194908</pub-id><pub-id pub-id-type="pmid">21051642</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goncalves</surname><given-names>NR</given-names></name><name><surname>Ban</surname><given-names>H</given-names></name><name><surname>Sánchez-Panchuelo</surname><given-names>RM</given-names></name><name><surname>Francis</surname><given-names>ST</given-names></name><name><surname>Schluppeck</surname><given-names>D</given-names></name><name><surname>Welchman</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>7 tesla FMRI reveals systematic functional organization for binocular disparity in dorsal visual cortex</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>3056</fpage><lpage>3072</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3047-14.2015</pub-id><pub-id pub-id-type="pmid">25698743</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title><source>Nature Reviews. Neuroscience</source><volume>15</volume><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1038/nrn3747</pub-id><pub-id pub-id-type="pmid">24962370</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Harel</surname><given-names>M</given-names></name><name><surname>Levy</surname><given-names>I</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Large-scale mirror-symmetry organization of human occipito-temporal object areas</article-title><source>Neuron</source><volume>37</volume><fpage>1027</fpage><lpage>1041</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00144-2</pub-id><pub-id pub-id-type="pmid">12670430</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henriksson</surname><given-names>L</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Faciotopy-A face-feature map with face-like topology in the human occipital face area</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>72</volume><fpage>156</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.06.030</pub-id><pub-id pub-id-type="pmid">26235800</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name><name><surname>lecture</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Ferrier lecture. Functional architecture of macaque monkey visual cortex</article-title><source>Proceedings of the Royal Society of London. Series B, Biological Sciences</source><volume>198</volume><fpage>1</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1098/rspb.1977.0085</pub-id><pub-id pub-id-type="pmid">20635</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Mezer</surname><given-names>A</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Compressive spatial summation in human visual cortex</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>481</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1152/jn.00105.2013</pub-id><pub-id pub-id-type="pmid">23615546</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attention reduces spatial uncertainty in human ventral temporal cortex</article-title><source>Current Biology</source><volume>25</volume><fpage>595</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.12.050</pub-id><pub-id pub-id-type="pmid">25702580</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>NY</given-names></name><name><surname>Lee</surname><given-names>SM</given-names></name><name><surname>Erlendsdottir</surname><given-names>MC</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Discriminable spatial patterns of activation for faces and bodies in the fusiform gyrus</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>632</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00632</pub-id><pub-id pub-id-type="pmid">25177286</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landi</surname><given-names>SM</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Two areas for familiar face recognition in the primate brain</article-title><source>Science</source><volume>357</volume><fpage>591</fpage><lpage>595</lpage><pub-id pub-id-type="doi">10.1126/science.aan1139</pub-id><pub-id pub-id-type="pmid">28798130</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname><given-names>SB</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Communication in neuronal networks</article-title><source>Science</source><volume>301</volume><fpage>1870</fpage><lpage>1874</lpage><pub-id pub-id-type="doi">10.1126/science.1089662</pub-id><pub-id pub-id-type="pmid">14512617</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>A</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Perception of face parts and face configurations: an FMRI study</article-title><source>Journal of Cognitive Neuroscience</source><volume>22</volume><fpage>203</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21203</pub-id><pub-id pub-id-type="pmid">19302006</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorenz</surname><given-names>S</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Caspers</surname><given-names>J</given-names></name><name><surname>Mohlberg</surname><given-names>H</given-names></name><name><surname>Schleicher</surname><given-names>A</given-names></name><name><surname>Bludau</surname><given-names>S</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name><name><surname>Amunts</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Two New Cytoarchitectonic Areas on the Human Mid-Fusiform Gyrus</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>373</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv225</pub-id><pub-id pub-id-type="pmid">26464475</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meng</surname><given-names>M</given-names></name><name><surname>Cherian</surname><given-names>T</given-names></name><name><surname>Singal</surname><given-names>G</given-names></name><name><surname>Sinha</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Lateralization of face processing in the human brain</article-title><source>Proceedings. Biological Sciences</source><volume>279</volume><fpage>2052</fpage><lpage>2061</lpage><pub-id pub-id-type="doi">10.1098/rspb.2011.1784</pub-id><pub-id pub-id-type="pmid">22217726</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morgan</surname><given-names>PS</given-names></name><name><surname>Bowtell</surname><given-names>RW</given-names></name><name><surname>McIntyre</surname><given-names>DJO</given-names></name><name><surname>Worthington</surname><given-names>BS</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Correction of spatial distortion in EPI due to inhomogeneous static magnetic fields using the reversed gradient method</article-title><source>Journal of Magnetic Resonance Imaging</source><volume>19</volume><fpage>499</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1002/jmri.20032</pub-id><pub-id pub-id-type="pmid">15065175</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasr</surname><given-names>S</given-names></name><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Interdigitated Color- and Disparity-Selective Columns within Human Visual Cortical Areas V2 and V3</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>1841</fpage><lpage>1857</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3518-15.2016</pub-id><pub-id pub-id-type="pmid">26865609</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname><given-names>DF</given-names></name><name><surname>Betts</surname><given-names>LR</given-names></name><name><surname>Wilson</surname><given-names>HR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Position selectivity in face-sensitive visual cortex to facial and nonfacial stimuli: an fMRI study</article-title><source>Brain and Behavior</source><volume>6</volume><elocation-id>e00542</elocation-id><pub-id pub-id-type="doi">10.1002/brb3.542</pub-id><pub-id pub-id-type="pmid">27843696</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penfield</surname><given-names>W</given-names></name><name><surname>Boldrey</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1937">1937</year><article-title>SOMATIC MOTOR AND SENSORY REPRESENTATION IN THE CEREBRAL CORTEX OF MAN AS STUDIED BY ELECTRICAL STIMULATION</article-title><source>Brain : A Journal of Neurology</source><volume>60</volume><fpage>389</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1093/brain/60.4.389</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname><given-names>D</given-names></name><name><surname>Walsh</surname><given-names>V</given-names></name><name><surname>Yovel</surname><given-names>G</given-names></name><name><surname>Duchaine</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>TMS evidence for the involvement of the right occipital face area in early face processing</article-title><source>Current Biology</source><volume>17</volume><fpage>1568</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.07.063</pub-id><pub-id pub-id-type="pmid">17764942</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname><given-names>D</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Saxe</surname><given-names>RR</given-names></name><name><surname>Triantafyllou</surname><given-names>C</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Differential selectivity for dynamic versus static information in face-selective cortical regions</article-title><source>NeuroImage</source><volume>56</volume><fpage>2356</fpage><lpage>2363</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.03.067</pub-id><pub-id pub-id-type="pmid">21473921</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Dricot</surname><given-names>L</given-names></name><name><surname>Devolder</surname><given-names>A</given-names></name><name><surname>Bodart</surname><given-names>JM</given-names></name><name><surname>Crommelinck</surname><given-names>M</given-names></name><name><surname>De Gelder</surname><given-names>B</given-names></name><name><surname>Zoontjes</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Hemispheric asymmetries for whole-based and part-based face processing in the human fusiform gyrus</article-title><source>Journal of Cognitive Neuroscience</source><volume>12</volume><fpage>793</fpage><lpage>802</lpage><pub-id pub-id-type="doi">10.1162/089892900562606</pub-id><pub-id pub-id-type="pmid">11054921</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rotshtein</surname><given-names>P</given-names></name><name><surname>Henson</surname><given-names>RNA</given-names></name><name><surname>Treves</surname><given-names>A</given-names></name><name><surname>Driver</surname><given-names>J</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Morphing Marilyn into Maggie dissociates physical and identity face representations in the brain</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>107</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1038/nn1370</pub-id><pub-id pub-id-type="pmid">15592463</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>M</given-names></name><name><surname>Kemper</surname><given-names>VG</given-names></name><name><surname>Emmerling</surname><given-names>TC</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Columnar clusters in the human motion complex reflect consciously perceived motion axis</article-title><source>PNAS</source><volume>116</volume><fpage>5096</fpage><lpage>5101</lpage><pub-id pub-id-type="doi">10.1073/pnas.1814504116</pub-id><pub-id pub-id-type="pmid">30808809</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schyns</surname><given-names>PG</given-names></name><name><surname>Bonnar</surname><given-names>L</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Show me the features! Understanding recognition from the use of visual information</article-title><source>Psychological Science</source><volume>13</volume><fpage>402</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00472</pub-id><pub-id pub-id-type="pmid">12219805</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spiridon</surname><given-names>M</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Location and spatial profile of category-specific regions in human extrastriate cortex</article-title><source>Human Brain Mapping</source><volume>27</volume><fpage>77</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1002/hbm.20169</pub-id><pub-id pub-id-type="pmid">15966002</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teeuwisse</surname><given-names>WM</given-names></name><name><surname>Brink</surname><given-names>WM</given-names></name><name><surname>Webb</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Quantitative assessment of the effects of high-permittivity pads in 7 Tesla MRI of the brain</article-title><source>Magnetic Resonance in Medicine</source><volume>67</volume><fpage>1285</fpage><lpage>1293</lpage><pub-id pub-id-type="doi">10.1002/mrm.23108</pub-id><pub-id pub-id-type="pmid">21826732</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname><given-names>DY</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A cortical region consisting entirely of face-selective cells</article-title><source>Science</source><volume>311</volume><fpage>670</fpage><lpage>674</lpage><pub-id pub-id-type="doi">10.1126/science.1119983</pub-id><pub-id pub-id-type="pmid">16456083</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van de Moortele</surname><given-names>P-F</given-names></name><name><surname>Auerbach</surname><given-names>EJ</given-names></name><name><surname>Olman</surname><given-names>C</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Uğurbil</surname><given-names>K</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>T1 weighted brain images at 7 Tesla unbiased for Proton Density, T2* contrast and RF coil receive B1 sensitivity with simultaneous vessel visualization</article-title><source>NeuroImage</source><volume>46</volume><fpage>432</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.02.009</pub-id><pub-id pub-id-type="pmid">19233292</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wegrzyn</surname><given-names>M</given-names></name><name><surname>Vogt</surname><given-names>M</given-names></name><name><surname>Kireclioglu</surname><given-names>B</given-names></name><name><surname>Schneider</surname><given-names>J</given-names></name><name><surname>Kissler</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mapping the emotional face. How individual face parts contribute to successful emotion recognition</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0177239</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0177239</pub-id><pub-id pub-id-type="pmid">28493921</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Golarai</surname><given-names>G</given-names></name><name><surname>Caspers</surname><given-names>J</given-names></name><name><surname>Chuapoco</surname><given-names>MR</given-names></name><name><surname>Mohlberg</surname><given-names>H</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name><name><surname>Amunts</surname><given-names>K</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The mid-fusiform sulcus: a landmark identifying both cytoarchitectonic and functional divisions of human ventral temporal cortex</article-title><source>NeuroImage</source><volume>84</volume><fpage>453</fpage><lpage>465</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.068</pub-id><pub-id pub-id-type="pmid">24021838</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weliky</surname><given-names>M</given-names></name><name><surname>Bosking</surname><given-names>WH</given-names></name><name><surname>Fitzpatrick</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A systematic map of direction preference in primary visual cortex</article-title><source>Nature</source><volume>379</volume><fpage>725</fpage><lpage>728</lpage><pub-id pub-id-type="doi">10.1038/379725a0</pub-id><pub-id pub-id-type="pmid">8602218</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Harel</surname><given-names>N</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>High-field fMRI unveils orientation columns in humans</article-title><source>PNAS</source><volume>105</volume><fpage>10607</fpage><lpage>10612</lpage><pub-id pub-id-type="doi">10.1073/pnas.0804110105</pub-id><pub-id pub-id-type="pmid">18641121</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural decoding reveals impaired face configural processing in the right fusiform face area of individuals with developmental prosopagnosia</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>1539</fpage><lpage>1548</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2646-14.2015</pub-id><pub-id pub-id-type="pmid">25632131</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Q</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Luo</surname><given-names>YLL</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Resting-state neural activity across face-selective cortical regions is behaviorally relevant</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>10323</fpage><lpage>10330</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0873-11.2011</pub-id><pub-id pub-id-type="pmid">21753009</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zimmermann</surname><given-names>J</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>van de Moortele</surname><given-names>P-F</given-names></name><name><surname>Feinberg</surname><given-names>D</given-names></name><name><surname>Adriany</surname><given-names>G</given-names></name><name><surname>Chaimow</surname><given-names>D</given-names></name><name><surname>Shmuel</surname><given-names>A</given-names></name><name><surname>Uğurbil</surname><given-names>K</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Mapping the organization of axis of motion selective features in human area MT using high-field fMRI</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e28716</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0028716</pub-id><pub-id pub-id-type="pmid">22163328</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.70925.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Meng</surname><given-names>Ming</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>South China Normal University</institution><country>China</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.06.23.449598" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.06.23.449598"/></front-stub><body><p>How the brain is organized to represent various concepts has long been a central cognitive neuroscience research topic. Zhang and colleagues investigated the spatial distribution of feature tuning for different face-parts within face-selective regions of human visual cortex using ultra-high resolution 7.0 T fMRI. The findings complement non-human primate studies of face-selective patches and will be of interest to psychologists and system neuroscientists.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.70925.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Meng</surname><given-names>Ming</given-names></name><role>Reviewing Editor</role><aff><institution>South China Normal University</institution><country>China</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Silson</surname><given-names>Ed H</given-names></name><role>Reviewer</role><aff><institution>Edinburgh</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.06.23.449598">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.06.23.449598v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Spatial organization of face part representations within face-selective areas revealed by high-field fMRI&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by and Chris Baker as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Ed H Silson (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Tune down the claim from functional organization to functional preference. Typically, when we talk about the organization, it either has more than 2 subdivisions or it has a continuous representation about certain features. In this paper, the results are mainly about the comparison between two face parts, and they do not reveal other distinctive subareas showing preference to other face parts.</p><p>2) Clarify interpretation/motivation of the results: other than &quot;faciotopy&quot;, why would neurons that are tuning to different face parts be spatially clustered? Given that BOLD signals are indirect measurements of neural activity, can the results about fMRI voxels really suggest neuronal clustering? Would alternative interpretations such as activity synchronization be possible?</p><p>3) The inclusion of a non-face object condition was nice. However, it might be helpful to clarify the logic behind regressing out response patterns from whole faces or everyday objects. Wouldn't it make more sense to regress out response patterns from the comparison between whole faces and everyday objects, i.e. faces – objects?</p><p>4) It is currently unclear whether the current data are in full agreement with recent work (de Haas et al., 2021) showing similar face-part tuning within the OFA (or IOG) bilaterally. The current data suggest that feature tuning for eye and mouth parts progresses along the posterior-anterior axis within the right pFFA and right OFA. In this regard, the data are consistent. But de Haas and colleagues also demonstrated tuning for visual space that was spatially correlated (i.e. upper visual field representations overlapped upper face-part preferences and vice-versa). The current manuscript found little evidence for this correspondence within pFFA but does not report the data for OFA. For completeness this should be reported and any discrepancies with either the prior work, or between OFA and pFFA discussed.</p><p>5) It is somewhat challenging to fully interpret the responses to face-parts when they were presented at fixation and not in the typical visual field locations during real-world perception. For instance, we typically fixate faces either on or just below the eyes (Peterson et al., 2012) and so in the current experiment the eyes are in the typical viewing position, but the remainder of the face-parts are not (e.g. when fixating the eyes, the nose mouth and chin all fall in the lower visual field but in the current experimental paradigm they appear at fixation). Consideration of whether the reported face-part tuning would hold (or even be enhanced) if face-parts were presented in their typical locations should be included.</p><p>6) Although several experiments (including two controls) have been conducted, each one runs the risk of being underpowered (n ranges 3-10). One way to add reassurance when sample sizes are small is to include analyses of the reliability and replicability of the data within subjects through a split-half, or other cross-validation procedure. The main experiment here consisted of eight functional runs, which is more than sufficient for these types of analyses to be performed.</p><p>7) The current findings were only present within the right pFFA and right OFA. Although right lateralisation of face-processing is mentioned in the discussion, this is only cursory. A more expansive discussion of what such a face-part tuning might mean for our understanding of face-processing is warranted, particularly given that the recent work by de Haas and colleagues was bilateral.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>While I think in general this is a well-executed study with powerful high-field 7T fMRI, I still have a few suggestions for the authors to consider.</p><p>1. Clarify interpretation/motivation of the results: other than &quot;faciotopy&quot;, why would neurons that are tuning to different face parts be spatially clustered? Given that BOLD signals are indirect measurements of neural activity, can the results about fmri voxels really suggest neuronal clustering? Would alternative interpretations such as activity synchronization be possible?</p><p>2. The inclusion of a non-face object condition was nice. However, it might be helpful to clarify the logic behind regressing out response patterns from whole faces or everyday objects. Wouldn't it make more sense to regress out response patterns from the comparison between whole faces and everyday objects, i.e. faces – objects?</p><p>3. Testing functional connectivity profiles is a very interesting idea. Why not to take a look with the coverage of present data or perhaps datasets from openly available sources?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>1) Can the authors make it clear whether the analyses concerning the P-A axis were conducted across voxels (i.e. volume space) or across surface vertices (i.e. surface space).</p><p>2) An uncorrected threshold pf p&lt;0.01 for a functional localiser is non-standard and very liberal. Even with two-runs some form of reliability analysis can be performed.</p><p>3) Similarly, the main experiment comprises 8 functional runs. This amount of data can easily be split to perform reliability and reproducibility analyses. These would bolster the main claims as each experiment is relatively underpowered.</p><p>4) A pRF threshold of R2 = 0.02 (2%) is again very liberal. Can this selection be justified? Many prior pRF works even within FFA/PPA use a much higher threshold (~10-20%).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.70925.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Tune down the claim from functional organization to functional preference. Typically, when we talk about the organization, it either has more than 2 subdivisions or it has a continuous representation about certain features. In this paper, the results are mainly about the comparison between two face parts, and they do not reveal other distinctive subareas showing preference to other face parts.</p></disp-quote><p>We have followed the advice from the reviewer to tune down the claim of functional organization in our manuscript. To emphasize both the functional preferences to different face parts within face-selective regions and the consistent spatial profile across different individuals, we now use “spatial tuning of face parts” in the manuscript. The revision has been made in the manuscript.</p><disp-quote content-type="editor-comment"><p>2) Clarify interpretation/motivation of the results: other than &quot;faciotopy&quot;, why would neurons that are tuning to different face parts be spatially clustered? Given that BOLD signals are indirect measurements of neural activity, can the results about fMRI voxels really suggest neuronal clustering? Would alternative interpretations such as activity synchronization be possible?</p></disp-quote><p>The spatial clustering of neurons with similar tuning functions is one of the organization principles in the brain. Such clustering may optimize the efficiency of the neural computation by reducing the total wire length (Laughlin and Sejnowski, 2003), and is widely observed in the ventral pathway at different spatial scales (e.g., ocular dominance columns in V1, category-selective areas in VTC). Within category-selective regions like FFA, previous studies have found subpopulations with different response biases (Cukur et al., 2013; Weiner and Grill-Spector, 2010) or different connectivity profiles to other brain regions (Park et al., 2017), thus the clustering of neurons with similar feature tuning within face-selective regions could improve the processing efficiency to face stimuli.</p><p>Our results provide evidence from neural imaging data to support the voxel level neuronal clustering driven by the tuning to different face parts. Previous fMRI and single unit recording studies in monkey face processing network have demonstrated strong correspondence between fMRI signal and neuronal responses (Tsao et al., 2006), suggesting the face part tuning in our results may be driven by neuronal response biases. Here the reviewer has pointed out another possibility that the clustering could also reflect activity synchronization. These two mechanisms may interact to generate clustering at different spatial scales. We have added more discussion about this in the discussion session of the manuscript (Page 24).</p><disp-quote content-type="editor-comment"><p>3) The inclusion of a non-face object condition was nice. However, it might be helpful to clarify the logic behind regressing out response patterns from whole faces or everyday objects. Wouldn't it make more sense to regress out response patterns from the comparison between whole faces and everyday objects, i.e. faces – objects?</p></disp-quote><p>Regressing out the face and non-face object response pattern from the face part pattern is an important step in revealing the face part preferences in our results. We thank the reviewer for reminding us to clarify the logic behind this operation. The fMRI responses could be influenced by multiple factors other than neural responses, such as the distribution of the vein. While many studies have showed that the neural representations of faces and non-face objects are quite different in the pFFA, however, as shown in the Figure 3C in the manuscript, their spatial distributions of raw Β value are similar, which means that there is a shared factor driving the raw fMRI response patterns. Thus to eliminate such shared pattern from the patterns of different face parts, we regressed out the spatial patterns of the whole faces from patterns of each face part. Regression rather than subtraction was used here to avoid distorting the patterns specific to face parts. In addition, we regressed out response patterns of non-face objects, which further ensured that there was no distortion to the patterns specific to face parts. Both regression analyses yielded clear and similar response patterns of face parts. We have added more description about the logic behind the regression analysis in the Results session (Page 9).</p><p>Based on this logic, the pattern of “faces – objects” may not be a better regressor, as the shared patterns may be largely removed by the subtraction.</p><disp-quote content-type="editor-comment"><p>4) It is currently unclear whether the current data are in full agreement with recent work (de Haas et al., 2021) showing similar face-part tuning within the OFA (or IOG) bilaterally. The current data suggest that feature tuning for eye and mouth parts progresses along the posterior-anterior axis within the right pFFA and right OFA. In this regard, the data are consistent. But de Haas and colleagues also demonstrated tuning for visual space that was spatially correlated (i.e. upper visual field representations overlapped upper face-part preferences and vice-versa). The current manuscript found little evidence for this correspondence within pFFA but does not report the data for OFA. For completeness this should be reported and any discrepancies with either the prior work, or between OFA and pFFA discussed.</p></disp-quote><p>In the current study, three participants had data from both retinotopic mapping and face part mapping experiments. Consistent and robust part clustering were found in the right pFFA and right OFA. Following the reviewer’s suggestion, we analyzed these data for the right OFA and found the spatial patterns of eyes vs. mouths are similar to the patterns of visual field sensitivity on the vertical direction (i.e., upper to lower visual field), which are consistent with de Haas and colleagues’ findings. Note that we used more precise functional localization of OFA, while de Haas et al’s analysis was based on anatomically defined IOG, for which OFA is a part of. We have added this result in the Results session (Page 16), and also added a supplemental Figure 4—figure supplement 1.</p><disp-quote content-type="editor-comment"><p>5) It is somewhat challenging to fully interpret the responses to face-parts when they were presented at fixation and not in the typical visual field locations during real-world perception. For instance, we typically fixate faces either on or just below the eyes (Peterson et al., 2012) and so in the current experiment the eyes are in the typical viewing position, but the remainder of the face-parts are not (e.g. when fixating the eyes, the nose mouth and chin all fall in the lower visual field but in the current experimental paradigm they appear at fixation). Consideration of whether the reported face-part tuning would hold (or even be enhanced) if face-parts were presented in their typical locations should be included.</p></disp-quote><p>Our early visual cortex and some of the object-selective visual areas are sensitive to visual field locations. To dissociate the visual field tuning and face part tuning in face processing regions, in the main experiment of the current study the face part stimuli were presented at fixation to avoid the potential confounding contribution from visual field location. The spatial correlation between face part tuning and visual field tuning has been observed in posterior part of the face network (see response to point 4 above). It is unlikely that presenting the face parts at the fixation was responsible for the observed face part tuning. To directly test the role of stimulus location, we reanalyzed the data from control experiment 2 in which face parts were presented at their typical locations. Contrasting eyes above fixation vs. nose and mouth below fixation revealed similar anterior-posterior bias in the right pFFA, showing that the face part tuning in the right pFFA is invariant to the visual field location of stimuli. See comparison in <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>, note that the maps of eyes on top vs. nose and mouth on bottom are unsmoothed.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70925-sa2-fig1-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>6) Although several experiments (including two controls) have been conducted, each one runs the risk of being underpowered (n ranges 3-10). One way to add reassurance when sample sizes are small is to include analyses of the reliability and replicability of the data within subjects through a split-half, or other cross-validation procedure. The main experiment here consisted of eight functional runs, which is more than sufficient for these types of analyses to be performed.</p></disp-quote><p>Following the reviewer’s suggestion, we split the eight runs data from each participant in the main experiment into two data sets (odd-runs and even-runs), and estimated the eyes-mouth biases within each data set. Then we calculated the correlation coefficient between such biases across different voxels between the two data sets to estimate the reliability of the results in the right pFFA. The results demonstrate strong reliability of the data within participants. We have added these results in the Results session (Page 7 and Figure 2—figure supplement 1).</p><disp-quote content-type="editor-comment"><p>7) The current findings were only present within the right pFFA and right OFA. Although right lateralisation of face-processing is mentioned in the discussion, this is only cursory. A more expansive discussion of what such a face-part tuning might mean for our understanding of face-processing is warranted, particularly given that the recent work by de Haas and colleagues was bilateral.</p></disp-quote><p>The right lateralization of face-processing has been observed in face-selective network. Both the neural selectivity to faces (Kanwisher et al., 1997) and the decodable neural information of faces (Zhang et al., 2015) are higher in the right than in the left hemisphere. The neural clustering of face part tuning and consistent spatial patterns across individuals in the right rather than in the left face selective regions provides a potential computational advantage for right lateralization for face processing. The clustering of neurons with similar feature tuning have been found extensively in the ventral pathway, which may help to support a more efficient neural processing. Therefore, one of the neural mechanisms underlying the functional lateralization of face processing could be the existence of spatial clustering of face part tunings in the right hemisphere. We have added more discussion about the relevance between our results and lateralization of face processing (Page 25).</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>While I think in general this is a well-executed study with powerful high-field 7T fMRI, I still have a few suggestions for the authors to consider.</p><p>1. Clarify interpretation/motivation of the results: other than &quot;faciotopy&quot;, why would neurons that are tuning to different face parts be spatially clustered? Given that BOLD signals are indirect measurements of neural activity, can the results about fmri voxels really suggest neuronal clustering? Would alternative interpretations such as activity synchronization be possible?</p></disp-quote><p>Replied in to Essential Revisions point 2.</p><disp-quote content-type="editor-comment"><p>2. The inclusion of a non-face object condition was nice. However, it might be helpful to clarify the logic behind regressing out response patterns from whole faces or everyday objects. Wouldn't it make more sense to regress out response patterns from the comparison between whole faces and everyday objects, i.e. faces – objects?</p></disp-quote><p>Replied in Essential Revisions point 3.</p><disp-quote content-type="editor-comment"><p>3. Testing functional connectivity profiles is a very interesting idea. Why not to take a look with the coverage of present data or perhaps datasets from openly available sources?</p></disp-quote><p>We thought about testing functional connectivity profiles of each cluster within each face-selective region, but unfortunately the FOV of our current data is too limited (only covers part of the occipito-temporal cortex) and varied quite bit across subjects, thus making it difficult to perform such a connectivity analysis. It is also very difficult to find openly available sources that (1) have high spatial resolution, (2) cover the whole brain or at least a large part of the brain, and (3) have well defined face ROIs especially with face parts preference identified. In our future studies, we’ll keep this in mind to try to obtain data that could support such a connectivity analysis.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>1) Can the authors make it clear whether the analyses concerning the P-A axis were conducted across voxels (i.e. volume space) or across surface vertices (i.e. surface space).</p></disp-quote><p>We are sorry for the unclear description. The P-A axis was defined across surface vertices. We have clarified this information in the manuscript (Page 8).</p><disp-quote content-type="editor-comment"><p>2) An uncorrected threshold pf p&lt;0.01 for a functional localiser is non-standard and very liberal. Even with two-runs some form of reliability analysis can be performed.</p></disp-quote><p>The reason for choosing a liberal threshold of p&lt;0.01 for ROI identification is to allow voxels potentially with different sensitivities to different face parts to be included in the current analysis. As we show in the results, the neural responses to nose, hair, and chin were lower than the responses to eyes and mouth. A strict threshold would likely exclude some of the voxels with lower sensitivity to certain face parts. Thus we used a relatively liberal threshold to include them in the analyses. We would like to note that with this threshold, the identified face-selective ROIs were highly responsive to whole-faces (Figure 1B, gray bars), and very selective to faces (Figure 3C).</p><disp-quote content-type="editor-comment"><p>3) Similarly, the main experiment comprises 8 functional runs. This amount of data can easily be split to perform reliability and reproducibility analyses. These would bolster the main claims as each experiment is relatively underpowered.</p></disp-quote><p>The reliability analysis results are shown in the Figure 2—figure supplement 1, see also our response to Essential Revisions point 6.</p><disp-quote content-type="editor-comment"><p>4) A pRF threshold of R2 = 0.02 (2%) is again very liberal. Can this selection be justified? Many prior pRF works even within FFA/PPA use a much higher threshold (~10-20%).</p></disp-quote><p>Generally, the R2 of pRF fitting is lower for FFA than OFA, as OFA is spatially closer to the early visual cortex. In our study, with R2 threshold of 10%, the number of identified voxels is much less for pFFA than for OFA, which makes it hard to observe any pRF spatial pattern in pFFA. Thus we chose a lower threshold for pRF analysis. Considering the aim of pRF analysis in the current study is to examine the contribution from visual field sensitivity to the observed face part pattern, rather than to examine the existence of the visual field sensitivity in pFFA, we believe it would not change our conclusion that the spatial pattern of face part tuning in pFFA could not be explained by the visual field tuning in pFFA.</p><p>References:</p><p>Cukur, T., Huth, A. G., Nishimoto, S., and Gallant, J. L. (2013). Functional Subdomains within Human FFA. <italic>Journal of Neuroscience</italic>, <italic>33</italic>(42), 16748–16766. https://doi.org/10.1523/JNEUROSCI.1259-13.2013</p><p>Kanwisher, N., McDermott, J., and Chun, M. M. (1997). The fusiform face area: A module in human extrastriate cortex specialized for face perception. <italic>J Neurosci</italic>, <italic>17</italic>(11), 4302–4311.</p><p>Kim, N. Y., Lee, S. M., Erlendsdottir, M. C., and McCarthy, G. (2014). Discriminable spatial patterns of activation for faces and bodies in the fusiform gyrus. <italic>Frontiers in Human Neuroscience</italic>, <italic>8</italic>. https://doi.org/10.3389/fnhum.2014.00632</p><p>Laughlin, S. B., and Sejnowski, T. J. (2003). Communication in Neuronal Networks. <italic>Science</italic>, <italic>301</italic>(5641), 1870–1874. https://doi.org/10.1126/science.1089662</p><p>Park, S. H., Russ, B. E., McMahon, D. B. T., Koyano, K. W., Berman, R. A., and Leopold, D. A. (2017). Functional Subpopulations of Neurons in a Macaque Face Patch Revealed by Single-Unit fMRI Mapping. <italic>Neuron</italic>, <italic>95</italic>(4), 971-981.e5. https://doi.org/10.1016/j.neuron.2017.07.014</p><p>Tsao, D. Y., Freiwald, W. A., Tootell, R. B. H., and Livingstone, M. S. (2006). A Cortical Region Consisting Entirely of Face-Selective Cells. <italic>Science</italic>, <italic>311</italic>(5761), 670–674. https://doi.org/10.1126/science.1119983</p><p>Weiner, K. S., and Grill-Spector, K. (2010). Sparsely-distributed organization of face and limb activations in human ventral temporal cortex. <italic>NeuroImage</italic>, <italic>52</italic>(4), 1559–1573. https://doi.org/10.1016/j.neuroimage.2010.04.262</p><p>Zhang, J., Liu, J., and Xu, Y. (2015). Neural Decoding Reveals Impaired Face Configural Processing in the Right Fusiform Face Area of Individuals with Developmental Prosopagnosia. <italic>Journal of Neuroscience</italic>, <italic>35</italic>(4), 1539–1548. https://doi.org/10.1523/JNEUROSCI.2646-14.2015</p></body></sub-article></article>