<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">82819</article-id><article-id pub-id-type="doi">10.7554/eLife.82819</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Review Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>Transformer-based deep learning for predicting protein properties in the life sciences</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-290098"><name><surname>Chandra</surname><given-names>Abel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8497-028X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-290078"><name><surname>Tünnermann</surname><given-names>Laura</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-290100"><name><surname>Löfstedt</surname><given-names>Tommy</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-288551"><name><surname>Gratz</surname><given-names>Regina</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8820-7211</contrib-id><email>regina.gratz@slu.se</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05kb8h459</institution-id><institution>Department of Computing Science, Umeå University</institution></institution-wrap><addr-line><named-content content-type="city">Umeå</named-content></addr-line><country>Sweden</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02yy8x990</institution-id><institution>Umeå Plant Science Centre (UPSC), Department of Forest Genetics and Plant Physiology, Swedish University of Agricultural Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Umeå</named-content></addr-line><country>Sweden</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02yy8x990</institution-id><institution>Department of Forest Ecology and Management, Swedish University of Agricultural Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Umeå</named-content></addr-line><country>Sweden</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Dötsch</surname><given-names>Volker</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04cvxnb49</institution-id><institution>Goethe University</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Dötsch</surname><given-names>Volker</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04cvxnb49</institution-id><institution>Goethe University</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>18</day><month>01</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e82819</elocation-id><history><date date-type="received" iso-8601-date="2022-08-22"><day>22</day><month>08</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-01-06"><day>06</day><month>01</month><year>2023</year></date></history><permissions><copyright-statement>© 2023, Chandra et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Chandra et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-82819-v1.pdf"/><abstract><p>Recent developments in deep learning, coupled with an increasing number of sequenced proteins, have led to a breakthrough in life science applications, in particular in protein property prediction. There is hope that deep learning can close the gap between the number of sequenced proteins and proteins with known properties based on lab experiments. Language models from the field of natural language processing have gained popularity for protein property predictions and have led to a new computational revolution in biology, where old prediction results are being improved regularly. Such models can learn useful multipurpose representations of proteins from large open repositories of protein sequences and can be used, for instance, to predict protein properties. The field of natural language processing is growing quickly because of developments in a class of models based on a particular model—the Transformer model. We review recent developments and the use of large-scale Transformer models in applications for predicting protein characteristics and how such models can be used to predict, for example, post-translational modifications. We review shortcomings of other deep learning models and explain how the Transformer models have quickly proven to be a very promising way to unravel information hidden in the sequences of amino acids.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>deep learning</kwd><kwd>transformers</kwd><kwd>life sciences</kwd><kwd>protein property prediction</kwd><kwd>machine learning</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100007067</institution-id><institution>Kempestiftelserna</institution></institution-wrap></funding-source><award-id>JCK-2144</award-id><principal-award-recipient><name><surname>Löfstedt</surname><given-names>Tommy</given-names></name><name><surname>Gratz</surname><given-names>Regina</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100007067</institution-id><institution>Kempestiftelserna</institution></institution-wrap></funding-source><award-id>JCK-2015.1</award-id><principal-award-recipient><name><surname>Gratz</surname><given-names>Regina</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The recent developments in large-scale machine learning, especially with the recent Transformer models, display much potential for solving computational problems within protein biology and outcompete traditional computational methods in many recent studies and benchmarks.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1"><title>Computational protein property prediction</title><p>Proteins have properties that could either be global or local, that is, wholistic protein properties (e.g. stability of a protein) or regional protein properties (e.g. phosphorylation of an amino acid residue by a protein kinase). These different protein properties are usually determined through wet lab experiments, which can be challenging, time-consuming, and costly. The change in protein stability based on changes in protein sequence, for example, requires measuring the change in Gibbs free energy of folding of the purified wild-type and mutant proteins (<xref ref-type="bibr" rid="bib140">Walls and Loughran, 2017</xref>). Even though this experimental procedure provides direct understanding of protein stability, much time and high costs are involved, especially when multiple mutations in a sequence need to be analysed. This has driven interest into computational methods to guide mutation analysis and design (<xref ref-type="bibr" rid="bib100">Pan et al., 2022</xref>). Employing a computational approach can also aid the experimental approach by providing a ranked list of predictions for a property (e.g. to predict the likelihood of interaction between two given protein sequences) that can be experimentally verified or refuted by scientists in focused experimental testing, which can save much time and other resources (<xref ref-type="bibr" rid="bib40">Ehrenberger et al., 2015</xref>).</p><p>There has been an exponential growth in the number of protein sequences collected in public repositories using high-throughput technologies. However, the gap between the number of sequenced proteins and the number of protein property annotations continues to widen (<xref ref-type="bibr" rid="bib130">UniProt Consortium, 2019</xref>; <xref ref-type="bibr" rid="bib133">Varadi et al., 2022</xref>). Recently, machine learning (ML) methods, in general, and large-scale deep learning (DL) methods, in particular, have gained much attention due to their ability to extract complex patterns from large collections of protein data (<xref ref-type="bibr" rid="bib115">Shi et al., 2021</xref>; <xref ref-type="bibr" rid="bib82">Li et al., 2022</xref>) to automatically predict protein properties. There is now a vast and growing number of applications of DL methods used in the proteomic field that assist in building knowledge about various protein properties.</p><p>Since recent large-scale DL models have played a crucial role in computational protein property prediction (<xref ref-type="bibr" rid="bib11">Bileschi et al., 2022</xref>), we describe in this review the most common DL architectures in use today. DL methods, especially those coming from the field of natural language processing (NLP), are gaining popularity, and we therefore discuss DL methods in the context of NLP. We denote such models as <italic>language models</italic>. Further, we explain how language models relate and have been adopted to analyse protein sequences. Various language models have been developed in the protein area, and we highlight some recent examples where they have been used to predict protein properties in the life sciences. In particular, we discuss and explain the Transformer model, which has managed to overcome several of the shortcomings of previous methods. We further provide a proof-of-principle example, where we predict a post-translational modification (PTM) in proteins. PTMs are a common way of changing a protein’s functionality and are often associated with regulatory cascades and cellular signaling. PTMs can be determined in wet lab settings, for example, with mass spectrometry, but can also be predicted using computational approaches. In our proof-of-principle example, we set out to predict whether lysine residues in proteins are phosphoglycerylated or not. To do this, we compared the prediction performance when using traditional protein features, determined by the analyst, to the performance when using features automatically found using two types of Transformer models. By feature here, we mean, for instance, some description of a protein, a statistic, or a measurement. Finally, we discuss the future of protein property prediction and predict that Transformer-like models will be the standard approach for many computational biology and bioinformatics tasks in the near future.</p></sec><sec id="s2"><title>A brief introduction to deep learning</title><p>ML is a subarea of artificial intelligence (AI), and much of the recent developments within the field of AI come from progress made within ML. The aim of ML is to use data to solve a task, for instance, to predict a specific protein property based on measurements, that is<italic>,</italic> data, from other proteins where those properties are known. Most of the recent ML developments have been made within DL, a subarea of ML. However, NLP is also a subfield of AI, where the aim is to use computers to analyse and understand natural language, which is naturally evolved human language, and often uses ML to process and analyse text data. There is an overlap between ML/DL and NLP, however, and ideas flow both ways between the fields. Recently, developments in NLP have been driving much of the development within all of ML and DL.</p><p>DL methods are often based on deep artificial neural network models, a class of ML models that are very flexible in the sense that they are able to model very complicated relationships between the measurements (the input data, such as amino acid sequences) and the quantities to be predicted (such as a protein property). The main advantage of neural network models is that they can automatically learn rich feature representations, and they do that directly from large unstructured input data. This means, for instance, that they can take variable-length protein sequences as inputs and automatically find a way to represent them as a fixed-length real (floating-point) vector, where the learned representations contain all the relevant information from the protein sequences that is necessary to solve a particular prediction problem. Having found such a representation, these models also automatically perform a traditional machine learning tasks in the newly learnt representation, such as classification or regression (<xref ref-type="bibr" rid="bib20">Charte et al., 2019</xref>). Some examples of machine learning tasks are illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref>. In contrast, traditional ML models typically rely on input features determined by the analyst, which are computed from the raw unstructured data, after which a model is determined using those features in a second step. DL models are instead end-to-end systems, meaning that there is only a single step where they automatically learn to map directly from the input data, through an internal learned representation, the automatically determined features, to the target quantities that we want to predict, and they do this with unprecedented accuracy (<xref ref-type="bibr" rid="bib72">Khan et al., 2019</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Two common prediction tasks in machine learning (ML) are classification and regression.</title><p>For illustration purpose, two-dimensional plots are used, but in reality, the dimensions are much higher. (<bold>A</bold>) Binary classification tasks are for samples that can be separated into two groups, called classes. For instance, the samples can be several features of some proteins, where each protein is associated with one of two classes. A protein variant could either be stable or unstable (<xref ref-type="bibr" rid="bib46">Fang, 2020</xref>) or a lysine residue could be phosphoglycerylated or non-phosphoglycerylated (<xref ref-type="bibr" rid="bib19">Chandra et al., 2020</xref>). The ML task would be to build a model that can determine the class for a new sample. (<bold>B</bold>) The multiclass classification task is performed when the proteins belong to one of multiple classes. For instance, predicting which structural class a protein belongs to <xref ref-type="bibr" rid="bib26">Chou and Zhang, 1995</xref>. (<bold>C</bold>) The regression task is for applications where we want to predict real output values, for example, the brightness of a fluorescent protein (<xref ref-type="bibr" rid="bib88">Lu et al., 2021</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82819-fig1-v1.tif"/></fig><p>The <italic>architecture</italic> of a neural network model is the network layout and its components, such as the number of artificial neurons in each layer (the number of computational units in the layer), the number of layers (the number of levels of abstractions it can learn), and the type of connections between these layers (which layers are connected to which). The architecture of the network governs the overall behaviour of the neural network, what it can learn, and what assumptions about the data are built in.</p><p>There are many types of neural network models that are made for analysing different types of data. Some of the most well-known and successful types of neural network models include multilayer perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs) (<xref ref-type="bibr" rid="bib75">Koumakis, 2020</xref>; see illustrations in <xref ref-type="fig" rid="fig2">Figure 2</xref>). These models have been used by themselves, as well as in combination as hybrid models. Examples include work on protein fold recognition, which used a CNN for feature extraction together with an RNN model (<xref ref-type="bibr" rid="bib85">Liu et al., 2020</xref>), and the popularly used Word2Vec model that can provide embeddings for words to be used in language processing by neural networks, as in the continuous bag-of-words model and the continuous skip-gram model (<xref ref-type="bibr" rid="bib91">Mikolov et al., 2013a</xref>). MLPs are characterized by an input layer that accepts any type of inputs, for instance, features computed from proteins, several possible interconnected so-called hidden layers, and an output layer with the predicted value, such as a protein property. CNNs use convolution operations, or, technically, they use what is called linear spatial operation, in at least one of their layers. The convolution layers learn filters that detect features, or patterns, in the input signals. The filters automatically capture features in a local region, called the receptive field, and in order to capture feature more distantly, CNNs learn a hierarchy of features. The pooling operation after the convolution layer reduces the dimensionality of the captured features. CNNs have been very successful when analysing image and audio input data, and are very common in computer vision for analysing images, but have also been used to analyse protein sequence data (for instance, DNA-protein binding; <xref ref-type="bibr" rid="bib162">Zeng et al., 2016</xref>). RNN models have gained much attention since they were first introduced (<xref ref-type="bibr" rid="bib43">Elman, 1990</xref>) and have been applied widely in many NLP tasks, such as speech recognition (<xref ref-type="bibr" rid="bib90">Mikolov et al., 2011</xref>). They are suitable for modelling sequential data such as text or time series data, but can also model DNA and protein sequences. The elements in the sequence, for example, words or amino acids, are processed step-by-step one at a time using so-called recurrent connection units, where the output of each step depends on both the current and the previous steps. Common RNN models include the long short-term memory (LSTM) model (<xref ref-type="bibr" rid="bib62">Hochreiter and Schmidhuber, 1997</xref>) and the gated recurrent units (GRUs) (<xref ref-type="bibr" rid="bib29">Chung et al., 2014</xref>). There are also models that learn both forwards and backwards, such as the bidirectional LSTM model, BiLSTM (<xref ref-type="bibr" rid="bib64">Huang et al., 2015</xref>), which uses two LSTM models to capture information from a sequence in both directions. RNNs can model the contextual dependencies in language and were preferred over MLPs and CNNs for most NLP tasks for a long time (<xref ref-type="bibr" rid="bib157">Yin et al., 2017</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Three well-known deep learning models.</title><p>(<bold>A</bold>) Multilayer perceptrons (MLPs) are characterized by an input layer, several hidden layers, and an output layer. (<bold>B</bold>) Convolutional neural networks (CNNs) use convolution operations in their layers and learn filters that automatically extract features from the input sequences (e.g. from images, audio signals, time series, or protein sequences). At some point, the learned image features are strung out as a vector, called flattening, and are often passed on to fully connected layers at the end. (<bold>C</bold>) A recurrent neural network (RNNs) is a model that processes an input sequence step-by-step with one element in the sequence at a time.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82819-fig2-v1.tif"/></fig></sec><sec id="s3"><title>Basics of natural language processing</title><p>The field of NLP was founded in the 1950s and today covers a wide range of applications, such as sentiment analysis (extracting subjective qualities, like emotions, from text), named entity recognition (classify named entities from text, such as places or person names), machine translation, or question answering. Early NLP systems comprised hand-coded and rule-based assessments (grammar rules) by humans that were then encoded into special-purpose algorithms to predict some property of the sentence. This, however, produced unsatisfactory results and generally failed to deliver when applied to larger text volumes. More recent NLP systems often utilize DL models to automatically learn to solve natural language tasks based on very large volumes of raw, unstructured, and unlabelled text datasets.</p><p>To perform an NLP task, the input text data must be pre-processed to be in a form suitable for automation. The first steps in this process involve splitting the text up into either sentences, words, or parts of words. This process includes a step called tokenization, and the units the text is broken down into are called tokens. These tokens are translated into numerical representations called input embeddings, such as one-hot encoding, count vectors, or word embeddings (<xref ref-type="bibr" rid="bib142">Wang et al., 2018</xref>), as illustrated in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. Word embeddings are the most common real-valued vector representations of the tokens (<xref ref-type="bibr" rid="bib81">Levy and Goldberg, 2014</xref>) and are often automatically learned (<xref ref-type="bibr" rid="bib128">Turian and Ratinov, 2010</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Illustrations of embeddings and of next and masked token predictions.</title><p>(<bold>A</bold>) An illustration of real-valued vector representations (input embeddings) of the tokens for a sample sentence. Each square represents a numerical value in the vector representation. The vector for each word in the sentence is obtained by looking up the unique ID attributed to the word with the ID in a vocabulary. Each word embedding is of the same size, called the embedding size, and they must be found in the vocabulary (in the illustration, the vocabulary size is 10,000 words). (<bold>B</bold>) The two main training approaches for protein language models, and specifically for Transformers. The top part illustrates autoregressive language modelling (predicting the next token), and the bottom part illustrates masked language modelling (predict a few missing, or masked tokens).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82819-fig3-v1.tif"/></fig><p>The ability of DL methods to automatically learn feature representations of the input data significantly reduces the need for manual specification or extraction of features by natural language experts. Meaningful information can be extracted from unstructured data using DL methods at a fraction of the time and cost and also often considerably better than human experts (<xref ref-type="bibr" rid="bib158">Young et al., 2018</xref>; <xref ref-type="bibr" rid="bib97">Nauman et al., 2019</xref>). The most recent NLP methods, based on a particular model called the Transformer, learn feature representations automatically through the process of unsupervised learning, often called self-supervised learning (<xref ref-type="bibr" rid="bib36">Devlin et al., 2018</xref>; <xref ref-type="bibr" rid="bib102">Peters et al., 2018</xref>).</p><p>Most NLP tasks today are solved using DL methods based on the Transformer, with existing methods constantly being improved and new methods proposed (<xref ref-type="bibr" rid="bib106">Raffel et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Brown et al., 2020</xref>; <xref ref-type="bibr" rid="bib60">Heinzinger et al., 2021</xref>). Recent models are also trained on text data of ever-increasing sizes, which have made them perform even better.</p></sec><sec id="s4"><title>The Transformer model</title><p>The Transformer model was introduced in 2017 by <xref ref-type="bibr" rid="bib134">Vaswani et al., 2017</xref> and achieved state-of-the-art results in language translation using only a fraction of the previous training times. It is an encoder–decoder type of model (see <xref ref-type="fig" rid="fig4">Figure 4A</xref> for the general idea of an encoder–decoder model), where the encoder maps the vector representations of the tokens from an input text (the input embeddings) to an internal representation. The decoder then uses the internal representation and maps it to the output sequences (the target language, for instance). Compared to contemporary models at the time, the Transformer model did not use recurrent layers, nor did it use convolution layers—instead, it used an architecture component called <italic>attention</italic>. The attention module enables a model to consider the interactions among every pair of tokens in a sequence and automatically learn the relationships between tokens in a sequence that are relevant for the task at hand (<xref ref-type="bibr" rid="bib24">Cheng et al., 2021</xref>). There are many kinds of attention modules, but most of them, and the one used by Vaswani et al., automatically learn an interaction pattern between pairs of tokens. These interaction patterns give an importance weight to each input token for the prediction task at hand and allows the model to learn dependencies between tokens far apart in the input sequence. In most cases, not just one but several such attention modules are used in parallel, allowing the model to learn multiple different aspects of the relationships between input tokens—this is called multihead attention.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>An illustration of sequence-to-sequence models and of how to use the internal representations for down-stream machine learning tasks.</title><p>(<bold>A</bold>) The conceptual idea behind sequence-to-sequence models. The Transformer model by <xref ref-type="bibr" rid="bib134">Vaswani et al., 2017</xref> has a similar form, to map the input sequence to an output sequence using an encoder and a decoder. (<bold>B</bold>) An example application of the Transformer language model for protein property prediction. The input embedding is contextualized using the encoder block, which gives an internal representation, the model’s embedding of the input sequence. The internal representation is then used as features of the amino acids and can be passed in a second step to a machine learning model. The decoder block is not normally used after training since it does not serve much purpose in protein property prediction but is a critical component for training in natural language processing (NLP) applications such as language translation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82819-fig4-v1.tif"/></fig><p>The Transformer model by Vaswani et al. comprises multihead attention (eight parallel attention heads) and fully connected feed-forward networks (these networks are in fact MLP models, used as intermediate components in the overall Transformer model) in six layers of both the encoder and decoder blocks. To generate the input embeddings to the model, the authors used two schemes: encoding the sentences using byte-pair encoding (<xref ref-type="bibr" rid="bib13">Britz et al., 2017</xref>) or splitting tokens into word-piece vocabulary (<xref ref-type="bibr" rid="bib151">Wu et al., 2016</xref>), based on the training dataset. The model’s embedding layers produce contextualized embeddings (the internal representations) of size 512 (per token). The multihead attention in the different layers of each block of the network enabled the model to learn rich and useful representation by considering information from tokens at different positions in the input sequence.</p><p>The procedure used to train such models is called self-supervision and is typically one of the following two approaches: (1) to predict the next token in a sequence, given the previous tokens (<xref ref-type="bibr" rid="bib102">Peters et al., 2018</xref>) (this is called autoregressive language modelling), or (2) predict 'masked' tokens, where certain tokens are removed (typically 15% in an input sequence), and the model is made to predict them using the information available in the unmasked tokens in the sequence (this is called masked language modelling [MLM]) (<xref ref-type="bibr" rid="bib36">Devlin et al., 2018</xref>). These two types of training approaches are illustrated in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. The approach originally employed, already before the Transformer models, was to predict the next token in the input sequence. Transformers trained using the MLM approach have become very popular and successful, likely because it allows the model to consider the whole input sequence directly instead of everything up until the present point in the sequence (<xref ref-type="bibr" rid="bib95">Nambiar et al., 2020</xref>; <xref ref-type="bibr" rid="bib44">Elnaggar et al., 2020a</xref>; <xref ref-type="bibr" rid="bib111">Rives et al., 2021</xref>; <xref ref-type="bibr" rid="bib12">Brandes et al., 2021</xref>; <xref ref-type="bibr" rid="bib110">Rao et al., 2021</xref>; <xref ref-type="bibr" rid="bib58">He et al., 2021</xref>).</p><p>Many other models, based on the Transformer model, have been proposed after the introduction of the original Transformer. These models all have attention modules as their core components. An example is the BERT model (<xref ref-type="bibr" rid="bib36">Devlin et al., 2018</xref>), which is an encoder model that has achieved outstanding performance compared with other language models on many NLP tasks, such as machine translation, question answering, etc. (<xref ref-type="bibr" rid="bib29">Chung et al., 2014</xref>; <xref ref-type="bibr" rid="bib24">Cheng et al., 2021</xref>). The BERT model attained new state-of-the-art results on 11 NLP tasks. The work demonstrated that bidirectional pre-training is important for language representations and that the pre-trained model can be adapted to many other specific tasks, which is relatively inexpensive compared to building separate models for each individual task. There were two primary models developed: BERT<sub>BASE</sub> (with 12 layers, 12 attention heads, and 768-dimensional contextual embeddings) and BERT<sub>LARGE</sub> (24 layers, 16 attention heads, and 1024-dimensional contextual embeddings). The authors found that the BERT<sub>LARGE</sub> results surpassed the results of BERT<sub>BASE</sub> on all the tasks, which indicates the importance of the model size for the performance.</p><p>Because of the recent successes of DL-based language models in a myriad of NLP tasks, and particularly so when using the Transformer model, there has been an increased interest in such models for applications in other fields, such as in computational biology and bioinformatics. In these fields, NLP models can be applied to sequences of, for example, genomic or proteomic data, and recent results indicate that this is a highly successful approach for protein prediction applications (<xref ref-type="bibr" rid="bib25">Choromanski et al., 2020</xref>).</p><p>DL models are known to be computationally expensive and to take considerable amount of time to train. The Transformer models, however, avoid some of the challenges associated with traditional DL methods for sequence modelling.</p><p>For instance, RNN models capture information from previous positions in an input sequence, advancing from the beginning of the sequence (forward). But in doing so, they do not capture any input sequence context from the other side of the current position in the sequence. They also suffer from some fundamental problems (called the vanishing and exploding gradient problems) (<xref ref-type="bibr" rid="bib7">Bengio et al., 1994</xref>; <xref ref-type="bibr" rid="bib101">Pascanu et al., 2013</xref>; <xref ref-type="bibr" rid="bib56">Hanin, 2018</xref>), which makes them difficult to train (<xref ref-type="bibr" rid="bib32">Dai et al., 2018</xref>). The effect of this is that RNN models have problems to learn relationships between distant tokens in an input sequence (<xref ref-type="bibr" rid="bib7">Bengio et al., 1994</xref>; <xref ref-type="bibr" rid="bib101">Pascanu et al., 2013</xref>; <xref ref-type="bibr" rid="bib56">Hanin, 2018</xref>). Also, since the data is processed one token at a time, it is not possible to parallelize the computations, making the training slow (<xref ref-type="bibr" rid="bib143">Wang et al., 2019</xref>).</p><p>For a CNN to capture distant features in the input, they need to learn a hierarchy of features. It may take many such hierarchy levels to extract meaningful information from a larger part of an input sequence (<xref ref-type="bibr" rid="bib107">Raghu et al., 2021</xref>), which can make CNNs slow. CNNs are also invariant to spatial translations and do therefore not utilize the positional information that may be relevant in an input sequence (<xref ref-type="bibr" rid="bib3">Albawi et al., 2017</xref>). While CNNs have had a remarkable success on, for example., image data, they have not been as successful in sequence modelling.</p><p>The Transformer models solve many of the hurdles faced by conventional DL approaches, some of which were described above. The Transformer model’s attention module allows each token to influence weights for every other token in the sequence. This allows the Transformer model to attend to long-range dependencies between input tokens, a very beneficial property since it enables Transformers to consider the whole context of an input sequence (<xref ref-type="bibr" rid="bib35">Dehghani et al., 2018</xref>). As a result, they obtain superior results and sequence embeddings (<xref ref-type="bibr" rid="bib135">Väth et al., 2022</xref>). The direct connections between distant tokens also help when training the Transformer models, making it easy to train them (<xref ref-type="bibr" rid="bib32">Dai et al., 2018</xref>). The Transformer models are also highly parallelizable, and only have simple components such as attention modules and fully connected layers, which makes them computationally attractive (<xref ref-type="bibr" rid="bib143">Wang et al., 2019</xref>).</p></sec><sec id="s5"><title>Protein language models and representation learning</title><p>The models used in the field of NLP can thus also be used to learn and understand protein sequences and in this context, they are commonly referred to as <italic>protein language models</italic> (<xref ref-type="bibr" rid="bib59">Heinzinger et al., 2019</xref>). While there are abstract similarities between sentences and protein sequences, there are of course major differences in their properties, syntax, and semantics (<xref ref-type="bibr" rid="bib98">Ofer et al., 2021</xref>). When handling proteins, a word can be one of the individual twenty canonical amino acids (excluding unconventional and rare amino acids) <xref ref-type="bibr" rid="bib87">Lopez and Mohiuddin, 2020</xref> found in the genetic code or it could be a number of these amino acids grouped together, while a protein sequence would correspond to a sentence (<xref ref-type="bibr" rid="bib48">Ferruz et al., 2022</xref>; <xref ref-type="bibr" rid="bib41">ElAbd et al., 2020</xref>). A word being individual amino acids is the most common approach, and other alternatives do not appear to have been explored much (<xref ref-type="bibr" rid="bib98">Ofer et al., 2021</xref>). Just like with natural language, protein sequences contain long-range dependencies, making them excellent candidates for analysis by recent NLP models such as Transformers (<xref ref-type="bibr" rid="bib98">Ofer et al., 2021</xref>).</p><p><xref ref-type="fig" rid="fig4">Figure 4B</xref> illustrates how a Transformer language model can be applied to protein sequences. The encoder maps the amino acid tokens of an input protein sequence to an internal representation (the model’s embedding of the protein sequence). This internal representation is then used as a feature vector that represents the protein sequence and is passed on to a conventional machine learning model for classification or regression, for instance. For clarity, we will denote this internal representation the <italic>representation</italic> of a protein sequence in a given protein language model.</p><p>For properties of proteins, such as their 3D structure, the mapping from a sequence of amino acids to the corresponding 3D structure is quite challenging (<xref ref-type="bibr" rid="bib77">Kuhlman and Bradley, 2019</xref>; <xref ref-type="bibr" rid="bib66">Jiang et al., 2017</xref>), but there is typically an abundance of sequenced proteins openly available that a DL model can make use of. The largest open sources of protein sequence information and data are the Universal Protein Resource (UniProt) (<xref ref-type="bibr" rid="bib129">UniProt, 2021</xref>), Pfam (<xref ref-type="bibr" rid="bib94">Mistry et al., 2021</xref>), and the Big Fantastic Database (BFD) (<xref ref-type="bibr" rid="bib10">BFD, 2022</xref>). UniProt contains around 0.567M sequences that are reviewed and manually annotated and more than 230M sequences that are automatically annotated. The Pfam is a database containing protein families, where a family is determined by similar functional domains. Pfam currently has 19,632 families and clans (higher-level groupings), as per Pfam 35.0, and contains a total of 61M sequences (<xref ref-type="bibr" rid="bib103">Pfam 35.0, 2021</xref>). BFD is a very large collection of protein families publicly available. It comprises 65.9M families covering more than 2B protein sequences and was built using UniProt, and a reference protein catalogue (<xref ref-type="bibr" rid="bib121">Steinegger et al., 2019</xref>), clustered to 30% sequence identity. Protein language models are typically trained on such large open collections of protein data. The table in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> gives an overview of some of the pre-trained Transformer language models available in the literature.</p><p>Using self-supervised training procedures on large databases of proteins, protein language models are able to learn very complex relationships and patterns in protein sequences through the global biome and across time.</p><p>Large protein language models, trained using one of the two approaches described above (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) on very large databases of protein sequences, are used for downstream applications using one of two common approaches (<xref ref-type="bibr" rid="bib80">Laskar et al., 2020</xref>). The first, called feature-based, is where a model is trained in a self-supervised manner, for instance, using one of the two approaches illustrated in <xref ref-type="fig" rid="fig3">Figure 3B</xref>, without any labels. The trained model’s representation of each protein sequence is then considered a feature vector for a protein that can directly be used for downstream protein prediction tasks. This is called pre-training and is what we used in the post-translational modification example in the section ‘A proof-of-principle example’ below. The second, called fine-tuning, is where a model is trained first in a self-supervised manner without any labels for the protein sequences, and then updated, or fine-tuned, using protein sequences with labels of interest. After that, the model’s fine-tuned protein sequence representations are used for downstream prediction tasks.</p></sec><sec id="s6"><title>Solving protein prediction tasks using transformers</title><p>Protein prediction tasks for which the Transformer has been used include predictions of protein structure, protein residue contact, protein–protein interactions (PPI), drug–target interactions (DTI), PTMs, and homology studies. The task can either be local (sites of interest within the sequence) or global (entire sequence). The fixed size Transformer representation for the local task can be obtained by taking a fixed window around the sites of interest, while the fixed size representation of a protein for a global task is achieved, for instance, by averaging the residue vectors to yield the protein sequence vector (<xref ref-type="bibr" rid="bib135">Väth et al., 2022</xref>). It remains to be seen on which protein problems the Transformer models do not perform so well since the use of Transformer models is still spreading, and they are being used to solve more and more protein prediction tasks. Most of the state-of-the-art techniques for such predictions have been based on features from profile-to-profile comparison created from multiple sequence alignments (MSAs) of proteins using tools such as PSI-BLAST (<xref ref-type="bibr" rid="bib4">Altschul et al., 1997</xref>) and HMMER (<xref ref-type="bibr" rid="bib49">Finn et al., 2011</xref>). A protein profile is built by converting MSAs into a scoring system of amino acid positions based on their frequency of occurrence and is used to model protein families and domains. However, such techniques have limitations due to the existence of gaps in the protein sequences stemming from deletions and insertions (<xref ref-type="bibr" rid="bib51">Golubchik et al., 2007</xref>) and work unsatisfactory for sequences having few to no homologs to generate MSA and profiles (<xref ref-type="bibr" rid="bib104">Phuong et al., 2006</xref>). Moreover, predicted structural features, such as secondary structure, have also been popular when developing predictive models, but they suffer a limitation since the structural information problem is yet to be solved, which results in imperfect features (<xref ref-type="bibr" rid="bib123">Sułkowska et al., 2012</xref>; <xref ref-type="bibr" rid="bib113">Schmiedel and Lehner, 2019</xref>).</p><p>The results obtained using Transformer models on such tasks have been quite promising, and without the use of MSA tools that require homologous sequences, and also without structural information. A recent framework introduced by <xref ref-type="bibr" rid="bib28">Chowdhury et al., 2022</xref>, which has a Transformer-based language model at its core, outperformed AlphaFold2 (<xref ref-type="bibr" rid="bib69">Jumper et al., 2021</xref>), an MSA-based approach, in structure prediction for sequences that lack homologs. There are Transformer models that utilize evolutionary information extracted from MSAs during the pre-training stage, but pre-training is mostly done as a one-off process, and representation for new proteins is extracted using only the pretrained hidden states of the Transformer models. MSA tools generate alignment by searching homologs from the entire UniProt database, time-consuming (<xref ref-type="bibr" rid="bib63">Hong et al., 2021</xref>) process, whereby generating embeddings using protein language models is less cumbersome but it also builds richer and more complete features for low homologous proteins (<xref ref-type="bibr" rid="bib147">Wang et al., 2022</xref>).</p><p>In the following, we summarize typical problems from different fields of the life sciences, for which Transformer models have been used to aid in the prediction of protein properties. Most of these works employ pre-trained Transformer models to generate protein representations that can be used in downstream tasks for predictions.</p><sec id="s6-1"><title>Structure prediction</title><p>A fundamental task that has been pursued for decades is to predict a protein’s structure. The structure is encoded in a protein’s amino acid composition, and both the composition and the structure can determine a protein’s function (<xref ref-type="bibr" rid="bib69">Jumper et al., 2021</xref>). The protein structure prediction task can be broken down into two categories: secondary structure (α-helix, β-sheet, or coil) and tertiary structure (3D shape). These major tasks can further be broken down into other prediction tasks. For instance, predictions can be carried out to find 2D contacts, which can then be employed successively for 3D structure prediction since two residues in a sequence can be spatially close to each other in the 3D configuration (<xref ref-type="bibr" rid="bib39">Du et al., 2021</xref>). Protein contact prediction can be formulated as either a binary classification problem (whether two residues have a close distance between their central carbon atoms), a multiclass classification problem (encapsulating real distance predictions by dividing the distance measurements into discrete bins), or as a regression problem (predicting real-valued distances). The tasks of secondary structure prediction (<xref ref-type="bibr" rid="bib44">Elnaggar et al., 2020a</xref>; <xref ref-type="bibr" rid="bib111">Rives et al., 2021</xref>; <xref ref-type="bibr" rid="bib12">Brandes et al., 2021</xref>; <xref ref-type="bibr" rid="bib110">Rao et al., 2021</xref>; <xref ref-type="bibr" rid="bib109">Rao et al., 2019</xref>; <xref ref-type="bibr" rid="bib45">Elnaggar et al., 2020b</xref>; <xref ref-type="bibr" rid="bib122">Sturmfels et al., 2020</xref>) and contact prediction <xref ref-type="bibr" rid="bib111">Rives et al., 2021</xref>; <xref ref-type="bibr" rid="bib110">Rao et al., 2021</xref>; <xref ref-type="bibr" rid="bib58">He et al., 2021</xref>; <xref ref-type="bibr" rid="bib122">Sturmfels et al., 2020</xref> have been undertaken using multiple different Transformer models, and they show great promise. For example, <xref ref-type="bibr" rid="bib111">Rives et al., 2021</xref> predicted secondary structure and contact by training a neural network classifier using sequence profile features combined with the representation from their ESM-1b Transformer model. They evaluated the feature combination on the Critical Assessment of protein Structure Prediction (CASP) test set (<xref ref-type="bibr" rid="bib76">Kryshtafovych et al., 2019</xref>), and the results show an improved performance compared with other models. Other works on contact predictions include utilizing the feature combination of one-hot encoding, SPOT-1D-Single (<xref ref-type="bibr" rid="bib116">Singh et al., 2021</xref>), and the representation from ESM-1b (<xref ref-type="bibr" rid="bib111">Rives et al., 2021</xref>) to train a neural network classifier. This showed improvements over evolutionary-profile-based methods and over using ESM-1b representation alone (<xref ref-type="bibr" rid="bib117">Singh et al., 2022</xref>). Moreover, a novel Transformer was pre-trained and utilized the CASP14 benchmark (<xref ref-type="bibr" rid="bib76">Kryshtafovych et al., 2019</xref>) for contact prediction that outperformed the winner group of CASP14 contact prediction challenge (<xref ref-type="bibr" rid="bib163">Zhang et al., 2021</xref>).</p></sec><sec id="s6-2"><title>Homology prediction</title><p>In homology prediction, a non-annotated protein with unknown biological function is characterized by finding evolutionary related sequences with known function (<xref ref-type="bibr" rid="bib53">Gromiha et al., 2019</xref>). In microbiology and medicine, detection of remote homologs is of great interest, for instance, to detect emerging antibiotic-resistant genes (<xref ref-type="bibr" rid="bib125">Tavares et al., 2013</xref>). The conventional approach for homology prediction has been to use MSAs, where computational tools such as MMseqs2 (<xref ref-type="bibr" rid="bib120">Steinegger and Söding, 2017</xref>), Pfam profile (<xref ref-type="bibr" rid="bib42">ElGebali et al., 2019</xref>), and PSI-BLAST (<xref ref-type="bibr" rid="bib4">Altschul et al., 1997</xref>) align evolutionary related protein positions by deducing conserved sequence patterns based on evolutionary constraints that maintain the sequence’s structure and function. A major issue with these tools is that they fail to determine sequences that are distantly related (remote homology) (<xref ref-type="bibr" rid="bib149">Wilburn and Eddy, 2020</xref>). A new method was introduced by <xref ref-type="bibr" rid="bib161">Zare-Mirakabad et al., 2021</xref> that utilizes a pre-trained Transformer called ProtAlbert (<xref ref-type="bibr" rid="bib45">Elnaggar et al., 2020b</xref>) to predict a protein’s profile. To predict the profile for a protein, the protein sequence with masked tokens was fed to the model and predicted the most likely amino acids in those masked positions. The predicted profiles were compared with the sequence profiles in the HSSP dataset (<xref ref-type="bibr" rid="bib38">Dodge et al., 1998</xref>). They concluded that the high similarity between the two profiles (predicted and HSSP database) indicates the usefulness of their approach, and that it can assist researchers in obtaining prediction profiles for new sequences. Contrastive learning, which involves finding an embedding space where similar samples are brought together while dissimilar ones pushed apart, was investigated by <xref ref-type="bibr" rid="bib61">Heinzinger et al., 2022</xref>. The work utilized embeddings from the ProtT5 (<xref ref-type="bibr" rid="bib44">Elnaggar et al., 2020a</xref>) pre-trained Transformer model that were mapped using a feed-forward neural network to a new embedding space. The similarity between pairs, using Euclidean distance in the embedding space, was used to find homologous sequences, as well as to identify more distant relations. They observed that this approach required significantly less protein pre-processing time compared to MSA profiles from tools such as HMMER (<xref ref-type="bibr" rid="bib49">Finn et al., 2011</xref>). Their results not only showed similar performance to HMMER (<xref ref-type="bibr" rid="bib49">Finn et al., 2011</xref>) profiles but outperformed it for distant relations. Their work also found that the contrastive learning approach captured structural hierarchies that provide structural similarities between proteins. Protein profile prediction without sequence alignment was undertaken by <xref ref-type="bibr" rid="bib6">Behjati et al., 2022</xref>, who proposed a method for single protein profile prediction using the ProtAlbert (<xref ref-type="bibr" rid="bib45">Elnaggar et al., 2020b</xref>) Transformer. Their work found that attention heads of the pre-trained Transformer captured hidden protein characteristics in the sequence, such as amino acid neighbour interaction, biochemical and biophysical amino acid properties, protein secondary structure, etc. Homology prediction has also been part of many other works to demonstrate the benefits of newly developed Transformer models (<xref ref-type="bibr" rid="bib111">Rives et al., 2021</xref>; <xref ref-type="bibr" rid="bib12">Brandes et al., 2021</xref>; <xref ref-type="bibr" rid="bib109">Rao et al., 2019</xref>; <xref ref-type="bibr" rid="bib122">Sturmfels et al., 2020</xref>).</p></sec><sec id="s6-3"><title>Mutation prediction</title><p>Mutations in proteins is another important prediction task. Mutations are a vital part in evolution and introduce diversity to protein sequences. They can either be advantageous in evolution or cause illnesses, for example, a change in a protein’s stability may cause a disease. Predicting the impact of mutations is a step towards understanding protein function and stability. The approach of pre-training and fine-tuning a Transformer network was undertaken by <xref ref-type="bibr" rid="bib154">Yamaguchi and Saito, 2021</xref> for mutation prediction after fine-tuning of the evolutionary information which showed better accuracy compared to using an LSTM-based approach, and by <xref ref-type="bibr" rid="bib67">Jiang et al., 2021</xref> to predict the pathogenic missense mutations after pre-training a Transformer and fine-tuning on paired protein sequences which outperformed a variety of existing tools. Mutation prediction was also among one of the tasks in <xref ref-type="bibr" rid="bib111">Rives et al., 2021</xref> and <xref ref-type="bibr" rid="bib109">Rao et al., 2019</xref> to verify the potential of their new pre-trained Transformer models.</p></sec><sec id="s6-4"><title>Interaction prediction</title><p>Proteins interact with other molecules, and this interaction plays an important part in cellular processes as well as in disease pathogenesis. To gain insights regarding the function of a protein in its cellular context or to develop therapeutic procedures, it is crucial to identify potential interacting molecules (<xref ref-type="bibr" rid="bib89">McDowall et al., 2009</xref>; <xref ref-type="bibr" rid="bib37">Dick and Green, 2018</xref>). For instance, virus proteins infect the human body through interaction with human proteins. The impact of identifying PPIs can therefore encompass vaccine design. Similarly, identifying DTI is an essential task that is critical in drug discovery. DTI prediction can contribute by narrowing the search space and prune pairs that are unlikely to bind. The field has expanded to encompass new drug discovery, repurpose drugs already in existence, and identify novel proteins that might be interaction partners for approved drugs (<xref ref-type="bibr" rid="bib99">Öztürk et al., 2018</xref>). The existing methods of PPI and DTI are formulated as either a binary classification (interacting or non-interacting pairs), type of interaction (multiclass problem), or the strength of the interaction (regression task). Recent work in PPI has also predicted not only the interacting pairs, but also their quaternary structure (structure encompassing proteins that are closely packed together). Traditionally, PPI prediction was achieved by template-based modelling and free docking. The template-based approach involves matching sequences to related complexes for which the structure has been experimentally solved (<xref ref-type="bibr" rid="bib54">Guerler et al., 2013</xref>) while the docking methods incorporate energy functions, and a protein’s conformation and orientation in conjunction with correlation functions from the field of pattern recognition, for instance, (<xref ref-type="bibr" rid="bib70">Katchalski-Katzir et al., 1992</xref>) to determine the structure (<xref ref-type="bibr" rid="bib131">Vakser, 2014</xref>). After the success of the AlphaFold (<xref ref-type="bibr" rid="bib69">Jumper et al., 2021</xref>) model, approaches are now developed that utilize trained AlphaFold models for complex structure prediction. This is done by linking the chains of proteins and predicting the structure as if it was a single sequence (<xref ref-type="bibr" rid="bib93">Mirdita et al., 2022</xref>; <xref ref-type="bibr" rid="bib74">Ko and Lee, 2021</xref>). Recent works with Transformer-based models are starting to show promise in predicting interactions (<xref ref-type="bibr" rid="bib95">Nambiar et al., 2020</xref>).</p><p>PPI was one of the tasks considered by <xref ref-type="bibr" rid="bib95">Nambiar et al., 2020</xref>, where a Transformer model was pre-trained and fine-tuned using the HIPPIE database (<xref ref-type="bibr" rid="bib2">Alanis-Lobato et al., 2016</xref>) they formulated it as a binary classification problem. The method surpassed the results of previously used CNN models. <xref ref-type="bibr" rid="bib79">Lanchantin et al., 2021</xref> proposed to predict human and novel protein interactions through multiple Transformer pre-training stages (firstly: MLM; secondly: secondary structure, contact, and remote homology prediction) and fine-tuned on virus–host PPI data (<xref ref-type="bibr" rid="bib5">Ammari et al., 2016</xref>) for binary classification. Their approach outperformed the state-of-the-art method for this task. <xref ref-type="bibr" rid="bib153">Xue et al., 2022</xref> carried out cross-species PPI by pre-training a Transformer model using three separate features of proteins: sequence, structure, and function. The obtained embedding in combination with embedding from a BiLSTM model surpassed the performance when only the BiLSTM embedding was used.</p><p>For the DTI prediction, <xref ref-type="bibr" rid="bib16">Cai et al., 2021</xref> proposed a new protein embedding through Transformer pre-training that incorporated evolutionary information and used the model’s embeddings with a multilayer perceptron trained on several datasets (<xref ref-type="bibr" rid="bib50">Gaulton et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Chen et al., 2002</xref>; <xref ref-type="bibr" rid="bib17">Chan et al., 2015</xref>; <xref ref-type="bibr" rid="bib150">Wishart et al., 2006</xref>) to predict chemical-protein binding. The method outperformed the state of the art, which was also based on a Transformer model. A method was proposed by <xref ref-type="bibr" rid="bib145">wang et al., 2021</xref> to predict drug-target affinity using a regression approach by pre-training a Transformer model and using a CNN to extract features from the learned representation. They utilized multiple datasets (<xref ref-type="bibr" rid="bib124">Tang et al., 2014</xref>; <xref ref-type="bibr" rid="bib34">Davis et al., 2011</xref>; <xref ref-type="bibr" rid="bib83">Liu et al., 2007</xref>) to evaluate their method. The approach proved to be more accurate than the state-of-the-art DL methods, which included a CNN model based on amino acid features and an RNN model based on protein structural features.</p></sec><sec id="s6-5"><title>Post-translational modification prediction</title><p>PTM is a process of covalent and enzymatic modification of proteins after they are synthesized (<xref ref-type="bibr" rid="bib27">Chou, 2020</xref>). PTMs provide structural and functional diversity to proteins; however, they are also associated with major diseases like cancer. Identification of PTM sites is therefore vital for understanding it and to develop drugs for the many diseases it causes. A PTM is usually approached as a binary classification problem to identify whether a site along a protein sequence is modified or not. Prediction of lysine crotonylation, a PTM known to cause diseases like colon cancer and acute kidney injury, was undertaken by <xref ref-type="bibr" rid="bib105">Qiao et al., 2022</xref>, where a BiLSTM network was trained on BERT (<xref ref-type="bibr" rid="bib36">Devlin et al., 2018</xref>) embeddings of the amino acids in the protein sequences. The method outperformed the state-of-the-art model based on a CNN that utilised sequence, physicochemical properties, and Word2Vec (<xref ref-type="bibr" rid="bib92">Mikolov et al., 2013b</xref>) features. <xref ref-type="bibr" rid="bib164">Zhao et al., 2021</xref> attempted to predict S-nitrosylation, a PTM that causes disorders of the cardiovascular, musculoskeletal, and nervous systems. They used representations from a pre-trained BiLSTM model and the representation from a BERT model (<xref ref-type="bibr" rid="bib36">Devlin et al., 2018</xref>) to encode amino acids. Their approach surpassed the performance of several state-of-the-art methods, including a DL approach that had used position-specific scoring matrices (<xref ref-type="bibr" rid="bib68">Jones, 1999</xref>) (features from MSA).</p><p>The advantage of attention mechanism is also explored in the work by <xref ref-type="bibr" rid="bib144">Wang et al., 2020</xref>. In this work, the authors predict several PTMs, including a PTM called phosphorylation, which is one of the most studied PTMs, by including convolution layers with attention. Their framework performed better than existing methods for almost all PTMs.</p></sec></sec><sec id="s7"><title>Interpreting the Transformer model</title><p>Apart from the ground-breaking performance of Transformer models, they also offer possibilities for visualization and interpretation of their attention weights. In addition to traditional approaches such as using scatter plots (<xref ref-type="bibr" rid="bib132">Van der Maaten and Hinton, 2008</xref>; <xref ref-type="bibr" rid="bib1">Abdi and Williams, 2010</xref>) to visualize the learned representations (<xref ref-type="bibr" rid="bib95">Nambiar et al., 2020</xref>; <xref ref-type="bibr" rid="bib44">Elnaggar et al., 2020a</xref>; <xref ref-type="bibr" rid="bib111">Rives et al., 2021</xref>; <xref ref-type="bibr" rid="bib109">Rao et al., 2019</xref>), Transformers offer other prospects for interpretation, allowing a researcher to look inside their model to better understand its function rather than using the model as a black box. The analysis of attention heads reveal the weight assignments between pairs of input tokens, and these weights can be used to draw conclusions about the interactions between tokens that contributed to a model’s decision (<xref ref-type="bibr" rid="bib57">Hao et al., 2021</xref>). Note, however, that such an analysis does not necessarily point out the feature importance in the representation of the Transformer. Examples of some commonly used Transformer model visualizations include assessments of both the attention mechanism and the embeddings using, for example, attention weights (<xref ref-type="bibr" rid="bib44">Elnaggar et al., 2020a</xref>) and heatmaps (<xref ref-type="bibr" rid="bib12">Brandes et al., 2021</xref>; <xref ref-type="bibr" rid="bib163">Zhang et al., 2021</xref>; <xref ref-type="bibr" rid="bib154">Yamaguchi and Saito, 2021</xref>; <xref ref-type="bibr" rid="bib138">Vig et al., 2020</xref>). Attention weight visualization allows the portrayal of attention weights between an amino acid to the other amino acids in the form of intensity of line connections. Heatmaps can be used to show the colour shades for the different attention heads across the different layers for each amino acid, amino acid to amino acid maps using averaged weights of the heads across the network layers, etc. Transformer visualizations are not limited to the only ones listed here as new techniques are continually suggested in scientific publications which shows how versatile Transformer models are (<xref ref-type="bibr" rid="bib21">Chefer et al., 2021</xref>). Moreover, the intricacy in the existing ways of interpreting Transformer models, especially its multihead attention, is also being improved so that model’s internal learnings can be made more easy for analysis (<xref ref-type="bibr" rid="bib57">Hao et al., 2021</xref>; <xref ref-type="bibr" rid="bib137">Vig, 2019b</xref>).</p><p><xref ref-type="fig" rid="fig5">Figure 5A</xref> illustrates attention weights in an example using the BERT model. The model identifies dependencies in the sentence by showing which words attend to the word 'sick' in an attention head of a layer of the model. The model clearly connects the words 'tom' and 'he' to the word 'sick', indicating that the model has learnt to identify context in the sentence.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Visualisations of the attention weights in transformer models.</title><p>(<bold>A</bold>) A visualization of the attention weights in a BERT model. The weights are from the first attention head of the eighth layer of the model. The model has a total of 12 layers and 12 attention heads. In this example, the model connects the words 'tom' and 'he' to the word 'sick' (darker lines indicate larger weights). Visualization inspired by BertViz (<xref ref-type="bibr" rid="bib136">Vig, 2019a</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/jessevig/bertviz">https://github.com/jessevig/bertviz</ext-link>; <xref ref-type="bibr" rid="bib139">Vig, 2022</xref>). (<bold>B</bold>) Attention weights visualization showing that a protein language model learned to put more weight from one residue onto four other residues in one layer. The shades of a particular colour (horizontal order) correspond to an attention head in the layer of the Transformer. Dark shades indicate stronger attention and are hence shown with darker lines connecting the tokens.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82819-fig5-v1.tif"/></fig><p>Attention weight visualization was utilized by <xref ref-type="bibr" rid="bib44">Elnaggar et al., 2020a</xref>, who visualized the attention weights of each amino acid residue onto the other residues in the protein sequence, where a darker line represented a higher attention weight, as in <xref ref-type="fig" rid="fig5">Figure 5B</xref>. Specifically, they analysed the residue contacts (protein structural motifs) crucial for zinc-binding and found that the model had learned to put more weight from one residue onto three other residues in one layer, and all these residues collectively were involved in the ground truth binding coordination. Heatmaps were used by <xref ref-type="bibr" rid="bib154">Yamaguchi and Saito, 2021</xref>, who analysed effects of fine-tuning Transformer model with the use of evolutionary properties of proteins. The pre-trained and fine-tuned maps of a protein from the final layer were compared to the contact maps computed from its tertiary structure. It was observed that the fine-tuned model’s maps resembled that of contact maps and this pattern was absent from the map prior to fine-tuning. The visualization indicated that the structural information was captured by the model after fine-tuning on evolutionary related sequences. Heatmaps were also used by <xref ref-type="bibr" rid="bib163">Zhang et al., 2021</xref>, where they proposed an evolutionary information-based attention (co-evolution attention). They visualized the maps with and without the evolutionary information-based attention and concluded that their attention component was effective in extracting contact patterns.</p><p>Such visualizations can thus be used to understand biological aspects of different protein properties, and by visualizing a Transformer model’s internal state we can gather and present deeper biological insights.</p></sec><sec id="s8"><title>Transformer language model adoption</title><p>There has been a steady increase in the volume of scientific publications relating to Transformer-based models since their introduction in 2017 (<xref ref-type="bibr" rid="bib134">Vaswani et al., 2017</xref>). This is evident in the progresses of both NLP and in computational biology and bioinformatics research. <xref ref-type="fig" rid="fig6">Figure 6A</xref> illustrates the yearly count of publications from 2017 to 2021 for the query 'Transformer Language Model' in <xref ref-type="bibr" rid="bib52">Google Scholar, 2022</xref>. This resulted in a total number of publications of 1388. The plot has been extended to include the year 2022 by extrapolating the counts for 2022 (counts of scientific publications were until 2022-07-01 at the time of writing this article) until the end of the year. There is clearly an increase in how often Transformer-type of models are mentioned in the literature.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Yearly number of publications on Google Scholar for the years 2017–2021 and extrapolated count for the year 2022.</title><p>For: (<bold>A</bold>) the search query 'Transformer Language Model' and (<bold>B</bold>) the search query 'Life Science' that have cited the original Transformer paper by <xref ref-type="bibr" rid="bib134">Vaswani et al., 2017</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82819-fig6-v1.tif"/></fig><p>In <xref ref-type="fig" rid="fig6">Figure 6B</xref>, the number of publications per year is illustrated for scientific publications related to Transformer-based models which were identified by searching within articles in Google Scholar citing the 'Attention is all you need' paper by <xref ref-type="bibr" rid="bib134">Vaswani et al., 2017</xref>. The search was based on the query 'Life Science' and included all scientific research papers from 2017 to 2022 (cut-off on 2022-06-23). We excluded review papers and theses from the analysis. Articles focusing solely on method developments were excluded from the query results as well, leaving us with a total of 87 publications. The results were sorted by three main disciplines: medicine, pharmacology, and biology, as shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>. Within the main disciplines, articles were sorted by their sub-categories. The increased use of Transformer models in the different areas of bioinformatics indicates that it is an effective model to use when studying different protein properties.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>The article counts for the three main disciplines (medicine, pharmacology, and biology) and percentage breakdown of their sub-categories in Google Scholar citing the 'Attention is all you need' paper by <xref ref-type="bibr" rid="bib134">Vaswani et al., 2017</xref>.</title><p>The search was based on the query 'Life Science' and included all scientific research papers from 2017 to 2022 (cut-off on 2022-06-23).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82819-fig7-v1.tif"/></fig></sec><sec id="s9"><title>A proof-of-principle example</title><p>To illustrate how the features learned by the Transformer model can be used to directly improve results over traditional protein features, we have conducted a pilot study in phosphoglycerylation prediction. Phosphoglycerylation is a type of PTM discovered in human cells and mouse liver which occurs when the amino acid lysine in a protein sequence is covalently modified by a primary glycolytic intermediate (1,3-BPG) to form 3-phosphoglyceryl-lysine. This PTM has been found to be associated with cardiovascular diseases like heart failure (<xref ref-type="bibr" rid="bib15">Bulcun et al., 2012</xref>).</p><p>The task was to predict phosphoglycerylation, and for this we used the dataset from <xref ref-type="bibr" rid="bib18">Chandra et al., 2019</xref> which was originally obtained from the Protein Lysine Modification Database (PLMD, available at <ext-link ext-link-type="uri" xlink:href="http://plmd.biocuckoo.org">http://plmd.biocuckoo.org</ext-link>). The features used by <xref ref-type="bibr" rid="bib18">Chandra et al., 2019</xref> were based on position-specific scoring matrices that were obtained using the PSI-BLAST toolbox (<xref ref-type="bibr" rid="bib4">Altschul et al., 1997</xref>), which is an MSA tool, and then they calculated its profile bigrams (<xref ref-type="bibr" rid="bib114">Sharma et al., 2013</xref>) (a type of feature extraction) to produce the final feature set. We used this feature set, denoted <italic>BigramPGK</italic>, and compared the results to results when using features extracted from two pre-trained Transformer models. Additionally, we also used a second baseline feature set that composed of 10 commonly used physicochemical/biochemical properties of each amino acid (<xref ref-type="bibr" rid="bib159">Yu et al., 2017</xref>; <xref ref-type="bibr" rid="bib23">Chen et al., 2020</xref>; <xref ref-type="bibr" rid="bib31">Cortés and Aguilar-Ruiz, 2011</xref>; <xref ref-type="bibr" rid="bib84">Liu et al., 2012</xref>). We denote this feature set as Phy + Bio. The features were length of side chain, molecular weight, free energy of solution in water, melting point, hydrostatic pressure asymmetry index, isoelectric point, hydrophobicity index, ionization equilibrium constant (pK-a), pK (-COOH), and net charge. The Transformer models were the ESM-1b (<xref ref-type="bibr" rid="bib111">Rives et al., 2021</xref>) and the ProtT5-XL-UniRef50 (<xref ref-type="bibr" rid="bib44">Elnaggar et al., 2020a</xref>). We used two aggregation techniques for the ESM-1b model since it has a restriction on the length of the protein sequence that can be processed. First, the protein sequence was split into multiple parts by using 878 consecutive amino acids at a time, starting at each amino acid in the sequence. The ESM-1b model was used to extract a feature vector for each such subsequence of length 878, and the feature vectors were averaged to obtain a single feature vector for the entire protein sequence. We denote this approach <italic>ESM1b-avg</italic>. The second approach was to again split the protein sequence up into subparts of length 878, but this time splitting from where the last split ended and finally concatenating to get the resulting feature vector of the protein sequence. We denote this approach <italic>ESM1b-concate</italic>. We denote the features extracted from the ProtT5-XL-UniRef50 model as <italic>T5</italic>, and this model accepts variable-length input sequences so there was no need to aggregate multiple feature vectors manually.</p><p>After obtaining the representation of the protein sequences from the Transformer models, the feature of each sample was extracted by examining the sites of interest in the protein sequences and selecting the window size around those sites. The samples were extracted based on the standard practices, as outlined, for example, by <xref ref-type="bibr" rid="bib108">Ramazi and Zahiri, 2021</xref>, which include to consider a window size of 15 amino acid residues upstream and 15 amino acid residues downstream of the lysine sites (<xref ref-type="bibr" rid="bib86">López et al., 2017</xref>; <xref ref-type="bibr" rid="bib65">Jia et al., 2016</xref>; <xref ref-type="bibr" rid="bib152">Xu et al., 2018</xref>), to disregard the sites which did not have enough residues to make up the full upstream and downstream window (<xref ref-type="bibr" rid="bib141">Wang et al., 2017</xref>; <xref ref-type="bibr" rid="bib112">Saethang et al., 2016</xref>), and to take unlabelled sites as non-phosphoglycerylated samples only if the protein has two or more confirmed PTM sites in its sequence (<xref ref-type="bibr" rid="bib71">Khalili et al., 2022</xref>; <xref ref-type="bibr" rid="bib127">Trost and Kusalik, 2013</xref>). These conditions were applied to all the feature sets. The resultant dataset had a total of 526 samples (relating to each lysine) containing 101 phosphoglycerylated samples (positive labels) and 425 non-phosphoglycerylated samples (negative labels). We used random under-sampling to resolve the class imbalance ratio from 1:4 to 1:1.5 by randomly selecting the negative labels (<xref ref-type="bibr" rid="bib108">Ramazi and Zahiri, 2021</xref>). The final number of samples used in the experiment was 253 (with 101 phosphoglycerylated and 152 non-phosphoglycerylated samples).</p><p>We used and compared five classifiers: logistic regression with ridge regularization (denoted <italic>LR</italic>), a support vector machine with a polynomial kernel (denoted <italic>SVM (poly</italic>)), a support vector machine with a radial basis function kernel (denoted <italic>SVM (RBF</italic>)), random forest (denoted <italic>RF</italic>), and finally a light gradient-boosting machine (denoted <italic>LightGBM</italic>). We set aside 51 samples for final test of each model (maintaining the same ratio between the positive and negative labels, i.e., 1:1.5) and performed fivefold cross-validation on the remaining 202 samples to select the models’ hyper-parameters with standard scaling of the data, based on the training set in each cross-validation round. The hyper-parameters were tuned using Hyperopt (<xref ref-type="bibr" rid="bib9">Bergstra et al., 2013</xref>) with 15 evaluations.</p><p>We thus had five datasets (Phy + Bio, BigramPGK, T5, ESM1b-avg, and ESM1b-concate) and five classification models (LR, SVM (poly), SVM (RBF), RF, and LightGBM). We evaluated each model on all datasets using accuracy (ACC) and the area under the receiver operating characteristic curve (AUC), reporting both the fivefold cross-validation (CV) scores (those used to select the hyper-parameters) and the score obtained on the held-out test set (the 51 set-aside samples mentioned above). The results are presented in <xref ref-type="table" rid="table1">Table 1</xref>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>The performance on five datasets, i.e. the five feature sets (Phy + Bio, BigramPGK, T5, ESM1b-avg, and ESM1b-concate) by five classification models (LR, SVM (poly), SVM (RBF), RF, and LightGBM) evaluated using accuracy (ACC) and the area under the receiver operating characteristic curve (AUC).</title><p>The reported cross-validation results are the mean over the five CV rounds. Standard errors for both CV and test are in the parenthesis. The highest scores are highlighted in bold. CV: five-fold cross-validation; Test: held-out test set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="2">LR</th><th align="left" valign="bottom" colspan="2">SVM (poly)</th><th align="left" valign="bottom" colspan="2">SVM (RBF)</th><th align="left" valign="bottom" colspan="2">RF</th><th align="left" valign="bottom" colspan="2">LightGBM</th></tr></thead><tbody><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">ACC</td><td align="left" valign="bottom">AUC</td><td align="left" valign="bottom">ACC</td><td align="left" valign="bottom">AUC</td><td align="left" valign="bottom">ACC</td><td align="left" valign="bottom">AUC</td><td align="left" valign="bottom">ACC</td><td align="left" valign="bottom">AUC</td><td align="left" valign="bottom">ACC</td><td align="left" valign="bottom">AUC</td></tr><tr><td align="left" valign="bottom" rowspan="2">Phy +Bio</td><td align="left" valign="bottom">CV</td><td align="char" char="." valign="bottom">0.550 (0.032)</td><td align="char" char="." valign="bottom">0.546<break/>(0.012)</td><td align="char" char="." valign="bottom">0.614 (0.017)</td><td align="char" char="." valign="bottom">0.550 (0.027)</td><td align="char" char="." valign="bottom">0.545 (0.035)</td><td align="char" char="." valign="bottom">0.552 (0.013)</td><td align="char" char="." valign="bottom">0.609 (0.010)</td><td align="char" char="." valign="bottom">0.564 (0.034)</td><td align="char" char="." valign="bottom">0.525 (0.027)</td><td align="char" char="." valign="bottom">0.498 (0.029)</td></tr><tr><td align="left" valign="bottom">Test</td><td align="char" char="." valign="bottom">0.471 (0.071)</td><td align="char" char="." valign="bottom">0.395 (0.083)</td><td align="char" char="." valign="bottom">0.588 (0.070)</td><td align="char" char="." valign="bottom">0.552 (0.083)</td><td align="char" char="." valign="bottom">0.471 (0.071)</td><td align="char" char="." valign="bottom">0.371 (0.082)</td><td align="char" char="." valign="bottom">0.628 (0.068)</td><td align="char" char="." valign="bottom">0.489 (0.084)</td><td align="char" char="." valign="bottom">0.529 (0.071)</td><td align="char" char="." valign="bottom">0.503 (0.084)</td></tr><tr><td align="left" valign="bottom" rowspan="2">BigramPGK</td><td align="left" valign="bottom">CV</td><td align="char" char="." valign="bottom">0.678 (0.026)</td><td align="char" char="." valign="bottom">0.686 (0.019)</td><td align="char" char="." valign="bottom">0.590 (0.025)</td><td align="char" char="." valign="bottom">0.723 (0.025)</td><td align="char" char="." valign="bottom">0.599 (0.004)</td><td align="char" char="." valign="bottom">0.711 (0.030)</td><td align="char" char="." valign="bottom"><bold>0.698</bold> (<bold>0.008</bold>)</td><td align="char" char="." valign="bottom">0.707 (0.025)</td><td align="char" char="." valign="bottom">0.629 (0.024)</td><td align="char" char="." valign="bottom">0.627 (0.028)</td></tr><tr><td align="left" valign="bottom">Test</td><td align="char" char="." valign="bottom">0.628 (0.068)</td><td align="char" char="." valign="bottom">0.686 (0.074)</td><td align="char" char="." valign="bottom">0.647 (0.068)</td><td align="char" char="." valign="bottom">0.666 (0.076)</td><td align="char" char="." valign="bottom">0.608 (0.069)</td><td align="char" char="." valign="bottom">0.668 (0.076)</td><td align="char" char="." valign="bottom"><bold>0.686</bold> (<bold>0.066</bold>)</td><td align="char" char="." valign="bottom">0.742 (0.069)</td><td align="char" char="." valign="bottom"><bold>0.706</bold> (<bold>0.064</bold>)</td><td align="char" char="." valign="bottom">0.742 (0.069)</td></tr><tr><td align="left" valign="bottom" rowspan="2">T5</td><td align="left" valign="bottom">CV</td><td align="char" char="." valign="bottom">0.704 (0.038)</td><td align="char" char="." valign="bottom">0.742 (0.039)</td><td align="char" char="." valign="bottom">0.713 (0.035)</td><td align="char" char="." valign="bottom">0.744 (0.038)</td><td align="char" char="." valign="bottom">0.713 (0.034)</td><td align="char" char="." valign="bottom">0.737 (0.041)</td><td align="char" char="." valign="bottom">0.634 (0.021)</td><td align="char" char="." valign="bottom"><bold>0.747</bold> (<bold>0.041</bold>)</td><td align="char" char="." valign="bottom">0.668 (0.022)</td><td align="char" char="." valign="bottom"><bold>0.756</bold> (<bold>0.018</bold>)</td></tr><tr><td align="left" valign="bottom">Test</td><td align="char" char="." valign="bottom">0.647 (0.068)</td><td align="char" char="." valign="bottom">0.726 (0.070)</td><td align="char" char="." valign="bottom">0.628 (0.068)</td><td align="char" char="." valign="bottom">0.726 (0.070)</td><td align="char" char="." valign="bottom">0.628 (0.068)</td><td align="char" char="." valign="bottom">0.737 (0.069)</td><td align="char" char="." valign="bottom">0.647 (0.068)</td><td align="char" char="." valign="bottom">0.736 (0.070)</td><td align="char" char="." valign="bottom">0.471 (0.071)</td><td align="char" char="." valign="bottom">0.592 (0.081)</td></tr><tr><td align="left" valign="bottom" rowspan="2">ESM-1b-avg</td><td align="left" valign="bottom">CV</td><td align="char" char="." valign="bottom">0.768 (0.025)</td><td align="char" char="." valign="bottom">0.830 (0.025)</td><td align="char" char="." valign="bottom">0.748 (0.022)</td><td align="char" char="." valign="bottom">0.826 (0.028)</td><td align="char" char="." valign="bottom">0.599 (0.004)</td><td align="char" char="." valign="bottom">0.785 (0.055)</td><td align="char" char="." valign="bottom">0.639 (0.015)</td><td align="char" char="." valign="bottom">0.745 (0.058)</td><td align="char" char="." valign="bottom">0.708 (0.020)</td><td align="char" char="." valign="bottom">0.741 (0.044)</td></tr><tr><td align="left" valign="bottom">Test</td><td align="char" char="." valign="bottom">0.726 (0.063)</td><td align="char" char="." valign="bottom"><bold>0.803</bold> (<bold>0.061</bold>)</td><td align="char" char="." valign="bottom">0.667 (0.067)</td><td align="char" char="." valign="bottom">0.813 (0.059)</td><td align="char" char="." valign="bottom">0.608 (0.069)</td><td align="char" char="." valign="bottom"><bold>0.811</bold> (<bold>0.060</bold>)</td><td align="char" char="." valign="bottom">0.628 (0.068)</td><td align="char" char="." valign="bottom">0.719 (0.071)</td><td align="char" char="." valign="bottom">0.647 (0.068)</td><td align="char" char="." valign="bottom"><bold>0.748</bold> (<bold>0.068</bold>)</td></tr><tr><td align="left" valign="bottom" rowspan="2">ESM-1b-concate</td><td align="left" valign="bottom">CV</td><td align="char" char="." valign="bottom"><bold>0.782</bold> (<bold>0.012</bold>)</td><td align="char" char="." valign="bottom"><bold>0.852</bold> (<bold>0.015</bold>)</td><td align="char" char="." valign="bottom"><bold>0.792</bold> (<bold>0.014</bold>)</td><td align="char" char="." valign="bottom"><bold>0.853</bold> (<bold>0.015</bold>)</td><td align="char" char="." valign="bottom"><bold>0.773</bold> (<bold>0.015</bold>)</td><td align="char" char="." valign="bottom"><bold>0.844</bold> (<bold>0.023</bold>)</td><td align="char" char="." valign="bottom">0.609 (0.017)</td><td align="char" char="." valign="bottom">0.742 (0.048)</td><td align="char" char="." valign="bottom"><bold>0.718</bold> (<bold>0.017</bold>)</td><td align="char" char="." valign="bottom">0.755 (0.039)</td></tr><tr><td align="left" valign="bottom">Test</td><td align="char" char="." valign="bottom"><bold>0.745</bold> (<bold>0.062</bold>)</td><td align="char" char="." valign="bottom">0.797 (0.062)</td><td align="char" char="." valign="bottom"><bold>0.745</bold> (<bold>0.062</bold>)</td><td align="char" char="." valign="bottom"><bold>0.824</bold> (<bold>0.057</bold>)</td><td align="char" char="." valign="bottom"><bold>0.726</bold> (<bold>0.063</bold>)</td><td align="char" char="." valign="bottom">0.798 (0.061)</td><td align="char" char="." valign="bottom">0.628 (0.068)</td><td align="char" char="." valign="bottom"><bold>0.850</bold> (<bold>0.053</bold>)</td><td align="char" char="." valign="bottom">0.667 (0.067)</td><td align="char" char="." valign="bottom">0.726 (0.070)</td></tr></tbody></table></table-wrap><p>The Transformer models perform better in general than the BigramPGK protein features (based on MSAs) and the Phy + Bio features on the accuracy and AUC metrics across all the classifiers, except for the RF classifier where BigramPGK attained the highest accuracy on the fivefold cross-validation. Out of the five features, Phy + Bio had the lowest performance. We see that the concatenated features from the ESM-1b Transformer model generally perform better than all of the other feature sets, including the Transformer features (averaged features from ESM-1b, and the T5 features). While the differences are not always significant, it is clear that the trend is that the Transformer features perform better.</p></sec><sec id="s10"><title>Outlook</title><p>The Transformer family of models has shown large improvements over RNNs and other DL-based models. In just a few years, they have been used for many different prediction tasks and their representations have been used with very promising results. In contrast, it took decades for conventional features based on MSAs to reach their current performances. The Transformer models have their own set of limitations, and future improvements in their architecture will likely give further boosts in their performance.</p><p>For instance, the standard attention mechanisms can only process fixed-length input sequences. For longer sequences, they need to be split into smaller fragments before being fed to a model. However, splitting a sequence up means context is being lost beyond the split boundary. Recent developments have attempted to overcome the fixed-length issue, where, for instance, some variants allow hidden states from previous fragments to be used as inputs for the current fragment (<xref ref-type="bibr" rid="bib44">Elnaggar et al., 2020a</xref>; <xref ref-type="bibr" rid="bib33">Dai et al., 2019</xref>). ProtT5-XL-UniRef50 model used in the section ‘A proof-of-principle example’ uses the same technique to pass information from one fragment to the other in the protein sequence. This allows a Transformer model to consider very long dependencies and at least in theory handle unlimited-length contexts since the information from one segment can be passed on to the next infinitely (<xref ref-type="bibr" rid="bib143">Wang et al., 2019</xref>). Furthermore, some transformer models need the users to pre-process the sequences to adhere to a sequence length limit. This was apparent with the ESM-1b model in the ‘A proof-of-principle example’. The workaround was to break the longer sequences into fragments (maximum lengths of 878 in this work) to get the Transformer representations, which was then concatenated to produce a representation for the entire sequence. That approach worked out as the best-performing features in this study out of the features compared. Fragmenting the sequence of course results in loss of some contexts, and future improvements to the sequence length limit can lead to more robust performances.</p><p>The attention mechanism, which is an integral part of Transformer models, also brings a limitation when it comes to long sequences. Since each token attends to every other token, the memory and computational complexity of the model increases quadratically in the attention layers with respect to the sequence length. A solution using sparse attention mechanism was proposed by <xref ref-type="bibr" rid="bib160">Zaheer et al., 2020</xref> that changed the complexity from quadratic to linear and allowed up to eight times longer sequences to be handled on similar hardware. Their proposed attention mechanism consisted of three parts: (1) making some tokens global which attend to the entire sequence, (2) all tokens attend to a set of local neighbouring tokens, and (3) all tokens attend to a set of random tokens. This technique also allows the Transformer to handle longer contexts. Moreover, the memory requirements and the complexity of Transformer models was also addressed by <xref ref-type="bibr" rid="bib73">Kitaev et al., 2020</xref>, who introduced two techniques: (1) they replaced the dot-product attention with a locality-sensitive hashing which deals with a subset of nearest neighbours in high-dimensional spaces for the attention computation which saw the reduction of complexity from quadratic to log linear, and (2) they utilized reversible residual layers in place of standard residuals, thereby allowing the storage of activations only once instead of in every layer, which makes it much more memory efficient. Furthermore, <xref ref-type="bibr" rid="bib119">Sourkov, 2018</xref> proposed to replace pairwise dot-product attention mechanism with an IGLOO-base block to a get computational advantage. This new block did not require the computation of the full self-attention matrix, but rather a constant number of elements from distant parts of the sequence. This is particularly useful for bioinformatics tasks since these tasks often comprise long sequences.</p><p>The Transformer models, even though emerging as the new workhorse for NLP, were found not to perform well in comparison to LSTM in some tasks. For instance, <xref ref-type="bibr" rid="bib126">Tran et al., 2018</xref> compared LSTMs and Transformers in their ability to model the hierarchical structure in sentences. The tasks that were performed were subject–verb agreement and logical inference. They observed that the LSTMs consistently outperformed Transformers and the performance gap increased with the distance between the subject and the verb in a sentence. The task of logical inference, which is to predict logical relations between pairs of sentences, was also found to be modelled better with the LSTM architecture, especially in longer sequences. Work by <xref ref-type="bibr" rid="bib55">Hahn, 2020</xref> also showed that Transformers had problems to accurately evaluate logical formulas and to model hierarchical structures. Moreover, regarding linguistics, a Transformer model called GPT-2 had problems to learn poetry and rhyming (<xref ref-type="bibr" rid="bib146">Wang et al., 2021</xref>). Its successor, called GTP-3, did a bit better on this task, but not as much improvement as was seen in tasks like arithmetic. These shortcomings of the Transformer models are important to be aware of since they could also be considerable factors in protein prediction tasks for which these or similar properties are critical.</p><p>A better performance on downstream task can usually be achieved by increasing the Transformer model’s size (adding more layers and more parameters). Such high-capacity models face both memory limitation and longer training times. <xref ref-type="bibr" rid="bib78">Lan et al., 2019</xref> managed to limit these issues by employing techniques in their framework that lowers the memory consumption and increases training speed. These include projecting the word embeddings into a lower dimensional embedding, thereby resulting in parameter reduction, parameter sharing in feed-forward network and attention across the layers to improve parameter efficiency and employed a sentence prediction loss that helped improve downstream task performance.</p><p>In the MLM approach, where some of the tokens are masked, the model neglects dependency between these masked positions. One way of overcoming this limitation is to utilize the benefits of autoregressive language modelling and to combine it with bidirectional context capturing used in MLM, instead of the original MLM (<xref ref-type="bibr" rid="bib155">Yang et al., 2019</xref>). Moreover, the standard MLM approach is computationally expensive because it learns from only about 15% of the tokens at a time in most models (<xref ref-type="bibr" rid="bib148">Wettig et al., 2022</xref>). In recent developments, <xref ref-type="bibr" rid="bib30">Clark et al., 2020</xref> proposed a sample-efficient pre-training method called <italic>replaced token detection</italic> that allows the model to learn from all the input tokens, unlike the masked subset approach in MLM.</p><p>The recent improvements to the Transformer model architectures indicate that this model class is still in its infancy, is clearly under fast development, and shows much promise as the architecture continues to enhance and expand. Many developments are coming in from multiple directions, such as from ML in general, from NLP, from computer vision, and from computational biology and bioinformatics, among other areas. These developments are crucial since the original Transformer architecture was designed and optimized for natural language tasks; the application of these models to biological data such as protein sequences, which are usually longer, has the possibility of running into high computational costs and memory limitation as well as suboptimally capturing very long-range dependencies. We can expect many more improvements in the years to come, and we can suppose that whatever limitations exist today will be addressed tomorrow.</p><p>A trend in the development of large Transformer models in NLP has been to build larger and larger models. 'Standard' models in NLP today have hundreds of billions of model parameters, such as Openai’s <italic>GPT-3</italic> model with 175 billion model parameters (<xref ref-type="bibr" rid="bib14">Brown et al., 2020</xref>) or Microsoft and Nvidia’s <italic>Megatron-Turing NLG</italic> model with 530 billion model parameters (<xref ref-type="bibr" rid="bib118">Smith et al., 2022</xref>), but the very latest models have over a trillion model parameters (<xref ref-type="bibr" rid="bib47">Fedus et al., 2021</xref>; <xref ref-type="bibr" rid="bib96">Narayanan et al., 2021</xref>). This trend with ever larger models is unlikely to be sustainable since they require enormous amounts of memory and compute resources, and therefore severely limit who can build and train such models. But the trend is nevertheless clear that larger and larger models are built and are more successful. These models are also trained on ever larger sets of data. We can expect both trends to follow into computational biology and bioinformatics, with larger models trained on larger sets of data. Such a trend might limit future protein research to resource rich research institutes and companies and prevent such research to be performed at universities with limited resources.</p><sec id="s10-1"><title>Conclusions</title><p>This work has reviewed the potential of the Transformer models for protein prediction tasks. It has analysed some of the issues faced by the existing deep learning models and described how the latest language models, based on the Transformer, are proving to be promising models for protein prediction tasks. Transformer-based models are producing state-of-the-art results on many diverse tasks. This indicates that they are very capable models able to find relevant, important, and general features in and relationships between amino acid residues in a protein sequence. Transformer models can be analysed through their attention weights and an interpretation of the model internals can give more insight into the prediction task, and even lead to new knowledge about the underlying biology. As for all ML models, there are shortcomings also with the Transformer model, such as the quadratic growth in the memory requirement and the computational complexity of the attention layers as functions of the sequence length, the fact that the attention mechanisms process fixed length input sequences, the extensive pre-training which leads to longer training time for larger models, inadequacies in the MLM pre-training procedure, etc. Despite these shortcomings, the performance of Transformer models has been attracting a much interest and efforts from the ML community to improve the models as much as possible in the respective fields. While the Transformer model has been the go-to model in NLP tasks since 2017, their capabilities are just beginning to be explored when it comes to modelling proteins for different prediction tasks. Furthermore, it could be that Transformer models alone may not be the best approach for all the protein prediction tasks and that other or traditional methods would be required, perhaps in combination with components from Transformers, to obtain results past the current state-of-the-art methods. It is also important to be aware of other differences between Transformers and other methods, and that, for instance, differences in the training procedures, or other aspects of the whole analysis pipeline, could at least in part be the reason for some of the recent improvements. For example, the MLM pre-training and finetuning procedure has also been used with CNN models, and has shown promising results (<xref ref-type="bibr" rid="bib156">Yang et al., 2022</xref>). The AlphaFold model uses attention mechanism from Transformers to extract information in MSAs that shows Transformer model component with traditional features work quite well. Moreover, the breakthrough performance of the Transformer models has inspired other deep learning models to incorporate similar architectural enhancements. It will be interesting to follow the developments in Transformer-based models and other deep learning models as a whole and its application to understanding proteins and its properties.</p><p>We hope the discussion in this review provides the readers, both those experienced in and those without experience in ML, with a general understanding of DL and specifically about how the Transformer model from NLP is adopted to predict properties of proteins. Specifically, the proof-of principle example shows how the Transformer models can be used as general feature extractors that can improve results compared to traditional protein features, such as those based on MSAs. The result, however, does not prove that Transformer model representations are better in general compared, for instance, to MSAs but does show that this is a promising avenue to be explored further since there are recurrent evolutionary relations captured in the representations from such type of language models (<xref ref-type="bibr" rid="bib8">Bepler and Berger, 2021</xref>). In the example, we used standard models and used them as they were, but the future of computational biology and bioinformatics likely contains special-purpose models and model and training developments specifically made for analysing protein data that further improve such results. These are exciting times to follow the developments in the fields of computational biology and bioinformatics, which will likely be heavily based on Transformer models for the foreseeable future.</p></sec></sec></body><back><sec sec-type="additional-information" id="s11"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Investigation, Visualization, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Investigation, Methodology, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Data curation, Supervision, Funding acquisition, Investigation, Visualization, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s12"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Some of the commonly used pre-trained Transformer models in the literature.</title><p>The higher the number of Transformer parameters, the larger the model.</p></caption><media xlink:href="elife-82819-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><ack id="ack"><title>Acknowledgements</title><p>The authors thank the Kempe Foundations for funding this study (JCK-2015.1 and JCK-2144). They also thank the company Arevo AB for allowing Regina Gratz to take leave of absence to perform this research.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdi</surname><given-names>H</given-names></name><name><surname>Williams</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Principal component analysis</article-title><source>Wiley Interdisciplinary Reviews: Computational Statistics</source><volume>2</volume><fpage>433</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1098/rsta.2015.0202</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alanis-Lobato</surname><given-names>G</given-names></name><name><surname>Andrade-Navarro</surname><given-names>MA</given-names></name><name><surname>Schaefer</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>HIPPIE v2. 0: enhancing meaningfulness and reliability of protein–protein interaction networks</article-title><source>Nucleic Acids Research</source><volume>45</volume><fpage>D408</fpage><lpage>D414</lpage><pub-id pub-id-type="doi">10.1093/nar/gkw985</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Albawi</surname><given-names>S</given-names></name><name><surname>Mohammed</surname><given-names>TA</given-names></name><name><surname>Al-Zawi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Understanding of a convolutional neural network</article-title><conf-name>2017 International Conference on Engineering and Technology (ICET</conf-name><pub-id pub-id-type="doi">10.1109/ICEngTechnol.2017.8308186</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altschul</surname><given-names>SF</given-names></name><name><surname>Madden</surname><given-names>TL</given-names></name><name><surname>Schäffer</surname><given-names>AA</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Miller</surname><given-names>W</given-names></name><name><surname>Lipman</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Gapped blast and PSI-BLAST: a new generation of protein database search programs</article-title><source>Nucleic Acids Research</source><volume>25</volume><fpage>3389</fpage><lpage>3402</lpage><pub-id pub-id-type="doi">10.1093/nar/25.17.3389</pub-id><pub-id pub-id-type="pmid">9254694</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Ammari</surname><given-names>MG</given-names></name><name><surname>Gresham</surname><given-names>CR</given-names></name><name><surname>McCarthy</surname><given-names>FM</given-names></name><name><surname>Nanduri</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><data-title>HPIDB 2.0: a curated database for host–pathogen interactions</data-title><source>Database</source></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behjati</surname><given-names>A</given-names></name><name><surname>Zare-Mirakabad</surname><given-names>F</given-names></name><name><surname>Arab</surname><given-names>SS</given-names></name><name><surname>Nowzari-Dalini</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Protein sequence profile prediction using protalbert transformer</article-title><source>Computational Biology and Chemistry</source><volume>99</volume><elocation-id>107717</elocation-id><pub-id pub-id-type="doi">10.1016/j.compbiolchem.2022.107717</pub-id><pub-id pub-id-type="pmid">35802991</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Simard</surname><given-names>P</given-names></name><name><surname>Frasconi</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Learning long-term dependencies with gradient descent is difficult</article-title><source>IEEE Transactions on Neural Networks</source><volume>5</volume><fpage>157</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1109/72.279181</pub-id><pub-id pub-id-type="pmid">18267787</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bepler</surname><given-names>T</given-names></name><name><surname>Berger</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning the protein language: evolution, structure, and function</article-title><source>Cell Systems</source><volume>12</volume><fpage>654</fpage><lpage>669</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2021.05.017</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bergstra</surname><given-names>J</given-names></name><name><surname>Yamins</surname><given-names>D</given-names></name><name><surname>Cox</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</article-title><conf-name>International conference on machine learning; 2013: PMLR</conf-name></element-citation></ref><ref id="bib10"><element-citation publication-type="web"><person-group person-group-type="author"><collab>BFD</collab></person-group><year iso-8601-date="2022">2022</year><article-title>BFD</article-title><ext-link ext-link-type="uri" xlink:href="https://bfd.mmseqs.com">https://bfd.mmseqs.com</ext-link><date-in-citation iso-8601-date="2022-07-01">July 1, 2022</date-in-citation></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bileschi</surname><given-names>ML</given-names></name><name><surname>Belanger</surname><given-names>D</given-names></name><name><surname>Bryant</surname><given-names>DH</given-names></name><name><surname>Sanderson</surname><given-names>T</given-names></name><name><surname>Carter</surname><given-names>B</given-names></name><name><surname>Sculley</surname><given-names>D</given-names></name><name><surname>Bateman</surname><given-names>A</given-names></name><name><surname>DePristo</surname><given-names>MA</given-names></name><name><surname>Colwell</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Using deep learning to annotate the protein universe</article-title><source>Nature Biotechnology</source><volume>40</volume><fpage>932</fpage><lpage>937</lpage><pub-id pub-id-type="doi">10.1038/s41587-021-01179-w</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Brandes</surname><given-names>N</given-names></name><name><surname>Ofer</surname><given-names>D</given-names></name><name><surname>Peleg</surname><given-names>Y</given-names></name><name><surname>Rappoport</surname><given-names>N</given-names></name><name><surname>Linial</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>ProteinBERT: A Universal Deep-Learning Model of Protein Sequence and Function</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.05.24.445464</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Britz</surname><given-names>D</given-names></name><name><surname>Goldie</surname><given-names>A</given-names></name><name><surname>Luong</surname><given-names>MT</given-names></name><name><surname>Le</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Massive Exploration of Neural Machine Translation Architectures</article-title><conf-name>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</conf-name><pub-id pub-id-type="doi">10.18653/v1/D17-1151</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>T</given-names></name><name><surname>Mann</surname><given-names>B</given-names></name><name><surname>Ryder</surname><given-names>N</given-names></name><name><surname>Subbiah</surname><given-names>M</given-names></name><name><surname>Kaplan</surname><given-names>JD</given-names></name><name><surname>Dhariwal</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Language models are few-shot learners</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1877</fpage><lpage>1901</lpage></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bulcun</surname><given-names>E</given-names></name><name><surname>Ekici</surname><given-names>M</given-names></name><name><surname>Ekici</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Disorders of glucose metabolism and insulin resistance in patients with obstructive sleep apnoea syndrome</article-title><source>International Journal of Clinical Practice</source><volume>66</volume><fpage>91</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1111/j.1742-1241.2011.02795.x</pub-id><pub-id pub-id-type="pmid">22171909</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>T</given-names></name><name><surname>Lim</surname><given-names>H</given-names></name><name><surname>Abbu</surname><given-names>KA</given-names></name><name><surname>Qiu</surname><given-names>Y</given-names></name><name><surname>Nussinov</surname><given-names>R</given-names></name><name><surname>Xie</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>MSA-regularized protein sequence transformer toward predicting genome-wide chemical-protein interactions: application to gpcrome deorphanization</article-title><source>Journal of Chemical Information and Modeling</source><volume>61</volume><fpage>1570</fpage><lpage>1582</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.0c01285</pub-id><pub-id pub-id-type="pmid">33757283</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>WKB</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Brender</surname><given-names>JR</given-names></name><name><surname>Hur</surname><given-names>J</given-names></name><name><surname>Özgür</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Glass: a comprehensive database for experimentally validated GPCR-ligand associations</article-title><source>Bioinformatics</source><volume>31</volume><fpage>3035</fpage><lpage>3042</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btv302</pub-id><pub-id pub-id-type="pmid">25971743</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandra</surname><given-names>A</given-names></name><name><surname>Sharma</surname><given-names>A</given-names></name><name><surname>Dehzangi</surname><given-names>A</given-names></name><name><surname>Shigemizu</surname><given-names>D</given-names></name><name><surname>Tsunoda</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Bigram-PGK: phosphoglycerylation prediction using the technique of bigram probabilities of position specific scoring matrix</article-title><source>BMC Molecular and Cell Biology</source><volume>20</volume><elocation-id>57</elocation-id><pub-id pub-id-type="doi">10.1186/s12860-019-0240-1</pub-id><pub-id pub-id-type="pmid">31856704</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandra</surname><given-names>AA</given-names></name><name><surname>Sharma</surname><given-names>A</given-names></name><name><surname>Dehzangi</surname><given-names>A</given-names></name><name><surname>Tsunoda</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>RAM-PGK: prediction of lysine phosphoglycerylation based on residue adjacency matrix</article-title><source>Genes</source><volume>11</volume><elocation-id>1524</elocation-id><pub-id pub-id-type="doi">10.3390/genes11121524</pub-id><pub-id pub-id-type="pmid">33419274</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charte</surname><given-names>D</given-names></name><name><surname>Charte</surname><given-names>F</given-names></name><name><surname>García</surname><given-names>S</given-names></name><name><surname>Herrera</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A snapshot on nonstandard supervised learning problems: taxonomy, relationships, problem transformations and algorithm adaptations</article-title><source>Progress in Artificial Intelligence</source><volume>8</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1007/s13748-018-00167-7</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chefer</surname><given-names>H</given-names></name><name><surname>Gur</surname><given-names>S</given-names></name><name><surname>Wolf</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Transformer interpretability beyond attention visualization</article-title><conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Lin</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>M</given-names></name><name><surname>Gilson</surname><given-names>MK</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The binding database: data management and interface design</article-title><source>Bioinformatics</source><volume>18</volume><fpage>130</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/18.1.130</pub-id><pub-id pub-id-type="pmid">11836221</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>CW</given-names></name><name><surname>Lin</surname><given-names>MH</given-names></name><name><surname>Liao</surname><given-names>CC</given-names></name><name><surname>Chang</surname><given-names>HP</given-names></name><name><surname>Chu</surname><given-names>YW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>IStable 2.0: predicting protein thermal stability changes by integrating various characteristic modules</article-title><source>Computational and Structural Biotechnology Journal</source><volume>18</volume><fpage>622</fpage><lpage>630</lpage><pub-id pub-id-type="doi">10.1016/j.csbj.2020.02.021</pub-id><pub-id pub-id-type="pmid">32226595</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>J</given-names></name><name><surname>Bendjama</surname><given-names>K</given-names></name><name><surname>Rittner</surname><given-names>K</given-names></name><name><surname>Malone</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>BERTMHC: improved MHC-peptide class II interaction prediction with transformer and multiple instance learning</article-title><source>Bioinformatics</source><volume>37</volume><fpage>4172</fpage><lpage>4179</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btab422</pub-id><pub-id pub-id-type="pmid">34096999</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Choromanski</surname><given-names>K</given-names></name><name><surname>Likhosherstov</surname><given-names>V</given-names></name><name><surname>Dohan</surname><given-names>D</given-names></name><name><surname>Song</surname><given-names>X</given-names></name><name><surname>Gane</surname><given-names>A</given-names></name><name><surname>Sarlos</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.03555">https://arxiv.org/abs/2006.03555</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>KC</given-names></name><name><surname>Zhang</surname><given-names>CT</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Prediction of protein structural classes</article-title><source>Critical Reviews in Biochemistry and Molecular Biology</source><volume>30</volume><fpage>275</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.3109/10409239509083488</pub-id><pub-id pub-id-type="pmid">7587280</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>KC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Progresses in predicting post-translational modification</article-title><source>International Journal of Peptide Research and Therapeutics</source><volume>26</volume><fpage>873</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1007/s10989-019-09893-5</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chowdhury</surname><given-names>R</given-names></name><name><surname>Bouatta</surname><given-names>N</given-names></name><name><surname>Biswas</surname><given-names>S</given-names></name><name><surname>Floristean</surname><given-names>C</given-names></name><name><surname>Kharkare</surname><given-names>A</given-names></name><name><surname>Roye</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Single-sequence protein structure prediction using a language model and deep learning</article-title><source>Nature Biotechnology</source><volume>22</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/s41587-022-01432-w</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>J</given-names></name><name><surname>Gulcehre</surname><given-names>C</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.3555">https://arxiv.org/abs/1412.3555</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>K</given-names></name><name><surname>Luong</surname><given-names>MT</given-names></name><name><surname>Le</surname><given-names>QV</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>ELECTRA: Pre-Training Text Encoders as Discriminators Rather Than Generators</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2003.10555">https://arxiv.org/abs/2003.10555</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cortés</surname><given-names>GA</given-names></name><name><surname>Aguilar-Ruiz</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Predicting protein distance maps according to physicochemical properties</article-title><source>Journal of Integrative Bioinformatics</source><volume>8</volume><fpage>158</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1515/jib-2011-181</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Cohen</surname><given-names>WW</given-names></name><name><surname>Carbonell</surname><given-names>J</given-names></name><name><surname>Le</surname><given-names>QV</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Transformer-xl: language modeling with longer-term dependency</data-title><source>Transformer-Xl</source></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Carbonell</surname><given-names>J</given-names></name><name><surname>Le</surname><given-names>QV</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Transformer-Xl: Attentive Language Models beyond a Fixed-Length Context</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1901.02860">https://arxiv.org/abs/1901.02860</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>MI</given-names></name><name><surname>Hunt</surname><given-names>JP</given-names></name><name><surname>Herrgard</surname><given-names>S</given-names></name><name><surname>Ciceri</surname><given-names>P</given-names></name><name><surname>Wodicka</surname><given-names>LM</given-names></name><name><surname>Pallares</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Comprehensive analysis of kinase inhibitor selectivity</article-title><source>Nature Biotechnology</source><volume>29</volume><fpage>1046</fpage><lpage>1051</lpage><pub-id pub-id-type="doi">10.1038/nbt.1990</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dehghani</surname><given-names>M</given-names></name><name><surname>Gouws</surname><given-names>S</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Kaiser</surname><given-names>Ł</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Universal Transformers</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.03819">https://arxiv.org/abs/1807.03819</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>MW</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Toutanova</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</ext-link></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dick</surname><given-names>K</given-names></name><name><surname>Green</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reciprocal perspective for improved protein-protein interaction prediction</article-title><source>Scientific Reports</source><volume>8</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41598-018-30044-1</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dodge</surname><given-names>C</given-names></name><name><surname>Schneider</surname><given-names>R</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The HSSP database of protein structure-sequence alignments and family profiles</article-title><source>Nucleic Acids Research</source><volume>26</volume><fpage>313</fpage><lpage>315</lpage><pub-id pub-id-type="doi">10.1093/nar/26.1.313</pub-id><pub-id pub-id-type="pmid">9399862</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname><given-names>Z</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Ye</surname><given-names>L</given-names></name><name><surname>Wei</surname><given-names>H</given-names></name><name><surname>Peng</surname><given-names>Z</given-names></name><name><surname>Anishchenko</surname><given-names>I</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The trrosetta server for fast and accurate protein structure prediction</article-title><source>Nature Protocols</source><volume>16</volume><fpage>5634</fpage><lpage>5651</lpage><pub-id pub-id-type="doi">10.1038/s41596-021-00628-9</pub-id><pub-id pub-id-type="pmid">34759384</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ehrenberger</surname><given-names>T</given-names></name><name><surname>Cantley</surname><given-names>LC</given-names></name><name><surname>Yaffe</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Computational prediction of protein-protein interactions</article-title><source>Methods in Molecular Biology</source><volume>1278</volume><fpage>57</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1007/978-1-4939-2425-7_4</pub-id><pub-id pub-id-type="pmid">25859943</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>ElAbd</surname><given-names>H</given-names></name><name><surname>Bromberg</surname><given-names>Y</given-names></name><name><surname>Hoarfrost</surname><given-names>A</given-names></name><name><surname>Lenz</surname><given-names>T</given-names></name><name><surname>Franke</surname><given-names>A</given-names></name><name><surname>Wendorff</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Amino acid encoding for deep learning applications</article-title><source>BMC Bioinformatics</source><volume>21</volume><elocation-id>235</elocation-id><pub-id pub-id-type="doi">10.1186/s12859-020-03546-x</pub-id><pub-id pub-id-type="pmid">32517697</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>ElGebali</surname><given-names>S</given-names></name><name><surname>Mistry</surname><given-names>J</given-names></name><name><surname>Bateman</surname><given-names>A</given-names></name><name><surname>Eddy</surname><given-names>SR</given-names></name><name><surname>Luciani</surname><given-names>A</given-names></name><name><surname>Potter</surname><given-names>SC</given-names></name><name><surname>Qureshi</surname><given-names>M</given-names></name><name><surname>Richardson</surname><given-names>LJ</given-names></name><name><surname>Salazar</surname><given-names>GA</given-names></name><name><surname>Smart</surname><given-names>A</given-names></name><name><surname>Sonnhammer</surname><given-names>ELL</given-names></name><name><surname>Hirsh</surname><given-names>L</given-names></name><name><surname>Paladin</surname><given-names>L</given-names></name><name><surname>Piovesan</surname><given-names>D</given-names></name><name><surname>Tosatto</surname><given-names>SCE</given-names></name><name><surname>Finn</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The pfam protein families database in 2019</article-title><source>Nucleic Acids Research</source><volume>47</volume><fpage>D427</fpage><lpage>D432</lpage><pub-id pub-id-type="doi">10.1093/nar/gky995</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elman</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Finding structure in time</article-title><source>Cognitive Science</source><volume>14</volume><fpage>179</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1207/s15516709cog1402_1</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Elnaggar</surname><given-names>A</given-names></name><name><surname>Heinzinger</surname><given-names>M</given-names></name><name><surname>Dallago</surname><given-names>C</given-names></name><name><surname>Rehawi</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gibbs</surname><given-names>T</given-names></name><name><surname>Feher</surname><given-names>T</given-names></name><name><surname>Angerer</surname><given-names>C</given-names></name><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Bhowmik</surname><given-names>D</given-names></name><name><surname>Rost</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.07.12.199554</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Elnaggar</surname><given-names>A</given-names></name><name><surname>Heinzinger</surname><given-names>M</given-names></name><name><surname>Dallago</surname><given-names>C</given-names></name><name><surname>Rehawi</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gibbs</surname><given-names>T</given-names></name><name><surname>Feher</surname><given-names>T</given-names></name><name><surname>Angerer</surname><given-names>C</given-names></name><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Bhowmik</surname><given-names>D</given-names></name><name><surname>Rost</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>ProtTrans: Towards Cracking the Language of Life’s Code through Self-Supervised Deep Learning and High Performance Computing</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2007.06225">https://arxiv.org/abs/2007.06225</ext-link></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A critical review of five machine learning-based algorithms for predicting protein stability changes upon mutation</article-title><source>Briefings in Bioinformatics</source><volume>21</volume><fpage>1285</fpage><lpage>1292</lpage><pub-id pub-id-type="doi">10.1093/bib/bbz071</pub-id><pub-id pub-id-type="pmid">31273374</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Fedus</surname><given-names>W</given-names></name><name><surname>Zoph</surname><given-names>B</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2101.03961">https://arxiv.org/abs/2101.03961</ext-link></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferruz</surname><given-names>N</given-names></name><name><surname>Schmidt</surname><given-names>S</given-names></name><name><surname>Höcker</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>ProtGPT2 is a deep unsupervised language model for protein design</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>4348</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-32007-7</pub-id><pub-id pub-id-type="pmid">35896542</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finn</surname><given-names>RD</given-names></name><name><surname>Clements</surname><given-names>J</given-names></name><name><surname>Eddy</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>HMMER web server: interactive sequence similarity searching</article-title><source>Nucleic Acids Research</source><volume>39</volume><fpage>W29</fpage><lpage>W37</lpage><pub-id pub-id-type="doi">10.1093/nar/gkr367</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaulton</surname><given-names>A</given-names></name><name><surname>Bellis</surname><given-names>LJ</given-names></name><name><surname>Bento</surname><given-names>AP</given-names></name><name><surname>Chambers</surname><given-names>J</given-names></name><name><surname>Davies</surname><given-names>M</given-names></name><name><surname>Hersey</surname><given-names>A</given-names></name><name><surname>Light</surname><given-names>Y</given-names></name><name><surname>McGlinchey</surname><given-names>S</given-names></name><name><surname>Michalovich</surname><given-names>D</given-names></name><name><surname>Al-Lazikani</surname><given-names>B</given-names></name><name><surname>Overington</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>ChEMBL: a large-scale bioactivity database for drug discovery</article-title><source>Nucleic Acids Research</source><volume>40</volume><fpage>D1100</fpage><lpage>D1107</lpage><pub-id pub-id-type="doi">10.1093/nar/gkr777</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golubchik</surname><given-names>T</given-names></name><name><surname>Wise</surname><given-names>MJ</given-names></name><name><surname>Easteal</surname><given-names>S</given-names></name><name><surname>Jermiin</surname><given-names>LS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Mind the gaps: evidence of bias in estimates of multiple sequence alignments</article-title><source>Molecular Biology and Evolution</source><volume>24</volume><fpage>2433</fpage><lpage>2442</lpage><pub-id pub-id-type="doi">10.1093/molbev/msm176</pub-id><pub-id pub-id-type="pmid">17709332</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Google Scholar</collab></person-group><year iso-8601-date="2022">2022</year><article-title>Google Scholar</article-title><ext-link ext-link-type="uri" xlink:href="https://scholar.google.com/scholar?q=transformer+language+model+transformer+language+model&amp;hl=en&amp;as_sdt=0,5">https://scholar.google.com/scholar?q=transformer+language+model+transformer+language+model&amp;hl=en&amp;as_sdt=0,5</ext-link><date-in-citation iso-8601-date="2022-07-01">July 1, 2022</date-in-citation></element-citation></ref><ref id="bib53"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gromiha</surname><given-names>MM</given-names></name><name><surname>Nagarajan</surname><given-names>R</given-names></name><name><surname>Selvaraj</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Protein structural bioinformatics: an overview</data-title><source>Protein Structural Bioinformatics</source></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guerler</surname><given-names>A</given-names></name><name><surname>Govindarajoo</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Mapping monomeric threading to protein-protein structure prediction</article-title><source>Journal of Chemical Information and Modeling</source><volume>53</volume><fpage>717</fpage><lpage>725</lpage><pub-id pub-id-type="doi">10.1021/ci300579r</pub-id><pub-id pub-id-type="pmid">23413988</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hahn</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Theoretical limitations of self-attention in neural sequence models</article-title><source>Transactions of the Association for Computational Linguistics</source><volume>8</volume><fpage>156</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1162/tacl_a_00306</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hanin</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Which neural net architectures give rise to exploding and vanishing gradients?</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hao</surname><given-names>Y</given-names></name><name><surname>Dong</surname><given-names>L</given-names></name><name><surname>Wei</surname><given-names>F</given-names></name><name><surname>Xu</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Self-attention attribution: interpreting information interactions inside transformer</article-title><conf-name>Proceedings of the AAAI Conference on Artificial Intelligence</conf-name><fpage>12963</fpage><lpage>12971</lpage><pub-id pub-id-type="doi">10.1609/aaai.v35i14.17533</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>He</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Wu</surname><given-names>L</given-names></name><name><surname>Xia</surname><given-names>H</given-names></name><name><surname>Ju</surname><given-names>F</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Pre-Training Co-Evolutionary Protein Representation via A Pairwise Masked Language Model</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2110.15527">https://arxiv.org/abs/2110.15527</ext-link></element-citation></ref><ref id="bib59"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Heinzinger</surname><given-names>M</given-names></name><name><surname>Elnaggar</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Dallago</surname><given-names>C</given-names></name><name><surname>Nechaev</surname><given-names>D</given-names></name><name><surname>Matthes</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Modeling the Language of Life–Deep Learning Protein Sequences</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/614313</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Heinzinger</surname><given-names>M</given-names></name><name><surname>Littmann</surname><given-names>M</given-names></name><name><surname>Sillitoe</surname><given-names>I</given-names></name><name><surname>Bordin</surname><given-names>N</given-names></name><name><surname>Orengo</surname><given-names>C</given-names></name><name><surname>Rost</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Contrastive Learning on Protein Embeddings Enlightens Midnight Zone at Lightning Speed</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.11.14.468528</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinzinger</surname><given-names>M</given-names></name><name><surname>Littmann</surname><given-names>M</given-names></name><name><surname>Sillitoe</surname><given-names>I</given-names></name><name><surname>Bordin</surname><given-names>N</given-names></name><name><surname>Orengo</surname><given-names>C</given-names></name><name><surname>Rost</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Contrastive learning on protein embeddings enlightens midnight zone</article-title><source>NAR Genomics and Bioinformatics</source><volume>4</volume><elocation-id>lqac043</elocation-id><pub-id pub-id-type="doi">10.1093/nargab/lqac043</pub-id><pub-id pub-id-type="pmid">35702380</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Long short-term memory</article-title><source>Neural Computation</source><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>L</given-names></name><name><surname>Sun</surname><given-names>S</given-names></name><name><surname>Zheng</surname><given-names>L</given-names></name><name><surname>Tan</surname><given-names>Q</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>FastMSA: Accelerating Multiple Sequence Alignment with Dense Retrieval on Protein Language</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.12.20.473431</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Xu</surname><given-names>W</given-names></name><name><surname>Yu</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Bidirectional LSTM-CRF Models for Sequence Tagging</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1508.01991">https://arxiv.org/abs/1508.01991</ext-link></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Xiao</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Chou</surname><given-names>KC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>ISuc-pseopt: identifying lysine succinylation sites in proteins by incorporating sequence-coupling effects into pseudo components and optimizing imbalanced training dataset</article-title><source>Analytical Biochemistry</source><volume>497</volume><fpage>48</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1016/j.ab.2015.12.009</pub-id><pub-id pub-id-type="pmid">26723495</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Q</given-names></name><name><surname>Jin</surname><given-names>X</given-names></name><name><surname>Lee</surname><given-names>SJ</given-names></name><name><surname>Yao</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Protein secondary structure prediction: A survey of the state of the art</article-title><source>Journal of Molecular Graphics &amp; Modelling</source><volume>76</volume><fpage>379</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1016/j.jmgm.2017.07.015</pub-id><pub-id pub-id-type="pmid">28763690</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>T</given-names></name><name><surname>Fang</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>MutFormer: A Context-Dependent Transformer-Based Model to Predict Pathogenic Missense Mutations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2110.14746">https://arxiv.org/abs/2110.14746</ext-link></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>DT</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Protein secondary structure prediction based on position-specific scoring matrices</article-title><source>Journal of Molecular Biology</source><volume>292</volume><fpage>195</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1006/jmbi.1999.3091</pub-id><pub-id pub-id-type="pmid">10493868</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jumper</surname><given-names>J</given-names></name><name><surname>Evans</surname><given-names>R</given-names></name><name><surname>Pritzel</surname><given-names>A</given-names></name><name><surname>Green</surname><given-names>T</given-names></name><name><surname>Figurnov</surname><given-names>M</given-names></name><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Tunyasuvunakool</surname><given-names>K</given-names></name><name><surname>Bates</surname><given-names>R</given-names></name><name><surname>Žídek</surname><given-names>A</given-names></name><name><surname>Potapenko</surname><given-names>A</given-names></name><name><surname>Bridgland</surname><given-names>A</given-names></name><name><surname>Meyer</surname><given-names>C</given-names></name><name><surname>Kohl</surname><given-names>SAA</given-names></name><name><surname>Ballard</surname><given-names>AJ</given-names></name><name><surname>Cowie</surname><given-names>A</given-names></name><name><surname>Romera-Paredes</surname><given-names>B</given-names></name><name><surname>Nikolov</surname><given-names>S</given-names></name><name><surname>Jain</surname><given-names>R</given-names></name><name><surname>Adler</surname><given-names>J</given-names></name><name><surname>Back</surname><given-names>T</given-names></name><name><surname>Petersen</surname><given-names>S</given-names></name><name><surname>Reiman</surname><given-names>D</given-names></name><name><surname>Clancy</surname><given-names>E</given-names></name><name><surname>Zielinski</surname><given-names>M</given-names></name><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Pacholska</surname><given-names>M</given-names></name><name><surname>Berghammer</surname><given-names>T</given-names></name><name><surname>Bodenstein</surname><given-names>S</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Senior</surname><given-names>AW</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Kohli</surname><given-names>P</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Highly accurate protein structure prediction with alphafold</article-title><source>Nature</source><volume>596</volume><fpage>583</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id><pub-id pub-id-type="pmid">34265844</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katchalski-Katzir</surname><given-names>E</given-names></name><name><surname>Shariv</surname><given-names>I</given-names></name><name><surname>Eisenstein</surname><given-names>M</given-names></name><name><surname>Friesem</surname><given-names>AA</given-names></name><name><surname>Aflalo</surname><given-names>C</given-names></name><name><surname>Vakser</surname><given-names>IA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Molecular surface recognition: determination of geometric fit between proteins and their ligands by correlation techniques</article-title><source>PNAS</source><volume>89</volume><fpage>2195</fpage><lpage>2199</lpage><pub-id pub-id-type="doi">10.1073/pnas.89.6.2195</pub-id><pub-id pub-id-type="pmid">1549581</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khalili</surname><given-names>E</given-names></name><name><surname>Ramazi</surname><given-names>S</given-names></name><name><surname>Ghanati</surname><given-names>F</given-names></name><name><surname>Kouchaki</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Predicting protein phosphorylation sites in soybean using interpretable deep tabular learning network</article-title><source>Briefings in Bioinformatics</source><volume>23</volume><elocation-id>bbac015</elocation-id><pub-id pub-id-type="doi">10.1093/bib/bbac015</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Khan</surname><given-names>M</given-names></name><name><surname>Jan</surname><given-names>B</given-names></name><name><surname>Farman</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep learning: convergence to big data analytics</article-title><conf-name>Deep Learning Methods and Applications</conf-name><fpage>31</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1007/978-981-13-3459-7</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kitaev</surname><given-names>N</given-names></name><name><surname>Kaiser</surname><given-names>Ł</given-names></name><name><surname>Levskaya</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Reformer: The Efficient Transformer</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2001.04451">https://arxiv.org/abs/2001.04451</ext-link></element-citation></ref><ref id="bib74"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ko</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Can AlphaFold2 Predict Protein-Peptide Complex Structures Accurately?</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.07.27.453972</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koumakis</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep learning models in genomics; are we there yet?</article-title><source>Computational and Structural Biotechnology Journal</source><volume>18</volume><fpage>1466</fpage><lpage>1473</lpage><pub-id pub-id-type="doi">10.1016/j.csbj.2020.06.017</pub-id><pub-id pub-id-type="pmid">32637044</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kryshtafovych</surname><given-names>A</given-names></name><name><surname>Schwede</surname><given-names>T</given-names></name><name><surname>Topf</surname><given-names>M</given-names></name><name><surname>Fidelis</surname><given-names>K</given-names></name><name><surname>Moult</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Critical assessment of methods of protein structure prediction (CASP)-round XIII</article-title><source>Proteins</source><volume>87</volume><fpage>1011</fpage><lpage>1020</lpage><pub-id pub-id-type="doi">10.1002/prot.25823</pub-id><pub-id pub-id-type="pmid">31589781</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhlman</surname><given-names>B</given-names></name><name><surname>Bradley</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Advances in protein structure prediction and design</article-title><source>Nature Reviews. Molecular Cell Biology</source><volume>20</volume><fpage>681</fpage><lpage>697</lpage><pub-id pub-id-type="doi">10.1038/s41580-019-0163-x</pub-id><pub-id pub-id-type="pmid">31417196</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lan</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Goodman</surname><given-names>S</given-names></name><name><surname>Gimpel</surname><given-names>K</given-names></name><name><surname>Sharma</surname><given-names>P</given-names></name><name><surname>Soricut</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Albert: A Lite Bert for Self-Supervised Learning of Language Representations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1909.11942">https://arxiv.org/abs/1909.11942</ext-link></element-citation></ref><ref id="bib79"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lanchantin</surname><given-names>J</given-names></name><name><surname>Weingarten</surname><given-names>T</given-names></name><name><surname>Sekhon</surname><given-names>A</given-names></name><name><surname>Miller</surname><given-names>C</given-names></name><name><surname>Qi</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Transfer learning for predicting virus-host protein interactions for novel virus sequences</article-title><conf-name>BCB ’21: Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics</conf-name><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1145/3459930.3469527</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Laskar</surname><given-names>MTR</given-names></name><name><surname>Huang</surname><given-names>X</given-names></name><name><surname>Hoque</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Contextualized embeddings based transformer encoder for sentence similarity modeling in answer selection task</article-title><conf-name>Proceedings of The 12th Language Resources and Evaluation Conference</conf-name><fpage>5505</fpage><lpage>5514</lpage></element-citation></ref><ref id="bib81"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>O</given-names></name><name><surname>Goldberg</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dependency-based word embeddings</article-title><conf-name>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</conf-name></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Dong</surname><given-names>S</given-names></name><name><surname>Leier</surname><given-names>A</given-names></name><name><surname>Han</surname><given-names>M</given-names></name><name><surname>Guo</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Pan</surname><given-names>S</given-names></name><name><surname>Jia</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Webb</surname><given-names>GI</given-names></name><name><surname>Coin</surname><given-names>LJM</given-names></name><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Song</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Positive-unlabeled learning in bioinformatics and computational biology: a brief review</article-title><source>Briefings in Bioinformatics</source><volume>23</volume><elocation-id>bbab461</elocation-id><pub-id pub-id-type="doi">10.1093/bib/bbab461</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Y</given-names></name><name><surname>Wen</surname><given-names>X</given-names></name><name><surname>Jorissen</surname><given-names>RN</given-names></name><name><surname>Gilson</surname><given-names>MK</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>BindingDB: a web-accessible database of experimentally determined protein-ligand binding affinities</article-title><source>Nucleic Acids Research</source><volume>35</volume><fpage>D198</fpage><lpage>D201</lpage><pub-id pub-id-type="doi">10.1093/nar/gkl999</pub-id><pub-id pub-id-type="pmid">17145705</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Dong</surname><given-names>Q</given-names></name><name><surname>Lan</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Using amino acid physicochemical distance transformation for fast protein remote homology detection</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e46633</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0046633</pub-id><pub-id pub-id-type="pmid">23029559</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Li</surname><given-names>CC</given-names></name><name><surname>Yan</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>DeepSVM-fold: protein fold recognition by combining support vector machines and pairwise sequence similarity scores generated by deep learning networks</article-title><source>Briefings in Bioinformatics</source><volume>21</volume><fpage>1733</fpage><lpage>1741</lpage><pub-id pub-id-type="doi">10.1093/bib/bbz098</pub-id><pub-id pub-id-type="pmid">31665221</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>López</surname><given-names>Y</given-names></name><name><surname>Dehzangi</surname><given-names>A</given-names></name><name><surname>Lal</surname><given-names>SP</given-names></name><name><surname>Taherzadeh</surname><given-names>G</given-names></name><name><surname>Michaelson</surname><given-names>J</given-names></name><name><surname>Sattar</surname><given-names>A</given-names></name><name><surname>Tsunoda</surname><given-names>T</given-names></name><name><surname>Sharma</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>SucStruct: prediction of succinylated lysine residues by using structural properties of amino acids</article-title><source>Analytical Biochemistry</source><volume>527</volume><fpage>24</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1016/j.ab.2017.03.021</pub-id><pub-id pub-id-type="pmid">28363440</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Lopez</surname><given-names>MJ</given-names></name><name><surname>Mohiuddin</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Biochemistry, essential amino acids</data-title><source>Biochemistry</source></element-citation></ref><ref id="bib88"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>T</given-names></name><name><surname>Lu</surname><given-names>AX</given-names></name><name><surname>Moses</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Random Embeddings and Linear Regression Can Predict Protein Function</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2104.14661">https://arxiv.org/abs/2104.14661</ext-link></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDowall</surname><given-names>MD</given-names></name><name><surname>Scott</surname><given-names>MS</given-names></name><name><surname>Barton</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Pips: human protein-protein interaction prediction database</article-title><source>Nucleic Acids Research</source><volume>37</volume><fpage>D651</fpage><lpage>D656</lpage><pub-id pub-id-type="doi">10.1093/nar/gkn870</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mikolov</surname><given-names>T</given-names></name><name><surname>Kombrink</surname><given-names>S</given-names></name><name><surname>Burget</surname><given-names>L</given-names></name><name><surname>Černocký</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Extensions of recurrent neural network language model</article-title><conf-name>2011 IEEE international conference on acoustics, speech and signal processing (ICASSP); 2011: IEEE</conf-name><pub-id pub-id-type="doi">10.1109/ICASSP.2011.5947611</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mikolov</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>K</given-names></name><name><surname>Corrado</surname><given-names>GS</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>International Conference on Learning Representations</article-title><conf-name>Efficient Estimation of Word Representations in Vector Space</conf-name></element-citation></ref><ref id="bib92"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mikolov</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>K</given-names></name><name><surname>Corrado</surname><given-names>G</given-names></name><name><surname>Dean</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>Efficient Estimation of Word Representations in Vector Space</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</ext-link></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirdita</surname><given-names>M</given-names></name><name><surname>Schütze</surname><given-names>K</given-names></name><name><surname>Moriwaki</surname><given-names>Y</given-names></name><name><surname>Heo</surname><given-names>L</given-names></name><name><surname>Ovchinnikov</surname><given-names>S</given-names></name><name><surname>Steinegger</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>ColabFold: making protein folding accessible to all</article-title><source>Nature Methods</source><volume>19</volume><fpage>679</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01488-1</pub-id><pub-id pub-id-type="pmid">35637307</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mistry</surname><given-names>J</given-names></name><name><surname>Chuguransky</surname><given-names>S</given-names></name><name><surname>Williams</surname><given-names>L</given-names></name><name><surname>Qureshi</surname><given-names>M</given-names></name><name><surname>Salazar</surname><given-names>GA</given-names></name><name><surname>Sonnhammer</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Pfam: the protein families database in 2021</article-title><source>Nucleic Acids Research</source><volume>49</volume><fpage>D412</fpage><lpage>D419</lpage><pub-id pub-id-type="doi">10.1093/nar/gkaa913</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nambiar</surname><given-names>A</given-names></name><name><surname>Heflin</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Maslov</surname><given-names>S</given-names></name><name><surname>Hopkins</surname><given-names>M</given-names></name><name><surname>Ritz</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks</article-title><conf-name>BCB ’20: Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics</conf-name><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1145/3388440.3412467</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Narayanan</surname><given-names>D</given-names></name><name><surname>Shoeybi</surname><given-names>M</given-names></name><name><surname>Casper</surname><given-names>J</given-names></name><name><surname>LeGresley</surname><given-names>P</given-names></name><name><surname>Patwary</surname><given-names>M</given-names></name><name><surname>Korthikanti</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Efficient large-scale language model training on gpu clusters using megatron-lm</article-title><conf-name>Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</conf-name></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nauman</surname><given-names>M</given-names></name><name><surname>Ur Rehman</surname><given-names>H</given-names></name><name><surname>Politano</surname><given-names>G</given-names></name><name><surname>Benso</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Beyond homology transfer: deep learning for automated annotation of proteins</article-title><source>Journal of Grid Computing</source><volume>17</volume><fpage>225</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1007/s10723-018-9450-6</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ofer</surname><given-names>D</given-names></name><name><surname>Brandes</surname><given-names>N</given-names></name><name><surname>Linial</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The language of proteins: Nlp, machine learning &amp; protein sequences</article-title><source>Computational and Structural Biotechnology Journal</source><volume>19</volume><fpage>1750</fpage><lpage>1758</lpage><pub-id pub-id-type="doi">10.1016/j.csbj.2021.03.022</pub-id><pub-id pub-id-type="pmid">33897979</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Öztürk</surname><given-names>H</given-names></name><name><surname>Özgür</surname><given-names>A</given-names></name><name><surname>Ozkirimli</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepDTA: deep drug-target binding affinity prediction</article-title><source>Bioinformatics</source><volume>34</volume><fpage>i821</fpage><lpage>i829</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bty593</pub-id><pub-id pub-id-type="pmid">30423097</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Q</given-names></name><name><surname>Nguyen</surname><given-names>TB</given-names></name><name><surname>Ascher</surname><given-names>DB</given-names></name><name><surname>Pires</surname><given-names>DEV</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Systematic evaluation of computational tools to predict the effects of mutations on protein stability in the absence of experimental structures</article-title><source>Briefings in Bioinformatics</source><volume>23</volume><elocation-id>bbac025</elocation-id><pub-id pub-id-type="doi">10.1093/bib/bbac025</pub-id><pub-id pub-id-type="pmid">35189634</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pascanu</surname><given-names>R</given-names></name><name><surname>Mikolov</surname><given-names>T</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>On the difficulty of training recurrent neural networks</article-title><conf-name>International conference on machine learning; 2013: PMLR</conf-name></element-citation></ref><ref id="bib102"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Peters</surname><given-names>M</given-names></name><name><surname>Neumann</surname><given-names>M</given-names></name><name><surname>Iyyer</surname><given-names>M</given-names></name><name><surname>Gardner</surname><given-names>M</given-names></name><name><surname>Clark</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Zettlemoyer</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep Contextualized Word Representations</article-title><conf-name>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</conf-name><pub-id pub-id-type="doi">10.18653/v1/N18-1202</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Pfam 35.0</collab></person-group><year iso-8601-date="2021">2021</year><article-title>Pfam 35.0 is released 2021</article-title><ext-link ext-link-type="uri" xlink:href="https://xfam.wordpress.com/2021/11/19/pfam-35-0-is-released">https://xfam.wordpress.com/2021/11/19/pfam-35-0-is-released</ext-link><date-in-citation iso-8601-date="2022-07-01">July 1, 2022</date-in-citation></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phuong</surname><given-names>TM</given-names></name><name><surname>Do</surname><given-names>CB</given-names></name><name><surname>Edgar</surname><given-names>RC</given-names></name><name><surname>Batzoglou</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Multiple alignment of protein sequences with repeats and rearrangements</article-title><source>Nucleic Acids Research</source><volume>34</volume><fpage>5932</fpage><lpage>5942</lpage><pub-id pub-id-type="doi">10.1093/nar/gkl511</pub-id><pub-id pub-id-type="pmid">17068081</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiao</surname><given-names>Y</given-names></name><name><surname>Zhu</surname><given-names>X</given-names></name><name><surname>Gong</surname><given-names>H</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>BERT-kcr: prediction of lysine crotonylation sites by a transfer learning method with pre-trained BERT models</article-title><source>Bioinformatics</source><volume>38</volume><fpage>648</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btab712</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Raffel</surname><given-names>C</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Roberts</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Narang</surname><given-names>S</given-names></name><name><surname>Matena</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1910.10683">https://arxiv.org/abs/1910.10683</ext-link></element-citation></ref><ref id="bib107"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Raghu</surname><given-names>M</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Dosovitskiy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Do vision transformers see like convolutional neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramazi</surname><given-names>S</given-names></name><name><surname>Zahiri</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Posttranslational modifications in proteins: resources, tools and prediction methods</article-title><source>Database</source><volume>2021</volume><elocation-id>baab012</elocation-id><pub-id pub-id-type="doi">10.1093/database/baab012</pub-id><pub-id pub-id-type="pmid">33826699</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>R</given-names></name><name><surname>Bhattacharya</surname><given-names>N</given-names></name><name><surname>Thomas</surname><given-names>N</given-names></name><name><surname>Duan</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Canny</surname><given-names>J</given-names></name><name><surname>Abbeel</surname><given-names>P</given-names></name><name><surname>Song</surname><given-names>YS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Evaluating protein transfer learning with TAPE</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>9689</fpage><lpage>9701</lpage></element-citation></ref><ref id="bib110"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RM</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Verkuil</surname><given-names>R</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Canny</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>MSA transformer</article-title><conf-name>International Conference on Machine Learning; 2021: PMLR</conf-name></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rives</surname><given-names>A</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Goyal</surname><given-names>S</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Guo</surname><given-names>D</given-names></name><name><surname>Ott</surname><given-names>M</given-names></name><name><surname>Zitnick</surname><given-names>CL</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2016239118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id><pub-id pub-id-type="pmid">33876751</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saethang</surname><given-names>T</given-names></name><name><surname>Payne</surname><given-names>DM</given-names></name><name><surname>Avihingsanon</surname><given-names>Y</given-names></name><name><surname>Pisitkun</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A machine learning strategy for predicting localization of post-translational modification sites in protein-protein interacting regions</article-title><source>BMC Bioinformatics</source><volume>17</volume><elocation-id>307</elocation-id><pub-id pub-id-type="doi">10.1186/s12859-016-1165-8</pub-id><pub-id pub-id-type="pmid">27534850</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmiedel</surname><given-names>JM</given-names></name><name><surname>Lehner</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Determining protein structures using deep mutagenesis</article-title><source>Nature Genetics</source><volume>51</volume><fpage>1177</fpage><lpage>1186</lpage><pub-id pub-id-type="doi">10.1038/s41588-019-0431-x</pub-id><pub-id pub-id-type="pmid">31209395</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>A</given-names></name><name><surname>Lyons</surname><given-names>J</given-names></name><name><surname>Dehzangi</surname><given-names>A</given-names></name><name><surname>Paliwal</surname><given-names>KK</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A feature extraction technique using bi-gram probabilities of position specific scoring matrix for protein fold recognition</article-title><source>Journal of Theoretical Biology</source><volume>320</volume><fpage>41</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/j.jtbi.2012.12.008</pub-id><pub-id pub-id-type="pmid">23246717</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>Q</given-names></name><name><surname>Chen</surname><given-names>W</given-names></name><name><surname>Huang</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Xue</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Deep learning for mining protein data</article-title><source>Briefings in Bioinformatics</source><volume>22</volume><fpage>194</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1093/bib/bbz156</pub-id><pub-id pub-id-type="pmid">31867611</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>J</given-names></name><name><surname>Litfin</surname><given-names>T</given-names></name><name><surname>Paliwal</surname><given-names>K</given-names></name><name><surname>Singh</surname><given-names>J</given-names></name><name><surname>Hanumanthappa</surname><given-names>AK</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Martelli</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>SPOT-1D-single: improving the single-sequence-based prediction of protein secondary structure, backbone angles, solvent accessibility and half-sphere exposures using a large training set and ensembled deep learning</article-title><source>Bioinformatics</source><volume>37</volume><fpage>3464</fpage><lpage>3472</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btab316</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>J</given-names></name><name><surname>Litfin</surname><given-names>T</given-names></name><name><surname>Singh</surname><given-names>J</given-names></name><name><surname>Paliwal</surname><given-names>K</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>SPOT-contact-LM: improving single-sequence-based prediction of protein contact MAP using a transformer language model</article-title><source>Bioinformatics</source><volume>38</volume><fpage>1888</fpage><lpage>1894</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btac053</pub-id><pub-id pub-id-type="pmid">35104320</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>S</given-names></name><name><surname>Patwary</surname><given-names>M</given-names></name><name><surname>Norick</surname><given-names>B</given-names></name><name><surname>LeGresley</surname><given-names>P</given-names></name><name><surname>Rajbhandari</surname><given-names>S</given-names></name><name><surname>Casper</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Using Deepspeed and Megatron to Train Megatron-Turing Nlg 530b, a Large-Scale Generative Language Model</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2201.11990">https://arxiv.org/abs/2201.11990</ext-link></element-citation></ref><ref id="bib119"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sourkov</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Igloo: Slicing the Features Space to Represent Sequences</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.03402">https://arxiv.org/abs/1807.03402</ext-link></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Söding</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</article-title><source>Nature Biotechnology</source><volume>35</volume><fpage>1026</fpage><lpage>1028</lpage><pub-id pub-id-type="doi">10.1038/nbt.3988</pub-id><pub-id pub-id-type="pmid">29035372</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Mirdita</surname><given-names>M</given-names></name><name><surname>Söding</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold</article-title><source>Nature Methods</source><volume>16</volume><fpage>603</fpage><lpage>606</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0437-4</pub-id><pub-id pub-id-type="pmid">31235882</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sturmfels</surname><given-names>P</given-names></name><name><surname>Vig</surname><given-names>J</given-names></name><name><surname>Madani</surname><given-names>A</given-names></name><name><surname>Rajani</surname><given-names>NF</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Profile Prediction: An Alignment-Based Pre-Training Task for Protein Sequence Models</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2012.00195">https://arxiv.org/abs/2012.00195</ext-link></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sułkowska</surname><given-names>JI</given-names></name><name><surname>Morcos</surname><given-names>F</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name><name><surname>Hwa</surname><given-names>T</given-names></name><name><surname>Onuchic</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Genomics-aided structure prediction</article-title><source>PNAS</source><volume>109</volume><fpage>10340</fpage><lpage>10345</lpage><pub-id pub-id-type="doi">10.1073/pnas.1207864109</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>J</given-names></name><name><surname>Szwajda</surname><given-names>A</given-names></name><name><surname>Shakyawar</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>T</given-names></name><name><surname>Hintsanen</surname><given-names>P</given-names></name><name><surname>Wennerberg</surname><given-names>K</given-names></name><name><surname>Aittokallio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Making sense of large-scale kinase inhibitor bioactivity data sets: a comparative and integrative analysis</article-title><source>Journal of Chemical Information and Modeling</source><volume>54</volume><fpage>735</fpage><lpage>743</lpage><pub-id pub-id-type="doi">10.1021/ci400709d</pub-id><pub-id pub-id-type="pmid">24521231</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavares</surname><given-names>LS</given-names></name><name><surname>Silva</surname><given-names>CSF</given-names></name><name><surname>de Souza</surname><given-names>VC</given-names></name><name><surname>da Silva</surname><given-names>VL</given-names></name><name><surname>Diniz</surname><given-names>CG</given-names></name><name><surname>Santos</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Strategies and molecular tools to fight antimicrobial resistance: resistome, transcriptome, and antimicrobial peptides</article-title><source>Frontiers in Microbiology</source><volume>4</volume><elocation-id>412</elocation-id><pub-id pub-id-type="doi">10.3389/fmicb.2013.00412</pub-id><pub-id pub-id-type="pmid">24427156</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tran</surname><given-names>K</given-names></name><name><surname>Bisazza</surname><given-names>A</given-names></name><name><surname>Monz</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Importance of Being Recurrent for Modeling Hierarchical Structure</article-title><conf-name>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</conf-name><pub-id pub-id-type="doi">10.18653/v1/D18-1503</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trost</surname><given-names>B</given-names></name><name><surname>Kusalik</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Computational phosphorylation site prediction in plants using random forests and organism-specific instance weights</article-title><source>Bioinformatics</source><volume>29</volume><fpage>686</fpage><lpage>694</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btt031</pub-id><pub-id pub-id-type="pmid">23341503</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Turian</surname><given-names>J</given-names></name><name><surname>Ratinov</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Word representations: a simple and general method for semi-supervised learning</article-title><conf-name>Proceedings of the 48th annual meeting of the association for computational linguistics</conf-name></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>UniProt</collab></person-group><year iso-8601-date="2021">2021</year><article-title>UniProt: the universal protein knowledgebase in 2021</article-title><source>Nucleic Acids Research</source><volume>49</volume><fpage>D480</fpage><lpage>D489</lpage><pub-id pub-id-type="doi">10.1093/nar/gkaa1100</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>UniProt Consortium</collab></person-group><year iso-8601-date="2019">2019</year><article-title>UniProt: a worldwide hub of protein knowledge</article-title><source>Nucleic Acids Research</source><volume>47</volume><fpage>D506</fpage><lpage>D515</lpage><pub-id pub-id-type="doi">10.1093/nar/gky1049</pub-id><pub-id pub-id-type="pmid">30395287</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vakser</surname><given-names>IA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Protein-Protein docking: from interaction to interactome</article-title><source>Biophysical Journal</source><volume>107</volume><fpage>1785</fpage><lpage>1793</lpage><pub-id pub-id-type="doi">10.1016/j.bpj.2014.08.033</pub-id><pub-id pub-id-type="pmid">25418159</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Maaten</surname><given-names>L</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visualizing data using t-SNE</article-title><source>Journal of Machine Learning Research</source><volume>9</volume><elocation-id>11</elocation-id></element-citation></ref><ref id="bib133"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varadi</surname><given-names>M</given-names></name><name><surname>Anyango</surname><given-names>S</given-names></name><name><surname>Deshpande</surname><given-names>M</given-names></name><name><surname>Nair</surname><given-names>S</given-names></name><name><surname>Natassia</surname><given-names>C</given-names></name><name><surname>Yordanova</surname><given-names>G</given-names></name><name><surname>Yuan</surname><given-names>D</given-names></name><name><surname>Stroe</surname><given-names>O</given-names></name><name><surname>Wood</surname><given-names>G</given-names></name><name><surname>Laydon</surname><given-names>A</given-names></name><name><surname>Žídek</surname><given-names>A</given-names></name><name><surname>Green</surname><given-names>T</given-names></name><name><surname>Tunyasuvunakool</surname><given-names>K</given-names></name><name><surname>Petersen</surname><given-names>S</given-names></name><name><surname>Jumper</surname><given-names>J</given-names></name><name><surname>Clancy</surname><given-names>E</given-names></name><name><surname>Green</surname><given-names>R</given-names></name><name><surname>Vora</surname><given-names>A</given-names></name><name><surname>Lutfi</surname><given-names>M</given-names></name><name><surname>Figurnov</surname><given-names>M</given-names></name><name><surname>Cowie</surname><given-names>A</given-names></name><name><surname>Hobbs</surname><given-names>N</given-names></name><name><surname>Kohli</surname><given-names>P</given-names></name><name><surname>Kleywegt</surname><given-names>G</given-names></name><name><surname>Birney</surname><given-names>E</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Velankar</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>AlphaFold protein structure database: massively expanding the structural coverage of protein-sequence space with high-accuracy models</article-title><source>Nucleic Acids Research</source><volume>50</volume><fpage>D439</fpage><lpage>D444</lpage><pub-id pub-id-type="doi">10.1093/nar/gkab1061</pub-id><pub-id pub-id-type="pmid">34791371</pub-id></element-citation></ref><ref id="bib134"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention is all you need</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib135"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Väth</surname><given-names>P</given-names></name><name><surname>Münch</surname><given-names>M</given-names></name><name><surname>Raab</surname><given-names>C</given-names></name><name><surname>Schleif</surname><given-names>FM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>PROVAL: A framework for comparison of protein sequence embeddings</article-title><source>Journal of Computational Mathematics and Data Science</source><volume>2022</volume><elocation-id>100044</elocation-id><pub-id pub-id-type="doi">10.1016/j.jcmds.2022.100044</pub-id></element-citation></ref><ref id="bib136"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vig</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>BertViz: A tool for visualizing multihead self-attention in the BERT model</article-title><conf-name>ICLR Workshop: Debugging Machine Learning Models</conf-name></element-citation></ref><ref id="bib137"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vig</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>A Multiscale Visualization of Attention in the Transformer Model</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1906.05714">https://arxiv.org/abs/1906.05714</ext-link></element-citation></ref><ref id="bib138"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vig</surname><given-names>J</given-names></name><name><surname>Madani</surname><given-names>A</given-names></name><name><surname>Varshney</surname><given-names>LR</given-names></name><name><surname>Xiong</surname><given-names>C</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Rajani</surname><given-names>NF</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Bertology Meets Biology: Interpreting Attention in Protein Language Models</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.15222">https://arxiv.org/abs/2006.15222</ext-link></element-citation></ref><ref id="bib139"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Vig</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Bertviz</data-title><version designator="04755ef">04755ef</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/jessevig/bertviz">https://github.com/jessevig/bertviz</ext-link></element-citation></ref><ref id="bib140"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Walls</surname><given-names>D</given-names></name><name><surname>Loughran</surname><given-names>ST</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Protein chromatography</chapter-title><person-group person-group-type="editor"><name><surname>Walls</surname><given-names>D</given-names></name></person-group><source>Protein Stability: Enhancement and Measurement</source><publisher-name>Springer</publisher-name><fpage>101</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1007/978-1-4939-6412-3</pub-id></element-citation></ref><ref id="bib141"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>ML</given-names></name><name><surname>Li</surname><given-names>BQ</given-names></name><name><surname>Zhai</surname><given-names>HL</given-names></name><name><surname>Liu</surname><given-names>JJ</given-names></name><name><surname>Li</surname><given-names>SY</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Prediction of phosphorylation sites based on krawtchouk image moments</article-title><source>Proteins</source><volume>85</volume><fpage>2231</fpage><lpage>2238</lpage><pub-id pub-id-type="doi">10.1002/prot.25388</pub-id><pub-id pub-id-type="pmid">28921635</pub-id></element-citation></ref><ref id="bib142"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Afzal</surname><given-names>N</given-names></name><name><surname>Rastegar-Mojarad</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Shen</surname><given-names>F</given-names></name><name><surname>Kingsbury</surname><given-names>P</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A comparison of word embeddings for the biomedical natural language processing</article-title><source>Journal of Biomedical Informatics</source><volume>87</volume><fpage>12</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1016/j.jbi.2018.09.008</pub-id><pub-id pub-id-type="pmid">30217670</pub-id></element-citation></ref><ref id="bib143"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Smola</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Language Models with Transformers</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1904.09408">https://arxiv.org/abs/1904.09408</ext-link></element-citation></ref><ref id="bib144"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>D</given-names></name><name><surname>Liu</surname><given-names>D</given-names></name><name><surname>Yuchi</surname><given-names>J</given-names></name><name><surname>He</surname><given-names>F</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Cai</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>MusiteDeep: a deep-learning based Webserver for protein post-translational modification site prediction and visualization</article-title><source>Nucleic Acids Research</source><volume>48</volume><fpage>W140</fpage><lpage>W146</lpage><pub-id pub-id-type="doi">10.1093/nar/gkaa275</pub-id><pub-id pub-id-type="pmid">32324217</pub-id></element-citation></ref><ref id="bib145"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>wang</surname><given-names>J</given-names></name><name><surname>Wen</surname><given-names>N</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Zhao</surname><given-names>L</given-names></name><name><surname>Cheng</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>ELECTRA-DTA: A new compound-protein binding affinity prediction model based on the contextualized sequence encoding</article-title><source>Journal of Cheminformatics</source><volume>14</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1186/s13321-022-00591-x</pub-id></element-citation></ref><ref id="bib146"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Suh</surname><given-names>C</given-names></name><name><surname>Rudin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>There once was a really bad poet, it was automated but you did ’'t know it</article-title><source>Transactions of the Association for Computational Linguistics</source><volume>9</volume><fpage>605</fpage><lpage>620</lpage><pub-id pub-id-type="doi">10.1162/tacl_a_00387</pub-id></element-citation></ref><ref id="bib147"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Wei</surname><given-names>J</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Lin</surname><given-names>M</given-names></name><name><surname>Ren</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Cui</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Cowen</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Prior knowledge facilitates low homologous protein secondary structure prediction with DSM distillation</article-title><source>Bioinformatics</source><volume>38</volume><fpage>3574</fpage><lpage>3581</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btac351</pub-id></element-citation></ref><ref id="bib148"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wettig</surname><given-names>A</given-names></name><name><surname>Gao</surname><given-names>T</given-names></name><name><surname>Zhong</surname><given-names>Z</given-names></name><name><surname>Mask</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Should You Mask 15% in Masked Language Modeling?</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2202.08005">https://arxiv.org/abs/2202.08005</ext-link></element-citation></ref><ref id="bib149"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilburn</surname><given-names>GW</given-names></name><name><surname>Eddy</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Remote homology search with hidden Potts models</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008085</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008085</pub-id><pub-id pub-id-type="pmid">33253143</pub-id></element-citation></ref><ref id="bib150"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wishart</surname><given-names>DS</given-names></name><name><surname>Knox</surname><given-names>C</given-names></name><name><surname>Guo</surname><given-names>AC</given-names></name><name><surname>Shrivastava</surname><given-names>S</given-names></name><name><surname>Hassanali</surname><given-names>M</given-names></name><name><surname>Stothard</surname><given-names>P</given-names></name><name><surname>Chang</surname><given-names>Z</given-names></name><name><surname>Woolsey</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>DrugBank: a comprehensive resource for in silico drug discovery and exploration</article-title><source>Nucleic Acids Research</source><volume>34</volume><fpage>D668</fpage><lpage>D672</lpage><pub-id pub-id-type="doi">10.1093/nar/gkj067</pub-id><pub-id pub-id-type="pmid">16381955</pub-id></element-citation></ref><ref id="bib151"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Schuster</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Le</surname><given-names>QV</given-names></name><name><surname>Norouzi</surname><given-names>M</given-names></name><name><surname>Macherey</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1609.08144">https://arxiv.org/abs/1609.08144</ext-link></element-citation></ref><ref id="bib152"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Song</surname><given-names>J</given-names></name><name><surname>Wilson</surname><given-names>C</given-names></name><name><surname>Whisstock</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>PhosContext2vec: a distributed representation of residue-level sequence contexts and its application to general and kinase-specific phosphorylation site prediction</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>8240</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-26392-7</pub-id><pub-id pub-id-type="pmid">29844483</pub-id></element-citation></ref><ref id="bib153"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xue</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Fang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Multimodal Pre-Training Model for Sequence-based Prediction of Protein-Protein Interaction</article-title><conf-name>Machine Learning in Computational Biology; 2022: PMLR</conf-name></element-citation></ref><ref id="bib154"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamaguchi</surname><given-names>H</given-names></name><name><surname>Saito</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Evotuning protocols for transformer-based variant effect prediction on multi-domain proteins</article-title><source>Briefings in Bioinformatics</source><volume>22</volume><elocation-id>bbab234</elocation-id><pub-id pub-id-type="doi">10.1093/bib/bbab234</pub-id></element-citation></ref><ref id="bib155"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Dai</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Carbonell</surname><given-names>J</given-names></name><name><surname>Salakhutdinov</surname><given-names>RR</given-names></name><name><surname>Le</surname><given-names>QV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Xlnet: generalized autoregressive pretraining for language understanding</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib156"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>KK</given-names></name><name><surname>Lu</surname><given-names>AX</given-names></name><name><surname>Fusi</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Convolutions Are Competitive with Transformers for Protein Sequence Pretraining</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.05.19.492714</pub-id></element-citation></ref><ref id="bib157"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>W</given-names></name><name><surname>Kann</surname><given-names>K</given-names></name><name><surname>Yu</surname><given-names>M</given-names></name><name><surname>Schütze</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Comparative Study of CNN and RNN for Natural Language Processing</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1702.01923">https://arxiv.org/abs/1702.01923</ext-link></element-citation></ref><ref id="bib158"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Young</surname><given-names>T</given-names></name><name><surname>Hazarika</surname><given-names>D</given-names></name><name><surname>Poria</surname><given-names>S</given-names></name><name><surname>Cambria</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Recent trends in deep learning based natural language processing [review article]</article-title><source>IEEE Computational Intelligence Magazine</source><volume>13</volume><fpage>55</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1109/MCI.2018.2840738</pub-id></element-citation></ref><ref id="bib159"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Gutman</surname><given-names>I</given-names></name><name><surname>Shi</surname><given-names>Y</given-names></name><name><surname>Dehmer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Protein sequence comparison based on physicochemical properties and the position-feature energy matrix</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>46237</elocation-id><pub-id pub-id-type="doi">10.1038/srep46237</pub-id><pub-id pub-id-type="pmid">28393857</pub-id></element-citation></ref><ref id="bib160"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zaheer</surname><given-names>M</given-names></name><name><surname>Guruganesh</surname><given-names>G</given-names></name><name><surname>Dubey</surname><given-names>KA</given-names></name><name><surname>Ainslie</surname><given-names>J</given-names></name><name><surname>Alberti</surname><given-names>C</given-names></name><name><surname>Ontanon</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Big bird: transformers for longer sequences</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>17283</fpage><lpage>17297</lpage></element-citation></ref><ref id="bib161"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zare-Mirakabad</surname><given-names>F</given-names></name><name><surname>Behjati</surname><given-names>A</given-names></name><name><surname>Arab</surname><given-names>SS</given-names></name><name><surname>Nowzari-Dalini</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Protein Sequence Profile Prediction Using Protalbert Transformer1</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.09.23.461475</pub-id></element-citation></ref><ref id="bib162"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Edwards</surname><given-names>MD</given-names></name><name><surname>Liu</surname><given-names>G</given-names></name><name><surname>Gifford</surname><given-names>DK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Convolutional neural network architectures for predicting DNA-protein binding</article-title><source>Bioinformatics</source><volume>32</volume><fpage>i121</fpage><lpage>i127</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btw255</pub-id><pub-id pub-id-type="pmid">27307608</pub-id></element-citation></ref><ref id="bib163"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Ju</surname><given-names>F</given-names></name><name><surname>Zhu</surname><given-names>J</given-names></name><name><surname>He</surname><given-names>L</given-names></name><name><surname>Shao</surname><given-names>B</given-names></name><name><surname>Zheng</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Co-evolution Transformer for Protein Contact Prediction</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib164"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Q</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Xie</surname><given-names>F</given-names></name><name><surname>Lv</surname><given-names>Z</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Mul-SNO: A novel prediction tool for S-nitrosylation sites based on deep learning methods</article-title><conf-name>IEEE Journal of Biomedical and Health Informatics</conf-name><pub-id pub-id-type="doi">10.1109/JBHI.2021.3123503</pub-id></element-citation></ref></ref-list></back></article>