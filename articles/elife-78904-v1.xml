<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">78904</article-id><article-id pub-id-type="doi">10.7554/eLife.78904</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Successor-like representation guides the prediction of future events in human visual cortex and hippocampus</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-275933"><name><surname>Ekman</surname><given-names>Matthias</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1254-1392</contrib-id><email>matthias.ekman@donders.ru.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-275934"><name><surname>Kusch</surname><given-names>Sarah</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-28130"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6730-1452</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University Nijmegen, Donders Institute for Brain, Cognition and Behaviour</institution></institution-wrap><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Barense</surname><given-names>Morgan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>University of Toronto</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>02</day><month>02</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e78904</elocation-id><history><date date-type="received" iso-8601-date="2022-03-23"><day>23</day><month>03</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-01-13"><day>13</day><month>01</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-03-26"><day>26</day><month>03</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.03.23.485480"/></event></pub-history><permissions><copyright-statement>Â© 2023, Ekman et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Ekman et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-78904-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-78904-figures-v1.pdf"/><abstract><p>Human agents build models of their environment, which enable them to anticipate and plan upcoming events. However, little is known about the properties of such predictive models. Recently, it has been proposed that hippocampal representations take the form of a predictive map-like structure, the so-called successor representation (SR). Here, we used human functional magnetic resonance imaging to probe whether activity in the early visual cortex (V1) and hippocampus adhere to the postulated properties of the SR after visual sequence learning. Participants were exposed to an arbitrary spatiotemporal sequence consisting of four items (A-B-C-D). We found that after repeated exposure to the sequence, merely presenting single sequence items (e.g., - B - -) resulted in V1 activation at the successor locations of the full sequence (e.g., C-D), but not at the predecessor locations (e.g., A). This highlights that visual representations are skewed toward future states, in line with the SR. Similar results were also found in the hippocampus. Moreover, the hippocampus developed a coactivation profile that showed sensitivity to the temporal distance in sequence space, with fading representations for sequence events in the more distant past and future. V1, in contrast, showed a coactivation profile that was only sensitive to spatial distance in stimulus space. Taken together, these results provide empirical evidence for the proposition that both visual and hippocampal cortex represent a predictive map of the visual world akin to the SR.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>hippocampus</kwd><kwd>visual cortex</kwd><kwd>prediction</kwd><kwd>predictive map</kwd><kwd>successor representation</kwd><kwd>replay</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>Veni Grant No. 016.Veni.195.435</award-id><principal-award-recipient><name><surname>Ekman</surname><given-names>Matthias</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100019180</institution-id><institution>HORIZON EUROPE European Research Council</institution></institution-wrap></funding-source><award-id>ERC Consolidator Grant 101000942 &quot;Surprise&quot;</award-id><principal-award-recipient><name><surname>de Lange</surname><given-names>Floris P</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Early visual cortex (V1) and hippocampal cortex represent a predictive map of the visual world akin to the successor representation.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Anticipation and planning of future visual input require knowledge of the relational structure between events. The relational structure, for instance that stimulus B usually follows stimulus A, is learned through exposure during past experiences (<xref ref-type="bibr" rid="bib5">Behrens et al., 2018</xref>; <xref ref-type="bibr" rid="bib17">Finnie et al., 2021</xref>; <xref ref-type="bibr" rid="bib22">Gavornik and Bear, 2014</xref>) and can be used to build a model or cognitive map (<xref ref-type="bibr" rid="bib61">Tolman, 1948</xref>) that enables us to generate inferences in situation with noisy or partial input (<xref ref-type="bibr" rid="bib14">Ekman et al., 2017</xref>; <xref ref-type="bibr" rid="bib43">Momennejad, 2020</xref>; <xref ref-type="bibr" rid="bib54">Schwartenbeck et al., 2021</xref>).</p><p>In the visual domain, with rapidly changing input, it remains unknown what the inherent properties of the model underlying our predictions are. On the one hand, such a model needs to be efficient enough to generate predictions from a constant stream of visual input, while on the other hand, it also allows for flexible updating in an ever-changing environment. In the context of hippocampal representations, the successor representation (SR) has been recently proposed (<xref ref-type="bibr" rid="bib9">Dayan, 1993</xref>; <xref ref-type="bibr" rid="bib58">Stachenfeld et al., 2017</xref>) to combine the trade-off between both flexible and efficient model properties. The SR postulates a predictive representation in which the current state is represented in terms of its future (successor) states, in a temporally discounted fashion. The SR is dependent on the actual experience, with states experienced more frequently being represented more strongly. This enables learning in an environment without explicit reward (<xref ref-type="bibr" rid="bib24">GlÃ¤scher et al., 2010</xref>). This hypothesis captures many aspects of empirical hippocampal place cell firing pattern, like the exponential decay toward distant future locations (<xref ref-type="bibr" rid="bib1">Alvernhe et al., 2011</xref>; <xref ref-type="bibr" rid="bib39">Mehta et al., 2000</xref>).</p><p>Previous research has repeatedly shown that prior expectations influence neural activity in the visual cortex (<xref ref-type="bibr" rid="bib14">Ekman et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Gavornik and Bear, 2014</xref>; <xref ref-type="bibr" rid="bib26">Hindy et al., 2016</xref>; <xref ref-type="bibr" rid="bib34">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib64">Xu et al., 2012</xref>). It remains, however, unknown if SR-like representations are present outside the hippocampus in areas like the early visual cortex (V1) that have a strong retinotopic organization. Theoretically, it is possible that V1 receptive fields, analogous to hippocampal place fields, become tuned to respond not only to the current input, but also to expected future inputs. Here, we propose that the computationally efficient and flexible properties of the SR could in theory also underlie the anticipation of future events in V1.</p><p>To directly test this hypothesis, we conducted a functional magnetic resonance imaging (fMRI) study in which participants were presented with an arbitrary visual dot sequence (A-B-C-D). After initial sequence exposure, we introduced occasional omission trials, where only one element of the sequence was presented (e.g., B), while the rest of the sequence (e.g., A, C, and D) was omitted. These partial sequence trials allowed us to study expectations of future stimulus sequences in the absence of physical stimulation. This design allowed us to test the specific assumptions of the SR and also assess whether V1 predictions were better described by an alternative mechanism called pattern completion. Pattern completion describes a framework in which autoassociative connections within the hippocampal CA3 regions reactivate related sequence items from partial input (<xref ref-type="bibr" rid="bib11">Deuker et al., 2014</xref>; <xref ref-type="bibr" rid="bib38">Leutgeb and Leutgeb, 2007</xref>; <xref ref-type="bibr" rid="bib48">Rolls, 2013</xref>) that is then propagated to sensory regions such as V1 (<xref ref-type="bibr" rid="bib26">Hindy et al., 2016</xref>). In contrast to the SR, pattern completion predicts reactivations of all associated items, without any skewing toward future locations or temporal discounting of events that are farther in the future.</p><p>Using fMRI, we found reactivations of future sequence locations (e.g., C-D), but not of past locations (e.g., A) in both V1 and hippocampus. In line with the SR, a model comparison confirmed that predictive representations constitute a map-like structure, with exponential decay toward distant future states. Further, more detailed analysis of predictive codes revealed that hippocampus represented visual locations based on their temporal proximity within the sequence, rather than spatial distance.</p><p>Taken together, these data suggest that humans predict upcoming visual input by using a generative model whose properties resemble the SR. Importantly, the presence of SR-like representations in V1 indicates that SR might be a more ubiquitous coding schema that is present beyond hippocampal place cells. Finally, while SR-like representations were found to be present in both V1 and hippocampus, the predictive codes between these areas revealed complementary tuning properties, with hippocampus being sensitive to temporal distance and V1 being more sensitive to retinotopic spatial distance.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Human observers (N = 35) were exposed to four dots presented in rapid succession that formed an arbitrary visual sequence A-B-C-D (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Dot locations were sampled from eight locations (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) and the resulting possible sequences were randomly assigned across subjects. After an initial exposure period with the full sequence (352 trials outside the scanner, 160 trials inside the scanner), occasionally only one item of the sequence was presented, omitting the remaining sequence items (e.g., partial sequence trial â- B - -â where B is shown and A, C, and D are omitted; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Participants were instructed to maintain fixation throughout the experiment, and tasked to detect a slight temporal onset delay (170 ms vs. 17 ms) of the last sequence dot that occurred in ~40% of the full sequence trials. The task was designed to be demanding (group averaged hit rate = 78%, SD = 8%) and to keep participantsâ attention on the sequence.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Sequence paradigm to probe successor-like representations.</title><p>(<bold>a</bold>) Stimulus timing for full sequence trials (<italic>top</italic>) and partial sequence trials (<italic>bottom</italic>). During full sequence trials, four dots were presented in rapid succession in a fixed sequence order (A-B-C-D). During partial sequence trials, only one of the four dots was presented, omitting the remaining sequence dots. Here shown for -B - -, while A- - -, - -C-, and - - -D partial trials were also presented. (<bold>b</bold>) Sequences were randomized across subjects such that sequence locations were sampled from a total of eight possible locations with the constraint that every quadrant was stimulated once. Dot locations were evenly spaced around central fixation at a radius of 7 degrees visual angle (dva). (<bold>c</bold>) Independent stimulus localizer trials to map out stimulus representations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78904-fig1-v1.tif"/></fig><p>We hypothesized that presenting only one item of the sequence would elicit anticipatory activity at the omitted sequence locations that followed the presented stimulus (i.e., successor states), but not at the sequence location that preceded the sequence item (i.e., predecessor states). For instance, during partial â- B - -â trials, we expected activity at omitted sequence locations C (+1) and D (+2), but not at omitted location A (â1).</p><sec id="s2-1"><title>Stimulus sequences elicit spatially specific responses in V1</title><p>To test our prediction, we first selected V1 sub regions of interest (ROIs) that responded selectively to the eight stimulus locations based on an independent localizer session (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Stimulus-response profiles of these eight (retinotopic) ROIs show little coactivation of neighboring locations in the visual field which allows for a precise investigation of location-specific activity (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Unsurprisingly, during full sequence trials, BOLD activity at the sequence locations receiving bottom-up visual input was markedly enhanced compared to non-stimulated control locations (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Population-based receptive field (pRF) data that was acquired for a subset of participants confirmed that the selected voxels correspond to the retinotopic stimulus locations as expected. For all analyses, we subtracted the average BOLD activity of all control locations from the sequence location activity (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), which provides an accurate measure of stimulus-specific responses independent of global signal fluctuations for instance due to attention.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>V1 stimulus mapping.</title><p>(<bold>a</bold>) An independent stimulus localizer was used to identify V1 subpopulations that respond to individual dot locations (<italic>left</italic>). Stimulus-response profiles show tuning properties for selected V1 populations (<italic>middle</italic>). Visualizing stimulus activity by projecting group averaged BOLD activity (n=35) into stimulus space (<italic>right</italic>) shows focal activity at the stimulated location with minimal spreading to neighboring locations. (<bold>b</bold>) Identified V1 subpopulations during full sequence trials (left) show heightened BOLD activity compared to non-stimulated control locations (<italic>middle</italic>). Group averaged (n=7) sequence activity projected into stimulus space shows spatially specific activity at the stimulated locations (<italic>right</italic>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78904-fig2-v1.tif"/></fig><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Successor-like representation of future sequence events in V1.</title><p>(<bold>a</bold>) BOLD activity during full sequence trials. (<bold>b</bold>) Schematic of all partial sequence trials (<italic>left</italic>) illustrating the omission of different predecessor (purple), or successor (orange) sequence locations. Group averaged (n=35) V1 activity during partial sequence trials (<italic>right</italic>) shows enhanced activation of successor locations compared to predecessor locations. (<bold>c</bold>) Group averaged V1 activity for individual partial sequence trials. Error bars denote Â± s.e.m.; two-tailed t test, ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05, uncorrected for multiple comparisons.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78904-fig3-v1.tif"/></fig></sec><sec id="s2-2"><title>Anticipated stimulus sequences in V1</title><p>Briefly flashing individual dots during partial sequence trials, while omitting the other dots of the sequence, allowed us to probe anticipatory activity at the successor and predecessor locations (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). In line with our predictions, V1 BOLD activity was indeed enhanced at the non-stimulated successor locations compared to the non-stimulated predecessor locations (averaged across all partial trials and sequence locations; t(34) = 6.45, p = 2.23 Ã 10<sup>â7</sup>). The same pattern of future-directed prediction was also evident from the visual inspection of BOLD activity for all partial sequence trials separately (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Further, these results of greater activity for successor compared to predecessor activity also holds when comparing individual sequence locations without averaging (i.e., comparing non-stimulated location B when successor vs. predecessor, t(34) = 5.72, p = 2.02 Ã 10<sup>â6</sup>; and location C when successor vs. predecessor, t(34) = 3.13, p = 0.0035).</p><p>The activity decay toward distant future locations was formally tested by fitting an exponentially decaying factor gamma Î³ â [0,1] to each participantâs data. Here, values closer to 0 indicate a steeper decay and values closer to 1 indicate no decay. In line with our predictions, we found a group averaged decaying factor of <italic>Î³</italic>=0.14 (Â±0.03 s.e.m.) that was statistically significantly different from 1 (non-parametric t test t(34) = â17.17, p = 2.54 Ã 10<sup>â18</sup>).</p><p>One might argue that participants with stronger predictions toward future locations would perform better at the behavioral detection task. However, no such correlation between individual V1 BOLD activity and task accuracy was found in an across-subject correlation analysis (see Materials and methods, spearman r = 0.05; p = 0.769).</p></sec><sec id="s2-3"><title>Successor-like representation in V1</title><p>Next, we sought to formally test how well the observed data fits the prediction of the SR, namely an exponential decay of states farther into the future.</p><p>For each subject, we fitted partial sequence trials with an SR model (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), keeping the exponential decay parameter Î³ as a free parameter (see Materials and methods). In order to evaluate how well the SR model resembled the data, we then computed the error between the SR prediction and the actual data (lower values indicate a better model fit). For comparison, we additionally fitted a traditional pattern-completion co-occurrence (CO) model that predicts that events that occur together, will be reactivated together (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). In contrast to the SR model, predictions of the CO model are non-directional, meaning that it predicts equal reactivation of both successor and predecessor locations. Furthermore, while the SR model predicts a temporal discounting toward future states, the CO model assumes no differential activity of reactivated states. In our implementation of the CO model, anticipatory activity was modulated by one multiplicative parameter Ï. Additionally, as a baseline model, we also evaluated a null model (H0) that assumes no predictive activity (i.e., no difference between successor and predecessor locations). Note that in order to be interpretable as a predictive representation, the best-fitting model should not only have the smallest error, but also differ significantly from the H0 model.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Model comparison favors successor-like representation in V1.</title><p>(<bold>a</bold>) Probing predictions of the successor representation (SR) against the competing co-occurrence (CO) model. The relational structure of the full sequence A-B-C-D is translated into a transition matrix (<italic>top</italic>), where a non-zero value indicates a transition between two states in the sequence. The SR matrix (<italic>bottom</italic>) is computed from the transition matrix, here shown with a temporal discount factor of Î³ = 0.3 (see Materials and methods). (<bold>b</bold>) The relational structure in the CO model is non-directional, resulting in a constant prediction of past and future states weighted by a factor Ï. (<bold>c</bold>) Competing model predictions were fitted to partial sequence trial V1 data of each individual participant with Î³ and Ï as free parameters. Comparison of model errors showed that the data is most in line with the SR. A null model (<italic>bottom</italic>), resembling no prediction of past and future locations, was included in the model comparison as baseline. Error bars denote Â± s.e.m.; <italic>BIC</italic>, Bayesian Information Criterion (taking into account that the H0 model has fewer parameters). Two-tailed t test, ***p &lt; 0.001, uncorrected for multiple comparisons.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78904-fig4-v1.tif"/></fig><p>Our results show that anticipatory activity in V1 is best described by the predictions of the SR (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; SR vs. CO t(34) = â2.29, p = 0.028). Additionally, both SR and CO describe the data better than the null model (SR vs. H0 t(34) = â8.25, p = 1.24 Ã 10<sup>â9</sup>; CO vs. H0: t test t(34) = â7.59, p = 8.22 Ã 10<sup>â9</sup>).</p></sec><sec id="s2-4"><title>SR in hippocampus</title><p>The predictive neural representation in the form of a SR was originally postulated for the hippocampus (<xref ref-type="bibr" rid="bib58">Stachenfeld et al., 2017</xref>). We therefore wanted to investigate whether the predictive representations that we observed in V1 were also present in the hippocampus. Note that while the hippocampal formation and nearby entorhinal cortex might feature a coarse representation of visual space (<xref ref-type="bibr" rid="bib31">Killian et al., 2012</xref>; <xref ref-type="bibr" rid="bib33">Knapen, 2021</xref>; <xref ref-type="bibr" rid="bib45">Nau et al., 2018b</xref>; <xref ref-type="bibr" rid="bib55">Silson et al., 2020</xref>), it does not feature the same fine-scale retinotopic organization present in V1 (<xref ref-type="bibr" rid="bib13">Dumoulin and Wandell, 2008</xref>). Therefore, instead of focusing on univariate BOLD activity within certain hippocampal subregions, we focused on population activity patterns across the entire hippocampus using a decoding approach similar to previous studies (<xref ref-type="bibr" rid="bib15">Ekman et al., 2022</xref>; <xref ref-type="bibr" rid="bib35">Kok and Turk-Browne, 2018</xref>; <xref ref-type="bibr" rid="bib36">Kurth-Nelson et al., 2016</xref>; <xref ref-type="bibr" rid="bib49">Russek et al., 2021</xref>; <xref ref-type="bibr" rid="bib51">Schapiro et al., 2012</xref>).</p><p>In keeping with the V1 analysis, we used the independent stimulus localizer to extract location-specific activity patterns in the hippocampus and then tested during partial sequence trials to what extent location-specific representations were reactivated. Specifically, we trained a pattern classifier to distinguish between the eight dot locations within the localizer. Before applying the trained classifier to omission trials of the main task (see Materials and methods), we confirmed that cross-validated decoding accuracies within the localizer were above chance-level to ensure that the hippocampal pattern shows a reliable representation of space.</p><p>Within localizer decoding accuracy results confirmed that hippocampus has a coarse representation of the eight stimulus locations (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) within the localizer (two-sided one-sample t test; t(34) = 3.28, p = 0.002; cross-validated accuracy = 15 Â± 3.6%, mean Â± s.d.; see Materials and methods). Notably, compared to V1 (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), within localizer accuracy was relatively low and as a consequence tuning curves in hippocampus appeared less sharp (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). In order to maximize sensitivity for the hippocampus, we averaged classification evidence across successor and predecessor locations. Non-averaged results can be found in <xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Hippocampus represents spatial locations and engages in future-directed predictions.</title><p>(<bold>a</bold>) Hippocampus region of interest (green). (<bold>b</bold>) A pattern classifier was trained to distinguish between the eight stimulus locations during a perceptual localizer. Resulting stimulus-response profiles reveal that hippocampus distinguishes between individual stimulus locations. (<bold>c</bold>) Averaged (n=35) tuning profiles shifted to one location. (<bold>d</bold>) A classifier that was trained on the perceptual localizer was applied to partial sequence trials during the main task to probe whether hippocampal representations skew toward predecessor locations (purple), or successor locations (orange). (<bold>e</bold>) Classifier evidence, averaged across possible successor and predecessor locations, shows that hippocampus predominantly represents future (successor) stimulus locations over predecessor locations. (<bold>f</bold>) Since the hemodynamic properties of hippocampal functions are not well understood, the decoding analysis was additionally performed in a time-resolved manner and fitted with a canonical hemodynamic function to estimate the time to peak. The difference time-course (successor minus predecessor) showed a temporally distinct peak around 4.7 s indicating that the future-directed prediction occurs as transient response to the partial stimulus input and not as a sustained signal throughout the trial. Error bars denote Â± s.e.m.; **p &lt; 0.01.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78904-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 1.</label><caption><title>Successor-like representation of future sequence events in hippocampus.</title><p>(<bold>a</bold>) Group averaged (n=35) hippocampus classification evidence during partial sequence trials shows enhanced activation of successor locations compared to predecessor locations. (<bold>b</bold>) Model comparison favors successor-like representation in hippocampus. Competing model predictions were fitted to partial sequence trial data of each individual participant. Comparison of model fits showed that the data is most in line with the successor representation (lower BIC values correspond to a better fit). A null model (H0), resembling no prediction of past and future locations was included in the model comparison as baseline. Error bars denote Â± s.e.m.; <italic>BIC</italic>, Bayesian Information Criterion (taking into account that the H0 model has fewer parameters); two-tailed t test, ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05, uncorrected for multiple comparisons.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78904-fig5-figsupp1-v1.tif"/></fig></fig-group><p>Applying the trained classifier to partial sequence trials of the main task, we asked whether hippocampus would preferentially reactivate successor or predecessor locations (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). To answer this question, we first subtracted the probabilistic classifier evidence for the control locations from the classifier evidence of the sequence locations. Consequently, values greater than 0 reflect evidence for the reactivation of sequence representations, while values smaller than 0 reflect a relative suppression of sequence locations. After that we averaged the evidence across all successor and predecessor locations, respectively, and tested for differences across participants. Our results reveal that hippocampus representations were preferentially biased toward successor locations (<xref ref-type="fig" rid="fig5">Figure 5E</xref>; paired-sample t test, t(34) = 2.74, p = 0.009), mirroring the results found in V1.</p><p>Finally, in order to better understand the temporal dynamics of the anticipatory representations in hippocampus, we repeated the decoding analysis in a time-resolved manner. We reasoned that if reactivations of future sequence locations were triggered by the brief presentations of partial sequence dots, the evidence time-course should follow a transient response profile. Alternatively, if hippocampus were to signal a constant bias toward future sequence locations, the evidence time-course should be unrelated to the stimulus onset and show a sustained temporal profile.</p><p>Results of the evidence difference time-course clearly show a transient response peaking approximately 4.7 s post-stimulus onset (<xref ref-type="fig" rid="fig5">Figure 5F</xref>) indicating that hippocampal predictions were triggered by the partial sequence dot. Note that the decoding time-course reflects the evidence for successor locations versus predecessor locations independent of the bottom-up stimulus. The transient decoding profile can therefore not simply reflect the onset of a given trial.</p><p>In order to probe the relationship between hippocampus and V1 successor reactivations, we performed an across-subject analysis, correlating V1 BOLD activity, averaged across all successor locations, with hippocampus classifier evidence, averaged across all successor locations. No significant relationship was observed (spearman correlation, r = â0.08, p = 0.668).</p><p>One could ask whether our findings are specific to V1 and hippocampus, or widespread throughout the brain. In order to answer this question, we repeated the analysis for low-level visual area V2. In contrast to V1, no predictive effects were found in area V2. V2 BOLD activity was not enhanced at the non-stimulated successor locations compared to the non-stimulated predecessor locations (averaged across all partial trials and sequence locations; t(34) = 1.41, p = 0.168).</p></sec><sec id="s2-5"><title>Hippocampal codes preserve spatiotemporal tuning</title><p>In contrast to V1, hippocampal representations are not inherently retinotopic and feature only a coarse representation of visual space (<xref ref-type="bibr" rid="bib33">Knapen, 2021</xref>; <xref ref-type="bibr" rid="bib45">Nau et al., 2018b</xref>; <xref ref-type="bibr" rid="bib56">Silson et al., 2021</xref>). Instead, hippocampal place cells provide a detailed representation of the allocentric position in an environment. However, more recently, the intriguing picture emerged that hippocampus also contributes to a more general organization of information by representing non-spatial aspects of experience in a map-like way (<xref ref-type="bibr" rid="bib8">Constantinescu et al., 2016</xref>; <xref ref-type="bibr" rid="bib21">Garvert et al., 2017</xref>; <xref ref-type="bibr" rid="bib58">Stachenfeld et al., 2017</xref>), similar to the representation of space (<xref ref-type="bibr" rid="bib2">Aronov et al., 2017</xref>).</p><p>Inspired by these recent observations, we asked what the underlying properties of the reported hippocampus representations were. Given that we successfully trained a classifier based on eight spatial locations, it might seem obvious to conclude that the underlying code for these representations is purely spatial (retinotopic) as well. This is however not necessarily the case, given that the localizer was shown after the main task and might therefore reflect persistent predictive representations. Instead, robust discrimination of sequence locations could theoretically also be based on coding of temporal properties of the sequence. Indeed, <xref ref-type="bibr" rid="bib12">Deuker et al., 2016</xref> have recently shown that hippocampus representations can reflect in principle both spatial and temporal aspects. In our case, a temporal coding mechanism could represent stimulus locations not based on proximity in space, but rather by proximity in time.</p><p>In order to address this question, we conducted a detailed analysis of the coactivation pattern in the stimulus localizer (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Note that the localizer was shown at the end of the study, allowing us to test whether learned associations persisted even after the full sequence was not relevant anymore. Here, coactivations were defined as activation of non-stimulated locations. For instance, when presenting stimulus A, locations B-C-D might become activated as well. In general, such coactivations are often attributed to noise or ambivalent responses driven by overlapping receptive fields. However, in this case, we made use of the coactivation pattern to draw inferences about the learned persistent representations.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Stimulus localizer reveals complementary coactivation (tuning) properties in hippocampus and V1.</title><p>(<bold>a</bold>) Schematic of the localizer trial with the stimulated location âAâ and the non-stimulated locations (B, C, D, dashed circle) that were part of the sequence in the main task preceding the localizer. (<bold>b</bold>) Illustration of coactivation (tuning) of sequence locations based on spatial (Euclidean) distance from the stimulated location (<italic>left</italic>) and temporal distance in sequence space (<italic>right</italic>). Note how sequence locations A and B are far apart in the spatial (Euclidean) domain, but close in terms of temporal distance in sequence space. (<bold>c</bold>) Hypothetical activation pattern for representational tuning of spatial distance and temporal distance for illustration shown in (<bold>b</bold>). (<bold>d</bold>) Illustration of tuning pattern averaged across all localizer conditions for temporal tuning (<italic>top</italic>), spatial tuning (<italic>middle</italic>), Successor Representation (SR, middle), and no coactivation (H0, <italic>bottom</italic>). For visualization purposes the x-axis is sorted by time for all three tuning patterns. (<bold>e</bold>) Classifier evidence for current, future, and past locations for hippocampus (<italic>left</italic>) and V1 (<italic>right</italic>). (<bold>f</bold>) Comparing model errors (i.e., lower is better) show that hippocampal representations were best described by temporal coactivation (<italic>left</italic>), while V1 (<italic>right</italic>) was best described by spatial coactivation and the absence of coactivation (H0) of sequence locations. Error bars denote Â± s.e.m.; two-tailed t test, ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05, uncorrected for multiple comparisons; <italic>BIC</italic>, Bayesian Information Criterion (taking into account that the H0 model has fewer parameters).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78904-fig6-v1.tif"/></fig><p>Specifically, for a representation based on spatial coactivation (tuning) one would expect a coactivation of nearby spatial locations. In other words, the coactivation of non-stimulated locations should be modulated by the spatial (Euclidean) distance to the stimulated location (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). This spatial tuning pattern is typically seen in early visual areas with overlapping receptive fields and is also visually present in our V1 results (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Alternatively, for a representation based on temporal coactivation (tuning) one would expect a coactivation of nearby locations in sequence space (<xref ref-type="fig" rid="fig6">Figure 6C</xref>).</p><p>Thus, spatial and temporal tuning codes lead to different coactivation patterns (<xref ref-type="fig" rid="fig6">Figure 6D</xref>) that can be disentangled with our stimulus paradigm. For instance, sequence locations A and B were far apart in the spatial (Euclidean) domain, but close in the temporal domain (distance in sequence space). Conversely, locations A and D are close in terms of spatial distance, but far apart in terms of temporal distance (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Note, while spatial tuning is in principle independent of any task-specific experience, temporal tuning on the other hand requires exposure to a sequential structure and can therefore only occur for the four dots that were part of the sequence. For this reason, we restricted the coactivation (tuning) analysis to the four dot locations that were part of the sequence.</p><p>For each participant, individual localizer data were fitted by a spatial coactivation model, a temporal coactivation model, an SR model, and a no-coactivation (H0) control model (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). The latter was included as a low-level baseline control. Visual inspection of the group averaged localizer coactivation pattern revealed a clear temporal tuning pattern in hippocampus but not in V1 (<xref ref-type="fig" rid="fig6">Figure 6E</xref>). These results were confirmed by a formal model comparison (<xref ref-type="fig" rid="fig6">Figure 6F</xref>, <italic>Hippocampus</italic>: two-sided t test, Temporal vs. Spatial t(34) = â2.36, p = 0.024; Temporal vs. SR t(34) = â3.24, p = 0.003; Temporal vs. H0 t(34) = â19.27, p = 7.12 Ã 10<sup>â20</sup>; V1: H0 vs. Temporal t(34) = â22.09, p = 9.49 Ã 10<sup>â22</sup>; H0 vs. Spatial t(34) = â14.26, p = 6.52 Ã 10<sup>â16</sup>; H0 vs. SR t(34) = â17.15, p = 2.61 Ã 10<sup>â18</sup>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Uncovering the computations that drive human prediction and planning is a central aspect when it comes to understanding human cognition. What are the general coding mechanisms that allow to utilize knowledge of the environment to make inferences and generalizations about future events? In this study, we sought to answer the question whether the map-like SR that has been posited for the hippocampus (<xref ref-type="bibr" rid="bib39">Mehta et al., 2000</xref>; <xref ref-type="bibr" rid="bib58">Stachenfeld et al., 2017</xref>) may also explain the shape of anticipatory activity in visual cortex (V1).</p><p>There is an extensive body of literature that shows how expectations elicit anticipatory activity in early visual cortices (<xref ref-type="bibr" rid="bib10">de Lange et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Hindy et al., 2016</xref>; <xref ref-type="bibr" rid="bib34">Kok et al., 2012</xref>). For instance, we have previously shown that flashing an individual dot of a simple, linear sequence triggers an activity wave in V1 that resembles the full stimulus sequence (<xref ref-type="bibr" rid="bib14">Ekman et al., 2017</xref>; <xref ref-type="bibr" rid="bib15">Ekman et al., 2022</xref>), akin to replay of place field activity during spatial navigation (<xref ref-type="bibr" rid="bib20">Foster and Wilson, 2006</xref>; <xref ref-type="bibr" rid="bib25">Gupta et al., 2010</xref>). However, what remains unknown is whether these sensory replay traces are guided by a generative model that represents the relational structure of the stimulus sequence, akin to a predictive map. Alternatively, anticipatory activity traces could simply reflect the association between different stimuli, based on their CO, without the added complexity of any temporal relational structure. The latter explanation appears plausible, given that predictive representations in early visual cortex are generally time critical and operate in parallel to a constant stream of new sensory input, which arguably requires efficient processing and in turn limits the complexity of such representations.</p><p>In fact, we previously speculated that cue-triggered reactivation of simple sequences might be driven by an automatic pattern completion-like mechanism that reactivates all associated items based on partial input (<xref ref-type="bibr" rid="bib14">Ekman et al., 2017</xref>). This idea is in line with the finding that predictive representations in V1 correlated with pattern completion-like activity in the hippocampus (<xref ref-type="bibr" rid="bib26">Hindy et al., 2016</xref>; <xref ref-type="bibr" rid="bib35">Kok and Turk-Browne, 2018</xref>) that might be driving V1 activity (<xref ref-type="bibr" rid="bib17">Finnie et al., 2021</xref>; <xref ref-type="bibr" rid="bib29">Ji and Wilson, 2007</xref>).</p><p>Our current findings directly challenge this interpretation and instead point to a predictive representation of expected, temporally discounted, future states. We accomplished this by using a paradigm in which one visual event (e.g., the presentation of one dot) was framed as one state in a directed transition matrix with a fixed relational structure. The SR hypothesis makes two testable predictions, namely that population activity represents future states over predecessor states, and that future state representations are temporally discounted, such that events in the close future are more prominently represented compared to events in the distant future. Using a paradigm in which we occasionally presented only single items of the full sequence, allowed us to investigate V1 activity at omitted sequence locations.</p><p>Confirming the SR predictions, V1 activity at the successor locations was enhanced compared to activity at the predecessor locations, indicating a representation skewed toward future locations and away from the past. Notably, this relative difference was not only due to an enhancement of successor states, but our results also showed a decrease of activity at the predecessor states (compared to baseline). This suppression of predecessor states might seem surprising at first given that SR postulates the mere absence of predecessor activity (<xref ref-type="bibr" rid="bib43">Momennejad, 2020</xref>; <xref ref-type="bibr" rid="bib58">Stachenfeld et al., 2017</xref>). We speculate that the observed decrease at the predecessor states might constitute a functional separation mechanism between predecessor and successor states, strengthening the future-directed representation of the sequence by selectively decreasing representations of the unexpected predecessor states.</p><p>One aspect that sets our study apart is that the viewing of the visual sequence does not require any predictive planning of the participant to evaluate different future outcomes. In contrast, related studies reporting neuronal evidence for SR-like representations in hippocampus and PFC (<xref ref-type="bibr" rid="bib4">Barron et al., 2020</xref>; <xref ref-type="bibr" rid="bib7">Brunec and Momennejad, 2022</xref>) and occipital cortex (<xref ref-type="bibr" rid="bib54">Schwartenbeck et al., 2021</xref>) have used paradigms in which participants were actively engaged in prospective planning and choice evaluation. Given the relatively passive nature of our task, one might therefore wonder whether it is expected to find any map-like activity at all. However, in this context, it is important to stress that the SR, unlike other model-based algorithms, does not depend on choice-dependent reward to build its transitional task structure (<xref ref-type="bibr" rid="bib41">Momennejad et al., 2017</xref>; <xref ref-type="bibr" rid="bib58">Stachenfeld et al., 2017</xref>) and therefore might not depend on participantsâ active engagement. Furthermore, <xref ref-type="bibr" rid="bib49">Russek et al., 2021</xref> have recently used a paradigm in which subjects were passively exposed to transitions between visual states and reported evidence for SR-like representations in the absence of active choices in line with the results of the present study. Further supporting this notion, we have previously shown that anticipatory sequence activity occurred even after subjectsâ attention was diverted from the sequence to a demanding task at fixation (<xref ref-type="bibr" rid="bib14">Ekman et al., 2017</xref>), rendering the sequence task irrelevant. Taken together, these observations indicate that SR-like representations are not limited to situations that require active planning, or multiple-choice evaluations but may rather be formed automatically and incidentally, as has been shown repeatedly in the domain of statistical learning (<xref ref-type="bibr" rid="bib19">Fiser and Aslin, 2002</xref>; <xref ref-type="bibr" rid="bib62">Turk-Browne et al., 2005</xref>).</p><p>While we have interpreted the neural activity patterns in the light of the SR, it is strictly speaking not possible to distinguish between model-based (MB) and SR algorithms within the context of our design. The key distinction between them is that SR caches a predictive map of states that the agent expects to visit in the future, whereas MB algorithms store a full model of the world and compute trajectories at the decision time (<xref ref-type="bibr" rid="bib23">Gershman, 2018</xref>; <xref ref-type="bibr" rid="bib41">Momennejad et al., 2017</xref>). Therefore, both predict a temporally discounted activation of successor states. It should be noted however that MB comes at a higher computational cost, and is more intensive both in terms of time and working memory resources. The activation of successor states that we observed, on the other hand, occurred in the absence of a decision-making process (i.e., participants did not perform any task on the trials where a single dot was presented). Also, importantly, we previously observed that this activation pattern was not dependent on the task, and was equally present when attentional resources were strongly drawn away from the stimuli (<xref ref-type="bibr" rid="bib14">Ekman et al., 2017</xref>). These observations may be more readily in line with the automatic (cached) activation of successor states that is embodied by SR, rather than the effortful iterative calculation of successor states that is the hallmark of MB. One future possibility to disentangle SR and MB algorithms could be to probe how well each model adapts to changes in the dot sequence structure. It has previously been shown, that compared to MB, the flexibility of the SR is somewhat limited to reflect changes in the transitional structure, because it requires the entire SR to be relearned (<xref ref-type="bibr" rid="bib42">Momennejad et al., 2018</xref>).</p><p>The hippocampal formation can acquire arbitrary relationships between objects (<xref ref-type="bibr" rid="bib2">Aronov et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Backus et al., 2016</xref>; <xref ref-type="bibr" rid="bib5">Behrens et al., 2018</xref>; <xref ref-type="bibr" rid="bib8">Constantinescu et al., 2016</xref>; <xref ref-type="bibr" rid="bib21">Garvert et al., 2017</xref>) beyond geometric location in space (<xref ref-type="bibr" rid="bib46">OâKeefe and Nadel, 1978</xref>). While our main focus in the current study was on V1 representations, we also wanted to test to what extent hippocampus showed a similar SR-like representation of visual sequences. Previous fMRI studies investigating hippocampal representations have mainly focused on either navigation in a spatial (<xref ref-type="bibr" rid="bib7">Brunec and Momennejad, 2022</xref>; <xref ref-type="bibr" rid="bib12">Deuker et al., 2016</xref>) or non-spatial task (<xref ref-type="bibr" rid="bib21">Garvert et al., 2017</xref>; <xref ref-type="bibr" rid="bib52">Schapiro et al., 2013</xref>; <xref ref-type="bibr" rid="bib53">Schuck and Niv, 2019</xref>) in which participants explore a relatively complex task space. It was recently shown that hippocampus has a rudimentary representation of visual space (<xref ref-type="bibr" rid="bib33">Knapen, 2021</xref>; <xref ref-type="bibr" rid="bib56">Silson et al., 2021</xref>), but it was not clear whether hippocampus would also engage in the representation of a comparably simple, low-level visual sequence presented in our paradigm.</p><p>Our results confirmed that hippocampus representations resemble an SR-like predictive map, favoring future over past sequence locations. This result highlights the compelling conceptual parallels between mnemonic expectations in hippocampus (<xref ref-type="bibr" rid="bib26">Hindy et al., 2016</xref>) and its perceptual manifestation in sensory cortex. On a conceptual level, navigation (in memory and space) and processing visual events both involve abstraction of the relational structure between events to enable forward planning and predictions. Similar to navigational space, visual space can be represented in terms of its relational structure-like direction and distance, and it has been suggested that similar mechanisms might underlie spatial and non-spatial representations (<xref ref-type="bibr" rid="bib44">Nau et al., 2018a</xref>), especially if there is sequential structure present (<xref ref-type="bibr" rid="bib17">Finnie et al., 2021</xref>). Supporting this notion, recently, a conceptual link between representations for visual understanding and spatial navigation has been proposed that suggests a common underlying map-like representation of visual and navigational task structure (<xref ref-type="bibr" rid="bib54">Schwartenbeck et al., 2021</xref>).</p><p>These conceptual links, as well as anatomical (<xref ref-type="bibr" rid="bib16">Felleman and Van Essen, 1991</xref>; <xref ref-type="bibr" rid="bib28">Huang et al., 2021</xref>) and functional connections between the hippocampus and visual cortex (<xref ref-type="bibr" rid="bib6">Bosch et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Hindy et al., 2016</xref>; <xref ref-type="bibr" rid="bib29">Ji and Wilson, 2007</xref>; <xref ref-type="bibr" rid="bib35">Kok and Turk-Browne, 2018</xref>; <xref ref-type="bibr" rid="bib37">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="bib44">Nau et al., 2018a</xref>) raise the question whether hippocampal representations are independent from V1, or whether V1 is instead receiving the predictions as a feedback signal from hippocampus. Supporting the idea of functional feedback, <xref ref-type="bibr" rid="bib17">Finnie et al., 2021</xref> recently showed that V1 predictions were heavily impaired after hippocampus damage. However, contrary to this notion, spatiotemporal sequence predictions have also been shown to occur locally within V1 without the need for top-down predictions (<xref ref-type="bibr" rid="bib22">Gavornik and Bear, 2014</xref>; <xref ref-type="bibr" rid="bib64">Xu et al., 2012</xref>). Our study showed no functional relationship between sequence prediction in V1 and hippocampus. However, our experimental paradigm was not primarily designed to address this question, as it does not exclude the possibility that an apparent coordination might be driven by other factors like attentional fluctuations across participants. Further, V1-hippocampus coordination might exist on a trial-by-trial level, which does not necessarily transfer to statistical comparisons across participants. Future experiments, using more than one stimulus sequence could potentially address this question by comparing evidence of sequence-specific representations in both areas within participants.</p><p>It is notable that while hippocampal and visual representations appear similar with respect to their SR-like representation, they also show qualitative differences with respect to their underlying coding properties. V1 representations of individual sequence items resembled a coding based on spatial tuning. Hippocampus on the other hand represented relevant items predominantly in terms of their temporal distance within the sequence, suggesting that representations capitulate on the transitionally structure of the visual sequence. These results align with previous reports that hippocampus can learn to represent temporal sequence structure (<xref ref-type="bibr" rid="bib59">Thavabalasingam et al., 2018</xref>; <xref ref-type="bibr" rid="bib60">Thavabalasingam et al., 2019</xref>) and temporal proximity in a spatial navigation task (<xref ref-type="bibr" rid="bib12">Deuker et al., 2016</xref>; <xref ref-type="bibr" rid="bib27">Howard et al., 2014</xref>), but to the best of our knowledge, constitute the first reports of coding temporal distance of a visual sequence.</p><p>Furthermore, hippocampus predictive codes were found to persist after the sequence task and coactivation of related sequence locations was still present during the stimulus localizer, potentially indicating that hippocampus representations reflect a more stable code operating on a longer timescale. V1 representations on the other hand did not persist throughout the stimulus localizer and reverted back to representing individual spatial locations without coactivation of related sequence locations, further highlighting another qualitative difference between V1 and hippocampus coding. According to the SR, it is expected that sequence predictions will change once the regularities of the environment change. The absence of SR-like pattern in V1 during the functional localizer is therefore not at odds with our results from the main task, but rather indicative of a dynamic updating of the generative model. Taken these qualitative differences together, it is reasonable to speculate that predictive activity in V1 does not merely reflect top-down feedback from hippocampus, but instead that SR-like representations in V1 are somewhat independent, and potentially complementary, to SR-like representations found in the hippocampus.</p><p>In conclusion, our data show that anticipatory activity in early visual cortex and hippocampus is guided by a generative model that represents the relational structure of the visual world, akin to a predictive map. Our results suggest that the observed SR-like representation underlying visual predictions can provide a sophisticated state space representation that enables flexible generalization from partial input to future sequence locations, while also being efficient enough to provide rapid visual computations.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Preregistration</title><p>The experimental design, data analyses, and hypotheses were all preregistered at Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/f8dv9/">https://osf.io/f8dv9/</ext-link>) prior to data collection.</p></sec><sec id="s4-2"><title>Participants</title><p>Thirty-seven right-handed subjects participated in the fMRI study. Two participants were excluded based on predetermined performance and motion criteria during scanning (error rate/relative motion three standard deviations above the group mean). The final sample included 35 subjects (20 females, mean age = 27 years). Target sample size was decided prior to data collection based on a power analysis (two-sided paired t test, power = 80%, Cohenâs d â¥ 0.5 and Î± = 0.05). Participants gave written informed consent in accordance with the institutional guidelines of the local ethical committee (CMO region Arnhem-Nijmegen, The Netherlands) and received monetary compensation for their participation. All participants had normal or corrected-to-normal visual acuity.</p></sec><sec id="s4-3"><title>Stimuli</title><p>Participants viewed a sequence of four white dots on a black background. Dot locations were sampled from eight possible locations (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The center of each dot location was 7 degrees visual angle (dva) away from the central white fixation cross (0.5 dva) and the locations were equally spaced around the center (distance in polar angle from the vertical line: 22.5Â°, 67.5Â°, 112.5Â°, 157.5Â°, 202.5Â°, 247.5Â°, 292.5Â°, and 337.5Â°, see <xref ref-type="fig" rid="fig1">Figure 1B</xref>). The dots had a diameter of 1.2 dva. Stimulus sequences were shown on an MRI safe LCD screen (BOLDscreen 32, 1920 Ã 1080 pixel resolution, 60 Hz refresh rate). Participants were positioned 134 cm away from the screen and viewed the stimuli via a mirror on top of the head coil.</p><p>During full sequence trials, each dot was shown for 100 ms with an interstimulus interval (ISI) of 17 ms, resulting in a total sequence duration of 451 ms. For 52 out of 128 full sequence trials, the onset of the last dot was delayed with an ISI of 170 ms (instead of 17 ms). Participants were instructed to detect and report these delayed sequence presentations via a button press with their right index finger.</p><p>Sequences were constructed such that each of the eight locations served as a starting location for one possible sequence. Further, each quadrant was stimulated once, which also excluded the possibility that neighboring dots were part of the same sequence. This constraint was chosen to minimize the potential spreading of activity from one location to neighboring sequence locations. Specifically, the second dot was always presented opposite of the starting location (180Â° clockwise from the start). The third dot was shown 90Â° clockwise from the second location and the last dot was on the opposite side of the third location. These constraints also served to decouple spatial and temporal distance within the sequence. With these constraints, there were eight possible visual sequences that were randomly assigned and counterbalanced in frequency across subjects. Dots that were part of the sequence are labeled as sequence dots A-B-C-D. While the remaining four dots at locations that were not part of the sequence are referred to as âcontrol dotsâ.</p><p>Note that because within each dot sequence, temporal order and spatial distance were not perfectly decorrelated (e.g., the second sequence dot was always farthest apart from the starting dot), it is not possible to estimate the combined influence of the SR model and the spatial coactivation model on the observed BOLD activity.</p></sec><sec id="s4-4"><title>Experimental design</title><p>The experiment lasted a total of 2 hr and consisted of three blocks (i) learning, (ii) main task, and (iii) a stimulus localizer. During the learning part, participants were familiarized with one of the eight sequences. The full sequence, consisting of four successively presented dots A-B-C-D, was shown 352 times outside and 160 times inside the scanner. In order to maintain participantsâ attention during the learning part, there was a delay detection task on 50% of the trials. Participants were instructed to detect a timing delay of the last dot for which they had 1 s to respond. After every 30 trials, participants were shown their aggregated detection accuracy. During the initial learning phase outside the scanner, participants received additional feedback after each trial on whether their response was correct or incorrect through changes in the color of the fixation cross (green for correct and red for incorrect answers). No trial-wise feedback was given inside the fMRI. Participants were instructed to maintain fixation throughout the experiment and eye movements were measured with an Eyelink 1000 eye-tracker system (SR Research, Ontario, Canada; 1000 Hz sampling rate).</p><p>The main task consisted of three runs of equal duration (about 13 min). There were 192 trials per run and 576 trials in total. Trials were separated by a variable inter-trial interval (ITI) with a duration drawn from a truncated exponential distribution with a minimum of 2 s, maximum of 10.9 s, and mean of 3.72 s. The variable ITI ensured that the experimental paradigm had no temporal structure that participants could learn to expect the onset of a trial. This allowed us to focus in the present study on the learning and representation of structural knowledge, independent of any temporal expectation effects.</p><p>To probe activity replay, we introduced partial sequence trials where only one of the four dots was shown for 100 ms, instead of the full sequence. Visually, there was no difference between the ITI and the part of the partial sequence trials where the dots were omitted, both showed a fixation cross at the center of the screen. During each run, two thirds of the 192 trials were full sequence trials (128 trials) and one third of the trials were partial sequence trials (64 trials). Trial order was pseudo-randomized with the constraint that partial sequence trials were always followed and preceded by a full sequence trial, excluding the possibility of partial sequence trial repetitions. The pseudo-randomization (perfect counterbalancing was numerically not possible with the set number of trial types and repetitions), rules out the possibility of systematic order effects. There was a task on ~40% of the full sequence trials (156/384 trials). At the end of each run, participants received feedback on their performance.</p><p>After the main task, we ran a functional localizer (~16 min) where each dot was flashed at 2 Hz for 13.5 s in a pseudo-randomized order, followed by 15 s rest period. In total, each dot location was presented eight times and each of the eight dots followed once immediately after the rest period. Participants performed a letter stream task at fixation where they had to detect target letters (âXâ and âZâ) in a stream of non-target letters (âAâ, âTâ, âNâ, âUâ, âVâ, âYâ, âHâ, and âRâ). The target probability was 10%. Each letter was presented for 500 ms.</p><p>For a small subset of N = 7 participants, after the localizer, we additionally presented moving bar stimuli, in order to map the pRFs of voxels in early visual cortex. During these runs, bars containing full-contrast flickering checkerboards (2 Hz) moved across the screen in a circular aperture with a diameter of 20Â°. The bars moved in eight different directions (four cardinal and four diagonal directions) in 20 steps of 1Â°. Four blank fixation screens (10.8 s) were inserted after each of the cardinally moving bars. Throughout each run (5.76 min), a colored fixation dot was presented in the center of the screen, changing color (red to green and green to red) at random time points. Participantsâ task was to press a button whenever this color change occurred. Participants performed four identical runs of this task.</p></sec><sec id="s4-5"><title>MRI acquisition</title><p>Functional and anatomical MRI data were acquired on a 3 T PrismaFit scanner (Siemens AG, Healthcare Sector, Erlangen, Germany) using a 32-channel head coil. The protocol included a T1-weighted anatomical scan and five functional runs. The anatomical scan was acquired with a Magnetization Prepared Rapid Acquisition Gradient Echo sequence (MP-RAGE; TR = 2300 ms, TI = 1100 ms, TE = 3 ms, flip angle = 8Â°, 1 Ã 1 Ã 1 mm<sup>3</sup> isotropic). To acquire the functional images, we used a T2*-weighted multiband 4 (<xref ref-type="bibr" rid="bib40">Moeller et al., 2010</xref>) sequence (TR = 1500 ms, TE = 39 ms, flip angle = 75Â°, 2 Ã 2 Ã 2 mm<sup>3</sup>, 68 slices). The five functional runs comprised of one learning run, three main task runs, and one localizer run. For two subjects only two main task runs were acquired because of time constraints. Seven participants participated in a previous study in which they completed four runs of pRF mapping.</p></sec><sec id="s4-6"><title>fMRI preprocessing</title><p>MRI data were preprocessed using FSL (version 6.00; FMRIB Software Library) (<xref ref-type="bibr" rid="bib57">Smith et al., 2004</xref>). We applied brain extraction using BET, motion correction using MCFLIRT, temporal high-pass filtering (100 s) and spatial smoothing (Gaussian kernel, FWHM = 5 mm). All analyses were carried out in native subject space. The first three volumes of each run were discarded to allow for signal stabilization. Registration of the functional images to the anatomical image was performed with FLIRT boundary-based registration. The anatomical image was registered to the MNI152 T1 2 mm standard space template (linear registration, 12 degrees of freedom).</p></sec><sec id="s4-7"><title>ROI selection</title><p>V1 and hippocampus ROIs were determined using the automatic cortical parcellation provided by Freesurfer (<xref ref-type="bibr" rid="bib18">Fischl, 2012</xref>) based on individual T1 images. Anatomical V1 and hippocampus masks were then transformed into native space using linear transformation. For V1, we used a preregistered voxel selection method to determine V1 subpopulations that are most responsive to individual stimulus locations.</p><p>First, the localizer data were fitted with a voxel-wise general linear model (GLM) using FSL FEAT (<xref ref-type="bibr" rid="bib57">Smith et al., 2004</xref>) with the following regressors: 8 regressors of interest for stimulation of each of the locations (duration = 13.5 s), 1 regressor for the instructions and the end-of-block screen (duration = 4.5 and 15 s, respectively) as well as the 24 FSL motion regressors.</p><p>Second, for each location, we calculated the GLM contrast by comparing one location to all other locations and selected the 25 most selective voxels (highest z-values). Third, we removed voxels from the selection that were selective for multiple dot locations. Finally, we determined the lowest number of selective voxels per region and removed the least active voxels from all other locations until all V1 subpopulations had the exact same number of selected voxels per location. This procedure was chosen to rule out the possibility that potential activity differences across locations could be attributed to different number of voxels per region. Across subjects, we selected on average 22.05 voxels (SD = 2.88) per location.</p></sec><sec id="s4-8"><title>V1 BOLD amplitude modulation</title><p>A GLM for the main task was created with the following regressors: 8 regressors for each single dot trial (4 sequence dots and 4 control dots), 1 regressor for the full sequence trial, 1 regressor of no interest to model the instructions and the feedback at the end of a run and 24 motion regressors (6 standard and 18 extended FSL motion parameters, i.e., the derivatives of the standard motion parameters, the squares of standard motion parameters, and the squares of the derivatives). Note that the control dot trials in the main task were modeled in the GLM, but treated as regressors of no interest. The model was convolved with a single gamma hemodynamic response function. Nine contrasts were set up that tested which voxels were more responsive to presentation of a single dot (eight contrasts, one for each dot) or the full sequence (one contrast) compared to baseline. The GLM was fit to each run separately and resulting beta estimates were averaged across runs for each participant. In order to obtain an estimate of stimulus-specific activity (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), we averaged the activity at the four control ROIs and subtracted it from the activity at the sequence ROIs.</p></sec><sec id="s4-9"><title>Correlation with behavior</title><p>In order to relate SR representations to behavior, we first calculated individual V1 BOLD differences for all successor versus all predecessor locations to get an estimate for how much participantâs predictions were skewed toward future locations. We then correlated these values with behavioral accuracy across subjects using Spearman correlation.</p></sec><sec id="s4-10"><title>V1 model comparison</title><p>For each participant, V1 BOLD activity from the partial trials was fitted with three models, SR, coactivation (CO), and a null-model (H0). The resulting root mean square error (RSME, lower values = better fit) between model fit and observed data was then tested across participants for significance using paired-sample t tests to address the question whether one model prediction describes the underlying data better than competing models.</p><p>The model prediction of the SR is based on the task structure, formalized in a transition matrix <inline-formula><mml:math id="inf1"><mml:mi>T</mml:mi></mml:math></inline-formula> of the sequence A-B-C-D (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The SR matrix <inline-formula><mml:math id="inf2"><mml:mi>M</mml:mi></mml:math></inline-formula> is then calculated as:<disp-formula id="equ1"><mml:math id="m1"><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mi> </mml:mi><mml:mi>Î³</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></disp-formula></p><p>where I is the identity matrix and <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>Î³</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mtext>Â </mml:mtext></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the discount factor or predictive horizon. During model fitting, Î³ was a free parameter, meaning that instead of using a fixed value, individual Î³ values were determined for each participant. Here, larger values of Î³ result in a smaller exponential decay of future states. The model prediction of the CO model is based on the CO of events. In contrast to SR, the task structure is non-directed and off-diagonal values in the CO model are constant and modulated in amplitude by a free multiplicative parameter Ï. The H0 (null) model serves as a baseline that assumes no off-diagonal (predictive) activity. In order to be interpretable any winning model should outperform the H0 model. The diagonal values in all three models reflect the bottom-up stimulation induced by the single dot of the partial trials.</p></sec><sec id="s4-11"><title>Hippocampal decoding</title><p>The decoding analysis was performed with scikit-learn (<xref ref-type="bibr" rid="bib47">Pedregosa et al., 2011</xref>). Individual voxel time courses were low-pass filtered using a Savitzky-Golay filter with a window length of 5 TRs and polynomial order of 3 (<xref ref-type="bibr" rid="bib50">Savitzky and Golay, 1964</xref>) and normalized to z-scores. Volumes for individual localizer trials were averaged between 3 and 13.5 s to capture only stimulus-related BOLD activity. A logistic regression classifier (default values, L2 regularization; C = 1) was trained to distinguish between eight stimulus locations during the independent localizer run. Before applying the trained classifier to the main task, we confirmed that the classifier was indeed able to distinguish between stimulus locations within the localizer. To this end, we performed a leave-one-out cross-validation and tested the decoding accuracy against chance level (1/8 = 12.5 %) across subjects using a one-sample t test. In addition to a binary classifier output for each class, we also looked at the probabilistic output. For each sample in the localizer test set, we obtained eight probability values, one for each class. We refer to the classifier probability as classifier evidence, as the probability reflects the evidence that a particular class is represented. For each participant, probability values were averaged across trials to obtain location-specific response profiles.</p><p>Next, we trained the classifier on all localizer trials and applied it to individual trials of the main task. Volumes for individual main task trials were averaged between 3 and 6 s to capture only stimulus-related BOLD activity. Note that the main task was an event-related design with shorter trial durations compared to the block-design localizer with 13.5 s stimulation periods; hence, the different averaging windows of 3â13.5 and 3â6 s. Similar to the BOLD analysis in V1, for each partial sequence trial in the main task, we averaged the classifier evidence for the four control locations and subtracted it from the evidence of the sequence locations. We then averaged the classifier evidence for all predecessor and successor locations, respectively, and compared the evidence across subjects with a paired-sample t test.</p><p>Finally, in order to rule out that the chosen time window had any influence on the results, we repeated the decoding analysis in a time-resolved manner, repeating the steps above for each volume from 0 to 13.5 s separately. Fitting a standard hemodynamic response function (hrf) revealed a transient decoding evidence peak at around 4.7 s.</p></sec><sec id="s4-12"><title>Hippocampus and V1 tuning</title><p>The tuning analysis investigates coactivation pattern during the localizer and focuses on the four locations that were part of the stimulus sequence in the preceding main task. Classifier evidence values within the localizer were averaged and sorted to reveal potential coactivation (tuning) pattern of sequence locations. Three tuning patterns were considered and tested: (i) temporal tuning, assuming a linear decay from the currently presented stimulus toward location that where farther in the past and future (two free parameters, slope, and intercept), (ii) spatial tuning, assuming a linear decay from the current stimulus toward other stimulus locations modulated by spatial distance (two free parameters, slope, and intercept), and (iii) a baseline no-coactivation pattern. Note that the latter model was considered because V1 tuning curves were rather sharp with little activity spread to immediate neighboring locations (5.4Â° apart; <xref ref-type="fig" rid="fig2">Figure 2A</xref>) and locations in the current analysis were 9.9Â° apart. For each participant, aggregated classifier evidence was fitted using three tuning patterns and resulting errors were compared across subjects to determine the best-fitting pattern. Fitting was performed using the curve_fit function in SciPy 1.6.2 (<xref ref-type="bibr" rid="bib63">Virtanen et al., 2020</xref>).</p></sec><sec id="s4-13"><title>pRF estimation and reconstruction</title><p>pRF data were available for seven participants from a previous study (<xref ref-type="bibr" rid="bib15">Ekman et al., 2022</xref>) and were used to validate visually that the voxel selection based on the functional localizer selected voxel that corresponds to the stimulated location is visual space (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Data from the moving bar runs were used to estimate the pRF of each voxel in the functional volumes using MrVista (<ext-link ext-link-type="uri" xlink:href="http://white.stanford.edu/software">http://white.stanford.edu/software</ext-link>). In this analysis, a predicted BOLD signal is calculated from the known stimulus parameters and a model of the underlying neuronal population. The model of the neuronal population consisted of a two-dimensional Gaussian pRF, with parameters x0, y0, and Ï0, where x0 and y0 are the coordinates of the center of the receptive field, and Ï0 indicates its spread (standard deviation), or size. All parameters were stimulus-referred, and their units were degrees of visual angle. These parameters were adjusted to obtain the best possible fit of the predicted to the actual BOLD signal. This method has been shown to produce pRF size estimates that agree well with electrophysiological receptive field measurements in monkey and human visual cortex (<xref ref-type="bibr" rid="bib32">Klink et al., 2021</xref>). For details of this procedure, see <xref ref-type="bibr" rid="bib13">Dumoulin and Wandell, 2008</xref>; <xref ref-type="bibr" rid="bib30">Kay et al., 2015</xref>. Once estimated, x0 and y0 were converted to eccentricity and polar-angle measures and co-registered with the functional images using linear transformation.</p><p>For the pRF-based stimulus reconstruction, for each participant, we first limited the pRF data to voxel that were selected based on the functional localizer (25 voxels per stimulus location, 200 voxels in total). This selection step would allow us to visually inspect whether the voxel selection accurately selected voxel corresponding to the respective stimulus location. Second, every voxel is described as a 2D Gaussian with parameters x0, y0, and s0 from the pRF estimation. The 2D Gaussians for each voxel, represented by a pixel Ã pixel image, were scaled based on the percent signal change obtained from the functional localizer GLM, and consecutively summed over voxels to create one 2D representation of the reconstructed stimulus. This procedure was repeated separately for all eight stimulus locations. Finally, for visualization purpose, the eight individual localizer conditions were rotated to one stimulus location (22.5Â°) and averaged across stimulus locations and participants.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Senior editor, <italic>eLife</italic></p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Methodology, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study followed institutional guidelines of the local ethics committee (CMO region Arnhem-Nijmegen, The Netherlands; Research Protocol &quot;Imaging Human Cognition&quot;, NL45659.091.14), including informed consent of all participants.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-78904-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data and code used for stimulus presentation and analysis are available on the Donders Repository (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.34973/bsy6-9h29">https://doi.org/10.34973/bsy6-9h29</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>M</given-names></name><name><surname>Kusch</surname><given-names>S</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Successor-like representation guides the prediction of future events in human visual cortex and hippocampus</data-title><source>Donders Repository</source><pub-id pub-id-type="doi">10.34973/bsy6-9h29</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alvernhe</surname><given-names>A</given-names></name><name><surname>Save</surname><given-names>E</given-names></name><name><surname>Poucet</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Local remapping of place cell firing in the tolman detour task</article-title><source>The European Journal of Neuroscience</source><volume>33</volume><fpage>1696</fpage><lpage>1705</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2011.07653.x</pub-id><pub-id pub-id-type="pmid">21395871</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aronov</surname><given-names>D</given-names></name><name><surname>Nevers</surname><given-names>R</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mapping of a non-spatial dimension by the hippocampal-entorhinal circuit</article-title><source>Nature</source><volume>543</volume><fpage>719</fpage><lpage>722</lpage><pub-id pub-id-type="doi">10.1038/nature21692</pub-id><pub-id pub-id-type="pmid">28358077</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Backus</surname><given-names>AR</given-names></name><name><surname>Bosch</surname><given-names>SE</given-names></name><name><surname>Ekman</surname><given-names>M</given-names></name><name><surname>Grabovetsky</surname><given-names>AV</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mnemonic convergence in the human hippocampus</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>11991</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms11991</pub-id><pub-id pub-id-type="pmid">27325442</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barron</surname><given-names>HC</given-names></name><name><surname>Reeve</surname><given-names>HM</given-names></name><name><surname>Koolschijn</surname><given-names>RS</given-names></name><name><surname>Perestenko</surname><given-names>PV</given-names></name><name><surname>Shpektor</surname><given-names>A</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Rothaermel</surname><given-names>R</given-names></name><name><surname>Campo-Urriza</surname><given-names>N</given-names></name><name><surname>OâReilly</surname><given-names>JX</given-names></name><name><surname>Bannerman</surname><given-names>DM</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Dupret</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neuronal computation underlying inferential reasoning in humans and mice</article-title><source>Cell</source><volume>183</volume><fpage>228</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.08.035</pub-id><pub-id pub-id-type="pmid">32946810</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Muller</surname><given-names>TH</given-names></name><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Mark</surname><given-names>S</given-names></name><name><surname>Baram</surname><given-names>AB</given-names></name><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>What is a cognitive map</article-title><source>Organizing Knowledge for Flexible Behavior. Neuron</source><volume>100</volume><fpage>490</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.002</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosch</surname><given-names>SE</given-names></name><name><surname>Jehee</surname><given-names>JFM</given-names></name><name><surname>FernÃ¡ndez</surname><given-names>G</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Reinstatement of associative memories in early visual cortex is signaled by the hippocampus</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>7493</fpage><lpage>7500</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0805-14.2014</pub-id><pub-id pub-id-type="pmid">24872554</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunec</surname><given-names>IK</given-names></name><name><surname>Momennejad</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Predictive representations in hippocampal and prefrontal hierarchies</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>299</fpage><lpage>312</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1327-21.2021</pub-id><pub-id pub-id-type="pmid">34799416</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinescu</surname><given-names>AO</given-names></name><name><surname>OâReilly</surname><given-names>JX</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Organizing conceptual knowledge in humans with a gridlike code</article-title><source>Science</source><volume>352</volume><fpage>1464</fpage><lpage>1468</lpage><pub-id pub-id-type="doi">10.1126/science.aaf0941</pub-id><pub-id pub-id-type="pmid">27313047</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Improving generalization for temporal difference learning: the successor representation</article-title><source>Neural Computation</source><volume>5</volume><fpage>613</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1162/neco.1993.5.4.613</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How do expectations shape perception?</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>764</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.06.002</pub-id><pub-id pub-id-type="pmid">30122170</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deuker</surname><given-names>L</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name><name><surname>Fell</surname><given-names>J</given-names></name><name><surname>Axmacher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Human neuroimaging studies on the hippocampal CA3 region - integrating evidence for pattern separation and completion</article-title><source>Frontiers in Cellular Neuroscience</source><volume>8</volume><elocation-id>64</elocation-id><pub-id pub-id-type="doi">10.3389/fncel.2014.00064</pub-id><pub-id pub-id-type="pmid">24624058</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deuker</surname><given-names>L</given-names></name><name><surname>Bellmund</surname><given-names>JL</given-names></name><name><surname>Navarro SchrÃ¶der</surname><given-names>T</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>An event map of memory space in the hippocampus</article-title><source>eLife</source><volume>5</volume><elocation-id>e16534</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.16534</pub-id><pub-id pub-id-type="pmid">27710766</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Population receptive field estimates in human visual cortex</article-title><source>NeuroImage</source><volume>39</volume><fpage>647</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.034</pub-id><pub-id pub-id-type="pmid">17977024</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>M</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Time-compressed preplay of anticipated events in human primary visual cortex</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15276</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15276</pub-id><pub-id pub-id-type="pmid">28534870</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>M</given-names></name><name><surname>Gennari</surname><given-names>G</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Probabilistic Forward Replay of Anticipated Stimulus Sequences in Human Primary Visual Cortex and Hippocampus</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.01.26.477907</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1-a</pub-id><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finnie</surname><given-names>PSB</given-names></name><name><surname>Komorowski</surname><given-names>RW</given-names></name><name><surname>Bear</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The spatiotemporal organization of experience dictates hippocampal involvement in primary visual cortical plasticity</article-title><source>Current Biology</source><volume>31</volume><fpage>3996</fpage><lpage>4008</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.06.079</pub-id><pub-id pub-id-type="pmid">34314678</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FreeSurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiser</surname><given-names>J</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Statistical learning of higher-order temporal structure from visual shape sequences</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>28</volume><fpage>458</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1037//0278-7393.28.3.458</pub-id><pub-id pub-id-type="pmid">12018498</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>DJ</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reverse replay of behavioural sequences in hippocampal place cells during the awake state</article-title><source>Nature</source><volume>440</volume><fpage>680</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1038/nature04587</pub-id><pub-id pub-id-type="pmid">16474382</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garvert</surname><given-names>MM</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A map of abstract relational knowledge in the human hippocampal-entorhinal cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e17086</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.17086</pub-id><pub-id pub-id-type="pmid">28448253</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gavornik</surname><given-names>JP</given-names></name><name><surname>Bear</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learned spatiotemporal sequence recognition and prediction in primary visual cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>732</fpage><lpage>737</lpage><pub-id pub-id-type="doi">10.1038/nn.3683</pub-id><pub-id pub-id-type="pmid">24657967</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The successor representation: its computational logic and neural substrates</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7193</fpage><lpage>7200</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0151-18.2018</pub-id><pub-id pub-id-type="pmid">30006364</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>GlÃ¤scher</surname><given-names>J</given-names></name><name><surname>Daw</surname><given-names>N</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>OâDoherty</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning</article-title><source>Neuron</source><volume>66</volume><fpage>585</fpage><lpage>595</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.04.016</pub-id><pub-id pub-id-type="pmid">20510862</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>AS</given-names></name><name><surname>van der Meer</surname><given-names>MAA</given-names></name><name><surname>Touretzky</surname><given-names>DS</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Hippocampal replay is not a simple function of experience</article-title><source>Neuron</source><volume>65</volume><fpage>695</fpage><lpage>705</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.034</pub-id><pub-id pub-id-type="pmid">20223204</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hindy</surname><given-names>NC</given-names></name><name><surname>Ng</surname><given-names>FY</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Linking pattern completion in the hippocampus to predictive coding in visual cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>665</fpage><lpage>667</lpage><pub-id pub-id-type="doi">10.1038/nn.4284</pub-id><pub-id pub-id-type="pmid">27065363</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>LR</given-names></name><name><surname>Javadi</surname><given-names>AH</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Mill</surname><given-names>RD</given-names></name><name><surname>Morrison</surname><given-names>LC</given-names></name><name><surname>Knight</surname><given-names>R</given-names></name><name><surname>Loftus</surname><given-names>MM</given-names></name><name><surname>Staskute</surname><given-names>L</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The hippocampus and entorhinal cortex encode the path and euclidean distances to goals during navigation</article-title><source>Current Biology</source><volume>24</volume><fpage>1331</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.05.001</pub-id><pub-id pub-id-type="pmid">24909328</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>CC</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Hsu</surname><given-names>CCH</given-names></name><name><surname>Feng</surname><given-names>J</given-names></name><name><surname>Lin</surname><given-names>CP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Extensive cortical connectivity of the human hippocampal memory system: beyond the â whatâ and â whereâ dual stream model</article-title><source>Cerebral Cortex</source><volume>31</volume><fpage>4652</fpage><lpage>4669</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhab113</pub-id><pub-id pub-id-type="pmid">34013342</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>D</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Coordinated memory replay in the visual cortex and hippocampus during sleep</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>100</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1038/nn1825</pub-id><pub-id pub-id-type="pmid">17173043</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attention reduces spatial uncertainty in human ventral temporal cortex</article-title><source>Current Biology</source><volume>25</volume><fpage>595</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.12.050</pub-id><pub-id pub-id-type="pmid">25702580</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Killian</surname><given-names>NJ</given-names></name><name><surname>Jutras</surname><given-names>MJ</given-names></name><name><surname>Buffalo</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A map of visual space in the primate entorhinal cortex</article-title><source>Nature</source><volume>491</volume><fpage>761</fpage><lpage>764</lpage><pub-id pub-id-type="doi">10.1038/nature11587</pub-id><pub-id pub-id-type="pmid">23103863</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klink</surname><given-names>PC</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Population receptive fields in nonhuman primates from whole-brain fmri and large-scale neurophysiology in visual cortex</article-title><source>eLife</source><volume>10</volume><elocation-id>e67304</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67304</pub-id><pub-id pub-id-type="pmid">34730515</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knapen</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Topographic connectivity reveals task-dependent retinotopic processing throughout the human brain</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2017032118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2017032118</pub-id><pub-id pub-id-type="pmid">33372144</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Jehee</surname><given-names>JFM</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>265</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.034</pub-id><pub-id pub-id-type="pmid">22841311</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Associative prediction of visual shape in the hippocampus</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>6888</fpage><lpage>6899</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0163-18.2018</pub-id><pub-id pub-id-type="pmid">29986875</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Economides</surname><given-names>M</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fast sequences of non-spatial state representations in humans</article-title><source>Neuron</source><volume>91</volume><fpage>194</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.05.028</pub-id><pub-id pub-id-type="pmid">27321922</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>ACH</given-names></name><name><surname>Yeung</surname><given-names>LK</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The hippocampus and visual perception</article-title><source>Frontiers in Human Neuroscience</source><volume>6</volume><elocation-id>91</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2012.00091</pub-id><pub-id pub-id-type="pmid">22529794</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leutgeb</surname><given-names>S</given-names></name><name><surname>Leutgeb</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Pattern separation, pattern completion, and new neuronal codes within a continuous CA3 MAP</article-title><source>Learning &amp; Memory</source><volume>14</volume><fpage>745</fpage><lpage>757</lpage><pub-id pub-id-type="doi">10.1101/lm.703907</pub-id><pub-id pub-id-type="pmid">18007018</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname><given-names>MR</given-names></name><name><surname>Quirk</surname><given-names>MC</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Experience-dependent asymmetric shape of hippocampal receptive fields</article-title><source>Neuron</source><volume>25</volume><fpage>707</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)81072-7</pub-id><pub-id pub-id-type="pmid">10774737</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Olman</surname><given-names>CA</given-names></name><name><surname>Auerbach</surname><given-names>E</given-names></name><name><surname>Strupp</surname><given-names>J</given-names></name><name><surname>Harel</surname><given-names>N</given-names></name><name><surname>UÄurbil</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Multiband multislice GE-EPI at 7 tesla, with 16-fold acceleration using partial parallel imaging with application to high spatial and temporal whole-brain fmri</article-title><source>Magnetic Resonance in Medicine</source><volume>63</volume><fpage>1144</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1002/mrm.22361</pub-id><pub-id pub-id-type="pmid">20432285</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Russek</surname><given-names>EM</given-names></name><name><surname>Cheong</surname><given-names>JH</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The successor representation in human reinforcement learning</article-title><source>Nature Human Behaviour</source><volume>1</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1038/s41562-017-0180-8</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Otto</surname><given-names>AR</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Offline replay supports planning in human reinforcement learning</article-title><source>eLife</source><volume>7</volume><elocation-id>e32548</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.32548</pub-id><pub-id pub-id-type="pmid">30547886</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Learning structures: predictive representations, replay, and generalization</article-title><source>Current Opinion in Behavioral Sciences</source><volume>32</volume><fpage>155</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.02.017</pub-id><pub-id pub-id-type="pmid">35419465</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Julian</surname><given-names>JB</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>How the brainâs navigation system shapes our visual experience</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>810</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.06.008</pub-id><pub-id pub-id-type="pmid">30031670</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Navarro SchrÃ¶der</surname><given-names>T</given-names></name><name><surname>Bellmund</surname><given-names>JLS</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Hexadirectional coding of visual space in human entorhinal cortex</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>188</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1038/s41593-017-0050-8</pub-id><pub-id pub-id-type="pmid">29311746</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>OâKeefe</surname><given-names>J</given-names></name><name><surname>Nadel</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1978">1978</year><source>The Hippocampus as a Cognitive Map</source><publisher-name>Clarendon Press</publisher-name></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name><name><surname>Vanderplas</surname><given-names>J</given-names></name><name><surname>Passos</surname><given-names>A</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Brucher</surname><given-names>M</given-names></name><name><surname>Perrot</surname><given-names>M</given-names></name><name><surname>Duchesnay</surname><given-names>Ã</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The mechanisms for pattern completion and pattern separation in the hippocampus</article-title><source>Frontiers in Systems Neuroscience</source><volume>7</volume><elocation-id>74</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2013.00074</pub-id><pub-id pub-id-type="pmid">24198767</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Russek</surname><given-names>EM</given-names></name><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural Evidence for the Successor Representation in Choice Evaluation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.08.29.458114</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savitzky</surname><given-names>A</given-names></name><name><surname>Golay</surname><given-names>MJE</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>Smoothing and differentiation of data by simplified least squares procedures</article-title><source>Analytical Chemistry</source><volume>36</volume><fpage>1627</fpage><lpage>1639</lpage><pub-id pub-id-type="doi">10.1021/ac60214a047</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Kustner</surname><given-names>LV</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Shaping of object representations in the human medial temporal lobe based on temporal regularities</article-title><source>Current Biology</source><volume>22</volume><fpage>1622</fpage><lpage>1627</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.06.056</pub-id><pub-id pub-id-type="pmid">22885059</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name><name><surname>Cordova</surname><given-names>NI</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural representations of events arise from temporal community structure</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>486</fpage><lpage>492</lpage><pub-id pub-id-type="doi">10.1038/nn.3331</pub-id><pub-id pub-id-type="pmid">23416451</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuck</surname><given-names>NW</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sequential replay of nonspatial task states in the human hippocampus</article-title><source>Science</source><volume>364</volume><elocation-id>eaaw5181</elocation-id><pub-id pub-id-type="doi">10.1126/science.aaw5181</pub-id><pub-id pub-id-type="pmid">31249030</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schwartenbeck</surname><given-names>P</given-names></name><name><surname>Baram</surname><given-names>A</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Mark</surname><given-names>S</given-names></name><name><surname>Muller</surname><given-names>T</given-names></name><name><surname>Dolan</surname><given-names>R</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Behrens</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Generative Replay for Compositional Visual Understanding in the Prefrontal-Hippocampal Circuit</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.06.06.447249</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Silson</surname><given-names>EH</given-names></name><name><surname>Zeidman</surname><given-names>P</given-names></name><name><surname>Knapen</surname><given-names>T</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Representation of Contralateral Visual Space in the Human Hippocampus</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.07.30.228361</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silson</surname><given-names>EH</given-names></name><name><surname>Zeidman</surname><given-names>P</given-names></name><name><surname>Knapen</surname><given-names>T</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Representation of contralateral visual space in the human hippocampus</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>2382</fpage><lpage>2392</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1990-20.2020</pub-id><pub-id pub-id-type="pmid">33500275</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Johansen-Berg</surname><given-names>H</given-names></name><name><surname>Bannister</surname><given-names>PR</given-names></name><name><surname>De Luca</surname><given-names>M</given-names></name><name><surname>Drobnjak</surname><given-names>I</given-names></name><name><surname>Flitney</surname><given-names>DE</given-names></name><name><surname>Niazy</surname><given-names>RK</given-names></name><name><surname>Saunders</surname><given-names>J</given-names></name><name><surname>Vickers</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>De Stefano</surname><given-names>N</given-names></name><name><surname>Brady</surname><given-names>JM</given-names></name><name><surname>Matthews</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Advances in functional and structural mr image analysis and implementation as fsl</article-title><source>NeuroImage</source><volume>23</volume><fpage>S208</fpage><lpage>S219</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The hippocampus as a predictive MAP</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1643</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1038/nn.4650</pub-id><pub-id pub-id-type="pmid">28967910</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thavabalasingam</surname><given-names>S</given-names></name><name><surname>OâNeil</surname><given-names>EB</given-names></name><name><surname>Lee</surname><given-names>ACH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multivoxel pattern similarity suggests the integration of temporal duration in hippocampal event sequence representations</article-title><source>NeuroImage</source><volume>178</volume><fpage>136</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.05.036</pub-id><pub-id pub-id-type="pmid">29775662</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thavabalasingam</surname><given-names>S</given-names></name><name><surname>OâNeil</surname><given-names>EB</given-names></name><name><surname>Tay</surname><given-names>J</given-names></name><name><surname>Nestor</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>ACH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Evidence for the incorporation of temporal duration information in human hippocampal long-term memory sequence representations</article-title><source>PNAS</source><volume>116</volume><fpage>6407</fpage><lpage>6414</lpage><pub-id pub-id-type="doi">10.1073/pnas.1819993116</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>Cognitive maps in rats and men</article-title><source>Psychological Review</source><volume>55</volume><fpage>189</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1037/h0061626</pub-id><pub-id pub-id-type="pmid">18870876</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>JungÃ©</surname><given-names>JA</given-names></name><name><surname>Scholl</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The automaticity of visual statistical learning</article-title><source>Journal of Experimental Psychology. General</source><volume>134</volume><fpage>552</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.134.4.552</pub-id><pub-id pub-id-type="pmid">16316291</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>Ä°</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>S</given-names></name><name><surname>Jiang</surname><given-names>W</given-names></name><name><surname>Poo</surname><given-names>MM</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Activity recall in a visual cortical ensemble</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>449</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1038/nn.3036</pub-id><pub-id pub-id-type="pmid">22267160</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78904.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Barense</surname><given-names>Morgan</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>University of Toronto</institution></institution-wrap><country>Canada</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.03.23.485480" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.03.23.485480"/></front-stub><body><p>In this paper, Ekman and colleagues present compelling fMRI evidence from a visual sequence task that both the early visual cortex (V1) and the hippocampus represent perceptual sequences in the form of a predictive &quot;successor&quot; representation, where the current state is represented in terms of its future (successor) states in a temporally discounted fashion. In both brain structures, there was evidence for upcoming, but not preceding steps in the sequence, and these results were found only in the temporal but not spatial domain. This study offers the fundamental suggestion that both the hippocampus and V1 represent temporally structured information in a predictive, future-oriented manner.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78904.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Barense</surname><given-names>Morgan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>University of Toronto</institution></institution-wrap><country>Canada</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Barron</surname><given-names>Helen</given-names></name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.03.23.485480">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.03.23.485480v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Successor-like representation guides the prediction of future events in human visual cortex and hippocampus&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Chris Baker as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Helen Barron (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The reviewers detail their essential revisions below. Our discussion converged on three key points:</p><p>1) We ask that the authors keep all model comparisons consistent across regions and tasks.</p><p>2) Additional analyses appear necessary to clarify the relationship between the hippocampus and V1.</p><p>3) In the revision it will be important to consider the successor representation model proposed here to other predictive sequence models.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1) If SR is the best name for the discussed model, it should be clarified why this is the case and, importantly, any difference with the SR as defined in the RL literature should be discussed. Otherwise, another term might be more appropriate.</p><p>2) It would be interesting to discuss in the discussion the distinction between the SR model and more complex models that might fit human behaviors and representations just as good or better. For example, with the current design, the SR model can't be disentangled from a more complex model in which all one-step transitions are stored and perhaps in which predictions are iteratively updated based on additional evidence (appearing items). A design in which each state is associated with multiple possible states (with different probabilities) might allow disentangling such additional possibilities.</p><p>3) There should be an additional analysis to investigate the relationship between the hippocampus and V1. I understand the limitations of fMRI and of the current experimental design, but there are still possible analyses, even if they are indirect and the results non-definitive (for example, a correlation of the hippocampal and V1 effects across individuals, as in Hindy et al., 2016, Nat Neurosci).</p><p>4) The goal of the tuning analysis and the interpretation of its result should be clarified.</p><p>5) It should be clarified whether the screen during the ITI is the same as during the omitted items of the partial sequence trials. If this is the case, the potential implications should be discussed.</p><p>6) It is unclear from the methods how the tuning analysis was performed exactly. It is a bit circular to define voxels sensitive to a given dot location based on the localizer data and then evaluate on that same data which dot representations were activated on a given trial. Was there some form of cross-validation performed? I could not find it in the code. Even if this was done correctly without double dipping, it seems strange conceptually to use the localizer data for both the fitting and testing purposes here because implicitly, the authors would both assume that the localizer data is independent of the learned associations (to determine the voxels sensitive to a given dot) and dependent on it (to assess temporal tuning). Relatedly, this somewhat applies to the other analyses too: since the localizer was performed after the main task, could it be that the authors did not select the right set, or the complete set, of voxels that are normally sensitive to a given dot location?</p><p>7) There seems to be a trend toward the last dot leading to a greater BOLD activity (Figure 3a). I'm wondering if this is because of the task, which is specific to the last dot. I don't think this explains the successor vs predecessor effect though, as you show in Figure 3c. However, this could explain the result of the current only statistical test performed in the &quot;Anticipated stimulus sequences in V1&quot; section. To formally exclude this possibility, the authors should test the difference in the activation of a given dot (B or C) when it is a successor vs when it is a predecessor.</p><p>8) The second important prediction of the SR model, in addition to the greater activation for successors than for predecessors, is the decreasing trend in activation for further successors. Although it is visible in the figures, it would be nice if this trend was also statistically tested and reported in the &quot;Anticipated stimulus sequences in V1&quot; section.</p><p>9) I don't find the time-resolved hippocampus analysis very convincing: couldn't this transient temporal profile be in response to the start of the trial rather than the missing dot (but see recommendation 5)? It would be best to perform the same analyses suggested above (recommendations 7 and 8) to really test whether the hippocampus exhibits the properties of the SR.</p><p>10) Continuing from above, concerning the time-resolved decoding: since trials are very short and ITI are jittered, it seems to me that the activity from previous trials could affect the results. Performing the decoding analysis on regression coefficients from a single-trial GLM analysis would help avoid this confound.</p><p>11) Could you show a similar figure as Figure 3c but in Figure 5 for the hippocampus? It would be helpful to see the activation related to each dot location (including the shown dot).</p><p>12) Background about predictions and predictive effects in V1 should be added to the introduction, this is currently lacking.</p><p>13) There is no mention of corrections for multiple comparisons in the paper. For example, are the tests for the significance of each item in Figure 3b corrected? This should be indicated at all relevant places in the manuscript and figure legends, along with whether the tests are one-tailed or two-tailed.</p><p>14) Concerning the model fitting analysis, I'm unsure whether the H0 model can be compared to the other two models using RMSE, since it seems to have fewer parameters. A criterion like BIC or AIC should be used in this case.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I had two thoughts, but I leave it to the authors to decide how to address these.</p><p>1. While I agree with the authors that this is the first evidence for SR in visual sequences (to the best of my knowledge), there is another set of studies that comes to mind looking at hippocampal contributions to sequence and duration coding of perceptual sequences, which the authors may wish to discuss:</p><p>Thavabalasingam, S., O'Neil, E. B., Tay, J., Nestor, A., and Lee, A. C. (2019). Evidence for the incorporation of temporal duration information in human hippocampal long-term memory sequence representations. Proceedings of the National Academy of Sciences, 116(13), 6407-6414.</p><p>Thavabalasingam, S., O'Neil, E. B., and Lee, A. C. (2018). Multivoxel pattern similarity suggests the integration of temporal duration in hippocampal event sequence representations. NeuroImage, 178, 136-146.</p><p>2. In the model fitting procedure, what exactly does it mean that the discount parameter Î³ was a free parameter (p. 18)? It would be helpful to provide a bit more clarity on this, but it's also potentially theoretically interesting in light of evidence that different neural structures represent information in line with different values of Î³.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>1. SR versus other predictive sequence models: It remains unclear to me whether the predictive activity observed in V1 is best explained by an SR model or by other models that capture predictive sequences (of which there are many). To assess whether the data is best explained by an SR model, it seems necessary to check whether two adjacent states that predict divergent future states have dissimilar representations, while two states that predict similar future states have similar representations. The data presented here is unfortunately not designed to test this comparison. Can the authors nevertheless distinguish between an SR model (e.g. Figure 4A) and a 'flat prediction' model where each stimulus predicts all possible successor states equally without any temporal discounting (i.e. A predicts B, C, and D with equal probability; B predicts C and D with equal probability but does not predict A; etc..)? It seems important to report this comparison and discuss how it may be difficult to distinguish between an SR model and a 'flat prediction' using the BOLD signal.</p><p>2. Related to point 1, it remains unclear to me why the authors consider this data to reflect an SR model, while in their previous data they characterise predictive sequences as reflecting preplay. Can the authors provide a clearer explanation for why this data is best described as an SR model rather than preplay, while Ekman et al., 2017 reflect preplay? Or do the authors consider these codes to be equivalent?</p><p>3. It is not clear to me how the ROIs are being used in Figure 3 and 4? If V1 activity reflects an SR, within a given ROI it should be possible to see evidence for backward skew in the representation of each location (consistent with Mehta et al., 2000), while at the population level there is a forward skew?</p><p>4. The authors seem to apply different models to data from different brain regions and to data from the task and localiser data. Why? For consistency and clarity would it be possible for the authors to apply the same set of models throughout, to both V1 and hippocampus, and to both task and localiser data? i.e. SR model, 'flat prediction' model, CO model, H0 model, spatial model, temporal model.</p><p>5. Related to point 4, in Figure 6 it seems that V1 data from the localiser scan does not support an SR model? This suggests that the task itself is driving the predictive sequence activity in Figures 3-4? This important difference in evidence for an SR-like code during the task and localiser scan should be emphasised and discussed.</p><p>6. How specific are these findings to V1 and hippocampus? If the authors use a searchlight analysis to look for multivariate patterns consistent with an SR model, do they not find that many brain regions show evidence for an SR representation?</p><p>7. In general, several of the reported analyses are not clearly explained. For example, how do the authors generate the reconstruction maps in Figure 2? Why was pRF mapping only performed in 7 subjects? Why were the data from the pRF maps not used to generate ROIs?</p><p>8. Statistics:</p><p>a) Can the authors clarify how they corrected for multiple comparisons when performing model comparisons?</p><p>b) The authors say they performed a one-sided t-test using data from Figure 5b. Can they clarify what they did here?</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Successor-like representation guides the prediction of future events in human visual cortex and hippocampus&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Chris Baker (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there is one remaining issue that needs to be addressed, as outlined by Reviewer #1. Specifically, we thought it would be helpful to provide a bit more detail on the differences between the predictions of an SR versus model-based algorithm:</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The authors have considerably revised their paper and they have addressed most of my comments satisfactorily. However, I remain uncertain about point 1.1.</p><p>I understand that there are no rewards in your task and that the SR algorithm can apply in the absence of rewards. I am not sure however that a model-based (MB) algorithm would make different predictions than SR in the context of your experiment. Indeed, it can be difficult to distinguish SR and MB in many contexts, especially if there is no reevaluation of the transition matrix during the experiment (Momennejad et al., 2017, Nat Hum Behav). Could the authors perhaps test what the predictions of a MB algorithm would be in their experiment (see, e.g., the equation reported in the Methods of the Momennejad paper), or otherwise explain why this would be irrelevant?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The authors have done a thorough job of addressing my comments. I don't have any further suggestions.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78904.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Reviewer #1 (Recommendations for the authors):</p><p>(1.1) If SR is the best name for the discussed model, it should be clarified why this is the case and, importantly, any difference with the SR as defined in the RL literature should be discussed. Otherwise, another term might be more appropriate.</p></disp-quote><p>We thank the reviewer for giving us the opportunity to clarify this aspect. The reviewer states that â<italic>the SR has previously been used only in a RL context, where there are rewards associated with specific states and where predictions are task-relevant.â.</italic> We believe that this might reflect a misunderstanding of the SR model and have revised our manuscript to point out more clearly that SR, in contrast to model-free RL algorithms, is in fact not reward dependent.</p><p>The SR learning algorithm is based on temporal-difference-learning, but instead of learning future rewards it learns discounted expected future state occupancies. This enables learning in an environment without reward and results in a representation that encodes each state in relation to its successor states, i.e. those states that are expected to be visited in the future. This aspect is now clarified in the Introduction and Discussion of our manuscript.</p><p>Introduction:</p><p><italic>â</italic>In the context of hippocampal representations, the successor representation (SR) has been recently proposed (Dayan, 1993; Stachenfeld et al., 2017) to combine the trade-off between both flexible and efficient model properties. The SR postulates a predictive representation in which the current state is represented in terms of its future (successor) states, in a temporally discounted fashion. The SR is dependent on the actual experience, with states experienced more frequently being represented more strongly. This enables learning in an environment without explicit reward (GlÃ¤scher et al., 2010).2</p><p>Our discussion now also includes the following paragraph highlighting the passive nature of our task and the absence of any reward:</p><p>Discussion:</p><p>âOne aspect that sets our study apart is that the viewing of the visual sequence does not require any predictive planning of the participant to evaluate different future outcomes. In contrast, related studies reporting neuronal evidence for SR-like representations in hippocampus and PFC (Barron et al., 2020; Brunec and Momennejad, 2022) and occipital cortex (Schwartenbeck et al., 2021) have used paradigms in which participants were actively engaged in prospective planning and choice evaluation. Given the relatively passive nature of our task, one might therefore wonder whether it is expected to find any map-like activity at all. However, in this context it is important to stress that the SR, unlike other model-based algorithms, does not depend on choice-dependent reward to build its transitional task structure (Momennejad et al., 2017; Stachenfeld et al., 2017) and therefore might not depend on participantsâ active engagement. Furthermore, Russek et al. (2021) have recently used a paradigm in which subjects were passively exposed to transitions between visual states and reported evidence for SR-like representations in the absence of active choices in line with the results of the present study. Further supporting this notion, we have previously shown that anticipatory sequence activity occurred even after subjectsâ attention was diverted from the sequence to a demanding task at fixation (Ekman et al., 2017), rendering the sequence task irrelevant. Taken together, these observations indicate that SR-like representations are not limited to situations that require active planning, or multiple-choice evaluations but may rather be formed automatically and incidentally, as has been shown repeatedly in the domain of statistical learning (Fiser and Aslin, 2002; Turk-Browne et al., 2005).â</p><disp-quote content-type="editor-comment"><p>(1.2) It would be interesting to discuss in the discussion the distinction between the SR model and more complex models that might fit human behaviors and representations just as good or better. For example, with the current design, the SR model can't be disentangled from a more complex model in which all one-step transitions are stored and perhaps in which predictions are iteratively updated based on additional evidence (appearing items). A design in which each state is associated with multiple possible states (with different probabilities) might allow disentangling such additional possibilities.</p></disp-quote><p>We agree with the reviewer that a different experimental design would be required to dissociate SR from other, more complex models. While there are a series of model-based algorithms that indeed store the entire transitional structure (and reward information) that can be iteratively updated, we are not aware of any such model that would align with the predictions of the SR (temporal discounting and directionality).</p><p>We have revised the manuscript Discussion to include this discussion point:</p><p><italic>â</italic>For future designs it would be interesting to include visual sequences where individual states have multiple possible successor states with different probabilities associated with them. Such a design would further allow to dissociate the SR representation from alternative models that simply store all one-step transitions and their respective probabilities.â</p><disp-quote content-type="editor-comment"><p>(1.3) The second important prediction of the SR model, in addition to the greater activation for successors than for predecessors, is the decreasing trend in activation for further successors. Although it is visible in the figures, it would be nice if this trend was also statistically tested and reported in the &quot;Anticipated stimulus sequences in V1&quot; section.</p></disp-quote><p>We added a statistical test to quantify the activity decay visible in Figure 3 and revised the manuscript as follows:</p><p><italic>â</italic>The activity decay toward distant future locations was formally tested by fitting an exponentially decaying factor Î³ Î³ â [0,1] to each participantâs data. Here, values closer to 0 indicate a steeper decay and values closer to 1 indicate no decay. In line with our predictions, we found a group averaged decaying factor of Î³ = 0.14 (+/- 0.03 s.e.m.) that was statistically significantly different from 1 (non-parametric t-test t(34) = -17.17, p=2.54 Ã 10<sup>-18</sup>).â</p><disp-quote content-type="editor-comment"><p>(1.4) There seems to be a trend toward the last dot leading to a greater BOLD activity (Figure 3a). I'm wondering if this is because of the task, which is specific to the last dot. I don't think this explains the successor vs predecessor effect though, as you show in Figure 3c. However, this could explain the result of the current only statistical test performed in the &quot;Anticipated stimulus sequences in V1&quot; section. To formally exclude this possibility, the authors should test the difference in the activation of a given dot (B or C) when it is a successor vs when it is a predecessor.</p></disp-quote><p>Following the reviewerâs suggestion, we compared V1 BOLD activity across predecessor vs successor states for individual dot locations B and C. The statistical results of this control analysis replicate our main results, showing larger activity at location B during trials when B is a successor state (i.e., when dot A is presented) compared to when it is a predecessor state (i.e., when dot C is presented): t(34) = 5.72, p = 2.02 Ã 10<sup>-6</sup>. The same pattern of results was observed for location C (t(34) = 3.13, p = 0.0035), providing an internal conceptual replication. These new results show that the reported effects are also present went comparing individual dots and therefore exclude the possibility that the statistical comparison was driven by an increase in BOLD across locations.</p><p>We revised the manuscript to include the additional analysis:</p><p><italic>â</italic>In line with our predictions, V1 BOLD activity was indeed enhanced at the non-stimulated successor locations compared to the non-stimulated predecessor locations (averaged across all partial trials and sequence locations; t(34) = 6.45, p = 2.23 Ã 10<sup>-7</sup>). The same pattern of future directed prediction was also evident from the visual inspection of BOLD activity for all partial sequence trials separately (Figure 3C).</p><p>Further, these results of greater activity for successor compared to predecessor activity also holds when comparing individual sequence locations without averaging (i.e., comparing non-stimulated location B when successor vs predecessor, t(34) = 5.72, p = 2.02 Ã 10<sup>-6</sup>; and location C when successor vs predecessor, t(34) = 3.13, p = 0.0035).â</p><disp-quote content-type="editor-comment"><p>(1.5) I don't find the time-resolved hippocampus analysis very convincing: couldn't this transient temporal profile be in response to the start of the trial rather than the missing dot (but see recommendation 5)? It would be best to perform the same analyses suggested above (recommendations 7 and 8) to really test whether the hippocampus exhibits the properties of the SR.</p></disp-quote><p>We thank the reviewer for bringing up this point. It appears that we had not properly explained what the transient profile reflects. Concretely, if the decoded information were only in response to the start of the trial, as the reviewer suggests, the time-resolved decoding profile would be completely flat.</p><p>We rephrased the result section to emphasize that the time-resolved hippocampus analysis reflects the <italic>decoding</italic> time-course (successor vs predecessor states) and therefore differs in its interpretation from a BOLD response. In the latter case, the transient profile could indeed reflect a bottom-up response to the starting dot, as the reviewer pointed out. However, in the case of decoding the transient profile shows that individual trials represent evidence specifically for dot locations associated with the successor states.</p><p><italic>â</italic>Results of the evidence difference time-course clearly show a transient response peaking approximately 4.7 s post stimulus onset (Figure 5F) indicating that hippocampal predictions were triggered by the partial sequence dot. Note, that the decoding time-course reflects the evidence for successor locations vs predecessor locations independent of the bottom-up stimulus. The transient decoding profile can therefore not simply reflect the onset of a given trial.â</p><disp-quote content-type="editor-comment"><p>(1.6) Continuing from above, concerning the time-resolved decoding: since trials are very short and ITI are jittered, it seems to me that the activity from previous trials could affect the results. Performing the decoding analysis on regression coefficients from a single-trial GLM analysis would help avoid this confound.</p></disp-quote><p>We thank the reviewer for this suggestion. We would like to point out that the order of our trial sequences was counterbalanced to prevent <italic>systematic</italic> influences from previous trials to the current trial. We therefore believe that the single-trial GLM analysis is not strictly required in this case.</p><p>We added the motivation for counterbalancing the trial order in the revised manuscript:</p><p><italic>â</italic>The pseudo-randomization (perfect counterbalancing was numerically not possible with the set number of trial types and repetitions), rules out the possibility of systematic order effects.â</p><disp-quote content-type="editor-comment"><p>2) There should be an additional analysis to investigate the relationship between the hippocampus and V1. I understand the limitations of fMRI and of the current experimental design, but there are still possible analyses, even if they are indirect and the results non-definitive (for example, a correlation of the hippocampal and V1 effects across individuals, as in Hindy et al., 2016, Nat Neurosci).</p></disp-quote><p>We appreciate this suggestion. To clarify, we had previously refrained from a V1-Hippocampus correlation analysis, and discussed why we believe that the results would not be very meaningful with the current design:</p><p><italic>â</italic>Our study was not designed to address the question to what extent V1 and hippocampus representations are independent of each other. Here, we purposefully refrained from reporting correlations between the two regions as we could not exclude that an apparent coordination might be driven by other factors like attentional fluctuations. Future experiments, using more than one stimulus sequence could potentially address this question by comparing evidence of sequence specific representations in both areas. (p. 13)â</p><p>However, to empirically address the reviewerâs question, we correlated the averaged BOLD activity across all successor locations in V1 with the average classifier evidence across successor locations in hippocampus, across participants. No significant relationship was observed (spearman correlation, r = -0.08, p = 0.668). While there could be several reasons for this lack of relationship, we believe that a lack of power precludes us from drawing strong conclusions from this null finding. Nevertheless, for completeness, we now revised the manuscript to include this new analysis:</p><p><italic>â</italic>In order to probe the relationship between hippocampus and V1 successor reactivations, we performed an across subject analysis, correlating V1 BOLD activity, averaged across all successor locations, with hippocampus classifier evidence, averaged across all successor locations. No significant relationship was observed (spearman correlation, r = -0.08, p = 0.668).â</p><p>We rephrased the discussion as follows:</p><p><italic>â</italic>Our study showed no functional relationship between sequence prediction in V1 and hippocampus. However, our experimental paradigm was not primarily designed to address this question, as it does not exclude the possibility that an apparent coordination might be driven by other factors like attentional fluctuations across participants. Further, V1-hippocampus coordination might exist on a trial-by-trial level, which does not necessarily transfer to statistical comparisons across participants. Future experiments, using more than one stimulus sequence could potentially address this question by comparing evidence of sequence specific representations in both areas within participants.â</p><disp-quote content-type="editor-comment"><p>(3.1) The goal of the tuning analysis and the interpretation of its result should be clarified.</p></disp-quote><p>We thank the reviewer for giving us the opportunity to clarify the localizer analysis and interpretation.</p><p>We realized that the term âtuningâ might have been misunderstood to imply that we were quantifying the neural coding properties of a cortical region. However, this is not the case, instead we had intended to use the term âtuningâ to refer to a learned association, as in âafter exposure hippocampus representations become tuned to a certain stimulusâ.</p><p>We have now changed most occurrences of the terms âspatial and temporal tuningâ and replaced it with âspatial and temporal coactivation patternâ to avoid this confusion. Whenever we use the term âtuningâ, we made clear that we are talking about it in the context of learned coactivation pattern. Further, we have put bigger emphasis on the fact that we are investigating and interpreting the localizer coactivation pattern as learned associations that might persists from the main task.</p><p>Here we highlight some of these changes from the Results section:</p><p><italic>â</italic>Given that we successfully trained a classifier based on eight spatial locations it might seem obvious to conclude that the underlying code for these representations is purely spatial (retinotopic) as well. This is however not necessarily the case, given that the localizer was shown after the main task and might therefore reflect persistent predictive representations. Instead, robust discrimination of sequence locations could theoretically also be based on coding of temporal properties of the sequence. Indeed, Deuker et al. (2016) have recently shown that hippocampus representations can reflect in principle both spatial and temporal aspects. In our case, a temporal coding mechanism could represent stimulus locations not based on proximity in space, but rather by proximity in time.</p><p>In order to address this question, we conducted a detailed analysis of the coactivation pattern in the stimulus localizer (Figure 6A). Note that the localizer was shown at the end of the study, allowing us to test whether learned associations persisted even after the full sequence was not relevant anymore. Here, coactivations were defined as activation of non-stimulated locations. For instance, when presenting stimulus A, locations B-C-D might become activated as well. In general, such coactivations are often attributed to noise or ambivalent responses driven by overlapping receptive fields. However, in this case we made use of the coactivation pattern to draw inferences about the learned persistent representations.â</p><p>Regarding the absence of blank screens in the localizer. The localizer did in fact have so called null-events (blank screens) where only the fixation cross was shown. Further, the presentation order of dot locations in the localizer was counterbalanced, avoiding any systematic influence of previous trials on the current trial. To answer the reviewerâs question, we donât see how the blank screens could contribute to the pattern observed in hippocampus. Arguably, if there were any issues with the blank screen causing a certain pattern of activity, that should be visible in both V1 and hippocampus. However, the temporal coactivation pattern we describe was only observed in hippocampus, but not in V1, rendering this possibility unlikely.</p><disp-quote content-type="editor-comment"><p>(3.2) It is unclear from the methods how the tuning analysis was performed exactly. It is a bit circular to define voxels sensitive to a given dot location based on the localizer data and then evaluate on that same data which dot representations were activated on a given trial. Was there some form of cross-validation performed? I could not find it in the code. Even if this was done correctly without double dipping, it seems strange conceptually to use the localizer data for both the fitting and testing purposes here because implicitly, the authors would both assume that the localizer data is independent of the learned associations (to determine the voxels sensitive to a given dot) and dependent on it (to assess temporal tuning).</p></disp-quote><p>The reviewer is correct that we used leave-one-out cross-validation for the localizer analysis to prevent double dipping.</p><p>This is described in the method section:</p><p><italic>â</italic>Before applying the trained classifier to the main task, we confirmed that the classifier was indeed able to distinguish between stimulus locations within the localizer. To this end, we performed a leave-one-out cross validation and tested the decoding accuracy against chance level (1/8 = 12.5 %) across subjects using a one-sample t-test. In addition to a binary classifier output for each class, we also looked at the probabilistic output. For each sample in the localizer test set, we obtained 8 probability values, one for each class. We refer to the classifier probability as classifier evidence, as the probability reflects the evidence that a particular class is represented. For each participant probability values were averaged across trials to obtain location specific response profiles.â</p><p>We apologise that the analysis code was not sufficiently documented. We previously provided analysis code that recreates the article Figures from pre-processed, intermediate data specific to each figure, and code that creates the intermediate data from the raw data. The cross-validation analysis was not included in the scripts associated with the figures. We have now improved the documentation of our analysis scripts and separated the code for the article figures from the code that processes the raw data, which makes the distinction more obvious.</p><p>Considering the aspect of âconceptual strangenessâ, in the localizer, we are simply assessing the structure of persistent activity pattern after the main task. Our results show that no such structure is present in V1, which basically rules out any concerns. The V1 results are also confirmed using independent pRF data (see detailed response below).</p><p>The hippocampus does show a co-activation pattern, whereby not only the presented stimulus, but also other stimuli were represented, albeit to a lesser degree. Importantly, in contrast to V1, the hippocampus analysis is based on a classification analysis, that <italic>does not</italic> rely on a two-step process where relevant voxels are first identified, and then characterized based on their BOLD activity. Instead, the classifier takes all hippocampus voxels and outputs a probability for each possible stimulus location based on the multivariate structure.</p><p>Since the presented localizer stimulus is also the one correctly identified as most likely by the classifier (despite evidence for other stimuli), we can use the classifier to identify the presented and reactivated stimuli in the main task.</p><disp-quote content-type="editor-comment"><p>Relatedly, this somewhat applies to the other analyses too: since the localizer was performed after the main task, could it be that the authors did not select the right set, or the complete set, of voxels that are normally sensitive to a given dot location?</p></disp-quote><p>We thank the reviewer for bringing up the issue of correct voxel selection.</p><p>In V1, the voxel for each location were selected by (1) contrasting the BOLD activity at one location with all other locations and then (2) selecting the 25 most active voxels (highest z-value) for that location. For the main results shown in Figure 3, this contrast approach ensures that we select only voxel specific to one dot location. Even in the case of co-activation, or activity spread to neighboring locations, the contrast approach will ensure that only voxel specific to the stimulated location were selected (assuming that the region receiving the bottom-up stimulus input will always elicit the strongest BOLD response).</p><p>The location selectivity can also be empirically seen in Figure 2a (right), where we plot the BOLD activity of the selected voxel, projected into stimulus space, using independently acquired receptive-field data. Figure 2a (right) shows that the selected voxels were indeed at the expected stimulus location, and not at other receptive field locations in the visual field.</p><p>We have now stressed more clearly that the pRF data in Figure 2 validate that we selected the right set of voxels for a given dot location:</p><p><italic>â</italic>Stimulus response profiles of these eight (retinotopic) ROIs show little coactivation of neighboring locations in the visual field which allows for a precise investigation of location specific activity (Figure 2A). Unsurprisingly, during full sequence trials BOLD activity at the sequence locations receiving bottom-up visual input was markedly enhanced compared to non-stimulated control locations (Figure 2B). Population-based receptive field (pRF) data, that was acquired for a subset of participants confirmed that the selected voxels correspond to the retinotopic stimulus locations as expected.â</p><p>For the additional results shown in Figure 6, we show data from a <italic>baseline contrast</italic> (opposed to the direct contrast employed for the main results).</p><disp-quote content-type="editor-comment"><p>4) It should be clarified whether the screen during the ITI is the same as during the omitted items of the partial sequence trials. If this is the case, the potential implications should be discussed.</p></disp-quote><p>The reviewer is correct that there is no visual difference between the inter-trial interval (ITI) and the part of the sequence where no dot is shown. By design, the variable ITI prevents the subject from learning any temporal structures related to the start of a trial. In doing so, we can focus on the predictive process that is triggered by the presentation of a sequence dot, independent of any temporal expectation effects.</p><p>In the revised manuscript, we have now clarified that the variable ITI looks visually identical to the omission of sequence dots and therefore ensures that we are not confounding the predictive effects of interest with any temporal expectation effects.</p><p>âThe variable ITI ensured that the experimental paradigm had no temporal structure that participants could learn to expect the onset of a trial. This allowed us to focus in the present study on the learning and representation of structural knowledge, independent of any temporal expectation effects.</p><p>To probe activity replay we introduced partial sequence trials where only one of the four dots was shown for 100 ms, instead of the full sequence. Visually, there was no difference between the ITI and the part of the partial sequence trials where the dots were omitted, both showed a fixation cross at the center of the screen.â</p><disp-quote content-type="editor-comment"><p>5) Could you show a similar figure as Figure 3c but in Figure 5 for the hippocampus? It would be helpful to see the activation related to each dot location (including the shown dot).</p></disp-quote><p>Given the significant, but very low classification accuracy in within the localizer (accuracy = 15% 3.6%, mean Â± s.d.; p = 0.002), we had previously decided to only report averaged location results for the hippocampus as the non-averaged predictions would be very noisy. To put the hippocampus classification accuracy into context, in V1 cross-validated accuracy within the localizer was (92% Â± 12%, mean Â± s.d.).</p><p>We now stressed this difference between V1 and hippocampus decoding in the Results section and motivate our reason for presenting averaged results:</p><p><italic>â</italic>Within localizer decoding accuracy results confirmed that hippocampus has a coarse representation of the eight stimulus locations (Figure 5B) within the localizer (one-sample t-test; t(34) = 3.28, p = 0.002; cross-validated accuracy = 15% Â± 3.6%, mean Â± s.d.; see <italic>Materials and methods</italic>). Notably, compared to V1 (cf. Figure 2A), within localizer accuracy was relatively low and as a consequence tuning curves in hippocampus appeared less sharp (Figure 5C). In order to maximize sensitivity for the hippocampus, we averaged classification evidence across successor and predecessor locations. Non-averaged results can be found in Supplementary Figure 1A.â</p><p>Further, we followed the reviewerâs suggestion and added a new supplementary Figure including the non-averaged results for hippocampus. The new Figure also includes the model comparison the reviewers had asked for.</p><disp-quote content-type="editor-comment"><p>6) Background about predictions and predictive effects in V1 should be added to the introduction, this is currently lacking.</p></disp-quote><p>We rephrased the introduction to focus more on predictive effects in V1:</p><p><italic>â</italic>Previous research has repeatedly shown that prior expectations influence neural activity in the visual cortex (Ekman et al., 2017; Gavornik and Bear, 2014; Hindy et al., 2016; Kok et al., 2012; Xu et al., 2012). It remains, however, unknown if SR-like representations are present outside the hippocampus in areas like the early visual cortex (V1) that have a strong retinotopic organization. Theoretically it is possible that V1 receptive fields, analogous to hippocampal place fields, become tuned to respond not only to the current input, but also to expected future inputs. Here we propose that the computationally efficient and flexible properties of the SR could in theory also underlie the anticipation of future events in V1.â</p><disp-quote content-type="editor-comment"><p>7) There is no mention of corrections for multiple comparisons in the paper. For example, are the tests for the significance of each item in Figure 3b corrected? This should be indicated at all relevant places in the manuscript and figure legends, along with whether the tests are one-tailed or two-tailed.</p></disp-quote><p>We thank the reviewer for pointing this out. We added the information to the legend of Figure 3, Figure 4 and Figure 6:</p><p><italic>â</italic>Error bars denote Â± s.e.m.; two-tailed t-test, ***P&lt;0.001; **P&lt;0.01; *P&lt;0.05 uncorrected for multiple comparisons.â</p><disp-quote content-type="editor-comment"><p>8) Concerning the model fitting analysis, I'm unsure whether the H0 model can be compared to the other two models using RMSE, since it seems to have fewer parameters. A criterion like BIC or AIC should be used in this case.</p></disp-quote><p>We implemented this suggestion and calculated BIC, instead of RMSE for every subject, thereby controlling for the difference in model parameters. The results remain unchanged compared to the previous version of the manuscript. In short, the SR model has the smallest BIC value (smaller = more likely), followed by the CO model and the H0 model.</p><p>We updated Figure 4c, Figure 6f and the related Results and Method section.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I had two thoughts, but I leave it to the authors to decide how to address these.</p><p>1. While I agree with the authors that this is the first evidence for SR in visual sequences (to the best of my knowledge), there is another set of studies that comes to mind looking at hippocampal contributions to sequence and duration coding of perceptual sequences, which the authors may wish to discuss:</p><p>Thavabalasingam, S., O'Neil, E. B., Tay, J., Nestor, A., and Lee, A. C. (2019). Evidence for the incorporation of temporal duration information in human hippocampal long-term memory sequence representations. Proceedings of the National Academy of Sciences, 116(13), 6407-6414.</p><p>Thavabalasingam, S., O'Neil, E. B., and Lee, A. C. (2018). Multivoxel pattern similarity suggests the integration of temporal duration in hippocampal event sequence representations. NeuroImage, 178, 136-146.</p></disp-quote><p>We thank the reviewer for pointing out these articles. We have now included both references in the revised manuscript.</p><p><italic>â</italic>Hippocampus on the other hand represented relevant items predominantly in terms of their temporal distance within the sequence, suggesting that representations capitulate on the transitionally structure of the visual sequence. These results align with previous reports that hippocampus can learn to represent temporal sequence structure (Thavabalasingam et al., 2018, 2019) and temporal proximity in a spatial navigation task (Deuker et al., 2016; Howard et al., 2014), but to the best of our knowledge, constitute the first reports of coding temporal distance of a visual sequence.â</p><disp-quote content-type="editor-comment"><p>2. In the model fitting procedure, what exactly does it mean that the discount parameter Î³ was a free parameter (p. 18)? It would be helpful to provide a bit more clarity on this, but it's also potentially theoretically interesting in light of evidence that different neural structures represent information in line with different values of Î³.</p></disp-quote><p>Keeping Î³ as a âfree parameterâ was meant to convey, that instead of using a fixed value for Î³ (e.g., based on previous literature) and fitting the curve to all participants, the value of Î³ was determined during data fitting for each participant individually. We rephrased this formulation to make that clearer.</p><p><italic>â</italic>During model fitting Î³ was a free parameter, meaning that instead of using a fixed value, individual Î³ values were determined for each participant. Here, larger values of Î³ result in a smaller exponential decay of future states.â</p><p>We also report group statistics of obtained Î³ values:</p><p><italic>â</italic>The activity decay toward distant future locations was formally tested by fitting an exponentially decaying factor Î³ Î³ â [0,1] to each participantâs data. Here, values closer to 0 indicate a steeper decay and values closer to 1 indicate no decay. In line with our predictions, we found a group averaged decaying factor of Î³ = 0.14 (+/- 0.03 s.e.m.) that was statistically significantly different from 1 (non-parametric t-test t(34) = -17.17, p=2.54 Ã 10<sup>-18</sup>).â</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>1. SR versus other predictive sequence models: It remains unclear to me whether the predictive activity observed in V1 is best explained by an SR model or by other models that capture predictive sequences (of which there are many). To assess whether the data is best explained by an SR model, it seems necessary to check whether two adjacent states that predict divergent future states have dissimilar representations, while two states that predict similar future states have similar representations. The data presented here is unfortunately not designed to test this comparison. Can the authors nevertheless distinguish between an SR model (e.g. Figure 4A) and a 'flat prediction' model where each stimulus predicts all possible successor states equally without any temporal discounting (i.e. A predicts B, C, and D with equal probability; B predicts C and D with equal probability but does not predict A; etc.)? It seems important to report this comparison and discuss how it may be difficult to distinguish between an SR model and a 'flat prediction' using the BOLD signal.</p></disp-quote><p>The reviewer points out that there are many other possible predictive activity patterns that could be expected. We are actually not aware of any existing (biological) model that would generate predictions, selectively for successor states and not for predecessor states.</p><p>While we agree with the reviewerâs point that there are many possible predictive activity <italic>patterns</italic>, like âflat predictionâ, âlinear decrease predictionâ, âlinear increase predictionsâ, to the best of our knowledge, none of these predictions can be derived from existing <italic>models</italic>. Thatâs why we had previously only included one alternative model, the co-occurrence model which is based on a biological framework in which autoassociative connections within the hippocampal CA3 regions reactivate related sequence items from partial input, without skewing toward future locations. To the best of our knowledge, the SR model is the only model that predicts an asymmetry toward future locations.</p><p>While we like to keep the focus of our manuscript on the two existing, biologically motivated models, we have calculated the suggested âflat predictionâ model for the revision letter.</p><p><italic>â</italic>Comparing the model fit (BIC) of the suggested âflat predictionâ pattern with the SR model showed that the SR model describes the data significantly better (two-sided t-test, t(34) = 6.12, p = 5.98 x 10<sup>-7</sup>).â</p><disp-quote content-type="editor-comment"><p>2. Related to point 1, it remains unclear to me why the authors consider this data to reflect an SR model, while in their previous data they characterise predictive sequences as reflecting preplay. Can the authors provide a clearer explanation for why this data is best described as an SR model rather than preplay, while Ekman et al., 2017 reflect preplay? Or do the authors consider these codes to be equivalent?</p></disp-quote><p>Previous to the present study we didnât know whether the observed preplay/replay traces were guided by a generative model that represents the relational structure of the environment. Our previous paradigm in Ekman et al. 2017 was not designed to address this question, as the dot sequence (i) had no intermediate omissions and (ii) the dot locations have different eccentricities from fixation which hinders the interpretation of the absolute BOLD values.</p><p>The difference with our previous study is discussed as follows:</p><p><italic>â</italic>There is an extensive body of literature that shows how expectations elicit anticipatory activity in early visual cortices (de Lange et al., 2018; Hindy et al., 2016; Kok et al., 2012). For instance, we have previously shown that flashing an individual dot of a simple, linear sequence triggers an activity wave in V1 that resembles the full stimulus sequence (Ekman et al., 2017, 2022), akin to replay of place field activity during spatial navigation (Foster and Wilson, 2006; Gupta et al., 2010). However, what remains unknown is whether these sensory replay traces are guided by a generative model that represents the relational structure of the stimulus sequence, akin to a predictive map. Alternatively, anticipatory activity traces could simply reflect the association between different stimuli, based on their co-occurrence, without the added complexity of any temporal relational structure. The latter explanation appears plausible, given that predictive representations in early visual cortex are generally time critical and operate in parallel to a constant stream of new sensory input, which arguably requires efficient processing and in turn limits the complexity of such representations.</p><p>In fact, we previously speculated that cue-triggered reactivation of simple sequences might be driven by an automatic pattern completion-like mechanism that reactivates all associated items based on partial input (Ekman et al., 2017). This idea is in line with the finding that predictive representations in V1 correlated with pattern completion-like activity in the hippocampus (Hindy et al., 2016; Kok and Turk-Browne, 2018) that might be driving V1 activity (Finnie et al., 2021; Ji and Wilson, 2007).</p><p>Our current findings directly challenge this interpretation and instead point to a predictive representation of expected, temporally discounted, future states. We accomplished this by using a paradigm in which one visual event (e.g., the presentation of one dot) was framed as one state in a directed transition matrix with a fixed relational structure. The SR hypothesis makes two testable predictions, namely that population activity represents future states over predecessor states, and that future state representations are temporally discounted, such that events in the close future are more prominently represented compared to events in the distant future. Using a paradigm in which we occasionally presented only single items of the full sequence, allowed us to investigate V1 activity at omitted sequence locations.â</p><disp-quote content-type="editor-comment"><p>3. It is not clear to me how the ROIs are being used in Figure 3 and 4? If V1 activity reflects an SR, within a given ROI it should be possible to see evidence for backward skew in the representation of each location (consistent with Mehta et al., 2000), while at the population level there is a forward skew?</p></disp-quote><p>We believe that this is indeed what our data shows. For example, within the V1 ROI that is responsive to dot location B, there is elevated activity when dot A is shown. This could be interpreted as âbackward skewâ of this ROI: the ROI that is tuned to location B also starts responding to location A.</p><p>At the level of the entire V1 population, however, this results in âforward skewâ: when presenting dot location A, the population response is skewed forward, by virtue of the anticipatory activity in V1 neurons that are tuned to successor location B.</p><disp-quote content-type="editor-comment"><p>4. The authors seem to apply different models to data from different brain regions and to data from the task and localiser data. Why? For consistency and clarity would it be possible for the authors to apply the same set of models throughout, to both V1 and hippocampus, and to both task and localiser data? i.e. SR model, 'flat prediction' model, CO model, H0 model, spatial model, temporal model.</p></disp-quote><p>We appreciate the suggestion made by the reviewer to apply the same models to both V1 and hippocampus.</p><p>For the hippocampus, we had previously analysed averaged classifier outputs across locations. This was done to effectively improve the signal-to-noise ratio. However, averaging the output (i.e., all successor locations vs all predecessor locations), did not allow us to do any model fitting. In the revised version, we have now implemented six changes: (1) we added our motivation for collapsing the hippocampus data (2) we now show the non-averaged hippocampus results as a Supplementary Figure (3) we report the same model comparison for hippocampus that was done for V1, thereby keeping the model comparison consistent across regions (4) we now include the SR model in the model comparison for the localizer (5) we added our motivation for applying the spatial and temporal models to the localizer and not to the main task. (6) we renamed the no coactivation (NoCo) model from the localizer to H0 model, indicating more clearly that this is the same âbaselineâ model used in the main task. The different names (H0, NoCo) might have previously contributed to the impression that these are different models, despite being conceptually the same.</p><p>Below we copy our response to Reviewer #1 from above, who brought up a similar point.</p><p>Given the significant, but very low classification accuracy in within the localizer (accuracy = 15% 3.6%, mean Â± s.d.; p = 0.002), we had previously decided to only report averaged location results for the hippocampus as the non-averaged predictions would be very noisy. To put the hippocampus classification accuracy into context, in V1 cross-validated accuracy within the localizer was (92% Â± 12%, mean Â± s.d.).</p><p>We no stressed this difference between V1 and hippocampus decoding in the Results section and motivate our reason for presenting averaged results:</p><p><italic>â</italic>Within localizer decoding accuracy results confirmed that hippocampus has a coarse representation of the eight stimulus locations (Figure 5B) within the localizer (one-sample t-test; t(34) = 3.28, p = 0.002; cross-validated accuracy = 15% Â± 3.6%, mean Â± s.d.; see <italic>Materials and methods</italic>). Notably, compared to V1 (cf. Figure 2A), within localizer accuracy was relatively low and as a consequence tuning curves in hippocampus appeared less sharp (Figure 5C). In order to maximize sensitivity for the hippocampus, we averaged classification evidence across successor and predecessor locations. Non-averaged results can be found in Supplementary Figure 1A.â</p><p>Further, we followed the reviewerâs suggestion and added a new supplementary Figure including the non-averaged results for hippocampus. The new Figure also includes the model comparison the reviewers had asked for.</p><p>Additionally, we also followed the reviewerâs suggestion and included the SR model to the localizer analysis, confirming that the localizer is not best described by an SR coactivation pattern.</p><p>Finally, we explained why we donât apply the temporal and spatial coactivation models to the main task. Here we copy our reply to Reviewer #2 from above, who had a similar point:</p><p>The reviewer is correct that the fact that the sequence order and spatial distance were not fully decorrelated (second presentation was always farthest away from starting dot, third and fourth dot always the same distance from start) prevents us from quantifying the interaction of the SR and CO model with a spatial model during the main task.</p><p>We added the following to the Method section to clarify this:</p><p><italic>â</italic>Note that because within each dot sequence, temporal order and spatial distance were not perfectly decorrelated (e.g. the second sequence dot was always farthest apart from the starting dot), it is not possible to estimate the combined influence of the SR model and the spatial coactivation model on the observed BOLD activity.â</p><p>Having said that, we believe that there is little concern that the reported reactivations of the main task are driven by the Euclidean distance in a meaningful way for two reasons:</p><p>(1) Detailed analysis of the localizer data showed that there is no spatial spreading from one dot location to the other sequence locations (Figure 6). This is likely because the relevant dot locations were sufficiently spaced apart. Given the lack of spreading during the localizer, where the dot was flashed for 13.5s, makes the presence of spreading during the main task, where the dot was flashed for only 100ms, equally unlikely.</p><p>(2) The presence of spatial spreading would actually obfuscate the reported SR-like pattern and could not have caused it. Specifically, because the second sequence dot was always farthest apart from the start, this is where one would assume the least amount of activity spread (greatest Euclidean distance). Sequence dots three and four should be more active given that they are both closer to the starting point in terms of Euclidean distance. Our reported results are the opposite of that pattern, ruling out the possibility that these were caused by spatial spreading.</p><disp-quote content-type="editor-comment"><p>5. Related to point 4, in Figure 6 it seems that V1 data from the localiser scan does not support an SR model? This suggests that the task itself is driving the predictive sequence activity in Figures 3-4? This important difference in evidence for an SR-like code during the task and localiser scan should be emphasised and discussed.</p></disp-quote><p>The reviewer is correct that V1 data from the localizer does not show the persistent SR-like predictions from the main task. In the revised version, we now included a formal test for this (see response above).</p><p>We believe that this is to be expected as the sequence predictions are learned and updated based on exposure. Within the localizer, no more dot sequences are shown. Instead, individual dot locations are repeatedly flashed for 13.5 s at the same location. It is therefore expected that sequence predictions, related to the previous task, would eventually fade away. This can be understood in the context of continuous updating of the predictions once the regularities of the environment change and does not constitute any evidence against the SR model.</p><p>We have added the following to the Discussion:</p><p><italic>â</italic>Furthermore, hippocampus predictive codes were found to persist after the sequence task and coactivation of related sequence locations were still present during the stimulus localizer, potentially indicating that hippocampus representations reflect a more stable code operating on a longer timescale. V1 representations on the other hand did not persist throughout the stimulus localizer and reverted back to representing individual spatial locations without coactivation of related sequence locations, further highlighting another qualitative difference between V1 and hippocampus coding. According to the SR, it is expected that sequence predictions will change once the regularities of the environment change. The absence of SR-like pattern in V1 during the functional localizer is therefore not at odds with our results from the main task, but rather indicative of a dynamic updating of the generative model.â</p><disp-quote content-type="editor-comment"><p>6. How specific are these findings to V1 and hippocampus? If the authors use a searchlight analysis to look for multivariate patterns consistent with an SR model, do they not find that many brain regions show evidence for an SR representation?</p></disp-quote><p>We addressed the reviewerâs question about the specificity of the effects, by testing another low-level visual area V2. These new results show that in contrast to V1 and hippocampus, V2 does not feature any predictive effects and suggest that the reported findings are not ubiquitous throughout the brain.</p><p>This new result is mentioned in the revised manuscript:</p><p><italic>â</italic>One could ask whether our findings are specific to V1 and hippocampus, or widespread throughout the brain. In order to answer this question, we repeated the analysis for low-level visual area V2. In contrast to V1, no predictive effects were found in area V2. V2 BOLD activity was not enhanced at the non-stimulated successor locations compared to the non-stimulated predecessor locations (averaged across all partial trials and sequence locations; t(34) = 1.41, p = 0.168).â</p><p>We appreciate the searchlight suggestion to complement our ROI analysis approach. However, we believe that an additional ROI analysis in this case is more meaningful compared to a searchlight analysis. The reason for this is that the dot locations in our experiment are up to ~14 degrees apart in visual space. Within retinotopically organized visual areas, these stimulus locations are represented in different hemifields, multiple centimetres away.</p><p>The sphere of a searchlight with a commonly used radius of ~4mm would not be able to capture that effect, simply because the sphere would be too small to include all relevant voxel. One could argue that running a searchlight analysis with a radius of ~40mm (a magnitude larger) would alleviate that problem, but that would result in a complete loss of spatial specificity. For this reason, we refrained from using a searchlight analysis and present the additional V2 results instead.</p><disp-quote content-type="editor-comment"><p>7. In general, several of the reported analyses are not clearly explained. For example, how do the authors generate the reconstruction maps in Figure 2? Why was pRF mapping only performed in 7 subjects? Why were the data from the pRF maps not used to generate ROIs?</p></disp-quote><p>We thank the reviewer for pointing out these unclarities. In short, the pRF data were available for 7 subjects, that had also participated in a previous experiment. Since the goal of the pRF mapping was only to confirm that the localizer voxel-selection resulted in accurate results (Figure 2), we decided not to invite all subjects for a second pRF session that would have taken 45 minutes of fMRI scanning time.</p><p>The ROIs were generated from the functional localizer, because the pRF data was not available for all subjects. Please note that the voxel selection via the functional localizer was also the method we had preregistered for the data analysis (https://osf.io/f8dv9/), so this was always the intended analysis.</p><p>In the revised manuscript we now stress, that the pRF data was from a previous experiment:</p><p><italic>â</italic>pRF estimation<italic>.</italic> pRF data was available for seven participants from a previous study and was used to validate visually that the voxel selection based on the functional localizer selected voxel that correspond to the stimulated location is visual space (Figure 2).â</p><p>We further added the missing information how the pRF reconstruction in Figure 2 was performed:</p><p>âFor the pRF-based stimulus reconstruction, for each participant, we first limited the pRF data to voxel that were selected based on the functional localizer (25 voxel per stimulus location, 200 voxels in total). This selection step would allow us to visually inspect whether the voxel selection accurately selected voxel corresponding to the respective stimulus location. Second, every voxel is described as a 2D Gaussian with parameters x0, y0, and s0 from the pRF estimation. The 2D Gaussians for each voxel, represented by a pixel x pixel image, were scaled based on the percent signal change obtained from the functional localizer GLM, and consecutively summed over voxels to create one 2D representation of the reconstructed stimulus. This procedure was repeated separately for all 8 stimulus locations. Finally, for visualization purpose, the 8 individual localizer conditions were rotated to one stimulus location (22.5Â°) and averaged across stimulus locations and participants.â</p><disp-quote content-type="editor-comment"><p>8. Statistics:</p><p>a) Can the authors clarify how they corrected for multiple comparisons when performing model comparisons?</p></disp-quote><p>Thank you for pointing this out. No correction for multiple comparisons was carried out during the model comparison. This was now added to the legend of Figure 6.</p><p><italic>â</italic>Error bars denote Â± s.e.m.; two-tailed t-test, ***P&lt;0.001; **P&lt;0.01; *P&lt;0.05 uncorrected for multiple comparisons.â</p><disp-quote content-type="editor-comment"><p>b) The authors say they performed a one-sided t-test using data from Figure 5b. Can they clarify what they did here?</p></disp-quote><p>We apologize for the unclarity. We performed a <italic>two-sided</italic> one-sample t-test. We revised the paragraph in the main text to refer to the Methods section where the analysis is explained:</p><p>âWithin localizer decoding accuracy results confirmed that hippocampus has a coarse representation of the eight stimulus locations (Figure 5B) within the localizer (two-sided one-sample t-test; t(34) = 3.28, p = 0.002; cross-validated accuracy = 15% Â± 3.6%, mean Â± s.d.; see <italic>Materials and methods</italic>).â</p><p>The Methods section under âHippocampal decodingâ reads:</p><p>âA logistic regression classifier (default values, L2 regularization; C=1) was trained to distinguish between 8 stimulus locations during the independent localizer run. Before applying the trained classifier to the main task, we confirmed that the classifier was indeed able to distinguish between stimulus locations within the localizer. To this end, we performed a leave-one-out cross validation and tested the decoding accuracy against chance level (1/8 = 12.5 %) across subjects using a one-sample t-test.â</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there is one remaining issue that needs to be addressed, as outlined by Reviewer #1. Specifically, we thought it would be helpful to provide a bit more detail on the differences between the predictions of an SR versus model-based algorithm:</p><p>Reviewer #1 (Recommendations for the authors):</p><p>The authors have considerably revised their paper and they have addressed most of my comments satisfactorily. However, I remain uncertain about point 1.1.</p><p>I understand that there are no rewards in your task and that the SR algorithm can apply in the absence of rewards. I am not sure however that a model-based (MB) algorithm would make different predictions than SR in the context of your experiment. Indeed, it can be difficult to distinguish SR and MB in many contexts, especially if there is no reevaluation of the transition matrix during the experiment (Momennejad et al., 2017, Nat Hum Behav). Could the authors perhaps test what the predictions of a MB algorithm would be in their experiment (see, e.g., the equation reported in the Methods of the Momennejad paper), or otherwise explain why this would be irrelevant?</p></disp-quote><p>We thank the reviewer for raising this issue, which has prompted us to reflect more thoroughly on this issue.</p><p>The reviewer is correct that, within the context of our design, it is strictly speaking not possible to distinguish between model-based (MB) and SR algorithms. The key distinction between them is that SR caches a predictive map of states that the agent expects to visit in the future, whereas MB algorithms store a full model of the world and compute trajectories at the decision time. Both predict a temporally discounted activation of successor states.</p><p>It should be noted however that MB comes at a higher computational cost, and is more intensive both in terms of time and working memory resources. The activation of successor states that we observed occurred in the absence of a decision-making process (i.e., participants did not perform any task on the trials where a single dot was presented). Also, and interestingly, we previously observed that this activation pattern was not dependent on the task, and was equally present when attentional resources were strongly drawn away from the stimuli (Ekman et al. Nat Comm 2017). These observations may be more readily in line with the automatic (cached) activation of successor states that is embodied by SR, rather than the effortful iterative calculation of successor states that is the hallmark of MB. Nevertheless, we agree with the reviewer that our study does not provide strong evidence in favor of SR over MB computations in the visual cortex. We have now made this clearer in the Discussion section of our manuscript (see below).</p><p>Also, the question raised by the reviewer inspired a potential follow-up experiment, which is outside of the scope of the current manuscript, but which would be a potentially promising avenue of future research. The transition revaluation manipulation described earlier (Momennejad et al. 2017) could also be applied to our experimental setting. After exposing participants to our dot sequences (ABCD), one could introduce a relearning phase in which participants are exposed to BDC. When participants, after this relearning phase, are exposed to A, there are competing predictions about the activation pattern of successor states: H1 (SR): BCD</p><p>H2 (MB): BDC</p><p>We believe that this could be an interesting follow-up experiment, and have added it to the Discussion section of the manuscript.</p><p>We have added the following section to the Discussion section (page 13):</p><p><italic>â</italic>While we have interpreted the neural activity patterns in the light of the SR, it is strictly speaking not possible to distinguish between model-based (MB) and SR algorithms within the context of our design. The key distinction between them is that SR caches a predictive map of states that the agent expects to visit in the future, whereas MB algorithms store a full model of the world and compute trajectories at the decision time (Mommenejad et al. 2017, Gershman 2018). Therefore, both predict a temporally discounted activation of successor states. It should be noted however that MB comes at a higher computational cost, and is more intensive both in terms of time and working memory resources. The activation of successor states that we observed, on the other hand, occurred in the absence of a decision-making process (i.e., participants did not perform any task on the trials where a single dot was presented). Also, importantly, we previously observed that this activation pattern was not dependent on the task, and was equally present when attentional resources were strongly drawn away from the stimuli (Ekman et al., 2017). These observations may be more readily in line with the automatic (cached) activation of successor states that is embodied by SR, rather than the effortful iterative calculation of successor states that is the hallmark of MB. One future possibility to disentangle SR and MB algorithms could be to probe how well each model adapts to changes in the dot sequence structure. It has previously been shown, that compared to MB, the flexibility of the SR is somewhat limited to reflect changes in the transitional structure, because it requires the entire SR to be relearned (Momennejad et al., 2017).â</p></body></sub-article></article>