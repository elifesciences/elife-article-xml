<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">82895</article-id><article-id pub-id-type="doi">10.7554/eLife.82895</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Contrary neuronal recalibration in different multisensory cortical areas</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-293157"><name><surname>Zeng</surname><given-names>Fu</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0000-6857-6485</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-167435"><name><surname>Zaidel</surname><given-names>Adam</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4405-8717</contrib-id><email>adam.zaidel@biu.ac.il</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-126840"><name><surname>Chen</surname><given-names>Aihua</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5066-2844</contrib-id><email>ahchen@brain.ecnu.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02n96ep67</institution-id><institution>Key Laboratory of Brain Functional Genomics (Ministry of Education), East China Normal University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03kgsv495</institution-id><institution>Gonda Multidisciplinary Brain Research Center, Bar-Ilan University</institution></institution-wrap><addr-line><named-content content-type="city">Ramat Gan</named-content></addr-line><country>Israel</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Fetsch</surname><given-names>Christopher R</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Johns Hopkins University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>06</day><month>03</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e82895</elocation-id><history><date date-type="received" iso-8601-date="2022-08-22"><day>22</day><month>08</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-02-21"><day>21</day><month>02</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-09-27"><day>27</day><month>09</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.09.26.509476"/></event></pub-history><permissions><copyright-statement>© 2023, Zeng et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Zeng et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-82895-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-82895-figures-v1.pdf"/><abstract><p>The adult brain demonstrates remarkable multisensory plasticity by dynamically recalibrating itself based on information from multiple sensory sources. After a systematic visual–vestibular heading offset is experienced, the unisensory perceptual estimates for subsequently presented stimuli are shifted toward each other (in opposite directions) to reduce the conflict. The neural substrate of this recalibration is unknown. Here, we recorded single-neuron activity from the dorsal medial superior temporal (MSTd), parietoinsular vestibular cortex (PIVC), and ventral intraparietal (VIP) areas in three male rhesus macaques during this visual–vestibular recalibration. Both visual and vestibular neuronal tuning curves in MSTd shifted – each according to their respective cues’ perceptual shifts. Tuning of vestibular neurons in PIVC also shifted in the same direction as vestibular perceptual shifts (cells were not robustly tuned to the visual stimuli). By contrast, VIP neurons demonstrated a unique phenomenon: both vestibular and visual tuning shifted in accordance with vestibular perceptual shifts. Such that, visual tuning shifted, surprisingly, contrary to visual perceptual shifts. Therefore, while unsupervised recalibration (to reduce cue conflict) occurs in early multisensory cortices, higher-level VIP reflects only a global shift, in vestibular space.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>cross-modal</kwd><kwd>plasticity</kwd><kwd>self-motion</kwd><kwd>vestibular</kwd><kwd>visual</kwd><kwd>adaptation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002855</institution-id><institution>Ministry of Science and Technology of the People's Republic of China</institution></institution-wrap></funding-source><award-id>2021ZD0202600</award-id><principal-award-recipient><name><surname>Chen</surname><given-names>Aihua</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32171034</award-id><principal-award-recipient><name><surname>Chen</surname><given-names>Aihua</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32061143003</award-id><principal-award-recipient><name><surname>Chen</surname><given-names>Aihua</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003977</institution-id><institution>Israel Science Foundation</institution></institution-wrap></funding-source><award-id>3318/20</award-id><principal-award-recipient><name><surname>Zaidel</surname><given-names>Adam</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Single-unit recordings from cortical neurons in behaving macaque monkeys expose differential aspects of multisensory plasticity across different multisensory areas during visual–vestibular recalibration.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Our different sensory systems each continuously adapt to changes in the environment (<xref ref-type="bibr" rid="bib69">Webster, 2012</xref>). Thus, to maintain stable and coherent perception in a multisensory and ever-changing world, the brain needs to dynamically adjust for sensory discrepancies between the different modalities. This process of multisensory recalibration takes place continually and is perhaps more fundamental than multisensory integration because integration would not be beneficial when the underlying cues are biased. While the neural bases of multisensory integration have received a lot of attention (<xref ref-type="bibr" rid="bib21">Chen et al., 2013a</xref>; <xref ref-type="bibr" rid="bib35">Gu et al., 2008</xref>; <xref ref-type="bibr" rid="bib61">Stein et al., 2014</xref>), the neural bases of multisensory recalibration have been explored to a much lesser degree.</p><p>Cross-modal recalibration has been observed in a variety of multisensory settings. One well-known example is the ventriloquist aftereffect, in which exposure to a consistent spatial discrepancy between auditory and visual stimuli induces a subsequent shift in the perceived location of sounds (<xref ref-type="bibr" rid="bib6">Bertelson and De Gelder, 2004</xref>; <xref ref-type="bibr" rid="bib17">Canon, 1970</xref>; <xref ref-type="bibr" rid="bib44">Kramer et al., 2019</xref>; <xref ref-type="bibr" rid="bib51">Radeau and Bertelson, 1974</xref>; <xref ref-type="bibr" rid="bib53">Recanzone, 1998</xref>; <xref ref-type="bibr" rid="bib68">Watson et al., 2021</xref>). Also, the rubber-hand illusion leads to an offset in hand proprioception in the direction of the visually observed rubber hand (<xref ref-type="bibr" rid="bib1">Abdulkarim et al., 2021</xref>; <xref ref-type="bibr" rid="bib7">Botvinick and Cohen, 1998</xref>; <xref ref-type="bibr" rid="bib41">Kennett et al., 2001</xref>; <xref ref-type="bibr" rid="bib62">Thériault et al., 2022</xref>; <xref ref-type="bibr" rid="bib63">Tsakiris and Haggard, 2005</xref>). Although it was initially thought that only the non-visual cues recalibrate to vision, termed visual dominance (<xref ref-type="bibr" rid="bib8">Brainard and Knudsen, 1993</xref>; <xref ref-type="bibr" rid="bib54">Rock and Victor, 1964</xref>), further work in a variety of paradigms has revealed both visual and non-visual recalibration (<xref ref-type="bibr" rid="bib3">Atkins et al., 2003</xref>; <xref ref-type="bibr" rid="bib45">Lewald, 2002</xref>; <xref ref-type="bibr" rid="bib65">van Beers et al., 2002</xref>; <xref ref-type="bibr" rid="bib72">Zaidel et al., 2011</xref>).</p><p>Most of what we know about multisensory recalibration is described at the behavioral level (<xref ref-type="bibr" rid="bib45">Lewald, 2002</xref>), with little known about its neuronal underpinnings. Recent studies in humans have shed some light on this question. In the ventriloquism aftereffect, cross-modal (audio-visual) recalibration of auditory signals (fMRI) is seen in low-level auditory cortical areas (<xref ref-type="bibr" rid="bib79">Zierul et al., 2017</xref>). According to that study and another recent (EEG) study (<xref ref-type="bibr" rid="bib50">Park and Kayser, 2021</xref>), higher-level parietal regions also play a central role in cross-modal spatial recalibration. Moreover, <xref ref-type="bibr" rid="bib50">Park and Kayser, 2021</xref> further suggest that frontal regions consolidate the behavioral shift under sustained multisensory discrepancies. However, these methods (fMRI and EEG) lack the resolution to probe recalibration at the level of single neurons.</p><p>In a series of classic studies, Kundsen and Brainard investigated multisensory plasticity at the neuronal and circuit levels in the barn owl (<xref ref-type="bibr" rid="bib43">Knudsen, 2002</xref>; <xref ref-type="bibr" rid="bib42">Knudsen and Brainard, 1991</xref>; <xref ref-type="bibr" rid="bib46">Linkenhoker and Knudsen, 2002</xref>). They found profound neuronal plasticity in juvenile owls reared with prismatic lenses that systematically displaced their field of view. In that case, the auditory space map in the optic tectum was recalibrated to be aligned with the displaced visual field (<xref ref-type="bibr" rid="bib42">Knudsen and Brainard, 1991</xref>). However, multisensory plasticity is not limited to the development, and the neuronal bases of how multiple sensory systems continuously adapt to one another in the adult brain remain fundamentally unknown.</p><p>Self-motion perception (the subjective feeling of moving through space) relies primarily on visual and vestibular cues (<xref ref-type="bibr" rid="bib16">Butler et al., 2015</xref>; <xref ref-type="bibr" rid="bib15">Butler et al., 2010</xref>; <xref ref-type="bibr" rid="bib27">de Winkel et al., 2010</xref>; <xref ref-type="bibr" rid="bib32">Fetsch et al., 2011</xref>; <xref ref-type="bibr" rid="bib31">Fetsch et al., 2009</xref>; <xref ref-type="bibr" rid="bib34">Gu et al., 2007</xref>; <xref ref-type="bibr" rid="bib67">Warren et al., 1988</xref>). Multisensory integration of visual and vestibular signals can improve heading perception (<xref ref-type="bibr" rid="bib16">Butler et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Dokka et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Gu et al., 2008</xref>). However, conflicting or inconsistent visual and vestibular information often leads to motion sickness (<xref ref-type="bibr" rid="bib49">Oman, 1990</xref>; <xref ref-type="bibr" rid="bib52">Reason and Brand, 1975</xref>). Interestingly, this subsides after prolonged exposure to the sensory motion conflict, presumably through brain mechanisms of multisensory recalibration (<xref ref-type="bibr" rid="bib39">Held, 1961</xref>; <xref ref-type="bibr" rid="bib60">Shupak and Gordon, 2006</xref>). Thus, self-motion perception – a vital skill for everyday function with intrinsic plasticity – offers a prime substrate to study cross-modal recalibration.</p><p>We previously investigated and found robust perceptual recalibration of both visual and vestibular cues in response to a systematic vestibular–visual heading discrepancy (<xref ref-type="bibr" rid="bib72">Zaidel et al., 2011</xref>). Similar results were seen for both humans and monkeys. In that paradigm, no external feedback was given. Thus, the need for recalibration arose solely because of the cue discrepancy (we therefore call this condition <italic>unsupervised</italic>). This led to shifts in subsequent visual and vestibular perceptual estimates toward each other, presumably to reduce the conflict. This is in line with the notion that unsupervised recalibration aims to maintain ‘internal consistency’ between the cues (<xref ref-type="bibr" rid="bib13">Burge et al., 2010</xref>). However, the neuronal basis of this everyday multisensory plasticity is unknown. This study aimed to test unsupervised recalibration of visual and vestibular neuronal tuning, and how it may differ across multisensory cortical areas.</p><p>In line with human neuroimaging studies that showed cross-modal (auditory–visual) recalibration in relatively early sensory areas (<xref ref-type="bibr" rid="bib2">Amedi et al., 2002</xref>; <xref ref-type="bibr" rid="bib79">Zierul et al., 2017</xref>), and because unsupervised recalibration is sensory driven (occurs as a result of the cross-modal discrepancy, in the absence of overt feedback) we expected to observe neural correlates of unsupervised visual–vestibular recalibration in relatively early cortical areas with self-motion signals. Previous studies with monkeys identified two relatively early multisensory cortical areas involved in self-motion perception: the medial superior temporal area (<xref ref-type="bibr" rid="bib33">Gu et al., 2006</xref>) and the parietal insular vestibular cortex (PIVC, <xref ref-type="bibr" rid="bib18">Chen et al., 2010</xref>). Neurons in MSTd respond to large optic flow stimuli, conducive to the visual perception of self-motion (<xref ref-type="bibr" rid="bib33">Gu et al., 2006</xref>). Vestibular responses are also present in MSTd, however visual self-motion signals dominate (<xref ref-type="bibr" rid="bib38">Gu, 2018</xref>; <xref ref-type="bibr" rid="bib37">Gu et al., 2012</xref>). PIVC has strong vestibular responses, without strong tuning to visual optic flow (<xref ref-type="bibr" rid="bib18">Chen et al., 2010</xref>).</p><p>The ventral intraparietal (VIP) area also has robust responses to visual and vestibular self-motion stimuli, however, it is marked by strong choice signals (<xref ref-type="bibr" rid="bib23">Chen et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Gu, 2018</xref>; <xref ref-type="bibr" rid="bib74">Zaidel et al., 2017</xref>). It is thus considered a higher-level multisensory area, possibly involved in perceptual decision-making or higher-order perceptual functions. Accordingly, and in line with findings of parietal involvement in human cross-modal recalibration (<xref ref-type="bibr" rid="bib50">Park and Kayser, 2021</xref>; <xref ref-type="bibr" rid="bib75">Zaidel et al., 2021</xref>; <xref ref-type="bibr" rid="bib79">Zierul et al., 2017</xref>), we were interested to see what correlates of unsupervised recalibration we would see in parietal neurons. Different types of multisensory recalibration observed in VIP vs. lower-level (MSTd and PIVC) multisensory areas can provide important insights into their differential underlying functions. Thus, in this study, we focused on these three multisensory cortical areas. We examined whether and how their visual and vestibular neural tuning changed in accordance with corresponding perceptual shifts during a single session (~1 hr) of unsupervised cross-modal recalibration.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Three monkeys performed a heading discrimination task before, during, and after undergoing cross-modal recalibration to spatially conflicting vestibular–visual signals. Simultaneous to behavioral performance, we recorded from single neurons extracellularly in areas MSTd (upper bank of the superior temporal sulcus, <italic>N</italic> = 83 total; 19 from monkey D, 64 from monkey K), PIVC (upper bank and the tip of the lateral sulcus, <italic>N</italic> = 160 total; 91 from monkey D, 69 from monkey B), and VIP (lower bank and tip of the intraparietal sulcus, <italic>N</italic> = 118 total; 103 from monkey D, 15 from monkey B).</p><p>The experiment paradigm followed similar methodology as our previous behavioral study (<xref ref-type="bibr" rid="bib72">Zaidel et al., 2011</xref>). It consisted of three consecutive blocks: pre-recalibration (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), recalibration (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), and post-recalibration (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). In the recalibration block, the monkeys were presented with combined stimuli (simultaneous visual and vestibular cues) with a systematic discrepancy between the visual and vestibular heading directions. In the pre- and post-recalibration blocks, the unisensory perception was measured using visual-only or vestibular-only cues. The effects of recalibration on visual and vestibular perception were measured by the shifts in the post- vs. pre-recalibration psychometric curves. We first (in the next section) present the monkeys’ perceptual recalibration results. Thereafter, we present the neural correlates thereof.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Multisensory recalibration paradigm.</title><p>(<bold>A</bold>) Pre-recalibration block. The vestibular stimulus was elicited by moving the motion platform (schematic in the middle, viewed from above). The visual stimulus, presented on the screen in front of the monkey, corresponded to optic flow (schematic on the right) as it would be experienced during self-motion (without motion of the platform). The self-motion stimuli comprised linear motions (of either vestibular or visual stimuli) in a forward motion with a small leftward or rightward component (black arrows, schematic in the middle). Monkeys were required to fixate on a central target (yellow circle) presented on the screen during the stimulus and then to report their perceived heading by making a saccade to one of two targets (left or right relative to straight ahead). The heading angle (<italic>θ</italic>) was varied across trials. (<bold>B</bold>) Recalibration block. Simultaneous vestibular and visual stimuli (combined) with a systematic discrepancy (Δ) between the vestibular and visual headings were presented. Only one discrepancy orientation (Δ<sup>+</sup> or Δ<sup>−</sup>) was used per session. The blue and red arrows represent the vestibular and visual headings, respectively. The gray arrows represent the headings (varied across trials) from which the vestibular and visual cues were offset (to either side by Δ/2). The black dashed lines represent straight ahead. (<bold>C</bold>) Post-recalibration block. The single-cue trials (like in A) were interleaved with combined-cue trials (like in B).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82895-fig1-v1.tif"/></fig><sec id="s2-1"><title>Vestibular and visual perceptual estimates shift toward each other</title><p><xref ref-type="fig" rid="fig2">Figure 2</xref> shows example psychophysical data from two experimental sessions. Replicating our previous behavioral results (<xref ref-type="bibr" rid="bib72">Zaidel et al., 2011</xref>), we found that both visual and vestibular psychometric functions shifted in the direction required to reduce cue conflict. Namely, when the vestibular and visual heading stimuli were systematically offset, such that they consistently deviated to the right and the left, respectively (Δ<sup>+</sup>, <xref ref-type="fig" rid="fig2">Figure 2A</xref>), the vestibular post-recalibration curve (blue) was shifted rightward vs. pre-recalibration (black). Note that a rightward shift of the psychometric curve indicates a <italic>leftward</italic> perceptual shift (identified by a lower propensity for ‘rightward’ choices at 0° heading for the blue curve). Complementarily, the visual post-recalibration psychometric curve (red) shifted leftward vs. pre-recalibration (black), albeit to a lesser degree, indicating a <italic>rightward</italic> perceptual shift. In a reverse manner, when the vestibular and visual heading stimuli were offset to the left and right, respectively (Δ<sup>−</sup>, <xref ref-type="fig" rid="fig2">Figure 2B</xref>), the vestibular post-recalibration curve (blue) shifted to the left, and the visual post-recalibration curve shifted to the right.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Multisensory recalibration behavior.</title><p>(<bold>A, B</bold>) Perceptual recalibration in two example sessions, with (<bold>A</bold>) Δ<sup>+</sup> (vestibular and visual headings offset to the right and left, respectively; monkey D, session #10) and (<bold>B</bold>) Δ<sup>−</sup> (vestibular and visual headings offset to the left and right, respectively; monkey D, session #48). Psychometric curves (cumulative Gaussian distribution functions) were fitted to the data (circles), and represent the proportion of rightward choices, as a function of stimulus heading direction. Pre-recalibration heading judgments are depicted by black curves, in the left and right columns for vestibular and visual cues, respectively. After recalibration, vestibular and visual curves (blue and red, respectively) were shifted in relation to the pre-calibration curves. (<bold>C</bold>) Blue and red histograms represent the distributions of the point of subjective equality (PSE) shifts (post-recalibration minus pre-recalibration PSE) for vestibular and visual cues, respectively. Histograms above and below the abscissa represent sessions with Δ<sup>+</sup> and Δ<sup>−</sup>, respectively. Inverted triangles (▼) and upright triangles (▲) with error bars represent mean ± standard error of the mean (SEM) shifts for sessions with Δ<sup>+</sup> and Δ<sup>−</sup>, respectively. The numbers on the plots represent the mean PSE shifts. Asterisk symbols indicate significant shifts (p &lt; 0.05). For the vestibular cue, p = 2.3 × 10<sup>−17</sup>, <italic>N</italic> = 241 sessions (Δ<sup>+</sup> condition), and p = 1.0 × 10<sup>−28</sup>, <italic>N</italic> = 227 sessions (Δ<sup>−</sup> condition), paired <italic>t</italic>-test. For the visual cue, p = 6.5 × 10<sup>−11</sup> (Δ<sup>+</sup> condition), and p = 5.4 × 10<sup>−23</sup> (Δ<sup>−</sup> condition), paired <italic>t</italic>-test. Summary statistics for the individual animals are presented in <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Individual monkey summary statistics of behavioral shifts.</title></caption><media mimetype="application" mime-subtype="doc" xlink:href="elife-82895-fig2-data1-v1.doc"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82895-fig2-v1.tif"/></fig><p>These perceptual shifts were quantified by the difference between the post- vs. pre-recalibration curves’ PSEs (points of subjective equality). A psychometric curve’s PSE represents the heading angle of equal right/left choice proportion, that is, the heading that participants would supposedly perceive as straight ahead. The vestibular and visual psychometric shifts were positive and negative, 3.40° and −1.01°, respectively, in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, and negative and positive, −3.68° and 1.00°, respectively, in <xref ref-type="fig" rid="fig2">Figure 2B</xref>. Thus, in both cases (<xref ref-type="fig" rid="fig2">Figure 2A, B</xref>), both the vestibular and the visual cues shifted in the direction required to reduce the cue conflict (i.e., in opposite directions).</p><p>In each session, only one discrepancy orientation was tested. Namely, vestibular and visual headings were either offset to the right and left (respectively), or vice versa. These discrepancies were arbitrarily defined as positive (Δ<sup>+</sup>) or negative (Δ<sup>−</sup>), respectively. In total, we collected data from 241 sessions with Δ<sup>+</sup> and 227 sessions with Δ<sup>−</sup>. Distributions of the vestibular and visual PSE shifts across sessions are presented in <xref ref-type="fig" rid="fig2">Figure 2C</xref> (above and below the abscissa for Δ<sup>+</sup> and Δ<sup>−</sup>, respectively<sc>)</sc>. The vestibular PSEs were shifted significantly to the right for the Δ<sup>+</sup> condition (mean ± SE = 1.12° ± 0.12°; p = 2.3 × 10<sup>−17</sup>, paired <italic>t</italic>-test), and significantly to the left for the Δ<sup>−</sup> condition (mean ± SE = −1.76° ± 0.14°; p = 1.0 × 10<sup>−28</sup>, paired <italic>t</italic>-test). The visual PSEs were shifted significantly to the left for the Δ<sup>+</sup> condition (mean ± SE = −0.73° ± 0.11°; p = 6.5 × 10<sup>−11</sup>, paired <italic>t</italic>-test), and significantly to the right for the Δ<sup>−</sup> condition (mean ± SE = 1.10° ± 0.10°; p = 5.4 × 10<sup>−23</sup>, paired <italic>t</italic>-test). Thus, consistent with our previous study, both cues shifted (in opposite directions) to reduce cue conflict.</p><p>Comparing the vestibular vs. visual shift magnitudes (pooled by flipping the vestibular and visual shift signs in the Δ<sc><sup>−</sup></sc> and <sc>Δ<sup>+</sup></sc> conditions, respectively) demonstrated significantly larger vestibular vs. visual shifts (1.43° ± 0.09° and 0.91° ± 0.07°, respectively; p = 6.8 × 10<sup>−5</sup>, paired <italic>t</italic>-test). This result is also consistent with our previous study. Thus, the behavioral results from the original study (performed in the Angelaki laboratory) were replicated in these experiments (in the Chen laboratory) using a new set of monkeys, with simultaneous neuronal recording. In the following sections, we present how neuronal responses in areas MSTd, PIVC, and VIP (<xref ref-type="fig" rid="fig3">Figures 3</xref>, <xref ref-type="fig" rid="fig4">4</xref> and <xref ref-type="fig" rid="fig5">5</xref>, respectively) were recalibrated in comparison to the perceptual shifts.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Dorsal medial superior temporal (MSTd) neuronal recalibration.</title><p>(<bold>A</bold>) An example recalibration session (Δ<sup>+</sup>) with simultaneous recording from MSTd. The left column depicts the behavioral responses, pre-, and post-recalibration. The vestibular psychometric curve shifted 3.01° (to the right) and the visual curve shifted −2.71° (to the left). Neuronal responses (middle column) as a function of heading (pre- and post-recalibration). Circles and error bars represent average firing rates (FRs, baseline subtracted) ± standard error of the mean (SEM). The right column shows corresponding neurometric curves with fitted cumulative Gaussian functions. Each data point shows the proportion of trials in which an ideal observer would make a rightward choice given the FRs of the neurons. The vestibular neuronal shift was 4.73° (to the right) and the visual neuronal shift was −1.22° (to the left). (<bold>B</bold>) Correlations between neuronal point of subjective equality (PSE) shifts and perceptual PSE shifts for the vestibular and visual cues (left and right plots, respectively). Only neurons that passed screening (had significant responses and reliable neurometric PSEs, see Methods for details) were included in this analysis. Solid symbols represent sessions with Δ<sup>+</sup> and open symbols represent Δ<sup>−</sup>. The solid lines illustrate the regression lines of the data. r, Pearson’s correlation coefficient. Summary statistics for the individual animals, and linear mixed model (LMM) results, are presented in <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref> and <xref ref-type="supplementary-material" rid="fig3sdata2">Figure 3—source data 2</xref>, respectively.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Individual monkey summary statistics for dorsal medial superior temporal (MSTd) correlations.</title></caption><media mimetype="application" mime-subtype="doc" xlink:href="elife-82895-fig3-data1-v1.doc"/></supplementary-material></p><p><supplementary-material id="fig3sdata2"><label>Figure 3—source data 2.</label><caption><title>Comparison of pooled model (PM) and linear mixed model (LMM) for dorsal medial superior temporal (MSTd).</title></caption><media mimetype="application" mime-subtype="docx" xlink:href="elife-82895-fig3-data2-v1.docx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82895-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Distribution of neurometric point of subjective equality (PSE) reliability.</title><p>Standard deviations (SDs) of the neurometric PSEs (calculated via boostrapping) for vestibular and visual stimuli (top and bottom rows, respectively) are presented for neurons recorded from (<bold>A</bold>) dorsal medial superior temporal (MSTd), (<bold>B</bold>) parietoinsular vestibular cortex (PIVC), and (<bold>C</bold>) ventral intraparietal (VIP). Only neurons with significant tuning in the pre- or post-recalibration blocks (p &lt; 0.05, Pearson correlation between firing rate and heading) were included here. The vertical dashed lines mark SD = 10°. Neurons with an unreliable PSE estimate (SD &gt;10°, in either the pre- or post-recalibration blocks) were excluded from subsequent analyses.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82895-fig3-figsupp1-v1.tif"/></fig></fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Parietoinsular vestibular cortex (PIVC) neuronal recalibration.</title><p>(<bold>A</bold>) An example recalibration session (Δ<sup>+</sup>) with simultaneous recording from PIVC (conventions are the same as <xref ref-type="fig" rid="fig3">Figure 3</xref>). The vestibular and visual psychometric curves shifted 1.37° and −0.51° (to the right and left, respectively). The vestibular neurometric curve shifted 5.37° (to the right). Although a visual neurometric curve is presented for this example, no visual neurometric shift was calculated, and the neuron was excluded from subsequent visual cue analyses, because it did not pass the screening for significant tuning to visual stimuli. (<bold>B</bold>) Correlations between neuronal point of subjective equality (PSE) shifts and perceptual PSE shifts for the vestibular and visual cues. Summary statistics for the individual animals, and linear mixed model (LMM) results, are presented in <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref> and <xref ref-type="supplementary-material" rid="fig4sdata2">Figure 4—source data 2</xref>, respectively.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Individual monkey summary statistics for parietoinsular vestibular cortex (PIVC) correlations.</title></caption><media mimetype="application" mime-subtype="doc" xlink:href="elife-82895-fig4-data1-v1.doc"/></supplementary-material></p><p><supplementary-material id="fig4sdata2"><label>Figure 4—source data 2.</label><caption><title>Comparison of pooled model (PM) and linear mixed model (LMM) for parietoinsular vestibular cortex (PIVC).</title></caption><media mimetype="application" mime-subtype="docx" xlink:href="elife-82895-fig4-data2-v1.docx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82895-fig4-v1.tif"/></fig><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Ventral intraparietal (VIP) neuronal recalibration.</title><p>(<bold>A</bold>) An example recalibration session (Δ<sup>+</sup>) with simultaneous recording from VIP (conventions are the same as <xref ref-type="fig" rid="fig3">Figure 3</xref>). The vestibular and visual psychometric curves shifted 4.81° and −1.13° (to the right and left, respectively). The vestibular and visual neurometric curves shifted 15.18° and 7.58°, respectively (both to the right). (<bold>B</bold>) Correlations between neuronal point of subjective equality (PSE) shifts and perceptual PSE shifts for the vestibular and visual cues. Summary statistics for the individual animals, and linear mixed model (LMM) results, are presented in <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref> and <xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref>, respectively.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Individual monkey summary statistics for ventral intraparietal (VIP) correlations.</title></caption><media mimetype="application" mime-subtype="doc" xlink:href="elife-82895-fig5-data1-v1.doc"/></supplementary-material></p><p><supplementary-material id="fig5sdata2"><label>Figure 5—source data 2.</label><caption><title>Comparison of pooled model (PM) and linear mixed model (LMM) for ventral intraparietal (VIP).</title></caption><media mimetype="application" mime-subtype="docx" xlink:href="elife-82895-fig5-data2-v1.docx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82895-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Neuronal vs. behavioral shifts by neuron type in area ventral intraparietal (VIP).</title><p>(<bold>A</bold>) Neurons with multisensory (green) and unisensory (blue and red, for vestibular and visual, respectively) tuning. (<bold>B</bold>) Multisensory neurons with congruent (dark green), or opposite (orange), vestibular and visual tuning. The neuronal shifts were positively correlated with the behavioral shifts for the vestibular cue (left column), and negatively correlated with the behavioral shifts for the visual cue (right column). Pearson correlation coefficients are presented on the corresponding plots.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82895-fig5-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Vestibular and visual tuning in MSTd shifted according to their respective perceptual shifts</title><p>Responses of an example neuron recorded from MSTd during unsupervised recalibration are presented in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. Behaviorally, the vestibular PSE shifted rightward and the visual PSE shifted leftward (left column, <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Shifts in neuronal tuning could be subtle, therefore we used neurometrics to expose and quantify the neuronal shifts. Specifically, we calculated neurometric responses for the heading stimuli using the neuron’s firing rates (FRs), and fit these with a cumulative Gaussian function. The neurometric PSE reflects the heading direction at which the fitted neurometric curve crosses 0.5, that is, estimated straight ahead according to the neuronal data, in reference to the mean pre-recalibration FRs (see Methods for details). Neurometric curves for this example neuron are presented in the rightmost column of <xref ref-type="fig" rid="fig3">Figure 3A</xref>.</p><p>For this MSTd neuron, the vestibular neurometric curve shifted to the right, while the visual neurometric curve shifted to the left. Thus, the shifts in vestibular and visual tuning were consistent with the perceptual shifts. For subsequent (group) analyses, only neurons that both: (1) were significantly tuned to the respective (visual or vestibular) stimulus, and (2) had reliable neurometric PSEs, were included (see Methods and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for details). Neuronal shifts were calculated, similar to perceptual shifts, by the difference between the post- vs. pre-recalibration neurometric curves’ PSEs. The MSTd neuronal shifts were significantly correlated with the perceptual shifts, both for vestibular and visual cues (<italic>r</italic> = 0.62, p = 0.019, <italic>N</italic> = 14, and <italic>r</italic> = 0.38, p = 2.7 × 10<sup>−3</sup>, <italic>N</italic> = 59, respectively; Pearson correlations, data pooled across monkeys). Similar results were found when analyzing the monkeys individually (<xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref>) and when using a linear mixed model (LMM) which took into account differences between individual monkeys (the LMM did not provide a better fit vs. the pooled model; <xref ref-type="supplementary-material" rid="fig3sdata2">Figure 3—source data 2</xref>). Therefore, in area MSTd neuronal recalibration occurs in accordance with perceptual recalibration, both for vestibular and visual cues.</p></sec><sec id="s2-3"><title>Vestibular tuning in PIVC shifted in accordance with vestibular perceptual shifts</title><p>In PIVC, a similar result was observed for vestibular tuning. The example vestibular neurometric curve (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, top right) shifted to the right, which was consistent with the vestibular perceptual shift (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, top left). Across the population of PIVC neurons, a significant positive correlation was seen between the neuronal and perceptual shifts for the vestibular cue (<italic>r</italic> = 0.80, p = 9.7 × 10<sup>−6</sup>, <italic>N</italic> = 30, Pearson correlation, data pooled across monkeys; <xref ref-type="fig" rid="fig4">Figure 4B</xref>, left panel). Similar results were found when analyzing the monkeys individually (<xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>) and when using an LMM (the LMM did not provide a better fit vs. the pooled model; <xref ref-type="supplementary-material" rid="fig4sdata2">Figure 4—source data 2</xref>).</p><p>In general, the PIVC neurons did not demonstrate robust responses to the visual stimuli. This example neuron was not significantly tuned to the visual stimuli (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, bottom, middle), thus it had poor visual neurometric curves (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, bottom, right) and was excluded from further visual (group) analyses. The correlation between the neuronal and perceptual shifts (performed for those neurons that did pass screening) was not significant for the visual cue (<italic>r</italic> = 0.26, p = 0.47, <italic>N</italic> = 10, Pearson correlation, data pooled across monkeys). Similar results were found when analyzing the monkeys individually (<xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>) and when using an LMM (the LMM did not provide a better fit vs. the pooled model; <xref ref-type="supplementary-material" rid="fig4sdata2">Figure 4—source data 2</xref>). A Bayesian Pearson correlation (BF<sub>10</sub> = 0.49) supported neither the alternative hypothesis (H<sub>1</sub>) of a correlation between neuronal and perceptual shifts for the visual cue, nor the null hypothesis (H<sub>0</sub>). The lack of support for or against visual recalibration in PIVC primarily reflects the lack of robust tuning to visual heading stimuli in PIVC.</p></sec><sec id="s2-4"><title>Neuronal tuning in VIP to both vestibular and visual stimuli shifted according to vestibular perceptual shifts</title><p><xref ref-type="fig" rid="fig5">Figure 5A</xref> presents an example neuron from VIP. The vestibular neurometric curve shifted rightward (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, top right), in accordance with the vestibular perceptual shift (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, top left). Surprisingly, the visual neurometric curve also shifted rightward (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, bottom right). This was unexpected because the visual psychometric curve shifted leftward (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, bottom left). Thus, while the vestibular and visual behavioral psychometric curves shifted in opposite directions (toward each other) the vestibular and visual neurometric curves shifted together, in accordance with the vestibular (not visual) perceptual shift.</p><p>Across the population of VIP neurons, the vestibular neurometric shifts were significantly positively correlated with the vestibular perceptual shifts (<italic>r</italic> = 0.77, p = 2.7 × 10<sup>−8</sup>, <italic>N</italic> = 37, Pearson correlation, data pooled across monkeys; <xref ref-type="fig" rid="fig5">Figure 5B</xref>, left). Similar results were found when analyzing the monkeys individually (<xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>) and when using an LMM (the LMM did not provide a better fit vs. the pooled model; <xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref>). Like in MSTd and PIVC, the positive correlation coefficient indicates that neuronal and behavioral curves shifted in the same direction for the vestibular cue.</p><p>By contrast, the visual neurometrics in VIP shifted in the opposite direction to the visual perceptual shifts. Neuronal and perceptual shifts for the visual cue were negatively correlated (<italic>r</italic> = −0.68, p = 8.4 × 10<sup>−7</sup>, <italic>N</italic> = 42, Pearson correlation, data pooled across monkeys; <xref ref-type="fig" rid="fig5">Figure 5B</xref>, right). Similar results were found when analyzing the monkeys individually (<xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>) and when using an LMM (the LMM did not provide a better fit vs. the pooled model; <xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref>). This exposes a striking mismatch between visual neuronal responses in VIP and visual perceptual function. It also exposes a striking mismatch between visual tuning in MSTd (which shifted in the same direction as visual perception) in comparison to visual tuning in area VIP (which shifted contrary to visual perception).</p><p>To test whether this mismatch between behavior and tuning for visual cues in VIP relates to specific subtypes of neurons, we sorted the VIP data into three subsets: multisensory neurons (respond significantly to both vestibular and visual stimuli), and two groups of unisensory neurons (respond significantly exclusively to vestibular or visual stimuli). Similar results were seen for both multisensory and unisensory neurons (the neuronal–perceptual correlations remained consistently positive and negative for vestibular and visual cues, respectively; <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>). We further sorted the multisensory neurons into those with congruent and opposite vestibular and visual heading preferences (<xref ref-type="bibr" rid="bib19">Chen et al., 2011a</xref>; <xref ref-type="bibr" rid="bib33">Gu et al., 2006</xref>) with no observable differences (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>). Therefore, the contrary shifts of visual tuning in VIP seem to reflect a general feature of this cortical area, rather than an anomaly of a subgroup of neurons.</p></sec><sec id="s2-5"><title>Temporal evolution of the correlation between neuronal and perceptual shifts</title><p>The neurometric curves in <xref ref-type="fig" rid="fig3">Figures 3</xref>—<xref ref-type="fig" rid="fig5">5</xref> were calculated using mean FRs averaged across the stimulus duration. But the self-motion stimuli generated by the platform and optic flow followed a specific dynamic time course, specifically, a Gaussian velocity profile and correspondingly a biphasic acceleration profile (see bottom row, <xref ref-type="fig" rid="fig6">Figure 6</xref>). Therefore, we further examined whether the correlations between neurometric and perceptual shifts depend on the time point within the stimulus interval.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Recalibration of neuronal responses within the stimulus time course.</title><p>Pearson correlations between neuronal and perceptual point of subjective equality (PSE) shifts, using the neuronal activity at specific time points during the stimulus, for (<bold>A</bold>) dorsal medial superior temporal (MSTd), (<bold>B</bold>) parietoinsular vestibular cortex (PIVC), and (<bold>C</bold>) ventral intraparietal (VIP). Top row: vestibular (blue histograms), middle row: visual (red histograms), bottom row: stimulus (acceleration and velocity) time course. Vertical dashed lines mark peak acceleration and peak deceleration. ‘<bold>*</bold>’ symbols mark significant correlations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82895-fig6-v1.tif"/></fig><p>For MSTd neurons, positive correlations (between neuronal and perceptual shifts) were seen for both vestibular and visual cues during the stimulus (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Correlations increased toward the middle of the stimulus, and dropped off rapidly at the end of the stimulus. Significant correlations (blue and red asterisk markers for vestibular and visual cues, respectively) were only seen around the middle of the stimulus. Thus neural recalibration in MSTd (in accordance with behavioral recalibration) could reflect the velocity responses.</p><p>For PIVC neurons, positive correlations (between neuronal and perceptual shifts) were seen only for vestibular cues, during the stimulus (upper panel in <xref ref-type="fig" rid="fig6">Figure 6B</xref>). Like MSTd, the vestibular correlations seemed to follow the velocity profile of the stimulus, with significant values around the middle of the stimulus. Correlations in the visual condition were very weak and not significant (middle panel in <xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p><p>A very different profile was seen in VIP. Firstly, as described above, correlations between neuronal and perceptual recalibration were positive for the vestibular cue (upper panel in <xref ref-type="fig" rid="fig6">Figure 6C</xref>) and negative for the visual cue (middle panel in <xref ref-type="fig" rid="fig6">Figure 6C</xref>). Furthermore, the time course of these correlations was different in VIP: they increased in size gradually (positively for vestibular and negatively for visual), reaching a maximum around the middle of the stimulus epoch (the velocity peak), but significant correlations were found for time intervals beyond the end of the stimulus. This pattern is in line with sustained neuronal activity described previously for VIP. However, here this sustained activity correlated with subsequent vestibular choices, and was contrary to visual choices.</p></sec><sec id="s2-6"><title>VIP choice signals are reduced after cross-modal recalibration</title><p>Previous studies have found that neuronal responses in VIP are strongly influenced (sometimes even dominated) by choice signals (<xref ref-type="bibr" rid="bib25">Chen et al., 2021</xref>; <xref ref-type="bibr" rid="bib74">Zaidel et al., 2017</xref>). Hence our finding here, that neuronal tuning recalibrated contrary to perceptual shifts for the visual cue, was surprising and counterintuitive. We, therefore, wondered what happened to the strong choice signals for which VIP is renowned, which would predict that neuronal tuning (also for visual cues) would shift with behavior.</p><p>To visualize choice tuning for an example VIP neuron, we plotted ‘choice-conditioned’ tuning curves, namely, neuronal responses as a function of heading, separately for rightward and leftward choices (<xref ref-type="fig" rid="fig7">Figure 7</xref>). In the pre-recalibration block vestibular responses were strongly choice related (<xref ref-type="fig" rid="fig7">Figure 7A</xref>, left panel) – neuronal responses to the same heading stimulus were larger when followed by rightward (►, blue) vs. leftward (◄, cyan) choices (the blue line lies above the cyan line). After recalibration, the choice effect decreased (<xref ref-type="fig" rid="fig7">Figure 7A</xref>, right panel) – the choice-conditioned tuning curves were no longer separate. Similarly, visual responses were strongly choice-related pre-recalibration, and this decreased post-recalibration (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). To quantify the choice (and sensory) components of neuronal activity, and to observe how these changed after recalibration, we applied a partial correlation analysis (<xref ref-type="bibr" rid="bib74">Zaidel et al., 2017</xref>). For this example neuron, the partial choice correlation values (<italic>R</italic><sub>c</sub>, presented on the plots) were reduced both for vestibular and visual cues.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Choice tuning is reduced post-recalibration in an example ventral intraparietal (VIP) neuron.</title><p>Neuronal responses for an example VIP neuron to (<bold>A</bold>) vestibular and (<bold>B</bold>) visual heading stimuli, pre- and post-recalibration (left and right columns, respectively). Blue and cyan curves depict choice-conditioned tuning curves (neuronal responses followed by rightward and leftward choices, respectively) for the vestibular cue. Red and magenta curves depict choice-conditioned tuning curves for the visual cue. Black curves (in the corresponding plots) represent all responses (not sorted by choice). Partial heading (<italic>R</italic><sub>h</sub>) and partial choice (<italic>R</italic><sub>c</sub>) correlations (with corresponding p values) are presented on the plots.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82895-fig7-v1.tif"/></fig><p>Across our sample of VIP neurons, the choice partial correlations in the post-recalibration block were significantly reduced compared to the pre-recalibration block, for both vestibular and visual cues (p = 6.0 × 10<sup>−4</sup> and p = 1.3 × 10<sup>−3</sup>, respectively, paired <italic>t</italic>-tests; <xref ref-type="fig" rid="fig8">Figure 8B</xref>). However, the heading partial correlations (<italic>R</italic><sub>h</sub>) did not differ significantly from pre- to post-recalibration, neither for vestibular not visual cues (p = 0.96 and p = 0.85, respectively, paired <italic>t</italic>-tests; <xref ref-type="fig" rid="fig8">Figure 8A</xref>). For these statistical comparisons and for plotting we used the squared partial correlations (which quantify the amount of unique variance explained by choice or heading). We did not observe any significant changes in partial correlations in areas PIVC and MSTd (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). Lastly, there was no evidence for differences between post- and pre-recalibration baseline FRs in any of the three areas (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>). Thus, shifts in neuronal tuning are not explained by changes in baseline activity.</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Choice tuning is reduced in ventral intraparietal (VIP) post-recalibration.</title><p>(<bold>A</bold>) Heading and (<bold>B</bold>) choice partial correlation coefficients (squared) are depicted post- vs. pre-recalibration. Blue and red circles (top and bottom rows) represent vestibular and visual cues, respectively. Filled (empty) circles indicate significant (non-significant) partial correlations for heading or choice. p values are presented on the corresponding plots (two-tailed paired <italic>t</italic>-tests).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82895-fig8-v1.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Choice and heading partial correlations.</title><p>(<bold>A</bold>) Dorsal medial superior temporal (MSTd) and (<bold>B</bold>) parietoinsular vestibular cortex (PIVC). Plotting conventions are the same as <xref ref-type="fig" rid="fig8">Figure 8</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82895-fig8-figsupp1-v1.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>Baseline firing rates.</title><p>The post- vs. pre-recalibration baseline firing rates in areas (<bold>A</bold>) dorsal medial superior temporal (MSTd), (<bold>B</bold>) parietoinsular vestibular cortex (PIVC), and (<bold>C</bold>) ventral intraparietal (VIP) are plotted for vestibular and visual cues (top and bottom rows, respectively). Solid symbols represent Δ<sup>+</sup> and open symbols represent Δ<sup>−</sup>. Bayes factors (BF<sub>10</sub>) &lt;⅓ (as for PIVC vestibular cue and VIP) provide substantial evidence against a change in baseline firing rates. Bayes factors between ⅓ and 3 (as for MSTd and PIVC visual cues) are inconclusive (provide no substantial evidence for, or against, changes).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82895-fig8-figsupp2-v1.tif"/></fig></fig-group></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>This study provides the first demonstration of unsupervised (cross-modal) neuronal recalibration, in conjunction with perceptual recalibration, in single sessions. Single neurons from MSTd, PIVC, and VIP revealed clear but different patterns of recalibration. In MSTd, neuronal responses to vestibular and visual cues shifted – each according to their respective cues’ perceptual shifts. In PIVC, vestibular tuning similarly shifted in the same direction as vestibular perceptual shifts (the PIVC cells were not robustly tuned to visual stimuli). However, recalibration in VIP was notably different: both vestibular and visual neuronal tuning shifted in the direction of the vestibular perceptual shifts. Thus, visual neuronal tuning shifted, surprisingly, contrary to visual perceptual shifts. These results indicate that neuronal recalibration differs profoundly across multisensory cortical areas.</p><sec id="s3-1"><title>Neural correlates of vestibular–visual recalibration</title><p>To investigate the neuronal bases of unsupervised cross-modal recalibration, we first replicated the perceptual results from our previous study (<xref ref-type="bibr" rid="bib72">Zaidel et al., 2011</xref>). Indeed, in the presence of a systematic vestibular–visual heading offset (with no external feedback) vestibular and visual cues both shifted in the direction required to reduce the cue conflict. And, as before, the vestibular shifts were larger compared to the visual shifts. Thus we confirmed robust recalibration of vestibular and visual cues, resulting from a systematic discrepancy between the cues’ headings in an unsupervised context (i.e., without external feedback).</p><p>Since there was no external feedback regarding which cue was (in)accurate, unsupervised recalibration is driven by the cue conflict, presumably through an internal mechanism to maintain consistency between vestibular and visual perceptual estimates (<xref ref-type="bibr" rid="bib72">Zaidel et al., 2011</xref>). Accordingly, we expected to see neuronal correlates of perceptual recalibration in early multisensory areas related to self-motion perception (<xref ref-type="bibr" rid="bib79">Zierul et al., 2017</xref>), specifically: MSTd, which primarily responds to visual (but also vestibular) self-motion stimuli, and PIVC, which primarily responds to vestibular stimuli. We further expected that the neuronal recalibration in MSTd and PIVC would propagate to higher-level multisensory area VIP.</p><p>In MSTd, we indeed found that both visual and vestibular neuronal signals shifted, each in accordance with their corresponding cue’s perceptual shifts. Hence, recalibration of visual self-motion responses was observed at least at the level of MSTd, which is the primary area in the visual hierarchy to respond to large field optic flow stimuli (<xref ref-type="bibr" rid="bib12">Britten, 2008</xref>; <xref ref-type="bibr" rid="bib10">Britten and van Wezel, 1998</xref>; <xref ref-type="bibr" rid="bib11">Britten and Van Wezel, 2002</xref>; <xref ref-type="bibr" rid="bib29">Duffy and Wurtz, 1995</xref>; <xref ref-type="bibr" rid="bib35">Gu et al., 2008</xref>; <xref ref-type="bibr" rid="bib37">Gu et al., 2012</xref>; <xref ref-type="bibr" rid="bib33">Gu et al., 2006</xref>; <xref ref-type="bibr" rid="bib70">Wurtz and Duffy, 1992</xref>). We cannot ascertain whether recalibration to visual responses occurred already in earlier visual regions, such as the middle temporal visual area, which projects to MSTd (<xref ref-type="bibr" rid="bib48">Maunsell and van Essen, 1983</xref>; <xref ref-type="bibr" rid="bib64">Ungerleider and Desimone, 1986</xref>), or whether it occurred only at the level of MSTd. Because MSTd is mainly a visual area, the recalibration of vestibular signals observed in MSTd likely occurred in upstream vestibular areas that project to MSTd, such as PIVC (<xref ref-type="bibr" rid="bib18">Chen et al., 2010</xref>; <xref ref-type="bibr" rid="bib19">Chen et al., 2011a</xref>). Indeed, robust vestibular recalibration (that was in line with the vestibular perceptual shifts) was observed in PIVC. Hence, neuronal correlates of perceptual recalibration were observed in relatively early multisensory areas related to self-motion perception (MSTd and PIVC).</p></sec><sec id="s3-2"><title>Modality-specific recalibration of vestibular and visual cues</title><p>Results from this experiment exposed modality-specific neuronal recalibration (in MSTd and PIVC). Namely, visual and vestibular tuning curves shifted differently (in opposite directions). This provides neuronal evidence against ‘visual dominance’, even for short-term recalibration (in single sessions). Rather, it supports the idea that cross-modal neuronal recalibration occurs also for visual (and not only for non-visual) cues. Furthermore, it exposes neuronal mechanisms to maintain internal consistency between vestibular and visual cues. This dynamic cross-modal plasticity may underlie our adept ability to adapt to sensory conflict commonly experienced in many modes of transport (on land, at sea, or in flight).</p><p>In a recent set of complementary studies, we tested <italic>supervised</italic> self-motion recalibration, by providing external feedback regarding cue accuracy (<xref ref-type="bibr" rid="bib75">Zaidel et al., 2021</xref>; <xref ref-type="bibr" rid="bib73">Zaidel et al., 2013</xref>). There, we found that supervised recalibration is a high-level cognitive process that compares the combined-cue (multisensory) estimate to feedback from the environment. Behaviorally, this resulted in ‘yoked’ recalibration – both cues shifted in the same direction, to reduce conflict between the combined estimate and external feedback (<xref ref-type="bibr" rid="bib73">Zaidel et al., 2013</xref>). Neuronally, robust recalibration of both vestibular and visual neuronal tuning was seen in VIP, such that tuning for both cues shifted together, in accordance with the behavior (<xref ref-type="bibr" rid="bib75">Zaidel et al., 2021</xref>).</p><p>However, because the shifts for both vestibular and visual cues were in the same direction in the supervised recalibration studies, neuronal tuning was also expected to shift in the same direction for both cues. Thus, we could not dissociate there whether neuronal shifts for a particular cue (e.g., visual) indeed followed the behavioral shifts for that cue (visual) or, less intuitively, the other cue (vestibular). By contrast, the unsupervised paradigm, tested in this study, elicits visual and vestibular shifts in opposite directions. It could thereby expose the (unexpected) finding that visual tuning in VIP actually shifts with vestibular (rather than visual) behavioral shifts.</p><p>The results here therefore also shed new light on the neuronal shifts observed in VIP after supervised recalibration (<xref ref-type="bibr" rid="bib75">Zaidel et al., 2021</xref>). They indicate that yoking of visual and vestibular tuning is observed in VIP irrespective of the paradigm (supervised or unsupervised). Hence, yoked recalibration may be a feature of VIP, not just a feature of supervised recalibration.</p></sec><sec id="s3-3"><title>Contrary recalibration in higher-level area VIP</title><p>VIP is a higher-level multisensory area (<xref ref-type="bibr" rid="bib9">Bremmer et al., 2002</xref>; <xref ref-type="bibr" rid="bib26">Colby et al., 1993</xref>; <xref ref-type="bibr" rid="bib30">Duhamel et al., 1998</xref>; <xref ref-type="bibr" rid="bib56">Schlack et al., 2002</xref>; <xref ref-type="bibr" rid="bib57">Schlack et al., 2005</xref>; <xref ref-type="bibr" rid="bib58">Schroeder and Foxe, 2002</xref>) with clear vestibular and visual heading selectivity (<xref ref-type="bibr" rid="bib19">Chen et al., 2011a</xref>; <xref ref-type="bibr" rid="bib20">Chen et al., 2011b</xref>). But the nature of these self-motion signals in VIP is not fully understood. In contrast to our prediction that recalibrated signals in MSTd and PIVC would simply propagate to VIP, we found a different and unexpected pattern of recalibration in VIP. While vestibular tuning shifted in line with vestibular perceptual shifts (like MSTd and PIVC), visual tuning shifted opposite in direction to the visual perceptual shifts (and opposite in direction to MSTd visual recalibration). These findings indicate that visual responses in VIP do not reflect a simple feed-forward projection from MSTd. They also suggest that visual responses in VIP are not decoded for heading perception (otherwise these would not shift in opposite directions). This interpretation is in line with findings that inactivation (<xref ref-type="bibr" rid="bib23">Chen et al., 2016</xref>) and microstimulation (<xref ref-type="bibr" rid="bib71">Yu and Gu, 2018</xref>) in VIP do not affect perceptual decisions. Thus, the convergence of visual and vestibular signals in VIP likely serves purposes other than cue integration.</p><p>We previously found strong choice-related activity in VIP neurons (<xref ref-type="bibr" rid="bib74">Zaidel et al., 2017</xref>). Accordingly, we considered that shifts in VIP neuronal tuning (after supervised recalibration) might simply reflect the altered choices (<xref ref-type="bibr" rid="bib75">Zaidel et al., 2021</xref>). However, choice-related activity cannot explain the results here, because the predicted shifts in neuronal tuning would be in the same direction as the altered choices (perceptual shifts), whereas we found contrary visual recalibration. To understand contrary shifts that could arise despite strong choice-related activity in VIP, we investigated choice tuning pre- and post-recalibration in VIP neurons. We found that choice tuning in VIP decreased after unsupervised recalibration. This allowed contrary shifts to be exposed, and opens up new and fascinating questions regarding the purpose of contrary visual recalibration in VIP.</p><p>Because visual and vestibular tuning in VIP both shifted in the same direction (in accordance with vestibular perceptual shifts) we speculate that VIP recalibration reflects a global shift in the vestibular reference frame. This notion is consistent with suggestions that VIP encodes self-motion and tactile stimuli in head or body-centered coordinates (<xref ref-type="bibr" rid="bib5">Avillac et al., 2005</xref>; <xref ref-type="bibr" rid="bib4">Avillac et al., 2004</xref>; <xref ref-type="bibr" rid="bib22">Chen et al., 2013b</xref>; <xref ref-type="bibr" rid="bib24">Chen et al., 2018</xref>; <xref ref-type="bibr" rid="bib77">Zhang et al., 2004</xref>), and that visual signals in VIP are remapped within these coordinates (<xref ref-type="bibr" rid="bib5">Avillac et al., 2005</xref>; <xref ref-type="bibr" rid="bib59">Sereno and Huang, 2014</xref>). Accordingly, visual responses in VIP are transformed into a vestibular-recalibrated space. This leads to a remarkable dissociation between visual tuning in VIP and MSTd. Interestingly, visual self-motion perception follows the MSTd (not VIP) recalibration. This is in line with a causal connection between MSTd and visual heading discrimination (<xref ref-type="bibr" rid="bib10">Britten and van Wezel, 1998</xref>; <xref ref-type="bibr" rid="bib37">Gu et al., 2012</xref>).</p><p>What purpose might such visual signals in VIP serve? One possible idea is that they might reflect an expectation signal – for example, predicted vestibular or somatosensory sensation, based on the current visual signal. During combined stimuli (in the recalibration and post-recalibration blocks), the visual signal always appeared together with the vestibular sensory input. Thus, if visual responses in VIP reflect vestibular expectations, then these would shift together with vestibular (rather than visual) recalibration.</p></sec><sec id="s3-4"><title>Limitations and future directions</title><p>Our results revealed correlations between neuronal recalibration and perceptual recalibration. However, they do not implicate any causal connections. Therefore, whether these cortical areas are actively involved in cross-modal recalibration (i.e., play a causal role) vs. simply reflecting the recalibrated signals (without playing a causal role) requires further research. To probe more directly for causal links, direct manipulation of neuronal activity might be required. For example, would reversible inactivation or microstimulation (of one or a combination of these multisensory areas) eliminate (or bias) unsupervised recalibration? In addition, future studies are needed to examine how the systematic error between vestibular and visual heading signals is detected. This likely involves additional brain areas, for example, the cerebellum, implicated in internal-model-based error monitoring (<xref ref-type="bibr" rid="bib47">Markov et al., 2021</xref>; <xref ref-type="bibr" rid="bib55">Rondi-Reig et al., 2014</xref>), and/or the anterior cingulate cortex, implicated in conflict monitoring (<xref ref-type="bibr" rid="bib14">Bush et al., 2000</xref>; <xref ref-type="bibr" rid="bib40">Holroyd and Coles, 2002</xref>). Thus, a wide-ranging effort to record and manipulate neural activity across a variety of brain regions will be necessary to tease apart the circuitry underlying this complex and important function.</p><p>The lack of evidence for (or against) visual recalibration in PIVC primarily reflects the lack of robust tuning to visual heading stimuli. We interpret the observed shifts in vestibular tuning in PIVC as lower-level, sensory, recalibration (similar to MSTd) based on the broader understanding that PIVC encodes lower-level vestibular signals, with transient time courses, and impoverished visual tuning (<xref ref-type="bibr" rid="bib23">Chen et al., 2016</xref>; <xref ref-type="bibr" rid="bib25">Chen et al., 2021</xref>). Our results are in line with this interpretation, and there is no reason to suspect that PIVC reflects more complex multisensory recalibration (like VIP). Nonetheless, the data could also be in line with alternative interpretations. A broader range of headings, and analyses beyond neurometrics, would be required to better understand whether (and how) visual signals in PIVC might be recalibrated.</p><p>The most surprising and intriguing finding in this study was the contrary recalibration of visual tuning in VIP. We propose that yoked recalibration of visual and vestibular responses in VIP (despite differential perceptual recalibration) might reflect a global shift in vestibular space. Accordingly, we suggest that visual responses in VIP might reflect an expectation signal (in vestibular space), for example, a simulation of the expected corresponding vestibular response (or integrated position, because VIP responses are sustained beyond the stimulus period). However, this idea is speculative, and the data from this study cannot address this question. Hence, further research is needed to investigate this idea, for example, by conditioning expectations for vestibular motion on other (non-motion) cues, and investigating whether these cues can induce simulated vestibular responses. If this hypothesis turns out to be true, it could greatly contribute to our understanding regarding the functions of the parietal cortex, and the brain mechanisms of perceptual inference.</p></sec><sec id="s3-5"><title>Concluding remarks</title><p>This study exposed modality-specific recalibration of neuronal signals, resulting from a cross-modal (visual–vestibular) cue conflict. It further revealed profound differences in neuronal recalibration across multisensory cortical areas MSTd, PIVC, and VIP. The results therefore provide novel insights into adult multisensory plasticity, and deepen our understanding regarding the different functions of these multisensory cortical areas.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Subjects and surgery</title><p>Three male rhesus monkeys (<italic>Macaca mulatta</italic>, monkeys D, B, and K) weighing 8–10 kg participated in the experiment. The monkeys were first trained to sit in a custom primate chair and gradually exposed to the laboratory environment. Then the monkeys were chronically implanted a head-restraint cap and a sclera coil for measuring eye movement. After full recovery, the monkeys were trained to perform experimental tasks. All animal surgeries and experimental procedures were approved by the Institutional Animal Care and Use Committee at East China Normal University (IACUC protocol number: Mo20200101).</p></sec><sec id="s4-2"><title>Equipment setup and motion stimuli</title><p>During the experiments, the monkeys were head-fixed and seated in a primate chair which was secured to a six degrees of freedom motion platform (Moog, East Aurora, NY, USA; MB-E-6DOF/12/1000 kg). The chair was also inside a magnetic field coil frame (Crist Instrument Co, Inc, Hagerstown, MD, USA) mounted on the platform for measuring eye movement with the sclera coil technique (for details, see <xref ref-type="bibr" rid="bib78">Zhao et al., 2021</xref>).</p><p>Vestibular stimuli corresponded to linear movements of the platform (for details, see <xref ref-type="bibr" rid="bib21">Chen et al., 2013a</xref>; <xref ref-type="bibr" rid="bib33">Gu et al., 2006</xref>; <xref ref-type="bibr" rid="bib78">Zhao et al., 2021</xref>). Visual stimuli were presented on a large computer screen (Philips BDL4225E, Royal Philips, Amsterdam, Netherlands), attached to the field coil frame. The display (62.5 cm × 51.5 cm) was viewed from a distance of 43 cm, thus subtending a visual angle of 72° × 62°. The sides of the coil frame were covered with a black enclosure, so the monkey could only see the visual stimuli on the screen (<xref ref-type="bibr" rid="bib33">Gu et al., 2006</xref>; <xref ref-type="bibr" rid="bib78">Zhao et al., 2021</xref>). The display had a pixel resolution of 1920 × 1080 and was updated at 60 Hz. Visual stimuli were programmed in OpenGL to simulate self-motion through a 3D cloud of ‘stars’ that occupied a virtual cube space 80 cm wide, 80 cm tall, and 80 cm deep, centered on the central fixation point on the screen. The ‘star’ density was 0.01/cm<sup>3</sup>. Each ‘star’ comprised a triangle with base by height: 0.15 cm × 0.15 cm. Monkeys wore custom stereo glasses made from Wratten filters (red #29 and green #61; Barrington, NJ, USA), such that the optic flow stimuli could be rendered in three dimensions as red-green anaglyphs.</p><p>The self-motion stimulus was either vestibular-only, visual-only, or combined (visual and vestibular stimuli). In the vestibular-only condition, there was no optic flow on the screen and the monkey was translated by the motion platform. In the visual-only condition, the motion platform remained stationary while optic flow was presented on the screen. For the combined condition, the monkeys experienced both translation and optic flow simultaneously. Each motion stimulus followed a Gaussian velocity profile with a duration of 1 s, and a displacement amplitude of 13 cm (bottom row, <xref ref-type="fig" rid="fig6">Figure 6</xref>). The peak velocity was 0.41 m/s, and the peak acceleration was 2.0 m/s<sup>2</sup>.</p></sec><sec id="s4-3"><title>Task and recalibration protocol</title><p>The monkeys were trained to report their perceived direction of self-motion in a two-alternative forced-choice (2AFC) heading discrimination task (for details, see <xref ref-type="bibr" rid="bib21">Chen et al., 2013a</xref>; <xref ref-type="bibr" rid="bib35">Gu et al., 2008</xref>). In each trial, the monkey primarily experienced a forward motion with a small leftward or rightward component. During stimulation, the monkey was required to maintain fixation on a central point, within a 3° × 3° window. At the end of the trial (after a 300-ms delay period beyond the end of the stimulus), the monkeys needed to make a saccade to one of two targets (located 5° to the left and right of the central fixation point) to report their motion percept as leftward or rightward relative to straight ahead. The saccade endpoint had to remain within 2.5° of the target for at least 150 ms to be considered a valid choice. Correct responses were rewarded with a drop of liquid.</p><p>To elicit recalibration, we used an unsupervised cue-conflict recalibration protocol previously tested behaviorally in humans and monkeys (<xref ref-type="bibr" rid="bib72">Zaidel et al., 2011</xref>). Each experimental session consisted of three consecutive blocks, as described below.</p><sec id="s4-3-1"><title>Pre-recalibration block</title><p>This block was used to deduce the baseline performance (psychometric curve) of each modality for the monkeys, thus only a single-cue (vestibular-only or visual-only) stimulus was presented (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Across trials, the heading angle was varied in small steps around straight ahead. Ten logarithmically spaced heading angles were tested for each monkey (±16°, ±8°, ±4°, ±2°, and ±1°). To accustom the monkeys to not getting a reward for all the trials, they were rewarded with 95% probability for correct choices, and not rewarded for incorrect choices.</p></sec><sec id="s4-3-2"><title>Recalibration block</title><p>Only combined vestibular–visual cues were presented in this block (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). A discrepancy (Δ) between the vestibular and visual cues was introduced gradually from 2° to 10° (or −2° to −10°) with steps of 2°, and then held at ±10° for the rest of the block. This gradual introduction was applied to avoid the monkeys from noticing the discrepancy. The sign of Δ represents the orientation of the discrepancy: positive Δ (marked by Δ<sup>+</sup>) indicates that the vestibular and visual cues were systematically offset to the right and to the left, respectively, and vice versa for negative Δ (Δ<sup>−</sup>). Only one discrepancy orientation (Δ<sup>+</sup> or Δ<sup>−</sup>) was used per session. The combined stimulus headings followed the same ten headings as the single-cue stimuli in the pre-recalibration block. For the combined stimuli, the vestibular and visual headings were each offset by Δ/2 (to opposite sides), such that the combined heading was defined in the middle between the vestibular and visual headings. Unlike the pre-recalibration block, monkeys only needed to maintain fixation on the central fixation point during the stimulus presentation and did not need to make choices at the end of trials. They were rewarded for all the trials for which they maintained fixation. 7–10 repetitions were run for each Δ increment, and an additional 10–16 repetitions were run for maximum Δ (±10°).</p></sec><sec id="s4-3-3"><title>Post-recalibration block</title><p>During this block, performance for the individual (visual/vestibular) modalities was once again tested using single-cue trials (as in the pre-recalibration block). Responses to these trials were used to measure recalibration. The single-cue trials were interleaved with combined-cue trials (with a ±10° discrepancy, like the end of the recalibration block, <xref ref-type="fig" rid="fig1">Figure 1C</xref>). The combined-cue trials were interleaved to maintain recalibration while it was measured (for details, see <xref ref-type="bibr" rid="bib72">Zaidel et al., 2011</xref>). To avoid perturbing the recalibrated behavior, we adjusted the reward probability for single-cue trials as follows: if the single-cue heading was of relatively large magnitude, such that, if it were part of a combined-cue trial also the other cue would lie to the same side (right or left), monkeys were rewarded as in the pre-recalibration block (95% probability reward for correct choices; no reward for incorrect choices). If, however, the heading for other modality would have been to the opposite side, the monkeys were rewarded stochastically (70% reward probability, regardless of their choices).</p></sec></sec><sec id="s4-4"><title>Electrophysiological recordings</title><p>We recorded extracellular activity from isolated single neurons in areas MSTd, PIVC, and VIP using tungsten microelectrodes (Frederick Haer Company, Bowdoin, ME, USA; tip diameter ~3 μm; impedance, 1–2 MΩ at 1 kHz). The microelectrode was advanced into the cortex through a transdural guide tube, using a hydraulic microdrive (Frederick Haer Company). Raw neural signals were amplified, band-pass filtered (400–5000 Hz), and digitized at 25 kHz using the AlphaOmega system (AlphaOmega Instruments, Nazareth Illit, Israel). Spikes were sorted online, and spike times along with all behavioral events were collected with 1-ms resolution using the Tempo system. If the online sorting was not adequate, offline spike sorting was performed.</p><p>The target areas (MSTd, PIVC, and VIP) were identified based on the patterns of gray and white matter transitions, magnetic resonance imaging scans, stereotaxic coordinates, and physiological response properties as described previously (MSTd: <xref ref-type="bibr" rid="bib33">Gu et al., 2006</xref>; PIVC: <xref ref-type="bibr" rid="bib18">Chen et al., 2010</xref>; VIP: <xref ref-type="bibr" rid="bib19">Chen et al., 2011a</xref>).</p></sec><sec id="s4-5"><title>Data analysis</title><p>Data analysis was performed with custom scripts in Matlab R2016a (The MathWorks, Natick, MA, USA). Psychometric plots were constructed by fitting the proportion of ‘rightward’ choices as a function of heading angle with a cumulative Gaussian distribution function, using the <italic>psignifit</italic> toolbox for MATLAB (version 2.5.6). Separate psychometric functions were constructed for each cue (visual and vestibular) and block (pre- and post-recalibration). The psychophysical threshold and PSE were defined, respectively, by the standard deviation (SD, <italic>σ</italic>) and mean (<italic>μ</italic>) of the fitted Gaussian function. The PSE represents the heading angle that would be perceived as straight ahead, also known as the ‘bias’. Vestibular and visual recalibration (PSE shift) was calculated for each session by subtracting the pre-recalibration PSE from the post-recalibration PSE:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mi> </mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Neuronal tuning curves were constructed for vestibular and visual cues, pre- and post-recalibration, by calculating the average (baseline subtracted) FR responses, as a function of heading. FR responses were calculated over the duration of stimulus presentation (<italic>t</italic> = 0–1 s), and baseline FRs were calculated (per block) by the average FR in the 1-s window before stimulus onset. A neuron was considered ‘tuned’ if the linear regression of FR responses by heading (over the narrow range of stimuli presented: −16° to 16°) had a significant slope (p &lt; 0.05).</p><p>This selection criterion was selective for neurons that have sloped tuning around straight ahead, and excluded neurons with flat tuning, or a tuning preference, straight ahead. In the cortical areas of interest in this study, a disproportionately large number of neurons have steep tuning slopes around straight ahead (<xref ref-type="bibr" rid="bib20">Chen et al., 2011b</xref>; <xref ref-type="bibr" rid="bib36">Gu et al., 2010</xref>). These neurons are most informative for heading discrimination (large Fisher information, <xref ref-type="bibr" rid="bib36">Gu et al., 2010</xref>). By contrast, neurons with relatively flat tuning around straight ahead are less informative for heading discrimination (low Fisher information). Accordingly, small shifts can be readily detected in neurons with sloped tuning (but not in those with flat tuning) around straight ahead. Therefore, in this study we focused on the prevalent neurons with sloped tuning around straight ahead.</p></sec><sec id="s4-6"><title>Neurometrics</title><p>For each neuron recorded, neurometric curves (per cue and block) were constructed from the FRs (<xref ref-type="bibr" rid="bib21">Chen et al., 2013a</xref>; <xref ref-type="bibr" rid="bib32">Fetsch et al., 2011</xref>; <xref ref-type="bibr" rid="bib35">Gu et al., 2008</xref>; <xref ref-type="bibr" rid="bib34">Gu et al., 2007</xref>). For this, the FRs were first normalized (<italic>z</italic>-scored) by subtracting the pre-recalibration mean, and dividing by the pre-recalibration SD. The same (pre-recalibration) mean and SD values were used to normalize both the pre- and post-recalibration FRs (per cue). A common reference (pre-recalibration mean, corresponding to <italic>z</italic>-score = 0) was needed to expose PSE shifts (calculating neurometric curves by comparing responses to positive vs. corresponding negative headings assumes PSE = 0°).</p><p>Then, for each heading, an ROC (receiver operating characteristic) curve was computed by moving a ‘criterion’ value from the minimum to the maximum <italic>z</italic>-score (in 100 steps), and plotting the probability that the <italic>z</italic>-scores exceeded the criterion vs. whether <italic>z</italic>-score = 0 (the pre-recalibration mean) exceeded that same criterion, or not (1 or 0, respectively). A single point on the ROC curve was produced for each increment in the criterion. The area under the ROC curve reflects the probability that an ideal observer would discriminate the neuronal responses for the given heading to the neuron’s preferred (vs. non-preferred) side (right/left), in relation to the pre-recalibration mean. Then these values were mapped onto the probability of a rightward choice and fitted with a cumulative Gaussian function (similar to perceptual psychometrics).</p></sec><sec id="s4-7"><title>Neuronal shifts</title><p>For subsequent analyses, that is, calculating neurometric shifts (and comparison thereof to perceptual shifts) only neurons that passed both of the following two screening criteria (per cue) were included: (1) significant tuning to the corresponding cue (either pre- or post-recalibration; see Data analysis subsection above for details). (2) Both the pre- and post-recalibration neurometrics produced reliable PSEs (bootstrapped SD of the PSE &lt;10°, both pre- and post-recalibration). The bootstrapped SDs of the PSEs (for the neurons that passed the first criterion, of significant tuning) are presented in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. This resulted in 14 and 59 MSTd neurons for vestibular and visual cues, respectively (<xref ref-type="fig" rid="fig3">Figure 3</xref>); 30 and 10 PIVC neurons for vestibular and visual cues, respectively (<xref ref-type="fig" rid="fig4">Figure 4</xref>); 37 and 42 PIVC neurons for vestibular and visual cues, respectively (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><p>Neuronal shifts were measured by the difference between the post- and pre-recalibration neurometric PSEs (similar to perceptual shifts, see <xref ref-type="disp-formula" rid="equ1">Equation. 1</xref>). For each recording area (MSTd, PIVC, and VIP) and cue (vestibular and visual) neuronal shifts were compared to perceptual shifts, using Pearson correlations (pooling data across monkeys). Additionally, to assess the relationship between neuronal and perceptual shifts, while taking into account the differences of individual monkeys, we used an LMM, which allowed for random effects in slope and intercept for the different monkeys. The goodness of fit was assessed for the LMM and the pooled model (which did not take into account differences of individual monkeys) using AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) (<xref ref-type="bibr" rid="bib66">Vrieze, 2012</xref>). The LMM did not provide better fits vs. the pooled model, and the results (fixed effects) remained similar compared to the pooled model (<xref ref-type="supplementary-material" rid="fig3sdata2">Figure 3—source data 2</xref>, <xref ref-type="supplementary-material" rid="fig4sdata2">Figure 4—source data 2</xref>, and <xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref>).</p><p>To measure neuronal shifts at different time points during the stimulus, we calculated neurometric shifts based on FRs in narrow (200 ms) windows, in increments of 100 ms. The time index (the center of the window) ranged from <italic>t</italic> = 0.1 s to <italic>t</italic> = 1.2 s (relative to stimulus onset). This range did not include the choice saccade, which could only take place after <italic>t</italic> = 1.3 s because of the delay period (300 ms) between the offset of the stimulus and the onset of the saccade targets. All neurons that passed both of the screening criteria (described above) were included in this analysis.</p></sec><sec id="s4-8"><title>Partial correlation analysis</title><p>To disassociate the unique contributions of heading stimuli and choices to the neural responses (FRs) we computed Pearson partial correlations between these variables (for details, see <xref ref-type="bibr" rid="bib25">Chen et al., 2021</xref>; <xref ref-type="bibr" rid="bib74">Zaidel et al., 2017</xref>). This produced: (1) a heading partial correlation (<italic>R</italic><sub>h</sub>) that captured the linear relationship between FRs and headings, given the monkey’s choices, and (2) a choice partial correlation (<italic>R</italic><sub>c</sub>) that captured the linear relationship between FRs and choices, given the stimulus headings. Partial correlations were calculated based on data acquired over the entire stimulus duration. Positive (negative) heading partial correlations indicate that FRs were greater (smaller) for rightward vs. leftward headings (given the choices). Likewise, positive (negative) choice partial correlations indicate that FRs were greater (smaller) for rightward vs. leftward choices (given the stimulus headings).</p></sec><sec id="s4-9"><title>Statistical analysis</title><p>To evaluate differences in monkey behavior (PSE), heading, or choice partial correlations, between pre- and post-recalibration, we used two-tailed paired <italic>t</italic>-tests. Possible differences in spontaneous (baseline) FRs between pre- and post-recalibration were evaluated using Bayesian paired-samples <italic>t</italic>-tests (BF<sub>10</sub> values). Relationships between neuronal and perceptual shifts were tested using Pearson’s correlation coefficients and LMMs. Statistical analysis was conducted using JASP (Version 0.16.3) and R (Version 4.2.2).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Formal analysis, Validation, Visualization, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Supervision, Funding acquisition, Validation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Formal analysis, Supervision, Funding acquisition, Investigation, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All animal surgeries and experimental procedures were approved by the Institutional Animal Care and Use Committee at East China Normal University (IACUC protocol number: Mo20200101).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-82895-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The data and analysis code for this study have been uploaded to Github and can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/FuZengBio/Recalibration">https://github.com/FuZengBio/Recalibration</ext-link> (copy archived at <xref ref-type="bibr" rid="bib76">Zeng, 2022</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by grants from the 'STI2030-major projects' (No. 2021ZD0202600), the National Basic Research Program of China (No. 32171034) to AC, and the ISF-NSFC joint research program to AC (No. 32061143003) and AZ (No. 3318/20). We thank Prof. Dora Angelaki for the helpful comments. We are also grateful to Minhu Chen for outstanding computer programming.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdulkarim</surname><given-names>Z</given-names></name><name><surname>Hayatou</surname><given-names>Z</given-names></name><name><surname>Ehrsson</surname><given-names>HH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Sustained rubber hand illusion after the end of visuotactile stimulation with a similar time course for the reduction of subjective ownership and proprioceptive drift</article-title><source>Experimental Brain Research</source><volume>239</volume><fpage>3471</fpage><lpage>3486</lpage><pub-id pub-id-type="doi">10.1007/s00221-021-06211-8</pub-id><pub-id pub-id-type="pmid">34524490</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amedi</surname><given-names>A</given-names></name><name><surname>Jacobson</surname><given-names>G</given-names></name><name><surname>Hendler</surname><given-names>T</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Zohary</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Convergence of visual and tactile shape processing in the human lateral occipital complex</article-title><source>Cerebral Cortex</source><volume>12</volume><fpage>1202</fpage><lpage>1212</lpage><pub-id pub-id-type="doi">10.1093/cercor/12.11.1202</pub-id><pub-id pub-id-type="pmid">12379608</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atkins</surname><given-names>JE</given-names></name><name><surname>Jacobs</surname><given-names>RA</given-names></name><name><surname>Knill</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Experience-dependent visual cue recalibration based on discrepancies between visual and haptic percepts</article-title><source>Vision Research</source><volume>43</volume><fpage>2603</fpage><lpage>2613</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(03)00470-x</pub-id><pub-id pub-id-type="pmid">14552802</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avillac</surname><given-names>M</given-names></name><name><surname>Olivier</surname><given-names>E</given-names></name><name><surname>Den</surname><given-names>S</given-names></name><name><surname>Ben Hamed</surname><given-names>S</given-names></name><name><surname>Duhamel</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Multisensory integration in multiple reference frames in the posterior parietal cortex</article-title><source>Cognitive Processing</source><volume>5</volume><fpage>159</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1007/s10339-004-0021-3</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avillac</surname><given-names>M</given-names></name><name><surname>Denève</surname><given-names>S</given-names></name><name><surname>Olivier</surname><given-names>E</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Duhamel</surname><given-names>J-R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Reference frames for representing visual and tactile locations in parietal cortex</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>941</fpage><lpage>949</lpage><pub-id pub-id-type="doi">10.1038/nn1480</pub-id><pub-id pub-id-type="pmid">15951810</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bertelson</surname><given-names>P</given-names></name><name><surname>De Gelder</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2004">2004</year><chapter-title>The psychology of multimodal perception</chapter-title><person-group person-group-type="editor"><name><surname>Charles</surname><given-names>Spence</given-names></name><name><surname>Jon</surname><given-names>Driver</given-names></name></person-group><source>Crossmodal Space and Crossmodal Attention</source><publisher-name>Oxford University Press</publisher-name><fpage>141</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1093/acprof:oso/9780198524861.001.0001</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Rubber hands “feel” touch that eyes see</article-title><source>Nature</source><volume>391</volume><elocation-id>756</elocation-id><pub-id pub-id-type="doi">10.1038/35784</pub-id><pub-id pub-id-type="pmid">9486643</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>MS</given-names></name><name><surname>Knudsen</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Experience-dependent plasticity in the inferior colliculus: A site for visual calibration of the neural representation of auditory space in the barn owl</article-title><source>The Journal of Neuroscience</source><volume>13</volume><fpage>4589</fpage><lpage>4608</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.13-11-04589.1993</pub-id><pub-id pub-id-type="pmid">8229186</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bremmer</surname><given-names>F</given-names></name><name><surname>Klam</surname><given-names>F</given-names></name><name><surname>Duhamel</surname><given-names>JR</given-names></name><name><surname>Ben Hamed</surname><given-names>S</given-names></name><name><surname>Graf</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Visual-Vestibular interactive responses in the macaque ventral intraparietal area (VIP)</article-title><source>The European Journal of Neuroscience</source><volume>16</volume><fpage>1569</fpage><lpage>1586</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2002.02206.x</pub-id><pub-id pub-id-type="pmid">12405971</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>van Wezel</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Electrical microstimulation of cortical area MST biases heading perception in monkeys</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>59</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1038/259</pub-id><pub-id pub-id-type="pmid">10195110</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Van Wezel</surname><given-names>RJA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Area MST and heading perception in macaque monkeys</article-title><source>Cerebral Cortex</source><volume>12</volume><fpage>692</fpage><lpage>701</lpage><pub-id pub-id-type="doi">10.1093/cercor/12.7.692</pub-id><pub-id pub-id-type="pmid">12050081</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mechanisms of self-motion perception</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>389</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.112953</pub-id><pub-id pub-id-type="pmid">18558861</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname><given-names>J</given-names></name><name><surname>Girshick</surname><given-names>AR</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Visual-haptic adaptation is determined by relative reliability</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>7714</fpage><lpage>7721</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6427-09.2010</pub-id><pub-id pub-id-type="pmid">20519546</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bush</surname><given-names>G</given-names></name><name><surname>Luu</surname><given-names>P</given-names></name><name><surname>Posner</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Cognitive and emotional influences in anterior cingulate cortex</article-title><source>Trends in Cognitive Sciences</source><volume>4</volume><fpage>215</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(00)01483-2</pub-id><pub-id pub-id-type="pmid">10827444</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butler</surname><given-names>JS</given-names></name><name><surname>Smith</surname><given-names>ST</given-names></name><name><surname>Campos</surname><given-names>JL</given-names></name><name><surname>Bülthoff</surname><given-names>HH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Bayesian integration of visual and vestibular signals for heading</article-title><source>Journal of Vision</source><volume>10</volume><elocation-id>23</elocation-id><pub-id pub-id-type="doi">10.1167/10.11.23</pub-id><pub-id pub-id-type="pmid">20884518</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butler</surname><given-names>JS</given-names></name><name><surname>Campos</surname><given-names>JL</given-names></name><name><surname>Bülthoff</surname><given-names>HH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Optimal visual-vestibular integration under conditions of conflicting intersensory motion profiles</article-title><source>Experimental Brain Research</source><volume>233</volume><fpage>587</fpage><lpage>597</lpage><pub-id pub-id-type="doi">10.1007/s00221-014-4136-1</pub-id><pub-id pub-id-type="pmid">25361642</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canon</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>Intermodality inconsistency of input and directed attention as determinants of the nature of adaptation</article-title><source>Journal of Experimental Psychology</source><volume>84</volume><fpage>141</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1037/h0028925</pub-id><pub-id pub-id-type="pmid">5480918</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Macaque parieto-insular vestibular cortex: Responses to self-motion and optic flow</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>3022</fpage><lpage>3042</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4029-09.2010</pub-id><pub-id pub-id-type="pmid">20181599</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>A comparison of vestibular spatiotemporal tuning in macaque parietoinsular vestibular cortex, ventral intraparietal area, and medial superior temporal area</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>3082</fpage><lpage>3094</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4476-10.2011</pub-id><pub-id pub-id-type="pmid">21414929</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>Representation of vestibular and visual cues to self-motion in ventral intraparietal cortex</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>12036</fpage><lpage>12052</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0395-11.2011</pub-id><pub-id pub-id-type="pmid">21849564</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>Functional specializations of the ventral intraparietal area for multisensory heading discrimination</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>3567</fpage><lpage>3581</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4522-12.2013</pub-id><pub-id pub-id-type="pmid">23426684</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>Diverse spatial reference frames of vestibular signals in parietal cortex</article-title><source>Neuron</source><volume>80</volume><fpage>1310</fpage><lpage>1321</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.006</pub-id><pub-id pub-id-type="pmid">24239126</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Evidence for a causal contribution of macaque vestibular, but not intraparietal, cortex to heading perception</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>3789</fpage><lpage>3798</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2485-15.2016</pub-id><pub-id pub-id-type="pmid">27030763</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Flexible egocentric and allocentric representations of heading signals in parietal cortex</article-title><source>PNAS</source><volume>115</volume><fpage>E3305</fpage><lpage>E3312</lpage><pub-id pub-id-type="doi">10.1073/pnas.1715625115</pub-id><pub-id pub-id-type="pmid">29555744</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>Zeng</surname><given-names>F</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Dynamics of heading and choice-related signals in the parieto-insular vestibular cortex of macaque monkeys</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>3254</fpage><lpage>3265</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2275-20.2021</pub-id><pub-id pub-id-type="pmid">33622780</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colby</surname><given-names>CL</given-names></name><name><surname>Duhamel</surname><given-names>JR</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Ventral intraparietal area of the macaque: Anatomic location and visual response properties</article-title><source>Journal of Neurophysiology</source><volume>69</volume><fpage>902</fpage><lpage>914</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.69.3.902</pub-id><pub-id pub-id-type="pmid">8385201</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Winkel</surname><given-names>KN</given-names></name><name><surname>Weesie</surname><given-names>J</given-names></name><name><surname>Werkhoven</surname><given-names>PJ</given-names></name><name><surname>Groen</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Integration of visual and inertial cues in perceived heading of self-motion</article-title><source>Journal of Vision</source><volume>10</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/10.12.1</pub-id><pub-id pub-id-type="pmid">21047733</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dokka</surname><given-names>K</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Multisensory integration of visual and vestibular signals improves heading discrimination in the presence of a moving object</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>13599</fpage><lpage>13607</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2267-15.2015</pub-id><pub-id pub-id-type="pmid">26446214</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duffy</surname><given-names>CJ</given-names></name><name><surname>Wurtz</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Response of monkey MST neurons to optic flow stimuli with shifted centers of motion</article-title><source>The Journal of Neuroscience</source><volume>15</volume><fpage>5192</fpage><lpage>5208</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.15-07-05192.1995</pub-id><pub-id pub-id-type="pmid">7623145</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duhamel</surname><given-names>JR</given-names></name><name><surname>Colby</surname><given-names>CL</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Ventral intraparietal area of the macaque: congruent visual and somatic response properties</article-title><source>Journal of Neurophysiology</source><volume>79</volume><fpage>126</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.79.1.126</pub-id><pub-id pub-id-type="pmid">9425183</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fetsch</surname><given-names>CR</given-names></name><name><surname>Turner</surname><given-names>AH</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Dynamic reweighting of visual and vestibular cues during self-motion perception</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>15601</fpage><lpage>15612</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2574-09.2009</pub-id><pub-id pub-id-type="pmid">20007484</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fetsch</surname><given-names>CR</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural correlates of reliability-based cue weighting during multisensory integration</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>146</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1038/nn.2983</pub-id><pub-id pub-id-type="pmid">22101645</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Watkins</surname><given-names>PV</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Visual and nonvisual contributions to three-dimensional heading selectivity in the medial superior temporal area</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>73</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2356-05.2006</pub-id><pub-id pub-id-type="pmid">16399674</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A functional link between area mstd and heading perception based on vestibular signals</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1038</fpage><lpage>1047</lpage><pub-id pub-id-type="doi">10.1038/nn1935</pub-id><pub-id pub-id-type="pmid">17618278</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural correlates of multisensory cue integration in macaque mstd</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1201</fpage><lpage>1210</lpage><pub-id pub-id-type="doi">10.1038/nn.2191</pub-id><pub-id pub-id-type="pmid">18776893</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Fetsch</surname><given-names>CR</given-names></name><name><surname>Adeyemo</surname><given-names>B</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Decoding of mstd population activity accounts for variations in the precision of heading perception</article-title><source>Neuron</source><volume>66</volume><fpage>596</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.04.026</pub-id><pub-id pub-id-type="pmid">20510863</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Causal links between dorsal medial superior temporal area neurons and multisensory heading perception</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>2299</fpage><lpage>2313</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5154-11.2012</pub-id><pub-id pub-id-type="pmid">22396405</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Vestibular signals in primate cortex for self-motion perception</article-title><source>Current Opinion in Neurobiology</source><volume>52</volume><fpage>10</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.04.004</pub-id><pub-id pub-id-type="pmid">29694922</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Held</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>Exposure-history as a factor in maintaining stability of perception and coordination</article-title><source>The Journal of Nervous and Mental Disease</source><volume>132</volume><fpage>26</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1097/00005053-196101000-00005</pub-id><pub-id pub-id-type="pmid">13713070</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holroyd</surname><given-names>CB</given-names></name><name><surname>Coles</surname><given-names>MGH</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The neural basis of human error processing: Reinforcement learning, dopamine, and the error-related negativity</article-title><source>Psychological Review</source><volume>109</volume><fpage>679</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.109.4.679</pub-id><pub-id pub-id-type="pmid">12374324</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennett</surname><given-names>S</given-names></name><name><surname>Taylor-Clarke</surname><given-names>M</given-names></name><name><surname>Haggard</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Noninformative vision improves the spatial resolution of touch in humans</article-title><source>Current Biology</source><volume>11</volume><fpage>1188</fpage><lpage>1191</lpage><pub-id pub-id-type="doi">10.1016/s0960-9822(01)00327-x</pub-id><pub-id pub-id-type="pmid">11516950</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knudsen</surname><given-names>EI</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Visual instruction of the neural map of auditory space in the developing optic tectum</article-title><source>Science</source><volume>253</volume><fpage>85</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1126/science.2063209</pub-id><pub-id pub-id-type="pmid">2063209</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knudsen</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Instructed learning in the auditory localization pathway of the barn owl</article-title><source>Nature</source><volume>417</volume><fpage>322</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1038/417322a</pub-id><pub-id pub-id-type="pmid">12015612</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kramer</surname><given-names>A</given-names></name><name><surname>Röder</surname><given-names>B</given-names></name><name><surname>Bruns</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Feedback modulates audio-visual spatial recalibration</article-title><source>Frontiers in Integrative Neuroscience</source><volume>13</volume><elocation-id>74</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2019.00074</pub-id><pub-id pub-id-type="pmid">32009913</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewald</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Rapid adaptation to auditory-visual spatial disparity</article-title><source>Learning &amp; Memory</source><volume>9</volume><fpage>268</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1101/lm.51402</pub-id><pub-id pub-id-type="pmid">12359836</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linkenhoker</surname><given-names>BA</given-names></name><name><surname>Knudsen</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Incremental training increases the plasticity of the auditory space map in adult barn owls</article-title><source>Nature</source><volume>419</volume><fpage>293</fpage><lpage>296</lpage><pub-id pub-id-type="doi">10.1038/nature01002</pub-id><pub-id pub-id-type="pmid">12239566</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markov</surname><given-names>DA</given-names></name><name><surname>Petrucco</surname><given-names>L</given-names></name><name><surname>Kist</surname><given-names>AM</given-names></name><name><surname>Portugues</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A cerebellar internal model calibrates a feedback controller involved in sensorimotor control</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>6694</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-26988-0</pub-id><pub-id pub-id-type="pmid">34795244</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maunsell</surname><given-names>JH</given-names></name><name><surname>van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The connections of the middle temporal visual area (MT) and their relationship to a cortical hierarchy in the macaque monkey</article-title><source>The Journal of Neuroscience</source><volume>3</volume><fpage>2563</fpage><lpage>2586</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.03-12-02563.1983</pub-id><pub-id pub-id-type="pmid">6655500</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oman</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Motion sickness: A synthesis and evaluation of the sensory conflict theory</article-title><source>Canadian Journal of Physiology and Pharmacology</source><volume>68</volume><fpage>294</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1139/y90-044</pub-id><pub-id pub-id-type="pmid">2178753</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The neurophysiological basis of the trial-wise and cumulative ventriloquism aftereffects</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>1068</fpage><lpage>1079</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2091-20.2020</pub-id><pub-id pub-id-type="pmid">33273069</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radeau</surname><given-names>M</given-names></name><name><surname>Bertelson</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>The after-effects of ventriloquism</article-title><source>The Quarterly Journal of Experimental Psychology</source><volume>26</volume><fpage>63</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1080/14640747408400388</pub-id><pub-id pub-id-type="pmid">4814864</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Reason</surname><given-names>JT</given-names></name><name><surname>Brand</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="1975">1975</year><source>Motion Sickness</source><publisher-loc>Cambridge, Massachusetts</publisher-loc><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Recanzone</surname><given-names>GH</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Rapidly induced auditory plasticity: The ventriloquism aftereffect</article-title><source>PNAS</source><volume>95</volume><fpage>869</fpage><lpage>875</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.3.869</pub-id><pub-id pub-id-type="pmid">9448253</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rock</surname><given-names>I</given-names></name><name><surname>Victor</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>VISION and touch: An experimentally created conflict between the two senses</article-title><source>Science</source><volume>143</volume><fpage>594</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1126/science.143.3606.594</pub-id><pub-id pub-id-type="pmid">14080333</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rondi-Reig</surname><given-names>L</given-names></name><name><surname>Paradis</surname><given-names>AL</given-names></name><name><surname>Lefort</surname><given-names>JM</given-names></name><name><surname>Babayan</surname><given-names>BM</given-names></name><name><surname>Tobin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>How the cerebellum may monitor sensory information for spatial representation</article-title><source>Frontiers in Systems Neuroscience</source><volume>8</volume><elocation-id>205</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2014.00205</pub-id><pub-id pub-id-type="pmid">25408638</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlack</surname><given-names>A</given-names></name><name><surname>Hoffmann</surname><given-names>KP</given-names></name><name><surname>Bremmer</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Interaction of linear vestibular and visual stimulation in the macaque ventral intraparietal area (VIP)</article-title><source>The European Journal of Neuroscience</source><volume>16</volume><fpage>1877</fpage><lpage>1886</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2002.02251.x</pub-id><pub-id pub-id-type="pmid">12453051</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlack</surname><given-names>A</given-names></name><name><surname>Sterbing-D’Angelo</surname><given-names>SJ</given-names></name><name><surname>Hartung</surname><given-names>K</given-names></name><name><surname>Hoffmann</surname><given-names>K-P</given-names></name><name><surname>Bremmer</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Multisensory space representations in the macaque ventral intraparietal area</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>4616</fpage><lpage>4625</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0455-05.2005</pub-id><pub-id pub-id-type="pmid">15872109</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The timing and laminar profile of converging inputs to multisensory areas of the macaque neocortex</article-title><source>Brain Research. Cognitive Brain Research</source><volume>14</volume><fpage>187</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1016/s0926-6410(02)00073-3</pub-id><pub-id pub-id-type="pmid">12063142</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Huang</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Multisensory maps in parietal cortex</article-title><source>Current Opinion in Neurobiology</source><volume>24</volume><fpage>39</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.08.014</pub-id><pub-id pub-id-type="pmid">24492077</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shupak</surname><given-names>A</given-names></name><name><surname>Gordon</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Motion sickness: Advances in pathogenesis, prediction, prevention, and treatment</article-title><source>Aviation, Space, and Environmental Medicine</source><volume>77</volume><fpage>1213</fpage><lpage>1223</lpage><pub-id pub-id-type="pmid">17183916</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>Stanford</surname><given-names>TR</given-names></name><name><surname>Rowland</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Development of multisensory integration from the perspective of the individual neuron</article-title><source>Nature Reviews. Neuroscience</source><volume>15</volume><fpage>520</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1038/nrn3742</pub-id><pub-id pub-id-type="pmid">25158358</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thériault</surname><given-names>R</given-names></name><name><surname>Landry</surname><given-names>M</given-names></name><name><surname>Raz</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The rubber hand illusion: Top-down attention modulates embodiment</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>75</volume><fpage>2129</fpage><lpage>2148</lpage><pub-id pub-id-type="doi">10.1177/17470218221078858</pub-id><pub-id pub-id-type="pmid">35073801</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsakiris</surname><given-names>M</given-names></name><name><surname>Haggard</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The rubber hand illusion revisited: Visuotactile integration and self-attribution</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>31</volume><fpage>80</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.31.1.80</pub-id><pub-id pub-id-type="pmid">15709864</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Cortical connections of visual area MT in the macaque</article-title><source>The Journal of Comparative Neurology</source><volume>248</volume><fpage>190</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1002/cne.902480204</pub-id><pub-id pub-id-type="pmid">3722458</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Beers</surname><given-names>RJ</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Haggard</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>When feeling is more important than seeing in sensorimotor adaptation</article-title><source>Current Biology</source><volume>12</volume><fpage>834</fpage><lpage>837</lpage><pub-id pub-id-type="doi">10.1016/s0960-9822(02)00836-9</pub-id><pub-id pub-id-type="pmid">12015120</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vrieze</surname><given-names>SI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Model selection and psychological theory: A discussion of the differences between the akaike information criterion (AIc) and the Bayesian information criterion (Bic)</article-title><source>Psychological Methods</source><volume>17</volume><fpage>228</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1037/a0027127</pub-id><pub-id pub-id-type="pmid">22309957</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>WH</given-names></name><name><surname>Morris</surname><given-names>MW</given-names></name><name><surname>Kalish</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Perception of translational heading from optical flow</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>14</volume><fpage>646</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.14.4.646</pub-id><pub-id pub-id-type="pmid">2974874</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname><given-names>DM</given-names></name><name><surname>Akeroyd</surname><given-names>MA</given-names></name><name><surname>Roach</surname><given-names>NW</given-names></name><name><surname>Webb</surname><given-names>BS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Multiple spatial reference frames underpin perceptual recalibration to audio-visual discrepancies</article-title><source>PLOS ONE</source><volume>16</volume><elocation-id>e0251827</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0251827</pub-id><pub-id pub-id-type="pmid">33999940</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webster</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Evolving concepts of sensory adaptation</article-title><source>F1000 Biology Reports</source><volume>4</volume><elocation-id>21</elocation-id><pub-id pub-id-type="doi">10.3410/B4-21</pub-id><pub-id pub-id-type="pmid">23189092</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurtz</surname><given-names>RH</given-names></name><name><surname>Duffy</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Neuronal correlates of optic flow stimulation</article-title><source>Annals of the New York Academy of Sciences</source><volume>656</volume><fpage>205</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.1992.tb25210.x</pub-id><pub-id pub-id-type="pmid">1599144</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>X</given-names></name><name><surname>Gu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Probing sensory readout via combined choice-correlation measures and microstimulation perturbation</article-title><source>Neuron</source><volume>100</volume><fpage>715</fpage><lpage>727</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.08.034</pub-id><pub-id pub-id-type="pmid">30244884</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaidel</surname><given-names>A</given-names></name><name><surname>Turner</surname><given-names>AH</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multisensory calibration is independent of cue reliability</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>13949</fpage><lpage>13962</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2732-11.2011</pub-id><pub-id pub-id-type="pmid">21957256</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaidel</surname><given-names>A</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Supervised calibration relies on the multisensory percept</article-title><source>Neuron</source><volume>80</volume><fpage>1544</fpage><lpage>1557</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.026</pub-id><pub-id pub-id-type="pmid">24290205</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaidel</surname><given-names>A</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Decoupled choice-driven and stimulus-related activity in parietal neurons may be misrepresented by choice probabilities</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>715</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-00766-3</pub-id><pub-id pub-id-type="pmid">28959018</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaidel</surname><given-names>A</given-names></name><name><surname>Laurens</surname><given-names>J</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Supervised multisensory calibration signals are evident in VIP but not mstd</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>10108</fpage><lpage>10119</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0135-21.2021</pub-id><pub-id pub-id-type="pmid">34716232</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Recalibration</data-title><version designator="swh:1:rev:b0ed5b1149e31138787adb56e323dc24be678961">swh:1:rev:b0ed5b1149e31138787adb56e323dc24be678961</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:45cdf7288075bf2f9d2fe4b7d4aed210f7ae2e5f;origin=https://github.com/FuZengBio/Recalibration;visit=swh:1:snp:15baeb811aba265d34808d53aea1f26ef73865ec;anchor=swh:1:rev:b0ed5b1149e31138787adb56e323dc24be678961">https://archive.softwareheritage.org/swh:1:dir:45cdf7288075bf2f9d2fe4b7d4aed210f7ae2e5f;origin=https://github.com/FuZengBio/Recalibration;visit=swh:1:snp:15baeb811aba265d34808d53aea1f26ef73865ec;anchor=swh:1:rev:b0ed5b1149e31138787adb56e323dc24be678961</ext-link></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Heuer</surname><given-names>HW</given-names></name><name><surname>Britten</surname><given-names>KH</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Parietal area VIP neuronal responses to heading stimuli are encoded in head-centered coordinates</article-title><source>Neuron</source><volume>42</volume><fpage>993</fpage><lpage>1001</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.06.008</pub-id><pub-id pub-id-type="pmid">15207243</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Encoding of vestibular and optic flow cues to self-motion in the posterior superior temporal polysensory area</article-title><source>The Journal of Physiology</source><volume>599</volume><fpage>3937</fpage><lpage>3954</lpage><pub-id pub-id-type="doi">10.1113/JP281913</pub-id><pub-id pub-id-type="pmid">34192812</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zierul</surname><given-names>B</given-names></name><name><surname>Röder</surname><given-names>B</given-names></name><name><surname>Tempelmann</surname><given-names>C</given-names></name><name><surname>Bruns</surname><given-names>P</given-names></name><name><surname>Noesselt</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The role of auditory cortex in the spatial ventriloquism aftereffect</article-title><source>NeuroImage</source><volume>162</volume><fpage>257</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.09.002</pub-id><pub-id pub-id-type="pmid">28889003</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82895.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Fetsch</surname><given-names>Christopher R</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Johns Hopkins University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.09.26.509476" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.09.26.509476"/></front-stub><body><p>This important study combines quantitative behavior and single-unit recordings in nonhuman primates to investigate the role of three cortical areas in cross-modal sensory calibration, a form of neural plasticity that is important for perception and learning. The results convincingly demonstrate key similarities and striking differences across the three areas and provide the first evidence for this form of calibration (in correspondence with behavior) at the level of single neurons. The work will be of broad interest to neuroscientists and psychologists studying multisensory perception, plasticity, and the role of sensory and association cortices in perceptual decisions.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82895.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Fetsch</surname><given-names>Christopher R</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Johns Hopkins University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Olcese</surname><given-names>Umberto</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dkp9463</institution-id><institution>University of Amsterdam</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Goris</surname><given-names>Robbe</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>The University of Texas at Austin</institution></institution-wrap><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.09.26.509476">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.09.26.509476v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Contrary neuronal recalibration in different multisensory cortical areas&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Michael Frank as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Umberto Olcese (Reviewer #2); Robbe Goris (Reviewer #3).</p><p>The reviewers' assessment of the work is overall quite positive; congratulations on an interesting and potentially impactful study. The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Please report summary statistics (means and correlation coefficients) and p values for individual animals, for the data in Figure 2C, 3B, 4B, and 5B. This is not to say that the results must be significant at the individual animal level in order to support the study's main conclusions; we are all aware of the practicalities and accepted conventions of the field. Yet we feel it is important to be up front about this limitation, if indeed some of the individual monkey results fail to reach significance.</p><p>To be clear, the reasoning behind this request is that repeated observations from the same subject/condition are not independent of each other, thus correlations calculated from pooled data might obscure, inflate, or even reverse the true relationship between the variables (e.g., Simpson's Paradox).</p><p>Apart from individual subject analyses, another approach (indeed our strong recommendation) is to perform a hierarchical analysis. There are multiple options for this; probably the easiest one is to calculate linear mixed regression models (LMM) with one variable as predictor, the other as outcome and no intercept. This can be done in the frequentist way using (for example) the lmer package in R, or in a Bayesian way using Stan, which even as an automatized module for GLMMs. Partial correlations can be achieved by adding the variable that is partialized out as a predictor.</p><p>In summary, the authors may choose one (or more) of the three mentioned approaches to enhance statistical rigor: (1) separate t-tests and correlations for each animal and condition, (2) frequentist hierarchical linear models, and (3) Bayesian hierarchical linear models. It is worth pointing out that Bayesian approaches have advantages over frequentist methods, for instance quantifying the evidence for the null hypothesis (and thus better evaluating negative results such as Figure 4B-right).</p><p>2) Please consider applying a goodness-of-fit criterion to the neurometric curves before inclusion of their PSEs in the neuron-behavior correlation analyses – AND/OR evaluate the reliability of the PSEs using standard error obtained from the fitting procedure, or a bootstrap-based confidence interval. We would not require individual neuron shifts (δ-PSE) to be significant according to such SEs (i.e., through error propagation), but a reanalysis after removing particularly poor fits seems appropriate. The criterion to use for this is difficult for us to specify and thus can use your best judgment.</p><p>3) Depending on how the first two points are handled, and the outcome thereof, it may be necessary to scrutinize the role of outliers in generating the correlation coefficients and p values obtained. For instance, is the correlation of Figure 3B-left still significant without the upper four points, and 3B-right without the rightmost three points? A hierarchical analysis and/or neurometric goodness-of-fit criterion could reduce the role of outliers, in which case no formal outlier correction or other way to address this is needed.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The results are really interesting, yet, the manuscript in its current form needs revisions along two dimensions, 1) data analysis and 2) writing.</p><p>Methods</p><p>I am aware that basically all analyses in this manuscript have been used in published papers. The problems outlined below hold for those papers, too, sorry. Bad (or good) luck having this time a reviewer from another field.</p><p>Correlations derived from data that includes multiple repetitions per subject (and condition) require a hierarchical analysis because repeated observations from the same observer are not independent of each other. Subject A might on average have higher shifts (behavioral and neurometric) than subject B. In that case, a non-hierarchical analysis might return a significant positive correlation even if within subject A negative behavioral shifts are associated with positive neurometric shifts and vice versa. There are multiple ways to account for intra-subject correlations, probably the easiest one is to calculate linear mixed regression models (LMM) with one variable as predictor, the other as outcome and no intercept. This can be done in the frequentist way using the lmer package in R and in a Bayesian way using Stan which even as an automatized module for GLMMs. Partial correlations can be achieved by adding the variable that is partialized out as a predictor. Of course, it is also possible to just analyze each subject separately.</p><p>The same holds for any comparisons of mean values (across heading offsets during the recalibration phase or across modalities). For example, a t-test that includes perceptual shifts from all sessions and all monkeys is not valid as the values from a single monkey are correlated. Again, there are several ways to account for this. For example, an LMM with cross-modal discrepancy as predictor (and a free intercept) or each monkey's data could be tested separately.</p><p>The procedure to derive neurometric curves, which are central to the study, should be explained better. In the main text and in the figures, it should be clarified that each data point shows the proportion of trials in which an ideal observer would make a rightwards choice given only the firing rates of the neurons (and assumed anti-neurons). In the methods section, it should be explained in detail how the ROC curve is derived. Readers unfamiliar with the method should understand that the ROC incorporates decisions for a range of decision thresholds and that the area under the ROC curve (AUROC, not ROC value) corresponds to a theoretical observer's ability to discriminate between leftwards and rightwards motion given firing rates across repetitions of a specific motion direction. Finally, those AUROC values (ranging from 0.5 to 1) are mapped onto the probability of a rightwards choice (ranging from 0 to 1) given a heading direction.</p><p>The use of PSEs for the correlational analysis should be conditional on the goodness of fit of the sigmoid and the SD should fall within a reasonable range. A PSE derived from an ill-fitting or very flat curve has no informational value and is likely to be extreme, which in turn is huge a problem for any correlational analysis.</p><p>The methods section states that only neurons tuned to heading directions as indicated by cues in a specific modality were included in the neurometric analysis for that modality. If that exclusion criterion was applied, why are the neurometric curves for 'the other modality' so bad? I would have expected that neurons tuned to visual heading cues result in a good neurometric function for visual heading no matter where these neurons are located in the brain. In turn, I would have expected flat neurometric functions for visual heading direction in PIVC if also neurons tuned to vestibular heading information were included in the analysis. What am I missing?</p><p>Another instance of me probably not getting something: to identify neurons tuned for heading directions as cued by one of the two modalities, spike rates were regressed onto heading directions and only those neurons with a significant slope were included. I get that the range of tested headings is smaller than the width of a typical tuning curve and thus a linear regression makes more sense than trying to fit a full tuning curve. However, it seems to me that his selection method excludes neurons tuned to straight-ahead as their slope should be flat for a symmetric range of headings.</p><p>Regression and correlational analyses are extremely sensitive to outliers. To be sure that the results are robust, please repeat the analyses after outlier correction, e.g., based on the 1.5xIQR rule along each dimension or based on each data points influence on the regression (Cook's distance).</p><p>Looking at the currently depicted sigmoids, I suggest increasing the maximal value for the lapse rate. Even though it is not fully clear to me what a lapse rate of neurometric curve actually means, achieving a better fit for the curves seems essential given that the PSEs are at the center of the results.</p><p>The methods section provides information about the number of neurons per area included in the analyses in general. Additional information about the number of neurons per neurometric curve would be useful.</p><p>Please add in l. 698 how the baseline activity for each neuron was determined, e.g., based on a single interval at the beginning of the session or based on recordings in between trials.</p><p>Probably just a typo but in l. 689 Pearson's correlation has nothing to do with linear regression.</p><p>Writing</p><p>Generally, I find the manuscript to be well-written and organized. However, in its current form, the manuscript is geared towards a small audience, electrophysiologists familiar with most publications by the Angelaki and DeAngelis labs. A much wider audience could be reached by 1) phrasing more precisely and 2) providing more information, reasoning, and explanations.</p><p>The verb 'recalibrate' is often used in a way that doesn't match the literature and the way recalibration is thought of. Information cannot be recalibrated; cues cannot recalibrate actively; responses do not recalibrate (together). A system is recalibrated and then it interprets incoming information differently. In most instances, a simple replacement of recalibrated with &quot;shifted&quot; will help -- AND/OR include a statement early on defining these more colloquial uses of recalibrate (e.g., &quot;we refer to this pattern of neural activity as 'recalibration'…)</p><p>A few examples for more common and precise phrasing:</p><p>l. 35 -&gt; recalibrating itself based on information from …</p><p>l. 36 -&gt; estimates for subsequently presented unisensory stimuli are shifted towards each other.</p><p>l. 25 -&gt; the tuning of neural responses to … cues was shifted in the same direction as the monkeys' perceptual judgments of subsequently presented unisensory stimuli.</p><p>l. 43 -&gt; tuning of vestibular neurons was shifted in the same direction as vestibular heading perception.</p><p>This issue is present throughout the manuscript but especially so in the abstract, in brief section, and the discussion. I doubt that someone not familiar with the literature can understand the abstract. There is a section in the discussion (l. 483f) that is written in a very abstract manner but phrased according to the literature, i.e., in accordance with the ways most people think about recalibration. Please adjust the rest of the text accordingly.</p><p>Comments in chronological order:</p><p>l. 57 Behavioral papers cited in a sentence about the neuronal basis of multisensory integration.</p><p>l. 76 Just saying that results exist is rather unusual. In a few words, what did the neuroimaging studies find?</p><p>l. 91 Burge et al. is not about heading perception but about visual-haptic slant perception.</p><p>l. 103 Again, the typical phrasing would be that perception was recalibrated as indicated by shifts in subsequent perceptual estimates.</p><p>l. 108f It would be easier for readers to learn about supervised recalibration in the discussion, as now they have to shift mentally from unsupervised to supervised and back to unsupervised recalibration.</p><p>l. 119 Why would diverging shifts in the tuning of neural mechanisms between cortical areas not be detectable when both perceptual estimates are shifted in the same direction?</p><p>l. 126 Isn't that what the neuroimaging studies showed? Changes in relatively early areas?</p><p>l. 140 &quot;therefore we expected to see perceptual shifts resulting from unsupervised recalibration in MSTd and PIVC&quot;. That is a surprising claim, which needs further explanation as it implies that perceptual decisions are based on neural activity in these relatively early areas. Maybe leave that conclusion entirely to the discussion.</p><p>l. 145 The claim that VIP underlies cognitive processing will startle many. Why not simply say that VIP seems to be involved in perceptual decision making or higher order perceptual functions some of which yet have to be understood?</p><p>l. 161 Please explain that 1) during the recalibration block, the monkeys are exposed to visual-vestibular stimulus pairs with a consistent discrepancy in heading direction and that 2) in pre- and post-recalibration blocks the perception of heading direction indicated by unisensory stimuli is measured and that 3) recalibration effects are measured as the difference between pre- and post-recalibration results. The text should make sense to a reader who has never performed a recalibration study. No reader should be forced to read another paper just to understand the most crucial aspects of the current one!</p><p>l. 181f It does not seem necessary to describe the figure in detail in the text. Explanations of how to read a figure are best placed in its caption. Given that these are selected-curves of a single-subject in one of several sessions, the size of the shifts should not be compared or discussed in the text.</p><p>l. 200 At this point the reader has never heard that the monkeys underwent several sessions nor in which way the sessions differed from each other. Please add that information.</p><p>l. 202 Why force the reader to learn what δ + means in this manuscript? It is much better to just speak of 'sessions in which visual heading was rightwards of vestibular heading during the recalibration phase'. Please apply that thought throughout.</p><p>l. 232 What does 'tuning recalibrate with perceptual shifts' mean? A good advice I got from a book on scientific writing is to take sentences literally even when they describe scientific matter. One possible general phrasing would be that the neural tuning shifted in the same (opposite for VIP) direction as heading perception. A more results-oriented phrasing is that the neurometric functions shifted in the same direction as the psychometric functions for each modality. Please repair this throughout the text!</p><p>l. 240 This description of the neurometric analysis is not sufficient (see comment in the methods section). In addition, &quot;PSEs were extracted similar to&quot; is confusing, PSEs always correspond to the 50% point of a psychometric function (e.g., the mean of the Gaussian distribution). More importantly, it should be explained to readers that the PSE of a neurometric curve is the physical heading direction at which the chances to make a rightward judgment based on the firing rates of the neurons are fifty-fifty, i.e., straight-ahead according to the neuronal data.</p><p>l. 245 I think this is the first instance in which the term &quot;behavioral shifts&quot; is used instead of &quot;perceptual shifts&quot; but then it occurs consistently and is even present in the figures. In psychophysics, the term &quot;behavioral shifts&quot; would be used if there is any reason to suspect that the behavior does not correspond to the percepts, e.g., because participants show a response bias rather than a perceptual bias. If this might be the case, it should be discussed, otherwise please use perceptual.</p><p>l. 286 Again, please phrase more precisely.</p><p>l. 315f I like the analysis and the paragraph could function as a prototype for the subsequent paragraphs regarding its degree of abstraction and briefness. Yet, again more precise phrasing would be nice, e.g., neurons don't respond visually they exclusively respond when visual stimuli are presented.</p><p>l. 334f Again, in my view it is not necessary to describe figures and do so panel by panel.</p><p>l. 339f The last sentence of the paragraph does not parse.</p><p>The claim that neural recalibration follows the velocity profile of the stimulus is too strong and not correct as the figures show correlations not neural recalibration. For the earlier areas, the claim that the significant correlation between neurometric and psychometric shifts is driven by firing rates during the period of maximal stimulus velocity might be correct.</p><p>l. 350f Similarly, for VIP neurons it is also not necessary to describe the figure.</p><p>l. 356 The claim that the correlations between neurometric and psychometric shifts given firing rates recorded at the end of the stimulus presentation has nothing to do with choice behavior but reflects neuronal recalibration is not substantiated at this point, an anti-correlation is a strong relation. The conclusion might be drawn based on the last section of the results.</p><p>l. 364 &quot;During recalibration&quot; means during the recalibration phase but I think that is not what the authors mean because there are no behavioral choices during the recalibration phase. Please search the text for this phrasing, it probably is not adequate at other instances, too.</p><p>l. 412 Not sure if this is the first instance, but cross-sensory is not a word used in the literature, please use either multisensory or cross-modal or across the senses. Some authors will point out that multisensory should only be used in the case of perceptual fusion. Please replace the word throughout the text.</p><p>l. 412f This holds for the full Discussion: See above comments, the phrasing is very uncommon, especially the use of the verb 'recalibrate'. Almost all instances of 'recalibrated' should be replaced with 'changed' or 'shifted'. &quot;Together with&quot; means &quot;shifts in same direction as perceptual shifts&quot; and so on. It gets much better from line 478 on.</p><p>l. 422 Not sure if vision scientists would call MSTd a multisensory area.</p><p>l. 432f Why does 'unsupervised' imply changes in early areas? I wondered the same in the introduction. And how does that go together with the suggestion that the conflict is detected in ACC? It might be easiest to just refer to the neuroimaging studies or simply not make a claim.</p><p>l. 457f What is &quot;individualized recalibration&quot;? The literature uses &quot;modality-specific&quot;. I fail to see what this section adds that is not in the previous section. Why would the yoking found in the supervised recalibration study for both perceptual and neuronal shifts predict uniformity in the neuronal shifts in a paradigm that leads to non-yoked perceptual shifts?</p><p>l. 499 Please phrase the conclusion as a possibility, as it remains unclear to which degree supervised and unsupervised recalibration correspond.</p><p>l. 513 'Shift in reference frame' does this refer to a change in the supramodal definition of straight-ahead based on the vestibular tuning in lower sensory areas? The idea that VIP is tuned in a vestibular reference frame fits with earlier studies investigating visual-tactile reference frames (e.g., Avillac et al., Sereno and Huang, and also Graziano recording in F4).</p><p>l. 531 what does &quot;cross-sensory recalibration, vs. simply reflecting the recalibrated signals&quot; mean? What are recalibrated signals and why are they different from cross-modal recalibration?</p><p>General writing guidelines:</p><p>All figures should be optimized for the outlet, which in the case of <italic>eLife</italic> is wide and short figures as the text is never set in two-columns.</p><p>All acronyms should be defined when they are used for the first time, e.g., point of subjective equivalence (PSE). It can be very helpful for readers to treat abstract, significance statement, main text, and methods as separate and define acronyms anew.</p><p>A number and its unit are separated by a space, e.g., 300 ms instead of 300ms. Please check this throughout the text including the figures.</p><p>Figure 1</p><p>A: A visualization of the optic flow stimuli (e.g., two subsequent frames or little arrows indicating the motion vectors) would be nice.</p><p>B: It should be indicated either in the figure or in the caption that δ was constant within a single session, but theta was varied within a session and could take on all values depicted in A.</p><p>The grey vector corresponds to the combined direction if both cues have the same reliability, which probably wasn't exactly the case given Figure 2A.</p><p>C: 'no choice' is misplaced.</p><p>All of them, please add a space between a number and its unit.</p><p>Figure 2</p><p>A,B please indicate the monkey and the session number.</p><p>A,B usually rightward choice ratio would be interpreted as the ratio of rightward to leftward choices, I assume the proportion of rightward responses is shown.</p><p>C indicate how the shift was calculated.</p><p>C I cannot see the error bars referred to in the caption.</p><p>C please indicate the distribution of each monkey to assure readers that the results hold within and across subjects (see comments on the statistical analysis).</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Aside from the major comments outlined in the public review, I found that some figure legends should be expanded. For instance, in Figure 8 it is unclear what the empty and filled circles indicate, respectively. I would recommend the authors to check the manuscript carefully.</p><p>While in general the manuscript is well written, I found the &quot;in brief&quot; section rather difficult and not suitable for a broader audience. I would suggest rewriting the section accordingly.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82895.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Please report summary statistics (means and correlation coefficients) and p values for individual animals, for the data in Figure 2C, 3B, 4B, and 5B. This is not to say that the results must be significant at the individual animal level in order to support the study's main conclusions; we are all aware of the practicalities and accepted conventions of the field. Yet we feel it is important to be up front about this limitation, if indeed some of the individual monkey results fail to reach significance.</p><p>To be clear, the reasoning behind this request is that repeated observations from the same subject/condition are not independent of each other, thus correlations calculated from pooled data might obscure, inflate, or even reverse the true relationship between the variables (e.g., Simpson's Paradox).</p><p>Apart from individual subject analyses, another approach (indeed our strong recommendation) is to perform a hierarchical analysis. There are multiple options for this; probably the easiest one is to calculate linear mixed regression models (LMM) with one variable as predictor, the other as outcome and no intercept. This can be done in the frequentist way using (for example) the lmer package in R, or in a Bayesian way using Stan, which even as an automatized module for GLMMs. Partial correlations can be achieved by adding the variable that is partialized out as a predictor.</p><p>In summary, the authors may choose one (or more) of the three mentioned approaches to enhance statistical rigor: (1) separate t-tests and correlations for each animal and condition, (2) frequentist hierarchical linear models, and (3) Bayesian hierarchical linear models. It is worth pointing out that Bayesian approaches have advantages over frequentist methods, for instance quantifying the evidence for the null hypothesis (and thus better evaluating negative results such as Figure 4B-right).</p></disp-quote><p>Thank you for raising this important point. In response, we 1) added summary statistics for the individual monkeys, and 2) applied the linear mixed model (LMM) analysis to the data, with neuronal shifts as the dependent variable, perceptual shifts as the fixed predictor, and monkeys as a random effects grouping factor. Results from both of these approaches indicate that the findings are consistent across monkeys. We added these results as supplementary tables (see source data associated with each figure) to the revised manuscript. The LMM analyses (with monkeys as a random effects factor) did not provide better fits than the pooled model (PM) analyses (with pooled data across monkeys). These additional analyses support the main findings of the manuscript.</p><disp-quote content-type="editor-comment"><p>2) Please consider applying a goodness-of-fit criterion to the neurometric curves before inclusion of their PSEs in the neuron-behavior correlation analyses – AND/OR evaluate the reliability of the PSEs using standard error obtained from the fitting procedure, or a bootstrap-based confidence interval. We would not require individual neuron shifts (δ-PSE) to be significant according to such SEs (i.e., through error propagation), but a reanalysis after removing particularly poor fits seems appropriate. The criterion to use for this is difficult for us to specify and thus can use your best judgment.</p></disp-quote><p>Thank you for this valid suggestion. To quantify the reliability of each neurometric PSE we used the standard deviation (SD) of the bootstrapped PSEs obtained from the fitting procedure. We considered that this measure would be a more direct estimate of PSE reliability (the parameter of interest) vs. more general goodness-of-fit measures (such as pseudo-R<sup>2</sup>) which are influenced by other parameters (e.g., thresholds and lapse rates) that are less relevant for assessing PSE estimate reliability. We required that the SD of the neuron’s bootstrapped PSE was &lt; 10°, in both the pre-and post-recalibration blocks. This cutoff was chosen empirically, based on the distribution of SD values across neurons (see Figure 3—figure supplement 1). This screening was applied in addition to (after) the initial screening for significant tuning (as described in the original manuscript). We have now updated the manuscript to say that the neurons were first screened for significant response tuning, and then the remaining neurons were screened for neurometric PSE reliability (please see the revised Methods subsection “Neuronal shifts”, paragraph 1).</p><p>For MSTd, the neurometric PSE reliability screening removed 9 neurons from the vestibular data and 9 neurons from the visual data. For PIVC, this removed 14 neurons from the vestibular data and 11 neurons from the visual data. For VIP, this removed 16 neurons from the vestibular data and 24 neurons from the visual data. We redid all the analyses including only the neurons that passed this additional screening (the results remained similar), updated the figures, and revised the manuscript accordingly.</p><disp-quote content-type="editor-comment"><p>3) Depending on how the first two points are handled, and the outcome thereof, it may be necessary to scrutinize the role of outliers in generating the correlation coefficients and p values obtained. For instance, is the correlation of Figure 3B-left still significant without the upper four points, and 3B-right without the rightmost three points? A hierarchical analysis and/or neurometric goodness-of-fit criterion could reduce the role of outliers, in which case no formal outlier correction or other way to address this is needed.</p></disp-quote><p>The robustness of the results was assessed with the systematic approaches described above (in response to the two previous comments). Namely: additional screening to remove neurons with low-reliability estimates for the neuronal PSE (see the response to Comment #2), and additional analysis (LMM) to account for the random effects of different monkeys (see the response to Comment #1). These specific data points passed this screening, and the results were robust to further analyses that took into account the random effects of monkeys. In addition, we tested the influence of outliers using ‘Cook's distance’ (<xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>) . Redoing the analyses after removing data points with Cook’s distances larger than three times the mean provided similar results as before (summarized in <xref ref-type="table" rid="sa2table1">Author response table 1</xref>).</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>Cook's distances.</title><p>Cook’s distances for the vestibular and visual data (top and bottom row, respectively) from areas (A) MSTd, (B) PIVC, and (C) VIP. The green dashed line marks three times the mean Cook's distance.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82895-sa2-fig1-v1.tif"/></fig><table-wrap id="sa2table1" position="float"><label>Author response table 1.</label><caption><title>‘Cook's distance’ outlier analysis.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom">MSTd</th><th valign="bottom">r</th><th valign="bottom">p</th><th valign="bottom">N</th><th valign="bottom"/></tr></thead><tbody><tr><td align="left" valign="bottom">Vestibular</td><td align="left" valign="bottom">All</td><td align="char" char="." valign="bottom">0.62</td><td align="char" char="." valign="bottom">0.019 *</td><td align="char" char="." valign="bottom">14</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Outliers Removed</td><td align="char" char="." valign="bottom">0.63</td><td align="char" char="." valign="bottom">0.021 *</td><td align="char" char="." valign="bottom">13</td></tr><tr><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">All</td><td align="char" char="." valign="bottom">0.38</td><td align="char" char="." valign="bottom">2.7 × 10<sup>-3</sup> ***</td><td align="char" char="." valign="bottom">59</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Outliers Removed</td><td align="char" char="." valign="bottom">0.36</td><td align="char" char="." valign="bottom">7.8 × 10<sup>-3</sup> ***</td><td align="char" char="." valign="bottom">53</td></tr><tr><td align="left" valign="bottom">PIVC</td><td align="left" valign="bottom">r</td><td align="left" valign="bottom">p</td><td align="left" valign="bottom">N</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Vestibular</td><td align="left" valign="bottom">All</td><td align="char" char="." valign="bottom">0.80</td><td align="char" char="." valign="bottom">9.7 × 10<sup>-8</sup> ***</td><td align="char" char="." valign="bottom">30</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Outliers Removed</td><td align="char" char="." valign="bottom">0.81</td><td align="char" char="." valign="bottom">9.3 × 10<sup>-8</sup> ***</td><td align="char" char="." valign="bottom">29</td></tr><tr><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">All</td><td align="char" char="." valign="bottom">0.26</td><td align="char" char="." valign="bottom">0.47</td><td align="char" char="." valign="bottom">10</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Outliers Removed</td><td align="char" char="." valign="bottom">0.26</td><td align="char" char="." valign="bottom">0.47</td><td align="char" char="." valign="bottom">10</td></tr><tr><td align="left" valign="bottom">VIP</td><td align="left" valign="bottom">r</td><td align="left" valign="bottom">p</td><td align="left" valign="bottom">N</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Vestibular</td><td align="left" valign="bottom">All</td><td align="char" char="." valign="bottom">0.77</td><td align="char" char="." valign="bottom">2.7× 10<sup>-8</sup> ***</td><td align="char" char="." valign="bottom">37</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Outliers Removed</td><td align="char" char="." valign="bottom">0.80</td><td align="char" char="." valign="bottom">1.1× 10<sup>-8</sup> ***</td><td align="char" char="." valign="bottom">35</td></tr><tr><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">All</td><td align="char" char="." valign="bottom">-0.68</td><td align="char" char="." valign="bottom">8.4× 10<sup>-7</sup> ***</td><td align="char" char="." valign="bottom">42</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Outliers Removed</td><td align="char" char="." valign="bottom">-0.70</td><td align="char" char="." valign="bottom">8.0 × 10<sup>-7</sup> ***</td><td align="char" char="." valign="bottom">38</td></tr></tbody></table><table-wrap-foot><fn><p>N = number of neurons, r, and p-values from Pearson correlations. *** p &lt; 0.001; * p &lt; 0.05.</p></fn></table-wrap-foot></table-wrap><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>The results are really interesting, yet, the manuscript in its current form needs revisions along two dimensions, 1) data analysis and 2) writing.</p><p>Methods</p><p>I am aware that basically all analyses in this manuscript have been used in published papers. The problems outlined below hold for those papers, too, sorry. Bad (or good) luck having this time a reviewer from another field.</p><p>Correlations derived from data that includes multiple repetitions per subject (and condition) require a hierarchical analysis because repeated observations from the same observer are not independent of each other. Subject A might on average have higher shifts (behavioral and neurometric) than subject B. In that case, a non-hierarchical analysis might return a significant positive correlation even if within subject A negative behavioral shifts are associated with positive neurometric shifts and vice versa. There are multiple ways to account for intra-subject correlations, probably the easiest one is to calculate linear mixed regression models (LMM) with one variable as predictor, the other as outcome and no intercept. This can be done in the frequentist way using the lmer package in R and in a Bayesian way using Stan which even as an automatized module for GLMMs. Partial correlations can be achieved by adding the variable that is partialized out as a predictor. Of course, it is also possible to just analyze each subject separately.</p></disp-quote><p>Thank you for raising this valid point. In response, we: (1) calculated the individual monkey summary statistics, and report these in the revised manuscript (Figure 2–source data 1 presents the behavioral shift data; Figure 3–source data 1, Figure 4–source data 1, and Figure 5–source data 1 present the correlations between neuronal and behavioral shifts for MSTd, PIVC, and VIP, respectively). (2) We performed the linear mixed model (LMM) analysis, and present the results, and comparison thereof to the pooled model (PM) with pooled data across monkeys, in the respective supplementary tables (see source data). Please see our response to Essential Revision #1 (above) for further details.</p><disp-quote content-type="editor-comment"><p>The same holds for any comparisons of mean values (across heading offsets during the recalibration phase or across modalities). For example, a t-test that includes perceptual shifts from all sessions and all monkeys is not valid as the values from a single monkey are correlated. Again, there are several ways to account for this. For example, an LMM with cross-modal discrepancy as predictor (and a free intercept) or each monkey's data could be tested separately.</p><p>The procedure to derive neurometric curves, which are central to the study, should be explained better. In the main text and in the figures, it should be clarified that each data point shows the proportion of trials in which an ideal observer would make a rightwards choice given only the firing rates of the neurons (and assumed anti-neurons). In the methods section, it should be explained in detail how the ROC curve is derived. Readers unfamiliar with the method should understand that the ROC incorporates decisions for a range of decision thresholds and that the area under the ROC curve (AUROC, not ROC value) corresponds to a theoretical observer's ability to discriminate between leftwards and rightwards motion given firing rates across repetitions of a specific motion direction. Finally, those AUROC values (ranging from 0.5 to 1) are mapped onto the probability of a rightwards choice (ranging from 0 to 1) given a heading direction.</p></disp-quote><p>In response to this comment, we added more detailed explanations about the procedure to derive the neurometric curves (please see the revised Methods subsection “Neurometrics”, paragraphs 1-2). We also added to the figure legends (regarding neurometric curves) that each data point shows the proportion of trials in which an ideal observer would make a rightward choice given the firing rates of the neurons. We would like to clarify that for this calculation we did not compare neurons to “anti-neurons” (which would assume PSE = 0°). Rather, we z-scored the data in reference to the <italic>pre-calibration</italic> firing rates. Using a common reference (pre-recalibration mean, corresponding to z-score = 0) to normalize both pre- and post-recalibration data was needed to allow for and expose PSE shifts. Then we calculated ROC curves by moving a ‘criterion’ value from the minimum to the maximum z-score (in 100 steps), and plotting the probability that the z-scores exceeded the criterion vs. whether z-score = 0 (the pre-recalibration mean) exceeded that same criterion, or not (1 or 0, respectively). We have also added this clarification to the revised manuscript (see Methods subsection “Neurometrics”, paragraph 2).</p><disp-quote content-type="editor-comment"><p>The use of PSEs for the correlational analysis should be conditional on the goodness of fit of the sigmoid and the SD should fall within a reasonable range. A PSE derived from an ill-fitting or very flat curve has no informational value and is likely to be extreme, which in turn is huge a problem for any correlational analysis.</p></disp-quote><p>In response to this comment we further screened the neurons by calculating the SD of the neurometric PSE (from bootstrapped values) and excluded neurons with SDs &gt; 10° (either pre- or post-recalibration). We considered that this would be a more direct measure of PSE reliability (the parameter of interest) vs. goodness-of-fit of the whole psychometric function, which could be affected by other parameters (such as thresholds and lapse rates). Please see our response to Essential Revision #2 (above) for further details.</p><disp-quote content-type="editor-comment"><p>The methods section states that only neurons tuned to heading directions as indicated by cues in a specific modality were included in the neurometric analysis for that modality. If that exclusion criterion was applied, why are the neurometric curves for 'the other modality' so bad? I would have expected that neurons tuned to visual heading cues result in a good neurometric function for visual heading no matter where these neurons are located in the brain. In turn, I would have expected flat neurometric functions for visual heading direction in PIVC if also neurons tuned to vestibular heading information were included in the analysis. What am I missing?</p></disp-quote><p>In the revised Methods subsection “Neuronal shifts”, paragraph 1, we now better explain the inclusion criteria for the different stages of analysis. First, we calculated neurometric functions for all recorded cells, and both modalities, whether or not the cell was significantly tuned. For the subsequent neuronal vs. behavioral shift analyses, we only included neurons that satisfied both the following criteria (the second criterion was added to the revised manuscript in response to Essential Revisions, Comment #2): i) significant tuning to the corresponding cue, and ii) reliable neurometric PSE estimates (SD &lt; 10°, both in the pre- and post-recalibration blocks). The example neuron from PIVC in Figure 4 responded significantly to vestibular (but not visual) stimuli. Thus, although neurometric fits are presented for both cues in Figure 4, this PIVC neuron was included only in the vestibular (but not the visual) PSE correlation analysis.</p><disp-quote content-type="editor-comment"><p>Another instance of me probably not getting something: to identify neurons tuned for heading directions as cued by one of the two modalities, spike rates were regressed onto heading directions and only those neurons with a significant slope were included. I get that the range of tested headings is smaller than the width of a typical tuning curve and thus a linear regression makes more sense than trying to fit a full tuning curve. However, it seems to me that his selection method excludes neurons tuned to straight-ahead as their slope should be flat for a symmetric range of headings.</p></disp-quote><p>Indeed our selection criteria were selective for neurons that have sloped tuning around straight-ahead, and exclude neurons that could be tuned to forward motion stimuli (i.e., with a tuning preference straight-ahead). The reasons for this are: (1) neurons with sloped tuning straight-ahead are most informative for heading discrimination (large Fisher information, Gu et al., 2010). By contrast, neurons with a tuning preference to straight-ahead stimuli have relatively flat responses around straight-ahead (low Fisher information) and are thus less informative for heading discrimination. Accordingly, small shifts can be readily detected in neurons with sloped tuning (but not in those with flat tuning) around straight ahead. (2) In the cortical areas of interest in this study, a disproportionately large number of neurons have steep tuning slopes around straight ahead, presumably to support heading discrimination (Chen et al., 2011b; Gu et al., 2010). (3) The standard neurometric analysis wouldn’t work for neurons with a tuning preference to straight-ahead (rightward and leftward headings around straight-ahead would elicit ambiguous firing rates). Thus estimating tuning shifts in these neurons would require different (heterogeneous and more complex) models, and the results would be very noisy. Therefore, in this study, we focused on the prevalent neurons with sloped tuning around straight-ahead. We have added these points to the revised Methods subsection “Data analysis”, paragraph 3.</p><disp-quote content-type="editor-comment"><p>Regression and correlational analyses are extremely sensitive to outliers. To be sure that the results are robust, please repeat the analyses after outlier correction, e.g., based on the 1.5xIQR rule along each dimension or based on each data points influence on the regression (Cook's distance).</p></disp-quote><p>In this revision, we added: 1) summary statistics of the individual monkeys, and performed an LMM analysis (in response to Essential Revisions, Comment #1), and 2) we screened the neurons based on their PSE estimate reliability (in response to Essential Revisions, Comment #2). In addition, we also tested the influence of outliers using ‘Cook's distance’ (see details in response to Essential Revisions, Comment #3). Results from these analyses support the robustness of the results.</p><disp-quote content-type="editor-comment"><p>Looking at the currently depicted sigmoids, I suggest increasing the maximal value for the lapse rate. Even though it is not fully clear to me what a lapse rate of neurometric curve actually means, achieving a better fit for the curves seems essential given that the PSEs are at the center of the results.</p></disp-quote><p>We assessed the effect of allowing for a broader range of lapse rates on the neurometric fits, and PSEs in particular. Allowing for a broader range of lapse rates increased pseudo-R<sup>2</sup> values (expected when increasing the number of model parameters) and sometimes affected threshold estimates (thresholds could be under- or overestimated, depending on whether lapse rates are allowed or not, respectively). However, it had little influence on the PSE estimates. For example, the post-recalibration neurometric curve in Figure 3A (blue; redrawn in <xref ref-type="fig" rid="sa2fig2">Author response image 2A</xref>) has a threshold = 17.8°. If larger lapse rates are allowed (<xref ref-type="fig" rid="sa2fig2">Author response image 2B</xref>), this could provide an unreasonably low threshold estimate (threshold = 0.6°). Thus, allowing (or not) large lapse rates can drastically affect threshold estimates. By contrast, the post-recalibration neurometric PSE was 3.6° and 2.8° for the two curves, respectively. Hence, in general, the effect of lapse rates on PSE estimates is small. Because PSE (not threshold) is the parameter of interest in this study, we opted for a more conservative neuromeric fit (with fewer parameters, i.e. tighter lapse rates) and exclusively used the PSE (not threshold) estimates. In addition, we added screening of neuronal PSE estimate reliability (via bootstrapping) in the revised manuscript (please see details in response to Essential Revisions, Comment #2).</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>Influence of lapse rates on example neurometric fit.</title><p>(A) Neurometric fit of vestibular responses, without allowing large lapse rates, for the example MSTd neuron (same as Figure 3A). (B) Neurometric fit of the same data, allowing large lapse rates.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82895-sa2-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>The methods section provides information about the number of neurons per area included in the analyses in general. Additional information about the number of neurons per neurometric curve would be useful.</p></disp-quote><p>We added this information in the Methods subsection “Neuronal shifts”, paragraph 1.</p><disp-quote content-type="editor-comment"><p>Please add in l. 698 how the baseline activity for each neuron was determined, e.g., based on a single interval at the beginning of the session or based on recordings in between trials.</p></disp-quote><p>Baseline FRs were calculated by taking the average FR in the 1 s window before stimulus onset. We have added this to the revised Methods, subsection “Data analysis”, paragraph 2.</p><disp-quote content-type="editor-comment"><p>Probably just a typo but in l. 689 Pearson's correlation has nothing to do with linear regression.</p></disp-quote><p>We have corrected this.</p><disp-quote content-type="editor-comment"><p>Writing</p><p>Generally, I find the manuscript to be well-written and organized. However, in its current form, the manuscript is geared towards a small audience, electrophysiologists familiar with most publications by the Angelaki and DeAngelis labs. A much wider audience could be reached by 1) phrasing more precisely and 2) providing more information, reasoning, and explanations.</p></disp-quote><p>In response to this comment and thanks to the comments below, we now provide more information and explanations, with improved phrasing in our revised manuscript.</p><disp-quote content-type="editor-comment"><p>The verb 'recalibrate' is often used in a way that doesn't match the literature and the way recalibration is thought of. Information cannot be recalibrated; cues cannot recalibrate actively; responses do not recalibrate (together). A system is recalibrated and then it interprets incoming information differently. In most instances, a simple replacement of recalibrated with &quot;shifted&quot; will help -- AND/OR include a statement early on defining these more colloquial uses of recalibrate (e.g., &quot;we refer to this pattern of neural activity as 'recalibration'…)</p></disp-quote><p>In the revised manuscript we now use the term ‘recalibrate’ more carefully. This entailed replacing many instances of “recalibrated” with “shifted”.</p><disp-quote content-type="editor-comment"><p>A few examples for more common and precise phrasing:</p><p>l. 35 -&gt; recalibrating itself based on information from …</p></disp-quote><p>Thank you for the improved phrasing. We have updated the text.</p><disp-quote content-type="editor-comment"><p>l. 36 -&gt; estimates for subsequently presented unisensory stimuli are shifted towards each other.</p></disp-quote><p>Thank you for the improved phrasing. We have updated that sentence.</p><disp-quote content-type="editor-comment"><p>l. 25 -&gt; the tuning of neural responses to … cues was shifted in the same direction as the monkeys' perceptual judgments of subsequently presented unisensory stimuli.</p></disp-quote><p>Thank you for the improved phrasing. We have updated that sentence.</p><disp-quote content-type="editor-comment"><p>l. 43 -&gt; tuning of vestibular neurons was shifted in the same direction as vestibular heading perception.</p><p>This issue is present throughout the manuscript but especially so in the abstract, in brief section, and the discussion. I doubt that someone not familiar with the literature can understand the abstract. There is a section in the discussion (l. 483f) that is written in a very abstract manner but phrased according to the literature, i.e., in accordance with the ways most people think about recalibration. Please adjust the rest of the text accordingly.</p></disp-quote><p>Thanks for bringing this to our attention. We have updated the relevant text accordingly.</p><disp-quote content-type="editor-comment"><p>Comments in chronological order:</p><p>l. 57 Behavioral papers cited in a sentence about the neuronal basis of multisensory integration.</p></disp-quote><p>We updated those references</p><disp-quote content-type="editor-comment"><p>l. 76 Just saying that results exist is rather unusual. In a few words, what did the neuroimaging studies find?</p></disp-quote><p>We agree and have now added a brief explanation of these neuroimaging studies in the revised Introduction.</p><disp-quote content-type="editor-comment"><p>l. 91 Burge et al. is not about heading perception but about visual-haptic slant perception.</p></disp-quote><p>Thank you. We removed this reference from the paper.</p><disp-quote content-type="editor-comment"><p>l. 103 Again, the typical phrasing would be that perception was recalibrated as indicated by shifts in subsequent perceptual estimates.</p></disp-quote><p>Thanks. We have updated that sentence.</p><disp-quote content-type="editor-comment"><p>l. 108f It would be easier for readers to learn about supervised recalibration in the discussion, as now they have to shift mentally from unsupervised to supervised and back to unsupervised recalibration.</p></disp-quote><p>We agree and have moved this paragraph about supervised recalibration to the Discussion.</p><disp-quote content-type="editor-comment"><p>l. 119 Why would diverging shifts in the tuning of neural mechanisms between cortical areas not be detectable when both perceptual estimates are shifted in the same direction?</p></disp-quote><p>We now better clarify this point in the revised manuscript. In the supervised calibration paradigm, the overall shifts for both vestibular and visual cues were “yoked” in the same direction. Thus, we could not dissociate whether neuronal shifts for a particular cue (e.g., vestibular) follow the behavioral shifts for that cue (vestibular) or the other cue (visual). Both predict shifts in the same direction, and it would be difficult to dissociate these based on small differences in the expected shift magnitudes (because of noise). This (unsupervised) paradigm elicits behavioral shifts in opposite directions, and thus can more readily discern if the vestibular neurometrics shift with visual (rather than vestibular) behavioral shifts. We have added this explanation to the revised Discussion subsection “Modality-specific recalibration of vestibular and visual cues”, paragraph 3.</p><disp-quote content-type="editor-comment"><p>l. 126 Isn't that what the neuroimaging studies showed? Changes in relatively early areas?</p></disp-quote><p>Yes, we have updated the sentence by referencing neuroimaging studies that found auditory-visual recalibration in relatively early sensory areas (Amedi et al., 2002; Zierul et al., 2017).</p><disp-quote content-type="editor-comment"><p>l. 140 &quot;therefore we expected to see perceptual shifts resulting from unsupervised recalibration in MSTd and PIVC&quot;. That is a surprising claim, which needs further explanation as it implies that perceptual decisions are based on neural activity in these relatively early areas. Maybe leave that conclusion entirely to the discussion.</p></disp-quote><p>We now justify this hypothesis based on the neuroimaging studies that found auditory-visual recalibration in relatively early sensory areas. Also, we further explain that because unsupervised plasticity is sensory-driven (occurs as a result of the cross-modal discrepancy, in the absence of overt feedback) we expected to observe neural correlates of recalibration in these lower-level sensory areas, and for these to propagate along the perceptual decision-making hierarchy.</p><disp-quote content-type="editor-comment"><p>l. 145 The claim that VIP underlies cognitive processing will startle many. Why not simply say that VIP seems to be involved in perceptual decision making or higher order perceptual functions some of which yet have to be understood?</p></disp-quote><p>We thank the reviewer for this refinement, and have updated that explanation accordingly.</p><disp-quote content-type="editor-comment"><p>l. 161 Please explain that 1) during the recalibration block, the monkeys are exposed to visual-vestibular stimulus pairs with a consistent discrepancy in heading direction and that 2) in pre- and post-recalibration blocks the perception of heading direction indicated by unisensory stimuli is measured and that 3) recalibration effects are measured as the difference between pre- and post-recalibration results. The text should make sense to a reader who has never performed a recalibration study. No reader should be forced to read another paper just to understand the most crucial aspects of the current one!</p></disp-quote><p>We agree and have now added more details to explain the experimental procedure.</p><disp-quote content-type="editor-comment"><p>l. 181f It does not seem necessary to describe the figure in detail in the text. Explanations of how to read a figure are best placed in its caption. Given that these are selected-curves of a single-subject in one of several sessions, the size of the shifts should not be compared or discussed in the text.</p></disp-quote><p>We have removed the unnecessary details from the text, and also removed the shift size comparison for the example plots. We also modified the figure legends.</p><disp-quote content-type="editor-comment"><p>l. 200 At this point the reader has never heard that the monkeys underwent several sessions nor in which way the sessions differed from each other. Please add that information.</p></disp-quote><p>Thank you. We have now added information (specifically the number of sessions, and how the sessions differed) so that the reader can better understand the results which follow (please see updated Results subsection “Vestibular and visual perceptual estimates shift toward each other”, paragraph 3).</p><disp-quote content-type="editor-comment"><p>l. 202 Why force the reader to learn what δ + means in this manuscript? It is much better to just speak of 'sessions in which visual heading was rightwards of vestibular heading during the recalibration phase'. Please apply that thought throughout.</p></disp-quote><p>We have now added (in response to the Reviewer’s previous comment) a better explanation regarding the two types of sessions, and the meaning of δ + and δ -. Because these appear numerously throughout the text, and there is not enough space in the figures to write explicitly 'sessions in which visual heading was rightwards of vestibular heading during the recalibration phase’ etc., we think that it is clearer to keep this nomenclature.</p><disp-quote content-type="editor-comment"><p>l. 232 What does 'tuning recalibrate with perceptual shifts' mean? A good advice I got from a book on scientific writing is to take sentences literally even when they describe scientific matter. One possible general phrasing would be that the neural tuning shifted in the same (opposite for VIP) direction as heading perception. A more results-oriented phrasing is that the neurometric functions shifted in the same direction as the psychometric functions for each modality. Please repair this throughout the text!</p></disp-quote><p>We have amended the language accordingly.</p><disp-quote content-type="editor-comment"><p>l. 240 This description of the neurometric analysis is not sufficient (see comment in the methods section). In addition, &quot;PSEs were extracted similar to&quot; is confusing, PSEs always correspond to the 50% point of a psychometric function (e.g., the mean of the Gaussian distribution). More importantly, it should be explained to readers that the PSE of a neurometric curve is the physical heading direction at which the chances to make a rightward judgment based on the firing rates of the neurons are fifty-fifty, i.e., straight-ahead according to the neuronal data.</p></disp-quote><p>We have updated the text accordingly.</p><disp-quote content-type="editor-comment"><p>l. 245 I think this is the first instance in which the term &quot;behavioral shifts&quot; is used instead of &quot;perceptual shifts&quot; but then it occurs consistently and is even present in the figures. In psychophysics, the term &quot;behavioral shifts&quot; would be used if there is any reason to suspect that the behavior does not correspond to the percepts, e.g., because participants show a response bias rather than a perceptual bias. If this might be the case, it should be discussed, otherwise please use perceptual.</p></disp-quote><p>We agree with the reviewer’s suggestion. We have replaced the term &quot;behavioral shifts&quot; with &quot;perceptual shifts&quot;.</p><disp-quote content-type="editor-comment"><p>l. 286 Again, please phrase more precisely.</p></disp-quote><p>We now changed this subtitle to “Neuronal tuning in VIP to both vestibular and visual stimuli shifted according to vestibular perceptual shifts”.</p><disp-quote content-type="editor-comment"><p>l. 315f I like the analysis and the paragraph could function as a prototype for the subsequent paragraphs regarding its degree of abstraction and briefness. Yet, again more precise phrasing would be nice, e.g., neurons don't respond visually they exclusively respond when visual stimuli are presented.</p></disp-quote><p>We have revised the phrasing accordingly.</p><disp-quote content-type="editor-comment"><p>l. 334f Again, in my view it is not necessary to describe figures and do so panel by panel.</p></disp-quote><p>We have removed unnecessary descriptions of the figure from the main text.</p><disp-quote content-type="editor-comment"><p>l. 339f The last sentence of the paragraph does not parse.</p><p>The claim that neural recalibration follows the velocity profile of the stimulus is too strong and not correct as the figures show correlations not neural recalibration. For the earlier areas, the claim that the significant correlation between neurometric and psychometric shifts is driven by firing rates during the period of maximal stimulus velocity might be correct.</p></disp-quote><p>We agree with this point and have refined the explanation in line with the reviewer’s suggestion.</p><disp-quote content-type="editor-comment"><p>l. 350f Similarly, for VIP neurons it is also not necessary to describe the figure.</p></disp-quote><p>Thanks, we have refined this paragraph.</p><disp-quote content-type="editor-comment"><p>l. 356 The claim that the correlations between neurometric and psychometric shifts given firing rates recorded at the end of the stimulus presentation has nothing to do with choice behavior but reflects neuronal recalibration is not substantiated at this point, an anti-correlation is a strong relation. The conclusion might be drawn based on the last section of the results.</p></disp-quote><p>We have removed that sentence.</p><disp-quote content-type="editor-comment"><p>l. 364 &quot;During recalibration&quot; means during the recalibration phase but I think that is not what the authors mean because there are no behavioral choices during the recalibration phase. Please search the text for this phrasing, it probably is not adequate at other instances, too.</p></disp-quote><p>We agree with the reviewer’s suggestion. We have replaced the phrase &quot;during recalibration&quot; with &quot;after recalibration&quot; where appropriate in the text.</p><disp-quote content-type="editor-comment"><p>l. 412 Not sure if this is the first instance, but cross-sensory is not a word used in the literature, please use either multisensory or cross-modal or across the senses. Some authors will point out that multisensory should only be used in the case of perceptual fusion. Please replace the word throughout the text.</p></disp-quote><p>We agree with the reviewer’s suggestion. We have replaced the phrase &quot;cross-sensory &quot; with &quot; cross-modal&quot; throughout the text.</p><disp-quote content-type="editor-comment"><p>l. 412f This holds for the full Discussion: See above comments, the phrasing is very uncommon, especially the use of the verb 'recalibrate'. Almost all instances of 'recalibrated' should be replaced with 'changed' or 'shifted'. &quot;Together with&quot; means &quot;shifts in same direction as perceptual shifts&quot; and so on. It gets much better from line 478 on.</p></disp-quote><p>We agree with the reviewer’s suggestion. We replaced many instances of the word “recalibrate” with “shifted”. We also replaced phrases like “recalibrated together with the corresponding vestibular perceptual shifts” with “shifted in the same direction as vestibular perceptual shifts” etc.</p><disp-quote content-type="editor-comment"><p>l. 422 Not sure if vision scientists would call MSTd a multisensory area.</p></disp-quote><p>Although MSTd is part of the extra-striate visual cortex, it has been extensively shown to have multisensory (including vestibular) responses. We back up this claim in the manuscript with relevant references.</p><disp-quote content-type="editor-comment"><p>l. 432f Why does 'unsupervised' imply changes in early areas? I wondered the same in the introduction. And how does that go together with the suggestion that the conflict is detected in ACC? It might be easiest to just refer to the neuroimaging studies or simply not make a claim.</p></disp-quote><p>We now justify this hypothesis (in the revised Introduction) based on the neuroimaging studies, as recommended by the reviewer.</p><disp-quote content-type="editor-comment"><p>l. 457f What is &quot;individualized recalibration&quot;? The literature uses &quot;modality-specific&quot;. I fail to see what this section adds that is not in the previous section. Why would the yoking found in the supervised recalibration study for both perceptual and neuronal shifts predict uniformity in the neuronal shifts in a paradigm that leads to non-yoked perceptual shifts?</p></disp-quote><p>We agree with the reviewer’s suggested phrasing and replaced &quot;individualized recalibration&quot; with &quot;modality-specific recalibration&quot;. In the supervised calibration paradigm, the overall shifts for both vestibular and visual cues were “yoked” in the same direction. Thus, we could not dissociate whether neuronal shifts for a particular cue (e.g., vestibular) follow the behavioral shifts for that cue (vestibular) or the other cue (visual). Both predict shifts in the same direction, and it would be difficult to dissociate these based on the expected shift magnitudes (because of noise). This (unsupervised) paradigm elicits behavioral shifts in opposite directions, and thus can more readily discern if the vestibular neurometrics shift with visual (rather than vestibular) behavioral shifts.</p><disp-quote content-type="editor-comment"><p>l. 499 Please phrase the conclusion as a possibility, as it remains unclear to which degree supervised and unsupervised recalibration correspond.</p></disp-quote><p>We agree with the reviewer’s suggestion and have amended the language accordingly.</p><disp-quote content-type="editor-comment"><p>l. 513 'Shift in reference frame' does this refer to a change in the supramodal definition of straight-ahead based on the vestibular tuning in lower sensory areas? The idea that VIP is tuned in a vestibular reference frame fits with earlier studies investigating visual-tactile reference frames (e.g., Avillac et al., Sereno and Huang, and also Graziano recording in F4).</p></disp-quote><p>We do not know if this necessarily means “a change in the <italic>supramodal</italic> definition of straight-ahead based on the vestibular tuning in lower sensory areas”. We just point out that neuronal signals in VIP have been previously shown to be in head/body centered coordinates, and that the shifts we observe here are in line with that notion. We have clarified this section, and added these relevant references, in the revised Discussion subsection “Contrary recalibration in higher-level area VIP ” paragraph 3. Thank you.</p><disp-quote content-type="editor-comment"><p>l. 531 what does &quot;cross-sensory recalibration, vs. simply reflecting the recalibrated signals&quot; mean? What are recalibrated signals and why are they different from cross-modal recalibration?</p></disp-quote><p>By “cortical areas actively involved in cross-modal recalibration” we mean that they had an instrumental (causal) role in vestibular-visual recalibration. By “simply reflecting the recalibrated signals” we mean that they were not instrumental (causal) for recalibration, and thus only reflect signals that were recalibrated in/by other brain areas. We have now refined and better explained this idea in the revised Discussion.</p><disp-quote content-type="editor-comment"><p>General writing guidelines:</p><p>All figures should be optimized for the outlet, which in the case of eLife is wide and short figures as the text is never set in two-columns.</p></disp-quote><p>We have changed the layout of Figures 3, 4, and 5 accordingly.</p><disp-quote content-type="editor-comment"><p>All acronyms should be defined when they are used for the first time, e.g., point of subjective equivalence (PSE). It can be very helpful for readers to treat abstract, significance statement, main text, and methods as separate and define acronyms anew.</p><p>A number and its unit are separated by a space, e.g., 300 ms instead of 300ms. Please check this throughout the text including the figures.</p></disp-quote><p>We thank the reviewer for these suggestions, and have amended the text accordingly.</p><disp-quote content-type="editor-comment"><p>Figure 1</p><p>A: A visualization of the optic flow stimuli (e.g., two subsequent frames or little arrows indicating the motion vectors) would be nice.</p></disp-quote><p>We added a schematic (rightmost in Figure 1A) to visualize the optic flow.</p><disp-quote content-type="editor-comment"><p>B: It should be indicated either in the figure or in the caption that δ was constant within a single session, but theta was varied within a session and could take on all values depicted in A.</p></disp-quote><p>We added these points to the legend of Figure 1.</p><disp-quote content-type="editor-comment"><p>The grey vector corresponds to the combined direction if both cues have the same reliability, which probably wasn't exactly the case given Figure 2A.</p></disp-quote><p>The grey vector was not meant to reflect the <italic>perceived</italic> ‘combined cue’, rather, it is just a convention for defining the stimuli (visual and vestibular stimuli were offset ± 5° relative to this). We have clarified this in the legend.</p><disp-quote content-type="editor-comment"><p>C: 'no choice' is misplaced.</p></disp-quote><p>Thanks. We have corrected this.</p><disp-quote content-type="editor-comment"><p>All of them, please add a space between a number and its unit.</p></disp-quote><p>OK. We have updated accordingly.</p><disp-quote content-type="editor-comment"><p>Figure 2</p><p>A,B please indicate the monkey and the session number.</p></disp-quote><p>We added this information to the figure legend.</p><disp-quote content-type="editor-comment"><p>A,B usually rightward choice ratio would be interpreted as the ratio of rightward to leftward choices, I assume the proportion of rightward responses is shown.</p></disp-quote><p>Yes. We replaced &quot;ratio&quot; with &quot;proportion&quot; in the y-label.</p><disp-quote content-type="editor-comment"><p>C indicate how the shift was calculated.</p></disp-quote><p>We added to the legend how the shift was calculated.</p><disp-quote content-type="editor-comment"><p>C I cannot see the error bars referred to in the caption.</p></disp-quote><p>The error bars are indeed difficult to see because the SEs are small. To improve visibility, we reduced the triangle symbol size and made the error bars thicker.</p><disp-quote content-type="editor-comment"><p>C please indicate the distribution of each monkey to assure readers that the results hold within and across subjects (see comments on the statistical analysis).</p></disp-quote><p>We updated Figure 2C to display the distribution of each monkey (using different texture patterns). In addition, the summary statistics for each monkey are presented in Figure 2-source data 1.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>Aside from the major comments outlined in the public review, I found that some figure legends should be expanded. For instance, in Figure 8 it is unclear what the empty and filled circles indicate, respectively. I would recommend the authors to check the manuscript carefully.</p></disp-quote><p>We thank the reviewer for bringing this to our attention. Filled and empty circles indicate significant and non-significant partial correlations respectively. We have added this and other details to the figure legends.</p><disp-quote content-type="editor-comment"><p>While in general the manuscript is well written, I found the &quot;in brief&quot; section rather difficult and not suitable for a broader audience. I would suggest rewriting the section accordingly.</p></disp-quote><p>We removed the &quot;in brief&quot; and “highlights” sections from the manuscript because these are not standard in <italic>eLife</italic>.</p></body></sub-article></article>