<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">76534</article-id><article-id pub-id-type="doi">10.7554/eLife.76534</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Cell Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A scalable and modular automated pipeline for stitching of large electron microscopy datasets</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-254246"><name><surname>Mahalingam</surname><given-names>Gayathri</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3718-5013</contrib-id><email>gayathrim@alleninstitute.org</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-254245"><name><surname>Torres</surname><given-names>Russel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2876-4382</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-265712"><name><surname>Kapner</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-28200"><name><surname>Trautman</surname><given-names>Eric T</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8588-0569</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-265713"><name><surname>Fliss</surname><given-names>Tim</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-140632"><name><surname>Seshamani</surname><given-names>Shamishtaa</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-265714"><name><surname>Perlman</surname><given-names>Eric</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5542-1302</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-265709"><name><surname>Young</surname><given-names>Rob</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-265710"><name><surname>Kinn</surname><given-names>Samuel</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-53087"><name><surname>Buchanan</surname><given-names>JoAnn</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-254244"><name><surname>Takeno</surname><given-names>Marc M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8384-7500</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-265711"><name><surname>Yin</surname><given-names>Wenjing</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con12"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-136118"><name><surname>Bumbarger</surname><given-names>Daniel J</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con13"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-111055"><name><surname>Gwinn</surname><given-names>Ryder P</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con14"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-85330"><name><surname>Nyhus</surname><given-names>Julie</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con15"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-214850"><name><surname>Lein</surname><given-names>Ed</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con16"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-53088"><name><surname>Smith</surname><given-names>Steven J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2290-8701</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con17"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-4315"><name><surname>Reid</surname><given-names>R Clay</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8697-6797</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con18"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-181656"><name><surname>Khairy</surname><given-names>Khaled A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9274-5928</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con19"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-19822"><name><surname>Saalfeld</surname><given-names>Stephan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4106-1761</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con20"/><xref ref-type="fn" rid="conf3"/></contrib><contrib contrib-type="author" id="author-140148"><name><surname>Collman</surname><given-names>Forrest</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0280-7022</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con21"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-264772"><name><surname>Macarico da Costa</surname><given-names>Nuno</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2001-4568</contrib-id><email>nunod@alleninstitute.org</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con22"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00dcv1019</institution-id><institution>Allen Institute for Brain Science</institution></institution-wrap><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013sk6x84</institution-id><institution>HHMI Janelia Research Campus</institution></institution-wrap><addr-line><named-content content-type="city">Ashburn</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Yikes LLC</institution><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Epilepsy Surgery and Functional Neurosurgery, Swedish Neuroscience Institute</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02r3e0967</institution-id><institution>St. Jude Children's Research Hospital</institution></institution-wrap><addr-line><named-content content-type="city">Memphis</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Cardona</surname><given-names>Albert</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>26</day><month>07</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e76534</elocation-id><history><date date-type="received" iso-8601-date="2021-12-20"><day>20</day><month>12</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-07-10"><day>10</day><month>07</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-11-25"><day>25</day><month>11</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.11.24.469932"/></event></pub-history><permissions><copyright-statement>© 2022, Mahalingam, Torres et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Mahalingam, Torres et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-76534-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-76534-figures-v2.pdf"/><abstract><p>Serial-section electron microscopy (ssEM) is the method of choice for studying macroscopic biological samples at extremely high resolution in three dimensions. In the nervous system, nanometer-scale images are necessary to reconstruct dense neural wiring diagrams in the brain, so -called <italic>connectomes</italic>. The data that can comprise of up to 10<sup>8</sup> individual EM images must be assembled into a volume, requiring seamless 2D registration from physical section followed by 3D alignment of the stitched sections. The high throughput of ssEM necessitates 2D stitching to be done at the pace of imaging, which currently produces tens of terabytes per day. To achieve this, we present a modular volume assembly software pipeline <italic>ASAP</italic> (Assembly Stitching and Alignment Pipeline) that is scalable to datasets containing petabytes of data and parallelized to work in a distributed computational environment. The pipeline is built on top of the <italic>Render</italic> Trautman and Saalfeld (2019) services used in the volume assembly of the brain of adult <italic>Drosophila melanogaster</italic> (Zheng et al. 2018). It achieves high throughput by operating only on image meta-data and transformations. ASAP is modular, allowing for easy incorporation of new algorithms without significant changes in the workflow. The entire software pipeline includes a complete set of tools for stitching, automated quality control, 3D section alignment, and final rendering of the assembled volume to disk. ASAP has been deployed for continuous stitching of several large-scale datasets of the mouse visual cortex and human brain samples including one cubic millimeter of mouse visual cortex (Yin et al. 2020); Microns Consortium et al. (2021) at speeds that exceed imaging. The pipeline also has multi-channel processing capabilities and can be applied to fluorescence and multi-modal datasets like array tomography.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>connectomics</kwd><kwd>image processing</kwd><kwd>image alignment</kwd><kwd>image stitching</kwd><kwd>large-scale microscopy</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Intelligence Advanced Research Projects Activity (IARPA) of the Department of Interior/Interior Business Center</institution></institution-wrap></funding-source><award-id>D16PC00004</award-id><principal-award-recipient><name><surname>Mahalingam</surname><given-names>Gayathri</given-names></name><name><surname>Torres</surname><given-names>Russel</given-names></name><name><surname>Kapner</surname><given-names>Daniel</given-names></name><name><surname>Fliss</surname><given-names>Tim</given-names></name><name><surname>Seshamani</surname><given-names>Shamishtaa</given-names></name><name><surname>Young</surname><given-names>Rob</given-names></name><name><surname>Kinn</surname><given-names>Samuel</given-names></name><name><surname>Buchanan</surname><given-names>JoAnn</given-names></name><name><surname>Takeno</surname><given-names>Marc M</given-names></name><name><surname>Yin</surname><given-names>Wenjing</given-names></name><name><surname>Bumbarger</surname><given-names>Daniel J</given-names></name><name><surname>Reid</surname><given-names>R Clay</given-names></name><name><surname>Collman</surname><given-names>Forrest</given-names></name><name><surname>Macarico da Costa</surname><given-names>Nuno</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A novel image processing infrastructure that is capable of processing petascale datasets and can operate on both electron and light microscopy data.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Serial section electron microscopy (ssEM) provides the high spatial resolution in the range of a few nanometers per pixel that is necessary to reconstruct the structure of neurons and their connectivity. However, imaging at a high resolution produces a massive amount of image data even for a volume that spans a few millimeters. For example, a cubic millimeter of cortical tissue imaged at a resolution of 4 × 4 × 40 nm<sup>3</sup> generates more than a petabyte of data and contains more than 100 million individual image tiles (W et al. 2020). These millions of images are then stitched in 2D for each section and aligned in 3D to assemble a volume that is then used for neuronal reconstruction. With parallelized high-throughput microscopes producing tens of terabytes of data per day, it is necessary that this volume assembly process is automated and streamlined into a pipeline, so that it does not become a bottleneck. The ideal pipeline should be capable of processing data at the speed of imaging (<xref ref-type="bibr" rid="bib16">Lichtman et al., 2014</xref>) and produce a high-fidelity assembled volume. To match the speed of the EM imaging, the volume assembly pipeline needs to be automated to handle millions of images per day from multiple microscopes. Though electron microscopy is notorious for creating very large datasets, other volume microscopy technologies that collect 3D data would also gain from advances in automated and scalable methods for stitching and alignment.</p><p>Imaging and 3D reconstruction of biological samples usually involve a series of stages from preparing the tissue, cutting it into serial sections, imaging them using an image acquisition system, 2D registration and 3D alignment of those serial sections, and finally 3D segmentation (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Each serial section imaged comprises of several hundreds to several thousands of images depending on the resolution at which they are imaged. The volume assembly process that registers and aligns these images works under the assumption that the images within a serial section carry some overlap between neighboring tile images (<xref ref-type="fig" rid="fig1">Figure 1b and c</xref>). The images are registered based on some points of interest that are extracted from the overlapping region (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). This also requires the raw tile images to be corrected for any lens distortion effects that arise from the acquisition system (<xref ref-type="fig" rid="fig1">Figure 1e–g</xref>). The stitched serial sections can then be 3D aligned using a similar process of matching patterns between the montages. The challenge in the volume assembly process arises when the throughput has to be matched with the acquisition system for large-scale datasets. Also, a highly accurately aligned 3D volume is necessary for further segmentation and reconstruction.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Volume assembly pipeline.</title><p>(<bold>a</bold>) Different stages of the electron microscopy (EM) dataset collection pipeline. The biological sample is prepared and cut into thin slices that are imaged using the desired image acquisition system (electron microscopy for datasets discussed in this work). The raw tile images from each section are then stitched together in 2D followed by a 3D alignment of them. (<bold>b</bold>) A pair of raw tile images before 2D stitching. The tiles have a certain overlap between them and are not aligned (the zoomed-in regions show the misalignment) and hence require a per-tile transformation to <italic>stitch</italic> them together. (<bold>c</bold>) The pair of tile images from (<bold>b</bold>) after stitching is performed. The zoomed-in regions illustrate the alignment of these images after stitching. (<bold>d</bold>) Conceptual diagram illustrating the series of steps that are involved in the 2D stitching of the serial sections. The steps include computation of lens distortion correction transformation followed by generation of point correspondences between the overlapping tile images and, finally, computation of per-tile montage transformations using the point correspondences. (<bold>e</bold>) A raw tile image without any lens distortion correction. (<bold>f</bold>) Tile image from (<bold>e</bold>) after lens distortion correction transformation is applied. (<bold>g</bold>) A quiver plot showing the magnitude and direction of distortion caused by the lens from the acquisition system.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-fig1-v2.tif"/></fig><p>Several tools used in various stages of volume assembly pipelines perform image registration by extracting and matching similar features across overlapping images (<xref ref-type="bibr" rid="bib6">Cardona et al., 2012</xref>; <xref ref-type="bibr" rid="bib34">Wetzel et al., 2016</xref>; <xref ref-type="bibr" rid="bib5">Bock et al., 2011</xref>; <xref ref-type="bibr" rid="bib13">Karsh, 2016</xref>). Image registration using Fourier transformation (<xref ref-type="bibr" rid="bib34">Wetzel et al., 2016</xref>) was used to successfully align mouse and zebrafish brain datasets acquired using wager mapper ssEM imaging technology. The Fiji (<xref ref-type="bibr" rid="bib27">Schindelin et al., 2012</xref>; <xref ref-type="bibr" rid="bib24">Rasband, 2012</xref>) plugin TrakEM2 (<xref ref-type="bibr" rid="bib6">Cardona et al., 2012</xref>) includes a comprehensive set of tools and algorithms to perform stitching and alignment of various types of microscopy image formats. AlignTK (<xref ref-type="bibr" rid="bib5">Bock et al., 2011</xref>) implements scalable deformable 2D stitching and serial section alignment for large serial section datasets using local cross-correlation. An end-to-end pipeline to perform volume assembly and segmentation using existing tools was developed by R. <xref ref-type="bibr" rid="bib33">Vescovi, 2020</xref> and was designed to run on varied computational systems. The pipeline was shown to process smaller datasets through supercomputers efficiently. While these approaches have been successfully used in the volume assembly of smaller datasets, they do not scale well for large-scale datasets, lack support for different classes of geometric transformations, or do not incorporate reliable filters for false matches due to imaging artifacts (<xref ref-type="bibr" rid="bib15">Khairy et al., 2018</xref>).</p><p>We propose a volume assembly pipeline – <italic>ASAP</italic> (Assembly Stitching and Alignment Pipeline; <ext-link ext-link-type="uri" xlink:href="https://github.com/AllenInstitute/asap-modules">https://github.com/AllenInstitute/asap-modules</ext-link>; <xref ref-type="bibr" rid="bib19">Mahalingam, 2022</xref>) – that is capable of processing petascale EM datasets with high-fidelity stitching and processing rates that match the speed of imaging. Our pipeline is based on the volume assembly framework proposed in <xref ref-type="bibr" rid="bib36">Zheng et al., 2018</xref> and is capable of achieving high throughput by means of metadata operations on every image in the dataset. The metadata and transformations associated with each image are stored in a MongoDB database fronted by <italic>Render</italic> (<xref ref-type="bibr" rid="bib32">Trautman and Saalfeld, 2019</xref>) services to dynamically render the output at any stage in the pipeline. The effectiveness of the pipeline has been demonstrated in the volume assembly of multiple petascale volumes and integrates well with SEAMLeSS (<xref ref-type="bibr" rid="bib18">Macrina et al., 2021</xref>), which provided the final 3D alignment of these volumes.</p><p>The pipeline described here for assembly of large connectomics volumes is divided into two sections: (1) a software package that is scalable, modular, and parallelized and is deployable in varied computing environments to perform volume assembly of EM serial sections; (2) a workflow engine and a volume assembly workflow that utilizes these tools to automate the processing of raw EM images from a multiscope setup using high-performance computing (HPC) systems. The tools in ASAP are open source and include abstract-level functionalities to execute macro-level operations that execute a series of steps required for the execution of different stages of the pipeline. An example of such a macro operation is the computation of point-match correspondences, which requires the generation of tile pairs and generation of point matches using those tile pairs. The modularity of the tools allows for easy implementation of other algorithms into the pipeline without making major changes to the existing setup. The software tools can be easily deployed in different computing environments such as HPC systems, cloud-based services, or on a desktop computer in a production-level setting. The software stack also includes a set of quality c ontrol (QC) tools that can be run in an automated fashion to assess the quality of the stitched montages. These software tools can be easily utilized by workflow managers running the volume assembly workflow to achieve high throughput. The tools are designed to generalize well for other datasets from different domains (that carry the assumption of generating overlapping images) and can be adapted to process such datasets. We have also developed a workflow manager <italic>BlueSky</italic> (<ext-link ext-link-type="uri" xlink:href="https://github.com/AllenInstitute/blue_sky_workflow_engine">https://github.com/AllenInstitute/blue_sky_workflow_engine</ext-link>; <xref ref-type="bibr" rid="bib20">Melchor et al., 2021</xref>) that implements the volume assembly workflow using our software stack. The proposed pipeline combined with <italic>BlueSky</italic> has been successfully used to stitch and align several high-resolution mm<sup>3</sup> EM volume from the mouse visual cortex and a human dataset at speeds higher than the imaging rate of these serial sections from a highly parallelized multiscope setup.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Development of a stitching and alignment pipeline</title><p>The pipeline (ASAP) described in this work is based on the principles described by <xref ref-type="bibr" rid="bib14">Kaynig et al., 2010</xref>, <xref ref-type="bibr" rid="bib26">Saalfeld et al., 2010</xref>, and <xref ref-type="bibr" rid="bib36">Zheng et al., 2018</xref>, and scales the software infrastructure to stitch and align petascale datasets. It includes the following stages: (1) lens distortion computation, (2) 2D stitching, (3) global section-based nonlinear 3D alignment, (4) fine 3D alignment, and (5) volume assembly. ASAP performs feature-based stitching and alignment in which point correspondences between two overlapping images are extracted and a geometric transformation is computed using these point correspondences to align the images.</p><p><xref ref-type="fig" rid="fig2">Figure 2</xref> shows the volume assembly pipeline (ASAP) for building 3D reconstruction out of serial section transmission electron microscopy (ssTEM) images. First, single images from serial sections from ssTEM are collected. As the field of view is limited, multiple images that overlap with each other are imaged to cover the entire section. Images acquired by ssTEMs can include dynamic nonlinear distortions brought about by the lens system. A compensating 2D thin plate spline transformation is derived using a custom triangular mesh-based strategy (<xref ref-type="bibr" rid="bib7">Collins et al., 2019</xref>) based on point correspondences of overlapping image tiles as in <xref ref-type="bibr" rid="bib14">Kaynig et al., 2010</xref>. The point correspondences (also referred to as point matches) are extracted using SIFT <xref ref-type="bibr" rid="bib17">Lowe, 2004</xref> and a robust geometric consistency filter using a local optimization variant of RANSAC <xref ref-type="bibr" rid="bib11">Fischler and Bolles, 1981</xref> and robust regression (<xref ref-type="bibr" rid="bib26">Saalfeld et al., 2010</xref>) (see ‘Methods’ for more details). These point correspondences, in lens-corrected coordinates, are then used to find a per-image affine/polynomial transformation that aligns the images in a section with each other to create a <italic>montage</italic>. The affine/polynomial transformations are computed using a custom Python package, BigFeta, which implements a direct global sparse matrix solving strategy based on <xref ref-type="bibr" rid="bib15">Khairy et al., 2018</xref>. The stitched montages are then globally aligned with each other in 3D. The 3D global alignment is performed by extracting point correspondences between low-resolution version of the 2D stitched sections and solved with BigFeta to obtain a thin plate spline per section transformation. This 3D alignment is the result of a progressive sequence of rotational, affine, and thin plate spline solves with tuned regularization parameters such that each solution initializes the next more deformable, yet increasingly regularized transformation. The globally aligned transformations can then be used as an initialization for computing finer and denser alignment transformations (an example of this is the fine alignment described in <xref ref-type="bibr" rid="bib18">Macrina et al., 2021</xref>), which is computed on a per-image basis at a much higher resolution. Several iterations of the global 3D alignment are performed to achieve a good initialization for the fine alignment process. For all the datasets presented in this article, the 2D stitching and global alignment was performed using ASAP, and afterward the data was materialized and transferred outside of ASAP for fine alignment using SEAMLeSS (<xref ref-type="bibr" rid="bib18">Macrina et al., 2021</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Assembly Stitching and Alignment Pipeline (ASAP) – volume assembly workflow.</title><p>(<bold>a</bold>) The different steps of image processing in <italic>ASAP</italic> for electron microscopy (EM) serial sections. The infrastructure permits multiple possible strategies for 3D alignment, including a chunk-based approach in case it is not possible to 3D align the complete dataset at once, as well as using other workflows outside ASAP (<xref ref-type="bibr" rid="bib18">Macrina et al., 2021</xref>; <ext-link ext-link-type="uri" xlink:href="https://www.microns-explorer.org/cortical-mm3">https://www.microns-explorer.org/cortical-mm3</ext-link>) for fine 3D alignment with the global 3D aligned volume obtained using ASAP. (<bold>b–d</bold>) Representation of different modules in the software infrastructure. The green boxes represent software components, the orange boxes represent processes, and the purple processes represent databases. The color of the outline of the box matches its representation in the image processing steps shown in (<bold>a</bold>). (<bold>b</bold>) Schematic showing the lens distortion computation. (<bold>c</bold>) Schematic describing the process of data transfer and storage along with MIPmaps generation using the data transfer service <italic>Aloha</italic>. (<bold>d</bold>) Schematic illustrating the montaging process of serial sections. The same software infrastructure of (<bold>d</bold>) is then also used for 3D alignment as shown by the red boxes in (<bold>a</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-fig2-v2.tif"/></fig><p>In a continuous processing workflow scenario, the serial sections from multiple ssTEMs are stitched immediately once they are imaged. 3D alignment is performed on chunks of contiguous sections that partially overlap with their neighboring chunks. These independently 3D aligned chunks can be assembled to a full volume by aligning them rigidly and interpolating the transformations in the overlapping region (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p></sec><sec id="s2-2"><title>Software infrastructure supporting stitching and alignment</title><p>Our software infrastructure is designed to support EM imaging pipelines such as piTEAM (<xref ref-type="bibr" rid="bib34">Wetzel et al., 2016</xref>) that produce multiple serial sections from a parallelized scope setup every hour. The infrastructure is designed for processing petascale datasets consisting of millions of partially overlapping EM images. The infrastructure consists of four core components: (1) a modular set of software tools that implements each stage of ASAP (asap-modules), (2) a service with REST APIs to transfer data from the microscopes to storage hardware (Aloha); (3) REST APIs for creating, accessing, and modifying image metadata (Render); and (4) a matrix-based registration system (BigFeta). Below we provide a brief description of these components with a more detailed description in the section ‘ASAP modules’.</p><p>ASAP is implemented as a modular set of tools that includes abstract-level functions to execute for each stage of the volume assembly pipeline. It also includes QC tools to assess stitching quality, render results to disk at any stage of the pipeline, obtain optimal parameters for computing point correspondences, and obtain optimal parameters for solving optimal transformations. Asap-modules is supported by <italic>render-python</italic> for read/writes to the database and <italic>argschema</italic> for its input and output data validation (see ‘Methods’ for more details).</p><p>Aloha is an image transfer service (<xref ref-type="fig" rid="fig2">Figure 2c</xref>) that receives raw images and their metadata from the microscopes, stores them in primary data storage, and losslessly compresses the original data to reduce the storage footprint. It includes REST APIs for clients to GET/POST images and their metadata. It also produces downsampled representations of the images for faster processing and visualization.</p><p>Render (<xref ref-type="bibr" rid="bib32">Trautman and Saalfeld, 2019</xref>) provides logic for image transformation, interpolation, and rendering. It is backed by a MongoDB document store that contains JSON (JavaScript Object Notation) tile specifications with image metadata and transformations. Render’s REST APIs are accessed by asap-modules using render-python to create, access, and modify image metadata in the database. The REST APIs allow the user to access the current state of any given set of image tiles during the stitching process. Render also includes a point-match service that handles the storage and retrieval of point correspondences in a database since computing point correspondences between millions of pairs of images are computationally expensive. Another advantage of storing the point correspondences in a database is that it is agnostic to the algorithm that is used for the computation of these point correspondences. The <italic>point-match service</italic> (<xref ref-type="fig" rid="fig2">Figure 2c and e</xref>) handles the data ingestion and retrieval from the database using REST APIs with both operations being potentially massively distributed.</p><p>BigFeta is a matrix-based registration system that estimates the image transformations using the point correspondences associated with the image. BigFeta includes transformations such as rotations to implement rigid alignments, and 2D thin plate spline transformations that are useful for 3D image alignments. BigFeta can also be integrated with distributed solver packages such as PETSc (<xref ref-type="bibr" rid="bib3">Balay et al., 2019</xref>) for solving large sparse matrices involving billions of point correspondences.</p><p>We also developed a workflow manager <italic>BlueSky</italic> as well as an associated volume assembly workflow to automatically process serial sections as they are continuously ingested during the imaging process. It utilizes the abstract-level functions in asap-modules to create workflows for each stage of the volume assembly pipeline.</p><p>Our alignment pipelines operate only on metadata (point correspondences and transformations) derived from image tiles – a feature derived from the Render services, thus allowing efficient processing of petascale datasets and the feasibility of real-time stitching with proper infrastructure. Where possible, the pipeline works with downscaled versions of image tiles (MIPmaps) that dramatically increases processing speed and reduces disk usage as raw data can be moved to a long-term storage for later retrieval.</p><p>Beyond the use of this software infrastructure for EM data, which drove the development that we describe in this article, the pipeline also has multichannel processing capabilities and can be applied to fluorescence and multimodal datasets like array tomography (see Figure 8).</p></sec><sec id="s2-3"><title>Data acquisition and initiation of image processing</title><p>An important first step in our pipeline is the correction of lens distortion effects on raw images. Lens distortions are calculated from a special set of images with high tile overlap. These <italic>calibration montages</italic> are collected at least daily and after any event that might affect the stability of the beam (e.g., filament replacement). This step is followed by the acquisition of the neuroanatomical dataset, for which a bounding box is drawn around the region of interest (ROI) in each ultra-thin section. In certain situations, multiple ROIs are required per section. The volume assembly workflow accepts multiple entries referencing the same placeholder label to support reimaging. At the end of each acquisition session, the tiles, tile manifest, and session log are uploaded to the data center storage cluster and the lens correction and montaging workflows in the volume assembly workflow are triggered. <xref ref-type="fig" rid="fig3">Figure 3</xref> shows the specialized services that facilitate data transfer and tracking from high-throughput microscopes to shared compute resources.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Data flow diagram.</title><p>A schematic diagram showing the flow of image data, metadata, and processed data between microscopes. Raw images and metadata are transferred from microscopes to our data transfer system (<italic>Aloha</italic>) and transmission electron microscopy (TEM) database, respectively. Aloha generates MIPmaps and compresses images and transfers them to the storage cluster for further processing by <italic>ASAP</italic>. Metadata is transferred to BlueSky through TEM database, which triggers the stitching and alignment process. The metadata from the stitching process is saved in the Render services database. The final assembled volume is transferred to the cloud for further fine alignment and segmentation. The hardware configurations are presented in Appendix 5.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-fig3-v2.tif"/></fig><p>This infrastructure was used to process multiple petascale datasets, including a 1 mm<sup>3</sup> (mouse dataset 1) of the mouse brain that is publicly available at <ext-link ext-link-type="uri" xlink:href="https://www.microns-explorer.org/">microns-explorer</ext-link> (<xref ref-type="bibr" rid="bib21">MICrONS Consortium et al., 2021</xref>). Over 26,500 sections were imaged at 4 nm/pixel resolution using five microscopes, running in a continuous and automated fashion (W et al. 2020). Each montage is composed of ~5000 tiles of 15 µm × 15 µm with an overlap of 13% in both <inline-formula><mml:math id="inf1"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf2"><mml:mi>y</mml:mi></mml:math></inline-formula> directions. The total file size of a single montage is about 80 GB, and thus a daily throughput of 3.6 TB per system is produced in a continuous imaging scenario. Part of the dataset was imaged using a 50 MP camera with an increased tile size to 5408 × 5408 pixels. This resulted in montages with ~2600 tiles at an overlap of 9% in both <inline-formula><mml:math id="inf3"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf4"><mml:mi>y</mml:mi></mml:math></inline-formula> directions. The infrastructure was also used to process two other large mouse datasets and a human dataset. The details about these datasets are shown in <xref ref-type="table" rid="table1">Table 1</xref>, where the ROI size and total nonoverlapping dataset size (without repeated pixels) were determined from montage metadata, including pixel size and nominal overlap.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Details of datasets processed using Assembly Stitching and Alignment Pipeline (ASAP) – volume assembly pipeline.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dataset</th><th align="left" valign="bottom">No. of sections</th><th align="left" valign="bottom">Pixel resolution (nm/pixel)</th><th align="left" valign="bottom">Image size</th><th align="left" valign="bottom">Tile overlap</th><th align="left" valign="bottom">Total size (PiB)</th><th align="left" valign="bottom">Total size (nonoverlap) (PiB)</th><th align="left" valign="bottom">ROI size (in microns)</th></tr></thead><tbody><tr><td align="left" valign="bottom">Mouse dataset 1</td><td align="char" char="." valign="bottom">19,945</td><td align="left" valign="bottom">3.95–4</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf5"><mml:mrow><mml:mn>3840</mml:mn><mml:mo>×</mml:mo><mml:mn>3840</mml:mn></mml:mrow></mml:math></inline-formula> (20MP camera), <inline-formula><mml:math id="inf6"><mml:mrow><mml:mn>5408</mml:mn><mml:mo>×</mml:mo><mml:mn>5408</mml:mn></mml:mrow></mml:math></inline-formula> (50MP camera)</td><td align="left" valign="bottom">13%, 9%</td><td align="char" char="." valign="bottom">1.6</td><td align="char" char="." valign="bottom">1.2</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf7"><mml:mrow><mml:mn>1300</mml:mn><mml:mo>×</mml:mo><mml:mn>870</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Mouse dataset 2</td><td align="char" char="." valign="bottom">17,593</td><td align="left" valign="bottom">4.67–5 (4.78 mean)</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf8"><mml:mrow><mml:mn>5376</mml:mn><mml:mo>×</mml:mo><mml:mn>5376</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">9–10%, 9.4% mean</td><td align="char" char="." valign="bottom">0.984</td><td align="char" char="." valign="bottom">0.807</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf9"><mml:mrow><mml:mn>1191</mml:mn><mml:mo>×</mml:mo><mml:mn>815</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Mouse dataset 3</td><td align="char" char="." valign="bottom">17,310</td><td align="left" valign="bottom">3.8–4.15 (3.91 mean)</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf10"><mml:mrow><mml:mn>5376</mml:mn><mml:mo>×</mml:mo><mml:mn>5376</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">9–10%, 9.1% mean</td><td align="char" char="." valign="bottom">0.665</td><td align="char" char="." valign="bottom">0.549</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf11"><mml:mrow><mml:mn>647</mml:mn><mml:mo>×</mml:mo><mml:mn>672</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Human</td><td align="char" char="." valign="bottom">9673</td><td align="left" valign="bottom">4.65–4.95 (4.78 mean)</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf12"><mml:mrow><mml:mn>5376</mml:mn><mml:mo>×</mml:mo><mml:mn>5376</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">9–10%, 9.5% mean</td><td align="char" char="." valign="bottom">1.18</td><td align="char" char="." valign="bottom">0.968</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf13"><mml:mrow><mml:mn>2315</mml:mn><mml:mo>×</mml:mo><mml:mn>987</mml:mn></mml:mrow></mml:math></inline-formula></td></tr></tbody></table><table-wrap-foot><fn><p>ROI, region of interest.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s2-4"><title>Automated petascale stitching</title><p>Besides stitching and aligning large-scale datasets, a requirement for the volume assembly pipeline is to achieve a rate that matches or exceeds the imaging speed so as to provide rapid feedback on issues with the raw data encountered during the stitching process. This is achieved in our pipeline using an automated workflow manager (BlueSky) that executes the volume assembly pipeline to continuously process serial sections from five different autoTEMs (<xref ref-type="bibr" rid="bib34">Wetzel et al., 2016</xref>).</p><p>The images from the autoTEMs are transferred to the Aloha service without sending them to storage servers directly. The Aloha service generates MIPmaps, compresses the raw images, and then writes them to the storage servers. The sections processed by Aloha are then POSTed to the BlueSky workflow manager, which initiates the montaging process. During an imaging run, each microscope uploads raw data and metadata to Aloha using a concurrent upload client. Limitations of the autoTEM acquisition computers cap the Aloha client throughput at 0.8–1.2 Gbps per microscope, which is sufficient for daily imaging with a 50 MP camera as described in <xref ref-type="bibr" rid="bib35">Yin et al., 2020</xref>; <xref ref-type="bibr" rid="bib34">Wetzel et al., 2016</xref>. Transferring previously imaged directories from high-performance storage servers has shown that an Aloha deployment on multiple machines is capable of saturating a 10 Gbps network uplink. The serial sections are assigned <italic>pseudo z</italic> indices to account for errors in metadata from the scopes such as barcode reading errors that assigns incorrect z indices. The lens correction workflow is triggered to compute a transformation that can correct lens distortion effects on the raw images. This transformation is updated in the image metadata so as to be used in subsequent stages of volume assembly. The montaging workflow in BlueSky triggers the generation of point correspondences and stores them in the database using the point-match service, followed by calculating the globally optimal affine/polynomial transformation for each image tile in the montage using the BigFeta solver. The transformations are saved as metadata associated with each tile image in the Render services database. The montages go through an automated QC process to ensure a high-fidelity stitching (see ‘Automated montage QC’), followed by a global 3D alignment of the entire dataset.</p><p>ASAP is capable of performing the global 3D alignment in chunks, making it scalable to use in larger datasets or with fewer computational resources. However, all our datasets have been 3D aligned as a single chunk. The montages are rendered to disk at a scale of 0.01 and point correspondences are computed between the neighboring sections represented by their downsampled versions. A per-section thin plate spline transformation is computed using 25–49 control points in a rectangular grid. The per-section transformation is then applied to all the tile images in that section to globally align them in 3D.</p></sec><sec id="s2-5"><title>Automated montage QC</title><p>QC is a crucial step at each stage of processing in EM volume assembly to ensure that the outcome at each stage is of high quality. ASAP-modules include a comprehensive set of tools to perform QC of the computed lens correction transformations, stitched montages, and 3D aligned volume. These tools are integrated within the lens correction and montaging workflow in the volume assembly workflow to automatically compute statistical metrics indicating the stitching quality and also generates maps of montages showing potential stitching issues (see <xref ref-type="fig" rid="fig4">Figure 4</xref>). The stitched montages that pass QC are automatically moved to the next stage of processing, thus enabling faster processing with minimal human intervention but ensuring a high-quality volume assembly.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>2D stitching and automated assessment of montage quality.</title><p>(<bold>a</bold>) Schematic diagram of the montage transformation using point correspondences. (<bold>b</bold>) Montage 2D stitched section from mouse dataset 1 (publicly available at <ext-link ext-link-type="uri" xlink:href="https://www.microns-explorer.org">https://www.microns-explorer.org</ext-link>; <xref ref-type="bibr" rid="bib21">MICrONS Consortium et al., 2021</xref>). (<bold>c</bold>) Single-acquisition tile from the section in (<bold>b</bold>). (<bold>d</bold>, <bold>e</bold>) Detail of synapses (arrow heads) from the tile shown in (<bold>c</bold>). (<bold>f</bold>) Quality control (QC) plot of a stitched electron microscopy (EM) serial section with nonoptimal parameters. Each blue square represents a tile image of how they appear aligned in the montage. The red squares represent tile images that have gaps in stitching with neighboring tile images and are usually located in regions with resin or film. (<bold>g</bold>) A zoom-in region of the 2D montage in (<bold>f</bold>) showing the seam (white arrows) between tiles causing misalignment (red arrowheads) between membranes. (<bold>h</bold>) A zoomed-in region of the section showing a tile having a gap with its neighbors. (<bold>i</bold>) QC plot of a stitched EM serial section after parameter optimization. (<bold>j</bold>) A zoom-in region of the 2D montage in (<bold>i</bold>) showing no seams in the same region as in (<bold>g</bold>). The red arrowheads show the same locations as in (<bold>g</bold>). (<bold>k</bold>) A schematic plot representing the number of point correspondences between every pair of tile images for a section of the human dataset. Each edge of the squares in the plot represents the existence of point correspondences between tile images centered at the end points of the edge. The color of the edge represents the number of point correspondences computed between those tile image pairs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-fig4-v2.tif"/></fig><p>The stitching issues that are identified include misalignments between stitched tiles, gaps in montages, and seams. These issues are identified based on the mean residual between point correspondences from every pair of tiles. This represents how well the point correspondences have aligned from each of the tiles after montage transformations are applied to them (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). This metric is represented in pixel distance and is used to locate areas of misalignments and seams. The gaps in stitching are identified by means of how many neighbor a tile image has before and after stitching and based on their area of overlap with its neighbors. A seam appears as misalignment between a tile and many of its neighbors and is identified using a cluster of point correspondences whose residuals are above a certain threshold. In addition to these metrics, we also compute the mean absolute deviation (MAD) (<xref ref-type="fig" rid="fig5">Figure 5</xref>) that measures the amount of distortion a tile image undergoes with the transformation. The MAD statistics is a measure using which we identify montages that are distorted (<xref ref-type="fig" rid="fig5">Figure 5</xref>) once it passes the automated QC identifying other issues. Since the crux of our computations is based on the point correspondences, we also generate plots to quickly visualize the density of point correspondences between tile images within a section (<xref ref-type="fig" rid="fig4">Figure 4k</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Median absolute deviation (MAD) statistics for montage distortion detection.</title><p>(<bold>a</bold>) Schematic description of computation of MAD statistics for a montage. (<bold>b</bold>) A scatter plot of <inline-formula><mml:math id="inf14"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf15"><mml:mi>y</mml:mi></mml:math></inline-formula> MAD values for each montage. A good stitched section without distorted tile images falls in the third quadrant (where point d is shown). (<bold>c</bold>) An example of a distorted montage of a section solved using unoptimized set of parameters. Row 1 shows the downsampled version of the montaged section, row 2 shows the quality control (QC) plot of the section showing the distortions, and row 3 shows the <inline-formula><mml:math id="inf16"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf17"><mml:mi>y</mml:mi></mml:math></inline-formula> absolute deviation distribution for the unoptimized montage. (<bold>d</bold>) Section shown in (<bold>c</bold>) solved with optimized parameters with row 1 showing the downsampled montage, row 2 showing the QC plot of the section, and row 3 showing the <inline-formula><mml:math id="inf18"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf19"><mml:mi>y</mml:mi></mml:math></inline-formula> absolute deviation distribution for the section.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Parameter optimization.</title><p>(<bold>a</bold>) Plots showing the residuals (red curve) with variations in the scale parameter (blue curve) and median absolute deviation (MAD) parameter (purple curve). The optimal solve occurs between (<bold>d</bold>) and (<bold>e</bold>) in the figure. A high MAD value indicates deformation of the tiles and is consistent with the scale changes to the tile outline images. An optimal set of solver parameters that produce a low MAD and residuals are selected for montaging. (<bold>b</bold>) Representation of a serial section with each square representing a tile image. The section shows significant deformations when solved with unoptimal parameters (location b in <bold>[a]</bold>). (<bold>c</bold>) The same serial section from (<bold>b</bold>) solved with parameters in the region pointed by c in (<bold>a</bold>). (<bold>d</bold>) The same serial section from (<bold>b</bold>) solved with parameters in the region pointed by d in (<bold>a</bold>). (<bold>e</bold>) The same serial section from (<bold>b</bold>) solved with optimal set of parameters from the region pointed by e in (<bold>a</bold>). The tile images shown in the last column show the quality of solve from each of these parameter sets.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-fig5-figsupp1-v2.tif"/></fig></fig-group><p>The QC maps (<xref ref-type="fig" rid="fig4">Figure 4f and i</xref>) of the montages provide a rapid means to visually inspect and identify stitching issues associated with the montage without the need to materialize or visualize large-scale serial sections. The QC map reveals the location of gaps and seams between tiles in addition to providing an accurate thumbnail representation of the stitched section. The QC maps also provide an interactive way for the user to click on the thumbnail representation of a tile to visualize the tile image along with its neighbors in the stitched montage. This provides a means to quickly inspect individual tiles that have stitching issues. While the QC maps provide a quick view of the issues related to a montage, the Neuroglancer (<xref ref-type="bibr" rid="bib22">Neuroglancer, 2010</xref>) tool can further facilitate the dynamic rendering of an ROI or the entire montaged section for further inspection. This provides the advantage of not requiring to render the intermediate output to disk.</p><p>A seam (<xref ref-type="fig" rid="fig4">Figure 4g</xref>) is defined as a misalignment between two tiles and is identified by means of the pixel residuals between point correspondences between the tiles. Misalignments can be eliminated by solving correct transformations using optimized sets of parameters. A gap between tile images (<xref ref-type="fig" rid="fig4">Figure 4h</xref>) is usually the result of inaccurate montage transformations that are caused by lack of point correspondences between tile pairs where the gap appears. Tile pairs that include features like blood vessel, resin, or film region, etc. (see <xref ref-type="fig" rid="fig4">Figure 4h</xref>), lack point correspondences, thus causing a gap between the tiles during stitching. The stitching issues associated with the resin or film region are ignored, while the gaps in tiles containing blood vessels are solved with optimal parameters to ensure no misalignments between the tile and its neighbors. The tile images that are entirely part of a blood vessel lack textural features for generation of point matches and hence are dropped by the solver during montaging. However, tile images that partially cover the blood vessel region undergo generation of point correspondences at a higher resolution followed by montaging using optimal parameters. This usually resolves the misalignments, but our framework does not limit the use of other algorithms such as phase correlation or cross-correlation for resolving such issues.</p><p>Sections that failed QC are examined by a human proofreader and moved to the appropriate stage of reprocessing. A manual proofreading process typically includes examining the QC plot for issues and further visualizing those areas in montage with issues to ensure that those issues either correspond to resin or film region tiles or tiles corresponding to tissue region. The regions with misalignments are further examined to send them to the appropriate stage of processing. If the misalignments are caused due to insufficient point correspondences, then they are sent to the point-matching stage of the montage workflow for generation of point correspondences at a higher resolution. Misaligned sections with sufficient point correspondences are sent to the solver stage with new parameters. These parameters were heuristically chosen by means of a parameter optimization algorithm based on the stitching quality metrics (see’Montage parameter optimization’ for more details and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> for optimized parameter selection plots).</p><p>Unoptimized parameters can also lead to distorted montages where individual tiles are distorted (see <xref ref-type="fig" rid="fig5">Figure 5c and d</xref> for distorted and undistorted versions of the same montage). The median absolute deviation (MAD) (<xref ref-type="fig" rid="fig5">Figure 5a and b</xref>) statistic provides a computational assessment of the quality of the montage and aids in the selection of optimized set of parameters to solve for a montage. The optimal <inline-formula><mml:math id="inf20"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf21"><mml:mi>y</mml:mi></mml:math></inline-formula> MAD statistic values were heuristically selected for every dataset.</p></sec><sec id="s2-6"><title>Performance of the volume assembly pipeline: ASAP</title><p>High-quality 2D stitching and 3D alignment are necessary for accurate neuroanatomy reconstruction and detection of synaptic contacts. The 2D stitching quality is assessed by a residual metric, which computes the sum of squared distances between point correspondences post stitching (see <xref ref-type="fig" rid="fig6">Figure 6a</xref>). A median residual of &lt;5 pixels was achieved for sections from all our datasets (top figure in <xref ref-type="fig" rid="fig6">Figure 6b–e</xref>), which is a requirement for successful 3D segmentation (<xref ref-type="bibr" rid="bib18">Macrina et al., 2021</xref>) in addition to having no other stitching issues as described above. We aimed at 5 pixels (20 nm) as the target accuracy of the stitching because it is 10 times smaller than the average diameter of a spine neck (<xref ref-type="bibr" rid="bib1">Arellano et al., 2007</xref>) and half the diameter of very thin spine necks. The violin plots in <xref ref-type="fig" rid="fig6">Figure 6</xref> depict the density distribution of the median residual values computed for every serial section from our datasets and are grouped by the acquisition systems. It can be seen that the density of distribution is below the threshold value (the horizontal line in these plots), indicating the stitching quality of the serial sections. A small number of sections reported high residuals even with the optimized set of solver parameters (<xref ref-type="fig" rid="fig6">Figure 6b–e</xref>). An attempt to re-montage them with parameters that will reduce the residuals resulted in distorting individual tile images. Hence, these sections were montaged using a set of parameters that produces a montage with less distorted tiles and a residual that can be tolerated by the 3D fine alignment process and further segmentation. Overall, we aim to achieve high-fidelity stitching by attempting to keep the residuals within the threshold, while preserving the image scales in both <inline-formula><mml:math id="inf22"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf23"><mml:mi>y</mml:mi></mml:math></inline-formula> closer to 1 (<xref ref-type="fig" rid="fig6">Figure 6</xref>) and occasionally allowing montages with residuals above the threshold.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Performance of 2D stitching pipeline.</title><p>(<bold>a</bold>) Schematic diagram explaining the computation of residuals between a pair of tile images post stitching. Residuals is a metric that is used to assess the quality of stitching in our pipeline. (<bold>b–e</bold>, top): panels (<bold>b–e</bold>) show the median of tile residuals per section grouped by their acquisition transmission electron microscopy (TEM). The horizontal line in these figures marks the threshold value that is set to assess the quality of stitching. <xref ref-type="table" rid="table2">Table 2</xref> shows the median residual values in <italic>nm</italic> for all our datasets. (<bold>b–e</bold> , bottom): panels (<bold>b–e</bold>) show the median <inline-formula><mml:math id="inf24"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf25"><mml:mi>y</mml:mi></mml:math></inline-formula> scale distribution of the tile images for all the datasets grouped by their acquisition system. The <inline-formula><mml:math id="inf26"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf27"><mml:mi>y</mml:mi></mml:math></inline-formula> scales of the tile images post 2D stitching indicate the level of deformation that a tile image undergoes post stitching – an indicator of the degree of quality of the 2D montaged section. (<bold>b</bold>) Mouse dataset 1. (<bold>c</bold>) Mouse dataset 2. (<bold>d</bold>) Human dataset. (<bold>e</bold>) Mouse dataset 3.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-fig6-v2.tif"/></fig><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Performance of 2D stitching pipeline.</title><p>Table shows the values of metrics computed to assess the quality of the stitched montages from all our datasets. The median residual value and the median scale in the x-axis and y-axis measures the accuracy of the alignment and the deformation factor of the individual tile images.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Dataset</th><th align="left" valign="top">Median residuals (pixels)</th><th align="left" valign="top">Median residuals (nm)</th><th align="left" valign="top">Median scale (x)</th><th align="left" valign="top">Median scale (y)</th></tr></thead><tbody><tr><td align="left" valign="top">Human dataset</td><td align="left" valign="top">2.59</td><td align="left" valign="top">12.38</td><td align="left" valign="top">0.98</td><td align="left" valign="top">1.02</td></tr><tr><td align="left" valign="top">Mouse dataset 1</td><td align="left" valign="top">2.10</td><td align="left" valign="top">8.36</td><td align="left" valign="top">0.99</td><td align="left" valign="top">1.00</td></tr><tr><td align="left" valign="top">Mouse dataset 2</td><td align="left" valign="top">2.21</td><td align="left" valign="top">10.56</td><td align="left" valign="top">0.98</td><td align="left" valign="top">1.00</td></tr><tr><td align="left" valign="top">Mouse dataset 3</td><td align="left" valign="top">2.37</td><td align="left" valign="top">9.27</td><td align="left" valign="top">0.99</td><td align="left" valign="top">1.01</td></tr></tbody></table></table-wrap><p>The global 3D alignment process produces a volume that is ‘roughly’ aligned as the point correspondences are generated from montages materialized at 1% scale. This rough alignment provides a good initial approximation for fine alignment of the volume and for generating point correspondences at higher resolutions. The quality of global nonlinear 3D alignment is measured by computing the angular residuals between pairs of sections (within a distance of three sections in <inline-formula><mml:math id="inf28"><mml:mi>z</mml:mi></mml:math></inline-formula>). The angular residual is computed using the point correspondences between a section and its neighbors. The angular residual is defined as the angle between two vectors formed by a point coordinate (from first section) and its corresponding point coordinate from a neighboring section. The origin of the two vectors is defined as the centroid of the first sections’ point coordinates. The median of the angular residuals is reported as a quality metric for the global 3D alignment for our datasets (<xref ref-type="fig" rid="fig7">Figure 7f</xref>). The quality metric ensures a high-quality global nonlinear 3D alignment of the sections in all three (<inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula>) planes of the volume (see <xref ref-type="fig" rid="fig7">Figure 7</xref> for global nonlinearly 3D aligned slices from mouse dataset 1 and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplements 1</xref>–<xref ref-type="fig" rid="fig7s3">3</xref> for slices from other datasets). For the datasets described in this article, this global alignment was the initialization point for the fine alignment done outside ASAP with SEAMLeSS (<xref ref-type="bibr" rid="bib18">Macrina et al., 2021</xref>). An illustration of the fine-aligned volume using SEAMLeSS on mouse dataset 1 can be found at <ext-link ext-link-type="uri" xlink:href="https://www.microns-explorer.org/cortical-mm3">https://www.microns-explorer.org/cortical-mm3</ext-link>. The infrastructure present in ASAP can be however extended to ‘fine’ alignments because ASAP is ready to implement 3D transformation both at the level of sections and at the level of individual image tiles. The quality of the fine alignment will depend on the transform that the user chooses to implement, ASAP is just a framework/vehicle for that transform.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Global nonlinear 3D aligned volume of the mouse dataset 1.</title><p>(<bold>a</bold>) View of the global nonlinear 3D aligned volume from the <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula> plane. The figure shows the view of the global nonlinear 3D alignment of the sections with the volume sliced at position marked by the red lines in (<bold>e</bold>). (<bold>b</bold>) View of the global nonlinear 3D aligned volume from the <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula> plane. Figure shows the view of the volume sliced at position marked by the red lines in (<bold>e</bold>). (<bold>c</bold>) Zoomed-in area from (<bold>a</bold>) showing the quality of global nonlinear 3D alignment in the <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula> plane. (<bold>d</bold>) Zoomed-in area from (<bold>b</bold>) showing the quality of global nonlinear 3D alignment in the <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula> plane. (<bold>e</bold>) Maximum pixel intensity projection of the global nonlinear 3D aligned sections in the z-axis showing the overall alignment of sections within the volume. The red lines represent the slicing location in both <inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf37"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula> plane for the cross-sectional slices shown in (<bold>a</bold>) and (<bold>b</bold>). (<bold>f</bold>) A plot showing the distribution of median angular residuals from serial sections grouped by the dataset.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Global nonlinear 3D aligned volume of the mouse dataset 2.</title><p>(<bold>a</bold>) View of the global nonlinear 3D aligned volume from the <italic>xz</italic> plane. The figure shows the view of the global nonlinear 3D alignment of the sections with the volume sliced at position marked by the red lines in (<bold>e</bold>). (<bold>b</bold>) View of the global nonlinear 3D aligned volume from the <italic>yz</italic> plane. Figure shows the view of the volume sliced at position marked by the red lines in (<bold>e</bold>). (<bold>c</bold>) Zoomed-in area from (<bold>a</bold>) showing the quality of global nonlinear 3D alignment in the <italic>xz</italic> plane. (<bold>d</bold>) Zoomed-in area from (<bold>b</bold>) showing the quality of global nonlinear 3D alignment in the <italic>yz</italic> plane. (<bold>e</bold>) Maximum pixel intensity projection of the global nonlinear 3D aligned sections in the z-axis showing the overall alignment of sections within the volume. The red lines represent the slicing location in both <italic>xz</italic> and <italic>yz</italic> plane for the cross-sectional figures in (<bold>a</bold>) and (<bold>b</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-fig7-figsupp1-v2.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Global nonlinear 3D aligned volume of the human dataset.</title><p>(<bold>a</bold>) View of the global nonlinear 3D aligned volume from the <italic>xz</italic> plane. The figure shows the view of the global nonlinear 3D alignment of the sections with the volume sliced at position marked by the red lines in (<bold>e</bold>). (<bold>b</bold>) View of the global nonlinear 3D aligned volume from the <italic>yz</italic> plane. Figure shows the view of the volume sliced at position marked by the red lines in (<bold>e</bold>). (<bold>c</bold>) Zoomed-in area from (<bold>a</bold>) showing the quality of global nonlinear 3D alignment in the <italic>xz</italic> plane. (<bold>d</bold>) Zoomed-in area from (<bold>b</bold>) showing the quality of global nonlinear 3D alignment in the <italic>yz</italic> plane. (<bold>e</bold>) Maximum pixel intensity projection of the global nonlinear 3D aligned sections in the z-axis showing the overall alignment of sections within the volume. The red lines represent the slicing location in both <italic>xz</italic> and <italic>yz</italic> plane for the cross-sectional figures in (<bold>a</bold>) and (<bold>b</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-fig7-figsupp2-v2.tif"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 3.</label><caption><title>Global nonlinear 3D aligned volume of the mouse dataset 3.</title><p>(<bold>a</bold>) View of the global nonlinear 3D aligned volume from the <italic>xz</italic> plane. The figure shows the view of the global nonlinear 3D alignment of the sections with the volume sliced at position marked by the red lines in (<bold>e</bold>). (<bold>b</bold>) View of the global nonlinear 3D aligned volume from the <italic>yz</italic> plane. Figure shows the view of the volume sliced at position marked by the red lines in (<bold>e</bold>). (<bold>c</bold>) Zoomed-in area from (<bold>a</bold>) showing the quality of global nonlinear 3D alignment in the <italic>xz</italic> plane. (<bold>d</bold>) Zoomed-in area from (<bold>b</bold>) showing the quality of global nonlinear 3D alignment in the <italic>yz</italic> plane. (<bold>e</bold>) Maximum pixel intensity projection of the global nonlinear 3D aligned sections in the z-axis showing the overall alignment of sections within the volume. The red lines represent the slicing location in both <italic>xz</italic> and <italic>yz</italic> plane for the cross-sectional figures in (<bold>a</bold>) and (<bold>b</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-fig7-figsupp3-v2.tif"/></fig></fig-group><p><xref ref-type="table" rid="table3">Table 3</xref> provides a comparison of both dataset acquisition times and their volume assembly. The acquisition times represent the serial sections imaged using five different ssTEMs running in parallel. Each of the dataset processing times is under the same infrastructure settings (see ‘BlueSky workflow engine for automated processing’ for details on hardware setting), but with several optimizations implemented in ASAP with every dataset. The ASAP processing times also include the manual QC processing time duration. For each dataset, the manual QC processing time is roughly a few minutes per serial section, but has not been quantified for an accurate estimation that can be reported here. All of our datasets were processed in a time frame that matches or exceeds the acquisition time, thus achieving high-throughput volume assembly.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Processing time comparison between acquisition system and Assembly Stitching and Alignment Pipeline (ASAP).</title><p>The acquisition times shown are based on serial sections imaged using five different serial section transmission electron microscopies (ssTEMs) running in parallel. The stitching time for all the datasets includes the time it took to stitch all the serial sections including semi-automated quality control (QC) and reprocessing sections that failed QC on the first run and the global 3D alignment. The stitching was done in a noncontinuous fashion that included correctly uploading/reuploading corrupted, duplicate sections, etc. Each section was stitched using a single node from the compute cluster. The different processing times of the different datasets reflect the optimization of the pipeline over time, while still keeping a throughput in pace with imaging acquisition.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Dataset</th><th align="left" valign="top">No. of sections</th><th align="left" valign="top">Acquisition time (months)</th><th align="left" valign="top">ASAP processing time (includes manual processing times)</th></tr></thead><tbody><tr><td align="left" valign="top">Mouse dataset 1</td><td align="char" char="." valign="top">26,500</td><td align="char" char="." valign="top">6</td><td align="char" char="." valign="top">4 months</td></tr><tr><td align="left" valign="top">Mouse dataset 2</td><td align="char" char="." valign="top">17,584</td><td align="char" char="." valign="top">3</td><td align="char" char="." valign="top">6 weeks</td></tr><tr><td align="left" valign="top">Human dataset</td><td align="char" char="." valign="top">9,661</td><td align="char" char="." valign="top">2</td><td align="char" char="." valign="top">4 weeks</td></tr><tr><td align="left" valign="top">Mouse dataset 3</td><td align="char" char="." valign="top">17,309</td><td align="char" char="." valign="top">3</td><td align="char" char="." valign="top">10 days</td></tr></tbody></table></table-wrap></sec><sec id="s2-7"><title>Application to other imaging pipelines: Array tomography</title><p>The software infrastructure described in this article can also be applied to fluorescence and multimodal datasets such as array tomography (<xref ref-type="fig" rid="fig8">Figure 8</xref>). Array tomography presents some unique challenges for image processing because imaging can be performed in both light and electron microscopy. In addition, multiple channels can be imaged simultaneously and multiple rounds of imaging can be performed on the same physical sections with light microscopy (<xref ref-type="bibr" rid="bib8">Collman et al., 2015</xref>). To properly integrate all these images, in addition to the image processing steps of 2D stitching and alignment that apply to EM, the multiple rounds of light microscopy of the same section must be registered to one another, and the higher resolution EM data must be co-registered with the light microscopy data. Finally, alignments based on one set of images must be applied to the other rounds and/or modalities of data. The Render services allow for image processing steps to define new transformations on the image tiles without making copies of the data, including transformations that dramatically alter the scale of the images, such as when registering between EM and light microscopy data. The Render and point-match services provide a flexible framework for corresponding positions between tiles to be annotated, allowing those correspondences to be used as constraints in calculating the appropriate transformations at each step of the pipeline. The result is a highly multimodal representation of the dataset that can be dynamically visualized in multiple channels and resolutions at each step of the process through the integration of Render services with the Neuroglancer visualization tool (<xref ref-type="fig" rid="fig8">Figure 8</xref>).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Stitching of multichannel conjugate array tomography data.</title><p>(<bold>a, </bold>top) Experimental steps in conjugate array tomography: Serial sections are collected onto glass coverslips and exposed to multiple rounds of immunofluorescent (IF) staining, imaging, and elution, followed by post-staining and imaging under a field emission scanning electron microscopy (FESEM). (<bold>a</bold>, bottom) Schematic illustrating the substeps of image processing large-scale conjugate array tomography data. 2D stitching must be performed on each round of IF imaging and EM imaging. Multiple rounds of IF imaging of the same physical section must be registered together to form a highly multiplexed IF image of that section. The higher resolution by typically smaller spatial scale FESEM data must then be registered to the lower resolution but larger spatial scale IF data for each individual 2D section and FESEM montage. Finally, alignments of the data across sections must be calculated from the IF, or alternatively EM datasets. In all cases, the transformations of each of these substeps must be composed to form a final coherent multimodal, multiresolution representation of the dataset. (<bold>b–d</bold>) Screenshots of a processed dataset, rendered dynamically in Neuroglancer through the Render web services. (<bold>b</bold>) An overview image of a single section of conjugate array tomography data that shows the result of stitching and registering multiple rounds of IF an EM data. Channels shown are GABA (blue), TdTomato (Red), Synapsin1a (green), PSD95 (yellow), and MBP (purple). Small white box highlights the region shown in (<bold>c</bold>). (<bold>c</bold>) A zoom-in of one area of the section where FESEM data was acquired, small white box shows the detailed region shown in (<bold>d</bold>). (<bold>d</bold>) A high-resolution view of an area of FESEM data with IF data overlaid on top. One can observe the tight correspondence between the locations of IF signals and corresponding ultrastructural correlates, such a myelinated axons on MBP, and postsynaptic densities and PSD95.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-fig8-v2.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The volume assembly pipeline ASAP was designed to produce high throughput if it is scalable, flexible, modular, upgradeable, and easily deployable in a variety of environments, including large-scale distributed systems. The pipeline leverages Render service’s capability of processing by means of metadata operations and persisting data in databases. This largely facilitates multiple iterations of processing the data until a desired aligned volume is achieved. The need for rendering intermediate output is also eliminated at each iteration since the output can be dynamically rendered by applying the metadata associated with the images. This potentially saves computational time and resources in addition to increasing the throughput. Demonstrating its scalability, ASAP has been used to process several large-scale datasets, including a millimeter cube of mouse cortex that is already public at <ext-link ext-link-type="uri" xlink:href="https://www.microns-explorer.org">https://www.microns-explorer.org</ext-link>. Though ASAP is compatible with several strategies for fine alignment (<xref ref-type="fig" rid="fig2">Figure 2</xref>), the one used for all the datasets in this article was SEAMLeSS, which is described in <xref ref-type="bibr" rid="bib18">Macrina et al., 2021</xref>.</p><p>The volume assembly pipeline maximizes the speed and quality of stitching and alignment on large-scale datasets. One of the main improvements is the addition of a parameter optimization module that generates optimized sets of parameters for 2D stitching. This parameter optimization was introduced for montaging in mouse dataset 2, mouse dataset 3, and the human dataset. The use of optimization parameters resulted in less distorted montages with residuals within acceptable threshold values. It also compensated for some deviation in lens distortion correction accuracy, while reducing the number of iterations of processing.</p><sec id="s3-1"><title>Quality assessment</title><p>In a software pipeline that processes tens of millions of images, it is essential to have automated metrics of quality control. The statistical metrics such as MAD of the image scales to auto-detect deformed montages combined with detecting other stitching issues by the QC module facilitates faster processing while ensuring that the stitched sections meet the QC criteria. Also, early detection of poor point correspondences by the QC module drastically reduces the need for reprocessing montages through several iterations. About 2% of sections undergo this re-computation of point correspondences at a higher scale. Speed-up is also achieved by automating data transfer and ingestion into our volume assembly workflow from imaging. This is achieved by means of automatically querying the imaging database for sections that have been imaged and have passed imaging QC (W et al. 2020). The metadata of the QC passed sections are automatically ingested into the volume assembly workflow, which also triggers the stitching process. The imaging database was not developed during imaging of mouse dataset 1, hence the status of imaging and QC for each section was maintained in a spreadsheet and manually updated.</p><p>ASAP is capable of handling reimaged serial sections without overwriting the metadata for its earlier versions during processing. Also, the system is capable of handling missing sections (in case of serial section loss during sectioning or aperture burst/damage during imaging) and partial sections (sections that are cut partially from the volume). The missing sections are treated as ‘gaps’ in the volume and have minimal impact on the quality of alignment. Currently, the pipeline has successfully handled a gap of three consecutive sections (and five consecutive sections for the human dataset) in the volume. Feature-based computation of point correspondences is effective in finding features across sections with gaps between them and also robust to contrast and scale variations between image pairs.</p><p>The software stack includes capabilities to interface with different solvers through BigFeta including a scipy.sparse-based solver and the interfaces provided by PETSc (<xref ref-type="bibr" rid="bib3">Balay et al., 2019</xref>, <xref ref-type="bibr" rid="bib4">Balay et al., 2021</xref>, <xref ref-type="bibr" rid="bib2">Balay et al., 1997</xref>). This has allowed us to nonlinearly globally 3D align an entire volume on a single workstation as well as on a distributed system. Our code base was also improved to allow for reprocessing individual sections that are reimaged and inserting them in existing global nonlinear 3D aligned volume. In addition to file storage, our software tools now support object stores using an S3 Application Program Interface (API) such as Ceph, Cloudian, and AWS, enabling real-time processing of large-scale datasets in the cloud as well as on-premises. The entire software stack is designed and developed using open-source dependencies and licensed under the permissive Allen Institute Software License. Also, our software stack and its modules are containerized allowing rapid deployment and portability. It also includes integration tests for each module for seamless development and code coverage. Automated processing of EM datasets can be accomplished with a custom workflow based on an open-source workflow manager (BlueSky) that is well suited to incorporate complex workflows with readable, flexibility workflow diagrams allowing rapid development.</p></sec><sec id="s3-2"><title>Image processing at the speed of imaging</title><p>The reconstruction of neural circuits requires high spatial resolution images provided by EM and drastic advances made in the field of EM connectomics (<xref ref-type="bibr" rid="bib21">MICrONS Consortium et al., 2021</xref>; <xref ref-type="bibr" rid="bib34">Wetzel et al., 2016</xref>; <xref ref-type="bibr" rid="bib21">MICrONS Consortium et al., 2021</xref>; <xref ref-type="bibr" rid="bib29">Shapson-Coe et al., 2021</xref>) that make it suitable for imaging large-scale EM volumes and producing dense reconstructions. ASAP aligns well with such large-scale EM volume production systems facilitating seamless processing of data through automated data ingestion, 2D stitching, 3D alignment, and QC – all chained together as a continuous process. Developing a pipeline that can produce 2D stitching at a rate better than imaging was the most challenging problem. In addition, we invested heavily to develop a set of software tools that is modular, easily adaptable and upgradeable to new algorithms, computing systems and other domains, and able to run in a production-level setting.</p><p>The offline processing duration of all our datasets using ASAP has been shown to exceed the speed of imaging. ASAP is capable of processing the datasets in parallel with the imaging session with sufficient computational resources. The mouse dataset 1 was processed in parallel with imaging (stitching of serial sections) followed by a chunk-based global 3D alignment (first iteration). Efficient data transfer from the multiscope infrastructure, coupled with automated processing capabilities of ASAP, assisted in the processing of the mouse dataset 1 in parallel to imaging and at speeds that match the imaging. The <italic>em stitch</italic> software package leverages the GPU-based computations on the scope for imaging QC to stitch the montages on-scope. This accelerates the stitching process and the rapid feedback loop between imaging and volume assembly. Though our current processing rate already outperforms image acquisition, the next step is to perform the image processing in real time, ideally close to the microscopes and as images are collected. Such strategy has been proposed by Jeff Lichtman and colleagues (<xref ref-type="bibr" rid="bib16">Lichtman et al., 2014</xref>), and there are many aspects of the work presented here that will facilitate transition to on-scope real-time stitching and alignment.</p></sec><sec id="s3-3"><title>Scaling to larger volumes and across image modalities</title><p>Our pipeline was developed with a focus on standardization and was built entirely with open-source libraries as an open-source package. Our intention is for others to use and potentially scale it beyond the work described in this article. As we demonstrate in <xref ref-type="fig" rid="fig8">Figure 8</xref>, the use of ASAP goes well beyond electron microscopy, and it is being used in fluorescent data as well. The modularity of ASAP can be leveraged to include GPU-based algorithms at various stages of the pipeline, thus paving the way for further increase in throughput. Processing in parallel with imaging, we were able to stitch and globally nonlinearly 3D align 2 PB of EM images from the 1 mm<sup>3</sup> mouse visual cortex at synapse resolution within a period of ~4 months, and other petascale datasets with a montaging rate exceeding the imaging rate. With improvements made to the pipeline, stitching and global nonlinear 3D alignment of a dataset similar in size took just 10 days of processing time for mouse dataset 3. This throughput makes the volume assembly pipeline suitable for processing exascale datasets that spans larger cortical areas of the brain across species. Although the pipeline was designed for EM connectomics, it can be easily adapted to process datasets from any other domain of image sets that share the basic underlying assumptions in imaging.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Imaging with electron microscopy</title><p>Three of the samples processed by the infrastructure described in this article originated from mice. All procedures were carried out in accordance with the Institutional Animal Care and Use Committee approval at the Allen Institute for Brain Science with protocol numbers 1503, 1801, and 1808. All mice were housed in individually ventilated cages, 20–26°C, 30–70% relative humidity, with a 12 hr light/dark cycle. Mouse genotypes used were as follows: mouse 1, Slc-Cre/GCaMP6s (JAX stock 023527 and 031562); mouse 2, Slc17a7-IRES2-Cre/CamK2a-tTA/Ai94 (JAX stock 023527, 024115); mouse 3, Dlx5-CreER/Slc-Cre/GCaMP6s (JAX stock 010705, 023527, 031562).</p><p>Preparation of samples was performed as described earlier (W et al. 2020); briefly, mice were transcardially perfused with a fixative mixture of 2.5% paraformaldehyde and 1.25% glutaraldehyde in buffer. After dissection, slices were cut with a vibratome and post-fixed for 12–48 hr. Human surgical specimen was obtained from a local hospital in collaboration with local neurosurgeon. The sample collection was approved by the Western Institutional Review Board (protocol # SNI 0405). The patient provided informed consent, and experimental procedures were approved by the hospital institute review boards before commencing the study. A block of tissue ~ 1 × 1 × 1 cm of anteromedial temporal lobe was obtained from a patient undergoing acute surgical treatment for epilepsy. This sample was excised in the process of accessing the underlying epileptic focus. Immediately after excision the sample was placed into a fixative solution of 2.5% paraformaldehyde, 1.25% glutaraldehyde, 2 mM calcium chloride, in 0.08 M sodium cacodylate buffer for 72 hr. The samples were then trimmed and sectioned with a vibratome to 1000-µm-thick slices and placed back in fixative for ~96 hr. After fixation, slices of mouse and human were extensively washed and prepared for reduced osmium treatment (rOTO) based on the protocol of <xref ref-type="bibr" rid="bib12">Hua et al., 2015</xref>. Potassium ferricyanide was used to reduce osmium tetroxide and thiocarbohydrazide (TCH) for further intensification of the staining. Uranyl acetate and lead aspartate were used to enhance contrast. After resin embedding, ultrathin sections (40 nm or 45 nm) were manually cut in a Leica UC7 ultra-microtome and an RMC Atumtome. After sectioning, the samples were loaded into the automated transmission electron microscopes (autoTEM), and we followed the TEM operation routine (described in <xref ref-type="bibr" rid="bib34">Wetzel et al., 2016</xref> and <xref ref-type="bibr" rid="bib21">MICrONS Consortium et al., 2021</xref>) to bring up the HT voltage and filament current and then align the beam. Calibration of the autoTEM involved tape and tension calibration for barcode reading, measuring beam rotation and camera pixels, and stage alignment. Then, EM imaging was started. The mouse datasets were obtained from primary visual cortex and higher visual areas, and the human dataset was obtained from the Medial Temporal Gyrus (MTG).</p></sec><sec id="s4-2"><title>Image catcher (Aloha) service</title><p>Aloha is a core component of our acquisition infrastructure designed to facilitate the transfer and preprocessing of images intended for the image processing workflow. Aloha is implemented as a scale-out Python web service using flask/gunicorn. This service is designed to accept image arrays defined by a flat-buffers protocol and atomically write them in a designated location in losslessly compressed tiff format. While the array is in memory, the service also writes progressively downsampled versions of that image (MIPmaps) to another designated location. By using the uri-handler library (<xref ref-type="bibr" rid="bib30">Torres, 2021a</xref>), Aloha can write to various cloud providers and on-premises object storage systems as well as file system-based storage. The Aloha library includes a set of client scripts that allow uploading from an existing autoTEM-defined directory as well as utilities to encode numpy arrays for the REST API. Aloha web service is configured to interact with the piTEAM’s TEMdb backend and tracks the state of transfers in a set of custom fields. In the automated workflow, a process queries these fields in order to ingest completed montage sets to the volume assembly workflow. Aloha can be easily replaced with a data transfer module of choice based on the imaging infrastructure and the volume assembly workflow allowing for modularity.</p></sec><sec id="s4-3"><title>Render services</title><p>The Render services are a core component of the infrastructure. They provide the main logic for image transformation, interpolation, and rendering. They also provide a rich API:</p><list list-type="bullet"><list-item><p>A REST API for creating and manipulating collections of tiles or image ‘boxes’ (also called canvases; canvases are regions that can span multiple and partial tiles).</p></list-item><list-item><p>A REST API for accessing image tile, section, and stack meta information; for example, the number of tiles, dimensions, ids, and camera setup.</p></list-item><list-item><p>A REST API and core logic for rendering/materializing image tiles/canvases, arbitrary regions that span a number of (or partial) tiles, or even whole sections. In that capacity, it is used to reflect the current state of any given tile collection. (This is invaluable to proofreading intermediate stitching results.) In combination with dynamic rendering (i.e., rendering that is not based on materializing image files to storage), the Render services support lightweight web pages with feedback to detect imaging and stitching issues.</p></list-item></list><p>The Render services are backed by a MongoDB document store that contains all tile/canvas data including tile transformations. Both the Render services and the MongoDB document store are supported by dedicated hardware. The Render services code base is available and documented at <ext-link ext-link-type="uri" xlink:href="https://github.com/saalfeldlab/render">https://github.com/saalfeldlab/render</ext-link>; <xref ref-type="bibr" rid="bib23">Preibisch, 2022</xref>.</p></sec><sec id="s4-4"><title>Point-match service</title><p>A time-consuming and CPU-intensive process in the volume assembly pipeline is the computation of point correspondences between image tile pairs since this is the only stage of processing where the image data is read in memory besides the process of rendering the aligned volume to disk. Persisting this data is therefore invaluable. Robust rotation and contrast invariant correspondence candidates are generated using SIFT (<xref ref-type="bibr" rid="bib17">Lowe, 2004</xref>). These candidates are then filtered by their consensus with respect to an optimal geometric transformation, in our case an affine transformation. We use a local optimization variant of RANSAC <xref ref-type="bibr" rid="bib11">Fischler and Bolles, 1981</xref> followed by robust regression <xref ref-type="bibr" rid="bib26">Saalfeld et al., 2010</xref>. Local optimization means that, instead of picking the ‘winner’ from a minimal set of candidates as in classic RANSAC, we iteratively optimize the transformation using all inlier candidates and then update the inlier set. The ‘winner’ of this procedure (the largest set of inliers) is then further trimmed by iteratively removing candidates with a residual larger than 3 standard deviations of the residual distribution with respect to the optimal transformation and then reoptimizing the transformation. We use direct least-squares fits to optimize transformations. The computed point correspondences are stored in a database and can be retrieved/modified using the point-match service. The advantage of such a database is that it is agnostic to the source of point correspondences. Therefore, it can receive input from the point-match generator, regardless of the method of point-match generation such as SURF, SIFT, phase correlation, etc.</p></sec><sec id="s4-5"><title>Render-python API</title><p>The other core component of the software stack includes <italic>render-python</italic>, a Python API client and transformation library that interacts with both asap-modules and the Render services. The render-python components interact with Render service Java clients that perform computationally expensive operations locally to avoid taxing Render services running on centralized shared hardware.</p><p><italic>Render-python</italic> is a python-based API client and transformation library that replicates the data models in the Render services. While Render services utilize the mpicbg Fiji library to implement transformations, render-python reproduces these using using numpy to enable analysis in a Python ecosystem. Render-python is continuously integration tested against Render for compatibility and provides dynamic access to the database and client scripts provided by Render. The source code for <italic>render-python</italic> is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/AllenInstitute/render-python">https://github.com/AllenInstitute/render-python</ext-link> (<xref ref-type="bibr" rid="bib10">Collman et al., 2022</xref>).</p><p>Besides render-python, ASAP interfaces with other tools for solving transformations and visualizations. A description of these tools is as follows:</p><list list-type="simple"><list-item><p>BigFeta: The <italic>BigFeta</italic> package implements a Python-based sparse solver implementation of alignment problems based on the formulation in EMAligner (<xref ref-type="bibr" rid="bib15">Khairy et al., 2018</xref>). In addition to optimizations and new transform functionality, BigFeta extends the previous approach to use PETSc (<ext-link ext-link-type="uri" xlink:href="https://petsc.org/release/">petsc.org</ext-link>) for scalable computation and allows input and output using render-python objects, as well as JSON file, MongoDB document store, and Render services interaction.</p></list-item><list-item><p>em stitch: <italic>em stitch</italic> includes tools based on BigFeta and render-python for use as a standalone montage processing package without connecting to a database or REST API, ideal for online processing utilizing the same hardware running a microscope. Importantly, <italic>em stitch</italic> includes a module to derive a mesh-based lens distortion correction from highly overlapping calibration acquisitions and pre-generated point correspondences.</p></list-item><list-item><p>vizrelay: <italic>vizrelay</italic> is a configurable microservice designed to build links from a Render services instance to a Neuroglancer-based service and issue redirects to facilitate visualization. This provides a useful mechanism for setting Neuroglancer defaults, such as the extent of the volume or color channel options when reviewing alignments.</p></list-item></list></sec><sec id="s4-6"><title>ASAP modules</title><p>The ASAP volume assembly pipeline includes a series of modules developed using Python and the render-python library that implement workflow tasks with standardized input and output formatting. The source code for ASAP modules is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/AllenInstitute/asap-modules">https://github.com/AllenInstitute/asap-modules</ext-link>; <xref ref-type="bibr" rid="bib19">Mahalingam, 2022</xref>.</p><p>The submodules in ASAP include scripts to execute a series of tasks at each stage of the volume assembly pipeline. Some of the workflow tasks included in ASAP are as follows:</p><list list-type="bullet"><list-item><p><italic>asap.dataimport</italic>: Import image (tile) metadata to the Render services from custom microscope files, generate MIPmaps and update the metadata, render downsampled version of the montaged serial section.</p></list-item><list-item><p><italic>asap.mesh_lens_correction</italic>: Include scripts to compute the lens distortion correction transformation.</p></list-item><list-item><p><italic>asap.pointmatch</italic>: Generate tile pairs (see <xref ref-type="fig" rid="fig2">Figure 2d</xref>) and point correspondences for stitching and alignment.</p></list-item><list-item><p><italic>asap.point_match_optimization</italic>: Find the best possible set of parameters for a given set of tile pairs.</p></list-item><list-item><p><italic>asap.solver</italic>: Interface with BigFeta solver for stitching the serial sections.</p></list-item><list-item><p><italic>asap.em_montage_qc</italic>: Generate QC statistics on the stitched sections as explained in ‘Automated petascale stitching.’</p></list-item><list-item><p><italic>asap.rough_align</italic>: Compute per-section transformation for 3D alignment and scale them to match their original layered montage collection and generate new metadata describing the alignment at full resolution.</p></list-item><list-item><p><italic>asap.register</italic>: Register an individual section with another section in a chunk. This module is typically used to align reimaged sections to an already aligned volume.</p></list-item><list-item><p><italic>asap.materialize</italic>: Materialize final volume as well as downsampled versions of sections in a variety of supported formats.</p></list-item></list><p>ASAP modules are schematized for ease of use with <italic>argschema</italic>, an extension of the marshmallow Python package that allows marshaling of command-line arguments and input files. ASAP modules interact with other tools that comprise the peta-scale stitching and alignment software tools eco-system (see <xref ref-type="fig" rid="fig9">Figure 9</xref>).</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Set of software tools developed to perform petascale real-time stitching.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-fig9-v2.tif"/></fig></sec><sec id="s4-7"><title>Montage parameter optimization</title><p>In two dimensions (<italic>x, y</italic>), BigFeta implements the optimization described by <xref ref-type="bibr" rid="bib15">Khairy et al., 2018</xref> as the following regularized least-squares problem:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the unknowns for which to solve – these are interpreted to define the parameters of tile transformations, <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is an <italic>m × n</italic> matrix from <italic>m</italic> point correspondences derived from 2<italic>n</italic> total unknowns, <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is an <italic>mxm</italic> diagonal matrix weighting the point correspondences, λ is an <italic>n × n</italic> diagonal matrix containing regularization factors for the unknowns, <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the initialization for the unknowns against which the regularization penalizes, and <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a right-hand-side term to the unknowns introduced to generalize the method to additional transformation models.</p><p>Bigfeta allows the regularization parameter λ to differently constrain distinct terms of a given transformation such as the translation, affine, and polynomial factors on an individual tile basis.</p><p>Montage quality in ASAP is evaluated by metrics of residuals and rigidity of the output montage (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig5">Figure 5</xref>). For tile deformations that are well-defined by an affine transformation, these metrics are most impacted by the translation and affine regularization parameters (λ) used in the BigFeta solver step (<xref ref-type="disp-formula" rid="equ2">equation 2</xref>). As the optimal configuration of these values can be impacted by the accuracy of the initialization as well as the weight and distribution of point correspondences, it is sometimes necessary to re-evaluate the regularization parameters for different imaging, tissue, or preprocessing conditions. We provide an ‘optimization’ module, asap.solver.montage_optimization, which leverages the fast solving capabilities of BigFeta to sweep across a range of regularization parameters and provide an acceptable set of parameters given targets for correspondence residual in pixels and tile scale MAD value.</p><p>In each dataset where montage optimization was used, we found that a MAD cutoff of 0.005 in both <inline-formula><mml:math id="inf43"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf44"><mml:mi>y</mml:mi></mml:math></inline-formula> was usually sufficient to provide a group of acceptable montages. However, in some cases individual montages or sets of montages must be run with relaxed criteria – for these cases, candidate montages were usually found by increasing the MAD statistic to 0.006 and 0.007 in <italic>x</italic> and <italic>y</italic>, respectively. In our experience, these sets of montages seem to share a lens distortion model, and we assume that this inadequate model requires additional deformation at the tile and montage level. To help with these cases, our implementation of montage optimization has the option to iteratively relax these constraints to a predetermined boundary such that inadequate cutoffs can be increased until there are a desired number of candidate montages.</p></sec><sec id="s4-8"><title>3D realignment</title><p>A common use case after 3D global alignment involves realigning a subset of the dataset while maintaining the global alignment reached for the rest of the volume. The asap.solver.realign_zs module implements this operation by increasing the λ parameters in <xref ref-type="disp-formula" rid="equ1">equation 1</xref> for neighboring sections while allowing custom inputs for the sections that need to be realigned. As such, it is possible to integrate re-montaged sections, re-computed point correspondences, or more deformable transformations into an existing 3D alignment without requiring changes on the global scale. For all the datasets presented in this article, after global alignment the data was then transferred for fine alignment using SEAMLeSS (<xref ref-type="bibr" rid="bib18">Macrina et al., 2021</xref>). The fine alignment was performed by the team of Sebastian Seung in Princeton or ZettaAI.</p></sec><sec id="s4-9"><title>Chunk fusion</title><p>The asap.fusion package provides modules to support chunk-based 3D alignment workflows. The 3D aligned chunks can then be fused together. asap.fusion.register_adjacent_stack provides utilities to register overlapping 3D aligned chunks using translation, rigid, similarity, or affine transformations. Then, given a JSON-defined tree describing the layout and relative transformation between chunks, asap.fusion.fuse_stacks will assemble metadata representing a combined volume using Render’s ‘InterpolatedTransform’ to interpolate between independently optimized transformations in the overlap region of two chunks.</p></sec><sec id="s4-10"><title>Materialization</title><p>Alignment through BigFeta produces tile metadata that can be interpreted by Render and its clients to produce ‘materialized’ representations of the stack. These representations are rendered client-side, having transformations applied and flattening overlapping tiles. The output of materialization can be in a number of file formats including n5 and the CATMAID large data tilesource (<xref ref-type="bibr" rid="bib28">Schneider-Mizell et al., 2016</xref>; <xref ref-type="bibr" rid="bib25">Saalfeld et al., 2009</xref>), which are included in Render. It is also possible to implement custom output formats based on available Python libraries using render-python to access Render’s client scripts. As SEAMLeSS expects data in the cloud-volume-compatible Neuroglancer precomputed format Neuroglancer, the datasets in <xref ref-type="table" rid="table1">Table 1</xref> were materialized with a Python script that uses render-python to write to Neuroglancer precomputed format using the cloud-volume library.</p></sec><sec id="s4-11"><title>BlueSky workflow engine for automated processing</title><p>The automated workflow engine called BlueSky was developed in Django backed by a PostgreSQL database with stable backups, graceful restarts, and easy migrations. It provides a web-based user interface for the user to visualize, run, and edit running jobs at various stages in the workflow. BlueSky uses Celery and RabbitMQ to run workflow tasks in diverse computing environments, from local execution on a workstation to remote execution using a compute cluster (PBS, MOAB, SLURM). BlueSky is flexible in terms of designing complex workflows as the workflow diagrams (see <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>, <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref>) can be specified in readable formats such as YAML, JSON, or Django allowing rapid development. BlueSky can be used for many different purposes, but for the image processing task related to this article the workflow includes the following steps: (1) ingest montage sets, (2) generate MIPmaps, (3) apply MIPmaps, (4) wait for the assigned lens correction transform, (5) apply the lens correction transform, (6) extract tile pairs for determining point correspondences, (7) generate 2D montage point correspondences, (8) run the 2D montage solver, (9) automatically check for defects, (10) place potential defects in a manual QC queue, and (11) generate downsampled montage. BlueSky is publicly available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/AllenInstitute/blue_sky_workflow_engine">https://github.com/AllenInstitute/blue_sky_workflow_engine</ext-link>). The volume assembly workflow is designed to use BlueSky workflow engine for processing our datasets. The custom EM volume assembly workflow (<ext-link ext-link-type="uri" xlink:href="https://github.com/AllenInstitute/em_imaging_workflow">https://github.com/AllenInstitute/em_imaging_workflow</ext-link>; <xref ref-type="bibr" rid="bib31">Torres et al., 2021b</xref>) facilitates continuous processing of the datasets at speeds that match or exceed data acquisition rates (see <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1</xref>, <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>).</p><p>For all our datasets, BlueSky utilized three different kinds of hardware nodes in our HPC cluster. The configurations are as follows:</p><list list-type="bullet"><list-item><p>2× Intel Xeon 2620 processor with 256 GB RAM</p></list-item><list-item><p>2× Intel Xeon 2630 processor with 256 GB RAM</p></list-item><list-item><p>2× Intel Xeon Gold 6238 processor with 512 GB RAM</p></list-item></list><p>All our datasets were processed using a maximum of 50 nodes to perform 2D stitching of sections in parallel. A combination of the nodes with the above configuration was used in processing.</p></sec><sec id="s4-12"><title>Array tomography alignment</title><p>To register the light and electron array tomography data into a registered conjugate image stack, we developed a set of modules that used a manual process for image registration. Briefly, one module (make_EM_LM_registration_projects_multi) created a TrakEm2 project in which the EM data is in one z-layer, and each light microscopy channel is in a different z-layer. From there, we created a blended view of DAPI and MBP stain to create an image with recognizable features between the two datasets. Users manually identified sets of correspondences between the images, including the centers of myelinated processes, the centers of mitochondria visible often in the autofluorescence of the DAPI channel, and spatially distinct regions of heterochromatin that appears bright in the DAPI channel and dark in the EM. Between 12 and 20 corresponding points were identified in each section, and TrackEm2 was used to fit as similarity transform to bring the images into register. A second module (import_LM_subset_from_EM_registration_multi) then exported the transformations saved in this Trakem2 project back into the render framework. We implemented this custom workflow outside of the main automated EM image processing pipeline so it is available in a separate repository (<ext-link ext-link-type="uri" xlink:href="https://www.github.com/AllenInstitute/render-python-apps">https://www.github.com/AllenInstitute/render-python-apps</ext-link>; <xref ref-type="bibr" rid="bib9">Collman, 2018</xref>) within the submodule ‘renderapps/cross_modal_registration’.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>has a competing interest in Yikes LLC</p></fn><fn fn-type="COI-statement" id="conf3"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Optimized the pipeline software (asap-modules) and executed the stitched and aligning of the datasets. Developed the software packages (asap-modules, render-python) and maintained the infrastructure and continuous integration testing used by the Render backed pipeline</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Optimized the pipeline software (asap-modules) and executed the stitched and aligning of the datasets. Primary developer of Aloha . Developed the software packages (asap-modules, render-python) and maintained the infrastructure and continuous integration testing used by the Render backed pipeline</p></fn><fn fn-type="con" id="con3"><p>Software, Methodology, Writing – review and editing, Developed EM aligner</p></fn><fn fn-type="con" id="con4"><p>Software, Methodology, Writing – review and editing, Primary developers of the Render services</p></fn><fn fn-type="con" id="con5"><p>Software, Methodology, Writing – review and editing, Developed BlueSky workflow manager and the volume assembly workflow</p></fn><fn fn-type="con" id="con6"><p>Software, Methodology, Writing – review and editing, Developed the software packages (asap-modules, render-python) and maintained the infrastructure and continuous integration testing used by the Render backed pipeline. Generated Array tomography data</p></fn><fn fn-type="con" id="con7"><p>Software, Methodology, Writing – review and editing, Developed multi-channel support for asap-modules and vizrelay. Developed the software packages (asap-modules, render-python) and maintained the infrastructure and continuous integration testing used by the Render backed pipeline</p></fn><fn fn-type="con" id="con8"><p>Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Software, Writing – review and editing, Developed Aloha</p></fn><fn fn-type="con" id="con10"><p>Methodology, Writing – review and editing, Contributed to the generation of mouse EM data. Contributed to the generation of human EM data</p></fn><fn fn-type="con" id="con11"><p>Methodology, Writing – review and editing, Contributed to the generation of mouse EM data. Contributed to the generation of human EM data</p></fn><fn fn-type="con" id="con12"><p>Methodology, Writing – review and editing, Contributed to the generation of mouse EM data. Contributed to the generation of human EM data</p></fn><fn fn-type="con" id="con13"><p>Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con14"><p>Methodology, Writing – review and editing, Contributed to the generation of human EM data</p></fn><fn fn-type="con" id="con15"><p>Methodology, Writing – review and editing, Contributed to the generation of human EM data</p></fn><fn fn-type="con" id="con16"><p>Methodology, Writing – review and editing, Contributed to the generation of human EM data</p></fn><fn fn-type="con" id="con17"><p>Supervision, Funding acquisition, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con18"><p>Conceptualization, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con19"><p>Conceptualization, Software, Supervision, Writing – review and editing, Primary developer of the MATLAB version of EM aligner. Conceptualized the stitching pipeline using Render services</p></fn><fn fn-type="con" id="con20"><p>Conceptualization, Software, Supervision, Investigation, Writing – review and editing, Primary developers of the Render services</p></fn><fn fn-type="con" id="con21"><p>Software, Supervision, Writing – review and editing, Developed the software packages (asap-modules, render-python) and maintained the infrastructure and continuous integration testing used by the Render backed pipeline. Generated Array tomography data</p></fn><fn fn-type="con" id="con22"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Methodology, Writing - original draft, Project administration, Contributed to the generation of mouse EM data. Contributed to the generation of human EM data</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Human surgical specimen was obtained from local hospital in collaboration with local neurosurgeon. The sample collection was approved by the Western Institutional Review Board (Protocol # SNI 0405). Patient provided informed consent and experimental procedures were approved by hospital institute review boards before commencing the study.</p></fn><fn fn-type="other"><p>All procedures were carried out in accordance with Institutional Animal Care and Use Committee approval at the Allen Institute for Brain Science with protocol numbers 1503, 1801 and 1808.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-76534-transrepform1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript describes is a software infrastructure resource that is being made publicly available. The manuscript is not a data generation manuscript. Nevertheless, one of the datasets used is already publicly available on <ext-link ext-link-type="uri" xlink:href="https://www.microns-explorer.org/cortical-mm3#em-imagery">https://www.microns-explorer.org/cortical-mm3#em-imagery</ext-link> with available imagery and segmentation (<ext-link ext-link-type="uri" xlink:href="https://tinyurl.com/cortical-mm3">https://tinyurl.com/cortical-mm3</ext-link>). Moreover cloud-volume (<ext-link ext-link-type="uri" xlink:href="https://github.com/seung-lab/cloud-volume">https://github.com/seung-lab/cloud-volume</ext-link>) can be used to programmatically download EM imagery from either Amazon or Google with the cloud paths described below. The imagery was reconstructed in two portions, referred to internally by their nicknames 'minnie65' and 'minnie35' reflecting their relative portions of the total data. The two portions are aligned across an interruption in sectioning. minnie65: AWS Bucket: precomputed: <ext-link ext-link-type="uri" xlink:href="https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/em">https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/em</ext-link> Google Bucket: precomputed: <ext-link ext-link-type="uri" xlink:href="https://storage.googleapis.com/iarpa_microns/minnie/minnie65/emminnie35">https://storage.googleapis.com/iarpa_microns/minnie/minnie65/emminnie35</ext-link>: AWS Bucket: precomputed: <ext-link ext-link-type="uri" xlink:href="https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie35/em">https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie35/em</ext-link> Google Bucket: precomputed: <ext-link ext-link-type="uri" xlink:href="https://storage.googleapis.com/iarpa_microns/minnie/minnie35/em">https://storage.googleapis.com/iarpa_microns/minnie/minnie35/em</ext-link>. We have also made available in Dryad raw data of the remaining datasets <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.qjq2bvqhr">https://doi.org/10.5061/dryad.qjq2bvqhr</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Macarico da Costa</surname><given-names>N</given-names></name><name><surname>Mahalingam</surname><given-names>G</given-names></name><name><surname>Torres</surname><given-names>R</given-names></name><name><surname>Buchanan</surname><given-names>J</given-names></name><name><surname>Takeno</surname><given-names>M</given-names></name><name><surname>Yin</surname><given-names>W</given-names></name><name><surname>Bumbarger</surname><given-names>D</given-names></name><name><surname>Collman</surname><given-names>F</given-names></name><name><surname>Reid</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>ASAP-TEM-sample</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.qjq2bvqhr</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank our project manager Shelby Suckow for her exceptional work in keeping us aligned and on time. We thank Allan Jones and Gerry Rubin for starting the conversation that led to this collaboration and for their support and leadership. We thank Hongkui Zeng and Christof Koch for their support and leadership. We thank D Brittain, M Scott, and J Borseth for their help in collecting and imaging the EM datasets. We thank Sebastian Seung, Thomas Macrina, Nico Kemnitz, Manuel Castro, Dodam Ih, and Sergiy Popovych from Princeton University and ZettaAI for discussions and feedback on image processing strategies and improvements. We thank Brian Youngstrom, Stuart Kendrick, and the Allen Institute IT team for support with infrastructure, data management, and data transfer. We thank Jay Borseth, DeepZoom LLC, for his contributions to <italic>em stitch</italic>. We thank Andreas Tolias, Jacob Reimer, and their teams at the Baylor College of Medicine for providing mice used for electron microscopy. We thank Saskia de Vries, Jerome Lecoq, Jack Waters, and their teams at the Allen Institute for providing mice used for electron microscopy. This work was supported by the Intelligence Advanced Research Projects Activity (IARPA) of the Department of Interior/Interior Business Center (DoI/IBC) through contract number D16PC00004 and by the Allen Institute for Brain Science. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies or endorsements, either expressed or implied, of the funding sources including IARPA, DoI/IBC, or the US government. We wish to thank the Allen Institute founder, Paul G Allen, for his vision, encouragement, and support.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arellano</surname><given-names>JI</given-names></name><name><surname>Benavides-Piccione</surname><given-names>R</given-names></name><name><surname>Defelipe</surname><given-names>J</given-names></name><name><surname>Yuste</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Ultrastructure of dendritic spines: correlation between synaptic and spine morphologies</article-title><source>Frontiers in Neuroscience</source><volume>1</volume><fpage>131</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.3389/neuro.01.1.1.010.2007</pub-id><pub-id pub-id-type="pmid">18982124</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Balay</surname><given-names>S</given-names></name><name><surname>Gropp</surname><given-names>WD</given-names></name><name><surname>McInnes</surname><given-names>LC</given-names></name><name><surname>Smith</surname><given-names>BF</given-names></name></person-group><year iso-8601-date="1997">1997</year><source>Modern Software Tools in Scientific Computing</source><publisher-loc>New York, USA</publisher-loc><publisher-name>springer</publisher-name></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Balay</surname><given-names>S</given-names></name><name><surname>Abhyankar</surname><given-names>S</given-names></name><name><surname>Adams</surname><given-names>MF</given-names></name><name><surname>Brown</surname><given-names>J</given-names></name><name><surname>Brune</surname><given-names>P</given-names></name><name><surname>Buschelman</surname><given-names>K</given-names></name><name><surname>Dalcin</surname><given-names>L</given-names></name><name><surname>Dener</surname><given-names>A</given-names></name><name><surname>Eijkhout</surname><given-names>V</given-names></name><name><surname>Gropp</surname><given-names>WD</given-names></name><name><surname>Karpeyev</surname><given-names>D</given-names></name><name><surname>Kaushik</surname><given-names>D</given-names></name><name><surname>Knepley</surname><given-names>MG</given-names></name><name><surname>May</surname><given-names>DA</given-names></name><name><surname>McInnes</surname><given-names>LC</given-names></name><name><surname>Mills</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>PETSc Users Manual</source><publisher-loc>Lemont, USA</publisher-loc><publisher-name>Argonne National Laboratory</publisher-name></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Balay</surname><given-names>S</given-names></name><name><surname>Abhyankar</surname><given-names>S</given-names></name><name><surname>Adams</surname><given-names>MF</given-names></name><name><surname>Brown</surname><given-names>J</given-names></name><name><surname>Brune</surname><given-names>P</given-names></name><name><surname>Buschelman</surname><given-names>K</given-names></name><name><surname>Dalcin</surname><given-names>L</given-names></name><name><surname>Dener</surname><given-names>A</given-names></name><name><surname>Eijkhout</surname><given-names>V</given-names></name><name><surname>Gropp</surname><given-names>WD</given-names></name><name><surname>Karpeyev</surname><given-names>D</given-names></name><name><surname>Kaushik</surname><given-names>D</given-names></name><name><surname>Knepley</surname><given-names>MG</given-names></name><name><surname>May</surname><given-names>DA</given-names></name><name><surname>McInnes</surname><given-names>LC</given-names></name><name><surname>Mills</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2021">2021</year><source>Petsc/Tao User Manua</source><publisher-loc>Lemont, USA</publisher-loc><publisher-name>Argonne National Laboratory</publisher-name></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bock</surname><given-names>DD</given-names></name><name><surname>Lee</surname><given-names>W-CA</given-names></name><name><surname>Kerlin</surname><given-names>AM</given-names></name><name><surname>Andermann</surname><given-names>ML</given-names></name><name><surname>Hood</surname><given-names>G</given-names></name><name><surname>Wetzel</surname><given-names>AW</given-names></name><name><surname>Yurgenson</surname><given-names>S</given-names></name><name><surname>Soucy</surname><given-names>ER</given-names></name><name><surname>Kim</surname><given-names>HS</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Network anatomy and in vivo physiology of visual cortical neurons</article-title><source>Nature</source><volume>471</volume><fpage>177</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1038/nature09802</pub-id><pub-id pub-id-type="pmid">21390124</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cardona</surname><given-names>A</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>Schindelin</surname><given-names>J</given-names></name><name><surname>Arganda-Carreras</surname><given-names>I</given-names></name><name><surname>Preibisch</surname><given-names>S</given-names></name><name><surname>Longair</surname><given-names>M</given-names></name><name><surname>Tomancak</surname><given-names>P</given-names></name><name><surname>Hartenstein</surname><given-names>V</given-names></name><name><surname>Douglas</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>TrakEM2 software for neural circuit reconstruction</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e038011</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0038011</pub-id><pub-id pub-id-type="pmid">22723842</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>W</given-names></name><name><surname>Rouleau</surname><given-names>N</given-names></name><name><surname>Bonzanni</surname><given-names>M</given-names></name><name><surname>Kapner</surname><given-names>K</given-names></name><name><surname>Jeremiah</surname><given-names>A</given-names></name><name><surname>Du</surname><given-names>C</given-names></name><name><surname>Pothos</surname><given-names>EN</given-names></name><name><surname>Kaplan</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Functional effects of a neuromelanin analogue on dopaminergic neurons in 3d cell culture</article-title><source>ACS Biomaterials Science &amp; Engineering</source><volume>5</volume><fpage>308</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1021/acsbiomaterials.8b00976</pub-id><pub-id pub-id-type="pmid">33405867</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collman</surname><given-names>F</given-names></name><name><surname>Buchanan</surname><given-names>J</given-names></name><name><surname>Phend</surname><given-names>KD</given-names></name><name><surname>Micheva</surname><given-names>KD</given-names></name><name><surname>Weinberg</surname><given-names>RJ</given-names></name><name><surname>Smith</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping synapses by conjugate light-electron array tomography</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>5792</fpage><lpage>5807</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4274-14.2015</pub-id><pub-id pub-id-type="pmid">25855189</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Collman</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Render-python-apps</data-title><version designator="swh:1:rev:0c2c940d72558dbee273a0d5231151acb0d2aad3">swh:1:rev:0c2c940d72558dbee273a0d5231151acb0d2aad3</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://www.github.com/AllenInstitute/ren">https://www.github.com/AllenInstitute/ren</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Collman</surname><given-names>F</given-names></name><name><surname>Torres</surname><given-names>R</given-names></name><name><surname>Kapner</surname><given-names>D</given-names></name><name><surname>Trautman</surname><given-names>ET</given-names></name><name><surname>Perlman</surname><given-names>E</given-names></name><collab>sharmishtaa</collab><name><surname>Schorb</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Render-python</data-title><version designator="c4a1ba4">c4a1ba4</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/AllenInstitute/render-python">https://github.com/AllenInstitute/render-python</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischler</surname><given-names>MA</given-names></name><name><surname>Bolles</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</article-title><source>Communications of the ACM</source><volume>24</volume><fpage>381</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1145/358669.358692</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hua</surname><given-names>Y</given-names></name><name><surname>Laserstein</surname><given-names>P</given-names></name><name><surname>Helmstaedter</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Large-volume en-bloc staining for electron microscopy-based connectomics</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>7923</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms8923</pub-id><pub-id pub-id-type="pmid">26235643</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Karsh</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><data-title>Aligner for large scale serial section image data</data-title><version designator="d95675d">d95675d</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/billkarsh/Alignment_Projects">https://github.com/billkarsh/Alignment_Projects</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaynig</surname><given-names>V</given-names></name><name><surname>Fischer</surname><given-names>B</given-names></name><name><surname>Müller</surname><given-names>E</given-names></name><name><surname>Buhmann</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Fully automatic stitching and distortion correction of transmission electron microscope images</article-title><source>Journal of Structural Biology</source><volume>171</volume><fpage>163</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1016/j.jsb.2010.04.012</pub-id><pub-id pub-id-type="pmid">20450977</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Khairy</surname><given-names>K</given-names></name><name><surname>Denisov</surname><given-names>G</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Joint deformable registration of large em image volumes: a matrix solver approach</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/1804.10019.pdf">https://arxiv.org/pdf/1804.10019.pdf</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lichtman</surname><given-names>JW</given-names></name><name><surname>Pfister</surname><given-names>H</given-names></name><name><surname>Shavit</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The big data challenges of connectomics</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1448</fpage><lpage>1454</lpage><pub-id pub-id-type="doi">10.1038/nn.3837</pub-id><pub-id pub-id-type="pmid">25349911</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowe</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Distinctive image features from scale-invariant keypoints</article-title><source>International Journal of Computer Vision</source><volume>60</volume><fpage>91</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1023/B:VISI.0000029664.99615.94</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Macrina</surname><given-names>T</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Lu</surname><given-names>R</given-names></name><name><surname>Turner</surname><given-names>NL</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Popovych</surname><given-names>S</given-names></name><name><surname>Silversmith</surname><given-names>W</given-names></name><name><surname>Kemnitz</surname><given-names>N</given-names></name><name><surname>Bae</surname><given-names>JA</given-names></name><name><surname>Castro</surname><given-names>MA</given-names></name><name><surname>Dorkenwald</surname><given-names>S</given-names></name><name><surname>Halageri</surname><given-names>A</given-names></name><name><surname>Jia</surname><given-names>Z</given-names></name><name><surname>Jordan</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Mitchell</surname><given-names>E</given-names></name><name><surname>Mondal</surname><given-names>SS</given-names></name><name><surname>Mu</surname><given-names>S</given-names></name><name><surname>Nehoran</surname><given-names>B</given-names></name><name><surname>Wong</surname><given-names>W</given-names></name><name><surname>Yu</surname><given-names>S -c</given-names></name><name><surname>Bodor</surname><given-names>AL</given-names></name><name><surname>Brittain</surname><given-names>D</given-names></name><name><surname>Buchanan</surname><given-names>J</given-names></name><name><surname>Bumbarger</surname><given-names>DJ</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Collman</surname><given-names>F</given-names></name><name><surname>Elabbady</surname><given-names>L</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Kapner</surname><given-names>D</given-names></name><name><surname>Kinn</surname><given-names>S</given-names></name><name><surname>Mahalingam</surname><given-names>G</given-names></name><name><surname>Papadopoulos</surname><given-names>S</given-names></name><name><surname>Patel</surname><given-names>S</given-names></name><name><surname>Schneider-Mizell</surname><given-names>CM</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Takeno</surname><given-names>M</given-names></name><name><surname>Torres</surname><given-names>R</given-names></name><name><surname>Yin</surname><given-names>W</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Costa</surname><given-names>N d</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Petascale neural circuit reconstruction: automated methods</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.08.04.455162</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Mahalingam</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Asap-modules</data-title><version designator="swh:1:rev:c29b2126622e39d94957bcb178c97da993363f2b">swh:1:rev:c29b2126622e39d94957bcb178c97da993363f2b</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/AllenInstitute/asap-modules">https://github.com/AllenInstitute/asap-modules</ext-link></element-citation></ref><ref id="bib20"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Melchor</surname><given-names>J</given-names></name><collab>Allen Institute</collab><collab>tfliss</collab><collab>madiganz</collab></person-group><year iso-8601-date="2021">2021</year><data-title>Blue_sky_workflow_engine</data-title><version designator="98ba529">98ba529</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/AllenInstitute/blue_sky_workflow_engine">https://github.com/AllenInstitute/blue_sky_workflow_engine</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><collab>MICrONS Consortium</collab><name><surname>Bae</surname><given-names>JA</given-names></name><name><surname>Baptiste</surname><given-names>M</given-names></name><name><surname>Bodor</surname><given-names>AL</given-names></name><name><surname>Brittain</surname><given-names>D</given-names></name><name><surname>Buchanan</surname><given-names>J</given-names></name><name><surname>Bumbarger</surname><given-names>DJ</given-names></name><name><surname>Castro</surname><given-names>MA</given-names></name><name><surname>Celii</surname><given-names>B</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Collman</surname><given-names>F</given-names></name><name><surname>da Costa</surname><given-names>NM</given-names></name><name><surname>Dorkenwald</surname><given-names>S</given-names></name><name><surname>Elabbady</surname><given-names>L</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Fliss</surname><given-names>T</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Gager</surname><given-names>J</given-names></name><name><surname>Gamlin</surname><given-names>C</given-names></name><name><surname>Halageri</surname><given-names>A</given-names></name><name><surname>Hebditch</surname><given-names>J</given-names></name><name><surname>Jia</surname><given-names>Z</given-names></name><name><surname>Jordan</surname><given-names>C</given-names></name><name><surname>Kapner</surname><given-names>D</given-names></name><name><surname>Kemnitz</surname><given-names>N</given-names></name><name><surname>Kinn</surname><given-names>S</given-names></name><name><surname>Koolman</surname><given-names>S</given-names></name><name><surname>Kuehner</surname><given-names>K</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Lu</surname><given-names>R</given-names></name><name><surname>Macrina</surname><given-names>T</given-names></name><name><surname>Mahalingam</surname><given-names>G</given-names></name><name><surname>McReynolds</surname><given-names>S</given-names></name><name><surname>Miranda</surname><given-names>E</given-names></name><name><surname>Mitchell</surname><given-names>E</given-names></name><name><surname>Mondal</surname><given-names>SS</given-names></name><name><surname>Moore</surname><given-names>M</given-names></name><name><surname>Mu</surname><given-names>S</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Nehoran</surname><given-names>B</given-names></name><name><surname>Ogedengbe</surname><given-names>O</given-names></name><name><surname>Papadopoulos</surname><given-names>C</given-names></name><name><surname>Papadopoulos</surname><given-names>S</given-names></name><name><surname>Patel</surname><given-names>S</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Popovych</surname><given-names>S</given-names></name><name><surname>Ramos</surname><given-names>A</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Schneider-Mizell</surname><given-names>CM</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name><name><surname>Silverman</surname><given-names>B</given-names></name><name><surname>Silversmith</surname><given-names>W</given-names></name><name><surname>Sterling</surname><given-names>A</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Smith</surname><given-names>CL</given-names></name><name><surname>Suckow</surname><given-names>S</given-names></name><name><surname>Takeno</surname><given-names>M</given-names></name><name><surname>Tan</surname><given-names>ZH</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Torres</surname><given-names>R</given-names></name><name><surname>Turner</surname><given-names>NL</given-names></name><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Williams</surname><given-names>G</given-names></name><name><surname>Williams</surname><given-names>S</given-names></name><name><surname>Willie</surname><given-names>K</given-names></name><name><surname>Willie</surname><given-names>R</given-names></name><name><surname>Wong</surname><given-names>W</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>C</given-names></name><name><surname>Yang</surname><given-names>R</given-names></name><name><surname>Yatsenko</surname><given-names>D</given-names></name><name><surname>Ye</surname><given-names>F</given-names></name><name><surname>Yin</surname><given-names>W</given-names></name><name><surname>Yu</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Functional Connectomics Spanning Multiple Areas of Mouse Visual Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.07.28.454025</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Neuroglancer</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Neuroglancer</source><publisher-name>Neuroglancer Inc</publisher-name></element-citation></ref><ref id="bib23"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Preibisch</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Render</data-title><version designator="swh:1:rev:ebd7387eadb92647a9824bfb607499f61ac8d818">swh:1:rev:ebd7387eadb92647a9824bfb607499f61ac8d818</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/saalfeldlab/render">https://github.com/saalfeldlab/render</ext-link></element-citation></ref><ref id="bib24"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Rasband</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012</year><data-title>Imagej - image processing and analysis in java</data-title><publisher-name>SDA</publisher-name><ext-link ext-link-type="uri" xlink:href="https://ui.adsabs.harvard.edu/abs/2012ascl.soft06013R/abstract">https://ui.adsabs.harvard.edu/abs/2012ascl.soft06013R/abstract</ext-link></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>Cardona</surname><given-names>A</given-names></name><name><surname>Hartenstein</surname><given-names>V</given-names></name><name><surname>Tomancak</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>CATMAID: collaborative annotation toolkit for massive amounts of image data</article-title><source>Bioinformatics</source><volume>25</volume><fpage>1984</fpage><lpage>1986</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btp266</pub-id><pub-id pub-id-type="pmid">19376822</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>Cardona</surname><given-names>A</given-names></name><name><surname>Hartenstein</surname><given-names>V</given-names></name><name><surname>Tomančák</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>As-rigid-as-possible mosaicking and serial section registration of large ssTEM datasets</article-title><source>Bioinformatics</source><volume>26</volume><fpage>i57</fpage><lpage>i63</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btq219</pub-id><pub-id pub-id-type="pmid">20529937</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schindelin</surname><given-names>J</given-names></name><name><surname>Arganda-Carreras</surname><given-names>I</given-names></name><name><surname>Frise</surname><given-names>E</given-names></name><name><surname>Kaynig</surname><given-names>V</given-names></name><name><surname>Longair</surname><given-names>M</given-names></name><name><surname>Pietzsch</surname><given-names>T</given-names></name><name><surname>Preibisch</surname><given-names>S</given-names></name><name><surname>Rueden</surname><given-names>C</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>Schmid</surname><given-names>B</given-names></name><name><surname>Tinevez</surname><given-names>J-Y</given-names></name><name><surname>White</surname><given-names>DJ</given-names></name><name><surname>Hartenstein</surname><given-names>V</given-names></name><name><surname>Eliceiri</surname><given-names>K</given-names></name><name><surname>Tomancak</surname><given-names>P</given-names></name><name><surname>Cardona</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fiji: an open-source platform for biological-image analysis</article-title><source>Nature Methods</source><volume>9</volume><fpage>676</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id><pub-id pub-id-type="pmid">22743772</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider-Mizell</surname><given-names>CM</given-names></name><name><surname>Gerhard</surname><given-names>S</given-names></name><name><surname>Longair</surname><given-names>M</given-names></name><name><surname>Kazimiers</surname><given-names>T</given-names></name><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Zwart</surname><given-names>MF</given-names></name><name><surname>Champion</surname><given-names>A</given-names></name><name><surname>Midgley</surname><given-names>FM</given-names></name><name><surname>Fetter</surname><given-names>RD</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>Cardona</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Quantitative neuroanatomy for connectomics in <italic>Drosophila</italic></article-title><source>eLife</source><volume>5</volume><elocation-id>e12059</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.12059</pub-id><pub-id pub-id-type="pmid">26990779</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Shapson-Coe</surname><given-names>A</given-names></name><name><surname>Januszewski</surname><given-names>M</given-names></name><name><surname>Berger</surname><given-names>DR</given-names></name><name><surname>Pope</surname><given-names>A</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Blakely</surname><given-names>T</given-names></name><name><surname>Schalek</surname><given-names>RL</given-names></name><name><surname>Li</surname><given-names>PH</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Maitin-Shepard</surname><given-names>J</given-names></name><name><surname>Karlupia</surname><given-names>N</given-names></name><name><surname>Dorkenwald</surname><given-names>S</given-names></name><name><surname>Sjostedt</surname><given-names>E</given-names></name><name><surname>Leavitt</surname><given-names>L</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Bailey</surname><given-names>L</given-names></name><name><surname>Fitzmaurice</surname><given-names>A</given-names></name><name><surname>Kar</surname><given-names>R</given-names></name><name><surname>Field</surname><given-names>B</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Wagner-Carena</surname><given-names>J</given-names></name><name><surname>Aley</surname><given-names>D</given-names></name><name><surname>Lau</surname><given-names>J</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Wei</surname><given-names>D</given-names></name><name><surname>Pfister</surname><given-names>H</given-names></name><name><surname>Peleg</surname><given-names>A</given-names></name><name><surname>Jain</surname><given-names>V</given-names></name><name><surname>Lichtman</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A Connectomic Study of A Petascale Fragment of Human Cerebral Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.05.29.446289</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Torres</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021a</year><data-title>Uri-handler</data-title><version designator="10">10</version><source>Handler</source><ext-link ext-link-type="uri" xlink:href="https://www.slideshare.net/sayaleepote/uri-handlers">https://www.slideshare.net/sayaleepote/uri-handlers</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Torres</surname><given-names>R</given-names></name><collab>madiganz</collab><collab>tfliss</collab></person-group><year iso-8601-date="2021">2021b</year><data-title>Em_imaging_workflow</data-title><version designator="b9c201d">b9c201d</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/AllenInstitute/em_imaging_workflow">https://github.com/AllenInstitute/em_imaging_workflow</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Trautman</surname><given-names>E</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Render Tools and Services</data-title><source>Render</source></element-citation></ref><ref id="bib33"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vescovi</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Toward an automated HPC pipeline for processing large scale electron microscopy data</article-title><conf-name>2020 IEEE/ACM 2nd Annual Workshop on Extreme-Scale Experiment-in-the-Loop Computing (XLOOP</conf-name><fpage>16</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1109/XLOOP51963.2020.00008</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wetzel</surname><given-names>AW</given-names></name><name><surname>Bakal</surname><given-names>J</given-names></name><name><surname>Dittrich</surname><given-names>M</given-names></name><name><surname>Hildebrand</surname><given-names>DGC</given-names></name><name><surname>Morgan</surname><given-names>JL</given-names></name><name><surname>Lichtman</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Registering large volume serial-section electron microscopy image sets for neural circuit reconstruction using FFT signal whitening</article-title><conf-name>2016 IEEE Applied Imagery Pattern Recognition Workshop (AIPR</conf-name><pub-id pub-id-type="doi">10.1109/AIPR.2016.8010595</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>W</given-names></name><name><surname>Brittain</surname><given-names>D</given-names></name><name><surname>Borseth</surname><given-names>J</given-names></name><name><surname>Scott</surname><given-names>ME</given-names></name><name><surname>Williams</surname><given-names>D</given-names></name><name><surname>Perkins</surname><given-names>J</given-names></name><name><surname>Own</surname><given-names>CS</given-names></name><name><surname>Murfitt</surname><given-names>M</given-names></name><name><surname>Torres</surname><given-names>RM</given-names></name><name><surname>Kapner</surname><given-names>D</given-names></name><name><surname>Mahalingam</surname><given-names>G</given-names></name><name><surname>Bleckert</surname><given-names>A</given-names></name><name><surname>Castelli</surname><given-names>D</given-names></name><name><surname>Reid</surname><given-names>D</given-names></name><name><surname>Lee</surname><given-names>W-CA</given-names></name><name><surname>Graham</surname><given-names>BJ</given-names></name><name><surname>Takeno</surname><given-names>M</given-names></name><name><surname>Bumbarger</surname><given-names>DJ</given-names></name><name><surname>Farrell</surname><given-names>C</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>da Costa</surname><given-names>NM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A petascale automated imaging pipeline for mapping neuronal circuits with high-throughput transmission electron microscopy</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>4949</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-18659-3</pub-id><pub-id pub-id-type="pmid">33009388</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>Z</given-names></name><name><surname>Lauritzen</surname><given-names>JS</given-names></name><name><surname>Perlman</surname><given-names>E</given-names></name><name><surname>Robinson</surname><given-names>CG</given-names></name><name><surname>Nichols</surname><given-names>M</given-names></name><name><surname>Milkie</surname><given-names>D</given-names></name><name><surname>Torrens</surname><given-names>O</given-names></name><name><surname>Price</surname><given-names>J</given-names></name><name><surname>Fisher</surname><given-names>CB</given-names></name><name><surname>Sharifi</surname><given-names>N</given-names></name><name><surname>Calle-Schuler</surname><given-names>SA</given-names></name><name><surname>Kmecova</surname><given-names>L</given-names></name><name><surname>Ali</surname><given-names>IJ</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Trautman</surname><given-names>ET</given-names></name><name><surname>Bogovic</surname><given-names>JA</given-names></name><name><surname>Hanslovsky</surname><given-names>P</given-names></name><name><surname>Jefferis</surname><given-names>GSXE</given-names></name><name><surname>Kazhdan</surname><given-names>M</given-names></name><name><surname>Khairy</surname><given-names>K</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>Fetter</surname><given-names>RD</given-names></name><name><surname>Bock</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A complete electron microscopy volume of the brain of adult <italic>Drosophila melanogaster</italic></article-title><source>Cell</source><volume>174</volume><fpage>730</fpage><lpage>743</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.06.019</pub-id><pub-id pub-id-type="pmid">30033368</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Intelligence Advanced Research Projects Activity (IARPA) MICrONS phase 2 montage workflow with support for lens correction and manual intervention.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-app1-fig1-v2.tif"/></fig></app><app id="appendix-2"><title>Appendix 2</title><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>Electron microscopy (EM) workflow diagram from 10 represented in YAML as DAG.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-app2-fig1-v2.tif"/></fig></app><app id="appendix-3"><title>Appendix 3</title><fig id="app3fig1" position="float"><label>Appendix 3—figure 1.</label><caption><title>Performance of BlueSky workflow manager.</title><p>Time spent by the sections from all the datasets in each job queue in the montaging workflow. The processing times include the duration between the time the job started running in a node and the time the node releases the job as successfully completed. Processing times shown are based on running the job in a single computing node in every job queue.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-app3-fig1-v2.tif"/></fig></app><app id="appendix-4"><title>Appendix 4</title><fig id="app4fig1" position="float"><label>Appendix 4—figure 1.</label><caption><title>Performance of BlueSky workflow manager.</title><p>Total processing time for sections montaged using the BlueSky workflow manager for all the datasets. Processing times shown are based on running the job in a single computing node in every job queue.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76534-app4-fig1-v2.tif"/></fig></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s8"><title>Hardware configuration of all our services</title><p>The data transfer system (Aloha) by design scales out to multiple hosts, and networking is often the key factor for throughput. Currently, it is running (asymmetrically) on one of the following configurations:</p><list list-type="bullet"><list-item><p>2× Intel Xeon Platinum 8160, 1 TB RAM</p></list-item><list-item><p>2× Intel Xeon E5-2630 v4, 128 GB RAM</p></list-item></list><p>BlueSky is a lightweight service and shares hardware resources with the Render service and Mongo database. The hardware configuration is as follows:</p><list list-type="bullet"><list-item><p>2× Intel Xeon Gold 6128 CPU, 512 GB RAM, 6x HPE 12 TB SAS 7200 RPM HDD</p></list-item></list><p>The TEM database is also a lightweight service except that it requires a Mongo database, but the service rests on a dedicated server with the following configuration:</p><list list-type="bullet"><list-item><p>2× Intel Xeon E5-2630 v4, 128 GB RAM, 2 × 1 TB Seagate pro SSDs.</p></list-item></list></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76534.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Cardona</surname><given-names>Albert</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.11.24.469932" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.11.24.469932"/></front-stub><body><p>Datasets in volume electron microscopy have been growing fruit of the labor of the combined efforts of sample preparation specialists and electron microscopy engineers. A missing piece has been a method for the automation of the composition of continuous volumes out of collections of individual image tiles capable of handling the growing scales of the datasets. Pushing the boundaries of what is possible, this work illustrates how a successful approach looks like, demonstrated by its application to cubic millimeter volumes imaged at nanometer resolution. All being said, this work is but step 1 of a two-step process, whereby first a coarse but mostly correct alignment is computed, and then a refinement step using more local cues and with existing methods is applied, setting the stage for the subsequent automated reconstruction of neuronal arbors and their synapses from which to infer a cellular connectome.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76534.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Cardona</surname><given-names>Albert</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Cardona</surname><given-names>Albert</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Arganda-Carreras</surname><given-names>Ignacio</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000xsnr85</institution-id><institution>University of the Basque Country</institution></institution-wrap><country>Spain</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Tischer</surname><given-names>Christian</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03mstc592</institution-id><institution>EMBL</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.11.24.469932">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.11.24.469932v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A Scalable and Modular Automated Pipeline for Stitching of Large Electron Microscopy Datasets&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Albert Cardona as Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Ronald Calabrese as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Ignacio Arganda-Carreras (Reviewer #2); Christian Tischer (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Please make more clear and visible that the current framework, while impressively handling enormously large collections of image tiles, requires additional fine alignment for use in e.g., cellular connectomics.</p><p>2) Please could you elaborate on how the achieved registration accuracy relates to what would be needed to solve the scientific task.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>In general, the paper is easy to read and the main ideas are easy to follow. However, some sections seem too technical for a non-expert audience and might benefit from adding some definitions (as a glossary or maybe footnotes). For example, in the &quot;Software infrastructure supporting stitching and alignment&quot; section: &quot;It includes REST APIs for clients to GET/POST images […]&quot;, &quot;[…] backed by a MongoDB document store that contains JSON tile specifications […]&quot;</p><p>Regarding reproducibility, although the effort on providing open source solutions is remarkable, some documentation on how to set up the full pipeline would be greatly appreciated by the community, especially for a more modest or toy example. So far, the main site of the project (https://github.com/AllenInstitute/asap-modules) reads &quot;We are not currently supporting this code, but simply releasing it to the community AS IS but are not able to provide any guarantees of support, as it is under active development. The community is welcome to submit issues, but you should not expect an active response.&quot;. While understandable, this is discouraging for all the potential users and it lessens the impact of the paper.</p><p>Nothing is mentioned about the specifics of the hardware resources needed to process each of the datasets. Indeed, it is not clear if the same exact computing power was used in all cases. Including those details would help the readers have a better idea of the scale of the problem being addressed and also of its requirements.</p><p>In Figure 2, maybe a shaded box including &quot;ASAP&quot; (or ASAP modules) should appear to clarify which components belong to it.</p><p>In Table 1, the total size of the datasets without overlap is a bit confusing. Does it involve full stitching/registration or is it simply the size without repeated pixels?</p><p>In Figures 4, 5 and 7, some of the fonts are really small and hard to read, please enlarge them.</p><p>In Figure 6c, adding an arrow to the blood vessel text would help the non-expert eyes.</p><p>In Table 2, it would be very interesting to see an estimation of the time spent in proofreading and quality control tasks, thus showing the degree of automation of the process. Also, in that caption it reads &quot; The stitching time for all the datasets include&quot; where it should read &quot; The stitching time for all the datasets includes&quot;.</p><p>On page 16, where it reads &quot;A description of these tools are as follows;&quot; the sentence should end with a colon.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Software</p><p>1. ASAP-modules: The documentation is missing. It looks like the CI-generated docpages still refer to &quot;render-modules&quot;, which seems to be deprecated/old.</p><p>2. The installation is not straight-forward. It requires adding deprecated Qt4 (700MB of extra downloads) libraries to support building an old (&gt;3 years) opencv dependency, which is a lengthy procedure. Many dependencies are not available through the standard installation procedure (python setup or pip) but require manual intervention or actively downgrading installed packages using conda. It would be great if the installation could be simplified.</p><p>3. em_stitch: We could not find documentation or installation/usage tutorials for this software. The manuscript introduces this software as a stand-alone version of the alignment workflow, an interested reader will therefore likely start by exploring this software. Thus, an accessible tutorial including sample data will be very helpful. If em-stitch does not offer the full functionality to align &quot;small&quot; tiled datasets, a tutorial for setting up a simple pipeline (using the applicable tools) for a small example dataset would be very useful to someone interested in implementing the pipeline.</p><p>General remarks regarding the manuscript</p><p>1. The order in which the pipeline is presented is slightly confusing as it does not fit with the logical progress of tasks. While PointMatch determination and quality of the Solve step is shown in Figure 4, the QC determined by PM and resulting analysis in acquisition quality is shown afterwards in Figure 5.</p><p>2. What is the data resulting from running the pipeline? We assume it is a cloud-stored volume in N5 format to be visualised using Neuroglancer, but this does not become clear in the article. Does the pipeline also support outputs in different formats? (Sub-region as Tiff stack for local analysis in &quot;traditional&quot; software, OME-Zarr,..)</p><p>3. In line with our suggestion in the &quot;Public Review&quot; we suggest expanding Figure 1a such that the individual images are visible, e.g. such that one could appreciate the lens corrections, and include screenshots such as presented in Figure 3 and 6, as well as conceptual drawings such as in Figure 5a. The content of Figure 1b could then come later in the article, e.g, in a section on the &quot;software stack and implementation details&quot;. Essentially separating example data and concepts (new Figure 1) from implementation details (Figure X). We also suggest just showing one of the options in Figure 1a(dddd) to have more space for the aforementioned additions from the other figures. Like this we would hope that the first part of the publication could provide an attractive overview of the concepts as well as some examples for a broader audience.</p><p>Detail questions regarding the manuscript:</p><p>1. QC: The metrics used for quality control are not entirely clear. Is it purely the number of PointMatches and their deviation for each tile pair?</p><p>2. What is a typical manual intervention for tackling miss-alignments? How is it done in practice? How exactly does the mentioned &quot;parameter optimization&quot; work?</p><p>3. Figure 5b-e: The actual shape of the distributions is not discussed in the text, thus a table with median, min, max may be sufficient for the main text and the distributions could go to the supplement. Would it be possible to give the residuals (also) in nm instead of pixels? This would make it easier to judge whether the accuracy is sufficient for connectomics.</p><p>4. Figure 6: Is this a rough or fine alignment (compared with Suppl. Figures1-3)? If it depicts only the rough aligned data, please provide an idea of how the final result looks like.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76534.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Please make more clear and visible that the current framework, while impressively handling enormously large collections of image tiles, requires additional fine alignment for use in e.g., cellular connectomics.</p></disp-quote><p>We have made more clear throughout the text that in the presented datasets we used SEAMLess for fine alignment:</p><p>In the “Development of a stitching and alignment pipeline” of the Results section</p><p>“For all the datasets presented in this manuscript, the 2D stitching and global alignment was performed using ASAP, and afterwards the data was materialized and transferred outside of ASAP for fine alignment using SEAMLESS [18].”</p><p>In figure 2:</p><p>“The infrastructure permits multiple possible strategies for 3D alignment, including a chunk based approach in case it is not possible to 3D align the complete dataset at once, as well as using other workflows outside ASAP [18] (https://www.microns-explorer.org/cortical-mm3) for fine 3D alignment with the global 3D aligned volume obtained using ASAP.”</p><p>In the “Performance of the volume assembly pipeline” of the Results section:</p><p>“For the datasets described in this manuscript, this global alignment was the initialization point for the fine alignment done outside ASAP with SEAMLess [18]. The infrastructure present in ASAP can be however extended to &quot;fine&quot; alignments because ASAP is ready to implement 3D transformation both at the level of sections and at the level of individual image tiles. The quality of the fine alignment will depend on the transform that the user chooses to implement, ASAP is just a framework/vehicle for that transform.”</p><p>This topic is also addressed in the reply to reviewers 1 and 3 comments (R1-C7, R1-C9 and R3-C10) below which we also copy here.</p><disp-quote content-type="editor-comment"><p>2) Please could you elaborate on how the achieved registration accuracy relates to what would be needed to solve the scientific task.</p></disp-quote><p>One of our major scientific tasks in creating and analyzing these datasets is the analysis of connectivity between neurons. A major source of this connectivity is chemical synapses, and in the neocortex the most common location of synapses are dendritic spines. Spines are formed by a head (where the synapse is located) and a thin neck that connects the head to the main dendritic shaft. We reason that these small processes would be some of the most sensitive structures to misalignments of the imaging tiles and therefore we aimed for an accuracy of the stitching that would be approximately 10 times higher than the average diameter of a spine neck. The spine neck diameter is usually on average ~200 nm but has a broad range across different spines, hence the choice of 5 pixels (~ 20 nm). Twenty nanometers is also half the diameter of many of the very thin spine necks in the volume.</p><p>Bellow we also address individually each of the reviewers comments and recommendations.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>In general, the paper is easy to read and the main ideas are easy to follow. However, some sections seem too technical for a non-expert audience and might benefit from adding some definitions (as a glossary or maybe footnotes). For example, in the &quot;Software infrastructure supporting stitching and alignment&quot; section: &quot;It includes REST APIs for clients to GET/POST images […]&quot;, &quot;[…] backed by a MongoDB document store that contains JSON tile specifications […]&quot;</p></disp-quote><p>We thank the reviewer for the suggestion. We have added a Glossary section to the manuscript that includes descriptions of the technical acronyms in the manuscript.</p><disp-quote content-type="editor-comment"><p>Regarding reproducibility, although the effort on providing open source solutions is remarkable, some documentation on how to set up the full pipeline would be greatly appreciated by the community, especially for a more modest or toy example. So far, the main site of the project (https://github.com/AllenInstitute/asap-modules) reads &quot;We are not currently supporting this code, but simply releasing it to the community AS IS but are not able to provide any guarantees of support, as it is under active development. The community is welcome to submit issues, but you should not expect an active response.&quot;. While understandable, this is discouraging for all the potential users and it lessens the impact of the paper.</p></disp-quote><p>We have updated our github repository to include the most recent version of the documentation for ASAP and its modules. The documentation now includes complete step by step instructions to install and run ASAP and its modules. We do provide support for users wishing to use our software and also welcome their suggestions and improvements in the form of pull requests and active discussions as we have done in the past.</p><disp-quote content-type="editor-comment"><p>Nothing is mentioned about the specifics of the hardware resources needed to process each of the datasets. Indeed, it is not clear if the same exact computing power was used in all cases. Including those details would help the readers have a better idea of the scale of the problem being addressed and also of its requirements.</p></disp-quote><p>We have included descriptions and details of our hardware resources that were used in the volume assembly process. These details are included in the Methods section “BlueSky workflow engine for automated processing”. The following paragraph included in the manuscript details this information.</p><p>“For all our datasets, BlueSky utilized three different kinds of hardware nodes in our HPC cluster. The configurations are as follows;</p><p>– 2x Intel xeon 2620 processor with 256GB RAM</p><p>– 2x Intel xeon 2630 processor with 256GB RAM</p><p>– 2x Intel xeon gold 6238 processor with 512GB RAM</p><p>All our datasets were processed using a maximum of 50 nodes to perform 2D stitching of sections in parallel. A combination of the nodes with the above configuration was used in processing.”</p><p>In addition to this, the performance of the workflow manager (BlueSky) that utilizes these resources for processing our datasets is illustrated in Supplemental figures 7 and 8.</p><disp-quote content-type="editor-comment"><p>In Figure 2, maybe a shaded box including &quot;ASAP&quot; (or ASAP modules) should appear to clarify which components belong to it.</p></disp-quote><p>ASAP modules are loaded in our HPC compute cluster during processing and it interacts with other components of the workflow. We have modified Figure 3 (previous Figure 2) to include this change.</p><disp-quote content-type="editor-comment"><p>In Table 1, the total size of the datasets without overlap is a bit confusing. Does it involve full stitching/registration or is it simply the size without repeated pixels?</p></disp-quote><p>This is the size without repeated pixels. We have added to the manuscript:</p><p>“The details about these datasets are shown in Table 1, where the ROI size and total non-overlapping dataset size (without repeated pixels) were determined from montage metadata including pixel size and nominal overlap.”</p><disp-quote content-type="editor-comment"><p>In Figures 4, 5 and 7, some of the fonts are really small and hard to read, please enlarge them.</p></disp-quote><p>The fonts on all the suggested figures were increased. Please note that the figure numbers have been increased by 1 due to the addition of a new Figure 1 with the introduction to the pipeline, following the suggestion of reviewer 3.</p><disp-quote content-type="editor-comment"><p>In Figure 6c, adding an arrow to the blood vessel text would help the non-expert eyes.</p></disp-quote><p>We have made appropriate changes to the figure to make it more clear for the readers. Please note that the figure numbers have been increased by 1 due to the addition of a new Figure 1 with the introduction to the pipeline, following the suggestion of reviewer 3.</p><disp-quote content-type="editor-comment"><p>In Table 2, it would be very interesting to see an estimation of the time spent in proofreading and quality control tasks, thus showing the degree of automation of the process. Also, in that caption it reads &quot; The stitching time for all the datasets include&quot; where it should read &quot; The stitching time for all the datasets includes&quot;.</p></disp-quote><p>The ASAP processing times also include the manual QC processing time duration. For each dataset, the manual QC processing time is roughly a few minutes per serial section, this was however not quantified thoroughly for the sake of reporting and therefore we do not report it in the manuscript.</p><disp-quote content-type="editor-comment"><p>On page 16, where it reads &quot;A description of these tools are as follows;&quot; the sentence should end with a colon.</p></disp-quote><p>We thank the reviewer for the careful review. We have made appropriate changes to the punctuations in the text.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>Software</p><p>1. ASAP-modules: The documentation is missing. It looks like the CI-generated docpages still refer to &quot;render-modules&quot;, which seems to be deprecated/old.</p></disp-quote><p>The documentation to ASAP-modules has been updated to match with the latest version of the code. The github repository now points to the correct and detailed version of the documentation.</p><disp-quote content-type="editor-comment"><p>2. The installation is not straight-forward. It requires adding deprecated Qt4 (700MB of extra downloads) libraries to support building an old (&gt;3 years) opencv dependency, which is a lengthy procedure. Many dependencies are not available through the standard installation procedure (python setup or pip) but require manual intervention or actively downgrading installed packages using conda. It would be great if the installation could be simplified.</p></disp-quote><p>We are very thankful to the Reviewer, for going through the installation process. We have implemented the suggestions and fixes. Specifically, we added a new pypi publishing workflow to em_stitch, render-python, and bigfeta. With these modifications we’ve been able to update the stack to expect modern versions of opencv. Now ASAP will also install with pip and run tests in a fresh docker container. We’ve always been quick to help when others trying to use the software have issues and we changed the language on the “Level of Support '' which was also a suggestion from Reviewer 2.</p><disp-quote content-type="editor-comment"><p>3. em_stitch: We could not find documentation or installation/usage tutorials for this software. The manuscript introduces this software as a stand-alone version of the alignment workflow, an interested reader will therefore likely start by exploring this software. Thus, an accessible tutorial including sample data will be very helpful. If em-stitch does not offer the full functionality to align &quot;small&quot; tiled datasets, a tutorial for setting up a simple pipeline (using the applicable tools) for a small example dataset would be very useful to someone interested in implementing the pipeline.</p></disp-quote><p>We simplified the installation process for em_stitch and added these instructions to the repository's README file. We also added two jupyter notebooks which can be used with the data in the public repository -- one which derives a lens correction and shows an example plotting the distortion field, and another which solves for a montage given the derived lens correction. Both of these examples use point correspondences that were collected by the autoTEM at the time of acquisition, but can be implemented using any method for defining point correspondences.</p><disp-quote content-type="editor-comment"><p>General remarks regarding the manuscript</p><p>1. The order in which the pipeline is presented is slightly confusing as it does not fit with the logical progress of tasks. While PointMatch determination and quality of the Solve step is shown in Figure 4, the QC determined by PM and resulting analysis in acquisition quality is shown afterwards in Figure 5.</p></disp-quote><p>Figures 4 and 5 in the current submission shows the QC plots and QC statistics plots while Figure 6 (previous Figure 5 in the original submission) shows the stitching quality not the acquisition quality. The violin plots show the distribution of the median residual values and the scale distribution of the tile images post stitching. These two metrics are used to assess the quality of stitched montages for all our datasets.</p><disp-quote content-type="editor-comment"><p>2. What is the data resulting from running the pipeline? We assume it is a cloud-stored volume in N5 format to be visualised using Neuroglancer, but this does not become clear in the article. Does the pipeline also support outputs in different formats? (Sub-region as Tiff stack for local analysis in &quot;traditional&quot; software, OME-Zarr,..)</p></disp-quote><p>The pipeline also supports other output formats (though not explicitly Tiff). We have added to the Materialization subsection of the manuscript:</p><p>“Alignment through BigFeta produces tile metadata that can be interpreted by Render and its clients to produce &quot;materialized&quot; representations of the stack. These representations are rendered client-side, having transformations applied and flattening overlapping tiles. The output of materialization can be in a number of file formats including n5 and the CATMAID large data tilesource, which are included in Render. It is also possible to implement custom output formats based on available python libraries using render-python to access Render's client scripts. As SEAMLeSS expects data in the cloud-volume compatible neuroglancer precomputed format, the datasets in Table 1 were materialized with a python script that uses render-python to write to neuroglancer precomputed format using the cloud-volume library.”</p><disp-quote content-type="editor-comment"><p>3. In line with our suggestion in the &quot;Public Review&quot; we suggest expanding Figure 1a such that the individual images are visible, e.g. such that one could appreciate the lens corrections, and include screenshots such as presented in Figure 3 and 6, as well as conceptual drawings such as in Figure 5a. The content of Figure 1b could then come later in the article, e.g, in a section on the &quot;software stack and implementation details&quot;. Essentially separating example data and concepts (new Figure 1) from implementation details (Figure X). We also suggest just showing one of the options in Figure 1a(dddd) to have more space for the aforementioned additions from the other figures. Like this we would hope that the first part of the publication could provide an attractive overview of the concepts as well as some examples for a broader audience.</p></disp-quote><p>We have added a new Figure 1 addressing most of the reviewer suggestions, including the broad overview, conceptual drawings and as well more detail of individual images. Figure 1 of the initial submission is now Figure 2.</p><disp-quote content-type="editor-comment"><p>Detail questions regarding the manuscript:</p><p>1. QC: The metrics used for quality control are not entirely clear. Is it purely the number of PointMatches and their deviation for each tile pair?</p></disp-quote><p>To clarify this point we changed the manuscript text to:</p><p>“The stitching issues that are identified include misalignments between stitched tiles, gaps in montages, and seams. These issues are identified based on the mean residual between point correspondences from every pair of tiles. This represents how well the point correspondences have aligned from each of the tiles after montage transformations are applied to them (see Figure 4.a). This metric is represented in pixel distance and is used to locate areas of misalignments and seams. The gaps in stitching are identified by means of how many neighbors a tile image has before and after stitching and based on their area of overlap with its neighbors. A seam appears as misalignment between a tile and many of its neighbors and is identified using a cluster of point correspondences whose residuals are above a certain threshold. In addition to these metrics, we also compute the mean absolute deviation (MAD) (see Figure 5) that measures the amount of distortion a tile image undergoes with the transformation. The MAD statistics is a measure using which we identify montages that are distorted (see Figure 5) once it passes the automated QC identifying other issues. Since the crux of our computations are based on the point correspondences, we also generate plots to quickly visualize the density of point correspondences between tile images within a section (see Figure 4.k).”</p><disp-quote content-type="editor-comment"><p>2. What is a typical manual intervention for tackling miss-alignments? How is it done in practice? How exactly does the mentioned &quot;parameter optimization&quot; work?</p></disp-quote><p>To address this question as well as the comments R1C6 from Reviewer 1 we have made added the following text as well as a new Supplementary Figure 1:</p><p>“Sections that failed QC are examined by a human proofreader and moved to the appropriate stage of re-processing. A manual proofreading process typically includes examining the QC plot for issues and further visualizing those areas in montage with issues to ensure that those issues either correspond to resin or film region tiles or tiles corresponding to the tissue region. The regions with misalignments are further examined to send them to the appropriate stage of processing. If the misalignments are caused due to insufficient point correspondences, then they are sent to the point-matching stage of the montage workflow for generation of point correspondences at a higher resolution. Misaligned sections with sufficient point correspondences are sent to the solver stage with new parameters. These parameters were heuristically chosen by means of a parameter optimization algorithm based on the stitching quality metrics (see Methods section for more details and Supplementary Figure 1 for optimized parameter selection plots).”</p><disp-quote content-type="editor-comment"><p>3. Figure 5b-e: The actual shape of the distributions is not discussed in the text, thus a table with median, min, max may be sufficient for the main text and the distributions could go to the supplement. Would it be possible to give the residuals (also) in nm instead of pixels? This would make it easier to judge whether the accuracy is sufficient for connectomics.</p></disp-quote><p>We have included a table showing the median residual values and median x and y scale values in a table (Table 2) in addition to the description of the Figure 5b-e (now Figure 6b-e). The violin plots in the figure depicts the density distribution of the median residual values computed for every serial section from our datasets and are grouped by the acquisition systems. It can be seen that the density of distribution is below the threshold value (the horizontal line in these plots) indicating the stitching quality of the serial sections.</p><disp-quote content-type="editor-comment"><p>4. Figure 6: Is this a rough or fine alignment (compared with Suppl. Figures1-3)? If it depicts only the rough aligned data, please provide an idea of how the final result looks like.</p></disp-quote><p>Figure 7 (Figure 6 in the original submission) as well as supplemental figures 2-5, show the cross sectional views of the global alignment (“rough” alignment) of the four datasets discussed in this work. The final result after fine alignment with SeamLESS can be found in www.microns-explorer.org/mm3. Please see also reply to R1C7</p></body></sub-article></article>