<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">99290</article-id><article-id pub-id-type="doi">10.7554/eLife.99290</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.99290.4</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neural dynamics of visual working memory representation during sensory distraction</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Degutis</surname><given-names>Jonas Karolis</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0003-8682-3920</contrib-id><email>j.karolis.degutis@maxplanckschools.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Weber</surname><given-names>Simon</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8440-1156</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Soch</surname><given-names>Joram</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8879-5666</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Haynes</surname><given-names>John-Dylan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1786-6954</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/001w7jn25</institution-id><institution>Bernstein Center for Computational Neuroscience Berlin and Berlin Center for Advanced Neuroimaging, Charité Universitätsmedizin Berlin, Corporate member of the Freie Universität Berlin, Humboldt-Universität zu Berlin, and Berlin Institute of Health</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hhn8329</institution-id><institution>Max Planck School of Cognition</institution></institution-wrap><addr-line><named-content content-type="city">Munich</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Department of Psychology, Humboldt-Universität zu Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Research Training Group 'Extrospection' and Berlin School of Mind and Brain, Humboldt-Universität zu Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>Institute of Psychology, Otto von Guericke University</institution></institution-wrap><addr-line><named-content content-type="city">Magdeburg</named-content></addr-line><country>Germany</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Leipzig</named-content></addr-line><country>Germany</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/043j0f473</institution-id><institution>German Center for Neurodegenerative Diseases</institution></institution-wrap><addr-line><named-content content-type="city">Bonn</named-content></addr-line><country>Germany</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/042aqky30</institution-id><institution>Collaborative Research Center 'Volition and Cognitive Control', Technische Universität Dresden</institution></institution-wrap><addr-line><named-content content-type="city">Dresden</named-content></addr-line><country>Germany</country></aff><aff id="aff9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v4gjf40</institution-id><institution>Research Cluster of Excellence 'Science of Intelligence', Technische Universität Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Xue</surname><given-names>Gui</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>Beijing Normal University</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University, Howard Hughes Medical Institute</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>24</day><month>06</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP99290</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-05-23"><day>23</day><month>05</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-05-23"><day>23</day><month>05</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.04.12.589170"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-08-27"><day>27</day><month>08</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.99290.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-04-11"><day>11</day><month>04</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.99290.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-06-05"><day>05</day><month>06</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.99290.3"/></event></pub-history><permissions><copyright-statement>© 2024, Degutis et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Degutis et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-99290-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-99290-figures-v1.pdf"/><abstract><p>Recent studies have provided evidence for the concurrent encoding of sensory percepts and visual working memory (VWM) contents across visual areas; however, it has remained unclear how these two types of representations are concurrently present. Here, we reanalyzed an open-access fMRI dataset where participants memorized a sensory stimulus while simultaneously being presented with sensory distractors. First, we found that the VWM code in several visual regions did not fully generalize between different time points, suggesting a dynamic code. A more detailed analysis revealed that this was due to shifts in coding spaces across time. Second, we collapsed neural signals across time to assess the degree of interference between VWM contents and sensory distractors, specifically by testing the alignment of their encoding spaces. We find that VWM and feature-matching sensory distractors are encoded in coding spaces that do not fully overlap, but the separation decreases when distractors negatively impact behavioral performance in recalling the target. Together, these results indicate a role of dynamic coding and temporally stable coding spaces in helping multiplex perception and VWM within visual areas.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>working memory</kwd><kwd>sensory distraction</kwd><kwd>dynamic coding</kwd><kwd>neural subspaces</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100018668</institution-id><institution>Max Planck School of Cognition</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Degutis</surname><given-names>Jonas Karolis</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>SFB 940</award-id><principal-award-recipient><name><surname>Haynes</surname><given-names>John-Dylan</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>Research Training Group 2386 451</award-id><principal-award-recipient><name><surname>Weber</surname><given-names>Simon</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>SFB-TRR 295</award-id><principal-award-recipient><name><surname>Haynes</surname><given-names>John-Dylan</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>EXC 2002/1</award-id><principal-award-recipient><name><surname>Weber</surname><given-names>Simon</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Dynamic shifts in neural coding combined with stable population subspaces enable visual areas to concurrently represent sensory inputs and working memory without mutual interference.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>To successfully achieve behavioral goals, humans rely on the ability to remember, update, and ignore information. Visual working memory (VWM) allows for a brief maintenance of visual stimuli that are no longer present within the environment (<xref ref-type="bibr" rid="bib10">Curtis and D’Esposito, 2003</xref>; <xref ref-type="bibr" rid="bib21">Goldman Rakic, 1995</xref>; <xref ref-type="bibr" rid="bib14">D’Esposito and Postle, 2015</xref>). Previous studies have revealed that the contents of VWM are present throughout multiple visual areas, starting from V1 (<xref ref-type="bibr" rid="bib18">Fuster and Alexander, 1971</xref>; <xref ref-type="bibr" rid="bib23">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib7">Christophel et al., 2012</xref>; <xref ref-type="bibr" rid="bib15">Ester et al., 2015</xref>; <xref ref-type="bibr" rid="bib43">Rademaker et al., 2019</xref>; <xref ref-type="bibr" rid="bib43">Rademaker et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Riggall and Postle, 2012</xref>; <xref ref-type="bibr" rid="bib45">Serences et al., 2009</xref>; <xref ref-type="bibr" rid="bib8">Christophel et al., 2017</xref>; <xref ref-type="bibr" rid="bib11">Curtis and Sprague, 2021</xref>). These findings raised the question of how areas that are primarily involved in visual perception can also maintain VWM information without interference between the two contents. Recent studies that had participants remember a stimulus while simultaneously being presented with sensory stimuli during the delay period have found supporting evidence that both VWM contents and sensory percepts are multiplexed in occipital and parietal regions (<xref ref-type="bibr" rid="bib43">Rademaker et al., 2019</xref>; <xref ref-type="bibr" rid="bib24">Iamshchinina et al., 2021a</xref>; <xref ref-type="bibr" rid="bib2">Bettencourt and Xu, 2016</xref>). However, the mechanism employed to segregate bottom-up visual input from VWM contents remains poorly understood.</p><p>One proposed mechanism to achieve the separation between sensory and memory representations is dynamic coding (<xref ref-type="bibr" rid="bib52">Stokes et al., 2020</xref>; <xref ref-type="bibr" rid="bib51">Stokes, 2015</xref>; <xref ref-type="bibr" rid="bib53">Stroud et al., 2024</xref>): the change of the population code encoding VWM representations across time. Recent work has shown that the format of VWM might not be as persistent and stable throughout the delay as previously thought (<xref ref-type="bibr" rid="bib36">Miller et al., 2018</xref>; <xref ref-type="bibr" rid="bib48">Sreenivasan et al., 2014a</xref>). Frontal regions display dynamic population coding across the delay during the maintenance of category (<xref ref-type="bibr" rid="bib35">Meyers et al., 2008</xref>) and spatial contents in the absence of interference (<xref ref-type="bibr" rid="bib37">Murray et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Spaak et al., 2017</xref>), and also show dynamic recoding of the memoranda after sensory distraction (<xref ref-type="bibr" rid="bib38">Parthasarathy et al., 2017</xref>; <xref ref-type="bibr" rid="bib39">Parthasarathy et al., 2019</xref>). The visual cortex in humans displays dynamic coding of contents during high load trials (<xref ref-type="bibr" rid="bib49">Sreenivasan et al., 2014b</xref>) and during a spatial VWM task (<xref ref-type="bibr" rid="bib29">Li and Curtis, 2023</xref>). However, it is not yet clear whether dynamic coding of VWM might help evade sensory distraction in human visual areas.</p><p>Another line of evidence suggests that perception could potentially be segregated from VWM representations using stable nonoverlapping coding spaces (<xref ref-type="bibr" rid="bib32">Lorenc et al., 2021</xref>). For example, evidence from neuroanatomy indicates that the sensory bottom-up visual pathway primarily projects to the cytoarchitectonic Layer 4 in V1, while feedback projections culminate in superficial and deep layers of the cortex (<xref ref-type="bibr" rid="bib16">Felleman and Van Essen, 1991</xref>). Functional results are in line with neuroanatomy by showing that VWM signals preferentially activate the superficial and deep layers in humans (<xref ref-type="bibr" rid="bib27">Lawrence et al., 2018</xref>) and nonhuman primates (<xref ref-type="bibr" rid="bib55">van Kerkoerle et al., 2017</xref>), while perceptual signals are more prevalent in the middle layers (<xref ref-type="bibr" rid="bib28">Lawrence et al., 2019</xref>). In addition to laminar separation, regional multiplexing of multiple items could potentially rely on rotated representations, as seen in memory and sensory representations orthogonally coded in the auditory cortex (<xref ref-type="bibr" rid="bib30">Libby and Buschman, 2021</xref>) and in the storage of a sequence of multiple spatial locations in the prefrontal cortex (PFC) (<xref ref-type="bibr" rid="bib60">Xie et al., 2022</xref>). Nonoverlapping orthogonal representations have also been seen in both humans and trained recurrent neural networks as a way of segregating attended and unattended VWM representations (<xref ref-type="bibr" rid="bib57">Wan et al., 2022</xref>; <xref ref-type="bibr" rid="bib58">Wan et al., 2023</xref>; <xref ref-type="bibr" rid="bib56">van Loon et al., 2018</xref>).</p><p>Here, we investigated whether the concurrent presence of VWM and sensory information is compatible with predictions offered by dynamic coding or by stable nonaligned coding spaces. For this, we reanalyzed an open-access fMRI dataset by <xref ref-type="bibr" rid="bib43">Rademaker et al., 2019</xref>, where participants performed a delayed-estimation VWM task with and without sensory distraction. To investigate dynamic coding, we employed a temporal cross-decoding analysis that assessed how well the multivariate code encoding VWM generalizes from one time point to another (<xref ref-type="bibr" rid="bib46">Spaak et al., 2017</xref>; <xref ref-type="bibr" rid="bib50">Stokes et al., 2013</xref>; <xref ref-type="bibr" rid="bib12">Degutis et al., 2023</xref>; <xref ref-type="bibr" rid="bib1">Anders et al., 2011</xref>), and a temporal neural subspace analysis that examined a sensitive way of looking at alignment of neural populations coding for VWM at different time points. To assess the nonoverlapping coding hypothesis, we used neural subspaces (<xref ref-type="bibr" rid="bib37">Murray et al., 2017</xref>; <xref ref-type="bibr" rid="bib29">Li and Curtis, 2023</xref>; <xref ref-type="bibr" rid="bib30">Libby and Buschman, 2021</xref>) to see whether temporally stable representations of the VWM target and the sensory distractor are coded in separable neural populations. Finally, we examined the multivariate VWM code changes during distractor trials when compared to the no-distractor VWM format.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Temporal cross-decoding in distractor and no-distractor trials</title><p>In the previously published study (<xref ref-type="bibr" rid="bib43">Rademaker et al., 2019</xref>). two groups of participants completed two VWM experiments where on a given trial they were asked to remember an orientation of a grating, which they had to then recall at the end of the trial. In the first experiment, the delay period was either left blank (no-distractor) or a noise or randomly oriented grating distractor was presented (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). To investigate the dynamics of the VWM code, we examined how the multivariate pattern of activity encoding VWM memoranda changed across the duration of the delay period. To do so, we ran a temporal cross-decoding analysis where we trained a decoder (periodic support vector regression [pSVR], see <xref ref-type="bibr" rid="bib59">Weber et al., 2024</xref>) on the target orientation, separately for each time point and tested on all time points in turn in a cross-validated fashion. If the information encoding VWM memoranda were to have the same code, the trained decoder would generalize to other time points, indicated by similar decoding accuracies on the diagonal and off-diagonal elements of the matrix. However, if the code exhibited dynamic properties, despite information about the memoranda being present (above-chance decoding on the diagonal of the matrix), both off-diagonal elements corresponding to a given on-diagonal element would have lower decoding accuracies (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Such off-diagonal elements are considered an indication of a dynamic code.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Task and temporal cross-decoding.</title><p>(<bold>a</bold>) On each trial, an oriented grating was presented for 0.5 s followed by a delay period of 13 s (<xref ref-type="bibr" rid="bib43">Rademaker et al., 2019</xref><ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?PxMErQ">)</ext-link>. In a third of the trials, a noise distractor was presented for 11 s during the middle of the delay; in another third, another orientation grating was presented; one-third of trials had no-distractor during the delay. (<bold>b</bold>) Illustration of dynamic coding elements. An off-diagonal element had to have a lower decoding accuracy compared to both corresponding diagonal elements (see Methods for details). (<bold>c</bold>) Temporal generalization of the multivariate code encoding VWM representations in three conditions across occipital and parietal regions. Across-participant mean temporal cross-decoding of no-distractor trials. Black outlines: matrix elements showing above-chance decoding (cluster-based permutation test; p&lt;0.05). Blue outlines with dots: dynamic coding elements; parts of the cross-decoding matrix where the multivariate code fails to generalize (off-diagonal elements having lower decoding accuracy than their corresponding two diagonal elements; conjunction between two cluster-based permutation tests; p&lt;0.05). (<bold>d</bold>) Same as <bold>c</bold>, but noise distractor trials. (<bold>e</bold>) Same as <bold>c</bold>, but orientation distractor trials. (<bold>f</bold>) Dynamicism index; the proportion of dynamic coding elements across time. High values indicate a dynamic non-generalizing code, while low values indicate a generalizing code. Time indicates the time elapsed since the onset of the delay period.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-99290-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Task and temporal cross-decoding of Experiment 2.</title><p>(<bold>a</bold>) On each trial, an oriented grating was presented for 0.5 s followed by a delay period of 13 s. In a third of the trials, a naturalistic distractor was presented for 11 s during the middle of the delay; in another third, a flickering orientation grating was presented; one-third of trials had no-distractor during the delay. (<bold>b</bold>) Temporal generalization of the multivariate code encoding visual working memory (VWM) representations in three conditions across occipital and parietal regions. Across-participant mean temporal cross-decoding of no-distractor trials. Black outlines: matrix elements showing above-chance decoding (cluster-based permutation test; p&lt;0.05). Blue outlines with dots: dynamic coding elements; parts of the cross-decoding matrix where the multivariate code fails to generalize (off-diagonal elements having lower decoding accuracy than their corresponding two diagonal elements; conjunction between two cluster-based permutation tests; p&lt;0.05). (<bold>c</bold>) Same as <bold>b</bold>, but noise distractor trials. Dynamic coding elements depicted in red. (<bold>d</bold>) Same as c, but orientation distractor trials. (<bold>e</bold>) Decoding accuracy (feature continuous accuracy [FCA]) across time for train and test on no-distractor trials (purple), train and test on naturalistic distractor trials (dark green), and train and test on flickering orientation distractor trials (light green). Horizontal lines indicate clusters where there is a difference between two time courses (all clusters p&lt;0.05; nonparametric cluster permutation test, see color code on the right).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-99290-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Simulations.</title><p>(<bold>a</bold>) Signal-to-noise ratio (SNR)-dependent decoding accuracy, obtained from simulated voxel responses. SNR decreases from left to right (training axis) and bottom to top (testing axis). (<bold>b</bold>) SNR-dependent decoding accuracy, obtained from no-distractor data from Experiment 1. SNR first decreases and then increases in both training and test axes. (<bold>c</bold>) SNR-dependent decoding accuracy, obtained from noise distractor data from Experiment 1. SNR first decreases and then increases in both training and test axes.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-99290-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Selected voxels.</title><p>Top: Experiment 1. Number of voxels (left) and proportion of voxels selected in a region of interest (ROI) (right). Error bars indicate SEM across participants. Bottom: same as top, but for Experiment 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-99290-fig1-figsupp3-v1.tif"/></fig></fig-group><p>We ran the temporal cross-decoding analysis in the first of two experiments for the three VWM delay conditions: no-distractor, noise distractor, and orientation distractor (feature-matching distractor). First, we examined each element of the cross-decoding matrix to test whether decoding accuracies were above chance. In all three conditions and throughout all regions of interest (ROIs), we found clusters where decoding was above chance (<xref ref-type="fig" rid="fig1">Figure 1c–e</xref>, black outline; nonparametric cluster permutation test against null; all clusters p&lt;0.05) from as early as 4 s after the onset of the delay period. We found that decoding on the diagonal was highest during no-distractor compared to noise and orientation distractor trials in most ROIs (Figure 4a).</p><p>Second, we examined off-diagonal elements to assess whether there was any indication that they reflected a non-generalizing dynamic code (see Methods for full details). Despite a high degree of temporal generalization, we found dynamic coding clusters in all three conditions. Some degree of dynamic coding was observed in all ROIs but LO2 in the noise distractor and no-distractor trials, while it was only present in V1, V2, V3, V4, and IPS in the orientation distractor condition (<xref ref-type="fig" rid="fig1">Figure 1c–e</xref>, blue outline). The difference between noise and orientation distractor conditions could not be explained by the amount of information present in each ROI, as the decoding accuracy of the diagonal was similar across all ROIs in both the noise and orientation distractor conditions (Figure 4a). We saw a nominally larger number of dynamic coding elements in V1, V2, and V3AB during the noise distractor condition and in V3 during the no-distractor condition (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>To qualitatively compare the amount of dynamic coding in the three conditions across the delay period, we calculated a dynamicism index (<xref ref-type="bibr" rid="bib46">Spaak et al., 2017</xref>; <xref ref-type="fig" rid="fig1">Figure 1e</xref>; see Methods), which measured the multivariate code’s uniqueness at each time point; more precisely, the proportion of dynamic elements corresponding to each diagonal element. High values indicate dynamic code, and low values indicate a generalizing code. Across all conditions, most dynamic elements occurred between the encoding and early delay periods (4–8 s), and the late delay and retrieval (14.4–16.8 s). Interestingly, during the noise distractor trials in V1, we also saw dynamic coding during the middle of the delay period; the multivariate code not only changed during the onset and offset of the noise stimulus, but also during its presentation and throughout the extent of the delay.</p><p>We also ran the same temporal cross-decoding analysis on the second experiment from <xref ref-type="bibr" rid="bib43">Rademaker et al., 2019</xref>. Participants performed the same type of VWM task with three conditions: a blank delay (no-distractor), a naturalistic distractor, and a flickering orientation distractor (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a</xref>). The decoding accuracies across all conditions, including the no-distractor condition, were nominally lower in the second experiment compared to the first. In some ROIs, the information about the target was not present during the delay period (chance decoding on the diagonal; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1b–d</xref>). Similarly to the first experiment’s orientation trials, the flickering orientation condition did not exhibit high levels of dynamic coding of the target (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1b–d</xref>). Due to the decreased amount of target information present across time points in the two distractor conditions (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1e</xref>), we do not further investigate the temporal dynamics of VWM storage in the second experiment.</p><p>We conducted several simulations to investigate whether changes in signal-to-noise ratio (SNR) could lead to dynamic coding clusters in our temporal cross-decoding analyses. This could occur if a decoder trained at one SNR fails to generalize to another, because each may rely on different features within the data. Specifically, we varied the added noise level in either a simulated dataset of voxel responses or the empirical results from V1 in the no-distractor or noise distractor trials in the first experiment. We then trained on a given noise level and tested on all other noise levels, corresponding to the temporal cross-decoding analysis. We see an absence of dynamic elements in the SNR cross-decoding matrix, as the decoding accuracy more strongly depends on the training data rather than test data. This results in some off-diagonal values in the decoding matrix that are higher, rather than smaller, than corresponding on-diagonal elements (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2a</xref>). In the second and third simulations, we used empirical data. To follow the initial decrease and subsequent increase in decoding accuracy found in most ROIs (Figure 4a), we initially decreased and then increased the SNR in the train and test axes of the matrix (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). Similarly to the first simulation, the cross-decoding matrix lacked dynamic elements, with the decoding accuracy of most off-diagonal elements exceeding their corresponding on-diagonal elements (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2b and c</xref>). These simulations indicate that SNR differences are unlikely to give rise to dynamic coding clusters.</p></sec><sec id="s2-2"><title>Dynamics of VWM neural subspaces across time</title><p>The temporal cross-decoding analysis of the first experiment revealed more dynamic coding in the early visual cortex primarily during the early and late delay phase and more generalized coding throughout the delay in higher-order regions. To understand the nature of these effects in more detail, we conducted a separate series of analyses that directly assessed the neural subspaces in which the orientations were encoded and how these potentially changed across time. Specifically, we followed a previous methodological framework (<xref ref-type="bibr" rid="bib29">Li and Curtis, 2023</xref>) and applied a principal component analysis (PCA) to the high-dimensional activity patterns at each time point to identify the two axes that explained maximal variance across orientations (see <xref ref-type="fig" rid="fig2">Figure 2</xref> and Methods).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Assessing the dynamics of neural subspaces in V1-V3AB.</title><p>(<bold>a</bold>) Schematic illustration of the neural subspace analysis. A given data matrix (voxels × orientation bins) was subjected to a principal components analysis (PCA), and the first two dimensions were used to define a neural subspace onto which a left-out test data matrix was projected. This resulted in a matrix of two coordinates for each orientation bin and was visualized (see right). The <italic>x</italic> and <italic>y</italic> axes indicate the first two principal components. Each color depicts an angular bin. (<bold>b</bold>) Schematic illustration of the calculation of an above-baseline principal angle (aPA). A principal angle (PA) is the angle between the 2D PCA-based neural subspaces (as in <bold>a</bold>) for two different time points t<sub>1</sub>, t<sub>2</sub>. A small angle would indicate alignment of coding spaces; an angle of above-baseline would indicate a shift in the coding space. The aPA is the angle for a comparison between two time points (<bold>t<sub>1</sub>, t<sub>2</sub></bold>) minus the angle between cross-validated pairs of the same time points. (<bold>c</bold>) Each row shows a projection that was estimated for one of two time ranges (middle and late delay) and then applied to all time points (using independent, split-half cross-validated data). Opacity increases from early to late time points. For visualization purposes, the subspaces were estimated on a participant-aggregated region of interest (ROI) (<xref ref-type="bibr" rid="bib29">Li and Curtis, 2023</xref>). The axes represent the first two principal components, with labels indicating the percent of total explained variance. <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> depicts the same projections as neural trajectories. (<bold>d</bold>) aPA between all pairwise time point comparisons (nonparametric permutation test against null; FDR-corrected p&lt;0.05) averaged across 1000 split-half iterations. Corresponding p-values can be found in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1, table S1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-99290-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Neural trajectories across time.</title><p>Same as <xref ref-type="fig" rid="fig2">Figure 2c</xref>, but the time dimension is on the <italic>z</italic> axis.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-99290-fig2-figsupp1-v1.tif"/></fig></fig-group><p>First, we visualized the consistency of the neural subspaces across time. For this, we computed low-dimensional 2D neural subspaces for a given time point and projected left-out data from six time points during the delay onto this subspace (<xref ref-type="bibr" rid="bib29">Li and Curtis, 2023</xref>; <xref ref-type="bibr" rid="bib30">Libby and Buschman, 2021</xref><ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?pmpeUT">)</ext-link>. A projection of data from a single time point resulted in four orientation bin values placed within the subspace (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, colored circles indicate orientation). Considering projected data from all time points, if the VWM code were generalizing, we would see a clustering of orientation points in a subspace; however, if orientation points were scattered around the neural subspace, this would show a non-generalizing code.</p><p>We examined the projections in a combined ROI spanning V1-V3AB aggregated across participants. We projected left-out data from all six time point bins onto subspaces generated from the early (7.2 s), middle (12 s), and late (16.8 s) time point data for each of the three conditions. Overall, the results showed generalization across time with some exceptions (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The clustering of orientation bins in the no-distractor condition was most pronounced (Figure 4a). In contrast, the noise distractor trials showed a resemblance of some degree of dynamic coding, as seen by less variance explained by early time points projected onto the middle subspace and the early and middle time points projected onto the late subspace (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>To quantify the visualized changes, we measured the alignment between each pair of subspaces by calculating the above-baseline principal angle (aPA) (<xref ref-type="fig" rid="fig2">Figure 2b</xref>) within the combined V1-V3AB ROI. The aPA measures the alignment between the 2D subspaces encoding the VWM representations: the higher the angle, the smaller the alignment between two subspaces and an indication of a changed neural coding space. Unlike in the projection of data from time points, the aPA was calculated participant-wise. Using a split-half approach, we measured the aPA between each split-pair of subspaces and subtracted the angles measured within each of the subspaces, with the latter acting as a null baseline.</p><p>All three conditions showed significant aPAs (<xref ref-type="fig" rid="fig2">Figure 2d</xref>; cyan stars; permutation test; p&lt;0.05, FDR-corrected). Corresponding to the results from the cross-decoding analysis, the early (4.8 s) and late (16.8 s) delay subspaces showed the highest number of significant pairwise aPAs in all conditions, with noise distractor trials having all pairwise aPAs, including the early and late subspaces, being significant. The three conditions each had two significant aPAs between time points in the middle of the delay period.</p></sec><sec id="s2-3"><title>Alignment between distractor and target subspaces in orientation distractor trials</title><p>Next, we assessed any similarity in encoding between the memorized orientation targets and the orientation distractors by focusing on those trials where both occurred. First, we examined whether the encoding of the sensory distractor in the first experiment is stable across its entire presentation duration (1.5–12.5 s after target onset) using the same approach as for the VWM target (<xref ref-type="fig" rid="fig1">Figure 1e</xref>). We found stable coding of the distractor in all ROIs with only a few dynamic elements in V2 and V3 (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). We then assessed whether the sensory distractor had a similar code to the VWM target by examining whether the multivariate code across time generalizes from the target to the distractor and vice versa. When cross-decoded, the sensory distractor (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1c</xref>) and target orientation (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1b</xref>) had lower decoding accuracies in the early visual cortex compared to when trained and tested on the same label type, indicative of a non-generalizing code. Such a difference was not seen in higher-order visual regions, as the decoding of the sensory distractor was low to begin with (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a–c</xref>).</p><p>Since we found minimal dynamics in the encoding of the distractor (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a</xref>) and target (<xref ref-type="fig" rid="fig1">Figure 1e</xref>) in the first experiment, we focused on temporally stable neural subspaces that encoded the target and sensory distractor. We computed stable neural subspaces where we disregarded the temporal variance by averaging across the whole delay period and binned the trials based on either the target orientation (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, left subpanel) or the distractor orientation (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, right subpanel). We then projected left-out data binned based on the target (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, green quadrilateral) or the distractor (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, gray quadrilateral). This projection provided us with both a baseline (as when training and testing on the same label) and a cross-generalization. Unsurprisingly, the target subspace explained the left-out target data well (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, left subpanel, green quadrilateral); however, the target subspace explained less variance of the left-out distractor data (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, left subpanel, gray quadrilateral), as qualitatively seen from the smaller spread of the sensory distractor orientations. A similar but less pronounced dissociation between projections was seen in the distractor subspace (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, left, quadrilateral in green) with the distractor subspace better explaining the left-out distractor data. We quantified the difference between the target and distractor subspaces and found a significant aPA between them (p=0.0297, one-tailed nonparametric permutation test; <xref ref-type="fig" rid="fig3">Figure 3b</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Generalization between target and distractor codes in orientation distractor visual working memory (VWM) trials in V1-V3AB.</title><p>(<bold>a</bold>) Left: projection of left-out target (green) and sensory distractor (gray) onto an orientation VWM target neural subspace. Right: same as left, but the projections are onto the sensory distractor subspace. The axes represent the first two principal components, with labels indicating the percent of total explained variance. (<bold>b</bold>) Principal angle between the sensory distractor and orientation VWM target subspaces (p=0.0297, one-tailed permutation test of sample mean), averaged across 1000 split-half iterations. Error bars indicate SEM across participants. (<bold>c</bold>) Same as <bold>a</bold>, but for flickering orientation distractor trials in the second experiment. (<bold>d</bold>) Same as<bold> b</bold>, but for flickering orientation distractor trials in the second experiment (p&lt;0.001, one-tailed permutation test of sample mean). The same figure for individual regions of interest (ROIs) can be seen in <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-99290-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Temporal cross-decoding of distractor and memory target in orientation distractor trials in Experiment 1.</title><p>(<bold>a</bold>) Across-participant mean temporal cross-decoding of the sensory distractor. Black outlines: matrix elements showing above-chance decoding (cluster-based permutation test; p&lt;0.05). Blue outlines with dots: dynamic coding element (conjunction between two cluster-based permutation tests; p&lt;0.05). (<bold>b</bold>) Same as a, but the decoder was trained on the target and tested on the sensory distractor in orientation visual working memory (VWM) trials. (<bold>c</bold>) Same as a, but trained on the sensory distractor and tested on the target.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-99290-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Temporal cross-decoding of distractor and memory target in flickering orientation distractor trials in Experiment 2.</title><p>(<bold>a</bold>) Across-participant mean temporal cross-decoding of the sensory distractor. Black outlines: matrix elements showing above-chance decoding (cluster-based permutation test; p&lt;0.05). Red outlines with dots: dynamic coding element (conjunction between two cluster-based permutation tests; p&lt;0.05). (<bold>b</bold>) Same as a, but the decoder was trained on the target and tested on the sensory distractor in orientation visual working memory (VWM) trials. (<bold>c</bold>) Same as a, but trained on the sensory distractor and tested on the target.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-99290-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Stable coding spaces of memory target and distractor in each region of interest (ROI).</title><p>(<bold>a</bold>) Experiment 1. Left: projection of left-out target (green) and sensory distractor (gray) onto an orientation visual working memory (VWM) target neural subspace. Right: same as left, but the projections are onto the sensory distractor subspace. (<bold>b</bold>) Experiment 2. Same as <bold>a</bold>, but for flickering orientation distractor trials. (<bold>c</bold>) Experiment 1. Principal angle between the sensory distractor and orientation VWM target subspaces in each ROI (from V1 to LO2: p=0.019, 0.045, 0.034, 0.034, 0.109, 0.045, 0, 0.034; one-tailed permutation test of sample mean, FDR-corrected), averaged across 1000 split-half iterations. Error bars indicate SEM across participants. (<bold>d</bold>) Experiment 2. Same as <bold>c</bold> but for flickering orientation distractor trial (from V1 to LO2: p=0.078, 0, 0, 0.078, 0.263, 0.214, 0.177, 0.263; one-tailed permutation test of sample mean, FDR-corrected).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-99290-fig3-figsupp3-v1.tif"/></fig></fig-group><p>We ran the same analyses on the flickering orientation distractor trials in the second experiment, in which the flickering distractor trials negatively impacted target recall compared to the no-distractor trials (<xref ref-type="bibr" rid="bib43">Rademaker et al., 2019</xref>). Similarly to the first experiment, we find reliable temporal cross-decoding of the distractor (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2a</xref>) and a non-generalizing code when trained and tested on the target and distractor, respectively, and vice versa (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2b and c</xref>). As the target (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1c</xref>) and distractor (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2a</xref>) displayed limited coding dynamics, we also computed stable neural subspaces of the target and distractor. As in the first experiment, the target subspace explained the left-out target data well (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, left subpanel, green quadrilateral); however, the same subspace explained the left-out distractor better than in the first experiment (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, left subpanel, gray quadrilateral), as seen from the larger overlap between the projected distractor and target quadrilaterals. Similarly, there was a large overlap between the projections in the distractor subspace (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, right subpanel). The aPA was significant (p&lt;0.001, one-tailed nonparametric permutation test; <xref ref-type="fig" rid="fig3">Figure 3d</xref>), but nominally smaller than in the first experiment. The results from the first and second experiments provide evidence for the presence of separable stable neural subspaces that might enable the multiplexing of VWM and perception across the extent of the delay period, and the separation of these subspaces is impacted by behavioral performance.</p></sec><sec id="s2-4"><title>Impact of distractors on VWM multivariate code</title><p>To further assess the impact of distractors on the available VWM information, we examined the decoding accuracies of distractor and no-distractor trials across time in the first experiment. Decoding accuracy was higher in the no-distractor trials compared to both orientation and noise distractor trials across all ROIs, but IPS (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, red and blue lines, p&lt;0.05, cluster permutation test) across several stages of the delay period. To further assess how distractors affected the delay period information, we increased sensitivity by collapsing across it, because time courses were comparable in all conditions (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). To assess to which degree VWM encoding generalized from no-distractor to distractor trials, we trained a decoder on no-distractor trials and tested it on both types of distractor trials (<xref ref-type="fig" rid="fig4">Figure 4b</xref> noise- and orientation-cross). We expressed the decoding accuracy of each distractor condition as a proportion of the decoding accuracy in the no-distractor condition. Values close to one indicate comparable information, while values below one mean the decoder does not generalize well. We found that the cross-decoding accuracies were significantly lower than the no-distractor in all ROIs but V4 (in both noise and orientation) and LO2 (only noise). Thus, in most areas, the decoder did not generalize well from the no-distractor to distractor conditions. However, the total amount of information in distractor trials was generally slightly lower (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). Thus, we also compared the generalization to a decoder trained and tested on the same distractor condition (<xref ref-type="fig" rid="fig4">Figure 4b</xref> noise- and orientation-within), which might thus be able to extract more information. We found that indeed information was recovered in areas V2 and V3AB in the noise distractor condition (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, pairwise permutation test). Thus, there was more information in the noise distractor condition, but it was not accessible to a decoder trained only on no-distractor trials. Additionally, a temporal cross-decoding analysis where all training time points were no-distractor trials had less dynamic coding in early visual regions (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>) when compared to the temporal cross-decoding matrix when trained and tested on noise distractor trials (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). These results indicate a change in the VWM format between the noise distractor and no-distractor trials.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Cross-decoding between distractor and no-distractor conditions in Experiment 1.</title><p>(<bold>a</bold>) Decoding accuracy (feature continuous accuracy [FCA]) across time for train and test on no-distractor trials (purple), train and test on noise distractor trials (dark green), and train and test on orientation distractor trials (light green). Horizontal lines indicate clusters where there is a difference between two time courses (all clusters p&lt;0.05; nonparametric cluster permutation test, see color code on the right). (<bold>b</bold>) Decoding accuracy as a proportion of no-distractor decoding estimated on the averaged delay period (4–16.8 s). Nonparametric permutation tests compared the decoding accuracy of each analysis to the no-distractor decoding baseline (indicated as a dashed line) and between a decoder trained and tested on distractor trials (noise- or orientation-within) and a decoder trained on no-distractor trials and tested on distractor trials (noise- or orientation-cross). FDR-corrected across regions of interest (ROIs). *p&lt;0.05, ***p&lt;0.001. Corresponding p-values can be found in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1, table S2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-99290-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Temporal cross-decoding generalization between distractor and no-distractor visual working memory (VWM) trials.</title><p>(<bold>a</bold>) Across-participant mean temporal cross-decoding of noise distractor trials when trained on no-distractor trials. (<bold>b</bold>) Same as <bold>a</bold>, but orientation distractor trials trained on no-distractor trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-99290-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Cross-decoding between distractor and no-distractor conditions in Experiment 2.</title><p>Decoding accuracy as a proportion of no-distractor decoding estimated on the averaged delay period (4–16.8 s). Nonparametric permutation tests compared the decoding accuracy of each analysis to the no-distractor decoding baseline (indicated as a dashed line) and between a decoder trained and tested on distractor trials (noise- or orientation-within) and a decoder trained on no-distractor trials and tested on distractor trials (noise- or orientation-cross). FDR-corrected across regions of interest (ROIs). *p&lt;0.05, **p<italic>&lt;</italic>0.01, ***p&lt;0.001. Corresponding p-values can be found in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1, table S3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-99290-fig4-figsupp2-v1.tif"/></fig></fig-group><p>We find a similar pattern of results in the second experiment’s naturalistic distractor condition, where there is a recovery of information in V3 and LO2 and LO1 in the flickering orientation distractor condition (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We examined the dynamics of VWM with and without distractors and explored the impact of sensory distractors on the coding spaces of VWM contents in visual areas by reanalyzing previously published data (<xref ref-type="bibr" rid="bib43">Rademaker et al., 2019</xref>). In two experiments, participants completed a task during which they had to maintain an orientation stimulus in VWM. In the first experiment, the delay period either had no-distractor, an orientation distractor, or a noise distractor. The second experiment had either no-distractor, flickering orientation distractor, or naturalistic distractor trials. We assessed two potential mechanisms that could help concurrently maintain the superimposed sensory and memory representations. First, we examined whether changes were observable in the multivariate code for memory contents across time, which we term dynamic coding. For this, we used two different analyses: temporal cross-classification and a direct assessment of angles between coding spaces. We found evidence for dynamic coding in all conditions of the first experiment, but there were differences in these dynamics between conditions and regions. Dynamic coding was most pronounced during the noise distractor trials in early visual regions. Second, we assessed the complementary question of temporally stable coding spaces. We computed the stable neural subspaces by averaging across the delay period and saw that coding of the VWM target and concurrent sensory distractors occurred in different stable neural subspaces. This overlap was less pronounced in the second experiment where the flickering orientation distractor impacted behavioral performance when recalling the memory target (<xref ref-type="bibr" rid="bib43">Rademaker et al., 2019</xref><ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?bPSM45">)</ext-link>. Finally, we observed that the format of the multivariate VWM code during the noise distraction differs from the VWM code when distractors were not present.</p><p>Dynamic encoding of VWM contents has been repeatedly examined before. Temporal cross-decoding analyses have been used in a number of nonhuman primate electrophysiology and human fMRI studies (<xref ref-type="bibr" rid="bib52">Stokes et al., 2020</xref>; <xref ref-type="bibr" rid="bib46">Spaak et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Parthasarathy et al., 2017</xref>; <xref ref-type="bibr" rid="bib49">Sreenivasan et al., 2014b</xref>; <xref ref-type="bibr" rid="bib29">Li and Curtis, 2023</xref>; <xref ref-type="bibr" rid="bib12">Degutis et al., 2023</xref>; <xref ref-type="bibr" rid="bib5">Cavanagh et al., 2018</xref>). <xref ref-type="bibr" rid="bib46">Spaak et al., 2017</xref>, found dynamic coding in the nonhuman primate PFC during a spatial VWM task. They observed a change in the multivariate code between different stages; specifically, a first shift between the encoding and maintenance periods, and also a second shift between the maintenance and retrieval periods. The initial transformation between the encoding and maintenance periods might recode the percept of the target into a stable VWM representation, whereas the second might transform the stable memoranda into a representation suited for initiation of motor output. A similar dynamic coding pattern was also observed in human visual regions using neuroimaging (<xref ref-type="bibr" rid="bib29">Li and Curtis, 2023</xref>). In this study, in all three conditions, we find a comparable pattern of results, where the multivariate code changes between the early delay and middle delay, and middle delay and late delay periods.</p><p>When noise distractors are added to the delay period, we find evidence of additional coding shifts in V1 during the middle of the delay. Previous research in nonhuman primates has shown that the presentation of a distractor induces a change in multivariate encoding for VWM in lateral PFC (lPFC) (<xref ref-type="bibr" rid="bib38">Parthasarathy et al., 2017</xref>). More precisely, a lack of generalization was observed between the population code encoding VWM before the presentation of a distractor (first half of the delay) and after its presentation (the second half). Additionally, continuous shifts in encoding have been observed in the extrastriate cortex throughout the extent of the delay period when decoding multiple remembered items at high VWM load (<xref ref-type="bibr" rid="bib49">Sreenivasan et al., 2014b</xref>). The dynamic code has been interpreted to enable multiplexing of representations when the visual cortex is overloaded by the maintenance of multiple stimuli at once. Future research could examine how properties of the distractor and of the target stimulus could interact to lead to dynamic coding. One intriguing hypothesis is that distractors that perturb the activity of feature channels that are used to encode VWM representations induce changes in its coding space over time. It is important to note that in the first experiment, the activation of the encoded target features was highest for the noise stimulus. Thus, the shared spatial frequencies between the noise distractor and the VWM contents potentially contribute to a more pronounced dynamic coding effect.</p><p>In a complementary analysis, we directly assessed subspaces in which orientations were encoded in VWM. We defined the subspaces for three different time windows: early, middle, and late. We find no evidence that the identity of orientations is confusable across time, e.g., we do not observe 45° at one given time point being recoded as 90° from a different time point. Such dynamics have been previously observed in the rotation of projected angles within a fixed neural subspace (<xref ref-type="bibr" rid="bib30">Libby and Buschman, 2021</xref>; <xref ref-type="bibr" rid="bib57">Wan et al., 2022</xref>) Rather, we find a decreased generalization between neural subspaces at different time points, as previously observed in a spatial VWM task (<xref ref-type="bibr" rid="bib29">Li and Curtis, 2023</xref>). These results suggest that the temporal dynamics across the VWM trial periods are driven by changes in the coding subspace of VWM. We do observe a preservation of the topology of the projected angles, as more similar angles remained closer together (e.g. the bin containing 45° was always closer to the bin containing 0° and 90°). Such a topology has been seen in V4 during a color perception task (<xref ref-type="bibr" rid="bib4">Brouwer and Heeger, 2009</xref>).</p><p>We also find evidence that the VWM contents are encoded in a different way depending on whether a noise distractor is presented or not. The decoder trained on no-distractor trials does not generalize well, presumably because it fails to fully access all the information present in noise distractor trials. If the decoders are trained directly on the distractor conditions, the VWM-related information is much higher. Additionally, we see that the code generalizes better across time when training on no-distractor trial time points and testing on noise distractor trials. This may imply that by training our decoder on the no-distractor trials, we are able to uncover an underlying stable population code encoding VWM in noise distractor trials. Consistent with this finding, <xref ref-type="bibr" rid="bib37">Murray et al., 2017</xref>, demonstrated that subspaces derived on the delay period could still generalize to the more dynamic encoding and retrieval periods, albeit not perfectly.</p><p>Interestingly, we found limited dynamic coding in the orientation distractor condition; primarily a change in the code between the early delay and middle delay periods was observed. Nonetheless, we find distinct temporally stable coding spaces in which sensory distractors and memory targets are encoded. These results correspond to prior research demonstrating a rotated format between perception and memory representations (<xref ref-type="bibr" rid="bib30">Libby and Buschman, 2021</xref>), attended and unattended VWM representations in both humans and recurrent-neural networks trained on a 2-back VWM (<xref ref-type="bibr" rid="bib57">Wan et al., 2022</xref>) and serial retro-cueing tasks (<xref ref-type="bibr" rid="bib58">Wan et al., 2023</xref>; <xref ref-type="bibr" rid="bib41">Piwek et al., 2023</xref>). Additionally, similar rotational dynamics have been observed between multiple spatial VWM locations stored in the nonhuman primate lPFC (<xref ref-type="bibr" rid="bib60">Xie et al., 2022</xref>). Considering the consistency of these results across different paradigms, we speculate that separate coding spaces might be a general mechanism of how feature-matching items can be concurrently multiplexed within visual regions. With growing evidence of the relationship between VWM capacity and neural resources available within the visual cortex (<xref ref-type="bibr" rid="bib9">Cohen et al., 2014</xref>; <xref ref-type="bibr" rid="bib17">Franconeri et al., 2013</xref>; <xref ref-type="bibr" rid="bib47">Sprague et al., 2014</xref>), further research could examine the number of feature-matching items that can be stored in nonaligned coding spaces.</p><p>In this study, the first experiment did not yield a behavioral deficit in the feature-matching orientation distractor trials, while the second experiment impacted target recall in the feature-matching flickering orientation trials (<xref ref-type="bibr" rid="bib43">Rademaker et al., 2019</xref>). There is evidence from behavioral and neural studies that show interactions between perception and VWM: feature-matching distractors behaviorally bias retrieved VWM contents (<xref ref-type="bibr" rid="bib42">Rademaker et al., 2015</xref>; <xref ref-type="bibr" rid="bib33">Mallett et al., 2020</xref>); VWM representations influence perception (<xref ref-type="bibr" rid="bib54">Teng and Kravitz, 2019</xref>; <xref ref-type="bibr" rid="bib26">Kang et al., 2011</xref>; <xref ref-type="bibr" rid="bib19">Gayet et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">Gayet et al., 2017</xref>); neural visual VWM representations in the early visual cortices are biased toward distractors (<xref ref-type="bibr" rid="bib31">Lorenc et al., 2018</xref>); and the fidelity of VWM neural representations within the visual cortex negatively correlates with behavioral errors when recalling VWM during a sensory distraction task (<xref ref-type="bibr" rid="bib22">Hallenbeck et al., 2021</xref>). In cases where a distractor does induce a drop in recall accuracy or biases the recalled VWM target, VWM and the sensory distractor neural subspaces might overlap more, as we see in the second experiment. However, it remains to be seen whether the degree of change or rotation between subspaces correlates with trial-to-trial behavior.</p><p>To our surprise, we did not observe a significant difference in the coding format of VWM between orientation distractor and no-distractor trials. Our initial expectation was that the VWM coding might undergo changes due to the target representation avoiding the distractor stimulus. However, the presence of a generalizing code between no-distractor and orientation distractor trials both in the first and second experiments, along with the nonaligned coding spaces between the target and distractor in both orientation trials, suggests an alternative explanation. We suggest that the sensory distractor stimulus occupies a distinct coding space throughout its presentation during the delay, while the coding space of the target remains the same in both orientation and no-distractor trials. Layer-specific coding differences in perception and VWM might explain these findings (<xref ref-type="bibr" rid="bib27">Lawrence et al., 2018</xref>; <xref ref-type="bibr" rid="bib28">Lawrence et al., 2019</xref>; <xref ref-type="bibr" rid="bib25">Iamshchinina et al., 2021b</xref>). Specifically, the sensory distractor neural subspace might predominantly reside in the bottom-up middle layers of early visual cortices, while the neural subspace encoding VWM might primarily occupy the superficial and deep layers.</p><p>We provide evidence for two types of mechanisms found in visual areas during the presence of both VWM and sensory distractors. First, our findings show dynamic coding of VWM within the human visual cortex during sensory distraction and indicate that such activity is present not only within the lPFC. Second, we find that VWM and feature-matching sensory distractors are encoded in shifted coding spaces, but the overlap between these subspaces increases in trials that negatively affect recall fidelity. Considering previous findings, we posit that different coding spaces within the same region might be a more general mechanism of segregating feature-matching stimuli. In sum, these results provide possible mechanisms of how VWM and perception are concurrently present within visual areas.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants, stimuli, procedure, and preprocessing</title><p>The following section is a brief explanation of parts of the methods covered in <xref ref-type="bibr" rid="bib43">Rademaker et al., 2019</xref>. Readers may refer to that paper for details. We reanalyzed data from Experiments 1 and 2.</p><p>In Experiment 1, six participants performed two tasks while in the scanner: a VWM task and a perceptual localizer task. In the perceptual localizer task, either a donut-shaped or a circle-shaped grating was presented in 9 s blocks. The participants had to respond whenever the grating dimmed. There was a total of 20 donut-shaped and 20 circle-shaped gratings in one run. Participants completed a total of 15–17 runs.</p><p>The visual VWM task began with the presentation of a colored 100% valid cue, which indicated the type of trial: no-distractor, orientation distractor, or noise distractor. Following the cue, the target orientation grating was presented centrally for 500 ms, followed by a 13 s delay period. In the trials with the distractor, a stimulus of the same shape and size as the target grating was presented centrally for 11 s in the middle of the delay period (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). The orientation and noise distractors reversed contrast at 4 Hz. At the end of the delay, a probe stimulus bar appeared at a random orientation. The participants had to align the bar to the target orientation and had to respond in 3 s. The orientations for the VWM sample were pseudorandomly chosen from six orientation bins, each consisting of 30 orientations. The orientation distractor and sample were counterbalanced in order not to have the same orientation presented as a distractor. Each run consisted of four trials of each condition. Across three sessions, participants completed 27 runs of the task, resulting in a total of 108 trials per condition.</p><p>In Experiment 2, seven participants performed the same type of VWM task with three trial types: no-distractor, flickering orientation distractor, and a naturalistic (gazebo or face) distractor (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a</xref>). The memory target presentation was the same as in Experiment 1. However, unlike Experiment 1, where distractors exhibited contrast reversals, the distractors in Experiment 2 alternated between being displayed on and off at 4 Hz. In the naturalistic distractor condition, a full set of 22 unique face images or 22 unique gazebo images was presented in a randomly shuffled sequence. In the flickering grating distractor condition, 22 gratings were shown, each sharing the same orientation but with a randomly assigned phase. As in Experiment 1, the orientations of both the target and distractor gratings were pseudorandomly drawn from six predefined orientation bins.</p><p>The data were acquired using a simultaneous multi-slice EPI sequence with a TR of 800 ms, TE of 35 ms, flip angle of 52°, and isotropic voxels of 2 mm. The data were preprocessed using FreeSurfer and FSL, and time series were z-scored across time for each voxel.</p></sec><sec id="s4-2"><title>Voxel selection</title><p>We used the same ROIs as in <xref ref-type="bibr" rid="bib43">Rademaker et al., 2019</xref>, which were derived using retinotopic mapping. In contrast to the original study, we reduced the size of our ROIs by selecting voxels that reliably responded to both the donut-shaped orientation perception task and the no-distractor VWM task. To select reliably activating voxels, we calculated four tuning functions for each voxel: two from the perceptual localizer and two from the no-distractor VWM task. The tuning functions spanned the continuous feature space in bins of 30°. Thus, to calculate the tuning functions, we ran a split-half analysis using stratified sampling where we binned all trials into six bins (of 30°). For both halves, tuning functions were estimated using a GLM that included six orientation regressors (one for each bin) and assumed an additive noise component independent and identically distributed across trials. We calculated Pearson’s correlations between the no-distractor memory and the perception tuning functions across the six parameter estimates extracted from the GLM, thus generating one memory-memory and one perception-perception correlation coefficient for each voxel.</p><p>The same analysis was additionally performed 1000 times on randomly permuted orientation labels to generate a null distribution for each participant and each ROI. These distributions were used to check for the reliability of voxel activation to perception and no-distractor VWM. After performing Fisher’s z-transformation on the correlations, we selected voxels that had a value above the 75th percentile of the null distributions in both the memory-memory and perception-perception correlations. This population of voxels was then used for all subsequent analyses. IPS included reliable voxels from retinotopically derived IPS0, IPS1, and IPS2. The number and proportion of voxels selected for each experiment is seen in <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>.</p></sec><sec id="s4-3"><title>Periodic support vector regression</title><p>We used pSVR to predict the target orientation from the multivariate BOLD activity (<xref ref-type="bibr" rid="bib59">Weber et al., 2024</xref>). PSVR uses a regression approach to estimate the sine and cosine components of a given orientation independently and therefore accounts for the circular nature of stimuli. To have a proper periodic function, orientation labels from the range [0°, 180°) were projected into the range [0, 2<italic>π</italic>).</p><p>We used the support vector regression algorithm using a nonlinear radial basis function kernel implemented in LIBSVM (<xref ref-type="bibr" rid="bib6">Chang and Lin, 2011</xref>) for orientation decoding. Specifically, sine and cosine components of the presented orientations were predicted based on multivariate fMRI signals from a set of voxels at specific time points within a trial (see <italic>Temporal cross-decoding</italic>). In each cross-validation fold, we rescaled the training data voxel activation into the range [0, 1] and applied the training data parameters to rescale the test data. For each participant, we had a total of three iterations in our cross-validation, where we trained on two-thirds (i.e. two sessions) and tested on one-third of the data (i.e. the left-out session). We selected three iterations to mitigate training and test data leakage (see <italic>Temporal cross-decoding</italic>).</p><p>After pSVR-based analysis, reconstructed orientations were obtained by plugging the predicted sine and cosine components into the four-quadrant inverse tangent:<disp-formula id="equ1"><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle \theta_p = atan2(x_p, y_p) $$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>x</italic><sub><italic>p</italic></sub> and <italic>y</italic><sub><italic>p</italic></sub> are pSVR outputs in the test set. Prediction accuracy was measured as the trial-wise absolute angular deviation between predicted orientation and actual orientation:<disp-formula id="equ2"><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mtext>circ</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle \Delta x = \left| (\theta - \theta_p)_{\text{circ}} \right| $$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>θ</italic> is the labeled orientation and <italic>θ</italic><sub><italic>p</italic></sub> is the predicted orientation. This measure was then transformed into a trial-wise feature continuous accuracy (FCA) (<xref ref-type="bibr" rid="bib40">Pilly and Seitz, 2009</xref>) as follows:<disp-formula id="equ3"><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>FCA</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>π</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mi>π</mml:mi></mml:mfrac><mml:mo>⋅</mml:mo><mml:mn>100</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle\text{FCA}= \frac{\pi-\Delta x}{\pi}\cdot 100$$\end{document}</tex-math></alternatives></disp-formula></p><p>The final across-trial accuracy was the mean of the trial-wise FCAs. Mean FCA was calculated across predicted orientations from all test sets after cross-validation was complete. The FCA is an equivalent measurement to standard accuracy measured in decoding analyses falling into the range between 0% and 100% but extended to the continuous domain. In the case of random guessing, the expected angular deviation is <italic>π</italic>/2, resulting in chance-level FCA at 50%.</p></sec><sec id="s4-4"><title>Temporal cross-decoding</title><p>To determine the underlying stability of the VWM code, we ran a temporal cross-decoding analysis using pSVR (<xref ref-type="fig" rid="fig1">Figure 1</xref>). We trained on data from a given time point and then predicted orientations for all time points, using the presented targets as labels. We trained on two-thirds of the trials per iteration and tested on the left-out third. Training and test data were never taken from the same trials, both when testing on the same and different time points.</p><p>We used a cluster-based approach to test for significance for above-chance decoding clusters (<xref ref-type="bibr" rid="bib34">Maris and Oostenveld, 2007</xref>) To determine whether the size of the cluster of the above-chance values was significantly larger than chance, we calculated a summed t-value for each cluster. We then generated a null distribution by randomly permuting the sign of the estimated above-chance accuracy (each FCA value was subtracted by 50%, such that 0 corresponds to chance level) of all components within the temporal cross-decoding matrix. We calculated the summed t-value for the largest randomly occurring above-chance cluster. This procedure was repeated 1000 times to estimate a null distribution. The empirical summed t-value of each cluster was then compared to the null distribution to determine significance (p&lt;0.05; without control of multiple cluster comparisons).</p><p>Dynamic coding clusters were defined as elements within the temporal cross-decoding matrix, where the multivariate code at a given time point did not fully generalize to another time point; in other words, an off-diagonal element was significantly smaller in accuracy compared to its two corresponding on-diagonal elements (<italic>a<sub>ij</sub> &lt; a<sub>ii</sub></italic> and <italic>a<sub>ij</sub> &lt; a<sub>jj</sub></italic>, <xref ref-type="fig" rid="fig1">Figure 1b</xref>). In order to test for the significance of these clusters, we ran two cluster permutation tests as done in previous studies to define dynamic clusters (<xref ref-type="bibr" rid="bib46">Spaak et al., 2017</xref>; <xref ref-type="bibr" rid="bib29">Li and Curtis, 2023</xref>). In each test, we subtracted one or the other corresponding diagonal elements from the off-diagonal elements (<italic>a<sub>ij</sub> – a<sub>ii</sub></italic> and <italic>a<sub>ij</sub> – a<sub>jj</sub></italic>). We then ran the same sign permutation test as for the above-chance decoding cluster for both comparisons. An off-diagonal element was deemed dynamic if both tests were significant (p&lt;0.05), and both corresponding diagonal elements were part of the above-chance decoding cluster.</p><p>Following <xref ref-type="bibr" rid="bib46">Spaak et al., 2017</xref>, we also computed the dynamicism index as a proportion of elements across time that were dynamic. Specifically, we calculated the proportion of (off-diagonal) dynamic elements corresponding to a diagonal time point in both columns (corresponding to the test time points) and rows (corresponding to the train time points) of the temporal cross-decoding matrix.</p></sec><sec id="s4-5"><title>Temporal cross-decoding simulations</title><p>To address the possibility that the dynamic clusters in the temporal cross-decoding analysis might arise as a result of the decoder picking up on different features within the signal as a function of the SNR, we ran three temporal cross-decoding simulations where the train and test data had varying levels of SNR.</p><p>In the first simulation, we created a dataset of 200 voxels that had a sine or cosine response function to orientations between 1° and 180°, the same orientations as the remembered target. A circular shift was applied to each voxel to vary preferred (or maximal) responses of each simulated voxel. This resulted in a dataset that captured the neural population’s response to all orientations. We then assessed the decoding performance under different SNR conditions during training and testing. In total, we ran seven iterations of the simulation, which correspond to the number of subjects in the second experiment. For each iteration, we randomly selected 108 responses from the full set of 180 for training, and then independently sampled another 108 from the same full set for testing. This ensured that the same orientation could appear in both sets, consistent with the structure of the original experiment. To increase variability, the selected trials differed in each iteration. Random white noise was applied to the data, and thus the SNR was independently scaled according to the specified levels for train and test data. We then use the same pSVR decoder as in the temporal cross-decoding analysis to train and test. We plot the SNR cross-decoding matrix as temporal decoding matrices (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2a</xref>).</p><p>The second and third simulations were conducted to investigate whether increased noise levels would induce the decoder to rely on different features of the no-distractor and noise distractor data from the first experiment. We used empirical data from the primary visual cortex (V1) under the no-distractor and noise distractor conditions for the second and third simulations, respectively. Data from time points 5.6–8.8 s after stimulus onset (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, V1) were averaged across five TRs. As in the first simulation, SNR was systematically manipulated by adding white noise. Additionally, to see whether the initial decrease in SNR and subsequent increase (as seen first during the delay and increase during the response period) would result in dynamic coding clusters, we initially increased and subsequently decreased the amplitude of added noise. The same pSVR decoder was used to train and test on the data in a cross-validated fashion with different levels of added noise. We plot the cross-decoding matrix as in the first simulation (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2b and c</xref>).</p></sec><sec id="s4-6"><title>Neural subspaces</title><p>We adapted the method from <xref ref-type="bibr" rid="bib29">Li and Curtis, 2023</xref>, to calculate two-dimensional neural subspaces encoding VWM information at a given time point. To do so, we used PCA. To maximize power, we binned trial-wise fMRI activations into four equidistant bins of 45° and averaged the signal across all trials within a bin (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). The data matrix <bold>Χ</bold> was defined as a <italic>ρ</italic>×<italic>υ</italic> matrix, where <italic>ρ</italic>=4 was the four orientation bins, and <italic>υ</italic> was the number of voxels. We mean-centered the columns (i.e. each voxel) of the data matrix.</p><p>This analysis focused on the time points from 4 s to 17.6 s after delay onset. The first TRs were not used since the temporal cross-decoding results showed no above-chance decoding. We averaged across every three TRs leading to six nonoverlapping temporal bins resulting in six <bold>Χ</bold> matrices. We calculated the principal components using eigen decomposition of the covariance matrix for each <bold>Χ</bold> and defined the matrix <bold>V</bold> using the two largest eigenvalues as a <italic>υ</italic>×2 matrix, resulting in six neural subspaces, one for each nonoverlapping temporal bin.</p></sec><sec id="s4-7"><title>Neural subspaces across time</title><p>For visualization purposes, we used three out of the total of six neural subspaces from the following time points: early (7.2 s), middle (12 s), and late (16.8 s). Following the aforementioned procedure, these subspaces were calculated on half of the trials, as we projected the left-out data onto the subspaces. The left-out data were binned into six temporal bins between 4 s and 17.6 s after target onset with no overlap, just like in the calculation of the six subspaces. The projection resulted in a <italic>ρ</italic>×2 matrix <bold>Ρ</bold> for each projected time bin (resulting in a total of six <bold>Ρ</bold> matrices). We use distinct colors to plot the temporal trajectories of each orientation bin across time in a 2D subspace flattened (<xref ref-type="fig" rid="fig2">Figure 2c</xref>) and not flattened (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) across the time dimension. Importantly, the visualization analysis was done on a combined participant-aggregated V1-V3AB region, which included all reliable voxels across the four regions and all six participants (see <italic>Voxel selection</italic>).</p><p>To measure the alignment between coding spaces at different times, we calculated an aPA between all subspaces (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). We used the MATLAB function <bold><monospace>subspace</monospace></bold> for an implementation of the method proposed by <xref ref-type="bibr" rid="bib3">Björck and Golub, 1973</xref>, to measure the angle between two <bold>V</bold> matrices. This provided us with a possible principal angle between 0° and 90°; the higher the angle, the larger the difference between the two subspaces. To avoid overfitting and as in the visualization analysis, we used a split-half approach to compute the aPA between subspaces. Half of the binned trials were used to calculate <bold>V<sub>i,A</sub></bold> and <bold>V<sub>j,A</sub>,</bold> and half for <bold>V<sub>i,B</sub> V<sub>j,B</sub></bold>, where <bold>A</bold> and <bold>B</bold> refer to the two halves of the split, and <bold>i</bold> and <bold>j</bold> refer to the two time bins compared. For significance testing, the within-subspace angle (the angle between two splits of the data within a given temporal bin [i.e. <bold>V<sub>i,A</sub></bold> and <bold>V<sub>i,B</sub></bold>]) was subtracted from the between-subspace PA (the angle between two different temporal bins [e.g. <bold>V<sub>i,A</sub></bold> and <bold>V<sub>j,B</sub></bold>]). Unlike the visualization analysis, the PA was calculated per participant 1000 times using different splits of the data on a combined V1-V3AB region that included the reliable voxels across the four regions (see <italic>Voxel selection</italic>). The final aPA value was an average across all iterations for each participant.</p></sec><sec id="s4-8"><title>Sensory distractor and orientation VWM target neural subspaces</title><p>For the orientation VWM target and sensory distractor neural subspace, we followed the aforementioned subspace analysis, but instead of calculating subspaces on six temporal bins, we averaged across the 4–17.6 s delay period and calculated a single subspace. As in the previous analysis, we split the orientation VWM trials in half. We then binned the trials either based on the target orientation or the sensory distractor. For visualization purposes, we projected the left-out data averaged based on the sensory distractor and the target onto subspaces derived from both the sensory distractor and target subspaces. As in the previous visualization, the analysis was run on a participant-aggregated V1-V3AB region.</p><p>To calculate the aPA, we had the following subspaces: <bold>V<sub>Target,A</sub></bold>, <bold>V<sub>Dist,A</sub></bold>, <bold>V<sub>Target,B</sub>,</bold> and <bold>V<sub>Dist,B</sub></bold>, where the subspaces were calculated on trials binned either based on the target orientation or the sensory distractor. The aPA was calculated by subtracting the within-subspace angle (<bold>V<sub>Target,A</sub></bold> and <bold>V<sub>Target,B</sub></bold>, <bold>V<sub>Dist,A</sub></bold> and <bold>V<sub>Dist,B</sub></bold>) from the sensory distractor and working memory angle (<bold>V<sub>Target,A</sub></bold> and <bold>V<sub>Dist,B</sub></bold>, <bold>V<sub>Target,B</sub></bold> and <bold>V<sub>Dist,A</sub></bold>). The split-half aPA analysis was performed 1000 times, and the final value was an average across these iterations for each participant.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Software, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Supervision, Writing – review and editing, Funding acquisition</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The study's dataset (Rademaker et al. 2019) was acquired at the University of California, San Diego, and was approved by the local Institutional Review Board. The participants provided written informed consent and were monetarily reimbursed.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Supplementary Tables.</title></caption><media xlink:href="elife-99290-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-99290-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The preprocessed data are shared open-access at <ext-link ext-link-type="uri" xlink:href="https://osf.io/dkx6y/">https://osf.io/dkx6y/</ext-link>. The analysis scripts and results are shared at <ext-link ext-link-type="uri" xlink:href="https://github.com/degutis/WM_dynamicCoding">https://github.com/degutis/WM_dynamicCoding</ext-link> (copy archived at <xref ref-type="bibr" rid="bib13">Degutis, 2025</xref>) and <ext-link ext-link-type="uri" xlink:href="https://osf.io/jq3ma/">https://osf.io/jq3ma/</ext-link>, respectively.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Degutis</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Neural dynamics of visual working memory representation during sensory distraction</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/jq3ma/">jq3ma</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Chunharas</surname><given-names>C</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Coexisting representations of sensory and mnemonic information in human visual cortex</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/dkx6y/">dkx6y</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>JKD was funded by the Max Planck Society and BMBF (as part of the Max Planck School of Cognition). JDH was supported by the Deutsche Forschungsgemeinschaft (DFG, Exzellenzcluster Science of Intelligence); SFB 940 'Volition and Cognitive Control'; and SFB-TRR 295 'Retuning dynamic motor network disorders using neuromodulation'. SW was supported by Deutsche Forschungsgemeinschaft (DFG) Research Training Group 2386 451 and EXC 2002/1 'Science of Intelligence'. We thank Rosanne Rademaker, Chaipat Chunharas, and John Serences for collecting and sharing their data open access, without which this reanalysis would not have been possible. We also thank Rosanne Rademaker, Michael Wolff, Amir Rawal, and Maria Servetnik for extensive discussions of the results. We also thank Vivien Chopurian and Thomas Christophel for their feedback on the manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anders</surname><given-names>S</given-names></name><name><surname>Heinzle</surname><given-names>J</given-names></name><name><surname>Weiskopf</surname><given-names>N</given-names></name><name><surname>Ethofer</surname><given-names>T</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Flow of affective information between communicating brains</article-title><source>NeuroImage</source><volume>54</volume><fpage>439</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.004</pub-id><pub-id pub-id-type="pmid">20624471</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bettencourt</surname><given-names>KC</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Decoding the content of visual short-term memory under distraction in occipital and parietal areas</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>150</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1038/nn.4174</pub-id><pub-id pub-id-type="pmid">26595654</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Björck</surname><given-names>Ȧke</given-names></name><name><surname>Golub</surname><given-names>GH</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Numerical methods for computing angles between linear subspaces</article-title><source>Mathematics of Computation</source><volume>27</volume><fpage>579</fpage><lpage>594</lpage><pub-id pub-id-type="doi">10.1090/S0025-5718-1973-0348991-3</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brouwer</surname><given-names>GJ</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding and reconstructing color from responses in human visual cortex</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>13992</fpage><lpage>14003</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3577-09.2009</pub-id><pub-id pub-id-type="pmid">19890009</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname><given-names>SE</given-names></name><name><surname>Towers</surname><given-names>JP</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Kennerley</surname><given-names>SW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reconciling persistent and dynamic hypotheses of working memory coding in prefrontal cortex</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>3498</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-05873-3</pub-id><pub-id pub-id-type="pmid">30158519</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>CC</given-names></name><name><surname>Lin</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>LIBSVM: A library for support vector machines</article-title><source>ACM Transactions on Intelligent Systems and Technology</source><volume>2</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1145/1961189.1961199</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decoding the contents of visual short-term memory from human visual and parietal cortex</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>12983</fpage><lpage>12989</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0184-12.2012</pub-id><pub-id pub-id-type="pmid">22993415</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Klink</surname><given-names>PC</given-names></name><name><surname>Spitzer</surname><given-names>B</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The distributed nature of working memory</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>111</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.12.007</pub-id><pub-id pub-id-type="pmid">28063661</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MA</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Rhee</surname><given-names>JY</given-names></name><name><surname>Nakayama</surname><given-names>K</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Processing multiple visual objects is limited by overlap in neural channels</article-title><source>PNAS</source><volume>111</volume><fpage>8955</fpage><lpage>8960</lpage><pub-id pub-id-type="doi">10.1073/pnas.1317860111</pub-id><pub-id pub-id-type="pmid">24889618</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Persistent activity in the prefrontal cortex during working memory</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>415</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(03)00197-9</pub-id><pub-id pub-id-type="pmid">12963473</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Persistent activity during working memory from front to back</article-title><source>Frontiers in Neural Circuits</source><volume>15</volume><elocation-id>696060</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2021.696060</pub-id><pub-id pub-id-type="pmid">34366794</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Degutis</surname><given-names>JK</given-names></name><name><surname>Chaimow</surname><given-names>D</given-names></name><name><surname>Haenelt</surname><given-names>D</given-names></name><name><surname>Assem</surname><given-names>M</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name><name><surname>Weiskopf</surname><given-names>N</given-names></name><name><surname>Lorenz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Dynamic layer-specific processing in the prefrontal cortex during working memory</article-title><source>bioRxiv</source><ext-link ext-link-type="uri" xlink:href="http://biorxiv.org/lookup/doi/10.1101/2023.10.27.564330">http://biorxiv.org/lookup/doi/10.1101/2023.10.27.564330</ext-link><pub-id pub-id-type="doi">10.1101/2023.10.27.564330</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Degutis</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>WM_dynamicCoding</data-title><version designator="swh:1:rev:0bb4d572d35cce0ce4a70c4a03e08463ba815561">swh:1:rev:0bb4d572d35cce0ce4a70c4a03e08463ba815561</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:339f0576323098b7257762c1b38f1f2e2af89d91;origin=https://github.com/degutis/WM_dynamicCoding;visit=swh:1:snp:51f0a2d0bd00f5b2292ae0ac88c8d46514f6300a;anchor=swh:1:rev:0bb4d572d35cce0ce4a70c4a03e08463ba815561">https://archive.softwareheritage.org/swh:1:dir:339f0576323098b7257762c1b38f1f2e2af89d91;origin=https://github.com/degutis/WM_dynamicCoding;visit=swh:1:snp:51f0a2d0bd00f5b2292ae0ac88c8d46514f6300a;anchor=swh:1:rev:0bb4d572d35cce0ce4a70c4a03e08463ba815561</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Esposito</surname><given-names>M</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The cognitive neuroscience of working memory</article-title><source>Annual Review of Psychology</source><volume>66</volume><fpage>115</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-010814-015031</pub-id><pub-id pub-id-type="pmid">25251486</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Parietal and frontal cortex encode stimulus-specific mnemonic representations during visual working memory</article-title><source>Neuron</source><volume>87</volume><fpage>893</fpage><lpage>905</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.07.013</pub-id><pub-id pub-id-type="pmid">26257053</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1-a</pub-id><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franconeri</surname><given-names>SL</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Flexible cognitive resources: competitive content maps for attention and memory</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>134</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.01.010</pub-id><pub-id pub-id-type="pmid">23428935</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuster</surname><given-names>JM</given-names></name><name><surname>Alexander</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Neuron activity related to short-term memory</article-title><source>Science</source><volume>173</volume><fpage>652</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1126/science.173.3997.652</pub-id><pub-id pub-id-type="pmid">4998337</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gayet</surname><given-names>S</given-names></name><name><surname>Paffen</surname><given-names>CLE</given-names></name><name><surname>Van der Stigchel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Information matching the content of visual working memory is prioritized for conscious access</article-title><source>Psychological Science</source><volume>24</volume><fpage>2472</fpage><lpage>2480</lpage><pub-id pub-id-type="doi">10.1177/0956797613495882</pub-id><pub-id pub-id-type="pmid">24121415</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gayet</surname><given-names>S</given-names></name><name><surname>Guggenmos</surname><given-names>M</given-names></name><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Haynes</surname><given-names>J-D</given-names></name><name><surname>Paffen</surname><given-names>CLE</given-names></name><name><surname>Van der Stigchel</surname><given-names>S</given-names></name><name><surname>Sterzer</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Visual working memory enhances the neural response to matching visual input</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>6638</fpage><lpage>6647</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3418-16.2017</pub-id><pub-id pub-id-type="pmid">28592696</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman Rakic</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Cellular basis of working memory</article-title><source>Neuron</source><volume>14</volume><fpage>477</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1016/0896-6273(95)90304-6</pub-id><pub-id pub-id-type="pmid">7695894</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hallenbeck</surname><given-names>GE</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Rahmati</surname><given-names>M</given-names></name><name><surname>Sreenivasan</surname><given-names>KK</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Working memory representations in visual cortex mediate distraction effects</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>4714</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-24973-1</pub-id><pub-id pub-id-type="pmid">34354071</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>SA</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title><source>Nature</source><volume>458</volume><fpage>632</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1038/nature07832</pub-id><pub-id pub-id-type="pmid">19225460</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iamshchinina</surname><given-names>P</given-names></name><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Gayet</surname><given-names>S</given-names></name><name><surname>Rademaker</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Essential considerations for exploring visual working memory storage in the human brain</article-title><source>Visual Cognition</source><volume>29</volume><fpage>425</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1080/13506285.2021.1915902</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iamshchinina</surname><given-names>P</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Yakupov</surname><given-names>R</given-names></name><name><surname>Haenelt</surname><given-names>D</given-names></name><name><surname>Sciarra</surname><given-names>A</given-names></name><name><surname>Mattern</surname><given-names>H</given-names></name><name><surname>Luesebrink</surname><given-names>F</given-names></name><name><surname>Duezel</surname><given-names>E</given-names></name><name><surname>Speck</surname><given-names>O</given-names></name><name><surname>Weiskopf</surname><given-names>N</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Perceived and mentally rotated contents are differentially represented in cortical depth of V1</article-title><source>Communications Biology</source><volume>4</volume><elocation-id>1069</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-021-02582-4</pub-id><pub-id pub-id-type="pmid">34521987</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kang</surname><given-names>MS</given-names></name><name><surname>Hong</surname><given-names>SW</given-names></name><name><surname>Blake</surname><given-names>R</given-names></name><name><surname>Woodman</surname><given-names>GF</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual working memory contaminates perception</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>18</volume><fpage>860</fpage><lpage>869</lpage><pub-id pub-id-type="doi">10.3758/s13423-011-0126-5</pub-id><pub-id pub-id-type="pmid">21713369</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawrence</surname><given-names>SJD</given-names></name><name><surname>van Mourik</surname><given-names>T</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Koopmans</surname><given-names>PJ</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Laminar organization of working memory signals in human visual cortex</article-title><source>Current Biology</source><volume>28</volume><fpage>3435</fpage><lpage>3440</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.08.043</pub-id><pub-id pub-id-type="pmid">30344121</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawrence</surname><given-names>SJ</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dissociable laminar profiles of concurrent bottom-up and top-down modulation in the human visual cortex</article-title><source>eLife</source><volume>8</volume><elocation-id>e44422</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.44422</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>HH</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neural population dynamics of human working memory</article-title><source>Current Biology</source><volume>33</volume><fpage>3775</fpage><lpage>3784</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2023.07.067</pub-id><pub-id pub-id-type="pmid">37595590</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Libby</surname><given-names>A</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Rotational dynamics reduce interference between sensory and memory representations</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>715</fpage><lpage>726</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00821-9</pub-id><pub-id pub-id-type="pmid">33821001</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorenc</surname><given-names>ES</given-names></name><name><surname>Sreenivasan</surname><given-names>KK</given-names></name><name><surname>Nee</surname><given-names>DE</given-names></name><name><surname>Vandenbroucke</surname><given-names>ARE</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Flexible coding of visual working memory representations during distraction</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>5267</fpage><lpage>5276</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3061-17.2018</pub-id><pub-id pub-id-type="pmid">29739867</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorenc</surname><given-names>ES</given-names></name><name><surname>Mallett</surname><given-names>R</given-names></name><name><surname>Lewis-Peacock</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Distraction in visual working memory: resistance is not futile</article-title><source>Trends in Cognitive Sciences</source><volume>25</volume><fpage>228</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.12.004</pub-id><pub-id pub-id-type="pmid">33397602</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mallett</surname><given-names>R</given-names></name><name><surname>Mummaneni</surname><given-names>A</given-names></name><name><surname>Lewis-Peacock</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Distraction biases working memory for faces</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>27</volume><fpage>350</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.3758/s13423-019-01707-5</pub-id><pub-id pub-id-type="pmid">31907852</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname><given-names>EM</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dynamic population coding of category information in inferior temporal and prefrontal cortex</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>1407</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1152/jn.90248.2008</pub-id><pub-id pub-id-type="pmid">18562555</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Lundqvist</surname><given-names>M</given-names></name><name><surname>Bastos</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Working Memory 2.0</article-title><source>Neuron</source><volume>100</volume><fpage>463</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.023</pub-id><pub-id pub-id-type="pmid">30359609</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>JD</given-names></name><name><surname>Bernacchia</surname><given-names>A</given-names></name><name><surname>Roy</surname><given-names>NA</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stable population coding for working memory coexists with heterogeneous neural dynamics in prefrontal cortex</article-title><source>PNAS</source><volume>114</volume><fpage>394</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1073/pnas.1619449114</pub-id><pub-id pub-id-type="pmid">28028221</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parthasarathy</surname><given-names>A</given-names></name><name><surname>Herikstad</surname><given-names>R</given-names></name><name><surname>Bong</surname><given-names>JH</given-names></name><name><surname>Medina</surname><given-names>FS</given-names></name><name><surname>Libedinsky</surname><given-names>C</given-names></name><name><surname>Yen</surname><given-names>SC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mixed selectivity morphs population codes in prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1770</fpage><lpage>1779</lpage><pub-id pub-id-type="doi">10.1038/s41593-017-0003-2</pub-id><pub-id pub-id-type="pmid">29184197</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parthasarathy</surname><given-names>A</given-names></name><name><surname>Tang</surname><given-names>C</given-names></name><name><surname>Herikstad</surname><given-names>R</given-names></name><name><surname>Cheong</surname><given-names>LF</given-names></name><name><surname>Yen</surname><given-names>SC</given-names></name><name><surname>Libedinsky</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Time-invariant working memory representations in the presence of code-morphing in the lateral prefrontal cortex</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>4995</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-12841-y</pub-id><pub-id pub-id-type="pmid">31676790</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pilly</surname><given-names>PK</given-names></name><name><surname>Seitz</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>What a difference a parameter makes: a psychophysical comparison of random dot motion algorithms</article-title><source>Vision Research</source><volume>49</volume><fpage>1599</fpage><lpage>1612</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2009.03.019</pub-id><pub-id pub-id-type="pmid">19336240</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piwek</surname><given-names>EP</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A recurrent neural network model of prefrontal brain activity during a working memory task</article-title><source>PLOS Computational Biology</source><volume>19</volume><elocation-id>e1011555</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011555</pub-id><pub-id pub-id-type="pmid">37851670</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Bloem</surname><given-names>IM</given-names></name><name><surname>De Weerd</surname><given-names>P</given-names></name><name><surname>Sack</surname><given-names>AT</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The impact of interference on short-term memory for visual orientation</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>41</volume><fpage>1650</fpage><lpage>1665</lpage><pub-id pub-id-type="doi">10.1037/xhp0000110</pub-id><pub-id pub-id-type="pmid">26371383</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Chunharas</surname><given-names>C</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Coexisting representations of sensory and mnemonic information in human visual cortex</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1336</fpage><lpage>1344</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0428-x</pub-id><pub-id pub-id-type="pmid">31263205</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riggall</surname><given-names>AC</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The relationship between working memory storage and elevated activity as measured with functional magnetic resonance imaging</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>12990</fpage><lpage>12998</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1892-12.2012</pub-id><pub-id pub-id-type="pmid">22993416</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Vogel</surname><given-names>EK</given-names></name><name><surname>Awh</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Stimulus-specific delay activity in human primary visual cortex</article-title><source>Psychological Science</source><volume>20</volume><fpage>207</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02276.x</pub-id><pub-id pub-id-type="pmid">19170936</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spaak</surname><given-names>E</given-names></name><name><surname>Watanabe</surname><given-names>K</given-names></name><name><surname>Funahashi</surname><given-names>S</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stable and dynamic coding for working memory in primate prefrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>6503</fpage><lpage>6516</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3364-16.2017</pub-id><pub-id pub-id-type="pmid">28559375</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Reconstructions of information in visual spatial working memory degrade with memory load</article-title><source>Current Biology</source><volume>24</volume><fpage>2174</fpage><lpage>2180</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.07.066</pub-id><pub-id pub-id-type="pmid">25201683</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sreenivasan</surname><given-names>KK</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>Revisiting the role of persistent neural activity during working memory</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>82</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.12.001</pub-id><pub-id pub-id-type="pmid">24439529</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sreenivasan</surname><given-names>KK</given-names></name><name><surname>Vytlacil</surname><given-names>J</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>Distributed and dynamic storage of working memory stimulus information in extrastriate cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>26</volume><fpage>1141</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00556</pub-id><pub-id pub-id-type="pmid">24392897</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Kusunoki</surname><given-names>M</given-names></name><name><surname>Sigala</surname><given-names>N</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Gaffan</surname><given-names>D</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic coding for cognitive control in prefrontal cortex</article-title><source>Neuron</source><volume>78</volume><fpage>364</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.039</pub-id><pub-id pub-id-type="pmid">23562541</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>“Activity-silent” working memory in prefrontal cortex: a dynamic coding framework</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>394</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.05.004</pub-id><pub-id pub-id-type="pmid">26051384</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Muhle-Karbe</surname><given-names>PS</given-names></name><name><surname>Myers</surname><given-names>NE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Theoretical distinction between functional states in working memory and their corresponding neural states</article-title><source>Visual Cognition</source><volume>28</volume><fpage>420</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1080/13506285.2020.1825141</pub-id><pub-id pub-id-type="pmid">33223922</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stroud</surname><given-names>JP</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>The computational foundations of dynamic coding in working memory</article-title><source>Trends in Cognitive Sciences</source><volume>28</volume><fpage>614</fpage><lpage>627</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2024.02.011</pub-id><pub-id pub-id-type="pmid">38580528</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teng</surname><given-names>C</given-names></name><name><surname>Kravitz</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Visual working memory directly alters perception</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>827</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0640-4</pub-id><pub-id pub-id-type="pmid">31285620</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Kerkoerle</surname><given-names>T</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Layer-specificity in the effects of attention and working memory on activity in primary visual cortex</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>13804</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13804</pub-id><pub-id pub-id-type="pmid">28054544</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Loon</surname><given-names>AM</given-names></name><name><surname>Olmos Solis</surname><given-names>K</given-names></name><name><surname>Fahrenfort</surname><given-names>JJ</given-names></name><name><surname>Olivers</surname><given-names>CN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Current and future goals are represented in opposite patterns in object selective cortex</article-title><source>eLife</source><volume>7</volume><elocation-id>e38677</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.38677</pub-id><pub-id pub-id-type="pmid">30394873</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wan</surname><given-names>Q</given-names></name><name><surname>Menendez</surname><given-names>JA</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Priority-based transformations of stimulus representation in visual working memory</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1009062</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009062</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wan</surname><given-names>Q</given-names></name><name><surname>Ardalan</surname><given-names>A</given-names></name><name><surname>Fulvio</surname><given-names>JM</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Representing Context and Priority in Working Memory</article-title><source>bioRxiv</source><ext-link ext-link-type="uri" xlink:href="http://biorxiv.org/lookup/doi/10.1101/2023.10.24.563608">http://biorxiv.org/lookup/doi/10.1101/2023.10.24.563608</ext-link><pub-id pub-id-type="doi">10.1101/2023.10.24.563608</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weber</surname><given-names>S</given-names></name><name><surname>Christophel</surname><given-names>T</given-names></name><name><surname>Görgen</surname><given-names>K</given-names></name><name><surname>Soch</surname><given-names>J</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Working memory signals in early visual cortex are present in weak and strong imagers</article-title><source>Human Brain Mapping</source><volume>45</volume><elocation-id>e26590</elocation-id><pub-id pub-id-type="doi">10.1002/hbm.26590</pub-id><pub-id pub-id-type="pmid">38401134</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Song</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Tang</surname><given-names>S</given-names></name><name><surname>Min</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Geometry of sequence working memory in macaque prefrontal cortex</article-title><source>Science</source><volume>375</volume><fpage>632</fpage><lpage>639</lpage><pub-id pub-id-type="doi">10.1126/science.abm0204</pub-id><pub-id pub-id-type="pmid">35143322</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.99290.4.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Xue</surname><given-names>Gui</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>Beijing Normal University</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib></contrib-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study reports a reanalysis of one experiment of a previously-published report to characterize the dynamics of neural population codes during visual working memory in the presence of distracting information. This paper presents <bold>solid</bold> evidence that working memory representations are dynamic and distinct from sensory representations of intervening distractions. This research will be of interest to cognitive neuroscientists working on the neural bases of visual perception and memory.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.99290.4.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this study, the authors re-analyzed a public dataset (Rademaker et al, 2019, Nature Neuroscience) which includes fMRI and behavioral data recorded while participants held an oriented grating in visual working memory (WM) and performed a delayed recall task at the end of an extended delay period. In that experiment, participants were pre-cued on each trial as to whether there would be a distracting visual stimulus presented during the delay period (filtered noise or randomly-oriented grating). In this manuscript, the authors focused on identifying whether the neural code in retinotopic cortex for remembered orientation was 'stable' over the delay period, such that the format of the code remained the same, or whether the code was dynamic, such that information was present, but encoded in an alternative format. They identify some timepoints - especially towards the beginning/end of the delay - where the multivariate activation pattern fails to generalize to other timepoints, and interpret this as evidence for a dynamic code. Additionally, the authors compare the representational format of remembered orientation in the presence vs absence of a distracting stimulus, averaged over the delay period. This analysis suggested a 'rotation' of the representational subspace between distracting orientations and remembered orientations, which may help preserve simultaneous representations of both remembered and viewed stimuli. Intriguingly, this rotation was a bit smaller for Expt 2, in which the orientation distractor had a greater behavioral impact on the participants' behavioral working memory recall performance, suggesting that more separation between subspaces is critical for preserving intact working memory representations.</p><p>Strengths:</p><p>(1) Direct comparisons of coding subspaces/manifolds between timepoints, task conditions, and experiments is an innovative and useful approach for understanding how neural representations are transformed to support cognition</p><p>(2) Re-use of existing dataset substantially goes beyond the authors' previous findings by comparing geometry of representational spaces between conditions and timepoints, and by looking explicitly for dynamic neural representations</p><p>(3) Simulations testing whether dynamic codes can be explained purely by changes in data SNR are an important contribution, as this rules out a category of explanations for the dynamic coding results observed</p><p>Weaknesses:</p><p>(1) Primary evidence for 'dynamic coding', especially in early visual cortex, appears to be related to the transition between encoding/maintenance and maintenance/recall, but the delay period representations seem overall stable, consistent with some previous findings. However, given the simulation results, the general result that representations may change in their format appears solid, though the contribution of different trial phases remains important for considering the overall result.</p><p>(2) Converting a continuous decoding metric (angular error) to &quot;% decoding accuracy&quot; serves to obfuscate the units of the actual results. Decoding precision (e.g., sd of decoding error histogram) would be more interpretable and better related to both the previous study and behavioral measures of WM performance.</p><p>Comments on revised version:</p><p>The authors have addressed all my previous concerns.</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.99290.4.sa2</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Degutis</surname><given-names>Jonas Karolis</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Bernstein Center for Computational Neuroscience Berlin and Berlin Center for Advanced Neuroimaging, Charité Universitätsmedizin Berlin, corporate member of the Freie Universität Berlin, Humboldt-Universität zu Berlin, and Berlin Institute of Health</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hhn8329</institution-id><institution>Max Planck School of Cognition</institution></institution-wrap><addr-line><named-content content-type="city">Munich</named-content></addr-line><country>Germany</country></aff><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Department of Psychology, Humboldt-Universität zu Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Weber</surname><given-names>Simon</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Bernstein Center for Computational Neuroscience Berlin and Berlin Center for Advanced Neuroimaging, Charité Universitätsmedizin Berlin, corporate member of the Freie Universität Berlin, Humboldt-Universität zu Berlin, and Berlin Institute of Health</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Research Training Group “Extrospection” and Berlin School of Mind and Brain, Humboldt-Universität zu Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Soch</surname><given-names>Joram</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Bernstein Center for Computational Neuroscience Berlin and Berlin Center for Advanced Neuroimaging, Charité Universitätsmedizin Berlin, corporate member of the Freie Universität Berlin, Humboldt-Universität zu Berlin, and Berlin Institute of Health</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>Institute of Psychology, Otto von Guericke University</institution></institution-wrap><addr-line><named-content content-type="city">Magdeburg</named-content></addr-line><country>Germany</country></aff><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Leipzig</named-content></addr-line><country>Germany</country></aff><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/043j0f473</institution-id><institution>German Center for Neurodegenerative Diseases</institution></institution-wrap><addr-line><named-content content-type="city">Bonn</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Haynes</surname><given-names>John-Dylan</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Bernstein Center for Computational Neuroscience Berlin and Berlin Center for Advanced Neuroimaging, Charité Universitätsmedizin Berlin, corporate member of the Freie Universität Berlin, Humboldt-Universität zu Berlin, and Berlin Institute of Health</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hhn8329</institution-id><institution>Max Planck School of Cognition</institution></institution-wrap><addr-line><named-content content-type="city">Munich</named-content></addr-line><country>Germany</country></aff><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Department of Psychology, Humboldt-Universität zu Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Research Training Group “Extrospection” and Berlin School of Mind and Brain, Humboldt-Universität zu Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v4gjf40</institution-id><institution>Research Cluster of Excellence “Science of Intelligence”, Technische Universität Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/042aqky30</institution-id><institution>Collaborative Research Center “Volition and Cognitive Control”, Technische Universität Dresden</institution></institution-wrap><addr-line><named-content content-type="city">Dresden</named-content></addr-line><country>Germany</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>(1) At several places in the reply to reviewers and the manuscript, when discussing the new simulations conducted, the authors mention they break the 180 trials into a train/test split of 108/108 - is this value correct? If so, how? (pg 19 of updated manuscript)</p></disp-quote><p>Thank you for pointing this out; it was not clearly explained. We have now added the explanation to the Methods section:</p><p>“For each iteration, we randomly selected 108 responses from the full set of 180 for training, and then independently sampled another 108 from the same full set for testing. This ensured that the same orientation could appear in both sets, consistent with the structure of the original experiment.”</p><disp-quote content-type="editor-comment"><p>(2) I appreciate the authors have added the variance explained of principal components to the axes of Fig. 3, though it took me a while to notice this, and this isn't described in the figure caption at all. It would likely help readers to directly explain what the % means on each axis of Fig. 3.</p></disp-quote><p>Thank you, we have now added a description in both Fig. 2 and 3:</p><p>“The axes represent the first two principal components, with labels indicating the percent of total explained variance.”</p><disp-quote content-type="editor-comment"><p>(3) I believe there is a typo/missing word in the new paragraph on pg 15: &quot;neural visual WM representations in the early visual cortices are [[biased]] towards distractors&quot; (I think the bracketed word may be omitted as a typo)</p></disp-quote><p>Thank you - fixed.</p></body></sub-article></article>