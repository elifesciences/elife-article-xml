<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">80935</article-id><article-id pub-id-type="doi">10.7554/eLife.80935</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Cortical activity during naturalistic music listening reflects short-range predictions based on long-term experience</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-285028"><name><surname>Kern</surname><given-names>Pius</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4796-1864</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-271502"><name><surname>Heilbron</surname><given-names>Micha</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-28130"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6730-1452</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-101312"><name><surname>Spaak</surname><given-names>Eelke</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2018-3364</contrib-id><email>eelke.spaak@donders.ru.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University Nijmegen, Donders Institute for Brain, Cognition and Behaviour</institution></institution-wrap><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Obleser</surname><given-names>Jonas</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00t3r8h32</institution-id><institution>University of Lübeck</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Büchel</surname><given-names>Christian</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zgy1s35</institution-id><institution>University Medical Center Hamburg-Eppendorf</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>23</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e80935</elocation-id><history><date date-type="received" iso-8601-date="2022-06-09"><day>09</day><month>06</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-12-22"><day>22</day><month>12</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-06-10"><day>10</day><month>06</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.06.08.495241"/></event></pub-history><permissions><copyright-statement>© 2022, Kern et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Kern et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-80935-v2.pdf"/><abstract><p>Expectations shape our experience of music. However, the internal model upon which listeners form melodic expectations is still debated. Do expectations stem from Gestalt-like principles or statistical learning? If the latter, does long-term experience play an important role, or are short-term regularities sufficient? And finally, what length of context informs contextual expectations? To answer these questions, we presented human listeners with diverse naturalistic compositions from Western classical music, while recording neural activity using MEG. We quantified note-level melodic surprise and uncertainty using various computational models of music, including a state-of-the-art transformer neural network. A time-resolved regression analysis revealed that neural activity over fronto-temporal sensors tracked melodic surprise particularly around 200ms and 300–500ms after note onset. This neural surprise response was dissociated from sensory-acoustic and adaptation effects. Neural surprise was best predicted by computational models that incorporated long-term statistical learning—rather than by simple, Gestalt-like principles. Yet, intriguingly, the surprise reflected primarily short-range musical contexts of less than ten notes. We present a full replication of our novel MEG results in an openly available EEG dataset. Together, these results elucidate the internal model that shapes melodic predictions during naturalistic music listening.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>naturalistic music listening</kwd><kwd>magnetoencephalography</kwd><kwd>predictive auditory processing</kwd><kwd>deep neural network</kwd><kwd>transformer</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>016.Veni.198.065</award-id><principal-award-recipient><name><surname>Spaak</surname><given-names>Eelke</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>101000942</award-id><principal-award-recipient><name><surname>de Lange</surname><given-names>Floris P</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>When a person listens to music, their brain continually predicts upcoming notes, based on that person's likely lifelong musical experience.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The second movement of Haydn’s symphony No. 94 begins with a string section creating the expectation of a gentle and soft piece, which is suddenly interrupted by a tutti fortissimo chord. This startling motif earned the composition the nickname ‘Surprise symphony’. All music, in fact, plays with listeners’ expectations to evoke musical enjoyment and emotions, albeit often in more subtle ways (<xref ref-type="bibr" rid="bib32">Huron, 2006</xref>; <xref ref-type="bibr" rid="bib34">Juslin and Västfjäll, 2008</xref>; <xref ref-type="bibr" rid="bib48">Meyer, 1957</xref>; <xref ref-type="bibr" rid="bib79">Salimpoor et al., 2015</xref>). A central element of music which induces musical expectations is melody, the linear sequence of notes alternating in pitch. Within a musical piece and style, such as Western classical music, certain melodic patterns appear more frequently than others, establishing a musical syntax (<xref ref-type="bibr" rid="bib39">Krumhansl, 2015</xref>; <xref ref-type="bibr" rid="bib62">Patel, 2003</xref>; <xref ref-type="bibr" rid="bib75">Rohrmeier et al., 2011</xref>). Human listeners have been proposed to continuously form predictions on how the melody will continue based on these regularities (<xref ref-type="bibr" rid="bib37">Koelsch et al., 2019</xref>; <xref ref-type="bibr" rid="bib48">Meyer, 1957</xref>; <xref ref-type="bibr" rid="bib93">Tillmann et al., 2014</xref>; <xref ref-type="bibr" rid="bib98">Vuust et al., 2022</xref>).</p><p>In support of prediction-based processing of music, it has been shown that listeners are sensitive to melodic surprise. Behaviorally, higher surprise notes are rated as more unexpected (<xref ref-type="bibr" rid="bib38">Krumhansl and Kessler, 1982</xref>; <xref ref-type="bibr" rid="bib44">Marmel et al., 2008</xref>; <xref ref-type="bibr" rid="bib45">Marmel et al., 2010</xref>; <xref ref-type="bibr" rid="bib65">Pearce et al., 2010</xref>; <xref ref-type="bibr" rid="bib82">Schmuckler, 1989</xref>) and impair performance, for example in dissonance detection tasks (<xref ref-type="bibr" rid="bib65">Pearce et al., 2010</xref>; <xref ref-type="bibr" rid="bib84">Sears et al., 2019</xref>). Listeners continue melodic primes with low-surprise notes in musical cloze tasks (<xref ref-type="bibr" rid="bib11">Carlsen, 1981</xref>; <xref ref-type="bibr" rid="bib52">Morgan et al., 2019</xref>; <xref ref-type="bibr" rid="bib82">Schmuckler, 1989</xref>). Neural activity tracks melodic surprise (<xref ref-type="bibr" rid="bib17">Di Liberto et al., 2020</xref>) and high-surprise notes elicit electrophysiological signatures indicative of surprise processing, in particular the mismatch negativity (<xref ref-type="bibr" rid="bib9">Brattico et al., 2006</xref>; <xref ref-type="bibr" rid="bib47">Mencke et al., 2021</xref>; <xref ref-type="bibr" rid="bib54">Näätänen et al., 2007</xref>; <xref ref-type="bibr" rid="bib71">Quiroga-Martinez et al., 2020</xref>) and P3 component (<xref ref-type="bibr" rid="bib71">Quiroga-Martinez et al., 2020</xref>) (for a review see <xref ref-type="bibr" rid="bib37">Koelsch et al., 2019</xref>), but also the P2 component (<xref ref-type="bibr" rid="bib58">Omigie et al., 2013</xref>), a late negative activity around 400ms (<xref ref-type="bibr" rid="bib50">Miranda and Ullman, 2007</xref>; <xref ref-type="bibr" rid="bib65">Pearce et al., 2010</xref>), and oscillatory activity (<xref ref-type="bibr" rid="bib59">Omigie et al., 2019</xref>; <xref ref-type="bibr" rid="bib65">Pearce et al., 2010</xref>). Despite this extensive body of neural and behavioral evidence on the effects of melodic expectations in music perception, the form and content of the internal model generating these expectations remain unclear. Furthermore, the evidence stems primarily from studying the processing of relatively artificial stimuli, and how these findings extend to a more naturalistic setting is unknown.</p><p>We set out to answer three related open questions regarding the nature of melodic expectations, as reflected in neural activity. First, are expectations best explained by a small set of Gestalt-like principles (<xref ref-type="bibr" rid="bib39">Krumhansl, 2015</xref>; <xref ref-type="bibr" rid="bib55">Narmour, 1990</xref>; <xref ref-type="bibr" rid="bib56">Narmour, 1992</xref>; <xref ref-type="bibr" rid="bib90">Temperley, 2008</xref>; <xref ref-type="bibr" rid="bib91">Temperley, 2014</xref>), or are they better captured by statistical learning (<xref ref-type="bibr" rid="bib63">Pearce, 2005</xref>; <xref ref-type="bibr" rid="bib66">Pearce and Wiggins, 2012</xref>; <xref ref-type="bibr" rid="bib76">Rohrmeier and Koelsch, 2012</xref>)? According to Gestalt-like models, expectations stem from relatively simple rules also found in music theory, for example that intervals between subsequent notes tend to be small. From a statistical learning perspective, in contrast, listeners acquire internal predictive models, capturing potentially similar or different principles, through exposure to music. Overall, statistical learning models have proven slightly better fits for musical data (<xref ref-type="bibr" rid="bib91">Temperley, 2014</xref>) and for human listeners’ expectations assessed behaviorally (<xref ref-type="bibr" rid="bib52">Morgan et al., 2019</xref>; <xref ref-type="bibr" rid="bib64">Pearce and Wiggins, 2006</xref>; <xref ref-type="bibr" rid="bib91">Temperley, 2014</xref>), but the two types of models have rarely been directly compared. Second, if statistical learning drives melodic expectations, does this rely on long-term exposure to music, or might it better reflect the local statistical structure of a given musical piece? Finally, independent of whether melodic expectations are informed by short or long-term <italic>experience</italic>, we ask how much temporal <italic>context</italic> is taken into account by melodic expectations; that is whether these are based on a short- or a longer-range context. On the one hand, the brain might use as much temporal context as possible in order to predict optimally. On the other hand, the range of echoic memory is limited and temporal integration windows are relatively short, especially in sensory areas (<xref ref-type="bibr" rid="bib25">Hasson et al., 2008</xref>; <xref ref-type="bibr" rid="bib30">Honey et al., 2012</xref>; <xref ref-type="bibr" rid="bib29">Himberger et al., 2018</xref>). Therefore, melodic expectations could be based on shorter-range context than would be statistically optimal. To address this question, we derived model-based probabilistic estimates of expectations using the Music Transformer (<xref ref-type="bibr" rid="bib31">Huang et al., 2018</xref>). This is a state-of-the-art neural network model that can take long-range (and variable) context into account much more effectively than the n-gram models previously used to model melodic expectations, since transformer models process blocks of (musical) context as a whole, instead of focusing on (note) sequences of variable, yet limited, length.</p><p>In the current project, we approached this set of questions as follows (<xref ref-type="fig" rid="fig1">Figure 1</xref>). First, we operationalized different sources of melodic expectations by simulating different predictive architectures: the Probabilistic Model of Melody Perception (<xref ref-type="bibr" rid="bib90">Temperley, 2008</xref>; <xref ref-type="bibr" rid="bib91">Temperley, 2014</xref>), which is a Gestalt-like model; the Information Dynamics of Music (IDyOM) model, an n-gram based statistical learning model (<xref ref-type="bibr" rid="bib63">Pearce, 2005</xref>; <xref ref-type="bibr" rid="bib66">Pearce and Wiggins, 2012</xref>); and the aforementioned Music Transformer. We compared the different computational models’ predictive performance on music data to establish them as different hypotheses about the sources of melodic expectations. We then analyzed a newly acquired MEG dataset obtained while participants (n=35) were listening to diverse, naturalistic, musical stimuli using time-resolved regression analysis. This allowed us to disentangle the contributions of different sources of expectations, as well as different lengths of contextual information, to the neural signature of surprise processing that is so central to our experience of music. To preview our results: we found that melodic surprise strongly modulates the evoked response, and that this effect goes beyond basic acoustic features and simple repetition effects, confirming that also in naturalistic music listening, brain responses are shaped by melodic expectations. Critically, we found that neural melodic surprise is best captured by long-term statistical learning; yet, intriguingly, depends primarily on short-range musical context. In particular, we observed a striking dissociation at a context window of about ten notes: models taking longer-range context into account become better at predicting music, but worse at predicting neural activity. Superior temporal cortical sources most strongly contributed to the surprise signature, primarily around 200ms and 300–500ms after note onset. Finally, we present a full replication of our findings in an independent openly available EEG dataset (<xref ref-type="bibr" rid="bib17">Di Liberto et al., 2020</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview of the research paradigm.</title><p>Listeners undergoing EEG (data from <xref ref-type="bibr" rid="bib17">Di Liberto et al., 2020</xref>) or MEG measurement (novel data acquired for the current study) were presented with naturalistic music synthesized from MIDI files. To model melodic expectations, we calculated note-level surprise and uncertainty estimates via three computational models reflecting different internal models of expectations. We estimated the regression evoked response or temporal response function (TRF) for different features using time-resolved linear regression on the M|EEG data, while controlling for low-level acoustic factors.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-fig1-v2.tif"/></fig></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Music analysis</title><p>We quantified the note-level surprise and uncertainty using different computational models of music, which were hypothesized to capture different sources of melodic expectation (see Materials and methods for details). The Probabilistic Model of Melody Perception (Temperley) (<xref ref-type="bibr" rid="bib90">Temperley, 2008</xref>; <xref ref-type="bibr" rid="bib91">Temperley, 2014</xref>) rests on a few principles derived from musicology and thus represents Gestalt-like perception (<xref ref-type="bibr" rid="bib52">Morgan et al., 2019</xref>). The Information Dynamics of Music (IDyOM) model (<xref ref-type="bibr" rid="bib66">Pearce and Wiggins, 2012</xref>) captures expectations from statistical learning, either based on short-term regularities in the current musical piece (IDyOM stm), long-term exposure to music (IDyOM ltm), or a combination of the former two (IDyOM both). The Music Transformer (MT) (<xref ref-type="bibr" rid="bib31">Huang et al., 2018</xref>) is a state-of-the-art neural network model, which also reflects long-term statistical learning but is more sensitive to longer-range structure. In a first step, we aimed to establish the different models as distinct hypotheses about the sources of melodic expectations. We examined how well the models predicted music data and to what extent their predictions improved when the amount of available context increased.</p></sec><sec id="s2-2"><title>IDyOM stm and Music Transformer show superior melodic prediction</title><p>First, we tested how well the different computational models predicted the musical stimuli presented in the MEG study (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Specifically, we quantified the accuracy with which the models predicted upcoming notes, given a certain number of previous notes as context information. While all models performed well above chance level accuracy (1/128=0.8%), the IDyOM stm (median accuracy across compositions: 57.9%), IDyOM both (53.5%), and Music Transformer (54.8%) models performed considerably better than the Temperley (19.3%) and IDyOM ltm (27.3%) models, in terms of median accuracy across compositions (<xref ref-type="fig" rid="fig2">Figure 2A</xref> left). This pattern was confirmed in terms of the models’ note-level surprise, which is a continuous measure of predictive performance. Here lower values indicate a better ability to predict the next note given the context (median surprise across compositions: Temperley = 2.18, IDyOM stm = 1.12, IDyOM ltm = 2.23, IDyOM both = 1.46, MT = 1.15, <xref ref-type="fig" rid="fig2">Figure 2A</xref> middle). The median surprise is closely related to the cross-entropy loss, which can be defined as the mean surprise across all notes (Temperley = 2.7, IDyOM stm = 2, ltm = 2.47, both = 1.86, Music Transformer = 1.81). Furthermore, the uncertainty, defined as the entropy of the probability distribution at each time point, characterizes each model’s confidence (inverse) in its predictions (maximum uncertainty = 4.85 given a uniform probability distribution). The Music Transformer model formed predictions more confidently than the other models, whereas the Temperley model displayed the highest uncertainty (median uncertainty across compositions: Temperley = 2.65, IDyOM stm = 2.23, ltm = 2.49, both = 2.28, MT = 1.69, <xref ref-type="fig" rid="fig2">Figure 2A</xref> right). Within the IDyOM class, the stm model consistently showed lower uncertainty compared to the ltm model, presumably reflecting a greater consistency of melodic patterns within versus across compositions. As a result, the both model was driven by the stm model, since it combines the ltm and stm components weighted by their uncertainty (mean stm weight = 0.72, mean ltm weight = 0.18).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Model performance on the musical stimuli used in the MEG study.</title><p>(<bold>A</bold>) Comparison of music model performance in predicting upcoming note pitch, as composition-level accuracy (left; higher is better), median surprise across notes (middle; lower is better), and median uncertainty across notes (right). Context length for each model is the best performing one across the range shown in (<bold>B</bold>). Vertical bars: single compositions, circle: median, thick line: quartiles, thin line: quartiles ±1.5 × interquartile range. (<bold>B</bold>) Accuracy of note pitch predictions (median across 19 compositions) as a function of context length and model class (same color code as (<bold>A</bold>)). Dots represent maximum for each model class. (<bold>C</bold>) Correlations between the surprise estimates from the best models. (For similar results for the musical stimuli used in the EEG study, see <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-fig2-v2.tif"/></fig></sec><sec id="s2-3"><title>Music Transformer utilizes long-range musical structure</title><p>Next, we examined to what extent the different models utilize long-range structure in musical compositions or rely on short-range regularities by systematically varying the context length <italic>k</italic> (above we considered each model at its optimal context length, defined by the maximum accuracy). The Music Transformer model proved to be the only model for which the predictive accuracy increased considerably as the context length increased, from about 9.17% (<italic>k</italic>=1) up to 54.82% (<italic>k</italic>=350) (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The IDyOM models’ performance, in contrast, plateaued early at context lengths between three and five notes (optimal <italic>k</italic>: stm: 25, ltm: 4, both: 3), reflecting the well-known sparsity issue of n-gram models (<xref ref-type="bibr" rid="bib33">Jurafsky and Martin, 2000</xref>). Although the Temperley model benefited from additional musical context slightly, the increment was small and the accuracy was lower compared to the other models across all context lengths (5.58% at <italic>k</italic>=1 to 19.25% at <italic>k</italic>=25).</p></sec><sec id="s2-4"><title>Computational models capture distinct sources of musical expectation</title><p>To further evaluate the differences between models, we tested how strongly their surprise estimates were correlated across all notes in the stimulus set (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Since the IDyOM stm model dominated the both model, the two were correlated most strongly (<italic>r</italic>=.87). The lowest correlations occurred between the IDyOM stm on the one hand and the IDyOM ltm (<italic>r</italic>=0.24) and Temperley model (<italic>r</italic>=0.22) on the other hand. Given that all estimates quantified surprise, positive correlations of medium to large size were expected. More importantly, the models appeared to pick up substantial unique variance, in line with the differences in predictive performance explored above.</p><p>Taken together, these results established that the computational models of music capture different sources of melodic expectation. Only the Music Transformer model was able to exploit long-range structure in music to facilitate predictions of note pitch. Yet, short-range regularities in the current musical piece alone enabled accurate melodic predictions already: the IDyOM stm model performed remarkably well, even compared to the much more sophisticated Music Transformer. We confirmed these results on the musical stimuli from the EEG study (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>).</p></sec><sec id="s2-5"><title>M|EEG analysis</title><p>We used a time-resolved linear regression approach (see Materials and methods for details) to analyse listeners’ M|EEG data. By comparing different regression models, we asked (1) whether there is evidence for the neural processing of melodic surprise and uncertainty during naturalistic music listening and (2) which sources of melodic expectations, represented by the different computational models, best capture that. We quantified the performance of each regression model in explaining the MEG data by computing the correlation r between predicted and observed neural data. Importantly, we estimated r using fivefold cross-validation, thereby ruling out any trivial increase in predictive performance due to increases in number of regressors (i.e. free parameters).</p><p>The simplest model, the Onset model, contained a single regressor coding note onsets in binary fashion. Unsurprisingly, this model significantly explained variance in the recorded MEG data (mean r across participants = 0.12, SD = 0.03; one-sample t-test versus zero, t<sub>34</sub>=25.42, p=1.06e-23, d=4.36, <xref ref-type="fig" rid="fig3">Figure 3A</xref> top left), confirming that our regression approach worked properly. The Baseline model included the note onset regressor, and additionally a set of regressors to account for sensory-acoustic features, such as loudness, sound type, pitch class (low/high), as well as note repetitions to account for sensory adaptation (<xref ref-type="bibr" rid="bib3">Auksztulewicz and Friston, 2016</xref>; <xref ref-type="bibr" rid="bib95">Todorovic and de Lange, 2012</xref>). The Baseline model explained additional variance beyond the Onset model (Δr<sub>Baseline-Onset</sub>=0.013, SD = 0.006; paired-sample t-test, t<sub>34</sub>=12.07, p=7.58e-14, d=2.07, <xref ref-type="fig" rid="fig3">Figure 3A</xref> bottom left), showing that differences in acoustic features and repetition further modulated neural activity elicited by notes.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Model performance on MEG data from 35 listeners.</title><p>(<bold>A</bold>) Cross-validated r for the Onset only model (top left). Difference in cross-validated r between the Baseline model including acoustic regressors and the Onset model (bottom left). Difference in cross-validated r between models including surprise estimates from different model classes (color-coded) and the Baseline model (right). Vertical bars: participants; box plot as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. (<bold>B</bold>) Comparison between the best surprise models from each model class as a function of context length. Lines: mean across participants, shaded area: 95% CI. (<bold>C</bold>) Predictive performance of the Music Transformer (MT) on the MEG data (left y-axis, dark, mean across participants) and the music data from the MEG study (right y-axis, light, median across compositions).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-fig3-v2.tif"/></fig></sec><sec id="s2-6"><title>Long-term statistical learning best explains listeners’ melodic surprise</title><p>We next investigated to which degree the surprise estimates from the different computational models of music could explain unique variance in the neural data, over and above that already explained by the Baseline model. All models performed significantly better than the Baseline model, providing evidence for tracking of neural surprise during naturalistic music listening (Temperley: Δr<sub>Surprise-Baseline</sub>=0.002, SD = 0.001, paired-sample t-test, t<sub>34</sub>=8.76 p=2.42e-09, d=1.5; IDyOM stm: Δr<sub>Surprise-Baseline</sub>=0.001, SD = 0.001, t<sub>34</sub>=5.66 p=9.39e-06, d=0.97; IDyOM ltm: Δr<sub>Surprise-Baseline</sub>=0.003, SD = 0.002, t<sub>34</sub>=12.74 p=2.51e-13, d=2.19; IDyOM both: Δr<sub>Surprise-Baseline</sub>=0.002, SD = 0.001, t<sub>34</sub>=8.77, p=2.42e-09, d=1.5; and Music Transformer: Δr<sub>Surprise-Baseline</sub>=0.004, SD = 0.002, t<sub>34</sub>=10.82, p=1.79e-11, d=1.86, corrected for multiple comparisons using the Bonferroni-Holm method) (<xref ref-type="fig" rid="fig3">Figure 3A</xref> right). Importantly, the Music Transformer and IDyOM ltm model significantly outperformed the other models (paired-sample t-test, MT-Temperley: t<sub>34</sub>=7.56, p=5.33e-08, d=1.30; MT-IDyOM stm: t<sub>34</sub>=9.51, p=4.12e-10, d=1.63, MT-IDyOM both: t<sub>34</sub>=8.87, p=2.07e-09, d=1.52), with no statistically significant difference between the two (paired-sample t-test, t<sub>34</sub>=1.634, p=0.225), whereas the IDyOM stm model performed worst. This contrasts with the music analysis, where the IDyOM stm model performed considerably better than the IDyOM ltm model. These observations suggest that listeners’ melodic surprise is better explained by musical enculturation (i.e., exposure to large amounts of music across the lifetime), modeled as statistical learning on a large corpus of music (IDyOM ltm and MT), rather than by statistical regularities within the current musical piece alone (IDyOM stm) or Gestalt-like rules (Temperley).</p></sec><sec id="s2-7"><title>Short-range musical context shapes listeners’ melodic surprise</title><p>We again systematically varied the context length <italic>k</italic> to probe which context length captures listeners’ melodic surprise best (above we again considered each model at its optimal context length, defined by the maximum Δr<sub>Surprise-Baseline</sub> averaged across participants). The Temperley and IDyOM models’ incremental predictive contribution were marginally influenced by context length, with early peaks for the IDyOM stm (<italic>k</italic>=1) and ltm (<italic>k</italic>=2) and later peaks for the both (<italic>k</italic>=75) and Temperley models (<italic>k</italic>=10) (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). The roughly constant level of performance was expected based on the music analysis, since these models mainly relied on short-range context and their estimates of surprise were almost constant. In contrast, we reported above that the Music Transformer model extracts long-range structure in music, with music-predictive performance increasing up to context lengths of 350 notes. Strikingly, however, surprise estimates from the MT predicted MEG data best at a context length of nine notes and decreased for larger context lengths, even below the level of shorter ones (&lt;10) (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><p>Together, these findings suggest that long-term experience of listeners (IDyOM ltm and MT) better captures neural correlates of melodic surprise than short-term statistical regularities (IDyOM stm). Yet, melodic expectations based on statistical learning might not necessarily rest on long-range temporal structure but rather shorter time scales between 5 and 10 notes. These results were replicated on the EEG data (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Model performance on EEG data from 20 listeners.</title><p>All panels as in <xref ref-type="fig" rid="fig3">Figure 3</xref>, but applied to the EEG data and its musical stimuli.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-fig4-v2.tif"/></fig></sec><sec id="s2-8"><title>Spatiotemporal neural characteristics of melodic surprise</title><p>To elucidate the spatiotemporal neural characteristics of naturalistic music listening, we further examined the temporal response functions (TRFs; or ‘regression evoked responses’) from the best model (MEG: MT at <italic>k</italic>=8, <xref ref-type="fig" rid="fig5">Figure 5</xref>; EEG: MT at <italic>k</italic>=7, <xref ref-type="fig" rid="fig6">Figure 6</xref>). Each TRF combines the time-lagged coefficients for one regressor. The resulting time course describes how the feature of interest modulates neural activity over time. Here, we focused on note onset, the repetition of notes, and melodic surprise. The TRFs were roughly constant around zero in the baseline period (−0.2–0 s before note onset) and showed a clear modulation time-locked to note onset (<xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>). This confirmed that the deconvolution of different features and the temporal alignment in the time-resolved regression worked well. Note that the MEG data were transformed to combined planar gradients to yield interpretable topographies (<xref ref-type="bibr" rid="bib5">Bastiaansen and Knösche, 2000</xref>), and therefore did not contain information about the polarity. While we reflect on the sign of modulations in the TRFs below, these judgements were based on inspection of the axial gradiometer MEG results (not shown) and confirmed on the EEG data (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Temporal response functions (TRFs, left column) and spatial topographies at four time periods (right column) for the best model on the MEG data.</title><p>(<bold>A</bold>): Note onset regressor. (<bold>B</bold>): Note repetition regressor. (<bold>C</bold>): Surprise regressor from the Music Transformer with a context length of eight notes. TRF plots: Grey horizontal bars: time points at which at least one channel in the ROI was significant. Lines: mean across participants and channels. Shaded area: 95% CI across participants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-fig5-v2.tif"/></fig><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>All panels as in <xref ref-type="fig" rid="fig5">Figure 5</xref>, but applied to the EEG data and its musical stimuli.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-fig6-v2.tif"/></fig><p>The TRF for the note onset regressor reflects the average neural response evoked by a note. The effect was temporally extended from note onset up to 0.8 s (MEG) and 1 s (EEG) and clustered around bilateral fronto-temporal MEG sensors (MEG: cluster-based permutation test p=0.035, <xref ref-type="fig" rid="fig5">Figure 5A</xref>; EEG: p=5e-04, <xref ref-type="fig" rid="fig6">Figure 6A</xref>). The time course resembled a P1-N1-P2 complex, typically found in ERP studies on auditory processing (<xref ref-type="bibr" rid="bib69">Picton, 2013</xref>; <xref ref-type="bibr" rid="bib70">Pratt, 2011</xref>), with a first positive peak at about 75ms (P1) and a second positive peak at about 200ms (P2). This was followed by a more sustained negative deflection between 300 and 600ms. We inspected the note repetition regressors to account for the repetition suppression effect, as a potential confound of melodic expectations (<xref ref-type="bibr" rid="bib94">Todorovic et al., 2011</xref>; <xref ref-type="bibr" rid="bib95">Todorovic and de Lange, 2012</xref>). We observed a negative deflection at temporal sensors peaking at about 200ms, reflecting lower neural activity for repeated versus non-repeated notes (MEG: p=5e-04, <xref ref-type="fig" rid="fig5">Figure 5B</xref>; EEG: p=0.008, <xref ref-type="fig" rid="fig6">Figure 6B</xref>). This extends the well-known auditory repetition suppression effect (<xref ref-type="bibr" rid="bib23">Grill-Spector et al., 2006</xref>; <xref ref-type="bibr" rid="bib95">Todorovic and de Lange, 2012</xref>) to the setting of naturalistic music listening. Finally, the TRF of the surprise regressor indicates how the level of model-based surprise modulates neural activity over and above simple repetition. A fronto-temporal cluster of MEG sensors exhibited a positive peak at about 200ms and a sustained negative deflection between 300 and 600ms (MEG: p=5e-04, <xref ref-type="fig" rid="fig5">Figure 5C</xref>; EEG: p=0.004, <xref ref-type="fig" rid="fig6">Figure 6C</xref>). The increased activity for more surprising notes is consistent with expectation suppression effects (<xref ref-type="bibr" rid="bib95">Todorovic and de Lange, 2012</xref>). We ruled out that the late negativity effect was an artifact arising from a negative correlation between surprise estimates of subsequent notes, since these temporal autocorrelations were consistently found to be positive. The surprise estimates from the Temperley and IDyOM models yielded similar, although slightly weaker, spatiotemporal patterns in the MEG and EEG data (<xref ref-type="fig" rid="app1fig3">Appendix 1—figures 3</xref> and <xref ref-type="fig" rid="app1fig4">4</xref>), indicating that they all captured melodic surprise given the cross-model correlations.</p></sec><sec id="s2-9"><title>Melodic processing is associated with superior temporal and Heschl’s gyri</title><p>To further shed light on the spatial profile of melody and surprise processing, we estimated the dominant neural sources corresponding to the peak TRF deflection (180–240ms post note onset) using equivalent current dipole (ECD) modeling of the MEG data (with one, two, or three dipoles per hemisphere, selected by comparing adjusted <italic>r<sup>2</sup></italic>). These simple models provided a good fit to the sensor-level TRF maps, indicated by the substantial amount of variance explained (mean adjusted <italic>r<sup>2</sup></italic> across participants = 0.98 / 0.98/0.97 for Onset / Repetition / Surprise regressors, SD = 0.013 / 0.011/0.020). We show the density of fit dipole locations in <xref ref-type="fig" rid="fig7">Figure 7</xref>. The TRF peak deflection for the Onset regressor was best explained by sources in bilateral Heschl’s gyri (<xref ref-type="fig" rid="fig7">Figure 7</xref>, top). The peak deflections for the Repetition and Surprise regressors were best explained by slightly more lateral sources encompassing both bilateral Heschl’s gyri as well as bilateral superior temporal gyri (see <xref ref-type="fig" rid="fig7">Figure 7</xref> for exact MNI coordinates of density peaks).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Source-level results for the MEG TRF data.</title><p>Volumetric density of estimated dipole locations across participants in the time window of interest identified in <xref ref-type="fig" rid="fig5">Figure 5</xref> (180–240ms), projected on the average Montreal Neurological Institute (MNI) template brain. MNI coordinates are given for the density maxima with anatomical labels from the Automated Anatomical Labeling atlas.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-fig7-v2.tif"/></fig></sec><sec id="s2-10"><title>No evidence for neural tracking of melodic uncertainty</title><p>Besides surprise, melodic expectations can be characterized by their note-level uncertainty. Estimates of surprise and uncertainty were positively correlated across different computational models (e.g. MT with a context of eight notes: <italic>r</italic>=0.21) (<xref ref-type="fig" rid="fig8">Figure 8A</xref>). Surprisingly, the addition of uncertainty and its interaction with surprise did not further improve but rather reduce models’ cross-validated predictive performance on listeners’ MEG data compared to surprise alone (MT Surprise: Δr<sub>Surpise-Baseline</sub>=0.004, SD = 0.002;+Uncertainty: Δr<sub>Uncertainty-Baseline</sub>=0.003, SD = 0.002, paired-sample t-test compared to Surprise, t<sub>34</sub>=–9.57, p=1.42e-10, d=–1.64;+Interaction S×U: Δr<sub>SxU-Baseline</sub>=0.002, SD = 0.002, t<sub>34</sub>=–13.81, p=1.66e-14, d=–2.37) (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). This result holds true for other computational models of music and for the EEG data. Therefore, we do not further examine the TRFs here.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Results for melodic uncertainty.</title><p>(<bold>A</bold>) Relationship between and distribution of surprise and uncertainty estimates from the Music Transformer (context length of eight notes). (<bold>B</bold>) Cross-validated predictive performance for the Baseline +surprise model (top), and for models with added uncertainty regressor (middle) and the interaction between surprise and uncertainty (SxU, bottom). Adding uncertainty and/or the interaction between surprise and uncertainty (SxU) did not improve but worsen the predictive performance on the MEG data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-fig8-v2.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In the present study, we investigated the nature of melodic expectations during naturalistic music listening. We used a range of computational models to calculate melodic surprise and uncertainty under different internal models. Through time-resolved regression on human listeners’ M|EEG activity, we gauged which model could most accurately predict neural indices of melodic surprise. In general, melodic surprise enhanced neural responses, particularly around 200ms and between 300 and 500ms after note onset. This was dissociated from sensory-acoustic and repetition suppression effects, supporting expectation-based models of music perception. In a comparison between computational models of musical expectation, melodic surprise estimates that were generated by an internal model that used <italic>long-term</italic> statistical learning best captured neural surprise responses, highlighting extensive experience with music as a key source of melodic expectations. Strikingly, this effect appeared to be driven by <italic>short-range</italic> musical context of up to 10 notes instead of longer range structure. This provides an important window into the nature and content of melodic expectations during naturalistic music listening.</p><p>Expectations are widely considered a hallmark of music listening (<xref ref-type="bibr" rid="bib32">Huron, 2006</xref>; <xref ref-type="bibr" rid="bib37">Koelsch et al., 2019</xref>; <xref ref-type="bibr" rid="bib39">Krumhansl, 2015</xref>; <xref ref-type="bibr" rid="bib48">Meyer, 1957</xref>; <xref ref-type="bibr" rid="bib93">Tillmann et al., 2014</xref>; <xref ref-type="bibr" rid="bib98">Vuust et al., 2022</xref>), which resonates with the predictive coding framework of perception and cognition (<xref ref-type="bibr" rid="bib12">Clark, 2013</xref>; <xref ref-type="bibr" rid="bib16">de Lange et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Friston, 2010</xref>). Here, we tested the role of melodic expectations during naturalistic music listening, for which neural evidence has been scarce. We quantified note-level surprise and uncertainty as markers of melodic expectations and examined their effect on neural music processing using time-resolved regression. Importantly, our analyses focused on disentangling different sources of melodic expectations, as well as elucidating the length of temporal context that the brain is taking into account when predicting which note will follow. This represents a critical innovation over earlier related work (<xref ref-type="bibr" rid="bib17">Di Liberto et al., 2020</xref>), from which conclusions were necessarily limited to establishing <italic>that</italic> the brain predicts something during music listening, whereas we begin to unravel <italic>what</italic> it is that is being predicted. Furthermore, our use of diverse naturalistic musical stimuli and MEG allows for a broader generalization of our conclusions than was previously possible. Of course, the stimuli do not fully reflect the richness of real-world music yet, as for example the MIDI velocity (i.e. loudness) was held constant and only monophonic compositions were presented. Monophony was a technical limitation given the application of the Temperley and IDyOM model. The reported performance of the MusicTransformer, which supports fully polyphonic music, opens new avenues for future work studying the neural basis of music processing in settings even closer to fully naturalistic.</p><p>A key signature of predictive auditory processing is the neural response to unexpected events, also called the prediction error response (<xref ref-type="bibr" rid="bib12">Clark, 2013</xref>; <xref ref-type="bibr" rid="bib19">Friston, 2010</xref>; <xref ref-type="bibr" rid="bib27">Heilbron and Chait, 2018</xref>). The degree to which notes violate melodic expectations can be quantified as the melodic surprise. Across different computational models of music, we found that melodic surprise explained M|EEG data from human listeners beyond sensory-acoustic factors and beyond simple repetition effects. We thereby generalize previous behavioral and neural evidence for listeners’ sensitivity to unexpected notes to a naturalistic setting (for reviews see <xref ref-type="bibr" rid="bib37">Koelsch et al., 2019</xref>; <xref ref-type="bibr" rid="bib76">Rohrmeier and Koelsch, 2012</xref>; <xref ref-type="bibr" rid="bib93">Tillmann et al., 2014</xref>; <xref ref-type="bibr" rid="bib101">Zatorre and Salimpoor, 2013</xref>).</p><p>While the role of expectations in music processing is well established, there is an ongoing debate about the <italic>nature</italic> of these musical expectations (<xref ref-type="bibr" rid="bib7">Bigand et al., 2014</xref>; <xref ref-type="bibr" rid="bib13">Collins et al., 2014</xref>; <xref ref-type="bibr" rid="bib76">Rohrmeier and Koelsch, 2012</xref>). It has been claimed that these stem from a small set of general, Gestalt-like, principles (<xref ref-type="bibr" rid="bib39">Krumhansl, 2015</xref>; <xref ref-type="bibr" rid="bib90">Temperley, 2008</xref>; <xref ref-type="bibr" rid="bib91">Temperley, 2014</xref>). Alternatively, they may reflect the outcome of a statistical learning process (<xref ref-type="bibr" rid="bib63">Pearce, 2005</xref>; <xref ref-type="bibr" rid="bib66">Pearce and Wiggins, 2012</xref>; <xref ref-type="bibr" rid="bib76">Rohrmeier and Koelsch, 2012</xref>), which, in turn, could reflect either short- or long-range regularities. For the first time, we present neural evidence that weighs in on these questions. We simulated note-level expectations from different predictive architectures of music, which reflected distinct sources of melodic expectations: Gestalt-like principles (Temperley model), short-term statistical learning during the present composition (IDyOM stm) or statistical learning through long-term exposure to music (IDyOM ltm, Music Transformer).</p><p>As a first core result, we found that long-term statistical learning (Music Transformer and IDyOM ltm) captured neural surprise processing better than short-term regularities or Gestalt principles. Our results thus stress the role of long-term exposure to music as a central source of neural melodic expectations. The human auditory system exhibits a remarkable sensitivity to detect and learn statistical regularities in sound (<xref ref-type="bibr" rid="bib78">Saffran et al., 1999</xref>; <xref ref-type="bibr" rid="bib86">Skerritt-Davis and Elhilali, 2018</xref>). This capacity has been corroborated in statistical learning paradigms using behavioral (<xref ref-type="bibr" rid="bib4">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="bib6">Bianco et al., 2020</xref>), eye-tracking (<xref ref-type="bibr" rid="bib49">Milne et al., 2021</xref>; <xref ref-type="bibr" rid="bib102">Zhao et al., 2019</xref>), and neuroimaging techniques (<xref ref-type="bibr" rid="bib4">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="bib51">Moldwin et al., 2017</xref>; <xref ref-type="bibr" rid="bib68">Pesnot Lerousseau and Schön, 2021</xref>). Furthermore, humans have extraordinary implicit memory for auditory patterns (<xref ref-type="bibr" rid="bib1">Agres et al., 2018</xref>; <xref ref-type="bibr" rid="bib6">Bianco et al., 2020</xref>). It has therefore been proposed that listeners learn the statistical regularities embedded in music through mere exposure (<xref ref-type="bibr" rid="bib67">Pearce, 2018</xref>; <xref ref-type="bibr" rid="bib75">Rohrmeier et al., 2011</xref>; <xref ref-type="bibr" rid="bib77">Rohrmeier and Rebuschat, 2012</xref>).</p><p>Short-term regularities and Gestalt principles also significantly predicted neural variance and might constitute concurrent, though weaker, sources of melodic expectations (<xref ref-type="bibr" rid="bib76">Rohrmeier and Koelsch, 2012</xref>). Gestalt principles, specifically, have been shown to adequately model listeners’ melodic expectations in behavioral studies (<xref ref-type="bibr" rid="bib15">Cuddy and Lunney, 1995</xref>; <xref ref-type="bibr" rid="bib52">Morgan et al., 2019</xref>; <xref ref-type="bibr" rid="bib64">Pearce and Wiggins, 2006</xref>; <xref ref-type="bibr" rid="bib91">Temperley, 2014</xref>). One shortcoming of Gestalt-like models, however, is that they leave unresolved how Gestalt rules emerge, assuming either innate principles (<xref ref-type="bibr" rid="bib55">Narmour, 1990</xref>) or being agnostic to this question (<xref ref-type="bibr" rid="bib90">Temperley, 2008</xref>). We propose that the well-established statistical learning framework can account for Gestalt-like principles. If the latter, for example pitch proximity, indeed fit a certain musical style, they have to be reflected in the statistical regularities. Music theoretical research has indeed shown that statistical learning based on bigrams can recover music theoretical Gestalt principles (<xref ref-type="bibr" rid="bib74">Rodriguez Zivic et al., 2013</xref>), even across different (musical) cultures (<xref ref-type="bibr" rid="bib80">Savage et al., 2015</xref>). This further backs up the role of statistical learning for musical expectations.</p><p>As a second core result, strikingly, we found that neural activity was best explained by those surprise estimates taking into account only relatively short-range musical context. Even though extracting the patterns upon which expectations are based requires long-term exposure (previous paragraph), the relevant context length of these patterns for predicting upcoming notes turned out to be short, around 7–8 notes. In contrast, for modeling music itself (i.e. independently of neural activity), the music transformer performed monotonically better with increasing context length, up to hundreds of notes. This pattern of results is very unlike similar studies in language processing, where models that perform best at next word prediction and can take the most context into account (i.e. transformers) also perform best at predicting behavioral and brain responses, and predictions demonstrably take long-term context into account (<xref ref-type="bibr" rid="bib21">Goodkind and Bicknell, 2018</xref>; <xref ref-type="bibr" rid="bib28">Heilbron et al., 2021</xref>; <xref ref-type="bibr" rid="bib81">Schmitt et al., 2021</xref>; <xref ref-type="bibr" rid="bib83">Schrimpf et al., 2021</xref>; <xref ref-type="bibr" rid="bib100">Wilcox et al., 2020</xref>). A cautious hypothesis is that musical motifs, groups of about 2–10 notes, are highly generalizable within a musical style compared to longer range structure (<xref ref-type="bibr" rid="bib39">Krumhansl, 2015</xref>). Motifs might thus drive statistical learning and melodic predictions, while other temporal scales contribute concurrently (<xref ref-type="bibr" rid="bib42">Maheu et al., 2019</xref>). However, several alternative explanations are possible, between which we cannot adjudicate, based on our data. First, the length of ten notes roughly corresponds to the limit of auditory short-term memory at about 2–4 s (<xref ref-type="bibr" rid="bib92">Thaut, 2014</xref>), which might constrain predictive sequence processing. Second, our analysis is only sensitive to time-locked note-level responses and those signals measured by M|EEG, whereas long-range musical structure might have different effects on neural processing (<xref ref-type="bibr" rid="bib39">Krumhansl, 2015</xref>; <xref ref-type="bibr" rid="bib76">Rohrmeier and Koelsch, 2012</xref>), in particular slower effects that are less precisely linked to note onsets. A third and final caveat is that the modeling of long-range structure by the music transformer model might be different from how human listeners process temporally extended or hierarchical structure.</p><p>Our approach of using temporal response function (TRF, or ‘regression evoked response’, rERP) analysis allowed us to investigate the spatiotemporal characteristics of continuously unfolding neural surprise processing. Melodic surprise modulated neural activity evoked by notes over fronto-temporal sensors with a positive peak at about 200ms, corresponding to a modulation of the P2 component (<xref ref-type="bibr" rid="bib69">Picton, 2013</xref>; <xref ref-type="bibr" rid="bib70">Pratt, 2011</xref>). Source modeling suggests superior temporal and Heschl’s gyri as likely sources of this neural response (although we note that MEG’s spatial resolution is limited and the exact localization of surprise responses within auditory cortex requires further research). Surprising notes elicited stronger neural responses, in line with previous reports by <xref ref-type="bibr" rid="bib17">Di Liberto et al., 2020</xref>. This finding is furthermore consistent with the more general effect of expectation suppression, the phenomenon that expected stimuli evoke weaker neural responses (<xref ref-type="bibr" rid="bib3">Auksztulewicz and Friston, 2016</xref>; <xref ref-type="bibr" rid="bib20">Garrido et al., 2009</xref>; <xref ref-type="bibr" rid="bib95">Todorovic and de Lange, 2012</xref>; <xref ref-type="bibr" rid="bib99">Wacongne et al., 2011</xref>) through gain modulation (<xref ref-type="bibr" rid="bib72">Quiroga-Martinez et al., 2021</xref>). In line with predictive coding, the brain might hence be predicting upcoming notes in order to explain away predicted sensory input, thereby leading to enhanced responses to surprising (i.e., not yet fully explainable) input.</p><p>Additionally, we found a sustained late negativity correlating with melodic surprise, which some studies have labeled a musical N400 or N500 (<xref ref-type="bibr" rid="bib10">Calma-Roddin and Drury, 2020</xref>; <xref ref-type="bibr" rid="bib36">Koelsch et al., 2000</xref>; <xref ref-type="bibr" rid="bib50">Miranda and Ullman, 2007</xref>; <xref ref-type="bibr" rid="bib61">Painter and Koelsch, 2011</xref>; <xref ref-type="bibr" rid="bib65">Pearce et al., 2010</xref>). Similar to its linguistic counterpart (<xref ref-type="bibr" rid="bib40">Kutas and Federmeier, 2011</xref>), the N400 has been interpreted as an index of predictive music processing. The literature has furthermore frequently emphasised the mismatch negativity (MMN) (<xref ref-type="bibr" rid="bib54">Näätänen et al., 2007</xref>) and P3 component in predictive music processing (<xref ref-type="bibr" rid="bib37">Koelsch et al., 2019</xref>), neither of which we observe for melodic surprise here. However, the MMN is typically found for deviants occurring in a stream of standard tones, such as in oddball paradigms, while the P3 is usually observed in the context of an explicit behavioral task (<xref ref-type="bibr" rid="bib37">Koelsch et al., 2019</xref>). In our study, listeners were listening passively to maximize the naturalistic setting, which could account for the absence of these components. Importantly, our results go beyond previous research by analysing the influence of melodic surprise in a continuous fashion, instead of focusing on deviants.</p><p>As a final novel contribution, we demonstrate the usefulness of a state-of-the-art deep learning model, the Music Transformer (MT) (<xref ref-type="bibr" rid="bib31">Huang et al., 2018</xref>), for the study of music cognition. The network predicted music and neural data at least on par with the IDyOM model, an n-gram model which is currently a highly popular model of musical expectations (<xref ref-type="bibr" rid="bib66">Pearce and Wiggins, 2012</xref>). We are likely severely underestimating the relative predictive power of the MT, since we constrained our stimuli to monophonic music in the present study. Monophonic music is the only type of music the other models (IDyOM, Temperley) are able to process, so this restriction was a technical necessity. The MT, in contrast, supports fully polyphonic music. This opens up new avenues for future work to study neural music processing in even more naturalistic settings.</p><p>To conclude, by using computational models to capture different hypotheses about the nature and source of melodic expectations and linking these to neural data recorded during naturalistic listening, we found that these expectations have their origin in long-term exposure to the statistical structure of music. Yet, strikingly, as listeners continuously exploit this long-term knowledge during listening, they do so primarily on the basis of short-range context. Our findings thereby elucidate the individual voices making up the ‘surprise symphony’ of music perception.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Data and code availability</title><p>The (anonymized, de-identified) MEG and music data are available from the Donders Repository (<ext-link ext-link-type="uri" xlink:href="https://data.donders.ru.nl/">https://data.donders.ru.nl/</ext-link>) under CC-BY-4.0 license. The persistent identifier for the data is <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.34973/5qxw-nn97">https://doi.org/10.34973/5qxw-nn97</ext-link>. The experiment and analysis code is also available from the Donders Repository.</p></sec><sec id="s4-2"><title>Participants</title><p>We recruited 35 healthy participants (19 female; 32 right-handed; age: 18–30 years, mean = 23.8, SD = 3.05) via the research participation system at Radboud University. The sample size was chosen to achieve a power of ≥80% for detecting a medium effect size (d=0.5) with a two-sided paired t-test at an α level of 0.05. All participants reported normal hearing. The study was approved under the general ethical approval for the Donders Centre for Cognitive Neuroimaging (Imaging Human Cognition, CMO2014/288) by the local ethics committee (CMO Arnhem-Nijmegen, Radboud University Medical Centre). Participants provided written informed consent before the experiment and received monetary compensation.</p></sec><sec id="s4-3"><title>Procedure</title><p>Participants listened to music, while their neural activity was recorded using magnetoencephalography (MEG) (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Participants started each musical stimulus with a button press and could take short breaks in between stimuli. Participants were instructed to fixate a dot displayed at the centre of a screen (~85 cm viewing distance) in order to reduce head and eye movements. Besides that, participants were only asked to listen attentively to the music and remain still. These minimal instructions were intended to maximize the naturalistic character of the study. Initially, three test runs (~10 s each) were completed, in which three short audio snippets from different compositions (not used in the main experiment) were presented. This was intended to make listeners familiar with the procedure and the different sounds, as well as to adjust the volume to a comfortable level.</p></sec><sec id="s4-4"><title>Musical stimuli</title><p>We selected 19 compositions (duration: total = 43 min, median across stimuli = 134 s, median absolute deviation (MAD, <xref ref-type="bibr" rid="bib41">Leys et al., 2013</xref>) = 39 s; note events: total = 9824, median = 448, MAD = 204) from Western classical music (see <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>). We chose this genre, since (a) participants recruited from the Nijmegen area were assumed to be somewhat familiar with it, (b) it entails relatively complex melodies and long-term structure allowing us to sample a broad range of surprise and uncertainty estimates, (c) many digital music files and corpora in MIDI format are publicly available, and (d) these included monophonic pieces. Monophonic refers to one note being played at a time, that is only containing a melody, compared to polyphonic music, which further includes chords and/or parallel voices. The constraint to monophonic compositions was necessary to enable the application of the Temperley and IDyOM model, which cannot parse polyphonic music. Based on the available databases, the selection aimed to cover various musical periods (1711–1951), composers, tempi (60–176 bpm), and key signatures, roughly matching the statistics of the training corpus for the music models (see below). The median note duration was about 161ms (MAD across all notes = 35ms, min = 20ms, max = 4498ms), with a median inter-note onset interval of 200ms (MAD across all notes = 50ms, min = 22ms, max = 2550ms).</p><p>We used the Musescore 3 software to synthesize and export the digital MIDI files as wav audio files (sampling rate = 44.1 kHz). This ensured accurate control over the note timing compared to live or studio recordings, facilitating time-locked analyses. The synthesisation via one of three virtual instruments from fluidsynth (piano, oboe, flute) ensured the natural character of the music. The MIDI velocity, corresponding to loudness (termed ‘velocity’ in MIDI terms because it refers to the velocity with which one could strike a piano key), was set to 100 for all notes, since most files were missing velocity information and the volume was thus held roughly constant across notes.</p></sec><sec id="s4-5"><title>Stimulus presentation</title><p>The experiment was run on a Windows computer using Matlab 2018b (The MathWorks) and the Psychophysics Toolbox (<xref ref-type="bibr" rid="bib8">Brainard, 1997</xref>). The music was presented binaurally via ear tubes (Doc’s Promolds NonVent with #13 thick prebent 1.85 mm ID tubes, Audine Healthcare, in combination with Etymotic ER3A earphones) at a sampling rate of 44.1 kHz. The volume was adjusted to a comfortable level for each participant during the initial three test runs. To ensure equivalent acoustic input in both ears, the right audio channel from potentially stereo recordings was duplicated, resulting in mono audio presentation. After participants initiated a run by a button press, the wav file was first loaded into the sound card buffer to ensure accurate timing. Once the file was fully loaded, the visual fixation cross appeared at the centre of the screen and after 1.5–2.5 s (random uniform distribution) the music started. The order of compositions was randomized across participants.</p></sec><sec id="s4-6"><title>MEG data acquisition</title><p>Neural activity was recorded on a 275-channel axial gradiometer MEG system (VSM/CTF Systems) in a magnetically shielded room, while the participant was seated. Eight malfunctioning channels were disabled during the recording or removed during preprocessing, leaving 267 MEG channels in the recorded data. We monitored the head position via three fiducial coils (left and right ear, nasion). When the head movement exceeded 5 mm, in between listening periods, the head position was shown to the participant, and they were instructed to reposition themselves (<xref ref-type="bibr" rid="bib89">Stolk et al., 2013</xref>). All data were low-pass filtered online at 300 Hz and digitized at a sampling rate of 1200 Hz.</p></sec><sec id="s4-7"><title>Further data acquisition</title><p>For source analysis, the head shape and the location of the three fiducial coils were measured using a Polhemus 3D tracking device. T1-weighted anatomical MRI scans were acquired on a 3T MRI system (Siemens) after the MEG session if these were not already available from the local database (MP-RAGE sequence with a GRAPPA acceleration factor of 2, TR = 2.3 s, TE = 3.03ms, voxel size 1 mm isotropic, 192 transversal slices, 8 ° flip angle). Additionally, during the MEG session, eye position, pupil diameter and blinks were recorded using an Eyelink 1000 eye tracker (SR Research) and digitized at a sampling rate of 1200 Hz. After the experiment, participants completed a questionnaire including a validated measure of musicality, the Goldsmith Musical Sophistication Index (<xref ref-type="bibr" rid="bib53">Müllensiefen et al., 2014</xref>). The eye tracking and questionnaire data were not analysed here.</p></sec><sec id="s4-8"><title>EEG dataset</title><p>In addition, we analysed an open data set from a recently published study (<xref ref-type="bibr" rid="bib17">Di Liberto et al., 2020</xref>) including EEG recordings from 20 participants (10 musicians, 10 non-musicians) listening to music. The musical stimuli were 10 violin compositions by J. S. Bach synthesized using a piano sound (duration: total = 27 min, median = 161.5 s, MAD = 18.5 s; note events: total = 7839,, median = 631, MAD = 276.5; see <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>), that were each presented three times in pseudo-randomized order (total listening time = 80 min). The median note duration was 145ms (MAD across all notes = 32ms, min = 70ms, max = 2571ms), with a median inter-note onset interval of 150ms (MAD across all notes = 30ms, min = 74ms, max = 2571ms). EEG was acquired using a 64-electrode BioSemi Active Two system and digitized at a sampling rate of 512 Hz.</p></sec><sec id="s4-9"><title>Music analysis</title><p>We used three types of computational models of music to investigate human listeners’ melodic expectations: the Temperley model (<xref ref-type="bibr" rid="bib90">Temperley, 2008</xref>; <xref ref-type="bibr" rid="bib91">Temperley, 2014</xref>), the IDyOM model (<xref ref-type="bibr" rid="bib66">Pearce and Wiggins, 2012</xref>), and the Music Transformer (<xref ref-type="bibr" rid="bib31">Huang et al., 2018</xref>). Based on their differences in computational architecture, we used these models to operationalize different sources of melodic expectations. All models take as input MIDI data, specifically note pitch values X ranging discretely from 0 to 127 (8.18–12543.85 Hz, middle C=60,~264 Hz). The models output a probability distribution for the next note pitch at time point <italic>t, X<sub>t</sub></italic>, given a musical context of <italic>k</italic> preceding consecutive note pitches:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>0..127</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mn>0.</mml:mn></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For the first note in each composition, we assumed a uniform distribution across pitches (<inline-formula><mml:math id="inf1"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>128</mml:mn></mml:math></inline-formula>). Based on these probability distributions, we computed the surprise <italic>S</italic> of an observed note pitch <italic>x<sub>t</sub></italic> given the musical context as<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Likewise, the uncertainty <italic>U</italic> associated with predicting the next note pitch was defined as the entropy of the probability distribution across all notes in the alphabet:<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>127</mml:mn></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-10"><title>Training corpora</title><p>All models were trained on the Monophonic Corpus of Complete Compositions (MCCC) (<ext-link ext-link-type="uri" xlink:href="https://osf.io/dg7ms/">https://osf.io/dg7ms/</ext-link>), which consists of 623 monophonic pieces (Note events: total = 500,000, median = 654, MAD = 309). The corpus spans multiple musical periods and composers and matches the statistics of the musical stimuli used in the MEG and EEG study regarding the distribution of note pitch and pitch interval (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>) as well as the proportion of major key pieces (MCCC: ~81%, Music<sub>MEG</sub>: ~74%, but Music<sub>EEG</sub>: 20%). Furthermore, the Maestro corpus V3 (<xref ref-type="bibr" rid="bib26">Hawthorne et al., 2019</xref>, <ext-link ext-link-type="uri" xlink:href="https://magenta.tensorflow.org/datasets/maestro">https://magenta.tensorflow.org/datasets/maestro</ext-link>), which comprises 1276 polyphonic compositions collected from human piano performances (Duration: total = 200 h, note events: total = 7 million), was used for the initial training of the Music Transformer (see below).</p></sec><sec id="s4-11"><title>Probabilistic Model of Melody Perception | Temperley</title><p>The Probabilistic Model of Melody Perception (<xref ref-type="bibr" rid="bib90">Temperley, 2008</xref>; <xref ref-type="bibr" rid="bib91">Temperley, 2014</xref>) is a Bayesian model based on three interpretable principles established in musicology. Therefore, it has been coined a Gestalt-model (<xref ref-type="bibr" rid="bib52">Morgan et al., 2019</xref>). The three principles are modeled by probability distributions (discretized for integer pitch values), whose free parameters were estimated, in line with previous literature, based on the MCCC:</p><list list-type="order"><list-item><p>Pitches <italic>x<sub>t</sub></italic> cluster in a narrow range around a central pitch <italic>c</italic> (central pitch tendency):<disp-formula id="equ4"> <mml:math id="m4"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>c</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The parameters <italic>c<sub>0</sub></italic> and <italic>var<sub>c0</sub></italic>: were set to the mean and variance of compositions’ mean pitch in the training corpus (<italic>c<sub>0</sub></italic>=72, <italic>var<sub>c0</sub></italic> = 34.4). The variance of the central pitch profile v<sub>r</sub> was set to the variance of each melody’s first note around its mean (<italic>v<sub>r</sub></italic> = 83.2).</p></list-item><list-item><p>Pitches tend to be close to the previous pitch <italic>x<sub>t−1</sub></italic>, in other words pitch intervals tend to be small (pitch proximity):<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The variance of the pitch proximity profile <italic>v<sub>x</sub></italic> was estimated as the variance of pitches around <italic>x<sub>t−1</sub></italic> considering only notes where <italic>x<sub>t−1</sub></italic>=<italic>c</italic> (<italic>v<sub>x</sub></italic> = 18.2).</p></list-item><list-item><p>Depending on the key, certain pitches occur more frequently given their scale degree (the position of a pitch relative to the tonic of the key). This key profile is modeled as the probability of a scale degree conditioned on the key (12 major and 12 minor keys) spread out across several octaves, weighted by the probability of major and minor keys (p<sub>maj</sub> = .81).</p></list-item></list><p>The final model multiplicatively combines these distributions to give the probability of the next note pitch given the context. The C code was provided by David Temperley in personal communication and adapted to output probabilities for all possible pitch values <italic>X</italic>. Specific choices in principles 1–3 above were made in accordance with earlier work (<xref ref-type="bibr" rid="bib52">Morgan et al., 2019</xref>; <xref ref-type="bibr" rid="bib90">Temperley, 2008</xref>; <xref ref-type="bibr" rid="bib91">Temperley, 2014</xref>).</p></sec><sec id="s4-12"><title>Information Dynamics of Music model | IDyOM</title><p>The Information Dynamics of Music (IDyOM) model is an unsupervised statistical learning model, specifically a variable order Markov model (<xref ref-type="bibr" rid="bib63">Pearce, 2005</xref>; <xref ref-type="bibr" rid="bib66">Pearce and Wiggins, 2012</xref>). Based on n-grams and the alphabet <italic>X</italic>, the probability of a note pitch <italic>x</italic> at time point <italic>t</italic>, <italic>x<sub>t</sub></italic>, given a context sequence of length <italic>k</italic>, <inline-formula><mml:math id="inf2"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> , is defined as the relative n-gram frequency of the continuation compared to the context:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The probabilities are computed for every possible n-gram length up to a bound <italic>k</italic> and combined through interpolated smoothing. The context length was, therefore, manipulated via the n-gram order bound. The model can operate on multiple musical features, called viewpoints. Here, we use pitch (in IDyOM terminology <italic>cpitch</italic>) to predict pitch, in line with the other models.</p><p>The IDyOM model class entails three different subtypes: a short-term model (stm), a long-term model (ltm), and a combination of the former two (both). The IDyOM stm model rests solely on the recent context in the current composition. As such, it approximates online statistical learning of short-term regularities in the present piece. The IDyOM ltm model, on the other hand, is trained on a corpus, reflecting musical enculturation, that is (implicit) statistical learning through long-term exposure to music. The IDyOM both model combines the stm and ltm model weighted by their entropy at each note.</p></sec><sec id="s4-13"><title>Music Transformer</title><p>The Music Transformer (MT) (<xref ref-type="bibr" rid="bib31">Huang et al., 2018</xref>) is a state-of-the-art neural network model that was developed to generate music with improved long-range coherence. To this end, it takes advantage of a Transformer architecture (<xref ref-type="bibr" rid="bib97">Vaswani et al., 2017</xref>) and relative self-attention (<xref ref-type="bibr" rid="bib85">Shaw et al., 2018</xref>), which better capture long-range structure in sequences than for example n-gram models. The MT is the only model used here that can process polyphonic music. This is possible due to a representation scheme that comprises four event types (note onset, note offset, velocity, and time-shift events) for encoding and decoding MIDI data. The note onset values are equivalent to pitch values and were used to derive probability distributions. Our custom scripts were based on an open adaptation for PyTorch (<ext-link ext-link-type="uri" xlink:href="https://github.com/gwinndr/MusicTransformer-Pytorch">https://github.com/gwinndr/MusicTransformer-Pytorch</ext-link>; <xref ref-type="bibr" rid="bib24">Gwinn et al., 2022</xref>).</p><p>The Music Transformer was initially trained on the polyphonic Maestro corpus for 300 epochs using the training parameters from the original paper (learning rate = 0.1, batch size = 2, number of layers = 6, number of attention heads = 6, dropout rate = 0.1, <xref ref-type="bibr" rid="bib31">Huang et al., 2018</xref>). The training progress was monitored based on the cross-entropy loss on the training data (80%) and test data (20%) (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). The cross-entropy loss is defined as the average surprise across all notes. The model is, thus, trained to minimize the surprise for upcoming notes. The minimal loss we achieved (1.97) was comparable to the original paper (1.835). The divergence between the loss curve for training and test set indicated some overfitting starting from about epoch 50, however, without a noticeable decrease in test performance. Therefore, we selected the weights at epoch 150 to ensure stable weights without severe overfitting.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Training (<bold>A</bold>) and fine-tuning (<bold>B</bold>) of the Music Transformer on the Maestro corpus and MCCC, respectively.</title><p>Cross-entropy loss (average surprise across all notes) on the test (dark) and training (light) data as a function of training epoch.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-fig9-v2.tif"/></fig><p>In order to adjust the model to monophonic music, we finetuned the pretrained Music Transformer on the MCCC for 100 epochs using the same training parameters (<xref ref-type="fig" rid="fig9">Figure 9B</xref>). Again, the training progress was evaluated based on the cross-entropy loss and the weights were selected based on the minimal loss. While the loss started at a considerably lower level on this monophonic dataset (0.78), it continued to decrease until epoch 21 (0.59), but quickly started to increase, indicating overfitting on the training data. Therefore, the weights from epoch 21 were selected for further analyses.</p></sec><sec id="s4-14"><title>Music model comparison</title><p>We compared the models’ predictive performance on music data as a function of model class and context length. Thereby, we aimed to scrutinize the hypothesis that the models reflect different sources of melodic expectations. We used the musical stimuli from the MEG and EEG study as test sets and assessed the accuracy, median surprise and uncertainty across compositions.</p></sec><sec id="s4-15"><title>M|EEG analysis</title><sec id="s4-15-1"><title>Preprocessing</title><p>The MEG data were preprocessed in Matlab 2018b using FieldTrip (<xref ref-type="bibr" rid="bib60">Oostenveld et al., 2011</xref>). We loaded the raw data separately for each composition including about 3 s pre- and post-stimulus periods. Based on the reference sensors of the CTF MEG system, we denoised the recorded MEG data using third-order gradient correction, after which the per-channel mean across time was subtracted. We then segmented the continuous data in 1 s segments. Using the semi-automatic routines in FieldTrip, we marked noisy segments according to outlying variance, such as MEG SQUID jumps, eye blinks or eye movements (based on the unfiltered data) or muscle artifacts (based on the data filtered between 110 and 130 Hz). After removal of noisy segments, the data were downsampled to 400 Hz. Independent component analysis (ICA) was then performed on the combined data from all compositions for each participant to identify components that reflected artifacts from cardiac activity, residual eye movements or blinks. Finally, we reloaded the data without segmentation, removed bad ICA components and downsampled the data to 60 Hz for subsequent analyses.</p><p>A similar preprocessing pipeline was used for the EEG data. Here, the data were re-referenced using the linked mastoids. Bad channels were identified via visual inspection and replaced through interpolation after removal of bad ICA components.</p></sec><sec id="s4-15-2"><title>TRF analysis</title><p>We performed time-resolved linear regression on the M|EEG data to investigate the neural signatures of melodic surprise and uncertainty (<xref ref-type="fig" rid="fig1">Figure 1</xref>), using the regression evoked response technique (‘rERP’, <xref ref-type="bibr" rid="bib88">Smith and Kutas, 2015</xref>).This approach allowed us to deconvolve the responses to different features and subsequent notes and correct for their temporal overlap. The preprocessed M|EEG data were loaded and band-pass filtered between 0.5 and 8 Hz (bidirectional FIR filter). All features of interest were modeled as impulse regressors with one value per note, either binary (x = {0,1}) or continuous (<inline-formula><mml:math id="inf3"><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:math></inline-formula>). The M|EEG channel data and continuous regressors were z-scored. We constructed a time-expanded regression matrix <italic>M</italic>, which contained time-shifted versions of each regressor column-wise (<italic>t<sub>min</sub></italic> = –0.2 s, <italic>t<sub>max</sub></italic> = 1 s relative to note onsets, 73 columns per regressor given the sampling rate of 60 Hz). After removal of bad time points identified during M|EEG preprocessing, we estimated the regression weights <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> using ordinary least squares (OLS) regression:<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Collectively, the weights form a response function known as the regression evoked response or temporal response function (TRF; <xref ref-type="bibr" rid="bib14">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="bib18">Ding and Simon, 2012</xref>). The TRF depicts how a feature modulates neural activity across time. Here, the units are arbitrary, since both binary and z-scored continuous regressors were included. Model estimation was performed using custom Python code built on the MNE rERP implementation (<xref ref-type="bibr" rid="bib22">Gramfort et al., 2013</xref>; <xref ref-type="bibr" rid="bib88">Smith and Kutas, 2015</xref>). Previous similar work has used ridge-regularized regression, rather than OLS (<xref ref-type="bibr" rid="bib17">Di Liberto et al., 2020</xref>). We instead opted to use OLS, since the risk for overfitting was low given the sparse design matrices and low correlations between the time-shifted regressors. To make sure this did not unduly influence our results, we also implemented ridge-regularized regression with the optimal cost hyperparameter alpha estimated via nested cross-validation. OLS (alpha = 0) was always among the best-fitting models and any increase in predictive performance for alpha &gt;0 for some participants was negligible. Results for this control analysis are shown for the best fitting model for the MEG and EEG data in <xref ref-type="fig" rid="app1fig5">Appendix 1—figures 5</xref> and <xref ref-type="fig" rid="app1fig6">6</xref>, respectively. In the rest of the manuscript we thus report the results from the OLS regression.</p></sec><sec id="s4-15-3"><title>Models and regressors</title><p>The <bold><italic>Onset model</italic></bold> contained a binary regressor, which coded for note onsets and was included in all other models too. The <bold><italic>Baseline model</italic></bold> added a set of regressors to control for acoustic properties of the music and other potential confounds. Binary regressors were added to code for (1) very high pitch notes (&gt;90% quantile), (2) very low pitch notes (&lt;10% quantile), since extreme pitch values go along with differences in perceived loudness, timbre, and other acoustic features; (3) the first note in each composition (i.e. composition onset); (4) repeated notes, to account for the repetition suppression effect and separate it from the surprise response. Since the MEG experiment used stimuli generated by different musical instruments, we additionally controlled for the type of sound, by including binary regressors for oboe and flute sounds. This was done since the different sounds have different acoustic properties, such as a lower attack time for piano sounds and longer sustain for oboe or flute sounds. For computing continuous acoustic regressors, we downsampled the audio signal to 22.05 kHz. We computed the mean for each variable of interest across the note duration to derive a single value for each note and create impulse regressors. The root-mean-square value (RMS) of the audio signal captures differences in (perceived) loudness. Flatness, defined as the ratio between the geometric and the arithmetic mean of the acoustic signal, controlled for differences in timbre. The variance of the broad-band envelope represented acoustic edges (<xref ref-type="bibr" rid="bib46">McDermott and Simoncelli, 2011</xref>). The broad-band envelope was derived by (a) filtering the downsampled audio signal through a gammatone filter bank (64 logarithmically spaced filter bands ranging between 50 and 8000 Hz), which simulates human auditory processing; (b) taking the absolute value of the Hilbert transform of the 64 band signals; (c) averaging across bands (<xref ref-type="bibr" rid="bib103">Zuk et al., 2021</xref>). The baseline regressors were also included in all of the following models. The <bold><italic>main models</italic></bold> of interest added note-level surprise, uncertainty, and/or their interaction from the different computational models of music, varying the model class and context length.</p></sec><sec id="s4-15-4"><title>Model comparison</title><p>We applied a fivefold cross-validation scheme (train: 80%, test: 20%, time window: 0–0.6 s) (<xref ref-type="bibr" rid="bib96">Varoquaux et al., 2017</xref>) to compare the regression models’ predictive performance on the M|EEG data. We computed the correlation between the predicted and recorded neural signal across time for each fold and channel on the hold out data. To increase the sensitivity of subsequent analyses, we selected the channels most responsive to musical notes for each participant according to the cross-validated performance for the Onset model (&gt;2/3 quantile). The threshold was determined through visual inspection of the spatial topographies, but did not affect the main results. The overall model performance was then determined as the median across folds and the mean across selected channels. Since the predictive performance was assessed on unseen hold out data, the approach controlled for overfitting the neural data and for differences in the number of regressors and free model parameters. For statistical inference, we computed one-sample or paired t-tests using multiple comparison correction (Bonferroni-Holm method).</p></sec><sec id="s4-15-5"><title>Cluster-based statistics</title><p>For visualizations and cluster-based statistics, we transformed the regression coefficients from the axial MEG data to a planar representation using FieldTrip (<xref ref-type="bibr" rid="bib5">Bastiaansen and Knösche, 2000</xref>). The regression coefficients estimated on the axial gradient data were linearly transformed to planar gradient data, for which the resulting synthetic horizontal and vertical planar gradient components were then non-linearly combined to a single magnitude per original MEG sensor. For the planar-transformed coefficients, we selected the most responsive channels according to the coefficients of the note onset regressor in the Onset model (&gt;5/6 quantile, time window: 0–0.6 s). The threshold was determined through visual inspection of the spatial topographies, but did not affect the main results. We then used cluster-based permutation tests (<xref ref-type="bibr" rid="bib43">Maris and Oostenveld, 2007</xref>) to identify significant spatio-temporally clustered effects compared to the baseline time window (−0.2–0 s, 2000 permutations). Using threshold free cluster enhancement (TFCE, <xref ref-type="bibr" rid="bib87">Smith and Nichols, 2009</xref>), we further determined significant time points, where at least one selected channel showed a significant effect. Mass-univariate testing was done via one-sample t-tests on the baseline-corrected M|EEG data with ‘hat’ variance adjustment (σ=1e−3) (<xref ref-type="bibr" rid="bib73">Ridgway et al., 2012</xref>).</p></sec><sec id="s4-15-6"><title>Source analysis</title><p>To localize the neural sources associated with the different regressors, we used equivalent current dipole modeling (ECD). Individuals’ anatomical MRI scans were realigned to CTF space based on the headshape data and the fiducial coil locations, using a semi-automatic procedure in Fieldtrip. The lead field was computed using a single-shell volume conduction model (<xref ref-type="bibr" rid="bib57">Nolte, 2003</xref>). Based on individuals’ time-averaged axial gradient TRF data in the main time window of interest (180–240ms), we used a non-linear fitting algorithm to estimate the dipole configuration that best explained the observed sensor maps (FieldTrip’s ft_dipolefitting). We compared three models with one to three dipoles per hemisphere. As the final solution per participant, we chose that with the largest adjusted-r<sup>2</sup> score in explaining the observed sensor topography (thereby adjusting for the additional 12 free parameters caused by introducing an extra dipole; 2 hemispheres times x/y/z/dx/dy/dz). As starting point for the search, we roughly specified bilateral primary auditory cortex (MNI coordinates x/y/z [48, -28, 10] mm (R), [-40,–28, 6] mm (L); <xref ref-type="bibr" rid="bib2">Anderson et al., 2011</xref>; <xref ref-type="bibr" rid="bib35">Kiviniemi et al., 2009</xref>), with a small random jitter (normally distributed with SD = 1 mm) to prevent exact overlap in starting positions of multiple dipoles. Note that the initial dipole location has a negligible effect on the final solution if the data are well explained by the final fit model. This was the case for our data, see Results. For visualization, we estimated the (volumetric) density of best-fit dipole locations across participants and projected this onto the average MNI brain template, separately for each regressor.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Senior editor, eLife</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Visualization, Methodology, Writing - original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Investigation, Methodology, Project administration</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study was approved under the general ethical approval for the Donders Centre for Cognitive Neuroimaging (Imaging Human Cognition, CMO2014/288) by the local ethics committee (CMO Arnhem-Nijmegen, Radboud University Medical Centre). Participants provided written informed consent before the experiment and received monetary compensation.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-80935-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data have been deposited into the Donders Repository under CC-BY-4.0 license, under identifier <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.34973/5qxw-nn97">https://doi.org/10.34973/5qxw-nn97</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Kern</surname><given-names>P</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Spaak</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Tracking predictions in naturalistic music listening using MEG and computational models of music</data-title><source>Donders Repository</source><pub-id pub-id-type="doi">10.34973/5qxw-nn97</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><collab>DiLiberto et al</collab></person-group><year iso-8601-date="2020">2020</year><data-title>Cortical encoding of melodic expectations in human temporal cortex</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.g1jwstqmh</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank David Temperley for providing the code for his model and Marcus Pearce for discussions on the IDyOM model. This work was supported by The Netherlands Organisation for Scientific Research (NWO Veni grant 016.Veni.198.065 awarded to ES) and the European Research Council (ERC Consolidator grant SURPRISE # 101000942 awarded to FPdL).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agres</surname><given-names>K</given-names></name><name><surname>Abdallah</surname><given-names>S</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Information-Theoretic properties of auditory sequences dynamically influence expectation and memory</article-title><source>Cognitive Science</source><volume>42</volume><fpage>43</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1111/cogs.12477</pub-id><pub-id pub-id-type="pmid">28121017</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>JS</given-names></name><name><surname>Ferguson</surname><given-names>MA</given-names></name><name><surname>Lopez-Larson</surname><given-names>M</given-names></name><name><surname>Yurgelun-Todd</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reproducibility of single-subject functional connectivity measurements</article-title><source>AJNR. American Journal of Neuroradiology</source><volume>32</volume><fpage>548</fpage><lpage>555</lpage><pub-id pub-id-type="doi">10.3174/ajnr.A2330</pub-id><pub-id pub-id-type="pmid">21273356</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auksztulewicz</surname><given-names>R</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Repetition suppression and its contextual determinants in predictive coding</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>80</volume><fpage>125</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.11.024</pub-id><pub-id pub-id-type="pmid">26861557</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barascud</surname><given-names>N</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Brain responses in humans reveal ideal observer-like sensitivity to complex acoustic patterns</article-title><source>PNAS</source><volume>113</volume><fpage>E616</fpage><lpage>E625</lpage><pub-id pub-id-type="doi">10.1073/pnas.1508523113</pub-id><pub-id pub-id-type="pmid">26787854</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastiaansen</surname><given-names>MC</given-names></name><name><surname>Knösche</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Tangential derivative mapping of axial MEG applied to event-related desynchronization research</article-title><source>Clinical Neurophysiology</source><volume>111</volume><fpage>1300</fpage><lpage>1305</lpage><pub-id pub-id-type="doi">10.1016/s1388-2457(00)00272-8</pub-id><pub-id pub-id-type="pmid">10880806</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname><given-names>R</given-names></name><name><surname>Harrison</surname><given-names>PM</given-names></name><name><surname>Hu</surname><given-names>M</given-names></name><name><surname>Bolger</surname><given-names>C</given-names></name><name><surname>Picken</surname><given-names>S</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Long-Term implicit memory for sequential auditory patterns in humans</article-title><source>eLife</source><volume>9</volume><elocation-id>e56073</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.56073</pub-id><pub-id pub-id-type="pmid">32420868</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bigand</surname><given-names>E</given-names></name><name><surname>Delbé</surname><given-names>C</given-names></name><name><surname>Poulin-Charronnat</surname><given-names>B</given-names></name><name><surname>Leman</surname><given-names>M</given-names></name><name><surname>Tillmann</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Empirical evidence for musical SYNTAX processing? computer simulations reveal the contribution of auditory short-term memory</article-title><source>Frontiers in Systems Neuroscience</source><volume>8</volume><elocation-id>94</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2014.00094</pub-id><pub-id pub-id-type="pmid">24936174</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brattico</surname><given-names>E</given-names></name><name><surname>Tervaniemi</surname><given-names>M</given-names></name><name><surname>Näätänen</surname><given-names>R</given-names></name><name><surname>Peretz</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Musical scale properties are automatically processed in the human auditory cortex</article-title><source>Brain Research</source><volume>1117</volume><fpage>162</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2006.08.023</pub-id><pub-id pub-id-type="pmid">16963000</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calma-Roddin</surname><given-names>N</given-names></name><name><surname>Drury</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Music, language, and the N400: Erp interference patterns across cognitive domains</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>11222</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-66732-0</pub-id><pub-id pub-id-type="pmid">32641708</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlsen</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Some factors which influence melodic expectancy</article-title><source>Psychomusicology</source><volume>1</volume><fpage>12</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1037/h0094276</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Whatever next? predictive brains, situated agents, and the future of cognitive science</article-title><source>The Behavioral and Brain Sciences</source><volume>36</volume><fpage>181</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1017/S0140525X12000477</pub-id><pub-id pub-id-type="pmid">23663408</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>T</given-names></name><name><surname>Tillmann</surname><given-names>B</given-names></name><name><surname>Barrett</surname><given-names>FS</given-names></name><name><surname>Delbé</surname><given-names>C</given-names></name><name><surname>Janata</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A combined model of sensory and cognitive representations underlying tonal expectations in music: from audio signals to behavior</article-title><source>Psychological Review</source><volume>121</volume><fpage>33</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1037/a0034695</pub-id><pub-id pub-id-type="pmid">24490788</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Bednar</surname><given-names>A</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The multivariate temporal response function (mtrf) toolbox: A MATLAB toolbox for relating neural signals to continuous stimuli</article-title><source>Frontiers in Human Neuroscience</source><volume>10</volume><elocation-id>604</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00604</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cuddy</surname><given-names>LL</given-names></name><name><surname>Lunney</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Expectancies generated by melodic intervals: perceptual judgments of melodic continuity</article-title><source>Perception &amp; Psychophysics</source><volume>57</volume><fpage>451</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.3758/bf03213071</pub-id><pub-id pub-id-type="pmid">7596743</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How do expectations shape perception?</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>764</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.06.002</pub-id><pub-id pub-id-type="pmid">30122170</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Pelofi</surname><given-names>C</given-names></name><name><surname>Bianco</surname><given-names>R</given-names></name><name><surname>Patel</surname><given-names>P</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Herrero</surname><given-names>JL</given-names></name><name><surname>de Cheveigné</surname><given-names>A</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical encoding of melodic expectations in human temporal cortex</article-title><source>eLife</source><volume>9</volume><elocation-id>e51784</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.51784</pub-id><pub-id pub-id-type="pmid">32122465</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title><source>PNAS</source><volume>109</volume><fpage>11854</fpage><lpage>11859</lpage><pub-id pub-id-type="doi">10.1073/pnas.1205381109</pub-id><pub-id pub-id-type="pmid">22753470</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The free-energy principle: a unified brain theory?</article-title><source>Nature Reviews. Neuroscience</source><volume>11</volume><fpage>127</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1038/nrn2787</pub-id><pub-id pub-id-type="pmid">20068583</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrido</surname><given-names>MI</given-names></name><name><surname>Kilner</surname><given-names>JM</given-names></name><name><surname>Kiebel</surname><given-names>SJ</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Baldeweg</surname><given-names>T</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Repetition suppression and plasticity in the human brain</article-title><source>NeuroImage</source><volume>48</volume><fpage>269</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.034</pub-id><pub-id pub-id-type="pmid">19540921</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Goodkind</surname><given-names>A</given-names></name><name><surname>Bicknell</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Predictive power of word surprisal for reading times is a linear function of language model quality</article-title><conf-name>Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018</conf-name><fpage>10</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.18653/v1/W18-0102</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Luessi</surname><given-names>M</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Strohmeier</surname><given-names>D</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Goj</surname><given-names>R</given-names></name><name><surname>Jas</surname><given-names>M</given-names></name><name><surname>Brooks</surname><given-names>T</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Hämäläinen</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Meg and EEG data analysis with MNE-python</article-title><source>Frontiers in Neuroscience</source><volume>7</volume><elocation-id>267</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id><pub-id pub-id-type="pmid">24431986</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Henson</surname><given-names>R</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Repetition and the brain: neural models of stimulus-specific effects</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>14</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.11.006</pub-id><pub-id pub-id-type="pmid">16321563</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gwinn</surname><given-names>D</given-names></name><name><surname>Myrick</surname><given-names>B</given-names></name><name><surname>Nélias</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Gwinndr/musictransformer-pytorch</data-title><version designator="1.0">1.0</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/gwinndr/MusicTransformer-Pytorch">https://github.com/gwinndr/MusicTransformer-Pytorch</ext-link></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>Vallines</surname><given-names>I</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Rubin</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A hierarchy of temporal receptive windows in human cortex</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>2539</fpage><lpage>2550</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5487-07.2008</pub-id><pub-id pub-id-type="pmid">18322098</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hawthorne</surname><given-names>C</given-names></name><name><surname>Stasyuk</surname><given-names>A</given-names></name><name><surname>Roberts</surname><given-names>A</given-names></name><name><surname>Simon</surname><given-names>I</given-names></name><name><surname>Huang</surname><given-names>CZA</given-names></name><name><surname>Dieleman</surname><given-names>S</given-names></name><name><surname>Elsen</surname><given-names>E</given-names></name><name><surname>Engel</surname><given-names>J</given-names></name><name><surname>Eck</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1810.12247">http://arxiv.org/abs/1810.12247</ext-link></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heilbron</surname><given-names>M.</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Great expectations: is there evidence for predictive coding in auditory cortex?</article-title><source>Neuroscience</source><volume>389</volume><fpage>54</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2017.07.061</pub-id><pub-id pub-id-type="pmid">28782642</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Armeni</surname><given-names>K</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A Hierarchy of Linguistic Predictions during Natural Language Comprehension</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.12.03.410399</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Himberger</surname><given-names>KD</given-names></name><name><surname>Chien</surname><given-names>HY</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Principles of temporal processing across the cortical hierarchy</article-title><source>Neuroscience</source><volume>389</volume><fpage>161</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2018.04.030</pub-id><pub-id pub-id-type="pmid">29729293</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Thesen</surname><given-names>T</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Silbert</surname><given-names>LJ</given-names></name><name><surname>Carlson</surname><given-names>CE</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Doyle</surname><given-names>WK</given-names></name><name><surname>Rubin</surname><given-names>N</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Slow cortical dynamics and the accumulation of information over long timescales</article-title><source>Neuron</source><volume>76</volume><fpage>423</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.08.011</pub-id><pub-id pub-id-type="pmid">23083743</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>CZA</given-names></name><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Simon</surname><given-names>I</given-names></name><name><surname>Hawthorne</surname><given-names>C</given-names></name><name><surname>Dai</surname><given-names>AM</given-names></name><name><surname>Hoffman</surname><given-names>MD</given-names></name><name><surname>Dinculescu</surname><given-names>M</given-names></name><name><surname>Eck</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Music Transformer</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1809.04281">http://arxiv.org/abs/1809.04281</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huron</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Sweet Anticipation: Music and the Psychology of Expectation</source><publisher-name>The MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/6575.001.0001</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jurafsky</surname><given-names>D</given-names></name><name><surname>Martin</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition</source><publisher-name>Prentice Hall PTR</publisher-name></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juslin</surname><given-names>PN</given-names></name><name><surname>Västfjäll</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Emotional responses to music: the need to consider underlying mechanisms</article-title><source>The Behavioral and Brain Sciences</source><volume>31</volume><fpage>559</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1017/S0140525X08005293</pub-id><pub-id pub-id-type="pmid">18826699</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiviniemi</surname><given-names>V</given-names></name><name><surname>Starck</surname><given-names>T</given-names></name><name><surname>Remes</surname><given-names>J</given-names></name><name><surname>Long</surname><given-names>X</given-names></name><name><surname>Nikkinen</surname><given-names>J</given-names></name><name><surname>Haapea</surname><given-names>M</given-names></name><name><surname>Veijola</surname><given-names>J</given-names></name><name><surname>Moilanen</surname><given-names>I</given-names></name><name><surname>Isohanni</surname><given-names>M</given-names></name><name><surname>Zang</surname><given-names>YF</given-names></name><name><surname>Tervonen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Functional segmentation of the brain cortex using high model order group pica</article-title><source>Human Brain Mapping</source><volume>30</volume><fpage>3865</fpage><lpage>3886</lpage><pub-id pub-id-type="doi">10.1002/hbm.20813</pub-id><pub-id pub-id-type="pmid">19507160</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname><given-names>S</given-names></name><name><surname>Gunter</surname><given-names>T</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name><name><surname>Schröger</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Brain indices of music processing: “ nonmusicians ” are musical</article-title><source>Journal of Cognitive Neuroscience</source><volume>12</volume><fpage>520</fpage><lpage>541</lpage><pub-id pub-id-type="doi">10.1162/089892900562183</pub-id><pub-id pub-id-type="pmid">10931776</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname><given-names>S</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Predictive processes and the peculiar case of music</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>63</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.10.006</pub-id><pub-id pub-id-type="pmid">30471869</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krumhansl</surname><given-names>CL</given-names></name><name><surname>Kessler</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Tracing the dynamic changes in perceived Tonal organization in a spatial representation of musical keys</article-title><source>Psychological Review</source><volume>89</volume><fpage>334</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.89.4.334</pub-id><pub-id pub-id-type="pmid">7134332</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krumhansl</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Statistics, structure, and style in music</article-title><source>Music Perception</source><volume>33</volume><fpage>20</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1525/mp.2015.33.1.20</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (Erp)</article-title><source>Annual Review of Psychology</source><volume>62</volume><fpage>621</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id><pub-id pub-id-type="pmid">20809790</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leys</surname><given-names>C</given-names></name><name><surname>Ley</surname><given-names>C</given-names></name><name><surname>Klein</surname><given-names>O</given-names></name><name><surname>Bernard</surname><given-names>P</given-names></name><name><surname>Licata</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Detecting outliers: do not use standard deviation around the mean, use absolute deviation around the median</article-title><source>Journal of Experimental Social Psychology</source><volume>49</volume><fpage>764</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1016/j.jesp.2013.03.013</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maheu</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Meyniel</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Brain signatures of a multiscale process of sequence learning in humans</article-title><source>eLife</source><volume>8</volume><elocation-id>e41541</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.41541</pub-id><pub-id pub-id-type="pmid">30714904</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marmel</surname><given-names>F</given-names></name><name><surname>Tillmann</surname><given-names>B</given-names></name><name><surname>Dowling</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Tonal expectations influence pitch perception</article-title><source>Perception &amp; Psychophysics</source><volume>70</volume><fpage>841</fpage><lpage>852</lpage><pub-id pub-id-type="doi">10.3758/pp.70.5.841</pub-id><pub-id pub-id-type="pmid">18613632</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marmel</surname><given-names>F</given-names></name><name><surname>Tillmann</surname><given-names>B</given-names></name><name><surname>Delbé</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Priming in melody perception: tracking down the strength of cognitive expectations</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>36</volume><fpage>1016</fpage><lpage>1028</lpage><pub-id pub-id-type="doi">10.1037/a0018735</pub-id><pub-id pub-id-type="pmid">20695715</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Sound texture perception via statistics of the auditory periphery: evidence from sound synthesis</article-title><source>Neuron</source><volume>71</volume><fpage>926</fpage><lpage>940</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.06.032</pub-id><pub-id pub-id-type="pmid">21903084</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mencke</surname><given-names>I</given-names></name><name><surname>Quiroga-Martinez</surname><given-names>DR</given-names></name><name><surname>Omigie</surname><given-names>D</given-names></name><name><surname>Michalareas</surname><given-names>G</given-names></name><name><surname>Schwarzacher</surname><given-names>F</given-names></name><name><surname>Haumann</surname><given-names>NT</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name><name><surname>Brattico</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Prediction under uncertainty: dissociating sensory from cognitive expectations in highly uncertain musical contexts</article-title><source>Brain Research</source><volume>1773</volume><elocation-id>147664</elocation-id><pub-id pub-id-type="doi">10.1016/j.brainres.2021.147664</pub-id><pub-id pub-id-type="pmid">34560052</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>LB</given-names></name></person-group><year iso-8601-date="1957">1957</year><source>Emotion and Meaning in Music</source><publisher-name>University of Chicago Press</publisher-name></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milne</surname><given-names>AE</given-names></name><name><surname>Zhao</surname><given-names>S</given-names></name><name><surname>Tampakaki</surname><given-names>C</given-names></name><name><surname>Bury</surname><given-names>G</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Sustained pupil responses are modulated by predictability of auditory sequences</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>6116</fpage><lpage>6127</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2879-20.2021</pub-id><pub-id pub-id-type="pmid">34083259</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miranda</surname><given-names>RA</given-names></name><name><surname>Ullman</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Double dissociation between rules and memory in music: an event-related potential study</article-title><source>NeuroImage</source><volume>38</volume><fpage>331</fpage><lpage>345</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.07.034</pub-id><pub-id pub-id-type="pmid">17855126</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moldwin</surname><given-names>T</given-names></name><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Sussman</surname><given-names>ES</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Statistical learning of melodic patterns influences the brain’s response to wrong notes</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>2114</fpage><lpage>2122</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01181</pub-id><pub-id pub-id-type="pmid">28850296</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morgan</surname><given-names>E</given-names></name><name><surname>Fogel</surname><given-names>A</given-names></name><name><surname>Nair</surname><given-names>A</given-names></name><name><surname>Patel</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Statistical learning and gestalt-like principles predict melodic expectations</article-title><source>Cognition</source><volume>189</volume><fpage>23</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2018.12.015</pub-id><pub-id pub-id-type="pmid">30913527</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müllensiefen</surname><given-names>D</given-names></name><name><surname>Gingras</surname><given-names>B</given-names></name><name><surname>Musil</surname><given-names>J</given-names></name><name><surname>Stewart</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The musicality of non-musicians: an index for assessing musical sophistication in the general population</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e89642</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0089642</pub-id><pub-id pub-id-type="pmid">24586929</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Näätänen</surname><given-names>R</given-names></name><name><surname>Paavilainen</surname><given-names>P</given-names></name><name><surname>Rinne</surname><given-names>T</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The mismatch negativity (MMN) in basic research of central auditory processing: a review</article-title><source>Clinical Neurophysiology</source><volume>118</volume><fpage>2544</fpage><lpage>2590</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2007.04.026</pub-id><pub-id pub-id-type="pmid">17931964</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Narmour</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1990">1990</year><source>The Analysis and Cognition of Basic Melodic Structures: The Implication-Realization Model</source><publisher-name>University of Chicago Press</publisher-name></element-citation></ref><ref id="bib56"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Narmour</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1992">1992</year><source>The Analysis and Cognition of Melodic Complexity: The Implication-Realization Model</source><publisher-name>University of Chicago Press</publisher-name></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nolte</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors</article-title><source>Physics in Medicine and Biology</source><volume>48</volume><fpage>3637</fpage><lpage>3652</lpage><pub-id pub-id-type="doi">10.1088/0031-9155/48/22/002</pub-id><pub-id pub-id-type="pmid">14680264</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Omigie</surname><given-names>D</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Williamson</surname><given-names>VJ</given-names></name><name><surname>Stewart</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Electrophysiological correlates of melodic processing in congenital amusia</article-title><source>Neuropsychologia</source><volume>51</volume><fpage>1749</fpage><lpage>1762</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2013.05.010</pub-id><pub-id pub-id-type="pmid">23707539</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Omigie</surname><given-names>D</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Lehongre</surname><given-names>K</given-names></name><name><surname>Hasboun</surname><given-names>D</given-names></name><name><surname>Navarro</surname><given-names>V</given-names></name><name><surname>Adam</surname><given-names>C</given-names></name><name><surname>Samson</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Intracranial recordings and computational modeling of music reveal the time course of prediction error signaling in frontal and temporal cortices</article-title><source>Journal of Cognitive Neuroscience</source><volume>31</volume><fpage>855</fpage><lpage>873</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01388</pub-id><pub-id pub-id-type="pmid">30883293</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Painter</surname><given-names>JG</given-names></name><name><surname>Koelsch</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Can out-of-context musical sounds convey meaning? an ERP study on the processing of meaning in music</article-title><source>Psychophysiology</source><volume>48</volume><fpage>645</fpage><lpage>655</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2010.01134.x</pub-id><pub-id pub-id-type="pmid">20883505</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Language, music, SYNTAX and the brain</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>674</fpage><lpage>681</lpage><pub-id pub-id-type="doi">10.1038/nn1082</pub-id><pub-id pub-id-type="pmid">12830158</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>The Construction and Evaluation of Statistical Models of Melodic Structure in Music Perception and Composition</source><publisher-name>Doctoral City University London</publisher-name></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Wiggins</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Expectation in melody: the influence of context and learning</article-title><source>Music Perception</source><volume>23</volume><fpage>377</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1525/mp.2006.23.5.377</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Ruiz</surname><given-names>MH</given-names></name><name><surname>Kapasi</surname><given-names>S</given-names></name><name><surname>Wiggins</surname><given-names>GA</given-names></name><name><surname>Bhattacharya</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Unsupervised statistical learning underpins computational, behavioural, and neural manifestations of musical expectation</article-title><source>NeuroImage</source><volume>50</volume><fpage>302</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.12.019</pub-id><pub-id pub-id-type="pmid">20005297</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Wiggins</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Auditory expectation: the information dynamics of music perception and cognition</article-title><source>Topics in Cognitive Science</source><volume>4</volume><fpage>625</fpage><lpage>652</lpage><pub-id pub-id-type="doi">10.1111/j.1756-8765.2012.01214.x</pub-id><pub-id pub-id-type="pmid">22847872</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Statistical learning and probabilistic prediction in music cognition: mechanisms of stylistic enculturation</article-title><source>Annals of the New York Academy of Sciences</source><volume>1423</volume><fpage>378</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1111/nyas.13654</pub-id><pub-id pub-id-type="pmid">29749625</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pesnot Lerousseau</surname><given-names>J</given-names></name><name><surname>Schön</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Musical expertise is associated with improved neural statistical learning in the auditory domain</article-title><source>Cerebral Cortex</source><volume>31</volume><fpage>4877</fpage><lpage>4890</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhab128</pub-id><pub-id pub-id-type="pmid">34013316</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Picton</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Hearing in time: evoked potential studies of temporal processing</article-title><source>Ear and Hearing</source><volume>34</volume><fpage>385</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1097/AUD.0b013e31827ada02</pub-id><pub-id pub-id-type="pmid">24005840</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pratt</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>Sensory ERP Components</source><publisher-name>The Oxford Handbook of Event-Related Potential Components</publisher-name><pub-id pub-id-type="doi">10.1093/oxfordhb/9780195374148.013.0050</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga-Martinez</surname><given-names>DR</given-names></name><name><surname>Hansen</surname><given-names>NC</given-names></name><name><surname>Højlund</surname><given-names>A</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Brattico</surname><given-names>E</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Decomposing neural responses to melodic surprise in musicians and non-musicians: evidence for a hierarchy of predictions in the auditory system</article-title><source>NeuroImage</source><volume>215</volume><elocation-id>116816</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116816</pub-id><pub-id pub-id-type="pmid">32276064</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga-Martinez</surname><given-names>DR</given-names></name><name><surname>Hansen</surname><given-names>NC</given-names></name><name><surname>Højlund</surname><given-names>A</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Brattico</surname><given-names>E</given-names></name><name><surname>Holmes</surname><given-names>E</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Musicianship and melodic predictability enhance neural gain in auditory cortex during pitch deviance detection</article-title><source>Human Brain Mapping</source><volume>42</volume><fpage>5595</fpage><lpage>5608</lpage><pub-id pub-id-type="doi">10.1002/hbm.25638</pub-id><pub-id pub-id-type="pmid">34459062</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ridgway</surname><given-names>GR</given-names></name><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>Flandin</surname><given-names>G</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Penny</surname><given-names>WD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The problem of low variance voxels in statistical parametric mapping; a new HAT avoids a “ haircut. ”</article-title><source>NeuroImage</source><volume>59</volume><fpage>2131</fpage><lpage>2141</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.027</pub-id><pub-id pub-id-type="pmid">22037420</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodriguez Zivic</surname><given-names>PH</given-names></name><name><surname>Shifres</surname><given-names>F</given-names></name><name><surname>Cecchi</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Perceptual basis of evolving western musical styles</article-title><source>PNAS</source><volume>110</volume><fpage>10034</fpage><lpage>10038</lpage><pub-id pub-id-type="doi">10.1073/pnas.1222336110</pub-id><pub-id pub-id-type="pmid">23716669</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohrmeier</surname><given-names>MA</given-names></name><name><surname>Rebuschat</surname><given-names>P</given-names></name><name><surname>Cross</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Incidental and online learning of melodic structure</article-title><source>Consciousness and Cognition</source><volume>20</volume><fpage>214</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2010.07.004</pub-id><pub-id pub-id-type="pmid">20832338</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohrmeier</surname><given-names>MA</given-names></name><name><surname>Koelsch</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Predictive information processing in music cognition. A critical review</article-title><source>International Journal of Psychophysiology</source><volume>83</volume><fpage>164</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/j.ijpsycho.2011.12.010</pub-id><pub-id pub-id-type="pmid">22245599</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohrmeier</surname><given-names>MA</given-names></name><name><surname>Rebuschat</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Implicit learning and acquisition of music</article-title><source>Topics in Cognitive Science</source><volume>4</volume><fpage>525</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.1111/j.1756-8765.2012.01223.x</pub-id><pub-id pub-id-type="pmid">23060126</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saffran</surname><given-names>JR</given-names></name><name><surname>Johnson</surname><given-names>EK</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name><name><surname>Newport</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Statistical learning of tone sequences by human infants and adults</article-title><source>Cognition</source><volume>70</volume><fpage>27</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1016/s0010-0277(98)00075-4</pub-id><pub-id pub-id-type="pmid">10193055</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salimpoor</surname><given-names>VN</given-names></name><name><surname>Zald</surname><given-names>DH</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Dagher</surname><given-names>A</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Predictions and the brain: how musical sounds become rewarding</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>86</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.12.001</pub-id><pub-id pub-id-type="pmid">25534332</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savage</surname><given-names>PE</given-names></name><name><surname>Brown</surname><given-names>S</given-names></name><name><surname>Sakai</surname><given-names>E</given-names></name><name><surname>Currie</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Statistical universals reveal the structures and functions of human music</article-title><source>PNAS</source><volume>112</volume><fpage>8987</fpage><lpage>8992</lpage><pub-id pub-id-type="doi">10.1073/pnas.1414495112</pub-id><pub-id pub-id-type="pmid">26124105</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmitt</surname><given-names>LM</given-names></name><name><surname>Erb</surname><given-names>J</given-names></name><name><surname>Tune</surname><given-names>S</given-names></name><name><surname>Rysop</surname><given-names>AU</given-names></name><name><surname>Hartwigsen</surname><given-names>G</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Predicting speech from a cortical hierarchy of event-based time scales</article-title><source>Science Advances</source><volume>7</volume><elocation-id>eabi6070</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abi6070</pub-id><pub-id pub-id-type="pmid">34860554</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmuckler</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Expectation in music: investigation of melodic and harmonic processes</article-title><source>Music Perception</source><volume>7</volume><fpage>109</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.2307/40285454</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Blank</surname><given-names>IA</given-names></name><name><surname>Tuckute</surname><given-names>G</given-names></name><name><surname>Kauf</surname><given-names>C</given-names></name><name><surname>Hosseini</surname><given-names>EA</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The neural architecture of language: integrative modeling converges on predictive processing</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2105646118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2105646118</pub-id><pub-id pub-id-type="pmid">34737231</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sears</surname><given-names>DR</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Spitzer</surname><given-names>J</given-names></name><name><surname>Caplin</surname><given-names>WE</given-names></name><name><surname>McAdams</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Expectations for tonal cadences: sensory and cognitive priming effects</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>72</volume><fpage>1422</fpage><lpage>1438</lpage><pub-id pub-id-type="doi">10.1177/1747021818814472</pub-id><pub-id pub-id-type="pmid">30404574</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shaw</surname><given-names>P</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Vaswani</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Self-Attention with Relative Position Representations</article-title><conf-name>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</conf-name><pub-id pub-id-type="doi">10.18653/v1/N18-2074</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skerritt-Davis</surname><given-names>B</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Detecting change in stochastic sound sequences</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006162</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006162</pub-id><pub-id pub-id-type="pmid">29813049</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Threshold-free cluster enhancement: addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title><source>NeuroImage</source><volume>44</volume><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.061</pub-id><pub-id pub-id-type="pmid">18501637</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Regression-Based estimation of Erp waveforms: I. the rerp framework</article-title><source>Psychophysiology</source><volume>52</volume><fpage>157</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1111/psyp.12317</pub-id><pub-id pub-id-type="pmid">25141770</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stolk</surname><given-names>A</given-names></name><name><surname>Todorovic</surname><given-names>A</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Online and offline tools for head movement compensation in MEG</article-title><source>NeuroImage</source><volume>68</volume><fpage>39</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.11.047</pub-id><pub-id pub-id-type="pmid">23246857</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Temperley</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A probabilistic model of melody perception</article-title><source>Cognitive Science</source><volume>32</volume><fpage>418</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1080/03640210701864089</pub-id><pub-id pub-id-type="pmid">21635341</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Temperley</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Probabilistic models of melodic interval</article-title><source>Music Perception</source><volume>32</volume><fpage>85</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1525/mp.2014.32.1.85</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Thaut</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Musical echoic memory training (MEM)</chapter-title><person-group person-group-type="editor"><name><surname>Thaut</surname><given-names>MH</given-names></name></person-group><source>Handbook of Neurologic Music Therapy</source><publisher-name>Oxford University Press</publisher-name><fpage>311</fpage><lpage>313</lpage></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tillmann</surname><given-names>B</given-names></name><name><surname>Poulin-Charronnat</surname><given-names>B</given-names></name><name><surname>Bigand</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The role of expectation in music: from the score to emotions and the brain</article-title><source>Wiley Interdisciplinary Reviews. Cognitive Science</source><volume>5</volume><fpage>105</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1002/wcs.1262</pub-id><pub-id pub-id-type="pmid">26304299</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorovic</surname><given-names>A</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Prior expectation mediates neural adaptation to repeated sounds in the auditory cortex: an MEG study</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>9118</fpage><lpage>9123</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1425-11.2011</pub-id><pub-id pub-id-type="pmid">21697363</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorovic</surname><given-names>A</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Repetition suppression and expectation suppression are dissociable in time in early auditory evoked fields</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>13389</fpage><lpage>13395</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2227-12.2012</pub-id><pub-id pub-id-type="pmid">23015429</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Raamana</surname><given-names>PR</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Hoyos-Idrobo</surname><given-names>A</given-names></name><name><surname>Schwartz</surname><given-names>Y</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Assessing and tuning brain decoders: cross-validation, caveats, and guidelines</article-title><source>NeuroImage</source><volume>145</volume><fpage>166</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.10.038</pub-id><pub-id pub-id-type="pmid">27989847</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name><name><surname>Kaiser</surname><given-names>Ł</given-names></name><name><surname>Polosukhin</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Attention is all you need</chapter-title><person-group person-group-type="editor"><name><surname>Guyon</surname><given-names>I</given-names></name><name><surname>Luxburg</surname><given-names>UV</given-names></name><name><surname>Bengio</surname><given-names>S</given-names></name><name><surname>Wallach</surname><given-names>H</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name><name><surname>Vishwanathan</surname><given-names>S</given-names></name><name><surname>Garnett</surname><given-names>R</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><fpage>5998</fpage><lpage>6008</lpage></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vuust</surname><given-names>P</given-names></name><name><surname>Heggli</surname><given-names>OA</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Kringelbach</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Music in the brain</article-title><source>Nature Reviews. Neuroscience</source><volume>23</volume><fpage>287</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1038/s41583-022-00578-5</pub-id><pub-id pub-id-type="pmid">35352057</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wacongne</surname><given-names>C</given-names></name><name><surname>Labyt</surname><given-names>E</given-names></name><name><surname>van Wassenhove</surname><given-names>V</given-names></name><name><surname>Bekinschtein</surname><given-names>T</given-names></name><name><surname>Naccache</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Evidence for a hierarchy of predictions and prediction errors in human cortex</article-title><source>PNAS</source><volume>108</volume><fpage>20754</fpage><lpage>20759</lpage><pub-id pub-id-type="doi">10.1073/pnas.1117807108</pub-id><pub-id pub-id-type="pmid">22147913</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wilcox</surname><given-names>EG</given-names></name><name><surname>Gauthier</surname><given-names>J</given-names></name><name><surname>Hu</surname><given-names>J</given-names></name><name><surname>Qian</surname><given-names>P</given-names></name><name><surname>Levy</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2006.01912">http://arxiv.org/abs/2006.01912</ext-link></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Salimpoor</surname><given-names>VN</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>From perception to pleasure: music and its neural substrates</article-title><source>PNAS</source><volume>110</volume><fpage>10430</fpage><lpage>10437</lpage><pub-id pub-id-type="doi">10.1073/pnas.1301228110</pub-id><pub-id pub-id-type="pmid">23754373</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>S</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name><name><surname>Dick</surname><given-names>F</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Furukawa</surname><given-names>S</given-names></name><name><surname>Liao</surname><given-names>HI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Pupil-linked phasic arousal evoked by violation but not emergence of regularity within rapid sound sequences</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>4030</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-12048-1</pub-id><pub-id pub-id-type="pmid">31492881</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuk</surname><given-names>NJ</given-names></name><name><surname>Murphy</surname><given-names>JW</given-names></name><name><surname>Reilly</surname><given-names>RB</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Envelope reconstruction of speech and music highlights stronger tracking of speech at low frequencies</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1009358</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009358</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Overview of the musical stimuli presented in the MEG (top) and EEG study (bottom).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" colspan="9">MusicMEG</th></tr><tr><th align="left" valign="bottom">Composer</th><th align="left" valign="bottom">Composition</th><th align="left" valign="bottom">Year</th><th align="left" valign="bottom">Key</th><th align="left" valign="bottom">Time signature</th><th align="left" valign="bottom">Tempo (bpm)</th><th align="left" valign="bottom">Duration (sec)</th><th align="left" valign="bottom">Notes</th><th align="left" valign="bottom">Sound</th></tr></thead><tbody><tr><td align="left" valign="bottom">Benjamin Britten</td><td align="left" valign="bottom">Metamorphoses Op. 49, II. Phaeton</td><td align="char" char="." valign="bottom">1951</td><td align="left" valign="bottom">C maj</td><td align="char" char="." valign="bottom">4/4</td><td align="char" char="." valign="bottom">110</td><td align="char" char="." valign="bottom">95</td><td align="char" char="." valign="bottom">384</td><td align="left" valign="bottom">Oboe</td></tr><tr><td align="left" valign="bottom">Benjamin Britten</td><td align="left" valign="bottom">Metamorphoses Op. 49, III. Niobe</td><td align="char" char="." valign="bottom">1951</td><td align="left" valign="bottom">Db maj</td><td align="char" char="." valign="bottom">4/4</td><td align="char" char="." valign="bottom">60</td><td align="char" char="." valign="bottom">101</td><td align="char" char="." valign="bottom">171</td><td align="left" valign="bottom">Oboe</td></tr><tr><td align="left" valign="bottom">Benjamin Britten</td><td align="left" valign="bottom">Metamorphoses Op. 49, IV. Bacchus</td><td align="char" char="." valign="bottom">1951</td><td align="left" valign="bottom">F maj</td><td align="char" char="." valign="bottom">4/4</td><td align="char" char="." valign="bottom">100</td><td align="char" char="." valign="bottom">114</td><td align="char" char="." valign="bottom">448</td><td align="left" valign="bottom">Oboe</td></tr><tr><td align="left" valign="bottom">César Franck</td><td align="left" valign="bottom">Violin Sonata IV. Allegretto poco mosso</td><td align="char" char="." valign="bottom">1886</td><td align="left" valign="bottom">A maj</td><td align="char" char="." valign="bottom">4/4</td><td align="char" char="." valign="bottom">150</td><td align="char" char="." valign="bottom">175</td><td align="char" char="." valign="bottom">458</td><td align="left" valign="bottom">Flute</td></tr><tr><td align="left" valign="bottom">Carl Philipp Emanuel Bach</td><td align="left" valign="bottom">Sonata for Solo Flute, Wq.132/H.564 III.</td><td align="char" char="." valign="bottom">1763</td><td align="left" valign="bottom">A min</td><td align="char" char="." valign="bottom">3/8</td><td align="char" char="." valign="bottom">98</td><td align="char" char="." valign="bottom">275</td><td align="char" char="." valign="bottom">1358</td><td align="left" valign="bottom">Flute</td></tr><tr><td align="left" valign="bottom">Ernesto Köhler</td><td align="left" valign="bottom">Flute Exercises Op. 33 a, V. Allegretto</td><td align="char" char="." valign="bottom">1880</td><td align="left" valign="bottom">G maj</td><td align="char" char="." valign="bottom">4/4</td><td align="char" char="." valign="bottom">124</td><td align="char" char="." valign="bottom">140</td><td align="char" char="." valign="bottom">443</td><td align="left" valign="bottom">Flute</td></tr><tr><td align="left" valign="bottom">Ernesto Köhler</td><td align="left" valign="bottom">Flute Exercises Op. 33b, VI. Presto</td><td align="char" char="." valign="bottom">1880</td><td align="left" valign="bottom">D min</td><td align="char" char="." valign="bottom">6/8</td><td align="char" char="." valign="bottom">176</td><td align="char" char="." valign="bottom">134</td><td align="char" char="." valign="bottom">664</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Georg Friedrich Händel</td><td align="left" valign="bottom">Flute Sonata Op. 1 No. 5, HWV 363b, IV. Bourrée</td><td align="char" char="." valign="bottom">1711</td><td align="left" valign="bottom">G maj</td><td align="char" char="." valign="bottom">4/4</td><td align="char" char="." valign="bottom">132</td><td align="char" char="." valign="bottom">84</td><td align="char" char="." valign="bottom">244</td><td align="left" valign="bottom">Oboe</td></tr><tr><td align="left" valign="bottom">Georg Friedrich Händel</td><td align="left" valign="bottom">Flute Sonata Op. 1 No. 3, HWV 379, IV. Allegro</td><td align="char" char="." valign="bottom">1711</td><td align="left" valign="bottom">E min</td><td align="char" char="." valign="bottom">3/8</td><td align="char" char="." valign="bottom">96</td><td align="char" char="." valign="bottom">143</td><td align="char" char="." valign="bottom">736</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Joseph Haydn</td><td align="left" valign="bottom">Little Serenade</td><td align="char" char="." valign="bottom">1785</td><td align="left" valign="bottom">F maj</td><td align="char" char="." valign="bottom">3/4</td><td align="char" char="." valign="bottom">92</td><td align="char" char="." valign="bottom">81</td><td align="char" char="." valign="bottom">160</td><td align="left" valign="bottom">Oboe</td></tr><tr><td align="left" valign="bottom">Johann Sebastian Bach</td><td align="left" valign="bottom">Flute Partita BWV 1013, II. Courante</td><td align="char" char="." valign="bottom">1723</td><td align="left" valign="bottom">A min</td><td align="char" char="." valign="bottom">3/4</td><td align="char" char="." valign="bottom">64</td><td align="char" char="." valign="bottom">176</td><td align="char" char="." valign="bottom">669</td><td align="left" valign="bottom">Flute</td></tr><tr><td align="left" valign="bottom">Johann Sebastian Bach</td><td align="left" valign="bottom">Flute Partita BWV 1013, IV. Bourrée angloise</td><td align="char" char="." valign="bottom">1723</td><td align="left" valign="bottom">A min</td><td align="char" char="." valign="bottom">2/4</td><td align="char" char="." valign="bottom">62</td><td align="char" char="." valign="bottom">138</td><td align="char" char="." valign="bottom">412</td><td align="left" valign="bottom">Oboe</td></tr><tr><td align="left" valign="bottom">Johann Sebastian Bach</td><td align="left" valign="bottom">Violin Concerto BWV 1042, I. Allegro</td><td align="char" char="." valign="bottom">1718</td><td align="left" valign="bottom">E maj</td><td align="char" char="." valign="bottom">2/2</td><td align="char" char="." valign="bottom">100</td><td align="char" char="." valign="bottom">122</td><td align="char" char="." valign="bottom">698</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Johann Sebastian Bach</td><td align="left" valign="bottom">Violin Concerto BWV 1042, III. Allegro Assai</td><td align="char" char="." valign="bottom">1718</td><td align="left" valign="bottom">E maj</td><td align="char" char="." valign="bottom">3/8</td><td align="char" char="." valign="bottom">92</td><td align="char" char="." valign="bottom">80</td><td align="char" char="." valign="bottom">413</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Ludwig van Beethoven</td><td align="left" valign="bottom">Sonatina (Anh. 5 No. 1)</td><td align="char" char="." valign="bottom">1807</td><td align="left" valign="bottom">G maj</td><td align="char" char="." valign="bottom">4/4</td><td align="char" char="." valign="bottom">128</td><td align="char" char="." valign="bottom">210</td><td align="char" char="." valign="bottom">624</td><td align="left" valign="bottom">Flute</td></tr><tr><td align="left" valign="bottom">Muzio Clementi</td><td align="left" valign="bottom">Sonatina Op. 36 No. 5, III. Rondo</td><td align="char" char="." valign="bottom">1797</td><td align="left" valign="bottom">G maj</td><td align="char" char="." valign="bottom">2/4</td><td align="char" char="." valign="bottom">112</td><td align="char" char="." valign="bottom">187</td><td align="char" char="." valign="bottom">915</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Modest Mussorgsky</td><td align="left" valign="bottom">Pictures at an Exhibition - Promenade</td><td align="char" char="." valign="bottom">1874</td><td align="left" valign="bottom">Bb maj</td><td align="char" char="." valign="bottom">5/4</td><td align="char" char="." valign="bottom">80</td><td align="char" char="." valign="bottom">106</td><td align="char" char="." valign="bottom">179</td><td align="left" valign="bottom">Oboe</td></tr><tr><td align="left" valign="bottom">Pyotr Ilyich Tchaikovsky</td><td align="left" valign="bottom">The Nutcracker Suite - Russian Dance Trepak</td><td align="char" char="." valign="bottom">1892</td><td align="left" valign="bottom">G maj</td><td align="char" char="." valign="bottom">2/4</td><td align="char" char="." valign="bottom">120</td><td align="char" char="." valign="bottom">78</td><td align="char" char="." valign="bottom">396</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Wolfgang Amadeus Mozart</td><td align="left" valign="bottom">The Magic Flute K620, Papageno’s Aria</td><td align="char" char="." valign="bottom">1791</td><td align="left" valign="bottom">F maj</td><td align="char" char="." valign="bottom">2/4</td><td align="char" char="." valign="bottom">72</td><td align="char" char="." valign="bottom">150</td><td align="char" char="." valign="bottom">452</td><td align="left" valign="bottom">Flute</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">2589</td><td align="char" char="." valign="bottom">9824</td><td align="left" valign="bottom"/></tr><tr><th align="left" valign="bottom" colspan="9">MusicEEG</th></tr><tr><th align="left" valign="bottom">Composer</th><th align="left" valign="bottom">Composition</th><th align="left" valign="bottom">Year</th><th align="left" valign="bottom">Key</th><th align="left" valign="bottom">Time signature</th><th align="left" valign="bottom">Tempo (bpm)</th><th align="left" valign="bottom">Duration (sec)</th><th align="left" valign="bottom">Notes</th><th align="left" valign="bottom">Sound</th></tr><tr><td align="left" valign="bottom">Johann Sebastian Bach</td><td align="left" valign="bottom">Flute Partita BWV 1013, I. Allemande</td><td align="char" char="." valign="bottom">1723</td><td align="left" valign="bottom">A min</td><td align="char" char="." valign="bottom">4/4</td><td align="char" char="." valign="bottom">100</td><td align="char" char="." valign="bottom">158</td><td align="char" char="." valign="bottom">1022</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Johann Sebastian Bach</td><td align="left" valign="bottom">Flute Partita BWV 1013, II. Corrente</td><td align="char" char="." valign="bottom">1723</td><td align="left" valign="bottom">A min</td><td align="char" char="." valign="bottom">3/4</td><td align="char" char="." valign="bottom">100</td><td align="char" char="." valign="bottom">154</td><td align="char" char="." valign="bottom">891</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Johann Sebastian Bach</td><td align="left" valign="bottom">Flute Partita BWV 1013, III. Sarabande</td><td align="char" char="." valign="bottom">1723</td><td align="left" valign="bottom">A min</td><td align="char" char="." valign="bottom">3/4</td><td align="char" char="." valign="bottom">70</td><td align="char" char="." valign="bottom">120</td><td align="char" char="." valign="bottom">301</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Johann Sebastian Bach</td><td align="left" valign="bottom">Flute Partita BWV 1013, IV. Bourree</td><td align="char" char="." valign="bottom">1723</td><td align="left" valign="bottom">A min</td><td align="char" char="." valign="bottom">2/4</td><td align="char" char="." valign="bottom">80</td><td align="char" char="." valign="bottom">135</td><td align="char" char="." valign="bottom">529</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Johann Sebastian Bach</td><td align="left" valign="bottom">Violin Partita BWV 1004, I. Allemande</td><td align="char" char="." valign="bottom">1723</td><td align="left" valign="bottom">D min</td><td align="char" char="." valign="bottom">4/4</td><td align="char" char="." valign="bottom">47</td><td align="char" char="." valign="bottom">165</td><td align="char" char="." valign="bottom">540</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Johann Sebastian Bach</td><td align="left" valign="bottom">Violin Sonata BWV 1001, IV. Presto</td><td align="char" char="." valign="bottom">1720</td><td align="left" valign="bottom">G min</td><td align="char" char="." valign="bottom">3/8</td><td align="char" char="." valign="bottom">125</td><td align="char" char="." valign="bottom">199</td><td align="char" char="." valign="bottom">1604</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Johann Sebastian Bach</td><td align="left" valign="bottom">Violin Partita BWV 1002, I. Allemande</td><td align="char" char="." valign="bottom">1720</td><td align="left" valign="bottom">Bb min</td><td align="char" char="." valign="bottom">4/4</td><td align="char" char="." valign="bottom">50</td><td align="char" char="." valign="bottom">173</td><td align="char" char="." valign="bottom">620</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Johann Sebastian Bach</td><td align="left" valign="bottom">Violin Partita BWV 1004, IV. Gigue</td><td align="char" char="." valign="bottom">1723</td><td align="left" valign="bottom">D min</td><td align="left" valign="bottom">12/8_</td><td align="char" char="." valign="bottom">120</td><td align="char" char="." valign="bottom">182</td><td align="char" char="." valign="bottom">1352</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Johann Sebastian Bach</td><td align="left" valign="bottom">Violin Partita BWV 1006, II. Loure</td><td align="char" char="." valign="bottom">1720</td><td align="left" valign="bottom">E maj</td><td align="char" char="." valign="bottom">6/4</td><td align="char" char="." valign="bottom">80</td><td align="char" char="." valign="bottom">134</td><td align="char" char="." valign="bottom">338</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom">Johann Sebastian Bach</td><td align="left" valign="bottom">Violin Partita BWV 1006, III. Gavotte</td><td align="char" char="." valign="bottom">1720</td><td align="left" valign="bottom">E maj</td><td align="char" char="." valign="bottom">4/4</td><td align="char" char="." valign="bottom">140</td><td align="char" char="." valign="bottom">178</td><td align="char" char="." valign="bottom">642</td><td align="left" valign="bottom">Piano</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">1598</td><td align="char" char="." valign="bottom">7839</td><td align="left" valign="bottom"/></tr></tbody></table></table-wrap><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Comparison of the pitch (left) and pitch interval distributions (right) for the music data from the MEG study (top), EEG study (middle), and MCCC corpus (bottom).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-app1-fig1-v2.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Model performance on the musical stimuli used in the EEG study.</title><p>(<bold>A</bold>) Comparison of music model performance in predicting upcoming note pitch, as composition-level accuracy (left; higher is better), median surprise across notes (middle; lower is better), and median uncertainty across notes (right). Context length for each model is the best performing one across the range shown in (<bold>B</bold>). Vertical bars: single compositions, circle: median, thick line: quartiles, thin line: quartiles ±1.5 × interquartile range. (<bold>B</bold>) Accuracy of note pitch predictions (median across 10 compositions) as a function of context length and model class (same color code as (<bold>A</bold>)). Dots represent maximum for each model class. (<bold>C</bold>) Correlations between the surprise estimates from the best models.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-app1-fig2-v2.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Comparison of the MEG TRFs and spatial topographies for the surprise estimates from the best models of each model class.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-app1-fig3-v2.tif"/></fig><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Comparison of the EEG TRFs and spatial topographies for the surprise estimates from the best models of each model class.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-app1-fig4-v2.tif"/></fig><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>Comparison of the predictive performance on the MEG data using ridge-regularized regression, with the optimal cost hyperparameter alpha estimated using nested cross-validation.</title><p>Results are shown for the best-performing model (MT, context length of 8 notes). Each line represents one participant. Lower panel: raw predictive performance (<bold>r</bold>). Upper panel: predictive performance expressed as percentage of a participant’s maximum.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-app1-fig5-v2.tif"/></fig><fig id="app1fig6" position="float"><label>Appendix 1—figure 6.</label><caption><title>Comparison of the predictive performance on the EEG data using ridge-regularized regression, with the optimal cost hyperparameter alpha estimated using nested cross-validation.</title><p>Results are shown for the best-performing model (MT, context length of 7 notes). Each line represents one participant. Lower panel: raw predictive performance (<bold>r</bold>). Upper panel: predictive performance expressed as percentage of a participant’s maximum.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-app1-fig6-v2.tif"/></fig></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80935.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Obleser</surname><given-names>Jonas</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00t3r8h32</institution-id><institution>University of Lübeck</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.06.08.495241" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.06.08.495241"/></front-stub><body><p>This study models the predictions a listener makes in music in two ways: how different model algorithms compare in their performance at predicting the upcoming notes in a melody, and how well they predict listeners' brain responses to these notes. The study will be important as it implements and compares three contemporary models of music prediction. In a set of convincing analyses, the authors find that musical melodies are best predicted by models taking into account long-term experience of musical melodies, whereas brain responses are best predicted by applying these models to only a few most recent notes.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80935.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Obleser</surname><given-names>Jonas</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00t3r8h32</institution-id><institution>University of Lübeck</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Sedley</surname><given-names>William</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kj2bm70</institution-id><institution>Newcastle University</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Doelling</surname><given-names>Keith</given-names></name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.06.08.495241">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.06.08.495241v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Cortical activity during naturalistic music listening reflects short-range predictions based on long-term experience&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Christian Büchel as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: William Sedley (Reviewer #2); Keith Doelling (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) The one non-standard feature of the analysis is the lack of regularization (e.g., ridge). The authors should perform the analysis using regularization (via nested cross-validation) and test if the predictions are improved (if they have already done this, they should report the results).</p><p>2) The authors make a distinction between &quot;Gestalt-like principles&quot; and &quot;statistical learning&quot; but they never define was is meant by this distinction. The Temperley model encodes a variety of important statistics of Western music, including statistics such as keys that are unlikely to reflect generic Gestalt principles. The Temperley model builds in some additional structure such as the notion of a key, which the n-gram and transformer models must learn from scratch. In general, the models being compared differ in so many ways that it is hard to conclude much about what is driving the observed differences in prediction accuracy, particularly given the small effect sizes. The context manipulation is more controlled, and the fact that neural prediction accuracy dissociates from the model performance is potentially interesting. However, we were not confident that the authors have a good neural index of surprise for the reasons described above, and this might limit the conclusions that can be drawn from this manipulation in the manuscript as is.</p><p>3) The authors may overstate the advancement of the Music Transformer with the present stimuli, as its increase in performance requires a considerably longer context than the other models. Secondly, the Baseline model, to which the other models are compared, does not contain any pitch information on which these models operate on. As such, it's unclear if the advancements of these models come from being based on new information or the operations it performs on this information as claimed.</p><p>4) Source analysis: See below in Rev #1 and Rev #3 for concerns over the results and interpretation of the source localisation.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The one non-standard feature of the analysis is the lack of regularization (e.g., ridge). The authors should perform the analysis using regularization (via nested cross-validation) and test if the predictions are improved (if they have already done this, they should report the results).</p><p>Source localization analysis with MEG or EEG is highly uncertain. The conclusion that the results are coming from &quot;fronto-temporal&quot; areas doesn't strike me as adding much value; I'd be inclined to remove this analysis from the manuscript. Minimally, the authors should note that source localization is highly uncertain.</p><p>The authors should report the earphones/tubes used to present sounds.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>– Figure 2: Music Transformer is presented as the state-of-the-art model throughout the paper, with the main advantage of grasping regularities on longer time scales. Yet the computational results in figure 2 tend to show that it does not bring much in terms of musical predictions compared to IDyOM. MT also needs a way larger context length to reach the same accuracy. It has a lower uncertainty, but this feature did not improve the results of the neural analysis. This point could be discussed to better understand why MT is a better model of human musical expectations and surprise.</p><p>– The source analysis is a bit puzzling to me. The results show that every feature (including Note Onset and Note Repetition) are localized in Broca's area, frontally located. Wouldn't the authors expect that a feature as fundamental as Note Onset should be clearly (or at least partially) localized in the primary auditory cortex? Because these results localize frontally, it is hard to fully trust the MT surprise results as well. Can the authors provide some form of sanity check to provide further clarity on these results? Perhaps the source localizing the M100 to show the analysis pipeline is not biased towards frontal areas? Do the authors expect note onsets in continuous music to be represented in higher-order areas than the representation of a single tone? Figure 7 and source-level results: the authors do not discuss the fact that the results are mainly found in a frontal area. It looks like there is no effect in the auditory cortex, which is surprising and should be discussed.</p><p>– The dipole fit shows a high correlation with behavior but it is not compared with any alternatives. Can the authors use some model comparison methods (e.g. AIC or BIC) to show that a single dipole per hemisphere is a better fit than two or three?</p><p>– Line 252: the order in which the transformation from axial to planar gradients is applied with respect to other processing steps (e.g., z-scoring of the MEG data and TRF fitting) is not clear. Is it the MEG data that was transformed as suggested here, or is the transformation applied to TRFs coefficients (as explained in line 710), with TRFs trained on axial gradiometers? This has downstream consequences that lead to a lack of clarity in the results. For example, the MEG data shows positive values in the repetition kernel which if the transformation was applied before the TRF would imply that repetition increases activity rather than reduces it as would be expected. From this, I infer that it is the TRF kernels that were transformed. I recognize that the authors can infer results regarding directionality in the EEG data but this analysis choice results in an unnecessary loss of information in the MEG analysis. Please clarify the methods and if the gradiometer transformation is performed on the kernels, I would recommend re-running the analysis with gradiometer transformation first.</p><p>– The Baseline model claims to include all relevant acoustic information but it does not include the note pitches themselves. As these pitches are provided to the model, it is difficult to tell whether the effects of surprise are related to model output predictions, or how well they represent their input. If the benefits from the models rely truly on their predictive aspect then including pitch information in the baseline model would be an important control. The authors have done their analysis in a way that fits what previous work has done but I think it is a good time to correct this error.</p><p>– I wonder if the authors could discuss a bit more about the self-attention component of the Music Transformer model. I'm not familiar with the model's inner workings but I am intrigued by this dichotomy of a larger context to improve musical predictions and a shorter context to improve neural predictions. I wonder if a part of this dissociation has to do with the self-attention feature of Transformers, that a larger context is needed to have a range of motifs to draw from but the size of the motifs that the model attends to should be of a certain size to fit neural predictions.</p><p>– Figure 3C: it could be interesting to show the same figure for IDyOM ltm. Given that context length does not impact ltm as much as MT, we could obtain different results. Since IDyOM ltm gets similar results to MT on the MEG data (figure 3A, no significant difference), it is thus hard to tell if the influence of context length comes from brain processing or the way MT works.</p><p>– Figure 8: Adding the uncertainty estimates did not improve the model's predictive performance compared to surprise alone, but what about TRFs trained on uncertainty without surprise? Without this result, it is hard to understand why the surprise was chosen over uncertainty.</p><p>– The sentence from lines 272 to 274 is not clear. In particular, the late negativity effect seems to be present in EEG data only, and it is thus hard to understand why a negative correlation between surprise estimates of subsequent notes would have such an effect in EEG and not MEG. Moreover, the same late negativity effect can be seen on the TRF of note onset but is not discussed.</p><p>– Some of the choices for the Temperley Model seem to unnecessarily oversimplify and restrict its potential performance. In the second principle, the model only assesses the relationships between neighboring notes when the preceding note is the tonal centroid. It would seem more prudent to include all notes to collect further data. In the third principle, the model marginalizes major and minor scales by weighting probabilities of each profile by the frequency of major and minor pieces in the database. Presumably, listeners can identify the minor or major key of the current piece (at least implicitly). Why not select the model for each piece, outright?</p><p>– Stimulus: Many small details make it so that the stimuli are not so naturalistic (MIDI velocity set to 100, monophonic, mono channel…). This results in a more controlled experiment, but the claim that it expands melodic expectation findings to naturalistic music listening is a bit bold.</p><p>– Line 478: authors refer to &quot;original compositions&quot; which may give the impression that the pieces were written for the experiment. From the rest of the text, I don't believe this to be true.</p><p>– Formula line 553: the first probability of Xt (on the left) should also be a conditional probability of Xt given previous values of x. This is the entropy of the probability distribution estimated by the model.</p><p>– Line 667: the study by Di Liberto from which the EEG data come uses a ridge regression (ridge regularization). Is there a reason to use a non-regularized regression in this case? This should be discussed in the methods.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80935.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The one non-standard feature of the analysis is the lack of regularization (e.g., ridge). The authors should perform the analysis using regularization (via nested cross-validation) and test if the predictions are improved (if they have already done this, they should report the results).</p></disp-quote><p>Our motivation for the ‘plain’ ordinary least squares (OLS) was, firstly, that we use the regression ERP/F modelling framework (Smith and Kutas, 2015). This means that our design matrices were very sparse, with little correlation between the time-shifted regressors and hence a comparatively low risk for overfitting. Moreover, the model comparisons were always performed in a cross-validated fashion (thus any potential overfitting would reduce, rather than artificially inflate, model performance). However, we appreciate that we hereby deviate from earlier similar work, which used ridgeregularized regression.</p><p>We therefore now also implemented ridge-regularized regression, with the optimal cost hyperparameter α estimated using nested cross-validation. The results for this are shown in Appendix—Figure 5 (one curve per participant; regression of best-performing model (MT, context length 8) on MEG data):</p><p>This clearly demonstrates that the OLS we used previously (α = 0) is always among the best-fitting models. Even for those participants that show an increase in cross-validated r for non-zero regularization, these increases are negligible. We therefore report the same OLS models in the manuscript as before, and have now added the above figure to the supplemental information.</p><p>This also holds for the EEG data. Appendix—figure 6 shows the results for the best-performing model (MT, context length 7).</p><disp-quote content-type="editor-comment"><p>2) The authors make a distinction between &quot;Gestalt-like principles&quot; and &quot;statistical learning&quot; but they never define was is meant by this distinction. The Temperley model encodes a variety of important statistics of Western music, including statistics such as keys that are unlikely to reflect generic Gestalt principles. The Temperley model builds in some additional structure such as the notion of a key, which the n-gram and transformer models must learn from scratch. In general, the models being compared differ in so many ways that it is hard to conclude much about what is driving the observed differences in prediction accuracy, particularly given the small effect sizes. The context manipulation is more controlled, and the fact that neural prediction accuracy dissociates from the model performance is potentially interesting. However, we were not confident that the authors have a good neural index of surprise for the reasons described above, and this might limit the conclusions that can be drawn from this manipulation in the manuscript as is.</p></disp-quote><p>First of all, we would like to apologize for any unclarity regarding the distinction between Gestalt-like and statistical models. We take Gestalt-like models to be those that explain music perception as following a restricted set of rules, such as that adjacent notes tend to be close in pitch. In contrast, as the reviewer correctly points out, statistical learning models have no such <italic>a priori</italic> principles and must learn similar or other principles from scratch. Importantly, the distinction between these two classes of models is not one we make for the first time in the context of music perception. Gestalt-like models have a long tradition in musicology and the study of music cognition dating back to (Meyer, 1957). The Implication-Realization model developed by Eugene Narmour (Narmour, 1990, 1992; Schellenberg, 1997) is another example for a rule-based theory of music listening, which has influenced the model by David Temperley, which we applied as the most recently influential Gestalt-model of melodic expectations in the present study. Concurrently to the development of Gestalt-like models, a second strand of research framed music listening in light of information theory and statistical learning (Bharucha, 1987; Cohen, 1962; Conklin and Witten, 1995; Pearce and Wiggins, 2012). Previous work has made the same distinction and compared models of music along the same axis (Krumhansl, 2015; Morgan et al., 2019a; Temperley, 2014). We have updated the manuscript to elaborate on this distinction and highlight that it is not uncommon.</p><p>Second, we emphasize that we compare the models directly in terms of their predictive performance both of upcoming musical notes and of neural responses. This predictive performance is not dependent on the internal details of any particular model; e.g. in principle it would be possible to include a “human expert” model where we ask professional composers to predict upcoming notes given a previous context. Because of this independence of the relevant comparison metric on model details, we believe comparing the models is justified. Again, this is in line with previously published work in music (Morgan et al., 2019a), language, (Heilbron et al., 2022; Schmitt et al., 2021; Wilcox et al., 2020), and other domains (Planton et al., 2021). Such work compares different models in how well they align with human statistical expectations by assessing how well different models explain predictability/surprise effects in behavioral and/or brain responses.</p><p>Third, regarding the doubts on the neural index of surprise used: we respond to this concern below, after reviewer 1’s first point to which the present comment refers (the referred-to comment was not included in the “Essential revisions” here).</p><disp-quote content-type="editor-comment"><p>3) The authors may overstate the advancement of the Music Transformer with the present stimuli, as its increase in performance requires a considerably longer context than the other models.</p></disp-quote><p>We do not believe to have overstated the advance presented by the MusicTransformer, for the following reasons. First, we appreciate that from the music analysis (Figure 2b and Figure A3b), it seems as if the Music Transformer requires much longer context to reach only slightly higher predictive performance on the musical stimuli. Note, however, that this only applies to the comparison between MT and the IDyOM-stm and IDyOM-both (which subsumes IDyOM-stm) models, but not to the comparison between MT and IDyOM-ltm or the Temperley model. The MT and IDyOM-stm (and therefore IDyOM-both) deal with context information rather differently, possibly leading to the wrong impression that predictive performance for the MT requires a lot more ‘data’. We go into these differences in more detail below.</p><p>Second, and importantly, the distinctive empirical contribution of our study is not the superiority of the MT over the other models per se, but the (neural and predictive) performance differences among model classes: statistical learning (IDyOM/MT) versus Gestalt/rule-based (Temperley), and the dependence of performance on context lengths. For these comparisons, the MT is a very useful tool because it efficiently tracks hierarchical structure in longer musical contexts (Huang et al., 2018). We furthermore demonstrate that it works at least as well as the previous state-of-the-art statistical model (IDyOM), yet may process a much larger class of music (i.e., polyphonic music; not yet explored).</p><p>Regarding the first point: There are small technical differences in the way previous context is used by IDyOM-stm and the MusicTransformer (MT). IDyOM-stm is an n-gram model, predicting the probability of an upcoming note x<sub>t</sub> given a previous context {x<sub>t-1</sub>…x<sub>t-n</sub>}. The context parameter we varied here governs the maximum length n of n-grams that IDyOM can take into account to make its predictions. Importantly, IDyOM-stm is an on-line statistical learning model: it updates the relative probabilities p(x<sub>t</sub> | {x<sub>t-1</sub>…x<sub>t-n</sub>}) as it is making predictions and parsing the ongoing composition. So while for any given note IDyOM-stm will only directly take into account the preceding n notes, the underlying statistical model against which it will interpret those n context notes can depend on all the n-grams and subsequent notes that preceded it. Because of this property, IDyOM-stm is in effect “learning” based on the current ongoing composition and therefore can indirectly leverage more information than the strict limit of n-grams considered. (It could be said that IDyOM-stm is ‘peeking’ at the test set to some extent, and therefore its predictive performance may be slightly overestimated.) Importantly, the type of on-line updating in IDyOM-stm still precludes the learning of any hierarchical structure encompassing longer context lengths than n (which is, for our purposes, an essential difference with the MT).</p><p>The MT model performs no on-line learning. Instead, the model only takes into account the strict n context notes that it is provided with when asked for an upcoming prediction. Critically, the transformer architecture enables the MT to make hierarchical predictions on those n notes, which depend on the musical corpus it was trained on.</p><p>Finally, regarding the second point: We further note that the dependence of predictive performance on context length is quite different between predicting music (Figure 2) and predicting neural responses (Figures 3 and 4). For predicting upcoming musical notes, indeed the MT required considerably larger context lengths in order to outperform IDyOM (stm and both), likely in part due to the reasons described above. In contrast, for the related surprise scores to predict neural responses, the context length required for the MT to peak was on the same order as IDyOM.</p><disp-quote content-type="editor-comment"><p>Secondly, the Baseline model, to which the other models are compared, does not contain any pitch information on which these models operate on. As such, it's unclear if the advancements of these models come from being based on new information or the operations it performs on this information as claimed.</p></disp-quote><p>We apologize for not being clear enough here. Importantly, none of the models compared contained any exact pitch information. We only used surprisal (and uncertainty) collapsed across the entire distribution of possible upcoming notes as a regressor for the MEG and EEG data, which by itself cannot be traced back to any particular pitch. Furthermore, we already did include a confound regressor encoding low versus high pitch, which, critically, was included identically in all the models (including the Baseline). We have updated the manuscript to emphasize this point.</p><disp-quote content-type="editor-comment"><p>4) Source analysis: See below in Rev #1 and Rev #3 for concerns over the results and interpretation of the source localisation.</p></disp-quote><p>In response to the reviewers’ comments, we revisited the source analysis considerably. Previously, we had only investigated the later peaks, for which we had no very strong expectations and hence the frontal peak did not appear particularly suspect. However, as a sanity check and as suggested by reviewer 3, we source localized the earliest peak of the Onset TRF, expecting a clear bilateral peak in early auditory cortex. In contrast, this peak was also localized to frontal cortex, which raised our suspicions that something was wrong in our source analysis pipeline. We indeed identified a bug: for all participants, the pipeline used the forward model computed for one participant, rather than each individual participant’s forward model. If this one participant happened to sit somewhat further to the front or back of the MEG helmet than the mean, and/or have a different head size than the mean, this would introduce a consistent spatial bias, which could explain the previous absence of a peak in auditory cortex. We apologize for this error in our code, and are intensely grateful to the reviewers for their well-founded suspicion.</p><p>We have now re-run the source analysis using the correct forward models, and additionally compare three models with one to three dipoles per hemisphere, as suggested by reviewer 3. As the final solution per participant, we use that with the largest adjusted-r<sup>2</sup> score in explaining the observed sensor topography. For the majority of participants, the three-dipole model fits best, although the gain in adjusted-r<sup>2</sup> over the one- and two-dipole models is modest:</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-sa2-fig1-v2.tif"/></fig><p>(Even for the one-dipole model, these adjusted-r<sup>2</sup> scores are higher than the previously reported r<sup>2</sup>, since now the correct forward models are used.) Using this corrected and updated pipeline, we now find consistent peaks in auditory cortex for all three regressors, also in the later time window of interest, and have updated the manuscript accordingly.Additionally, in accordance with reviewer 1’s suggestion, we have now additionally reflected on the limits of MEG’s spatial resolution in the Discussion.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>The one non-standard feature of the analysis is the lack of regularization (e.g., ridge). The authors should perform the analysis using regularization (via nested cross-validation) and test if the predictions are improved (if they have already done this, they should report the results).</p></disp-quote><p>See our response in “Essential revisions” above.</p><disp-quote content-type="editor-comment"><p>Source localization analysis with MEG or EEG is highly uncertain. The conclusion that the results are coming from &quot;fronto-temporal&quot; areas doesn't strike me as adding much value; I'd be inclined to remove this analysis from the manuscript. Minimally, the authors should note that source localization is highly uncertain.</p></disp-quote><p>See our response in “Essential revisions” above.</p><disp-quote content-type="editor-comment"><p>The authors should report the earphones/tubes used to present sounds.</p></disp-quote><p>The earmolds used to present sounds were Doc’s Promolds NonVent with #13 thick prebent 1.85 mm ID tubes, from Audine Healthcare, in combination with Etymotic ER3A earphones. We have now added this information to the manuscript.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>– Figure 2: Music Transformer is presented as the state-of-the-art model throughout the paper, with the main advantage of grasping regularities on longer time scales. Yet the computational results in figure 2 tend to show that it does not bring much in terms of musical predictions compared to IDyOM. MT also needs a way larger context length to reach the same accuracy. It has a lower uncertainty, but this feature did not improve the results of the neural analysis. This point could be discussed to better understand why MT is a better model of human musical expectations and surprise.</p></disp-quote><p>See our response in “Essential revisions” above.</p><disp-quote content-type="editor-comment"><p>– The source analysis is a bit puzzling to me. The results show that every feature (including Note Onset and Note Repetition) are localized in Broca's area, frontally located. Wouldn't the authors expect that a feature as fundamental as Note Onset should be clearly (or at least partially) localized in the primary auditory cortex? Because these results localize frontally, it is hard to fully trust the MT surprise results as well. Can the authors provide some form of sanity check to provide further clarity on these results? Perhaps the source localizing the M100 to show the analysis pipeline is not biased towards frontal areas? Do the authors expect note onsets in continuous music to be represented in higher-order areas than the representation of a single tone? Figure 7 and source-level results: the authors do not discuss the fact that the results are mainly found in a frontal area. It looks like there is no effect in the auditory cortex, which is surprising and should be discussed.</p></disp-quote><p>See our response in “Essential revisions” above.</p><disp-quote content-type="editor-comment"><p>– The dipole fit shows a high correlation with behavior but it is not compared with any alternatives. Can the authors use some model comparison methods (e.g. AIC or BIC) to show that a single dipole per hemisphere is a better fit than two or three?</p></disp-quote><p>See our response in “Essential revisions” above.</p><disp-quote content-type="editor-comment"><p>– Line 252: the order in which the transformation from axial to planar gradients is applied with respect to other processing steps (e.g., z-scoring of the MEG data and TRF fitting) is not clear. Is it the MEG data that was transformed as suggested here, or is the transformation applied to TRFs coefficients (as explained in line 710), with TRFs trained on axial gradiometers? This has downstream consequences that lead to a lack of clarity in the results. For example, the MEG data shows positive values in the repetition kernel which if the transformation was applied before the TRF would imply that repetition increases activity rather than reduces it as would be expected. From this, I infer that it is the TRF kernels that were transformed. I recognize that the authors can infer results regarding directionality in the EEG data but this analysis choice results in an unnecessary loss of information in the MEG analysis. Please clarify the methods and if the gradiometer transformation is performed on the kernels, I would recommend re-running the analysis with gradiometer transformation first.</p></disp-quote><p>Indeed, we estimated the TRFs on the original axial gradient data and subsequently (1) (linearly) transformed those axial TRFs to planar gradient data and (2) (nonlinearly) combined the resulting synthetic horizontal and vertical planar gradient components to a single magnitude per original MEG sensor. This has, first of all, the advantage that we can perform source analysis straightforwardly on the axial-gradient TRFs (analogous to an axial-gradient ERF). Most importantly, however, this order of operations prevents the amplification of noise that would result from executing the non-linear combination step (2) on the continuous, raw MEG data. For this latter reason, this order of operations is the de facto standard in ERF research (as well as in other recent studies employing TRFs or “regression ERFs”).</p><p>Estimating the TRFs on the non-combined synthetic planar gradient data (so after step 1, but before step 2) would not suffer from this noise amplification issue. However, since step 1 is linear, this would yield exactly the same results as the current order of operations, while doubling the computational cost of the regression.</p><p>We apologize for being unclear on the exact order of operations regarding the planar gradient transformation in the original manuscript and have now clarified this.</p><disp-quote content-type="editor-comment"><p>– The Baseline model claims to include all relevant acoustic information but it does not include the note pitches themselves. As these pitches are provided to the model, it is difficult to tell whether the effects of surprise are related to model output predictions, or how well they represent their input. If the benefits from the models rely truly on their predictive aspect then including pitch information in the baseline model would be an important control. The authors have done their analysis in a way that fits what previous work has done but I think it is a good time to correct this error.</p></disp-quote><p>See our response in “Essential revisions” above.</p><disp-quote content-type="editor-comment"><p>– I wonder if the authors could discuss a bit more about the self-attention component of the Music Transformer model. I'm not familiar with the model's inner workings but I am intrigued by this dichotomy of a larger context to improve musical predictions and a shorter context to improve neural predictions. I wonder if a part of this dissociation has to do with the self-attention feature of Transformers, that a larger context is needed to have a range of motifs to draw from but the size of the motifs that the model attends to should be of a certain size to fit neural predictions.</p></disp-quote><p>We were similarly intrigued by this dissociation; however, we believe the most likely explanation of the dissociation is neural in origin, rather than reflecting the self-attention mechanism. This mechanism indeed allows the transformer to “attend” preceding notes in various ways: a prediction might, for example, be based on a sequence of several preceding notes, while taking into account several notes (or motifs) from many notes ago, but nothing in between. We thus agree that large contexts allow the MT to “have a range of motifs to draw from”. This is a good way to put it, since the model itself ‘decides’ which part of the context to ‘draw from’.</p><p>However, we do not think this can explain the dissociation. Instead, we observe that increasing the context ‘seen’ by the MT steadily improves its predictions (Figure 2b), and we suggest that at some point (over 5-10 notes of context), the MT expectations become more sophisticated than the expectations reflected in the MEG signal. This does not mean they are more sophisticated than human listeners’ expectations: humans can clearly track and appreciate patterns in music much longer and richer than the MT can. However, it seems that such high-level, long-range, likely hierarchical expectations are not driving the surprise effects in the evoked responses, which instead seem to reflect more low-level predictions over shorter temporal scales. The neural processing of the highest-level, longest-range predictions are likely not time-locked to the onset of musical notes, which precludes these being detected with the techniques used in the present study.</p><p>Finally, another reason to believe that the dissociation between context-versus-music-prediction and context-versus-brain-response is not driven by the specific details of the MT is that the same dissociation is observed for the IDyOM models. There, the pattern is much less clear because the musical prediction performance plateaus early, such that the musical predictions never become ‘much smarter’ than the neural predictions. The same pattern is nonetheless observed: musical prediction continues to improve for longer contexts than the neural prediction.</p><disp-quote content-type="editor-comment"><p>– Figure 3C: it could be interesting to show the same figure for IDyOM ltm. Given that context length does not impact ltm as much as MT, we could obtain different results. Since IDyOM ltm gets similar results to MT on the MEG data (figure 3A, no significant difference), it is thus hard to tell if the influence of context length comes from brain processing or the way MT works.</p></disp-quote><p>In <xref ref-type="fig" rid="sa2fig2">Author response image 2</xref> we have added a plot showing the predictive performance of the IDyOM ltm model on MEG versus music data similar to Figure 3C for the Music Transformer. We believe the MT to be the more sensitive measure of context dependency, for reasons outlined earlier. For that reason, we have decided not to add it to the manuscript. Furthermore, even though this particular plot is not included, the exact same traces feature as part of Figures 2B and 3B, so the information is present in the manuscript nonetheless.</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80935-sa2-fig2-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>– Figure 8: Adding the uncertainty estimates did not improve the model's predictive performance compared to surprise alone, but what about TRFs trained on uncertainty without surprise? Without this result, it is hard to understand why the surprise was chosen over uncertainty.</p></disp-quote><p>We did not investigate regression models using only the uncertainty regressor for two reasons. First, we were <italic>a priori</italic> primarily interested in the neural response to surprise, rather than uncertainty. Surprise is a much more direct index of content-based expectations and their violation than (unspecific) uncertainty, and since our theoretical interest is in content-based expectations, we focused on the former.</p><p>Second, we did explore the effect of uncertainty as a secondary interest, but found that adding uncertainty to the regression model not only did not improve the cross-validated performance, but actually <italic>worsened</italic> it (Figure 8B). Surprise and uncertainty were modestly correlated (Figure 8A), and therefore the most likely interpretation of this drop in cross-validated performance is that uncertainty truly does not explain additional neural variance. (That is, any neural variance it would explain on its own is likely due to its correlation with surprise; if it were to capture unique neural variance by itself, then the performance of the joint model would be at least as high as the model featuring only surprise.) For this <italic>a posteriori</italic> reason, in addition to the <italic>a priori</italic> reason already formulated, we did not further explore regression models featuring only uncertainty.</p><disp-quote content-type="editor-comment"><p>– The sentence from lines 272 to 274 is not clear. In particular, the late negativity effect seems to be present in EEG data only, and it is thus hard to understand why a negative correlation between surprise estimates of subsequent notes would have such an effect in EEG and not MEG. Moreover, the same late negativity effect can be seen on the TRF of note onset but is not discussed.</p></disp-quote><p>We apologize for the unclarity here. We emphasize that any judgements regarding the polarity of the effects are based on the EEG data (Figure 6), as well as inspection of the axial-gradient MEG data (not shown). This was already made explicit in the manuscript around lines 259-261. We have now rephrased the relevant passage. We hope that this should alleviate any worry of a discrepancy between the MEG and EEG results.</p><p>Regarding the same late negativity for the Onset regressor: we do already emphasize the presence of this deflection (line 268), but since our interest is in the modulation of neural response by surprise (and, to a lesser extent, repetitions, which are related to surprise), we do not reflect on it in any further detail. Note that the presence of this deflection in the Onset TRF does not lessen the importance of its presence in the Surprise TRF – the latter remains indication of this particular peak being modulated by musical surprise.</p><disp-quote content-type="editor-comment"><p>– Some of the choices for the Temperley Model seem to unnecessarily oversimplify and restrict its potential performance. In the second principle, the model only assesses the relationships between neighboring notes when the preceding note is the tonal centroid. It would seem more prudent to include all notes to collect further data. In the third principle, the model marginalizes major and minor scales by weighting probabilities of each profile by the frequency of major and minor pieces in the database. Presumably, listeners can identify the minor or major key of the current piece (at least implicitly). Why not select the model for each piece, outright?</p></disp-quote><p>We used the Temperley model as David Temperley and others have formulated and applied it in previous research. While some of these choices could indeed be debated, we aimed to use the model in line with the existing literature, which has demonstrated the capabilities of the model in a variety of tasks, such as pitch prediction, key finding, or explaining behavioural ratings of melodic surprise (Morgan et al., 2019b; Temperley, 2008, 2014). We have now explicitly mentioned that the specifics in the three principles were chosen in accordance with earlier work.</p><disp-quote content-type="editor-comment"><p>– Stimulus: Many small details make it so that the stimuli are not so naturalistic (MIDI velocity set to 100, monophonic, mono channel…). This results in a more controlled experiment, but the claim that it expands melodic expectation findings to naturalistic music listening is a bit bold.</p></disp-quote><p>We agree, and do not wish to make the claim that the stimuli we used are representative of the full breadth of music that humans may encounter in everyday life. However, we do maintain that these stimuli are considerably closer to naturalistic music than much previous work on the neural basis of (the role of expectations in) music processing. It could be argued that the most severe limitation to a broad claim of ‘naturalistic’ is the use of strictly monophonic music. This was a technical necessity given two of the three model classes (IDyOM, Temperley). An important contribution of our work is to demonstrate that a different model (MusicTransformer) does at least equally well as the previous state-of-the-art. Critically, the MT support the processing of polyphonic music, and our work thus paves the way for future studies investigating neural expectations in music more representative of that which is encountered in daily life. In accordance with the reviewer’s suggestion, we have now nuanced our claim of ‘naturalistic’ in the Discussion.</p><disp-quote content-type="editor-comment"><p>– Line 478: authors refer to &quot;original compositions&quot; which may give the impression that the pieces were written for the experiment. From the rest of the text, I don't believe this to be true.</p></disp-quote><p>The reviewer is correct; this is now fixed.</p><disp-quote content-type="editor-comment"><p>– Formula line 553: the first probability of Xt (on the left) should also be a conditional probability of Xt given previous values of x. This is the entropy of the probability distribution estimated by the model.</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>– Line 667: the study by Di Liberto from which the EEG data come uses a ridge regression (ridge regularization). Is there a reason to use a non-regularized regression in this case? This should be discussed in the methods.</p></disp-quote><p>See our response in “Essential revisions” above.</p><p>References</p><p>Bharucha, J. J. (1987). Music cognition and perceptual facilitation: A connectionist framework. <italic>Music Perception</italic>, <italic>5</italic>, 1–30. https://doi.org/10.2307/40285384</p><p>Broderick, M. P., Anderson, A. J., Di Liberto, G. M., Crosse, M. J., and Lalor, E. C. (2018). Electrophysiological Correlates of Semantic Dissimilarity Reflect the Comprehension of Natural, Narrative Speech. <italic>Current Biology</italic>, <italic>28</italic>(5), 803-809.e3. https://doi.org/10.1016/j.cub.2018.01.080</p><p>Cohen, J. E. (1962). Information theory and music. <italic>Behavioral Science</italic>, <italic>7</italic>(2), 137–163.</p><p>https://doi.org/10.1002/bs.3830070202</p><p>Conklin, D., and Witten, I. H. (1995). Multiple viewpoint systems for music prediction. <italic>Journal of New Music Research</italic>, <italic>24</italic>(1), 51–73. https://doi.org/10.1080/09298219508570672</p><p>Heilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P., and de Lange, F. P. (2022). A hierarchy of linguistic predictions during natural language comprehension. <italic>Proceedings of the National Academy of Sciences</italic>, <italic>119</italic>(32), e2201968119. https://doi.org/10.1073/pnas.2201968119</p><p>Huang, C.-Z. A., Vaswani, A., Uszkoreit, J., Shazeer, N., Simon, I., Hawthorne, C., Dai, A. M., Hoffman, M. D., Dinculescu, M., and Eck, D. (2018). Music Transformer. <italic>ArXiv:1809.04281 [Cs, Eess, Stat]</italic>.</p><p>http://arxiv.org/abs/1809.04281</p><p>Krumhansl, C. L. (2015). Statistics, Structure, and Style in Music. <italic>Music Perception</italic>, <italic>33</italic>(1), 20–31. https://doi.org/10.1525/mp.2015.33.1.20</p><p>Liberto, G. M. D., Pelofi, C., Shamma, S., and Cheveigné, A. de. (2020). Musical expertise enhances the cortical tracking of the acoustic envelope during naturalistic music listening. <italic>Acoustical Science and Technology</italic>, <italic>41</italic>(1), 361–364. https://doi.org/10.1250/ast.41.361</p><p>Meyer, L. B. (1957). <italic>Emotion and Meaning in Music</italic>. University of Chicago Press.</p><p>Morgan, E., Fogel, A., Nair, A., and Patel, A. D. (2019a). Statistical learning and Gestalt-like principles predict melodic expectations. <italic>Cognition</italic>, <italic>189</italic>, 23–34. https://doi.org/10.1016/j.cognition.2018.12.015</p><p>Morgan, E., Fogel, A., Nair, A., and Patel, A. D. (2019b). Statistical learning and Gestalt-like principles predict melodic expectations. <italic>Cognition</italic>, <italic>189</italic>, 23–34. https://doi.org/10.1016/j.cognition.2018.12.015 Narmour, E. (1990). <italic>The analysis and cognition of basic melodic structures: The implication-realization model</italic> (pp. xiv, 485). University of Chicago Press.</p><p>Narmour, E. (1992). <italic>The Analysis and Cognition of Melodic Complexity: The Implication-Realization Model</italic>. University of Chicago Press.</p><p>Pearce, M. T., and Wiggins, G. A. (2012). Auditory Expectation: The Information Dynamics of Music Perception and Cognition. <italic>Topics in Cognitive Science</italic>, <italic>4</italic>(4), 625–652. https://doi.org/10.1111/j.17568765.2012.01214.x</p><p>Planton, S., Kerkoerle, T. van, Abbih, L., Maheu, M., Meyniel, F., Sigman, M., Wang, L., Figueira, S., Romano, S., and Dehaene, S. (2021). A theory of memory for binary sequences: Evidence for a mental compression algorithm in humans. <italic>PLOS Computational Biology</italic>, <italic>17</italic>(1), e1008598. https://doi.org/10.1371/journal.pcbi.1008598</p><p>Schellenberg, E. G. (1997). Simplifying the Implication-Realization Model of Melodic Expectancy. <italic>Music Perception: An Interdisciplinary Journal</italic>, <italic>14</italic>(3), 295–318. JSTOR. https://doi.org/10.2307/40285723</p><p>Schmitt, L.-M., Erb, J., Tune, S., Rysop, A. U., Hartwigsen, G., and Obleser, J. (2021). Predicting speech from a cortical hierarchy of event-based time scales. <italic>Science Advances</italic>, <italic>7</italic>(49), eabi6070. https://doi.org/10.1126/sciadv.abi6070</p><p>Smith, N. J., and Kutas, M. (2015). Regression-based estimation of ERP waveforms: I. The rERP framework. <italic>Psychophysiology</italic>, <italic>52</italic>(2), 157–168. https://doi.org/10.1111/psyp.12317</p><p>Temperley, D. (2008). A Probabilistic Model of Melody Perception. <italic>Cognitive Science</italic>, <italic>32</italic>(2), 418–444. https://doi.org/10.1080/03640210701864089</p><p>Temperley, D. (2014). Probabilistic Models of Melodic Interval. <italic>Music Perception</italic>, <italic>32</italic>(1), 85–99. https://doi.org/10.1525/mp.2014.32.1.85</p><p>Wilcox, E. G., Gauthier, J., Hu, J., Qian, P., and Levy, R. (2020). <italic>On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior</italic> (arXiv:2006.01912). arXiv. https://doi.org/10.48550/arXiv.2006.01912</p></body></sub-article></article>