<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">59161</article-id><article-id pub-id-type="doi">10.7554/eLife.59161</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Analysis of ultrasonic vocalizations from mice using computer vision and machine learning</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-189085"><name><surname>Fonseca</surname><given-names>Antonio HO</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7791-8010</contrib-id><email>antonio.fonseca@yale.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa1">†</xref></contrib><contrib contrib-type="author" corresp="yes" id="author-189086"><name><surname>Santana</surname><given-names>Gustavo M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1897-1625</contrib-id><email>gustavo.santana@yale.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa1">†</xref></contrib><contrib contrib-type="author" corresp="yes" id="author-229378"><name><surname>Bosque Ortiz</surname><given-names>Gabriela M</given-names></name><email>gabriela.borque@yale.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund10"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-189087"><name><surname>Bampi</surname><given-names>Sérgio</given-names></name><email>bampi@inf.ufrgs.br</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-7600"><name><surname>Dietrich</surname><given-names>Marcelo O</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9781-2221</contrib-id><email>marcelo.dietrich@yale.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund10"/><xref ref-type="other" rid="fund11"/><xref ref-type="other" rid="fund12"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Laboratory of Physiology of Behavior, Department of Comparative Medicine, Yale School of Medicine</institution><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Institute of Informatics, Federal University of Rio Grande do Sul</institution><addr-line><named-content content-type="city">Porto Alegre</named-content></addr-line><country>Brazil</country></aff><aff id="aff3"><label>3</label><institution>Graduate Program in Biological Sciences - Biochemistry, Federal University of Rio Grande do Sul</institution><addr-line><named-content content-type="city">Porto Alegre</named-content></addr-line><country>Brazil</country></aff><aff id="aff4"><label>4</label><institution>Interdepartmental Neuroscience Program, Biological and Biomedical Sciences Program, Graduate School in Arts and Sciences, Yale University</institution><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution>Department of Neuroscience, Yale School of Medicine</institution><addr-line><named-content content-type="city">Porto Alegre</named-content></addr-line><country>Brazil</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kimchi</surname><given-names>Tali</given-names></name><role>Reviewing Editor</role><aff><institution>Weizmann Institute of Science</institution><country>Israel</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Interdepartmental Neuroscience Program, Yale School of Medicine, New Haven, United States</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>31</day><month>03</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e59161</elocation-id><history><date date-type="received" iso-8601-date="2020-05-21"><day>21</day><month>05</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-03-30"><day>30</day><month>03</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Fonseca et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Fonseca et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-59161-v2.pdf"/><abstract><p>Mice emit ultrasonic vocalizations (USVs) that communicate socially relevant information. To detect and classify these USVs, here we describe VocalMat. VocalMat is a software that uses image-processing and differential geometry approaches to detect USVs in audio files, eliminating the need for user-defined parameters. VocalMat also uses computational vision and machine learning methods to classify USVs into distinct categories. In a data set of &gt;4000 USVs emitted by mice, VocalMat detected over 98% of manually labeled USVs and accurately classified ≈86% of the USVs out of 11 USV categories. We then used dimensionality reduction tools to analyze the probability distribution of USV classification among different experimental groups, providing a robust method to quantify and qualify the vocal repertoire of mice. Thus, VocalMat makes it possible to perform automated, accurate, and quantitative analysis of USVs without the need for user inputs, opening the opportunity for detailed and high-throughput analysis of this behavior.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>machine learning</kwd><kwd>vocalization</kwd><kwd>communication</kwd><kwd>social behavior</kwd><kwd>computer vision</kwd><kwd>ultrasonic vocalization</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000062</institution-id><institution>National Institute of Diabetes and Digestive and Kidney Diseases</institution></institution-wrap></funding-source><award-id>R01DK107916</award-id><principal-award-recipient><name><surname>Dietrich</surname><given-names>Marcelo O</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000874</institution-id><institution>Brain and Behavior Research Foundation</institution></institution-wrap></funding-source><award-id>NARSAD Young Investigator Grant ID 22709</award-id><principal-award-recipient><name><surname>Dietrich</surname><given-names>Marcelo O</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001391</institution-id><institution>Whitehall Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Dietrich</surname><given-names>Marcelo O</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001680</institution-id><institution>Charles H. Hood Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Dietrich</surname><given-names>Marcelo O</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100002889</institution-id><institution>Foundation for Prader-Willi Research</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Dietrich</surname><given-names>Marcelo O</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>Reginald and Michiko Spector Award in Neuroscience</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Dietrich</surname><given-names>Marcelo O</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100013043</institution-id><institution>Yale Center for Clinical Investigation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Dietrich</surname><given-names>Marcelo O</given-names></name></principal-award-recipient></award-group><award-group id="fund11"><funding-source><institution-wrap><institution>Yale Diabetes Research Center</institution></institution-wrap></funding-source><award-id>P30DK045735</award-id><principal-award-recipient><name><surname>Dietrich</surname><given-names>Marcelo O</given-names></name></principal-award-recipient></award-group><award-group id="fund12"><funding-source><institution-wrap><institution>Modern Diet and Physiology Research Center (The John B. Pierce Laboratory)</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Dietrich</surname><given-names>Marcelo O</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002322</institution-id><institution>Coordenação de Aperfeiçoamento de Pessoal de Nível Superior</institution></institution-wrap></funding-source><award-id>Finance Code 001</award-id><principal-award-recipient><name><surname>Fonseca</surname><given-names>Antonio HO</given-names></name><name><surname>Santana</surname><given-names>Gustavo M</given-names></name><name><surname>Bampi</surname><given-names>Sérgio</given-names></name><name><surname>Dietrich</surname><given-names>Marcelo O</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003593</institution-id><institution>Conselho Nacional de Desenvolvimento Científico e Tecnológico</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Bampi</surname><given-names>Sérgio</given-names></name><name><surname>Dietrich</surname><given-names>Marcelo O</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000011</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap></funding-source><award-id>Gilliam Fellowship</award-id><principal-award-recipient><name><surname>Bosque Ortiz</surname><given-names>Gabriela M</given-names></name><name><surname>Dietrich</surname><given-names>Marcelo O</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>An open-source tool based on computational vision and machine learning shows high accuracy and sensitivity in the analysis of ultrasonic vocalizations.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In animals, vocal communication transmits information about the state of the caller and influences the state of the listener. This information can be relevant for the identification of individuals or groups (<xref ref-type="bibr" rid="bib19">Hoffmann et al., 2012</xref>); status within the group (e.g., dominance, submission, fear, or aggression; <xref ref-type="bibr" rid="bib25">Nyby et al., 1976</xref>); next likely behavior (e.g., approach, flee, play, or mount; <xref ref-type="bibr" rid="bib23">Neunuebel et al., 2015</xref>); environmental conditions (e.g., presence of predators, location of food; <xref ref-type="bibr" rid="bib34">Slobodchikoff et al., 2012</xref>); and facilitation of mother–offspring interactions (<xref ref-type="bibr" rid="bib12">D'Amato et al., 2005</xref>).</p><p>Mice emit ultrasonic vocalizations (USVs) in a frequency range (≈30–110 kHz) above the human hearing range (≈2–20 kHz) (<xref ref-type="bibr" rid="bib41">Zippelius and Schleidt, 1956</xref>; <xref ref-type="bibr" rid="bib24">Noirot, 1972</xref>; <xref ref-type="bibr" rid="bib25">Nyby et al., 1976</xref>; <xref ref-type="bibr" rid="bib27">Nyby et al., 1977b</xref>; <xref ref-type="bibr" rid="bib26">Nyby et al., 1977a</xref>; <xref ref-type="bibr" rid="bib30">Sales and Smith, 1978</xref>; <xref ref-type="bibr" rid="bib5">Branchi et al., 2001</xref>; <xref ref-type="bibr" rid="bib18">Hahn and Lavooy, 2005</xref>; <xref ref-type="bibr" rid="bib13">Ehret, 2005</xref>; <xref ref-type="bibr" rid="bib6">Branchi et al., 2006</xref>). These USVs are organized in <italic>phrases</italic> or <italic>bouts</italic> composed of sequences of <italic>syllables</italic>. The syllables are defined as continuous units of sound not interrupted by a period of silence. The syllables are composed of one or more notes and are separated by salient pauses and occur as part of sequences (<xref ref-type="bibr" rid="bib2">Arriaga et al., 2012</xref>; <xref ref-type="bibr" rid="bib20">Holy and Guo, 2005</xref>). These transitions across syllables do not occur randomly (<xref ref-type="bibr" rid="bib20">Holy and Guo, 2005</xref>; <xref ref-type="bibr" rid="bib8">Castellucci et al., 2018</xref>), and the changes in the sequences, prevalence, and acoustic structure of syllables match current behavior (<xref ref-type="bibr" rid="bib9">Chabout et al., 2015</xref>), genetic strain (<xref ref-type="bibr" rid="bib38">Van Segbroeck et al., 2017</xref>; <xref ref-type="bibr" rid="bib32">Scattoni et al., 2011</xref>), and developmental stage (<xref ref-type="bibr" rid="bib17">Grimsley et al., 2011</xref>). USVs are commonly emitted by mouse pups when separated from the home nest (<xref ref-type="bibr" rid="bib31">Scattoni et al., 2008</xref>) and are modulated during development (<xref ref-type="bibr" rid="bib17">Grimsley et al., 2011</xref>; <xref ref-type="bibr" rid="bib14">Elwood and Keeling, 1982</xref>; <xref ref-type="bibr" rid="bib8">Castellucci et al., 2018</xref>). In the adult mouse, USVs are emitted in both positive and negative contexts (<xref ref-type="bibr" rid="bib3">Arriaga and Jarvis, 2013</xref>). Thus, understanding the complex structure of USVs will advance vocal and social communications research.</p><p>In the past years, tools for USV analysis advanced significantly (<xref ref-type="bibr" rid="bib10">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib38">Van Segbroeck et al., 2017</xref>; <xref ref-type="bibr" rid="bib23">Neunuebel et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Chabout et al., 2015</xref>; <xref ref-type="bibr" rid="bib2">Arriaga et al., 2012</xref>; <xref ref-type="bibr" rid="bib35">Tachibana et al., 2020</xref>). For the detection of USVs in audio recordings, the majority of the software tools available depend on user inputs (<xref ref-type="bibr" rid="bib23">Neunuebel et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Van Segbroeck et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Tachibana et al., 2020</xref>) or present limited detection capabilities (<xref ref-type="bibr" rid="bib2">Arriaga et al., 2012</xref>; <xref ref-type="bibr" rid="bib9">Chabout et al., 2015</xref>). An exception is DeepSqueak (<xref ref-type="bibr" rid="bib10">Coffey et al., 2019</xref>), which uses a neural network-based method for the detection of USVs from audio recordings. For the classification of USVs, different tools use supervised classification (<xref ref-type="bibr" rid="bib2">Arriaga et al., 2012</xref>; <xref ref-type="bibr" rid="bib9">Chabout et al., 2015</xref>; <xref ref-type="bibr" rid="bib10">Coffey et al., 2019</xref>) and unsupervised clustering (<xref ref-type="bibr" rid="bib7">Burkett et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Van Segbroeck et al., 2017</xref>; <xref ref-type="bibr" rid="bib10">Coffey et al., 2019</xref>) methods to assign USVs to different groups. However, no consensus exists on the biological function of the various USV subclasses, making it challenging to develop a tool for all purposes. Additionally, the accuracy of classification methods depends on the accurate detection of vocalizations in audio recordings, which remains a challenge in experimental conditions that include noise and low intensity vocalization. To overcome these limitations, our goal was to create a tool that does not need user inputs to detect and classify USVs with high accuracy and that extracts the spectral features of the USVs with precision.</p><p>Here, we describe the development of VocalMat, a software for robust and automated detection and classification of mouse USVs from audio recordings. VocalMat uses image processing and differential geometry approaches to detect USVs in spectrograms, eliminating the need for parameter tuning. VocalMat shows high accuracy in detecting USVs in manually validated audio recordings, preserving quantitative measures of their spectral features. This high accuracy allows the use of multiple tools for USV classification. In the current version of VocalMat, we embedded a supervised classification method that uses computer vision techniques and machine learning to label each USV into 11 classes. Thus, VocalMat is a highly accurate software to detect and classify mouse USVs in an automated and flexible manner.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Detection of mouse USVs using imaging processing</title><p>VocalMat uses multiple steps to analyze USVs in audio files (see <xref ref-type="fig" rid="fig1">Figure 1A</xref> for the general workflow). Initially, the audio recordings are converted into high-resolution spectrograms through a short-time Fourier transformation (see Materials and methods). The resulting spectrogram consists of a matrix, wherein each element corresponds to an intensity value (power spectrum represented in decibels) for each time-frequency component. The spectrogram is then analyzed as a gray-scale image, where high-intensity values are represented by brighter pixels and low-intensity values by darker pixels (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The gray-scale image undergoes contrast enhancement and adaptive thresholding for binarization (see Materials and methods). The segmented objects are further refined via morphological operations (<xref ref-type="fig" rid="fig1">Figure 1C</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), thus resulting in a list of segmented blobs (hereafter referred to as USV candidates) with their corresponding spectral features (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Finally, because experimental observations demonstrate a minimum of 10 ms of interval between two successive and distinct USVs, we combined into a single USV candidate blobs that were separated for less than 10 ms. The final list of USV candidates contain real USVs and noise (i.e., detected particles that are not part of any USV).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview of the VocalMat pipeline for ultrasonic vocalization (USV) detection and analysis.</title><p>(<bold>A</bold>) Workflow of the main steps used by VocalMat, from audio acquisition to data analysis. (<bold>B</bold>) Illustration of a segment of spectrogram. The time-frequency plan is depicted as a gray scale image wherein the pixel values correspond to intensity in decibels. (<bold>C</bold>) Example of segmented USV after contrast enhancement, adaptive thresholding, and morphological operations (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for further details of the segmentation process). (<bold>D</bold>) Illustration of some of the spectral information obtained from the segmentation. Information on intensity is kept for each time-frequency point along the segmented USV candidate.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59161-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Image processing pipeline for segmentation of ultrasonic vocalizations (USVs) in spectrograms.</title><p>Image processing pipeline for segmentation of USVs in spectrograms. (<bold>A</bold>) Segment of a spectrogram post contrast adjustment (<inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>B</bold>) Output image post binarization using adaptive thresholding. (<bold>C</bold>) Resulting image from the opening operation with rectangle 4 × 2. (<bold>D</bold>) Result from the dilation with line <italic>l</italic> = 4 and <inline-formula><mml:math id="inf2"><mml:mi mathvariant="normal">∠</mml:mi></mml:math></inline-formula> 90°. (<bold>E</bold>) Removal of too small objects (≤60 pixels), mean of cloud points for each detected USV candidate being shown in red and green lines shows an interval of 10 ms. (<bold>F</bold>) Result after separating syllables based on the criterion of maximum interval between two tones in a syllable. The different colors differentiate the syllables from each other.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59161-fig1-figsupp1-v2.tif"/></fig></fig-group><p>To reduce the amount of data stored for each USV, the features extracted from detected candidates are represented by a mean frequency and intensity every 0.5 ms. The means are calculated for all the individual candidates, including the ones overlapping in time, hence preserving relevant features such as duration, frequency, intensity, and harmonic components (<xref ref-type="fig" rid="fig1">Figure 1D</xref>).</p><p>Harmonic components are also referred to as nonlinear components or composite (<xref ref-type="bibr" rid="bib31">Scattoni et al., 2008</xref>). Here, we did not consider harmonic components as a different USV, but rather as an extra feature of a USV (<xref ref-type="bibr" rid="bib17">Grimsley et al., 2011</xref>). Therefore, each detected USV candidate may or may not present a harmonic component. A harmonic component was considered as a continuous sound (i.e., no discontinuities in time and/or frequency) overlapping in time with the main component of the USV (similar to <xref ref-type="bibr" rid="bib17">Grimsley et al., 2011</xref>).</p><p>Besides the list of USV candidates and their spectral features, the segmentation process also exports image files of 227 × 227 pixels, in which the USV candidate is centralized in windows of 220 ms (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>). This temporal length is defined as twice the maximum duration of USVs observed in mice (<xref ref-type="bibr" rid="bib17">Grimsley et al., 2011</xref>), thus preventing cropping.</p></sec><sec id="s2-2"><title>Eliminating noise using a contrast filter</title><p>Initially, we used VocalMat to detect USVs in a set of 64 audio recordings. These recordings were composed of experiments using mice of different strains, age, sex, and in a variety of experimental conditions (e.g., recorded inside a chamber that produce high levels of environmental noise) to increase the variability of the data set. In this data set, VocalMat initially detected a pool of 59,781 USV candidates, which includes real USVs and noise (<xref ref-type="fig" rid="fig2">Figure 2A</xref> and Materials and methods). Visual inspection of the data set revealed that artifacts generated during the segmentation process dominated the pool of USV candidates (see <xref ref-type="fig" rid="fig2">Figure 2B</xref> for examples of real USVs and noise in the pool of USV candidates). This type of artifact is characterized by its low intensity compared to real USVs. To remove these artifacts from the pool of USV candidates, we applied a <italic>Local Median Filter</italic> step, a method to estimate the minimum expected contrast between a USV and its background for each audio recording. This contrast is calculated based on the median intensity of the pixels in each detected USV candidate <inline-formula><mml:math id="inf3"><mml:mi>k</mml:mi></mml:math></inline-formula> (referred to as <inline-formula><mml:math id="inf4"><mml:mover accent="true"><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>), and the median intensity of the background pixels in a bounding box containing the candidate <inline-formula><mml:math id="inf5"><mml:mi>k</mml:mi></mml:math></inline-formula> (referred to as <inline-formula><mml:math id="inf6"><mml:mover accent="true"><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>) (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Thus, the contrast is defined as the ratio <inline-formula><mml:math id="inf7"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover><mml:mo>/</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Noise elimination process for ultrasonic vocalization (USV) candidates.</title><p>(<bold>A</bold>) In a set of 64 audio files, VocalMat identified 59,781 USV candidates. (<bold>B</bold>) Examples of USVs among the pool of candidates that were manually labeled as either noise or real USVs. The score (upper-right corner) indicates the calculated contrast <inline-formula><mml:math id="inf8"><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> for the candidate. (<bold>C</bold>) Example of contrast calculation ( <inline-formula><mml:math id="inf9"><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>) for a given USV candidate <inline-formula><mml:math id="inf10"><mml:mi>k</mml:mi></mml:math></inline-formula>. The red dots indicate the points detected as part of the USV candidate (<inline-formula><mml:math id="inf11"><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>) and the dashed-white rectangle indicates its evaluated neighborhood (<inline-formula><mml:math id="inf12"><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>). (<bold>D</bold>) Distribution of the <inline-formula><mml:math id="inf13"><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> for the USV candidates in the <italic>test</italic> data set. (<bold>E</bold>) Each USV candidate was manually labeled as real USV or noise. The distribution of <inline-formula><mml:math id="inf14"><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> for the real USVs (cyan) compared to the distribution for all the USV candidates (red) in the <italic>test</italic> data set. The blue line indicates the cumulative distribution function (CDF) of <inline-formula><mml:math id="inf15"><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> for all the USV candidates. The inflection point of the CDF curve is indicated by the arrow. (<bold>F</bold>) Example of a segment of spectrogram with three USVs. The analysis of this segment without the ’Local Median Filter’ results in an elevated number of false positives (noise detected as USV). ’Red’ and ’cyan’ ticks denote the time stamp of the identified USV candidates without and with the ’Local Median Filter’, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59161-fig2-v2.tif"/></fig><p>To validate this method, a group of investigators trained to detect USVs manually inspected the spectrograms and labeled the USVs in a subset of seven randomly selected audio recordings (hereafter referred to as <italic>test</italic> data set and described in Table 3). Each USV was labeled by at least two investigators and when discrepancy occurred, both individuals reviewed their annotation under the supervision of a third investigator. (This data set contains fully validated audio recordings with manually inspected USVs and is publicly available to facilitate the development and the test of performance of new tools.)</p><p>In the <italic>test</italic> data set, a total of 7741 USV candidates were detected using the segmentation process described above, representing 1.75 times more USV candidates than the manual counting (4441 USVs). Importantly, the segmentation step included 4428 real USVs within the pool of USV candidates, therefore missing 13 USVs compared to the ground-truth.</p><p>The distribution of <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> for real USVs and for noise showed that the peak at high <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> (i.e., low contrast) in the distribution was dominated by USV candidates corresponding to noise (<xref ref-type="fig" rid="fig2">Figure 2D,E</xref>). The <inline-formula><mml:math id="inf18"><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> of real USVs (mean = 0.642, SEM = 1.841 <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi/><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, median = 0.640, 95% CI [0.638, 0.646]; N = 4,428) was significantly lower than the <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> of noise (mean = 0.922, SEM = 9.605 <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi/><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, median = 0.936, 95% CI [0.921, 0.924]; n = 3,336; p &lt; 10<sup>−15</sup>, <italic>D</italic> = 0.894, Kolmogorov–Smirnov test; <xref ref-type="fig" rid="fig2">Figure 2D,E</xref>). This unbalanced bimodal distribution causes an inflection point on the cumulative distribution function (CDF) of <inline-formula><mml:math id="inf22"><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> that matches the ratio observed for segmentation artifacts (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). Therefore, based on these results, we used the automatically calculated inflection point, which is specific to each audio recording, as a threshold to effectively eliminate a substantial amount of noise from the pool of USV candidates (details on this calculation are provided in Materials and methods).</p><p>In the <italic>test</italic> data set, 5171 out of 7741 USV candidates survived the <italic>Local Median Filter</italic> step. This number includes real USVs (4421; out of the 4428 real USVs after automatic segmentation) and remaining noises of lower <inline-formula><mml:math id="inf23"><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>. Thus, this step eliminated seven real USVs of the pool of candidates, all of which presented a high <inline-formula><mml:math id="inf24"><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> (mean = 0.942, SEM = 5.871 <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi/><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, median = 0.943, 95% CI [0.927, 0.956]; n = 7). The remaining noises among the pool of candidates had high intensity and were commonly originated from external sources (<xref ref-type="fig" rid="fig2">Figure 2B,E</xref>).</p><p>To illustrate the performance of the <italic>Local Median Filter</italic> step, <xref ref-type="fig" rid="fig2">Figure 2F</xref> shows a segment of a spectrogram with 11 USV candidates detected and three real USVs. After applying the <italic>Local Median Filter</italic> step, only the real USVs remained in the pool of USV candidates. Thus, the <italic>Local Median Filter</italic> step effectively eliminates segmentation noise from the pool of USV candidates, which provides two main advantages: first, it decreases the number of USV candidates used in downstream analysis and, second, it reduces the number of false positives.</p><p>In an ideal experimental setting with complete sound insulation and without the generation of noise by the movement of the animal, no further step is required to detect USVs using VocalMat. Since this is difficult in experimental conditions, we applied a second step in the noise elimination process.</p></sec><sec id="s2-3"><title>Using convolutional neural network for noise identification</title><p>To identify USVs in the pool of USV candidates that passed the <italic>Local Median Filter</italic> step, we trained a convolutional neural network (CNN) to classify each USV candidate into one of 11 USV categories or noise (see Figure 4A for examples of the different USV categories). We used a data set containing 10,871 samples manually labeled as one of the 11 USV categories and 2083 samples manually labeled as noise (see Materials and methods). The output of the CNN is the probability of each USV candidate belonging to one of the 12 categories. The most likely category defines the label of the USV candidate (<xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>VocalMat ultrasonic vocalization (USV) classification using a convolutional neural network.</title><p>(<bold>A</bold>) Illustration of the AlexNet architecture post end-to-end training on our <italic>training</italic> data set. The last three layers of the network were replaced in order to perform a 12-categories (11 USV types plus noise) classification task. The output of the CNN is a probability distribution over the labels for each input image. (<bold>B</bold>) Linear regression between the number of USVs manually detected versus the number reported by VocalMat for the audio files in our <italic>test</italic> data set (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for individual confusion matrices). (<bold>C</bold>) Distribution of probabilities <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the true positive (green), false positive (red), false negative (cyan), and true negative (magenta). Ticks represent individual USV candidates.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59161-fig3-v2.tif"/></fig><p>To evaluate the performance of VocalMat in distinguishing between USVs and noise, we used the 5171 USV candidates in the <italic>test</italic> data set that passed the <italic>Local Median Filter</italic> step (Materials and methods). We compared the probability for the label <italic>Noise</italic> (<inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) to the sum over the probabilities of the 11 USV categories (<inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). The rate of detected USVs labeled as such (true positive or sensitivity) was 99.04 ± 0.31% (mean ± SEM; median = 99.37; 95% CI [98.27, 99.80]). The rate of detected USVs labeled as noise (false negative) was 0.96 ± 0.31% (mean ± SEM; median = 0.61; 95% CI [0.20, 1.73]). The rate of detected noise labeled as noise (true negative rate or specificity) was 94.40 ± 1.37% (mean ± SEM; median = 95.60; 95% CI [91.60, 97.74]). The rate of detected noise labeled as USV (false positive) was 5.60 ± 1.37% (mean ± SEM; median = 4.40; 95% CI [2.26, 8.94]), representing a total of 42 wrongly detected USVs out of the 5171 USV candidates in the <italic>test</italic> data set. Altogether, the accuracy in identifying USVs was 98.63 ± 0.20% (mean ± SEM; median = 98.55; 95% CI [98.14, 99.11]) for manually validated audio recordings. (The performance of VocalMat in identifying USVs in individual audio recordings is provided in <xref ref-type="table" rid="table1">Table 1</xref>.) Thus, VocalMat presents high accuracy to detect USVs from audio recordings and to remove noise (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), failing to identify approximately 1 in 75 USVs.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Summary of performance of VocalMat in detecting ultrasonic vocalizations (USVs) in the <italic>test</italic> data set.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Audio file</th><th>True positive</th><th>False negative</th><th>True negative</th><th>False positive</th><th>Accuracy (%)</th></tr></thead><tbody><tr><td>1</td><td>316</td><td>1</td><td>58</td><td>2</td><td>99.20</td></tr><tr><td>2</td><td>985</td><td>1</td><td>105</td><td>15</td><td>98.55</td></tr><tr><td>3</td><td>696</td><td>12</td><td>73</td><td>5</td><td>97.84</td></tr><tr><td>4</td><td>862</td><td>13</td><td>51</td><td>4</td><td>98.17</td></tr><tr><td>5</td><td>44</td><td>1</td><td>216</td><td>3</td><td>98.48</td></tr><tr><td>6</td><td>696</td><td>2</td><td>87</td><td>4</td><td>99.24</td></tr><tr><td>7</td><td>787</td><td>5</td><td>122</td><td>5</td><td>98.91</td></tr></tbody></table></table-wrap><p>We further calculated other measures of performance (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). For USVs wrongly labeled as noise (false negative), the probability of being a USV was 0.15 ± 0.03 (mean ± SEM; median = 0.04; 95% CI [0.09, 0.22]; <xref ref-type="fig" rid="fig3">Figure 3C</xref>), while for noise labeled as USV (false positive), the probability of being USV was 0.85 ± 0.03 (mean ± SEM; median = 0.86; 95% CI [0.80, 0.91]; <xref ref-type="fig" rid="fig3">Figure 3C</xref>). These probabilities contrast with cases in which VocalMat correctly identified USV and noise. USVs that were correctly identified had a probability of being USV of 0.99 ± 3.78 × 10<sup>−4</sup> (mean ± SEM; median = 1.00; 95% CI [0.99, 0.99]; <xref ref-type="fig" rid="fig3">Figure 3C</xref>). Noise that was correctly identified had a probability of being noise of 0.99 ± 1.78 × 10<sup>−3</sup> (mean ± SEM; median = 1.00; 95% CI [0.98, 0.99]; <xref ref-type="fig" rid="fig3">Figure 3C</xref>). These results indicate that the probability assigned by VocalMat <italic>flags</italic> likely errors in classification. These <italic>flagged</italic> candidates (i.e., with assigned low probability) can be manually inspected to correct the misclassification and retrain VocalMat.</p></sec><sec id="s2-4"><title>Performance of VocalMat compared to other tools</title><p>Next, we tested the performance of VocalMat in comparison to other software tools. We first searched for validated data sets that were used by previous tools. We obtained 15 audio recordings made publicly available by USVSEG (<xref ref-type="bibr" rid="bib35">Tachibana et al., 2020</xref>) and one audio recording made publicly available by DeepSqueak (<xref ref-type="bibr" rid="bib10">Coffey et al., 2019</xref>).</p><p>The USVSEG data set consisted of five audio recordings of 5–6 days old mouse pups containing 409 USVs and 10 audio recordings of adult mice containing 2401 USVs. The accuracy of VocalMat in identifying USVs in the USVSEG data set was 98.62 ± 0.53 (mean ± SEM, median = 99.61, 95% CI [97.49, 99.76]), which is comparable to the approximately 98% accuracy reported by USVSEG (<xref ref-type="bibr" rid="bib35">Tachibana et al., 2020</xref>). However, VocalMat detected approximately 6% more USVs than USVSEG. More precisely, in the audio recordings from pups, VocalMat presented a true positive rate of 97.53 ± 1.04% (mean ± SEM, median = 98.41, 95% CI [94.65, 100.4]), detecting 15 more USVs than USVSEG. In the audio recordings from adult mice, VocalMat presented a true positive rate of 99.05 ± 0.37% (mean ± SEM, median = 99.39, 95% CI [98.20, 99.90]), detecting 152 more USVs than USVSEG.</p><p>Similar to VocalMat, DeepSqueak uses deep learning to detect USVs (<xref ref-type="bibr" rid="bib10">Coffey et al., 2019</xref>). To directly compare DeepSqueak and VocalMat, we evaluated the performance of both tools on the single audio recording provided by DeepSqueak (<xref ref-type="bibr" rid="bib10">Coffey et al., 2019</xref>). First, we manually inspected the spectrogram and labeled each of the 762 USVs identified. Of these 762 USVs, VocalMat detected 747 with a true positive rate of 91.73%, whereas DeepSqueak detected 608, with a true positive rate of 77.95%. Thus, when tested in data sets from different laboratories, VocalMat shows better sensitivity for USV detection than DeepSqueak and USVSEG.</p><p>As described above, the <italic>test</italic> data set was fully curated by manual identification of USVs. To take advantage of this large data set, we further compared the performance of VocalMat with four tools (see Materials and methods). In addition to USVSEG (<xref ref-type="bibr" rid="bib35">Tachibana et al., 2020</xref>) and DeepSqueak (<xref ref-type="bibr" rid="bib10">Coffey et al., 2019</xref>), we also tested the performance of Ax (<xref ref-type="bibr" rid="bib23">Neunuebel et al., 2015</xref>) and MUPET (<xref ref-type="bibr" rid="bib38">Van Segbroeck et al., 2017</xref>). (<xref ref-type="table" rid="table2">Table 2</xref> summarizes the performance of these tools in our <italic>test</italic> set.)</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Summary of detection performance.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Tool</th><th>Missed ultrasonic vocalizations (USVs) rate (%)</th><th>False discovery rate (%)</th></tr></thead><tbody><tr><td>Ax</td><td>4.99</td><td>37.67</td></tr><tr><td>MUPET</td><td>33.74</td><td>38.78</td></tr><tr><td>USVSEG</td><td>6.53</td><td>7.58</td></tr><tr><td>DeepSqueak</td><td>27.13</td><td>7.61</td></tr><tr><td>VocalMat</td><td>1.64</td><td>0.05</td></tr></tbody></table></table-wrap><p>Ax requires a series of manual inputs for their detection algorithm (<xref ref-type="bibr" rid="bib23">Neunuebel et al., 2015</xref>). Combining the best configurations tested (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>), the percentage of missed USVs was 4.99 ± 1.34% (mean ± SEM; median = 4.07, 95% CI [1.73, 8.26]) and the false discovery rate was 37.67 ± 5.59% (mean ± SEM; median = 42.56, 95% CI [23.99, 51.34]). In comparison to Ax, MUPET has a lower number of parameters to be set by the user. Combining the best configurations tested (<xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>), the percentage of missed USVs was 33.74 ± 3.81% (mean ± SEM; median = 33.13, 95% CI [24.41, 43.07]) and the false discovery rate was 38.78 ± 6.56% (mean ± SEM; median = 32.97, 95% CI [22.72, 54.84]). Similar to Ax and MUPET, USVSEG requires setting parameters manually for USV detection (<xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). USVSEG displayed the best performance out of the manually configured tools, presenting a missed vocalization rate of 6.53 ± 2.56% (mean ± SEM; median = 4.26, 95% CI [0.26, 12.80]) and a false discovery rate of 7.58 ± 4.31% (mean ± SEM; median = 3.27, 95% CI [−2.97, 18.15]). It is important to emphasize that the tests with Ax, MUPET, and USVSEG did not explore all possible combinations of parameters and, therefore, other settings could potentially optimize the performance of these tools to detect USVs in the <italic>test</italic> data set.</p><p>Finally, we compared the performance of DeepSqueak with VocalMat (<xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>). The best values obtained were a rate of missed USVs of 27.13 ± 3.78% (mean ± SEM; median = 24.22, 95% CI [17.86, 36.40]) and a false discovery rate of 7.61 ± 2.35% (mean ± SEM; median = 4.73, 95% CI [1.84, 13.39]). The manual inspection of the USVs detected by DeepSqueak revealed cases of more than one USV being counted as a single USV, which could lead to an inflated number of missed USVs. Since we did not train DeepSqueak with our data set, it is possible that DeepSqueak could present much better performance than what we report here when custom-trained. Thus, using both external data sets from different laboratories and our own fully validated <italic>test</italic> data set, VocalMat presents high performance in detecting USVs in audio recordings without the need for <italic>any</italic> parameter tuning or custom training of the CNN.</p></sec><sec id="s2-5"><title>Detection of harmonic components</title><p>To measure the performance of VocalMat for the detection of harmonic components, we compared the output of VocalMat with the <italic>test</italic> data set. The rate of true positives was 93.32 ± 1.96% (mean ± SEM; median = 92.18; 95% CI [88.54, 98.11]). The rate of USVs wrongly labeled as having a harmonic component (false positive) was 5.39 ± 1.18% (mean ± SEM; median = 5.17; 95% CI [2.50, 8.27]). The rate of missed harmonic components (false negative) was 6.68 ± 1.96% (mean ± SEM; median = 7.82, 95% CI [1.89, 11.46]). All combined, the error rate in identifying harmonic components was 12.19 ± 3.44% (mean ± SEM; median = 11.92, 95% CI [3.34, 21.03]). Thus, VocalMat presents satisfactory performance in detecting the harmonic components of the USVs.</p></sec><sec id="s2-6"><title>Classification of USVs in categories</title><p>To evaluate the performance of VocalMat in classifying the detected USVs in distinct categories, we compared the most likely label assigned by the CNN to the labels assigned by the investigators (i.e., ground-truth). The accuracy of the VocalMat classifier module is 86% (<xref ref-type="fig" rid="fig4">Figure 4B,C</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, and <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>). VocalMat shows lower accuracy to detect rare USV types (e.g., reverse chevron; <xref ref-type="fig" rid="fig4">Figure 4A–C</xref>) or USVs with multiple components (e.g., multiple steps and two steps; <xref ref-type="fig" rid="fig4">Figure 4A–C</xref>). When we expanded our analysis to consider the two most likely labels assigned by the CNN, the accuracy of VocalMat was 94% (<xref ref-type="fig" rid="fig4">Figure 4E</xref> and <xref ref-type="supplementary-material" rid="supp6">Supplementary file 6</xref>). These observations suggest a possible overlap between the definition of categories. Based on these analyses, we reasoned that the distribution of probabilities for each of the 11 categories of USV types calculated by the CNN could provide a more fluid classification method to analyze the vocal repertoire of mice.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>VocalMat performance for ultrasonic vocalization (USV) classification.</title><p>(<bold>A</bold>) Example of the 11 categories of USVs plus noise that VocalMat used to classify the USV candidates. (<bold>B</bold>) Confusion matrix illustrating VocalMat’s performance in multiclass classification (see also <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for individual confusion matrices). (<bold>C</bold>) Comparison of classification performance for labels assigned based on the most likely label (Top-one) versus the two most likely labels (Top-two) (see <xref ref-type="supplementary-material" rid="supp6">Supplementary file 6</xref>). Symbols represent median ±95% confidence intervals.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59161-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Confusion matrix illustrating VocalMat’s performance in multiclass classification per recording file.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59161-fig4-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-7"><title>Using VocalMat to analyze and visualize the vocal repertoire of mice</title><p>To illustrate the use of the probability distribution of USV classification by VocalMat, we used data previously published by our group with over 45,000 USVs (<xref ref-type="bibr" rid="bib40">Zimmer et al., 2019</xref>). In this published data set, two groups of 10 days old mice were studied. At this age, mice vocalize in the ultrasonic range when separated from the nest. Two groups of mice were analyzed (control versus treatment) during two contiguous time points (baseline versus test). The difference between the two groups was that in the treatment group, a specific population of neurons in the brain was activated to induce higher rates of USV emission (<xref ref-type="bibr" rid="bib40">Zimmer et al., 2019</xref>).</p><p>To visualize the probability distribution of USV classification by VocalMat, we used Diffusion Maps (see Materials and methods). Diffusion Maps is a dimensionality reduction algorithm that allows the projection of the probability distribution into a Euclidean space (<xref ref-type="bibr" rid="bib11">Coifman et al., 2005</xref>). We compared all four experimental conditions against each other and visually verified that the manifolds representing the USV repertoires showed a degree of similarity (<xref ref-type="fig" rid="fig5">Figure 5A</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Vocal repertoire visualization using Diffusion Maps.</title><p>(<bold>A</bold>) Illustration of the embedding of the ultrasonic vocalizations (USVs) for each experimental condition. The probability distribution of all the USVs in each experimental condition is embedded in a Euclidean space given by the eigenvectors computed through Diffusion Maps. Colors identify the different USV types. (<bold>B</bold>) Pairwise distance matrix between the centroids of USV types within each manifold obtained for the four experimental conditions. (<bold>C</bold>) Comparison between the pairwise distance matrices in the four experimental conditions by Pearson’s correlation coefficient.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59161-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Alignment of the manifolds between pairs of experimental conditions.</title><p>(<bold>A</bold>) Illustration of the resulting manifold alignment for each pair of experimental conditions. The quality of the alignment between the manifolds is assessed by (<bold>B</bold>) Cohen’s coefficient and (<bold>C</bold>) overall projection accuracy into joint space.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59161-fig5-figsupp1-v2.tif"/></fig></fig-group><p>To quantify the similarities (or differences) between the manifolds, we calculated the pairwise distance between the centroids of USV types within each manifold (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). The pairwise distance matrices provide a metric for the manifold structure, allowing a direct comparison between the vocal repertoire of different groups. When we compared the similarity between the pairwise distance matrices in the four experimental conditions, we observed that the treatment group in the test condition presented a robust structural change in the vocal repertoire, which can be effectively represented by a matrix correlation (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). The degree of similarity between the experimental conditions can also be visualized by comparing the structure of the manifolds. Since the manifolds are calculated separately, their coordinate system needs to be aligned to allow visual comparisons, which we achieve using the Kernel Alignment algorithm (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> and Materials and methods) (<xref ref-type="bibr" rid="bib37">Tuia and Camps-Valls, 2016</xref>; <xref ref-type="bibr" rid="bib39">Wang and Mahadevan, 2011</xref>). The quality of the manifold alignment is assessed by Cohen’s coefficient and overall projection accuracy into a joint space (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>), showing the lowest scores for the treatment group in the test condition when compared to the other experimental conditions. Hence, these later analyses illustrate the use of the probability distribution for vocal classification and the power of dimensionality reduction techniques – such as Diffusion Maps – to provide a detailed analysis of the vocal repertoire of mice.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The premise of ethology is that to <italic>understand</italic> how the brain works, it is first important to quantify behavior (<xref ref-type="bibr" rid="bib29">Pereira et al., 2020</xref>; <xref ref-type="bibr" rid="bib36">Tinbergen, 1963</xref>). The vocal behavior of animals is especially tractable for precise quantification as sound waves can be recorded with extreme resolution in multiple dimensions (time, frequency, intensity). Methods for quantifying vocal behavior, however, are often dependent on labor-intense customization and parameter tuning. Here, we reported the development of VocalMat, a software to automatically detect and classify mouse USVs with high sensitivity. VocalMat eliminates noise from the pool of USV candidates, preserves the main statistical components for the detected USVs, and identifies harmonic components. Additionally, VocalMat uses machine learning to classify USV candidates into 11 different USV categories. VocalMat is open-source, and it is compatible with high-performance computing clusters that use the Slurm job scheduler, allowing parallel and high-throughput analysis.</p><p>VocalMat adds to the repertoire of tools developed to study mouse USVs (<xref ref-type="bibr" rid="bib38">Van Segbroeck et al., 2017</xref>; <xref ref-type="bibr" rid="bib7">Burkett et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Chabout et al., 2015</xref>; <xref ref-type="bibr" rid="bib2">Arriaga et al., 2012</xref>; <xref ref-type="bibr" rid="bib20">Holy and Guo, 2005</xref>; <xref ref-type="bibr" rid="bib10">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Tachibana et al., 2020</xref>). These other tools depend on several parameters ﻿defined by the user, so it is difficult to compare their performance to VocalMat effectively. Nevertheless, our tests show that VocalMat outperforms other tools in both sensitivity and accuracy in detecting USVs. More importantly, VocalMat uses differential geometry to automate the task of USV detection that in combination with CNNs maximizes sensitivity without sacrificing accuracy without the need for <italic>any</italic> customization or parameter tuning.</p><p>Moreover, VocalMat provides a flexible classification method by treating USV classification as a problem of probability distribution across different USV categories. This approach allows the analysis, visualization, and comparison of the repertoires of USVs of different mice and experimental groups using dimensionality reduction algorithms.</p><p>VocalMat uses a pattern recognition approach, based on CNNs, which learns directly from the training set without the need for feature extraction via segmentation processes (<xref ref-type="bibr" rid="bib33">Schmidhuber, 2015</xref>; <xref ref-type="bibr" rid="bib21">Krizhevsky et al., 2012</xref>). This characteristic provides the possibility for adaptability of VocalMat to different experimental settings, including its use with other species and vocal types. Because VocalMat preserves the features of the detected USVs – including temporal dynamics, frequency, intensity, and morphology – it also provides a rich data set to quantify vocal behavior.</p><p>In summary, VocalMat is a tool to detect and classify mouse USVs with outstanding sensitivity and accuracy while keeping all the relevant spectral features, including harmonic components. With VocalMat, USVs can be quantified with precision, thus creating the opportunity for a more detailed understanding of this behavior.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animals</title><p>All mice used to record the emission of USV were 5–15 days old from both sexes. Dams used were 2–6 months old and were bred in our laboratory. To maximize variability in the audio recordings, different mouse strains were used (all from The Jackson Laboratories): C57Bl6/J, NZO/HlLtJ, 129S1/SvImJ, NOD/ShiLtJ, and PWK/PhJ. Each mouse was only recorded once. All mice were kept in temperature- and humidity-controlled rooms, in a 12/12 hr light/dark cycle, with lights on from 7:00 AM to 7:00 PM. Food and water were provided ad libitum. All procedures were approved by the IACUC at Yale University School of Medicine.</p></sec><sec id="s4-2"><title>Audio acquisition</title><p>Mice were recorded in different conditions to maximize the levels of environmental noise and other experimental factors in the audio recordings. Mice were placed in a small box made of Plexiglas (15 × 15 × 10 cm). This small box was placed inside an open field (40 × 40 × 40 cm) with the walls covered by anechoic material (2’ Wedge Acoustic Foam, Auralex) or inside an environmental chamber (CAB-8, Sable System International). For the audio recordings from inside the environmental chamber, the heater was either turned on at 35°C or turned off, as summarized in <xref ref-type="table" rid="table3">Table 3</xref>. By turning the heater on, we increased the levels of ambient noise to train and test VocalMat in a wide range of conditions. Sound was recorded using the recorder module UltraSoundGate 416H and a condenser ultrasound microphone CM16/CMPA (Avisoft Bioacoustics, Berlin, Germany) placed 15 cm above the animal. The experiments were recorded with a sampling rate of 250 kHz. The recording system had a flat response for sounds within frequencies between 20 kHz and 140 kHz, preventing distortions for the frequency of interest. The recordings were made by using Avisoft RECORDER 4.2 (version 4.2.16; Avisoft Bioacoustics) in a Laptop with an Intel i5 2.4 GHz processor and 4 GB of RAM. Using these settings, 10 min of audio recording generated files of approximately 200 MB.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Summary of experimental conditions covered in the <italic>test</italic> data set.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Age</th><th>Microphone gain</th><th>Location</th><th>Heating</th></tr></thead><tbody><tr><td>P9</td><td>Maximum</td><td>Environmental chamber</td><td>No</td></tr><tr><td>P9</td><td>Maximum</td><td>Environmental chamber</td><td>No</td></tr><tr><td>P9</td><td>Maximum</td><td>Environmental chamber</td><td>No</td></tr><tr><td>P10</td><td>Intermediary</td><td>Open field</td><td>No</td></tr><tr><td>P10</td><td>Intermediary</td><td>Open field</td><td>No</td></tr><tr><td>P10</td><td>Maximum</td><td>Environmental chamber</td><td>Yes</td></tr><tr><td>P10</td><td>Maximum</td><td>Environmental chamber</td><td>Yes</td></tr></tbody></table></table-wrap></sec><sec id="s4-3"><title>Spectral power</title><p>USVs were segmented on the audio files by analysis of their spectrograms. Aiming the configuration that would grant us the best time-frequency resolution, the spectrograms were calculated through a short-time Fourier transformation (STFT, <italic>MATLAB</italic>’s spectrogram function) using the following parameters: 1024 sampling points to calculate the discrete Fourier transform (NFFT = 1024), Hamming window with length 256, and half-overlapping with adjacent windows to reduce artifacts at the boundary. The mathematical expression that gives us the short-time Fourier Transform is shown below:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mi>T</mml:mi><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>j</mml:mi><mml:mi>ω</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p> where the original signal <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is divided in chunks by the windowing function <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The Fourier Transformation of the chunks result in a matrix with magnitude and phase for each time-frequency point.</p><p>The spectral power density, represented in the logarithmic unit decibels, is then given by<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:mi>x</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>j</mml:mi><mml:mi>ω</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We used a high pass filter (45 kHz) to eliminate sources of noise in the audible range and to reduce the amount of data stored (<xref ref-type="bibr" rid="bib17">Grimsley et al., 2011</xref>).</p></sec><sec id="s4-4"><title>Normalization and contrast enhancement</title><p>Since USVs present higher intensity than the background and to avoid setting a fixed threshold for USV segmentation, we used contrast adjustment to highlight putative USV candidates and to reduce the variability across audio files. Contrast adjustment was obtained according to the following re-scaling equation:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>10</mml:mn><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>10</mml:mn><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf32"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the highest and the lowest intensity values of the adjusted image, respectively, and <inline-formula><mml:math id="inf33"><mml:mi>P</mml:mi></mml:math></inline-formula> is the power spectrum for each time-frequency point (pixel of the spectrogram). The parameter γ describes the shape of the mapping function between the original and the corrected image, such that <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> results in darker pixels and <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> in brighter pixels. We used a linear mapping for our application (<inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <italic>MATLAB</italic>’s imadjust function).</p></sec><sec id="s4-5"><title>Adaptive thresholding and morphological operations</title><p>Due to non-stationary background noise and dynamic changes in the intensity of USVs within and between the audio files, we use adaptive thresholding methods to binarize the spectrograms. The threshold is computed for each pixel using the local mean intensity around the neighborhood of the pixel (<xref ref-type="bibr" rid="bib4">Bradley and Roth, 2007</xref>). This method preserves hard contrast lines and ignores soft gradient changes. The integral image consists of a matrix <inline-formula><mml:math id="inf37"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that stores the sum of all pixel intensities <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to the left and above the pixel <inline-formula><mml:math id="inf39"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The computation is given by the following equation:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Therefore, the sum of the pixel values for any rectangle defined by a lower right corner (<inline-formula><mml:math id="inf40"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>) and upper left corner (<inline-formula><mml:math id="inf41"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>) is given as:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:munderover><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:munderover><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then, the method computes the average of an <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>s</mml:mi><mml:mo>×</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> window of pixels centered around each pixel. The average is calculated considering neighboring pixels on all sides for each pixel. If the value of the current pixel intensity is <inline-formula><mml:math id="inf43"><mml:mi>t</mml:mi></mml:math></inline-formula> percent less than this average, then it is set to black; otherwise it is set to white, as shown in the following equation:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:munderover><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:munderover><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the average around the pixel <inline-formula><mml:math id="inf45"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p>The binarized image is then constructed such as that pixels <inline-formula><mml:math id="inf46"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with intensity <inline-formula><mml:math id="inf47"><mml:mi>t</mml:mi></mml:math></inline-formula> percent lower than <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are set to black (<xref ref-type="bibr" rid="bib4">Bradley and Roth, 2007</xref>):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>≤</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf49"><mml:mi>t</mml:mi></mml:math></inline-formula> represents the sensitivity factor, and it was empirically chosen as <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> for our application. The resulting binarized image consists of background pixels (intensity = 0) and putative vocal segments (contiguous areas with intensity = 1). The segments are then subjected to a sequence of morphological operations: (i) opening (erosion followed by a dilation; <italic>MATLAB</italic>’s imopen) with a rectangle 4 × 2 pixels as kernel; (ii) dilation with a line of length <italic>l</italic> = 4 and <inline-formula><mml:math id="inf51"><mml:mi mathvariant="normal">∠</mml:mi><mml:mrow><mml:mn>90</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">°</mml:mi></mml:mrow></mml:math></inline-formula> relative to the horizontal axis as kernel (<italic>MATLAB</italic>’s imdilate); (iii) filtering out candidates (i.e., dense set of white pixels) with &lt;60 pixels (correspondent to approximately 2 ms syllable); and (iv) dilation with a line of length <italic>l</italic> = 4 and <inline-formula><mml:math id="inf52"><mml:mi mathvariant="normal">∠</mml:mi></mml:math></inline-formula> 0°, making the USV candidates proportional to their original shape.</p></sec><sec id="s4-6"><title>Local median filter</title><p>The image processing pipeline used for segmentation can create artifacts or noise that were not originally present in the spectrogram, especially in the binarization step. These noises that occur due to the segmentation process are not associated with an event in the recording (a real USV or external noise) and are part of the pool of USV candidates. To determine if a USV candidate is relevant for further analysis, we used a contrast filter – Local Median Filter – to compare the median intensity of the pixels in the USV candidate <inline-formula><mml:math id="inf53"><mml:mi>k</mml:mi></mml:math></inline-formula> (referred to as <inline-formula><mml:math id="inf54"><mml:mover accent="true"><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>) to the intensity of the pixels in a bounding box that encompasses the USV candidate (referred to as <inline-formula><mml:math id="inf55"><mml:mover accent="true"><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>). The Local Median Filter then determines if a USV candidate <inline-formula><mml:math id="inf56"><mml:mi>k</mml:mi></mml:math></inline-formula> is discarded based on the cumulative distribution of intensity ratio over all the USV candidates detected in the audio file <inline-formula><mml:math id="inf57"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>/</mml:mo><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. The bounding box that defines the window <inline-formula><mml:math id="inf58"><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> is a rectangle with its four vertices defined as a function of the frequencies (<inline-formula><mml:math id="inf59"><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>) for USV candidate <inline-formula><mml:math id="inf60"><mml:mi>k</mml:mi></mml:math></inline-formula> and its time stamps (<inline-formula><mml:math id="inf61"><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>). Thus, the bounding box is defined as follows:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>2.5</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mi/></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mn>2.5</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mi/></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>0.1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mi/></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mn>0.1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>As seen in (8), a 200 ms interval is analyzed around the USV candidate. Such a wide interval may present more than one USV in <inline-formula><mml:math id="inf62"><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>. However, the amount of pixels in <inline-formula><mml:math id="inf63"><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> represents only 2.43 ± 0.10% (mean ± SEM; median = 1.27, 95% CI [2.22 , 2.63]; n = 59,781 images analyzed) of the total number of pixels contained in the window <inline-formula><mml:math id="inf64"><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>. Given this proportion between the number of pixels in <inline-formula><mml:math id="inf65"><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf66"><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>, the median of the intensity distribution of the whole window (<inline-formula><mml:math id="inf67"><mml:mover accent="true"><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>) tends to converge to the median intensity of the background.</p><p>We used the ratio <inline-formula><mml:math id="inf68"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover><mml:mo>/</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> to exclude USV candidates that correspond to segmentation noise. We first calculated the CDF of <inline-formula><mml:math id="inf69"><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> over all the USV candidates in an audio file (now referred to as <inline-formula><mml:math id="inf70"><mml:mi mathvariant="normal">Υ</mml:mi></mml:math></inline-formula>). To find the inflection point in <inline-formula><mml:math id="inf71"><mml:mi mathvariant="normal">Υ</mml:mi></mml:math></inline-formula>, a second-order polynomial fit for every set of three consecutive points was used to obtain local parametric equations (<inline-formula><mml:math id="inf72"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Υ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) describing the segments of <inline-formula><mml:math id="inf73"><mml:mi mathvariant="normal">Υ</mml:mi></mml:math></inline-formula>. Since the calculation of the inflection point is done numerically, the number of points chosen for this calculation should be such that we can have as many points of curvature as possible while preserving information of local curvature. Then, after a screening for the best number of points, <inline-formula><mml:math id="inf74"><mml:mi mathvariant="normal">Υ</mml:mi></mml:math></inline-formula> was down-sampled to 35 equally spaced points and the inflection point was calculated. Using the local parametric equations, we calculated the tangent and normal vectors on each of the 35 points. Using these vectors, we estimated the changing rate of the tangent toward the normal at each point, which is the curvature κ (<xref ref-type="bibr" rid="bib28">O’neill, 2006</xref>) and can be calculated as follows:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>det</mml:mo><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="normal">Υ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="normal">Υ</mml:mi><mml:msup><mml:mi/><mml:mo>′′</mml:mo></mml:msup></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mo fence="true">∥</mml:mo><mml:msup><mml:mi mathvariant="normal">Υ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo fence="true">∥</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:math></disp-formula>or by using the parametric equations:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:msup><mml:mi/><mml:mo>′′</mml:mo></mml:msup></mml:msup></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:msup><mml:mi/><mml:mo>′′</mml:mo></mml:msup></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>The inflection point is then determined as the point with maximum curvature of the CDF curve, and adopted as threshold τ. This threshold is calculated individually for each audio file since it can vary according to the microphone gain and the distance of the microphone from the sound source. In audio files with a very low number of USVs, the point of maximum curvature of the CDF curve was not detected, and no τ was estimated. In these cases, a default threshold <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.92</mml:mn></mml:mrow></mml:math></inline-formula> was adopted as a conservative threshold, since no audio file presented inflection point as high as 0.92 in our <italic>training</italic> set. Only the USV candidates satisfying (<xref ref-type="disp-formula" rid="equ11">Equation 11</xref>) are kept for further analysis.<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>χ</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover><mml:mo>≤</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></disp-formula>where χ represents the set of USV candidates that survived the Local Median Filter. Of note, the intensity of each pixel is calculated in decibels, which is given in negative units due to the low power spectrum.</p></sec><sec id="s4-7"><title>CNNs for USV classification</title><p>We use CNNs to eliminate external noise from the pool of USV candidates and classify USVs in distinct types (see below). We use a transfer learning approach with an AlexNet (<xref ref-type="bibr" rid="bib21">Krizhevsky et al., 2012</xref>) model pre-trained on the ImageNet data set, and perform end-to-end training using our USVs data set. Briefly, the last three layers of the network were replaced in order to handle a 12-categories classification task for our data set (11 <italic>USV types + noise</italic>).</p><p>The outputs of the segmentation process with detected USV candidates were centralized in windows of 220 ms. These windows were twice the maximum duration of USVs observed in mice (<xref ref-type="bibr" rid="bib17">Grimsley et al., 2011</xref>) and were framed in individual 227 × 227 pixels images. Each image was then manually labeled by an experienced experimenter as noise (including acoustic or segmentation noise) or one of the USV categories. The labeled data set was used to train the CNN to classify the USV candidates.</p><p>The images in our data set were manually labeled based on definitions for USV classes found in previous studies (adapted from <xref ref-type="bibr" rid="bib31">Scattoni et al., 2008</xref> and <xref ref-type="bibr" rid="bib17">Grimsley et al., 2011</xref>). The USV classes are described below:</p><sec id="s4-7-1"><title>Complex</title><p>One-note syllables with two or more directional changes in frequency &gt;6 kHz. A total of 350 images were used for training.</p></sec><sec id="s4-7-2"><title>Step up</title><p>Two-notes syllables in which the second element was ≥6 kHz higher from the preceding element and there was no more than 10 ms between steps. A total of 1814 images were used for training.</p></sec><sec id="s4-7-3"><title>Step down</title><p>Two-notes syllables in which the second element was ≥6 kHz lower from the preceding element and there was no more than 10 ms between steps. A total of 389 images were used for training.</p></sec><sec id="s4-7-4"><title>Two steps</title><p>Three-notes syllables, in which the second element was ≥6 kHz or more different from the first, the third element was ≥6 kHz or more different from the second and there was no more than 10 ms between elements. A total of 701 images were used for training.</p></sec><sec id="s4-7-5"><title>Multiple steps</title><p>Four-notes syllables or more, in which each element was ≥6 kHz or more different from the previous one and there was no more than 10 ms between elements. A total of 74 images were used for training.</p></sec><sec id="s4-7-6"><title>Up-frequency modulation</title><p>Upwardly frequency modulated with a frequency change ≥6 kHz. A total of 1191 images were used for training.</p></sec><sec id="s4-7-7"><title>Down-frequency modulation</title><p>Downwardly frequency modulated with a frequency change ≥6 kHz. A total of 1775 images were used for training.</p></sec><sec id="s4-7-8"><title>Flat</title><p>Constant frequency syllables with modulation ≤5 kHz and duration ≥12 ms. A total of 1134 images were used for training.</p></sec><sec id="s4-7-9"><title>Short</title><p>Constant frequency syllables with modulation ≤5 kHz and duration ≤12 ms. A total of 1713 images were used for training.</p></sec><sec id="s4-7-10"><title>Chevron</title><p>Shaped like an inverted <italic>U</italic> in which the peak frequency was ≥6 kHz than the starting and ending frequencies. A total of 1594 images were used for training.</p></sec><sec id="s4-7-11"><title>Reverse chevron</title><p>Shaped like an <italic>U</italic> in which the peak frequency was ≥6 kHz than the starting and ending frequencies. A total of 136 images were used for training.</p></sec><sec id="s4-7-12"><title>Noise</title><p>Any sort of mechanical or segmentation noise detected during the segmentation process as a USV candidate. A total of 2083 images were used for training.</p><p>In order to purposely create some overlap between the categories, USVs with segments oscillating between 5 and 6 kHz were not defined or used for training. The assumption is that the CNN should find its transition method between two overlapping categories.</p><p>Our <italic>training</italic> data set consisted of 12,954 images, wherein 2083 were labeled as noise. This data set correspond to mice of different strains (C57Bl6/J, NZO/HlLtJ, 129S1/SvImJ, NOD/ShiLtJ, and PWK/PhJ) and ages (postnatal day (P)5, P10, and P15) from both sexes.</p><p>The CNN was trained using stochastic gradient descent with momentum, a batch size of M = 128 images, and with a maximum number of epochs set to 100. Through a screening process for the set of hyper-parameters that would maximize the average performance of the network, the chosen learning rate was α = 10<sup>−4</sup>, momentum of 0.9, and weight decay λ = 10<sup>−4</sup>. To validate the training performance, each data set was randomly split into two disjoint sets; <italic>training</italic> set (90%) and a <italic>validation</italic> set (10%). The training and validation sets were independently shuffled at every epoch during training. The training was set to stop when the classification accuracy on the validation set did not improve for three consecutive epochs. When running in a GeForce GTX 980 TI, the final validation accuracy was 95.28% after 17 min of training.</p></sec></sec><sec id="s4-8"><title>Testing detection performance</title><p>To evaluate the performance of VocalMat, neonatal mice were recorded for 10 min upon social isolation in different conditions (<xref ref-type="table" rid="table3">Table 3</xref>) to increase the variability of the data. To cover most of the recording setups, we included recordings with different microphone settings (maximum and intermediary gain), environments (open field or enclosed environmental chamber), and machinery noise (heater on or off). The spectrograms were manually inspected for the occurrence of USVs. The starting time for the detected USVs was recorded. USVs automatically detected by VocalMat with a start time matching manual annotation (±5 ms of tolerance) were considered correctly detected. USVs manually detected with no correspondent USV given by VocalMat were considered <italic>false negative</italic>. The false negatives were originated from missed USVs or USVs that the software labeled as noise. Finally, USVs registered by VocalMat without a correspondent in the manual annotation were considered <italic>false positive</italic>. In order to compare VocalMat to the other tools available, the same metrics were applied to the output of Ax (<xref ref-type="bibr" rid="bib23">Neunuebel et al., 2015</xref>), MUPET (<xref ref-type="bibr" rid="bib38">Van Segbroeck et al., 2017</xref>), USVSEG (<xref ref-type="bibr" rid="bib35">Tachibana et al., 2020</xref>), and DeepSqueak (<xref ref-type="bibr" rid="bib10">Coffey et al., 2019</xref>).</p></sec><sec id="s4-9"><title>Diffusion maps for USV class distribution visualization</title><p>One of the main characteristics of VocalMat is attributing to USVs a distribution of probabilities over all the possible vocal classes. Since we classify USV candidates in 11 categories, to have access to the distribution of probabilities we would need to visualize the data in 11 dimensions. Here, as an example of analytical methods that can be applied to the output data from VocalMat, we used Diffusion Maps (<xref ref-type="bibr" rid="bib11">Coifman et al., 2005</xref>) to reduce the dimensionality of the data to three dimensions. Diffusion Maps allow remapping of the data into a Euclidean space, which ultimately results in preserving the distance between USVs based on the similarity of their probability distribution. A Gaussian kernel function defines the connectivity between two data points in a Euclidean manifold. Such kernel provides the similarity value between two data points <inline-formula><mml:math id="inf76"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf77"><mml:mi>j</mml:mi></mml:math></inline-formula> as follows:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the similarity value between observations <inline-formula><mml:math id="inf79"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf80"><mml:mi>j</mml:mi></mml:math></inline-formula>. The parameter σ corresponds to the width of the kernel, and it is set based on the average Euclidean distance observed between observations of the same label (i.e., the intra-class distances). For our application, <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> was set based on the distance distribution observed in our data.</p><p>The similarity matrix is then turned into a probability matrix by normalizing the rows:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf82"><mml:mrow><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> has the row sum of <inline-formula><mml:math id="inf83"><mml:mi>W</mml:mi></mml:math></inline-formula> along its diagonal. The matrix <inline-formula><mml:math id="inf84"><mml:mi>M</mml:mi></mml:math></inline-formula> gives the probability of going from node <inline-formula><mml:math id="inf85"><mml:mi>i</mml:mi></mml:math></inline-formula> to any other node using a random walk. In other words, the probability that the USV <inline-formula><mml:math id="inf86"><mml:mi>i</mml:mi></mml:math></inline-formula> is close to another USV <inline-formula><mml:math id="inf87"><mml:mi>j</mml:mi></mml:math></inline-formula> given their probability distribution.</p><p>Once we take one step in such Euclidean space, the probabilities are updated, since the set of likely nodes for the next move are now different. This idea of moving from node to node while updating the probabilities results in a 'diffused map’.</p><p>The process of moving from a USV <inline-formula><mml:math id="inf88"><mml:mi>i</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf89"><mml:mi>j</mml:mi></mml:math></inline-formula> after <inline-formula><mml:math id="inf90"><mml:mi>t</mml:mi></mml:math></inline-formula> steps in this Euclidean space is computed as follows:<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For our application, we use <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>Next, we find the coordinate functions to embed the data in a lower-dimensional space. The eigenvectors of <inline-formula><mml:math id="inf92"><mml:mi>M</mml:mi></mml:math></inline-formula> give such a result. Because <inline-formula><mml:math id="inf93"><mml:mi>M</mml:mi></mml:math></inline-formula> is not symmetric, the eigendecomposition is computed through a SVD decomposition (<xref ref-type="bibr" rid="bib16">Golub and Kahan, 1965</xref>):<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>:=</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>W</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>W</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>and since <inline-formula><mml:math id="inf94"><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf95"><mml:mi>W</mml:mi></mml:math></inline-formula> are symmetric, <inline-formula><mml:math id="inf96"><mml:msub><mml:mi>M</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> is also symmetric and allows us to calculate its eigenvectors and eigenvalues. For the sake of notation, consider:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi mathvariant="normal">Λ</mml:mi><mml:msup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">⟹</mml:mo><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi mathvariant="normal">Λ</mml:mi><mml:msup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Considering <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (right eigenvectors of <inline-formula><mml:math id="inf98"><mml:mi>M</mml:mi></mml:math></inline-formula>) and <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (left eigenvectors of <inline-formula><mml:math id="inf100"><mml:mi>M</mml:mi></mml:math></inline-formula>), we verify that <inline-formula><mml:math id="inf101"><mml:mrow><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, therefore they are mutually orthogonal and <inline-formula><mml:math id="inf102"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf103"><mml:msub><mml:mi>M</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> are similar matrices. Thus,<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>and the diffusion component shown in <xref ref-type="disp-formula" rid="equ14">Equation (14)</xref> is incorporated as the power of the diagonal matrix composed by the eigenvalues of <inline-formula><mml:math id="inf104"><mml:mi>M</mml:mi></mml:math></inline-formula>:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We use the scaled right eigenvectors by their corresponding eigenvalues <inline-formula><mml:math id="inf105"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Λ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> as the coordinate functions. Since the first column of <inline-formula><mml:math id="inf106"><mml:mi mathvariant="normal">Γ</mml:mi></mml:math></inline-formula> is constant across all the observations, we use the second to fourth coordinates in our work.</p></sec><sec id="s4-10"><title>Vocal repertoire analysis via manifold alignment</title><p>The result of the embedding by Diffusion Maps allows 3D visualization of the probability distribution for the USVs. The direct comparison of different 3D maps is challenging to obtain as the manifolds depend on data distribution, which contains high variability in experimental samples. To address this problem and compare the topology of different manifolds, we used a manifold alignment method for heterogeneous domain adaptation (<xref ref-type="bibr" rid="bib39">Wang and Mahadevan, 2011</xref>; <xref ref-type="bibr" rid="bib37">Tuia and Camps-Valls, 2016</xref>). Using this method, two different domains are mapped to a new latent space, where samples with the same label are matched while preserving the topology of each domain.</p><p>We used the probability distribution for the USVs for each data set to build the manifolds (<xref ref-type="bibr" rid="bib39">Wang and Mahadevan, 2011</xref>). Each manifold was represented as a Laplacian matrix constructed from a graph that defines the connectivity between the samples in the manifold. The Laplacian matrix is then defined as <inline-formula><mml:math id="inf107"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ12">Equation (12)</xref>).</p><p>The final goal is to remap all the domains to a new shared space such that samples with similar labels become closer in this new space. In contrast, samples with different labels are pushed away while preserving the geometry of the manifolds. It leads to the necessity of three different graph Laplacians: <inline-formula><mml:math id="inf108"><mml:msub><mml:mi>L</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> (relative to the similarity matrix and responsible for connecting the samples with the same label), <inline-formula><mml:math id="inf109"><mml:msub><mml:mi>L</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> (dissimilarity matrix and responsible for connecting the samples with different labels), and <inline-formula><mml:math id="inf110"><mml:mi>L</mml:mi></mml:math></inline-formula> (similarity matrix responsible for preserving the topology of each domain). <xref ref-type="bibr" rid="bib39">Wang and Mahadevan, 2011</xref> show that the embedding that minimizes the joint function defined by the similarity and dissimilarity matrices is given by the eigenvectors corresponding to the smallest nonzero eigenvalues of the following eigendecomposition:<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf111"><mml:mi>Z</mml:mi></mml:math></inline-formula> is a block diagonal containing the data matrices <inline-formula><mml:math id="inf112"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, (where <italic>n</italic><sub><italic>i</italic></sub> samples and <italic>d</italic><sub><italic>i</italic></sub> dimensions are constants for the <inline-formula><mml:math id="inf113"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> domain) from the two domains. Thus, <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The matrix <inline-formula><mml:math id="inf115"><mml:mi>V</mml:mi></mml:math></inline-formula> contains the eigenvectors organized in rows for each domain, <inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. The <italic>μ</italic> is weight parameter, which goes from preserving both topology and instance matching equally (<inline-formula><mml:math id="inf117"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) or focusing more on topology preservation (<inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) (<xref ref-type="bibr" rid="bib37">Tuia and Camps-Valls, 2016</xref>).</p><p>From <xref ref-type="disp-formula" rid="equ19">Equation (19)</xref>, we then extract <inline-formula><mml:math id="inf119"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:msubsup><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> features, which is the sum of the dimensions of the individual domains (see details in <xref ref-type="bibr" rid="bib39">Wang and Mahadevan, 2011</xref>; <xref ref-type="bibr" rid="bib37">Tuia and Camps-Valls, 2016</xref>), and the projection of the data to a joint space <inline-formula><mml:math id="inf120"><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi></mml:math></inline-formula> defined by the mapping matrix <inline-formula><mml:math id="inf121"><mml:mi>V</mml:mi></mml:math></inline-formula> will be given by<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To measure the performance of the alignment, linear discriminant analysis (LDA) (<xref ref-type="bibr" rid="bib22">McLachlan, 2004</xref>) is used to show the ability to project the domains in a joint space. The LDA is trained on half of the samples in order to predict the other half. The error of the alignment is given as the percentage of samples that would be misclassified when projected into the new space (overall accuracy) (<xref ref-type="bibr" rid="bib37">Tuia and Camps-Valls, 2016</xref>).</p><p>Another measurement to quantify the quality of the alignment is by calculating the agreement between the projections, which is given by Cohen’s Kappa coefficient (κ) (<xref ref-type="bibr" rid="bib1">Agresti, 2018</xref>). In this method, the labels are treated as categorical, and the coefficient compares the agreement with that expected if ratings were independent. Thus, disagreements for labels that are close are treated the same as labels that are far apart.</p><p>Cohen’s coefficient is defined as:<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <italic>p</italic><sub>0</sub> is the observed agreement (<inline-formula><mml:math id="inf122"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> for a confusion matrix <inline-formula><mml:math id="inf123"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, in which <inline-formula><mml:math id="inf124"><mml:mi>n</mml:mi></mml:math></inline-formula> is the raw confusion matrix and <inline-formula><mml:math id="inf125"><mml:mi>N</mml:mi></mml:math></inline-formula> is the total number of samples, composed by the projection of the <inline-formula><mml:math id="inf126"><mml:mi>k</mml:mi></mml:math></inline-formula> labels), which corresponds to the accuracy; <italic>p</italic><sub><italic>e</italic></sub> is the probability of agreement by chance (<inline-formula><mml:math id="inf127"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo>.</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf128"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> is the number of times an entity of label <inline-formula><mml:math id="inf129"><mml:mi>i</mml:mi></mml:math></inline-formula> was labeled as any category and <inline-formula><mml:math id="inf130"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo>.</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of times any category was predicted as label <inline-formula><mml:math id="inf131"><mml:mi>i</mml:mi></mml:math></inline-formula>). Therefore, a <inline-formula><mml:math id="inf132"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> represents no agreement (or total misalignment of manifolds) and <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> is a total agreement.</p><p>In this context, the overall accuracy (<inline-formula><mml:math id="inf134"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula>) is given by <inline-formula><mml:math id="inf135"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf136"><mml:mi>N</mml:mi></mml:math></inline-formula> is the total number of samples.</p><p>The asymptotic variance for κ is given as follows:<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>κ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:msup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:msup></mml:mfrac></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>(which turns into accuracy once it is divided by <inline-formula><mml:math id="inf137"><mml:mi>N</mml:mi></mml:math></inline-formula>),<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mo>.</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mo>.</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>n</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mo>.</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>From <xref ref-type="disp-formula" rid="equ22">Equation (22)</xref> we can calculate the Z-score, which can express the significance of our κ:<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>κ</mml:mi><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>κ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>And the 95% confidence interval as<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>κ</mml:mi><mml:mo>+</mml:mo><mml:mn>1.96</mml:mn><mml:msqrt><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>κ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>κ</mml:mi><mml:mo>-</mml:mo><mml:mn>1.96</mml:mn><mml:msqrt><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>κ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The third form of error measurement is the evaluation of the projection per USV class from each domain remapped into the new space. This method is based on the fact that this new space is the one in which the cost function expressed by <xref ref-type="disp-formula" rid="equ19">Equation (19)</xref> is minimized and, therefore, the projection from each domain into the new space has its projection error for each class. As a consequence, the mean of the projection error from each domain to the new space for each class can be used as a quantitative measurement of misalignment of projected domains.</p></sec><sec id="s4-11"><title>Quantification and statistical analysis</title><p>MATLAB (2019a or above) and Prism 8.0 were used to analyze data and plot figures. All figures were created using Adobe Illustrator CS6/CC. Data were first subjected to a normality test using the D’Agostino and Pearson normality test or the Shapiro–Wilk normality test. When homogeneity was assumed, a parametric analysis of variance test was used. The Student’s t-test was used to compare two groups. The Mann–Whitney U-test was used to determine the significance between groups. Two sample Kolmogorov–Smirnov test was used to calculate the statistical differences between the contrast of USVs and noise. Statistical data are provided in text and in the figures. In the text, values are provided as mean ± SEM. p&lt;0.05 was considered statistically significant. The 95% confidence intervals are reported in reference to the mean. The true positive rate is computed as the ratio between true positive (hit) and real positive cases. The true negative rate is the ratio between true negative (correct rejection) and real negative cases. The false negative rate is the ratio between false negative (type I error) and real positives cases. The false positive (type II error) is the ratio between false positive and real negative cases. The false discovery rate is the ratio between false positive and the sum of false positives and real positives.</p></sec><sec id="s4-12"><title>Code and data availability</title><p>VocalMat is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/ahof1704/VocalMat.git">https://github.com/ahof1704/VocalMat.git</ext-link>; <xref ref-type="bibr" rid="bib15">Fonseca, 2021</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:2bc678b72a2de8addace7c3b7be7810d8c4637b7;origin=https://github.com/ahof1704/VocalMat.git;visit=swh:1:snp:db4e60f108184eea764b607229eb7fdffc108557;anchor=swh:1:rev:9384fabfc1fbd9bc0ef8ca460b652e72c5b6819f/">swh:1:rev:9384fabfc1fbd9bc0ef8ca460b652e72c5b6819f</ext-link>) for academic use. Our data set of vocalization images is available on OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/bk2uj/">https://osf.io/bk2uj/</ext-link>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank the lab members for critical data collection and insights in the manuscript. MOD was supported by a NARSAD Young Investigator Grant ID 22709 from the Brain and Behavior Research Foundation, by the National Institute Of Diabetes And Digestive And Kidney Diseases of the National Institutes of Health (R01DK107916), by a pilot grant from the Yale Diabetes Research Center (P30 DK045735), by the Yale Center for Clinical Investigation Scholar Award, by the Whitehall Foundation, by the Charles H Hood Foundation, Inc (Boston, MA), by a pilot grant from the Modern Diet and Physiology Research Center (The John B Pierce Laboratory), by a grant of the Foundation for Prader-Willi Research, and by the Reginald and Michiko Spector Award in Neuroscience. MOD also received support from the Conselho Nacional de Desenvolvimento Científico e Tecnologico (CNPq) and Coordenadoria de Aperfeiçoamento de Pessoal de Nível Superior (CAPES), Brazil. AHOF and GMS were supported in part by the Coordenadoria de Aperfeiçoamento de Pessoal de Nível Superior – Brasil (CAPES) – Finance Code 001. GBO was supported in part by the Gilliam Fellowship for Advanced Studies from the HHMI. The authors declare no conflict of interest.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Validation</p></fn><fn fn-type="con" id="con4"><p>Data curation, Supervision, Funding acquisition, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Data curation, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. The protocol was reviewed and approved by the Yale University Institutional Animal Care and Use Committee (IACUC). All of the animals were handled according to the approved IACUC protocol (#2018-20042) of the Yale University School of Medicine.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>List of parameters and performance of Ax.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-59161-supp1-v2.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>List of parameters and performance for MUPET.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-59161-supp2-v2.docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>List of parameters and performance for USVSEG.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-59161-supp3-v2.docx"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>List of parameters and performance for DeepSqueak.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-59161-supp4-v2.docx"/></supplementary-material><supplementary-material id="supp5"><label>Supplementary file 5.</label><caption><title>VocalMat accuracy per class.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-59161-supp5-v2.docx"/></supplementary-material><supplementary-material id="supp6"><label>Supplementary file 6.</label><caption><title>VocalMat accuracy considering the two most likely labels.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-59161-supp6-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-59161-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All the data and code used in this work are publicly available and can be found at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/bk2uj/">https://osf.io/bk2uj/</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://www.dietrich-lab.org/vocalmat">https://www.dietrich-lab.org/vocalmat</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Fonseca</surname><given-names>AHO</given-names></name><name><surname>Santana</surname><given-names>GM</given-names></name><name><surname>Bosque Ortiz</surname><given-names>GM</given-names></name><name><surname>Bampi</surname><given-names>Sr</given-names></name><name><surname>Dietrich</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>USV training set</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="accession" xlink:href="https://osf.io/bk2uj/">bk2uj</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Agresti</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>An Introduction To Categorical Data Analysis</source><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arriaga</surname> <given-names>G</given-names></name><name><surname>Zhou</surname> <given-names>EP</given-names></name><name><surname>Jarvis</surname> <given-names>ED</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Of mice, birds, and men: the mouse ultrasonic song system has some features similar to humans and song-learning birds</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e46610</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0046610</pub-id><pub-id pub-id-type="pmid">23071596</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arriaga</surname> <given-names>G</given-names></name><name><surname>Jarvis</surname> <given-names>ED</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Mouse vocal communication system: are ultrasounds learned or innate?</article-title><source>Brain and Language</source><volume>124</volume><fpage>96</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2012.10.002</pub-id><pub-id pub-id-type="pmid">23295209</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradley</surname> <given-names>D</given-names></name><name><surname>Roth</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Adaptive thresholding using the integral image</article-title><source>Journal of Graphics Tools</source><volume>12</volume><fpage>13</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1080/2151237X.2007.10129236</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Branchi</surname> <given-names>I</given-names></name><name><surname>Santucci</surname> <given-names>D</given-names></name><name><surname>Alleva</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Ultrasonic vocalisation emitted by infant rodents: a tool for assessment of neurobehavioural development</article-title><source>Behavioural Brain Research</source><volume>125</volume><fpage>49</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1016/S0166-4328(01)00277-7</pub-id><pub-id pub-id-type="pmid">11682093</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Branchi</surname> <given-names>I</given-names></name><name><surname>Santucci</surname> <given-names>D</given-names></name><name><surname>Alleva</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Analysis of ultrasonic vocalizations emitted by infant rodents</article-title><source>Current Protocols in Toxicology</source><volume>Chapter 13</volume><elocation-id>Unit13.12</elocation-id><pub-id pub-id-type="doi">10.1002/0471140856.tx1312s30</pub-id><pub-id pub-id-type="pmid">23045127</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burkett</surname> <given-names>ZD</given-names></name><name><surname>Day</surname> <given-names>NF</given-names></name><name><surname>Peñagarikano</surname> <given-names>O</given-names></name><name><surname>Geschwind</surname> <given-names>DH</given-names></name><name><surname>White</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>VoICE: a semi-automated pipeline for standardizing vocal analysis across models</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>10237</elocation-id><pub-id pub-id-type="doi">10.1038/srep10237</pub-id><pub-id pub-id-type="pmid">26018425</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castellucci</surname> <given-names>GA</given-names></name><name><surname>Calbick</surname> <given-names>D</given-names></name><name><surname>McCormick</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The temporal organization of mouse ultrasonic vocalizations</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0199929</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0199929</pub-id><pub-id pub-id-type="pmid">30376572</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chabout</surname> <given-names>J</given-names></name><name><surname>Sarkar</surname> <given-names>A</given-names></name><name><surname>Dunson</surname> <given-names>DB</given-names></name><name><surname>Jarvis</surname> <given-names>ED</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Male mice song syntax depends on social contexts and influences female preferences</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>9</volume><elocation-id>76</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2015.00076</pub-id><pub-id pub-id-type="pmid">25883559</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coffey</surname> <given-names>KR</given-names></name><name><surname>Marx</surname> <given-names>RG</given-names></name><name><surname>Neumaier</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepSqueak: a deep learning-based system for detection and analysis of ultrasonic vocalizations</article-title><source>Neuropsychopharmacology</source><volume>44</volume><fpage>859</fpage><lpage>868</lpage><pub-id pub-id-type="doi">10.1038/s41386-018-0303-6</pub-id><pub-id pub-id-type="pmid">30610191</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coifman</surname> <given-names>RR</given-names></name><name><surname>Lafon</surname> <given-names>S</given-names></name><name><surname>Lee</surname> <given-names>AB</given-names></name><name><surname>Maggioni</surname> <given-names>M</given-names></name><name><surname>Nadler</surname> <given-names>B</given-names></name><name><surname>Warner</surname> <given-names>F</given-names></name><name><surname>Zucker</surname> <given-names>SW</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Geometric diffusions as a tool for harmonic analysis and structure definition of data: diffusion maps</article-title><source>PNAS</source><volume>102</volume><fpage>7426</fpage><lpage>7431</lpage><pub-id pub-id-type="doi">10.1073/pnas.0500334102</pub-id><pub-id pub-id-type="pmid">15899970</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D'Amato</surname> <given-names>FR</given-names></name><name><surname>Scalera</surname> <given-names>E</given-names></name><name><surname>Sarli</surname> <given-names>C</given-names></name><name><surname>Moles</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Pups call, mothers rush: does maternal responsiveness affect the amount of ultrasonic vocalizations in mouse pups?</article-title><source>Behavior Genetics</source><volume>35</volume><fpage>103</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1007/s10519-004-0860-9</pub-id><pub-id pub-id-type="pmid">15674537</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ehret</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Infant rodent ultrasounds -- a gate to the understanding of sound communication</article-title><source>Behavior Genetics</source><volume>35</volume><fpage>19</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1007/s10519-004-0853-8</pub-id><pub-id pub-id-type="pmid">15674530</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elwood</surname> <given-names>RW</given-names></name><name><surname>Keeling</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Temporal organization of ultrasonic vocalizations in infant mice</article-title><source>Developmental Psychobiology</source><volume>15</volume><fpage>221</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1002/dev.420150306</pub-id><pub-id pub-id-type="pmid">7095288</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Fonseca</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>VocalMat</data-title><source>Software Heritage</source><version designator="swh:1:rev:9384fabfc1fbd9bc0ef8ca460b652e72c5b6819f">swh:1:rev:9384fabfc1fbd9bc0ef8ca460b652e72c5b6819f</version><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:2bc678b72a2de8addace7c3b7be7810d8c4637b7;origin=https://github.com/ahof1704/VocalMat.git;visit=swh:1:snp:db4e60f108184eea764b607229eb7fdffc108557;anchor=swh:1:rev:9384fabfc1fbd9bc0ef8ca460b652e72c5b6819f/">https://archive.softwareheritage.org/swh:1:dir:2bc678b72a2de8addace7c3b7be7810d8c4637b7;origin=https://github.com/ahof1704/VocalMat.git;visit=swh:1:snp:db4e60f108184eea764b607229eb7fdffc108557;anchor=swh:1:rev:9384fabfc1fbd9bc0ef8ca460b652e72c5b6819f/</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golub</surname> <given-names>G</given-names></name><name><surname>Kahan</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1965">1965</year><article-title>Calculating the singular values and Pseudo-Inverse of a matrix</article-title><source>Journal of the Society for Industrial and Applied Mathematics Series B Numerical Analysis</source><volume>2</volume><fpage>205</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1137/0702016</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grimsley</surname> <given-names>JM</given-names></name><name><surname>Monaghan</surname> <given-names>JJ</given-names></name><name><surname>Wenstrup</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Development of social vocalizations in mice</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e17460</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0017460</pub-id><pub-id pub-id-type="pmid">21408007</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hahn</surname> <given-names>ME</given-names></name><name><surname>Lavooy</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A review of the methods of studies on infant ultrasound production and maternal retrieval in small rodents</article-title><source>Behavior Genetics</source><volume>35</volume><fpage>31</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1007/s10519-004-0854-7</pub-id><pub-id pub-id-type="pmid">15674531</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffmann</surname> <given-names>F</given-names></name><name><surname>Musolf</surname> <given-names>K</given-names></name><name><surname>Penn</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spectrographic analyses reveal signals of individuality and kinship in the ultrasonic courtship vocalizations of wild house mice</article-title><source>Physiology &amp; Behavior</source><volume>105</volume><fpage>766</fpage><lpage>771</lpage><pub-id pub-id-type="doi">10.1016/j.physbeh.2011.10.011</pub-id><pub-id pub-id-type="pmid">22037196</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holy</surname> <given-names>TE</given-names></name><name><surname>Guo</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Ultrasonic songs of male mice</article-title><source>PLOS Biology</source><volume>3</volume><elocation-id>e386</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030386</pub-id><pub-id pub-id-type="pmid">16248680</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname> <given-names>A</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imagenet classification with deep convolutional neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McLachlan</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Discriminant Analysis and Statistical Pattern Recognition</source><publisher-name>John Wiley &amp; Sons</publisher-name><pub-id pub-id-type="doi">10.1002/0471725293</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neunuebel</surname> <given-names>JP</given-names></name><name><surname>Taylor</surname> <given-names>AL</given-names></name><name><surname>Arthur</surname> <given-names>BJ</given-names></name><name><surname>Egnor</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Female mice ultrasonically interact with males during courtship displays</article-title><source>eLife</source><volume>4</volume><elocation-id>e06203</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06203</pub-id><pub-id pub-id-type="pmid">26020291</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noirot</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>Ultrasounds and maternal behavior in small rodents</article-title><source>Developmental Psychobiology</source><volume>5</volume><fpage>371</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.1002/dev.420050410</pub-id><pub-id pub-id-type="pmid">4609822</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nyby</surname> <given-names>J</given-names></name><name><surname>Dizinno</surname> <given-names>GA</given-names></name><name><surname>Whitney</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Social status and ultrasonic vocalizations of male mice</article-title><source>Behavioral Biology</source><volume>18</volume><fpage>285</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1016/S0091-6773(76)92198-2</pub-id><pub-id pub-id-type="pmid">999582</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nyby</surname> <given-names>J</given-names></name><name><surname>Dizinno</surname> <given-names>G</given-names></name><name><surname>Whitney</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1977">1977a</year><article-title>Sexual dimorphism in ultrasonic vocalizations of mice (<italic>Mus musculus</italic>): gonadal hormone regulation</article-title><source>Journal of Comparative and Physiological Psychology</source><volume>91</volume><fpage>1424</fpage><lpage>1431</lpage><pub-id pub-id-type="doi">10.1037/h0077411</pub-id><pub-id pub-id-type="pmid">599200</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nyby</surname> <given-names>J</given-names></name><name><surname>Wysocki</surname> <given-names>CJ</given-names></name><name><surname>Whitney</surname> <given-names>G</given-names></name><name><surname>Dizinno</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1977">1977b</year><article-title>Pheromonal regulation of male mouse ultrasonic courtship (<italic>Mus musculus</italic>)</article-title><source>Animal Behaviour</source><volume>25</volume><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/0003-3472(77)90009-4</pub-id><pub-id pub-id-type="pmid">889149</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>O’neill</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Elementary Differential Geometry</source><publisher-name>Academic Press</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4471-3696-5</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname> <given-names>TD</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Quantifying behavior to understand the brain</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1537</fpage><lpage>1549</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00734-z</pub-id><pub-id pub-id-type="pmid">33169033</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sales</surname> <given-names>GD</given-names></name><name><surname>Smith</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Comparative studies of the ultrasonic calls of infant murid rodents</article-title><source>Developmental Psychobiology</source><volume>11</volume><fpage>595</fpage><lpage>619</lpage><pub-id pub-id-type="doi">10.1002/dev.420110609</pub-id><pub-id pub-id-type="pmid">720764</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scattoni</surname> <given-names>ML</given-names></name><name><surname>Gandhy</surname> <given-names>SU</given-names></name><name><surname>Ricceri</surname> <given-names>L</given-names></name><name><surname>Crawley</surname> <given-names>JN</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Unusual repertoire of vocalizations in the BTBR T+tf/J mouse model of autism</article-title><source>PLOS ONE</source><volume>3</volume><elocation-id>e3067</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0003067</pub-id><pub-id pub-id-type="pmid">18728777</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scattoni</surname> <given-names>ML</given-names></name><name><surname>Ricceri</surname> <given-names>L</given-names></name><name><surname>Crawley</surname> <given-names>JN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Unusual repertoire of vocalizations in adult BTBR T+tf/J mice during three types of social encounters</article-title><source>Genes, Brain and Behavior</source><volume>10</volume><fpage>44</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1111/j.1601-183X.2010.00623.x</pub-id><pub-id pub-id-type="pmid">20618443</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidhuber</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning in neural networks: an overview</article-title><source>Neural Networks</source><volume>61</volume><fpage>85</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2014.09.003</pub-id><pub-id pub-id-type="pmid">25462637</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slobodchikoff</surname> <given-names>CN</given-names></name><name><surname>Briggs</surname> <given-names>WR</given-names></name><name><surname>Dennis</surname> <given-names>PA</given-names></name><name><surname>Hodge</surname> <given-names>A-MC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Size and shape information serve as labels in the alarm calls of Gunnison’s prairie dogs Cynomys gunnisoni</article-title><source>Current Zoology</source><volume>58</volume><fpage>741</fpage><lpage>748</lpage><pub-id pub-id-type="doi">10.1093/czoolo/58.5.741</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tachibana</surname> <given-names>RO</given-names></name><name><surname>Kanno</surname> <given-names>K</given-names></name><name><surname>Okabe</surname> <given-names>S</given-names></name><name><surname>Kobayasi</surname> <given-names>KI</given-names></name><name><surname>Okanoya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>USVSEG: a robust method for segmentation of ultrasonic vocalizations in rodents</article-title><source>PLOS ONE</source><volume>15</volume><elocation-id>e0228907</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0228907</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tinbergen</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>On aims and methods of ethology</article-title><source>Zeitschrift Für Tierpsychologie</source><volume>20</volume><fpage>410</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1111/j.1439-0310.1963.tb01161.x</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tuia</surname> <given-names>D</given-names></name><name><surname>Camps-Valls</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Kernel manifold alignment for domain adaptation</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0148655</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0148655</pub-id><pub-id pub-id-type="pmid">26872269</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Segbroeck</surname> <given-names>M</given-names></name><name><surname>Knoll</surname> <given-names>AT</given-names></name><name><surname>Levitt</surname> <given-names>P</given-names></name><name><surname>Narayanan</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>MUPET-Mouse ultrasonic profile ExTraction: a Signal Processing Tool for Rapid and Unsupervised Analysis of Ultrasonic Vocalizations</article-title><source>Neuron</source><volume>94</volume><fpage>465</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.005</pub-id><pub-id pub-id-type="pmid">28472651</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>C</given-names></name><name><surname>Mahadevan</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Heterogeneous domain adaptation using manifold alignment</article-title><conf-name>Twenty-Second International Joint Conference on Arti1cial Intelligence</conf-name><fpage>1541</fpage><lpage>1546</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zimmer</surname> <given-names>MR</given-names></name><name><surname>Fonseca</surname> <given-names>AHO</given-names></name><name><surname>Iyilikci</surname> <given-names>O</given-names></name><name><surname>Pra</surname> <given-names>RD</given-names></name><name><surname>Dietrich</surname> <given-names>MO</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Functional ontogeny of hypothalamic agrp neurons in neonatal mouse behaviors</article-title><source>Cell</source><volume>178</volume><fpage>44</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.04.026</pub-id><pub-id pub-id-type="pmid">31104844</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zippelius</surname> <given-names>H-M</given-names></name><name><surname>Schleidt</surname> <given-names>WM</given-names></name></person-group><year iso-8601-date="1956">1956</year><article-title>Ultraschall-Laute bei jungen Mäusen</article-title><source>Die Naturwissenschaften</source><volume>43</volume><elocation-id>502</elocation-id><pub-id pub-id-type="doi">10.1007/BF00632534</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.59161.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kimchi</surname><given-names>Tali</given-names></name><role>Reviewing Editor</role><aff><institution>Weizmann Institute of Science</institution><country>Israel</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>The authors developed a new software (&quot;VocalMat&quot;) to automatically detect and classify mouse ultrasonic vocalizations into distinct categories. The software is based on tools of image processing and neural network classification of spectrograms, that is useful to analyze large dataset of pup and adult USVs in various mouse models and experimental designs. All the datasets collected and the software source codes are freely accessible.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Analysis of ultrasonic vocalizations from mice using computer vision and machine learning&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Andrew King as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>As the editors have judged that your manuscript is of interest, but as described below that additional experiments are required before it is published, we would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). First, because many researchers have temporarily lost access to the labs, we will give authors as much time as they need to submit revised manuscripts. We are also offering, if you choose, to post the manuscript to bioRxiv (if it is not already there) along with this decision letter and a formal designation that the manuscript is &quot;in revision at <italic>eLife</italic>&quot;. Please let us know if you would like to pursue this option. (If your work is more suitable for medRxiv, you will need to post the preprint yourself, as the mechanisms for us to do so are still in development.)</p><p>Summary:</p><p>This manuscript presents new tool to detect and classify mice ultrasonic vocalizations (USVs). The tool ( VocalMat) applies neural network technology for categorization of the various USVs to predetermined categories of pup calls. The paper in the form submitted seems to fit more as a methodology paper. Indeed, the authors state that the goal of their work is to: &quot;create a tool with high accuracy for USV detection that allows for the flexible use of any classification method.&quot;</p><p>The paper is well written and presents a useful tool to identify and classify USVs of mice. However, the reviewers think that the authors did not provide enough supporting evidence to claim that their method is significantly superior to other tools in the literature that attempted USV classification. For example Vogel et al. (2019) [https://doi.org/10.1038/s41598-019-44221-3], reported very similar (85%) accuracy using more mainstream ML approaches than attempted in this study with CNNs.</p><p>Moreover, some of the reviewers were not convinced that the comparison to other tools was conducted in an unbiased and completely fair manner and that the approach described in this paper really represents a significant advantage over other tools. For example, two reviewers claim that the authors used DeepSqueak on their dataset without properly training it for this type of data, while their tool is specifically trained for it. Also, the reviewers expect to see a confusion matrix to assess model performance and establish whether the model does indeed replicate accurately classes (or how skewed it is with dominating classes).</p><p>Overall, all the reviewers agree that they would like to see a more rigorous attempt to validate the findings presented (ideally also on an external database) and proper (unbiased) comparison with other similar software, to justify the claim that VocalMat performance in classification of USVs is indeed superior and novel to the methods already in use.</p><p>If the authors wish to have the manuscript considered as a research paper and not in the form of methods paper they should change the focus of the paper and provide more data showing a novel biological application of their pup calls classification findings. If not, we will be happy to consider a suitably revised version of the manuscript for the Tools and Resources section of <italic>eLife</italic>.</p><p>For your convenience the detailed comments of the 3 reviewers are included below.</p><p><italic>Reviewer #1:</italic></p><p>In the manuscript entitled &quot;Analysis of ultrasonic vocalizations from mice using computer vision and machine learning&quot;, Fonseca at al. present a novel computational tool for analysis of mice ultrasonic vocalizations (USVs). This tool aims to (1) detect USV candidates from audio clips; (2) separate the USVs from the noise; (3) apply neural network technology for categorization of the various USVs to predetermined categories; They use this tool to analyze a large dataset of pup calls and validate their tool as well as compare it with other computational tools published in the last decade. Finally, they show how they can use diffusion maps and manifold alignment to distinguish between calls of distinct groups of pups.</p><p>This tool is nice, but rather limited in its abilities and do not represent a conceptual or technical breakthrough. As for limitations, the tool presented here is designed, trained and validated for a specific type of murine calls (pup calls) and a predefined set of 11 categories, which may not cover all possible categories. As for technical advancement, the software combines criteria-based detection with neural network classification, similarly to previously published tools mentioned by the authors. Moreover, although the authors claim superiority of their software over published tools, I wasn't convinced that this comparison was conducted in an unbiased and completely fair manner and that their tool really represents a significant advantage on other tools. For example, they used DeepSqueak on their dataset without properly training it for this type of data, while their tool is specifically trained for it. Moreover, the success rates the authors report for other tools here are significantly lower than those reported by the relevant publications, and do not fit comparisons made by others. Also, I wasn't convinced that the software presented here will be working just as well as the authors claim on a distinct set of data coming from another lab and recorded in distinct conditions. My attempt to activate the software failed due to an error.</p><p><italic>Reviewer #2:</italic></p><p>Nice study overall, well-articulated and easy to follow. I also highly commend the authors for making all data + source code available.</p><p>1. The goal of the study could be encompassing a wider aim. The authors state: &quot;Our goal was to create a tool with high accuracy for USV detection that allows for the flexible use of any classification method.&quot; The task of USV detection is relatively simple (no surprise the authors get 98% accuracy), it is the accurate classification of the USV types that is of particular importance. I would suggest the authors rephrase to emphasize that aspect.</p><p>2. &quot;The output of the vocal classification provides the additional benefit of a probability distribution of vocal classes, allowing for the use of nonlinear dimensionality reduction techniques to analyze the vocal repertoire&quot;. Probably this needs some rephrasing into something like 'the output of the classifier enables the exploration of the probabilistic class membership'. More importantly though, that is not a pre-requisite for any dimensionality reduction techniques (linear or not). Dimensionality reduction could be directly applied to the extracted features, it is not dependent upon the classifier outputs.</p><p>3. &quot;A linear regression analysis between manually validated data from different audio files and the true positives of the CNN revealed an almost-perfect linearity.…&quot; I would expect to see a simple confusion matrix here assessing whether each of the USVs was correctly detected, rather than quantifying the number of USVs within each phonation (in which case indeed the methodology attempted by the authors would be appropriate). I think it is far more useful to assess on a case by case basis the USVs, and potentially determine whether e.g. one or more of the raw files was challenging for any reason. The authors could provide multiple confusion matrices 11x11 e.g. as a Supplementary Excel file.</p><p>4. &quot;In order to evaluate the performance of VocalMat in detecting USVs compared to other published tools, we analyzed the same test dataset…&quot; The authors' tool has an unfair advantage in this case, in that their algorithm was trained on the data collected under identical conditions like the test data. Moreover, the test data contains USVs from the same mouse.</p><p>5. &quot;In summary, VocalMat is a new tool to detect and classify mouse USVs with superior sensitivity and accuracy while keeping all the relevant spectral features…&quot; It is not clear if the authors mean superior to other studies in the literature that attempted USV classification. For example Vogel et al. (2019) [https://doi.org/10.1038/s41598-019-44221-3], reported very similar (85%) accuracy using more mainstream ML approaches than attempted in this study with CNNs.</p><p>6. If I understand correctly the methodology the authors used uses a single split of the data into training and testing, and data from a mouse could end up in both; thus the authors do not necessarily prove that their methods generalize well in new datasets. I would welcome the use of e.g. leave-one-mouse out assessment, and also the use of an additional external dataset collected under perhaps slightly different lab conditions (different lab) to see how well findings generalize. CNNs are extremely sensitive, and theoretical work has shown that adding imperceptible (visually) noise in images results in complete different results.</p><p><italic>Reviewer #3:</italic></p><p>The technical details should be better explained, with formulas and / or algorithm descriptions in one piece (not separated in main part and methods).</p><p>– Could you do a sensitivity analysis on their model for different number of observations?</p><p>– What is the efficiency of the method?</p><p>– Some state-of the art comparisons are missing: Did you compare it to the Automatic Mouse Ultrasound Detector? To DeepSqueak for classification? (This was only used for detection right?)</p><p>– What are the number of mice and number of records per mouse? How did you get them to vocalize?</p><p>– You have only used recordings of pups (5-15 d old). Do the results apply to adult mice?</p><p>– You used recordings of 5 strains of lab mice. Did you test or control for strain differences?</p><p>– There are many classifications that have been proposed, and so what was the basis for your syllable type classification? Can you please explain / motivate that more? Can mice discriminate these syllable types?</p><p>– How do you deal with the general machine learning problem that no insights are provided into the features that the model uses to classify syllable types ?</p><p>Is this method useful for recordings with a noisy background? Is there any reason to suspect that it might not work?</p><p>page1</p><p>The comments on the Abstract</p><p>&quot;…detected more than &gt;98% of the USVs&quot; – What is the ground truth?</p><p>page 2</p><p>&quot;…high accuracy for USV detection that&quot; – What is the ground truth? Please mention here.</p><p>&quot;…allows for the flexible use of any classification method.&quot; – What does that mean? /</p><p>&quot;…USVs, outperforming previous tools&quot; – Where is this shown? Where is the comparison on the same dataset?</p><p>…a highly accurate software to detect and..&quot; – Mention that this is a supervised approach.</p><p>&quot;…spectrogram is then analyzed in terms of its time-frequency plane,..&quot; – What do you mean? The spectogram is already in the TF plan.</p><p>&quot;…harmonic components (Figure 1D)&quot; – To support reproducible research and open science, please provide the algorithm to the reader. In an open science approach provide the scripts (and data) to reproduce the tables and figures, where possible.</p><p>page 3</p><p>&quot;…we manually inspected the spectrograms and labeled..&quot; – Who is &quot;we&quot;? Experts on USVs? The authors? Did you listen to the resampled USVs? What is the inter-human detection rate? So what is the difference between experts? How did you select the test data?</p><p>&quot;…the manual counting (4,441..)&quot; – So a lot of FPs, right? Please report all the FPS and Fns.</p><p>&quot;… artifacts of the segmentation process&quot; – Where does this come from?</p><p>page 5</p><p>&quot;…) to the sum over..&quot; – Why the sum?</p><p>&quot;…) to the sum over..&quot; whole paragraph – Please represent your results in a figure or table! This is very tough to read and digest.…..</p><p>&quot;Therefore, based on these results, we used the calculated inflection..&quot; – Do I understand it right, that you criticize previous work because they required users to manually determine thresholds, but do the same for different parts of your model? For example with the inflection point.</p><p>&quot;..In the test dataset,&quot; – How does the 14,5% FP rate compare to the one stated above?</p><p>&quot;..In the test dataset,&quot; – which 7 USVS? out of 13 missing USVs?</p><p>&quot;…compared the score for the label &quot;noise&quot;&quot; – the linear regression between what?</p><p>&quot;…suggesting high accuracy of VocalMat&quot; – So about 4% of USVs are labeled as noise, right?</p><p>&quot;…The rate of detected noise labeled as USV (false positive) was 5.60&quot; – 42 out of 750 FPs, are falsely labeled as USVs, this probably should be mentioned in the abstract.</p><p>&quot;Finally, the rate of USVs not detected (missed rate) was 0.28 Â{plus minus} 0.09% (mean Â{plus minus} SEM; median = 0.23; 95% CI [0.05, 0.51]).&quot; delete all this – &quot;USVs not detected..&quot; should not be discussed here. It is not related to classification problem and it is already discussed in section 2.2.</p><p>&quot;identify approximately 1 in 75&quot; – &quot;identify approximately 1 in 71&quot; – TNR and FPR are complementary so reporting one is enough. And 1 of 18 noises remains in the data.</p><p>&quot;Characteristics of mislabeled USV candidates by VocalMat&quot; – this paragraph does not give additional information, in my oppinion.</p><p>page 6</p><p>&quot;(Table S1).&quot; – Please add &quot;in the appendix&quot;</p><p>whole paragraph – Again a very ugly layout.. Why not put tables here, for example.</p><p>Why did you not try other pre-trained networks of DSQ for detection?</p><p>Your training data is highly imbalanced in classes U, C3, C and step down. Please discuss!</p><p>page 7</p><p>first paragraph – Again: Please do a graph or table!</p><p>&quot;we manually inspected the spectrogram of the sample..&quot; – Why? Do the same on the existing data set?</p><p>&quot;We compared all four experimental conditions against each other and visually..&quot; – Please compare to a SVM clustering.</p><p>&quot; Since we did not train DeepSqueak with our..&quot; – True, but your model also requires adaptation, so please argue why this is still a fair comparison.</p><p>&quot;..when compared to DeepSqueak.&quot; – What about false positives?</p><p>&quot;Detection of harmonic components&quot; – Why did you report the detection rate for harmonics separately?</p><p>&quot;..method to analyze the vocal repertoire of mice.&quot; – Which classes are grouped in a same class?</p><p>&quot;The difference between the two groups was that in the treatment group,..&quot; – Why did you not compare classification results? But used used diffusion maps instead?.</p><p>page 9</p><p>&quot;…study that reported the sensitivity to..&quot; – What about specificity?</p><p>&quot;..This characteristic provides the possibility for unique..&quot; – How feasible is it to use classifier for other vocal types?</p><p>page 10</p><p>&quot;..a euclidean space..&quot; – &quot;..an Euclidean space..&quot; – Is the caption of Figure 5 further explained in text?</p><p>page 11</p><p>(1) – give a reference, please! Which implementation did you use?</p><p>&quot;Normalization and contrast enhancement&quot; – Where significant parts cut out in the TF plane?</p><p>(3) – Add a reference, please.</p><p>&quot;adjusted image&quot; – What is considered one image? This means what regions was used to find L<sub>in</sub> and H<sub>ic</sub>?</p><p>&quot;We used a high pass filter (45 kHz) to eliminate sources of noise in the audible..&quot; – Is not 45 kHz a very high threshold?</p><p>&quot;..contrast adjustment..&quot; – What is the adjusted image exactely?</p><p>&quot;.. If the value of the current pixel i&quot; – What is t?</p><p>page 12</p><p>&quot;..it was empirically chosen as t = 0.2 for our application. &quot; – This contradicts the claim in the introduction, no?</p><p>&quot;segments&quot; – The super-windows?</p><p>rest of the paragraph – Please provide details how exactly?</p><p>&quot;Local Median Filter&quot; – Please define what a segment is!</p><p>First two sentence – What do you mean? Unclear! Where does the segmentation noise come from?</p><p>&quot; of the pixels in a window that contains..&quot; – Unclear, please provide details!</p><p>&quot;.file (now referred to as \Upsilon).&quot; – What is this? Where is the starting point?Is it the whole segment? The frequency curve? A binary spectrogram?</p><p>&quot;local parametric equations&quot; – Why? Please motivate!</p><p>(9) What do you mean by that? What is the structure of \Upsilon'?</p><p>&quot;..it was empirically chosen as t = 0.2 for our application. &quot; – Why and how?</p><p>&quot;The inflection point is then determined as the point with maximum..&quot; – How is the maximum curvature calculated? This is again a manual set value, right?</p><p>Is tau calculated for each audio file separately? Why or why not?</p><p>page 13</p><p>&quot;Our training dataset consisted of..&quot; – This makes the classification task easier, right? Is this fair?</p><p>page 14</p><p>Table 1 – Please explain!</p><p>Table2 – This is clear, no?</p><p>&quot;set (90%) and a validation..&quot; – How? randomly?</p><p>&quot; Diffusion maps for output visualization &quot; – What do you mean here?</p><p>..&quot;USVs as a distribution of probabilities..&quot; – Really? This is explained in the following, right?</p><p>&quot;..clustering of USVs..&quot; – Explain (or put it later after definitions!) Visualization or clustering?</p><p>&quot;Euclidean manifold&quot; – Of which dimension? What do you mean? R<sup>11</sup> is also Euliclidean!</p><p>&quot;bandwith&quot; – of what?</p><p>&quot;..of the same label.&quot; – How? Maximum intra-cluster distance?</p><p>page 15</p><p>&quot;..&quot;Euclidean space..&quot; – What is this space? R<sup>2</sup>?</p><p>&quot;s idea of moving from node to node&quot; – How? Provide details?</p><p>(14) What is e<sub>i</sub>? e<sub>j</sub>? Please define!</p><p>&quot;d through SVD decomposition&quot; – &quot;d through a SVD decomposition&quot;</p><p>&quot;Ms = D1/&quot; – &quot;Ms : = D1/&quot; (to make clear this is a defintion.)</p><p>&quot;..sake of notation, consider:&quot; – &quot;sake of notation, consider (for a unitary \Omega)&quot;</p><p>(16), (17), (18) – This is not new, so give a reference, please.</p><p>&quot;manifolds, we considered this a transfer learning problem&quot; – Explain, please!</p><p>&quot;L<sub>s</sub>&quot; and &quot;L<sub>d</sub>&quot; – please give formal definitions.</p><p>Notation has to be clarified, keep fixed and precisely defined</p><p>page 16</p><p>(19) – In this setting n and d are constant, right? Please discuss!</p><p>&quot;…n topology preservation (µ &gt; 1).&quot; – Please either use more or less details. Either put a reference and don't explain, or explain in more details.</p><p>&quot;… <inline-formula><mml:math id="inf138"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:msubsup><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>&quot; – Why? Motivate!&quot;common space F&quot; – Which space? Is this defined?</p><p>&quot;f the samples in order to predict the other half&quot; – Here? Like above?</p><p>So the separation is not 90:10?</p><p>Is &quot;n&quot; defined?</p><p>page 17</p><p>&quot;…and Prism 8.0 were used to…&quot; – References, please!</p><p>&quot;edited&quot; – In how far? Please explain in full details, how you changed each figure!</p><p>&quot;Data were first subjected&quot; – Which data?</p><p>&quot;Shapiro-Wilk&quot; – Please add a reference!</p><p>&quot;When homogeneity was a&quot; – Was this assumed here or not? Please stat that preciesly.</p><p>&quot;critical data collection and insights&quot; – Why those lab members not co-authors?</p><p>page 18</p><p>&quot;shows an interval of 10 ms.&quot; – Nice clarification, but formulate in text as precise formula, please</p><p>page 20</p><p>Can you please give a summary table of comparison with other approaches?</p><p>page 21</p><p>[4] [5] – format all references in the same way, please! E.g first names!</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.59161.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>In the manuscript entitled &quot;Analysis of ultrasonic vocalizations from mice using computer vision and machine learning&quot;, Fonseca at al. present a novel computational tool for analysis of mice ultrasonic vocalizations (USVs). This tool aims to (1) detect USV candidates from audio clips; (2) separate the USVs from the noise; (3) apply neural network technology for categorization of the various USVs to predetermined categories; They use this tool to analyze a large dataset of pup calls and validate their tool as well as compare it with other computational tools published in the last decade. Finally, they show how they can use diffusion maps and manifold alignment to distinguish between calls of distinct groups of pups.</p><p>This tool is nice, but rather limited in its abilities and do not represent a conceptual or technical breakthrough. As for limitations, the tool presented here is designed, trained and validated for a specific type of murine calls (pup calls) and a predefined set of 11 categories, which may not cover all possible categories. As for technical advancement, the software combines criteria-based detection with neural network classification, similarly to previously published tools mentioned by the authors.</p></disp-quote><p>As pointed out by the reviewers and in the manuscript, there are several tools available for USV detection. These tools typically need substantial user inputs and customization to obtain high performance. Our goal was to develop a tool that can be broadly used by the community with minimal (in fact, no) need for user inputs. VocalMat outperforms all other tools, at least in the datasets that were made available by the developers. Thus, we believe VocalMat will largely benefit the community by providing a highly accurate software that can be easily used.</p><disp-quote content-type="editor-comment"><p>Moreover, although the authors claim superiority of their software over published tools, I wasn't convinced that this comparison was conducted in an unbiased and completely fair manner and that their tool really represents a significant advantage on other tools. For example, they used DeepSqueak on their dataset without properly training it for this type of data, while their tool is specifically trained for it. Moreover, the success rates the authors report for other tools here are significantly lower than those reported by the relevant publications, and do not fit comparisons made by others.</p></disp-quote><p>The reviewer raises an important concern. Indeed we have not trained DeepSqueak on our dataset. Doing so would take a reasonable amount of time given that DeepSqueak demands a substantial amount of manual interactions to refine their detection network, which includes manually rejecting or accepting detected calls and manually refining the threshold for power or another &quot;score” used for detection. We did contact the developers of DeepSqueak to request additional audio recordings that they used to validate their tool. Unfortunately, the developers declined our request and maintained all datasets private precluding a more fair comparison among tools. Importantly, in their single audio recording made publicly available, VocalMat outperformed DeepSqueak. This is important because for this audio recording, DeepSqueak was trained on their dataset and showed lower performance than VocalMat that was trained in a completely different dataset. We have also expanded our comparisons with other tools by validating the dataset from USVSEG, which includes 15 audio recordings from mice in different experimental conditions. In these audio recordings, VocalMat also outperformed USVSEG in addition to identifying vocalizations that were often missed.</p><disp-quote content-type="editor-comment"><p>Also, I wasn't convinced that the software presented here will be working just as well as the authors claim on a distinct set of data coming from another lab and recorded in distinct conditions. My attempt to activate the software failed due to an error.</p></disp-quote><p>We understand the concerns of the reviewer. As we stated above, we have tried to obtain validated datasets from different laboratories but have failed with the exception of the developers of USVSEG. (It is relevant to emphasize that we made all our data and validation files publicly available since we deposited the manuscript on bioRxiv; unfortunately, most of the other tools do not make their datasets publicly available, making direct comparisons among tools difficult to perform). With the new datasets from USVSEG, we show that in audio files from different laboratories and in different experimental conditions, VocalMat still performs extremely well and outperforms other tools.</p><p>We sincerely apologize for the problems the reviewer faced when trying to use VocalMat. Shortly after our submission, we identified an URL check error in the installation procedure provided on our GitHub page and have updated it since.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>Nice study overall, well-articulated and easy to follow. I also highly commend the authors for making all data + source code available.</p><p>1. The goal of the study could be encompassing a wider aim. The authors state: &quot;Our goal was to create a tool with high accuracy for USV detection that allows for the flexible use of any classification method.&quot; The task of USV detection is relatively simple (no surprise the authors get 98% accuracy), it is the accurate classification of the USV types that is of particular importance. I would suggest the authors rephrase to emphasize that aspect.</p></disp-quote><p>We thank that reviewer for the suggestion and we have now rephrased our study goal to encompass this wider aim.</p><disp-quote content-type="editor-comment"><p>2. &quot;The output of the vocal classification provides the additional benefit of a probability distribution of vocal classes, allowing for the use of nonlinear dimensionality reduction techniques to analyze the vocal repertoire&quot;. Probably this needs some rephrasing into something like 'the output of the classifier enables the exploration of the probabilistic class membership'. More importantly though, that is not a pre-requisite for any dimensionality reduction techniques (linear or not). Dimensionality reduction could be directly applied to the extracted features, it is not dependent upon the classifier outputs.</p></disp-quote><p>We thank the reviewer for the suggestion and we have now re-written this section of the manuscript to address this point.</p><disp-quote content-type="editor-comment"><p>3. &quot;A linear regression analysis between manually validated data from different audio files and the true positives of the CNN revealed an almost-perfect linearity.…&quot; I would expect to see a simple confusion matrix here assessing whether each of the USVs was correctly detected, rather than quantifying the number of USVs within each phonation (in which case indeed the methodology attempted by the authors would be appropriate). I think it is far more useful to assess on a case by case basis the USVs, and potentially determine whether e.g. one or more of the raw files was challenging for any reason. The authors could provide multiple confusion matrices 11x11 e.g. as a Supplementary Excel file.</p></disp-quote><p>We thank the reviewer for the suggestion and we have now added Table 1 to the manuscript with the statistics for the performance of VocalMat in detecting USVs. Additionally, we also added individual confusion matrices as suggested by the reviewer as a supplementary material (Figure S2).</p><disp-quote content-type="editor-comment"><p>4. &quot;In order to evaluate the performance of VocalMat in detecting USVs compared to other published tools, we analyzed the same test dataset…&quot; The authors' tool has an unfair advantage in this case, in that their algorithm was trained on the data collected under identical conditions like the test data. Moreover, the test data contains USVs from the same mouse.</p></disp-quote><p>We thank the reviewer for raising this point and we have re-written section 2.4 in the results (Performance of VocalMat compared to other tools) in line of this comment.</p><p>More precisely, to account for performance bias towards the dataset used for training and evaluation, we have tested VocalMat in two datasets from other laboratories. First, we now provide comparisons between VocalMat and USVSEG in 15 audio recordings provided by USVSEG. We show that in these audio recordings, VocalMat still performs exceptionally well and outperforms USVSEG. Second, we use the single example audio recording provided by DeepSqueak to test the performance of VocalMat and show that VocalMat outperforms DeepSqueak. (Note: the DeepSqueak developers declined our request to provide additional audio recordings that were used to test their performance for our validation). Thus, VocalMat performs well in a variety of datasets collected at different laboratories and experimental conditions.</p><p>Finally, It is also important to note that the test data do not contain USVs from the same mouse (we have now clarified this point in the text). We used a variety of experimental conditions to create our training and testing datasets and we did not use the same audio recordings for both. Thus, even though the data were generated in the same laboratory, we tried to incorporate as much variability as possible in our datasets. We have also clarified these points in the methods.</p><disp-quote content-type="editor-comment"><p>5. &quot;In summary, VocalMat is a new tool to detect and classify mouse USVs with superior sensitivity and accuracy while keeping all the relevant spectral features…&quot; It is not clear if the authors mean superior to other studies in the literature that attempted USV classification. For example Vogel et al. (2019) [https://doi.org/10.1038/s41598-019-44221-3], reported very similar (85%) accuracy using more mainstream ML approaches than attempted in this study with CNNs.</p></disp-quote><p>We thank the reviewer for the comment. We have now edited this concluding sentence to avoid comparisons with other tools.</p><p>Nevertheless, we would like to clarify our choice of using CNNs. For the proper use of techniques such as Random Forest and SVM, it is important for the input data (extraction of features of the USVs) to be of very high quality. In Vogel et al. (2019), the authors used commercially available software (Avisoft) to extract features from the detected USVs. While Avisoft performs reasonably well in stable conditions with no noise, in our experience the performance of Avisoft deteriorates in noisy conditions and also needs intense parameter tuning when running several audio files. This is a laborious process that precludes high throughput analysis of audio recordings. In contrast to RF and SVM, CNNs can extract features from raw images, making the method more robust to noise. (In fact, we have originally tested RF and show lower performance compared to CNNs).</p><disp-quote content-type="editor-comment"><p>6. If I understand correctly the methodology the authors used uses a single split of the data into training and testing, and data from a mouse could end up in both; thus the authors do not necessarily prove that their methods generalize well in new datasets. I would welcome the use of e.g. leave-one-mouse out assessment, and also the use of an additional external dataset collected under perhaps slightly different lab conditions (different lab) to see how well findings generalize. CNNs are extremely sensitive, and theoretical work has shown that adding imperceptible (visually) noise in images results in complete different results.</p></disp-quote><p>We thank the reviewer again for raising this important point and we have clarified these points in the text. Importantly, the training and test datasets were two independent groups of audio recordings. Thus, the data from one mouse was only used in one of the datasets. There was no overlap between the datasets. We have also tested VocalMat in audio recordings used by USVSEG (15 recordings) and DeepSqueak (1 recording) and show high accuracy of VocalMat in detecting USVs.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>The technical details should be better explained, with formulas and / or algorithm descriptions in one piece (not separated in main part and methods).</p><p>– Could you do a sensitivity analysis on their model for different number of observations?</p></disp-quote><p>We thank the reviewer for the suggestion and we have now incorporated more performance tests in the text, including a new Table 1 that provides statistics of performance in different audio recordings with different number of observations.</p><disp-quote content-type="editor-comment"><p>– What is the efficiency of the method?</p></disp-quote><p>As stated above, we now provide several measures of software performance for VocalMat on Table 1 using seven independent audio recordings demonstrating an accuracy between 97.84-99.24%. We have also tested audio recordings from other groups and we now provide a statistical description of the performance of VocalMat in the text (section 2.4).</p><disp-quote content-type="editor-comment"><p>– Some state-of the art comparisons are missing: Did you compare it to the Automatic Mouse Ultrasound Detector? To DeepSqueak for classification? (This was only used for detection right?)</p></disp-quote><p>We thank the reviewer for raising this important point. We did not compare our detection performance with A-MUD as other tools have shown better performance and usability. Thus, we decided to compare VocalMat to USVSEG and DeepSqueak. These two softwares also had audio files that were made publicly available (USVSEG provided 15 audio recordings and DeepSqueak provided 1 audio recording – the developers of DeepSqueak declined our request to provide more of their validation files). We now provide the details of these comparisons on section 2.4. We also used these two softwares in addition to MUPET and Ax to test their performance on our fully curated test dataset (also in section 2.4). It is important to emphasize two points. First, our dataset is the first fully curated: different investigators manually inspected the entire spectrogram and labeled each USV, allowing the quantification of missed vocalizations (other softwares do not appear to do the same). Second, since the original submission of this manuscript, we made all our files and validation publicly available (with exception of USVSEG, other tools do not make their datasets and validation files publicly available and DeepSqueek even declined our request for access to additional validation files preventing further comparative analysis). In terms of classification, DeepSqueak’s classification network is trained on only 5 USV categories, which would not allow a direct comparison between the tools regarding their classification performance.</p><disp-quote content-type="editor-comment"><p>– What are the number of mice and number of records per mouse? How did you get them to vocalize?</p></disp-quote><p>We thank the reviewer for asking these questions and we have now clarified the number of animals in the methods (each mouse was only recorded once) and also the origins of the audio recordings. We have tested young mouse pups that vocalize when separated from the home nest and also tested audio recordings from adult mice that vocalize when in social context.</p><disp-quote content-type="editor-comment"><p>– You have only used recordings of pups (5-15 d old). Do the results apply to adult mice?</p></disp-quote><p>We have now provided validation of VocalMat performance on a set of adult recordings from a different laboratory (USVSEG dataset) and show excellent performance of VocalMat without the need of any parameter change.</p><disp-quote content-type="editor-comment"><p>– You used recordings of 5 strains of lab mice. Did you test or control for strain differences?</p></disp-quote><p>Our goal was to use several strains of mice and several experimental conditions to maximize variability of data for the development of a more powerful tool. We have now clarified this point in the text. We have not tested or controlled for strain differences.</p><disp-quote content-type="editor-comment"><p>– There are many classifications that have been proposed, and so what was the basis for your syllable type classification? Can you please explain / motivate that more? Can mice discriminate these syllable types?</p></disp-quote><p>We used the classification system generally used by experts in the field (Scattoni et al. (2008, 2011) and Grimsley et al. (2011)). Previous works using a similar definition of USV types show that mice modulate their call type use based on context (Warren et al. (2018) and Matsumoto et al. (2018)), which indicates that mice are capable of discriminating the different types. We have clarified this point in the text.</p><disp-quote content-type="editor-comment"><p>– How do you deal with the general machine learning problem that no insights are provided into the features that the model uses to classify syllable types ?</p></disp-quote><p>In the context of supervised learning, such as what our model uses, the features are implicitly defined by the user assigning the labels to the USVs that compose the training dataset. Thus, the machine learns recurrent patterns or features that link all the samples with similar labels. The correct classification of vocalizations that represent the stereotyped version of a given vocal class suggests the machine learned the appropriate features for the classification task.</p><disp-quote content-type="editor-comment"><p>Is this method useful for recordings with a noisy background? Is there any reason to suspect that it might not work?</p></disp-quote><p>This is a good point raised by the reviewer. We have clarified in the text that we used audio recordings in very different experimental conditions, including with high ambient noise (we provide these details now in the methods and main text). We have also tested datasets from different laboratories. In our experience, VocalMat performs exceptionally well in audio recordings with ample variability in environmental noise.</p><disp-quote content-type="editor-comment"><p>page1</p><p>The comments on the Abstract</p><p>&quot;…detected more than &gt;98% of the USVs&quot; –What is the ground truth?</p><p>page 2</p><p>&quot;…high accuracy for USV detection that&quot; – What is the ground truth? Please mention here.</p></disp-quote><p>We have clarified these points by stating in the abstract and introduction that these were manually validated audio recordings. We have also clarified the ground-truth in the text and methods.</p><disp-quote content-type="editor-comment"><p>&quot;…allows for the flexible use of any classification method.&quot; – What does that mean?</p></disp-quote><p>We have removed this sentence from the text to avoid confusion.</p><disp-quote content-type="editor-comment"><p>&quot;…USVs, outperforming previous tools&quot; – Where is this shown? Where is the comparison on the same dataset?</p></disp-quote><p>We thank the reviewer for raising this point. In response to this and other queries, we have now re-written section 2.4 in the results describing in more detail that comparison with other tools. We have used 3 different sets of audio recordings. Two sets were provided by the developers of different tools and one set was curated in our laboratory. We have clarified these points throughout the text.</p><disp-quote content-type="editor-comment"><p>…a highly accurate software to detect and..&quot; – Mention that this is a supervised approach.</p></disp-quote><p>Thank you for your suggestion. We have clarified that VocalMat uses a supervised approach.</p><disp-quote content-type="editor-comment"><p>&quot;…spectrogram is then analyzed in terms of its time-frequency plane,..&quot; – What do you mean? The spectogram is already in the TF plan.</p></disp-quote><p>Thank you for the comment. We have corrected the sentence.</p><disp-quote content-type="editor-comment"><p>&quot;…harmonic components (Figure 1D)&quot; – To support reproducible research and open science, please provide the algorithm to the reader. In an open science approach provide the scripts (and data) to reproduce the tables and figures, where possible.</p></disp-quote><p>We agree with the reviewer and since our original deposit of the manuscript on bioRxiv we have made all our datasets, all our validation files, and all our codes available.</p><disp-quote content-type="editor-comment"><p>page 3</p><p>&quot;…we manually inspected the spectrograms and labeled..&quot; – Who is &quot;we&quot;? Experts on USVs? The authors? Did you listen to the resampled USVs? What is the inter-human detection rate? So what is the difference between experts? How did you select the test data?</p></disp-quote><p>We thank the reviewer for these questions. We have now clarified in the text that our dataset was manually curated by two investigators trained to detect USVs in spectrograms and discrepancies were discussed with a third investigator. We made all our audio files and curated data publicly available. To the best of our knowledge this is the only dataset available with manually inspected spectrograms for the manual identification of each USV.</p><disp-quote content-type="editor-comment"><p>&quot;…the manual counting (4,441..&quot; – So a lot of FPs, right? Please report all the FPS and Fns).</p></disp-quote><p>We report all metrics of performance in sections 2.2 and 2.3 and provide a new Table 1 with summary of performance for each of the audio recordings tested.</p><disp-quote content-type="editor-comment"><p>&quot;… artifacts of the segmentation process&quot; – Where does this come from?</p></disp-quote><p>The artifacts originate from the sequences of operations performed on the image (of the spectrogram) in order to segment the objects of interest (USVs). Some of these operations may detect agglomeration of pixels as an object of interest, thus generating artifacts. The contrast filter step effectively eliminates these artifacts without eliminating real vocalizations that have low contrast.</p><disp-quote content-type="editor-comment"><p>page 5</p><p>&quot;…) to the sum over..&quot; – Why the sum?</p></disp-quote><p>The sum over all the USV types corresponding to the complement of the probability of a USV candidate being noise. In other words, P(USV) = 1- P(noise), which is equal to the sum of the probability distribution over all the USV types.</p><disp-quote content-type="editor-comment"><p>&quot;…) to the sum over..&quot; whole paragraph – Please represent your results in a figure or table! This is very tough to read and digest.…..</p></disp-quote><p>Thank you for your suggestion and we have now included Table 1 with the detail of these analyses.</p><disp-quote content-type="editor-comment"><p>&quot;Therefore, based on these results, we used the calculated inflection..&quot; – Do I understand it right, that you criticize previous work because they required users to manually determine thresholds, but do the same for different parts of your model? For example with the inflection point.</p></disp-quote><p>We thank the reviewer for bringing this point to our attention as we realized we haven’t made it clear in the original submission. The inflection point is calculated automatically for each audio file. There are no human inputs on setting this parameter. We have clarified this point in the text.</p><disp-quote content-type="editor-comment"><p>&quot;..In the test dataset,&quot; – How does the 14,5% FP rate compare to the one stated above?</p></disp-quote><p>The False Positive rate of 14.5% (750 of 5171 vocals) is not further discussed because it only refers to the contrast filter step and does not represent the final performance of VocalMat. We have updated the Results section and now include several metrics in Table 1, including the False Positives for each audio file in our test dataset.</p><disp-quote content-type="editor-comment"><p>&quot;..In the test dataset,&quot; – which 7 USVS? out of 13 missing USVs?</p></disp-quote><p>We have re-written the Results section to clarify these and other points. We have also provide a new Table 1 with the summary of the performance of VocalMat to facilitate reading the manuscript and comparing the numbers.</p><disp-quote content-type="editor-comment"><p>&quot;…compared the score for the label &quot;noise&quot;&quot; – the linear regression between what?</p></disp-quote><p>There is no linear regression associated with this sentence. This sentence was just an introduction to the paragraph and the analysis that followed. We have re-written the text for clarity.</p><disp-quote content-type="editor-comment"><p>&quot;…suggesting high accuracy of VocalMat&quot; – So about 4% of USVs are labeled as noise, right?</p></disp-quote><p>We have clarified this point in the text. In fact, VocalMat labels real</p><p>USVs as noise in only 0.96% of the cases. When we combined all missed</p><p>USVs--including the ones not detected by VocalMat--then the rate is 1.64%.</p><disp-quote content-type="editor-comment"><p>&quot;…The rate of detected noise labeled as USV (false positive) was 5.60&quot; – 42 out of 750 FPs, are falsely labeled as USVs, this probably should be mentioned in the abstract.</p></disp-quote><p>For clarity and simplicity we report the accuracy in the abstract, which takes into account the number of false positives.</p><disp-quote content-type="editor-comment"><p>&quot;Finally, the rate of USVs not detected (missed rate) was 0.28 Â{plus minus} 0.09% (mean Â{plus minus}</p><p>SEM; median = 0.23; 95% CI [0.05, 0.51]).&quot; delete all this – &quot;USVs not detected..&quot; should not be discussed here. It is not related to classification problem and it is already discussed in section 2.2.</p></disp-quote><p>Thank you for your suggestion, we have deleted this sentence from this part of the manuscript.</p><disp-quote content-type="editor-comment"><p>&quot;identify approximately 1 in 75&quot; – &quot;identify approximately 1 in 71&quot; – TNR and FPR are complementary so reporting one is enough. And 1 of 18 noises remains in the data.</p></disp-quote><p>Thank you for the comment.</p><disp-quote content-type="editor-comment"><p>&quot;Characteristics of mislabeled USV candidates by VocalMat&quot; – this paragraph does not give additional information, in my opinion.</p></disp-quote><p>Thank you for the comment. We decided to keep the paragraph to provide more transparency of the data to the reader and also to suggest ways to improve the classification method by flagging USVs with lower probabilities.</p><disp-quote content-type="editor-comment"><p>page 6</p><p>&quot;Table S1).&quot; – Please add &quot;in the appendix&quot;</p><p>whole paragraph – Again a very ugly layout.. Why not put tables here, for example.</p></disp-quote><p>Thank you for your suggestion. We have updated the text to include the reference to the appendix. We also now provide a summary of these results in the form of a table (Table 2 in Section 2.5).</p><disp-quote content-type="editor-comment"><p>Why did you not try other pre-trained networks of DSQ for detection?</p></disp-quote><p>Thank you for the suggestion. We have now tested the performance of DSQ with their own audio file and provide data on the performance of DSQ network. From these tests, it appears that VocalMat performs better than DSQ in detecting USVs without the need for any tuning of the network/parameters.</p><disp-quote content-type="editor-comment"><p>Your training data is highly imbalanced in classes U, C3, C and step down. Please discuss!</p></disp-quote><p>This is a great point. At this moment, we choose to use naturally occurring USVs in the training set and some specific types of USVs occur rather infrequently, making it difficult to obtain enough samples for training the network in equal amounts.</p><disp-quote content-type="editor-comment"><p>page 7</p><p>first paragraph – Again: Please do a graph or table!</p></disp-quote><p>Thank you for the suggestion. We have updated the Results section and added Table 2 with a summary of the results discussed in Section 2.5.</p><disp-quote content-type="editor-comment"><p>&quot;we manually inspected the spectrogram of the sample..&quot; – Why? Do the same on the existing data set?</p></disp-quote><p>Thank you for asking that question that we have clarified now in the text. Indeed we have manually validated all datasets and made them publicly and freely available to anyone who wants to use it to test their tools and software.</p><disp-quote content-type="editor-comment"><p>&quot;We compared all four experimental conditions against each other and visually..&quot; – Please compare to a SVM clustering.</p></disp-quote><p>Thank you for your suggestion, but it is not clear how SVM would contribute to the quantification of the similarity between the manifolds. Of course, we provide an example of how the data could be analyzed and many other tools can be used to compare the vocal repertoire of animals with existing tools and future developments.</p><disp-quote content-type="editor-comment"><p>&quot; Since we did not train DeepSqueak with our..&quot; – True, but your model also requires adaptation, so please argue why this is still a fair comparison.</p></disp-quote><p>This is another good point and we thank the reviewer for raising it.</p><p>In an attempt to address this point, we now show the performance of</p><p>VocalMat and DeepSqueak on the single audio recording provided by</p><p>DeepSqueak. Without any tuning of our model, VocalMat outperforms DeepSqueak. We would be glad to perform further tests with DeepSqueak’s dataset, but unfortunately the authors declined our request to share their validation dataset. Again, a policy that is contrary to us, as we made since the deposit of the preprint all our datasets and algorithms available.</p><disp-quote content-type="editor-comment"><p>&quot;..when compared to DeepSqueak.&quot; – What about false positives?</p></disp-quote><p>Thank you for inquiring about it. We have re-written the Results section and have added Table 2 with a summary of the performance for each tool. We report the False Discovery Rate which takes into account false positives (FDR=FP/(FP+TP)).</p><disp-quote content-type="editor-comment"><p>&quot;Detection of harmonic components&quot; – Why did you report the detection rate for harmonics separately?</p></disp-quote><p>Most other tools do not detect/classify harmonic components and it is not common sense to describe them in the field. Thus we decided to report that as an extra feature of VocalMat, as even direct comparisons with other tools are not possible.</p><disp-quote content-type="editor-comment"><p>&quot;..method to analyze the vocal repertoire of mice.&quot; – Which classes are grouped in a same class?</p></disp-quote><p>Classes are not grouped. The goal of this analysis is to show how much overlap between the classes exist regarding the probability distribution over the labels for each USV.</p><disp-quote content-type="editor-comment"><p>&quot;The difference between the two groups was that in the treatment group,..&quot; – Why did you not compare classification results? But used used diffusion maps instead?.</p></disp-quote><p>We compared the classification result in another report (Zimmer et al. (2019)). Here we used this large dataset as an example of how one could use the richness of the VocalMat’s output to analyze repertoire as probability distributions rather than just the assigned label by the classifier.</p><disp-quote content-type="editor-comment"><p>page 9</p><p>&quot;…study that reported the sensitivity to..&quot; – What about specificity?</p></disp-quote><p>Thank you for your comment. We do report the specificity (true negative rate). However, the study cited in this sentence does not provide details about their specificity and we unfortunately cannot discuss this metric in this context.</p><disp-quote content-type="editor-comment"><p>&quot;..This characteristic provides the possibility for unique..&quot; – How feasible is it to use classifier for other vocal types?</p></disp-quote><p>Definitely feasible, as long as it is trained to do so. With the pre-trained network, any new vocal type will be classified in function of the 11 types used for training.</p><disp-quote content-type="editor-comment"><p>page 10</p><p>&quot;..a euclidean space..&quot; – &quot;..an Euclidean space..&quot; – Is the caption of Figure 5 further explained in text?</p></disp-quote><p>Thank you for the comment. We will lead to editorial decision to use a or an in this case. We have consulted with a copyeditor who suggested to keep a as the word starts with a vowel that sounds like a consonant. We have also provided explanatory text in the manuscript as requested.</p><disp-quote content-type="editor-comment"><p>page 11</p><p>(1) – give a reference, please! Which implementation did you use?</p><p>&quot;Normalization and contrast enhancement&quot; – Where significant parts cut out in the TF plane?</p></disp-quote><p>We thank the reviewer for the comment. We have provided details about the method we used. We did not cut out any significant parts of the TF plane. We used a high pass filter at 45 kHz, as mice vocalize above this frequency.</p><disp-quote content-type="editor-comment"><p>(3) – Add a reference, please.</p><p>&quot;adjusted image&quot; – What is considered one image? This means what regions was used to find L<sub>in</sub> and H<sub>ic</sub>?</p></disp-quote><p>The full spectrogram being processed is considered the one image. L<sub>in</sub> and H<sub>in</sub> are obtained from the full spectrogram. We have included a reference to the MATLAB function used at each step of our detection pipeline.</p><disp-quote content-type="editor-comment"><p>&quot;We used a high pass filter (45 kHz) to eliminate sources of noise in the audible..&quot; – Is not 45 kHz a very high threshold?</p></disp-quote><p>According to experimental observations (Grimsley et al. 2011 and Heckman et al. 2016, for example) mice emit USVs above 45kHz, typically above 50kHz.</p><disp-quote content-type="editor-comment"><p>&quot;..contrast adjustment..&quot; – What is the adjusted image exactly?</p></disp-quote><p>The adjusted image is the image following the contrast enhancement step.</p><disp-quote content-type="editor-comment"><p>&quot;.. If the value of the current pixel i&quot; – What is t?</p></disp-quote><p>The intensity value of the pixel. We have updated the text.</p><disp-quote content-type="editor-comment"><p>page 12</p><p>&quot;..it was empirically chosen as t = 0.2 for our application. &quot; – This contradicts the claim in the introduction, no?</p></disp-quote><p>Our claim of having an automated method does not eliminate the existence of parameters, which are fixed and require no user input. We have updated the Results section and extended the comparison with other tools.</p><p>We now include several audio recordings from different labs and show that VocalMat outperforms other tools when using their own datasets. This provides further evidence that this set parameter does not require manual user input, as intended.</p><disp-quote content-type="editor-comment"><p>&quot;segments&quot; – The super-windows?</p><p>rest of the paragraph – Please provide details how exactly?</p><p>&quot;Local Median Filter&quot; – Please define what a segment is!</p><p>First two sentence – What do you mean? Unclear! Where does the segmentation noise come from?</p><p>&quot; of the pixels in a window that contains..&quot; – Unclear, please provide details!</p></disp-quote><p>We have updated the text to clarify these points.</p><disp-quote content-type="editor-comment"><p>&quot;.file (now referred to as \Upsilon).&quot; – What is this? Where is the starting point? Is it the whole segment? The frequency curve? A binary spectrogram?</p></disp-quote><p>Υ (\Upsilon) is the cumulative distribution function of the values of C<sub>k</sub> (C<sub>k</sub> is defined as the ratio of the intensity of the pixels belonging to a segment over the intensity of a window, or bounding box, around that segment).</p><disp-quote content-type="editor-comment"><p>&quot;local parametric equations&quot; – Why? Please motivate!</p></disp-quote><p>We describe in the text our goal of finding the inflection point of a function (the CDF of C<sub>k</sub> in this case). To that end, we model that function using local parametric equations and ultimately obtain the inflection point.</p><disp-quote content-type="editor-comment"><p>(9) What do you mean by that? What is the structure of \Upsilon'?</p><p>&quot;..it was empirically chosen as t = 0.2 for our application. &quot; – Why and how?</p><p>&quot;The inflection point is then determined as the point with maximum..&quot; – How is the maximum curvature calculated? This is again a manual set value, right?</p><p>Is tau calculated for each audio file separately? Why or why not?</p></disp-quote><p>Upsilon is the CDF of C<sub>k</sub>. The curvature is calculated as indicated in Equation (9) or (10). This is not a manually set value as indicated in the text.</p><disp-quote content-type="editor-comment"><p>page 13</p><p>&quot;Our training dataset consisted of..&quot; – This makes the classification task easier, right? Is this fair?</p></disp-quote><p>Our classification method uses supervised learning, so having a training dataset becomes a requirement.</p><disp-quote content-type="editor-comment"><p>page 14</p><p>Table 1 – Please explain!</p><p>Table2 – This is clear, no?</p><p>&quot;set (90%) and a validation..&quot; – How? randomly?</p><p>&quot; Diffusion maps for output visualization &quot; – What do you mean here?</p><p>..&quot;USVs as a distribution of probabilities..&quot; – Really? This is explained in the following, right?</p><p>&quot;..clustering of USVs..&quot; – Explain (or put it later after definitions!) Visualization or clustering?</p><p>&quot;Euclidean manifold&quot; – Of which dimension? What do you mean? R<sup>11</sup> is also Euliclidean!</p><p>&quot;bandwith&quot; – of what?</p><p>&quot;..of the same label.&quot; – How? Maximum intra-cluster distance?</p></disp-quote><p>We have updated the text to clarify these points.</p><disp-quote content-type="editor-comment"><p>page 15</p><p>&quot;..&quot;Euclidean space..&quot; – What is this space? R<sup>2</sup>?</p><p>&quot;s idea of moving from node to node&quot; – How? Provide details?</p><p>(14) What is e<sub>i</sub>? e<sub>j</sub>? Please define!</p><p>&quot;d through SVD decomposition&quot; – &quot;d through a SVD decomposition&quot;</p><p>&quot;Ms = D1/&quot; – &quot;Ms: = D1/&quot; (to make clear this is a defintion.)</p><p>&quot;..sake of notation, consider:&quot; – &quot;sake of notation, consider (for a unitary \Omega)&quot;</p><p>(16), (17), (18) – This is not new, so give a reference, please.</p><p>&quot;manifolds, we considered this a transfer learning problem&quot; – Explain, please!</p><p>&quot;L<sub>s</sub>&quot; and &quot;L<sub>d</sub>&quot; – please give formal definitions.</p><p>Notation has to be clarified, keep fixed and precisely defined</p></disp-quote><p>We have updated the text to clarify these points.</p><disp-quote content-type="editor-comment"><p>page 16</p><p>(19) – In this setting n and d are constant, right? Please discuss!</p><p>&quot;…n topology preservation (µ &gt; 1).&quot; – Please either use more or less details. Either put a reference and don't explain, or explain in more details.</p><p>&quot;… <inline-formula><mml:math id="inf139"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:msubsup><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>&quot; – Why? Motivate!&quot;common space F&quot; – Which space? Is this defined?</p><p>&quot;f the samples in order to predict the other half&quot; – Here? Like above?</p><p>So the separation is not 90:10?</p><p>Is &quot;n&quot; defined?</p></disp-quote><p>We have updated the text to clarify these points.</p><disp-quote content-type="editor-comment"><p>page 17</p><p>&quot;…and Prism 8.0 were used to…&quot; – References, please!</p><p>&quot;edited&quot; – In how far? Please explain in full details, how you changed each figure!</p><p>&quot;Data were first subjected&quot; – Which data?</p><p>&quot;Shapiro-Wilk&quot; – Please add a reference!</p><p>&quot;When homogeneity was a&quot; – Was this assumed here or not? Please stat that preciesly.</p><p>&quot;critical data collection and insights&quot; – Why those lab members not co-authors?</p></disp-quote><p>Most of these points are not common practice in biological journals such as <italic>eLife</italic> and we leave at the discretion of the editors to require more information or references for these basic operations.</p><disp-quote content-type="editor-comment"><p>page 18</p><p>&quot;shows an interval of 10 ms.&quot; – Nice clarification, but formulate in text as precise formula, please</p><p>page 20</p><p>Can you please give a summary table of comparison with other approaches?</p><p>page 21</p><p>[4] [5] – format all references in the same way, please! E.g first names!</p></disp-quote><p>We have updated the text to clarify these points.</p></body></sub-article></article>