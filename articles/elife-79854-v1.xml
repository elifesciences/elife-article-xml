<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">79854</article-id><article-id pub-id-type="doi">10.7554/eLife.79854</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>Generative power of a protein language model trained on multiple sequence alignments</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-279501"><name><surname>Sgarbossa</surname><given-names>Damiano</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7878-6061</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-279500"><name><surname>Lupo</surname><given-names>Umberto</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-12070"><name><surname>Bitbol</surname><given-names>Anne-Florence</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1020-494X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Institute of Bioengineering</institution>, <institution>Ecole Polytechnique Federale de Lausanne (EPFL)</institution>, <addr-line><named-content content-type="city">Lausanne</named-content></addr-line>, <country>Switzerland</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-8731"><name><surname>Colwell</surname><given-names>Lucy J</given-names></name><role>Reviewing editor</role><aff><institution>Cambridge University</institution>, <country>United Kingdom</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>umberto.lupo@epfl.ch</email> (UL);</corresp><corresp id="cor2"><label>*</label>For correspondence: <email>anne-florence.bitbol@epfl.ch</email> (AB);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>03</day><month>02</month><year>2023</year></pub-date><volume>12</volume><elocation-id>e79854</elocation-id><history><date date-type="received"><day>28</day><month>04</month><year>2022</year></date><date date-type="accepted"><day>02</day><month>02</month><year>2023</year></date></history><permissions><copyright-statement>Â© 2023, Sgarbossa et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Sgarbossa et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-79854-v1.pdf"/><abstract><p>Computational models starting from large ensembles of evolutionarily related protein sequences capture a representation of protein families and learn constraints associated to protein structure and function. They thus open the possibility for generating novel sequences belonging to protein families. Protein language models trained on multiple sequence alignments, such as MSA Transformer, are highly attractive candidates to this end. We propose and test an iterative method that directly employs the masked language modeling objective to generate sequences using MSA Transformer. We demonstrate that the resulting sequences score as well as natural sequences, for homology, coevolution and structure-based measures. For large protein families, our synthetic sequences have similar or better properties compared to sequences generated by Potts models, including experimentally-validated ones. Moreover, for small protein families, our generation method based on MSA Transformer outperforms Potts models. Our method also more accurately reproduces the higher-order statistics and the distribution of sequences in sequence space of natural data than Potts models. MSA Transformer is thus a strong candidate for protein sequence generation and protein design.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>851173</award-id><principal-award-recipient><name><surname>Sgarbossa</surname><given-names>Damiano</given-names></name><name><surname>Lupo</surname><given-names>Umberto</given-names></name><name><surname>Bitbol</surname><given-names>Anne-Florence</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>Python code for generating sequences using the iterative masking procedure is available in our GitHub repository: https://github.com/Bitbol-Lab/Iterative_masking.Raw data was collected from two public sources: 1) MSAs from the Pfam database (https://pfam.xfam.org/); 2) further MSAs from https://github.com/matteofigliuzzi/bmDCA. We generated sequences with bmDCA using code publicly available at https://github.com/ranganathanlab/bmDCA.</p></sec><supplementary-material><ext-link xlink:href="elife-79854-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>