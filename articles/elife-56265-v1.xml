<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">56265</article-id><article-id pub-id-type="doi">10.7554/eLife.56265</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Interrogating theoretical models of neural computation with emergent property inference</article-title> </title-group><contrib-group><contrib contrib-type="author" id="author-176211"><name><surname>Bittner</surname><given-names>Sean R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6773-5402</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund11"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-178143"><name><surname>Palmigiano</surname><given-names>Agostina</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund9"/><xref ref-type="other" rid="fund10"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-148412"><name><surname>Piet</surname><given-names>Alex T</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6529-1414</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-4644"><name><surname>Duan</surname><given-names>Chunyu A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3095-8653</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-3699"><name><surname>Brody</surname><given-names>Carlos D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4201-561X</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund14"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-178144"><name><surname>Miller</surname><given-names>Kenneth D</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund9"/><xref ref-type="other" rid="fund12"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-12309"><name><surname>Cunningham</surname><given-names>John</given-names></name><email>jpc2181@columbia.edu</email><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund11"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Neuroscience, Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Princeton Neuroscience Institute</institution><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Princeton University</institution><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Allen Institute for Brain Science</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution>Institute of Neuroscience, Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff6"><label>6</label><institution>Howard Hughes Medical Institute</institution><addr-line><named-content content-type="city">Chevy Chase</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution>Department of Statistics, Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Huguenard</surname><given-names>John R</given-names></name><role>Senior Editor</role><aff><institution>Stanford University School of Medicine</institution><country>United States</country></aff></contrib><contrib contrib-type="editor"><name><surname>O'Leary</surname><given-names>Timothy</given-names></name><role>Reviewing Editor</role><aff><institution>University of Cambridge</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>29</day><month>07</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e56265</elocation-id><history><date date-type="received" iso-8601-date="2020-02-21"><day>21</day><month>02</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-06-30"><day>30</day><month>06</month><year>2021</year></date></history><permissions><copyright-statement>Â© 2021, Bittner et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Bittner et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-56265-v1.pdf"/><related-article ext-link-type="doi" id="ra1" related-article-type="article-reference" xlink:href="10.7554/eLife.56261"/><related-article ext-link-type="doi" id="ra2" related-article-type="article-reference" xlink:href="10.7554/eLife.54997"/><abstract><p>A cornerstone of theoretical neuroscience is the circuit model: a system of equations that captures a hypothesized neural mechanism. Such models are valuable when they give rise to an experimentally observed phenomenon -- whether behavioral or a pattern of neural activity -- and thus can offer insights into neural computation. The operation of these circuits, like all models, critically depends on the choice of model parameters. A key step is then to identify the model parameters consistent with observed phenomena: to solve the inverse problem. In this work, we present a novel technique, emergent property inference (EPI), that brings the modern probabilistic modeling toolkit to theoretical neuroscience. When theorizing circuit models, theoreticians predominantly focus on reproducing computational properties rather than a particular dataset. Our method uses deep neural networks to learn parameter distributions with these computational properties. This methodology is introduced through a motivational example of parameter inference in the stomatogastric ganglion. EPI is then shown to allow precise control over the behavior of inferred parameters and to scale in parameter dimension better than alternative techniques. In the remainder of this work, we present novel theoretical findings in models of primary visual cortex and superior colliculus, which were gained through the examination of complex parametric structure captured by EPI. Beyond its scientific contribution, this work illustrates the variety of analyses possible once deep learning is harnessed towards solving theoretical inverse problems.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>theoretical neuroscience</kwd><kwd>deep learning</kwd><kwd>circuit models</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DGE-1644869</award-id><principal-award-recipient><name><surname>Bittner</surname><given-names>Sean R</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>NINDS</institution></institution-wrap></funding-source><award-id>5R01NS100066</award-id><principal-award-recipient><name><surname>Bittner</surname><given-names>Sean R</given-names></name><name><surname>Cunningham</surname><given-names>John</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000872</institution-id><institution>McKnight Endowment Fund for Neuroscience</institution></institution-wrap></funding-source><award-id>Faculty Award</award-id><principal-award-recipient><name><surname>Cunningham</surname><given-names>John</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000324</institution-id><institution>Gatsby Charitable Foundation</institution></institution-wrap></funding-source><award-id>GAT3708</award-id><principal-award-recipient><name><surname>Bittner</surname><given-names>Sean R</given-names></name><name><surname>Palmigiano</surname><given-names>Agostina</given-names></name><name><surname>Miller</surname><given-names>Kenneth D</given-names></name><name><surname>Cunningham</surname><given-names>John</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>542963</award-id><principal-award-recipient><name><surname>Bittner</surname><given-names>Sean R</given-names></name><name><surname>Cunningham</surname><given-names>John</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1707398</award-id><principal-award-recipient><name><surname>Bittner</surname><given-names>Sean R</given-names></name><name><surname>Palmigiano</surname><given-names>Agostina</given-names></name><name><surname>Miller</surname><given-names>Kenneth D</given-names></name><name><surname>Cunningham</surname><given-names>John</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>NIH</institution></institution-wrap></funding-source><award-id>5U19NS107613</award-id><principal-award-recipient><name><surname>Palmigiano</surname><given-names>Agostina</given-names></name><name><surname>Miller</surname><given-names>Kenneth D</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>Postdoctoral Fellowship</award-id><principal-award-recipient><name><surname>Duan</surname><given-names>Chunyu A</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>NIH</institution></institution-wrap></funding-source><award-id>U01-NS108683</award-id><principal-award-recipient><name><surname>Palmigiano</surname><given-names>Agostina</given-names></name><name><surname>Miller</surname><given-names>Kenneth D</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>NIH</institution></institution-wrap></funding-source><award-id>R01-EY029999</award-id><principal-award-recipient><name><surname>Palmigiano</surname><given-names>Agostina</given-names></name></principal-award-recipient></award-group><award-group id="fund11"><funding-source><institution-wrap><institution>Grossman Charitable Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Bittner</surname><given-names>Sean R</given-names></name><name><surname>Cunningham</surname><given-names>John</given-names></name></principal-award-recipient></award-group><award-group id="fund12"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>553017</award-id><principal-award-recipient><name><surname>Miller</surname><given-names>Kenneth D</given-names></name></principal-award-recipient></award-group><award-group id="fund14"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000011</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Brody</surname><given-names>Carlos D</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Emergent property inference, a novel machine learning methodology, learns distributions of neural circuit model parameters that produce computational properties and provides novel scientific insight through the quantification of the rich parametric structure it captures.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The fundamental practice of theoretical neuroscience is to use a mathematical model to understand neural computation, whether that computation enables perception, action, or some intermediate processing. A neural circuit is systematized with a set of equations â the model â and these equations are motivated by biophysics, neurophysiology, and other conceptual considerations (<xref ref-type="bibr" rid="bib47">Kopell and Ermentrout, 1988</xref>; <xref ref-type="bibr" rid="bib54">Marder, 1998</xref>; <xref ref-type="bibr" rid="bib1">Abbott, 2008</xref>; <xref ref-type="bibr" rid="bib96">Wang, 2010</xref>; <xref ref-type="bibr" rid="bib68">O'Leary et al., 2015</xref>). The function of this system is governed by the choice of model <italic>parameters</italic>, which when configured in a particular way, give rise to a measurable signature of a computation. The work of analyzing a model then requires solving the inverse problem: given a computation of interest, how can we reason about the distribution of parameters that give rise to it? The inverse problem is crucial for reasoning about likely parameter values, uniquenesses and degeneracies, and predictions made by the model (<xref ref-type="bibr" rid="bib33">Gutenkunst et al., 2007</xref>; <xref ref-type="bibr" rid="bib22">Erguler and Stumpf, 2011</xref>; <xref ref-type="bibr" rid="bib53">Mannakee et al., 2016</xref>).</p><p>Ideally, one carefully designs a model and analytically derives how computational properties determine model parameters. Seminal examples of this gold standard include our fieldâs understanding of memory capacity in associative neural networks (<xref ref-type="bibr" rid="bib41">Hopfield, 1982</xref>), chaos and autocorrelation timescales in random neural networks (<xref ref-type="bibr" rid="bib90">Sompolinsky et al., 1988</xref>), central pattern generation (<xref ref-type="bibr" rid="bib69">Olypher and Calabrese, 2007</xref>), the paradoxical effect (<xref ref-type="bibr" rid="bib94">Tsodyks et al., 1997</xref>), and decision making (<xref ref-type="bibr" rid="bib97">Wong and Wang, 2006</xref>). Unfortunately, as circuit models include more biological realism, theory via analytical derivation becomes intractable. Absent this analysis, statistical inference offers a toolkit by which to solve the inverse problem by identifying, at least approximately, the distribution of parameters that produce computations in a biologically realistic model (<xref ref-type="bibr" rid="bib25">Foster et al., 1993</xref>; <xref ref-type="bibr" rid="bib78">Prinz et al., 2004</xref>; <xref ref-type="bibr" rid="bib2">Achard and De Schutter, 2006</xref>; <xref ref-type="bibr" rid="bib24">Fisher et al., 2013</xref>; <xref ref-type="bibr" rid="bib67">O'Leary et al., 2014</xref>; <xref ref-type="bibr" rid="bib3">Alonso and Marder, 2019</xref>).</p><p>Statistical inference, of course, requires quantification of the sometimes vague term <italic>computation</italic>. In neuroscience, two perspectives are dominant. First, often we directly use an <italic>exemplar dataset</italic>: a collection of samples that express the computation of interest, this data being gathered either experimentally in the lab or from a computer simulation. Although a natural choice given its connection to experiment (<xref ref-type="bibr" rid="bib72">Paninski and Cunningham, 2018</xref>), some drawbacks exist: these data are well known to have features irrelevant to the computation of interest (<xref ref-type="bibr" rid="bib65">Niell and Stryker, 2010</xref>; <xref ref-type="bibr" rid="bib85">Saleem et al., 2013</xref>; <xref ref-type="bibr" rid="bib64">Musall et al., 2019</xref>), confounding inferences made on such data. Related to this point, use of a conventional dataset encourages conventional data likelihoods or loss functions, which focus on some global metric like squared error or marginal evidence, rather than the computation itself.</p><p>Alternatively, researchers often quantify an <italic>emergent property</italic> (EP): a statistic of data that directly quantifies the computation of interest, wherein the dataset is implicit. While such a choice may seem esoteric, it is not: the above âgold standardâ examples (<xref ref-type="bibr" rid="bib41">Hopfield, 1982</xref>; <xref ref-type="bibr" rid="bib90">Sompolinsky et al., 1988</xref>; <xref ref-type="bibr" rid="bib69">Olypher and Calabrese, 2007</xref>; <xref ref-type="bibr" rid="bib94">Tsodyks et al., 1997</xref>; <xref ref-type="bibr" rid="bib97">Wong and Wang, 2006</xref>) all quantify and focus on some derived feature of the data, rather than the data drawn from the model. An emergent property is of course a dataset by another name, but it suggests different approach to solving the same inverse problem: here, we directly specify the desired emergent property â a statistic of data drawn from the model â and the value we wish that property to have, and we set up an optimization program to find the distribution of parameters that produce this computation. This statistical framework is not new: it is intimately connected to the literature on approximate bayesian computation (<xref ref-type="bibr" rid="bib5">Beaumont et al., 2002</xref>; <xref ref-type="bibr" rid="bib57">Marjoram et al., 2003</xref>; <xref ref-type="bibr" rid="bib88">Sisson et al., 2007</xref>), parameter sensitivity analyses (<xref ref-type="bibr" rid="bib80">Raue et al., 2009</xref>; <xref ref-type="bibr" rid="bib42">Karlsson et al., 2012</xref>; <xref ref-type="bibr" rid="bib40">Hines et al., 2014</xref>; <xref ref-type="bibr" rid="bib79">Raman et al., 2017</xref>), maximum entropy modeling (<xref ref-type="bibr" rid="bib21">Elsayed and Cunningham, 2017</xref>; <xref ref-type="bibr" rid="bib87">Savin and TkaÄik, 2017</xref>; <xref ref-type="bibr" rid="bib61">MÅynarski et al., 2020</xref>), and approximate bayesian inference (<xref ref-type="bibr" rid="bib92">Tran et al., 2017</xref>; <xref ref-type="bibr" rid="bib31">GonÃ§alves et al., 2019</xref>); we detail these connections in Section 'Related approaches'.</p><p>The parameter distributions producing a computation may be curved or multimodal along various parameter axes and combinations. It is by quantifying this complex structure that emergent property inference offers scientific insight. Traditional approximation families (e.g. mean-field or mixture of gaussians) are limited in the distributional structure they may learn. To address such restrictions on expressivity, advances in machine learning have used deep probability distributions as flexible approximating families for such complicated distributions (<xref ref-type="bibr" rid="bib81">Rezende and Mohamed, 2015</xref>; <xref ref-type="bibr" rid="bib74">Papamakarios et al., 2019a</xref>) (see Section 'Deep probability distributions and normalizing flows'). However, the adaptation of deep probability distributions to the problem of theoretical circuit analysis requires recent developments in deep learning for constrained optimization (<xref ref-type="bibr" rid="bib51">Loaiza-Ganem et al., 2017</xref>), and architectural choices for efficient and expressive deep generative modeling (<xref ref-type="bibr" rid="bib18">Dinh et al., 2017</xref>; <xref ref-type="bibr" rid="bib45">Kingma and Dhariwal, 2018</xref>). We detail our method, which we call emergent property inference (EPI) in Section 'Emergent property inference via deep generative models'.</p><p>Equipped with this method, we demonstrate the capabilities of EPI and present novel theoretical findings from its analysis. First, we show EPIâs ability to handle biologically realistic circuit models using a five-neuron model of the stomatogastric ganglion (<xref ref-type="bibr" rid="bib34">Gutierrez et al., 2013</xref>): a neural circuit whose parametric degeneracy is closely studied (<xref ref-type="bibr" rid="bib28">Goldman et al., 2001</xref>). Then, we show EPIâs scalability to high dimensional parameter distributions by inferring connectivities of recurrent neural networks that exhibit stable, yet amplified responses â a hallmark of neural responses throughout the brain (<xref ref-type="bibr" rid="bib63">Murphy and Miller, 2009</xref>; <xref ref-type="bibr" rid="bib37">Hennequin et al., 2014</xref>; <xref ref-type="bibr" rid="bib11">Bondanelli et al., 2019</xref>). In a model of primary visual cortex (<xref ref-type="bibr" rid="bib50">Litwin-Kumar et al., 2016</xref>; <xref ref-type="bibr" rid="bib71">Palmigiano et al., 2020</xref>), EPI reveals how the recurrent processing across different neuron-type populations shapes excitatory variability: a finding that we show is analytically intractable. Finally, we investigated the possible connectivities of a superior colliculus model that allow execution of different tasks on interleaved trials (<xref ref-type="bibr" rid="bib20">Duan et al., 2021</xref>). EPI discovered a rich distribution containing two connectivity regimes with different solution classes. We queried the deep probability distribution learned by EPI to produce a mechanistic understanding of neural responses in each regime. Intriguingly, the inferred connectivities of each regime reproduced results from optogenetic inactivation experiments in markedly different ways. These theoretical insights afforded by EPI illustrate the value of deep inference for the interrogation of neural circuit models.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Motivating emergent property inference of theoretical models</title><p>Consideration of the typical workflow of theoretical modeling clarifies the need for emergent property inference. First, one designs or chooses an existing circuit model that, it is hypothesized, captures the computation of interest. To ground this process in a well-known example, consider the stomatogastric ganglion (STG) of crustaceans, a small neural circuit which generates multiple rhythmic muscle activation patterns for digestion (<xref ref-type="bibr" rid="bib56">Marder and Thirumalai, 2002</xref>). Despite full knowledge of STG connectivity and a precise characterization of its rhythmic pattern generation, biophysical models of the STG have complicated relationships between circuit parameters and computation (<xref ref-type="bibr" rid="bib28">Goldman et al., 2001</xref>; <xref ref-type="bibr" rid="bib78">Prinz et al., 2004</xref>).</p><p>A subcircuit model of the STG (<xref ref-type="bibr" rid="bib34">Gutierrez et al., 2013</xref>) is shown schematically in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. The fast population (f1 and f2) represents the subnetwork generating the pyloric rhythm and the slow population (s1 and s2) represents the subnetwork of the gastric mill rhythm. The two fast neurons mutually inhibit one another, and spike at a greater frequency than the mutually inhibiting slow neurons. The hub neuron couples with either the fast or slow population, or both depending on modulatory conditions. The jagged connections indicate electrical coupling having electrical conductance <inline-formula><mml:math id="inf1"><mml:msub><mml:mi>g</mml:mi><mml:mtext>el</mml:mtext></mml:msub></mml:math></inline-formula>, smooth connections in the diagram are inhibitory synaptic projections having strength <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>g</mml:mi><mml:mtext>synA</mml:mtext></mml:msub></mml:math></inline-formula> onto the hub neuron, and <inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mtext>synB</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> nS for mutual inhibitory connections. Note that the behavior of this model will be critically dependent on its parameterization â the choices of conductance parameters <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mtext>el</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mtext>synA</mml:mtext></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Emergent property inference in the stomatogastric ganglion.</title><p>(<bold>A</bold>) Conductance-based subcircuit model of the STG. (<bold>B</bold>) Spiking frequency <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is an emergent property statistic. Simulated at <inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mtext>el</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>4.5</mml:mn></mml:mrow></mml:math></inline-formula> nS and <inline-formula><mml:math id="inf7"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mtext>synA</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> nS. (<bold>C</bold>) The emergent property of intermediate hub frequency. Simulated activity traces are colored by log probability of generating parameters in the EPI distribution (Panel E). (<bold>D</bold>) For a choice of circuit model and emergent property, EPI learns a deep probability distribution of parameters <inline-formula><mml:math id="inf8"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula>. (<bold>E</bold>) The EPI distribution producing intermediate hub frequency. Samples are colored by log probability density. Contours of hub neuron frequency error are shown at levels of 0.525, 0.53, â¦ 0.575 Hz (dark to light gray away from mean). Dimension of sensitivity <inline-formula><mml:math id="inf9"><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> (solid arrow) and robustness <inline-formula><mml:math id="inf10"><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> (dashed arrow). (<bold>F</bold>) (Top) The predictions of the EPI distribution. The black and gray dashed lines show the mean and two standard deviations according the emergent property. (Bottom) Simulations at the starred parameter values.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 1.</label><caption><title>Emergent property inference in a 2D linear dynamical system.</title><p>(<bold>A</bold>) Two-dimensional linear dynamical system model, where real entries of the dynamics matrix <inline-formula><mml:math id="inf11"><mml:mi>A</mml:mi></mml:math></inline-formula> are the parameters. (<bold>B</bold>) The EPI distribution for a two-dimensional linear dynamical system with <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> that produces an average of 1 Hz oscillations with some small amount of variance. Dashed lines indicate the parameter axes. (<bold>C</bold>) Entropy throughout the optimization. At the beginning of each augmented lagrangian epoch (<inline-formula><mml:math id="inf13"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> iterations), the entropy dipped due to the shifted optimization manifold where emergent property constraint satisfaction is increasingly weighted. (<bold>D</bold>) Emergent property moments throughout optimization. At the beginning of each augmented lagrangian epoch, the emergent property moments adjust closer to their constraints.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 2.</label><caption><title>Analytic contours of inferred EPI distribution.</title><p>(<bold>A</bold>) Probability contours in the <inline-formula><mml:math id="inf14"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>-<inline-formula><mml:math id="inf15"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> plane were derived from the relationship to emergent property statistic of growth/decay factor <inline-formula><mml:math id="inf16"><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>B</bold>) Probability contours in the <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>-<inline-formula><mml:math id="inf18"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> plane were derived from the emergent property statistic of oscillation frequency <inline-formula><mml:math id="inf19"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mtext>imag</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 3.</label><caption><title>Sampled dynamical systems <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and their simulated activity from <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mfrac><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> colored by log probability.</title><p>(<bold>A</bold>) Each dimension of the simulated trajectories throughout time. (<bold>B</bold>) The simulated trajectories in phase space.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 4.</label><caption><title>EPI optimization of the STG model producing network syncing.</title><p>(<bold>A</bold>) Entropy throughout optimization. (<bold>B</bold>) The emergent property statistic means and variances converge to their constraints at 25,000 iterations following the fifth augmented lagrangian epoch.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig1-figsupp4-v1.tif"/></fig></fig-group><p>Second, once the model is selected, one must specify what the model should produce. In this STG model, we are concerned with neural spiking frequency, which emerges from the dynamics of the circuit model (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). An emergent property studied by Gutierrez et al. is the hub neuron firing at an intermediate frequency between the intrinsic spiking rates of the fast and slow populations. This emergent property (EP) is shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref> at an average frequency of 0.55 Hz. To be precise, we define intermediate hub frequency not strictly as 0.55 Hz, but frequencies of moderate deviation from 0.55 Hz between the fast (.35Hz) and slow (.68Hz) frequencies.</p><p>Third, the model parameters producing the emergent property are inferred. By precisely quantifying the emergent property of interest as a statistical feature of the model, we use emergent property inference (EPI) to condition directly on this emergent property. Before presenting technical details (in the following section), let us understand emergent property inference schematically. EPI (<xref ref-type="fig" rid="fig1">Figure 1D</xref>) takes, as input, the model and the specified emergent property, and as its output, returns the parameter distribution (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). This distribution â represented for clarity as samples from the distribution â is a parameter distribution constrained such that the circuit model produces the emergent property. Once EPI is run, the returned distribution can be used to efficiently generate additional parameter samples. Most importantly, the inferred distribution can be efficiently queried to quantify the parametric structure that it captures. By quantifying the parametric structure governing the emergent property, EPI informs the central question of this inverse problem: what aspects or combinations of model parameters have the desired emergent property?</p></sec><sec id="s2-2"><title>Emergent property inference via deep generative models</title><p>EPI formalizes the three-step procedure of the previous section with deep probability distributions (<xref ref-type="bibr" rid="bib81">Rezende and Mohamed, 2015</xref>; <xref ref-type="bibr" rid="bib74">Papamakarios et al., 2019a</xref>). First, as is typical, we consider the model as a coupled set of noisy differential equations. In this STG example, the model activity (or state) <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mtext>f1</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mtext>f2</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mtext>hub</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mtext>s1</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mtext>s2</mml:mtext></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the membrane potential for each neuron, which evolves according to the biophysical conductance-based equation:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf23"><mml:msub><mml:mi>C</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> = 1nF, and <inline-formula><mml:math id="inf24"><mml:mi mathvariant="bold">ð¡</mml:mi></mml:math></inline-formula> is a sum of the leak, calcium, potassium, hyperpolarization, electrical, and synaptic currents, all of which have their own complicated dependence on activity <inline-formula><mml:math id="inf25"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula> and parameters <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mtext>el</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mtext>synA</mml:mtext></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi mathvariant="bold">ð</mml:mi></mml:mrow></mml:math></inline-formula> is white gaussian noise (<xref ref-type="bibr" rid="bib34">Gutierrez et al., 2013</xref>;Â see Section 'STG model' for more detail).</p><p>Second, we determine that our model should produce the emergent property of âintermediate hub frequencyâ (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). We stipulate that the hub neuronâs spiking frequency â denoted by statistic <inline-formula><mml:math id="inf28"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mtext>hub</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> â is close to a frequency of 0.55 Hz, between that of the slow and fast frequencies. Mathematically, we define this emergent property with two constraints: that the mean hub frequency is 0.55 Hz,<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>hub</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.55</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula>and that the variance of the hub frequency is moderate<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>hub</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mn>0.025</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In the emergent property of intermediate hub frequency, the statistic of hub neuron frequency is an expectation over the distribution of parameters <inline-formula><mml:math id="inf29"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> and the distribution of the data <inline-formula><mml:math id="inf30"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula> that those parameters produce. We define the emergent property <inline-formula><mml:math id="inf31"><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:math></inline-formula> as the collection of these two constraints. In general, an emergent property is a collection of constraints on statistical moments that together define the computation of interest.</p><p>Third, we perform emergent property inference: we find a distribution over parameter configurations <inline-formula><mml:math id="inf32"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> of models that produce the emergent property; in other words, they satisfy the constraints introduced in <xref ref-type="disp-formula" rid="equ2 equ3">Equations 2 and 3</xref>. This distribution will be chosen from a family of probability distributions <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð¬</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">ð½</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">Î</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, defined by a deep neural network (<xref ref-type="bibr" rid="bib81">Rezende and Mohamed, 2015</xref>; <xref ref-type="bibr" rid="bib74">Papamakarios et al., 2019a</xref>; <xref ref-type="fig" rid="fig1">Figure 1D</xref>, EPI box). Deep probability distributions map a simple random variable <inline-formula><mml:math id="inf34"><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> (e.g. an isotropic gaussian) through a deep neural network with weights and biases <inline-formula><mml:math id="inf35"><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:math></inline-formula> to parameters <inline-formula><mml:math id="inf36"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> of a suitably complicated distribution (see Section 'Deep probability distributions and normalizing flows' for more details). Many distributions in <inline-formula><mml:math id="inf37"><mml:mi class="ltx_font_mathcaligraphic">ð¬</mml:mi></mml:math></inline-formula> will respect the emergent property constraints, so we select the most random (highest entropy) distribution, which also means this approach is equivalent to bayesian variational inference (see Section 'EPI as variational inference'). In EPI optimization, stochastic gradient steps in <inline-formula><mml:math id="inf38"><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:math></inline-formula> are taken such that entropy is maximized, and the emergent property <inline-formula><mml:math id="inf39"><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:math></inline-formula> is produced (see Section 'Emergent property inference (EPI)'). We then denote the inferred EPI distribution as <inline-formula><mml:math id="inf40"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, since the structure of the learned parameter distribution is determined by weights and biases <inline-formula><mml:math id="inf41"><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:math></inline-formula>, and this distribution is conditioned upon emergent property <inline-formula><mml:math id="inf42"><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:math></inline-formula>.</p><p>The structure of the inferred parameter distributions of EPI can be analyzed to reveal key information about how the circuit model produces the emergent property. As probability in the EPI distribution decreases away from the mode of <inline-formula><mml:math id="inf43"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1E</xref> yellow star), the emergent property deteriorates. Perturbing <inline-formula><mml:math id="inf44"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> along a dimension in which <inline-formula><mml:math id="inf45"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> changes little will not disturb the emergent property, making this parameter combination <italic>robust</italic> with respect to the emergent property. In contrast, if <inline-formula><mml:math id="inf46"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> is perturbed along a dimension with strongly decreasing <inline-formula><mml:math id="inf47"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, that parameter combination is deemed <italic>sensitive</italic> (<xref ref-type="bibr" rid="bib80">Raue et al., 2009</xref>; <xref ref-type="bibr" rid="bib79">Raman et al., 2017</xref>). By querying the second-order derivative (Hessian) of <inline-formula><mml:math id="inf48"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at a mode, we can quantitatively identify how sensitive (or robust) each eigenvector is by its eigenvalue; the more negative, the more sensitive and the closer to zero, the more robust (see Section 'Hessian sensitivity vectors'). Indeed, samples equidistant from the mode along these dimensions of sensitivity (<inline-formula><mml:math id="inf49"><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, smaller eigenvalue) and robustness (<inline-formula><mml:math id="inf50"><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, greater eigenvalue) (<xref ref-type="fig" rid="fig1">Figure 1E</xref>, arrows) agree with error contours (<xref ref-type="fig" rid="fig1">Figure 1E</xref> contours) and have diminished or preserved hub frequency, respectively (<xref ref-type="fig" rid="fig1">Figure 1F</xref> activity traces). The directionality of <inline-formula><mml:math id="inf51"><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> suggests that changes in conductance along this parameter combination will most preserve hub neuron firing between the intrinsic rates of the pyloric and gastric mill rhythms. Importantly and unlike alternative techniques, once an EPI distribution has been learned, the modes and Hessians of the distribution can be measured with trivial computation (see Section 'Deep probability distributions and normalizing flows').</p><p>In the following sections, we demonstrate EPI on three neural circuit models across ranges of biological realism, neural system function, and network scale. First, we demonstrate the superior scalability of EPI compared to alternative techniques by inferring high-dimensional distributions of recurrent neural network connectivities that exhibit amplified, yet stable responses. Next, in a model of primary visual cortex (<xref ref-type="bibr" rid="bib50">Litwin-Kumar et al., 2016</xref>; <xref ref-type="bibr" rid="bib71">Palmigiano et al., 2020</xref>), we show how EPI discovers parametric degeneracy, revealing how input variability across neuron types affects the excitatory population. Finally, in a model of superior colliculus (<xref ref-type="bibr" rid="bib20">Duan et al., 2021</xref>), we used EPI to capture multiple parametric regimes of task switching, and queried the dimensions of parameter sensitivity to characterize each regime.</p></sec><sec id="s2-3"><title>Scaling inference of recurrent neural network connectivity with EPI</title><p>To understand how EPI scales in comparison to existing techniques, we consider recurrent neural networks (RNNs). Transient amplification is a hallmark of neural activity throughout cortex and is often thought to be intrinsically generated by recurrent connectivity in the responding cortical area (<xref ref-type="bibr" rid="bib63">Murphy and Miller, 2009</xref>; <xref ref-type="bibr" rid="bib37">Hennequin et al., 2014</xref>; <xref ref-type="bibr" rid="bib11">Bondanelli et al., 2019</xref>). It has been shown that to generate such amplified, yet stabilized responses, the connectivity of RNNs must be non-normal (<xref ref-type="bibr" rid="bib29">Goldman, 2009</xref>; <xref ref-type="bibr" rid="bib63">Murphy and Miller, 2009</xref>), and satisfy additional constraints (<xref ref-type="bibr" rid="bib12">Bondanelli and Ostojic, 2020</xref>). In theoretical neuroscience, RNNs are optimized and then examined to show how dynamical systems could execute a given computation (<xref ref-type="bibr" rid="bib91">Sussillo, 2014</xref>; <xref ref-type="bibr" rid="bib4">Barak, 2017</xref>), but such biologically realistic constraints on connectivity (<xref ref-type="bibr" rid="bib29">Goldman, 2009</xref>; <xref ref-type="bibr" rid="bib63">Murphy and Miller, 2009</xref>; <xref ref-type="bibr" rid="bib12">Bondanelli and Ostojic, 2020</xref>) are ignored for simplicity or because constrained optimization is difficult. In general, access to distributions of connectivity that produce theoretical criteria like stable amplification, chaotic fluctuations (<xref ref-type="bibr" rid="bib90">Sompolinsky et al., 1988</xref>), or low tangling (<xref ref-type="bibr" rid="bib84">Russo et al., 2018</xref>) would add scientific value to existing research with RNNs. Here, we use EPI to learn RNN connectivities producing stable amplification, and demonstrate the superior scalability and efficiency of EPI to alternative approaches.</p><p>We consider a rank-2 RNN with <inline-formula><mml:math id="inf52"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons having connectivity <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mo>â¤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and dynamics<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mover accent="true"><mml:mi>ð±</mml:mi><mml:mo>Ë</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>ð±</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mo>â¢</mml:mo><mml:mi>ð±</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mi>Ï</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>U</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mi>Ï</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf56"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>â¢</mml:mo><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf57"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>U</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>â¼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð©</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. We infer connectivity parameters <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that produce stable amplification. Two conditions are necessary and sufficient for RNNs to exhibit stable amplification (<xref ref-type="bibr" rid="bib12">Bondanelli and Ostojic, 2020</xref>): <inline-formula><mml:math id="inf59"><mml:mrow><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf60"><mml:mrow><mml:msubsup><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf61"><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> is the eigenvalue of <inline-formula><mml:math id="inf62"><mml:mi>W</mml:mi></mml:math></inline-formula> with greatest real part and <inline-formula><mml:math id="inf63"><mml:msup><mml:mi>Î»</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:math></inline-formula> is the maximum eigenvalue of <inline-formula><mml:math id="inf64"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>W</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mo>â¤</mml:mo></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>. RNNs with <inline-formula><mml:math id="inf65"><mml:mrow><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>Â±</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf66"><mml:mrow><mml:msubsup><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mn>1.5</mml:mn><mml:mo>Â±</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> will be stable with modest decay rate (<inline-formula><mml:math id="inf67"><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> close to its upper bound of 1) and exhibit modest amplification (<inline-formula><mml:math id="inf68"><mml:msubsup><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi></mml:msubsup></mml:math></inline-formula> close to its lower bound of 1). EPI can naturally condition on this emergent property<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="script">ð³</mml:mi></mml:mrow><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mtext>real</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>Î»</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>0.5</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1.5</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mtext>real</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>Î»</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msup><mml:mn>0.25</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mn>0.25</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Variance constraints predicate that the majority of the distribution (within two standard deviations) are within the specified ranges.</p><p>For comparison, we infer the parameters <inline-formula><mml:math id="inf69"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> likely to produce stable amplification using two alternative simulation-based inference approaches. Sequential Monte Carlo approximate bayesian computation (SMC-ABC) (<xref ref-type="bibr" rid="bib88">Sisson et al., 2007</xref>) is a rejection sampling approach that uses SMC techniques to improve efficiency, and sequential neural posterior estimation (SNPE) (<xref ref-type="bibr" rid="bib31">GonÃ§alves et al., 2019</xref>) approximates posteriors with deep probability distributions (see Section 'Related approaches'). Unlike EPI, these statistical inference techniques do not constrain the predictions of the inferred distribution, so they were run by conditioning on an exemplar dataset <inline-formula><mml:math id="inf70"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð±</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">ð</mml:mi></mml:mrow></mml:math></inline-formula>, following standard practice with these methods (<xref ref-type="bibr" rid="bib88">Sisson et al., 2007</xref>; <xref ref-type="bibr" rid="bib31">GonÃ§alves et al., 2019</xref>). To compare the efficiency of these different techniques, we measured the time and number of simulations necessary for the distance of the predictive mean to be less than 0.5 from <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">ð±</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (see Section 'Scaling EPI for stable amplification in RNNs').</p><p>As the number of neurons <inline-formula><mml:math id="inf72"><mml:mi>N</mml:mi></mml:math></inline-formula> in the RNN, and thus the dimension of the parameter space <inline-formula><mml:math id="inf73"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>â¢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, is scaled, we see that EPI converges at greater speed and at greater dimension than SMC-ABC and SNPE (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). It also becomes most efficient to use EPI in terms of simulation count at <inline-formula><mml:math id="inf74"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). It is well known that ABC techniques struggle in parameter spaces of modest dimension (<xref ref-type="bibr" rid="bib89">Sisson et al., 2018</xref>), yet we were careful to assess the scalability of SNPE, which is a more closely related methodology to EPI. Between EPI and SNPE, we closely controlled the number of parameters in deep probability distributions by dimensionality (<xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>), and tested more aggressive SNPE hyperparameter choices when SNPE failed to converge (<xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2</xref>). In this analysis, we see that deep inference techniques EPI and SNPE are far more amenable to inference of high dimensional RNN connectivities than rejection sampling techniques like SMC-ABC, and that EPI outperforms SNPE in both wall time (elapsed real time) and simulation count.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Inferring recurrent neural networksÂ withÂ stable amplification.</title><p>(<bold>A</bold>) Wall time of EPI (blue), SNPE (orange), and SMC-ABC (green) to converge on RNN connectivities producing stable amplification. Each dot shows convergence time for an individual random seed. For reference, the mean wall time for EPI to achieve its full constraint convergence (means and variances) is shown (blue line). (<bold>B</bold>) Simulation count of each algorithm to achieve convergence. Same conventions as A. (<bold>C</bold>) The predictive distributions of connectivities inferred by EPI (blue), SNPE (orange), and SMC-ABC (green), with reference to <inline-formula><mml:math id="inf75"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð±</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">ð</mml:mi></mml:mrow></mml:math></inline-formula> (gray star). (<bold>D</bold>) Simulations of networks inferred by each method (<inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>100</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). Each trace (15 per algorithm) corresponds to simulation of one <inline-formula><mml:math id="inf77"><mml:mi>z</mml:mi></mml:math></inline-formula>. (Below) Ratio of obtained samples producing stable amplification, stable monotonic decay, and instability.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 1.</label><caption><title>Architecture parameter comparison of EPI and SNPE.</title><p>Number of parameters in deep probability distribution architectures of EPI (blue) and SNPE (orange) by RNN size (<inline-formula><mml:math id="inf78"><mml:mi>N</mml:mi></mml:math></inline-formula>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 2.</label><caption><title>SNPE convergence was enabled by increasing <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>n</mml:mi><mml:mtext>round</mml:mtext></mml:msub></mml:math></inline-formula>, not <inline-formula><mml:math id="inf80"><mml:msub><mml:mi>n</mml:mi><mml:mtext>atom</mml:mtext></mml:msub></mml:math></inline-formula>.</title><p>(<bold>A</bold>) Difference of mean predictions <inline-formula><mml:math id="inf81"><mml:msub><mml:mi mathvariant="bold">ð±</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> throughout optimization at <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> with by simulation count (left) and wall time (right) of SNPE with <inline-formula><mml:math id="inf83"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>round</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (light orange), SNPE with <inline-formula><mml:math id="inf84"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>round</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>25</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (dark orange), and EPI (blue). Each line shows an individual random seed. (<bold>B</bold>) Same conventions as A at <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> of SNPE with <inline-formula><mml:math id="inf86"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>atom</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> (light orange) and <inline-formula><mml:math id="inf87"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>atom</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (dark orange). (<bold>C</bold>) Same conventions as A at <inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> of SNPE with <inline-formula><mml:math id="inf89"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>round</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>25</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (light orange) and <inline-formula><mml:math id="inf90"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>round</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (dark orange).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 3.</label><caption><title>Model characteristics affect predictions of posteriors inferred by SNPE, while predictions of parameters inferred by EPI remain fixed.</title><p>(<bold>A</bold>) Predictive distribution of EPI (blue) and SNPE (orange) inferred connectivity of RNNs exhibiting stable amplification with <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> (top), <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> (bottom), <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula> (left), and <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> (right). (<bold>B</bold>) Entropy of parameter distribution approximations throughout optimization with <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> (top), <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> (bottom), <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> (dark shade), and <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula> (light shade). (<bold>C</bold>) Validation log probabilities throughout SNPE optimization. Same conventions as B. (<bold>D</bold>) Adherence to EPI constraints. Same conventions as B.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig2-figsupp3-v1.tif"/></fig></fig-group><p>No matter the number of neurons, EPI always produces connectivity distributions with mean and variance of <inline-formula><mml:math id="inf99"><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf100"><mml:msubsup><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi></mml:msubsup></mml:math></inline-formula> according to <inline-formula><mml:math id="inf101"><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, blue). For the dimensionalities in which SMC-ABC is tractable, the inferred parameters are concentrated and offset from the exemplar dataset <inline-formula><mml:math id="inf102"><mml:msub><mml:mi mathvariant="bold">ð±</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, green). When using SNPE, the predictions of the inferred parameters are highly concentrated at some RNN sizes and widely varied in others (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, orange). We see these properties reflected in simulations from the inferred distributions: EPI produces a consistent variety of stable, amplified activity norms <inline-formula><mml:math id="inf103"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula>, SMC-ABC produces a limited variety of responses, and the changing variety of responses from SNPE emphasizes the control of EPI on parameter predictions (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Even for moderate neuron counts, the predictions of the inferred distribution of SNPE are highly dependent on <inline-formula><mml:math id="inf104"><mml:mi>N</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf105"><mml:mi>g</mml:mi></mml:math></inline-formula>, while EPI maintains the emergent property across choices of RNN (see Section 'Effect of RNN parameters on EPI and SNPE inferred distributions').</p><p>To understand these differences, note that EPI outperforms SNPE in high dimensions by using gradient information (from <inline-formula><mml:math id="inf106"><mml:mrow><mml:msub><mml:mo>â</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mtext>real</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>). This choice agrees with recent speculation that such gradient information could improve the efficiency of simulation-based inference techniques (<xref ref-type="bibr" rid="bib17">Cranmer et al., 2020</xref>), as well as reflecting the classic tradeoff between gradient-based and sampling-based estimators (scaling and speed versus generality). Since gradients of the emergent property are necessary in EPI optimization, gradient tractability is a key criteria when determining the suitability of a simulation-based inference technique. If the emergent property gradient is efficiently calculated, EPI is a clear choice for inferring high dimensional parameter distributions. In the next two sections, we use EPI for novel scientific insight by examining the structure of inferred distributions.</p></sec><sec id="s2-4"><title>EPI reveals how recurrence with multiple inhibitory subtypes governs excitatory variability in a V1 model</title><p>Dynamical models of excitatory (E) and inhibitory (I) populations with supralinear input-output function have succeeded in explaining a host of experimentally documented phenomena in primary visual cortex (V1). In a regime characterized by inhibitory stabilization of strong recurrent excitation, these models give rise to paradoxical responses (<xref ref-type="bibr" rid="bib94">Tsodyks et al., 1997</xref>), selective amplification (<xref ref-type="bibr" rid="bib29">Goldman, 2009</xref>; <xref ref-type="bibr" rid="bib63">Murphy and Miller, 2009</xref>), surround suppression (<xref ref-type="bibr" rid="bib70">Ozeki et al., 2009</xref>), and normalization (<xref ref-type="bibr" rid="bib82">Rubin et al., 2015</xref>). Recent theoretical work (<xref ref-type="bibr" rid="bib38">Hennequin et al., 2018</xref>) shows that stabilized E-I models reproduce the effect of variability suppression (<xref ref-type="bibr" rid="bib16">Churchland et al., 2010</xref>). Furthermore, experimental evidence shows that inhibition is composed of distinct elements â parvalbumin (P), somatostatin (S), VIP (V) â composing 80% of GABAergic interneurons in V1 (<xref ref-type="bibr" rid="bib58">Markram et al., 2004</xref>; <xref ref-type="bibr" rid="bib83">Rudy et al., 2011</xref>; <xref ref-type="bibr" rid="bib93">Tremblay et al., 2016</xref>), and that these inhibitory cell types follow specific connectivity patterns (<xref ref-type="fig" rid="fig3">Figure 3A</xref>; <xref ref-type="bibr" rid="bib76">Pfeffer et al., 2013</xref>). Here, we use EPI on a model of V1 with biologically realistic connectivity to show how the structure of input across neuron types affects the variability of the excitatory population â the population largely responsible for projecting to other brain areas (<xref ref-type="bibr" rid="bib23">Felleman and Van Essen, 1991</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Emergent property inference in the stochastic stabilized supralinear network (SSSN).</title><p>(<bold>A</bold>) Four-population model of primary visual cortex with excitatory (black), parvalbumin (blue), somatostatin (red), and VIP (green) neurons (excitatory and inhibitory projections filled and unfilled, respectively). Some neuron-types largely do not form synaptic projections to others (<inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Î±</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.025</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Each neural population receives a baseline input <inline-formula><mml:math id="inf108"><mml:msub><mml:mi mathvariant="bold">ð¡</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:math></inline-formula>, and the E- and P-populations also receive a contrast-dependent input <inline-formula><mml:math id="inf109"><mml:msub><mml:mi mathvariant="bold">ð¡</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>. Additionally, each neural population receives a slow noisy input <inline-formula><mml:math id="inf110"><mml:mi mathvariant="bold-italic">Ïµ</mml:mi></mml:math></inline-formula>. (<bold>B</bold>) Transient network responses of the SSSN model. Traces are independent trials with varying initialization <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and noise <inline-formula><mml:math id="inf112"><mml:mi mathvariant="bold-italic">Ïµ</mml:mi></mml:math></inline-formula>. (<bold>C</bold>) Mean (solid line) and standard deviation <inline-formula><mml:math id="inf113"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (shading) across 100 trials. (<bold>D</bold>) EPI distribution of noise parameters <inline-formula><mml:math id="inf114"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> conditioned on E-population variability. The EPI predictive distribution of <inline-formula><mml:math id="inf115"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is show on the bottom-left. (<bold>E</bold>) (Top) Enlarged visualization of the <inline-formula><mml:math id="inf116"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula>-<inline-formula><mml:math id="inf117"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> marginal distribution of EPI <inline-formula><mml:math id="inf118"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>Hz</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf119"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>Hz</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Each black dot shows the mode at each <inline-formula><mml:math id="inf120"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula>. The arrows show the most sensitive dimensions of the Hessian evaluated at these modes. (<bold>F</bold>) The predictive distributions of <inline-formula><mml:math id="inf121"><mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> of each inferred distribution <inline-formula><mml:math id="inf122"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>Hz</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf123"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>Hz</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 1.</label><caption><title>EPI inferred distribution for <inline-formula><mml:math id="inf124"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>Hz</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 2.</label><caption><title>EPI optimization.</title><p>(<bold>A</bold>) Entropy throughout optimization. (<bold>B</bold>) The emergent property statistic means and variances converge to their constraints at 8000 iterations following the fourth augmented lagrangian epoch.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 3.</label><caption><title>EPI predictive distributions of the sum of squares of each pair of noise parameters.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig3-figsupp3-v1.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 4.</label><caption><title>SSSN simulations for small increases in neuron-type population inputÂ (left); average (solid) and standard deviation (shaded) of stochastic fluctuations of responsesÂ (right).</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig3-figsupp4-v1.tif"/></fig></fig-group><p>We considered response variability of a nonlinear dynamical V1 circuit model (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) with a state comprised of each neuron-type populationâs rate <inline-formula><mml:math id="inf125"><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. Each population receives recurrent input <inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>W</mml:mi><mml:mo>â¢</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf127"><mml:mi>W</mml:mi></mml:math></inline-formula> is the effective connectivity matrix (see Section 'Primary visual cortex') and an external input with mean <inline-formula><mml:math id="inf128"><mml:mi mathvariant="bold">ð¡</mml:mi></mml:math></inline-formula>, which determines population rate via supralinear nonlinearity <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>â</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mo>â</mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The external input has an additive noisy component <inline-formula><mml:math id="inf130"><mml:mi mathvariant="bold-italic">Ïµ</mml:mi></mml:math></inline-formula> with variance <inline-formula><mml:math id="inf131"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This noise has a slower dynamical timescale <inline-formula><mml:math id="inf132"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mtext>noise</mml:mtext></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:math></inline-formula> than the population rate, allowing fluctuations around a stimulus-dependent steady-state (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). This model is the stochastic stabilized supralinear network (SSSN) (<xref ref-type="bibr" rid="bib38">Hennequin et al., 2018</xref>)<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>ð±</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>ð±</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>â¢</mml:mo><mml:mi>ð±</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>ð¡</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">Ïµ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>generalized to have multiple inhibitory neuron types. It introduces stochasticity to four neuron-type models of V1 (<xref ref-type="bibr" rid="bib50">Litwin-Kumar et al., 2016</xref>). Stochasticity and inhibitory multiplicity introduce substantial complexity to the mathematical treatment of this problem (see Section 'Primary visual cortex: Mathematical intuition and challenges') motivating the analysis of this model with EPI. Here, we consider fixed weights <inline-formula><mml:math id="inf133"><mml:mi>W</mml:mi></mml:math></inline-formula> and input <inline-formula><mml:math id="inf134"><mml:mi mathvariant="bold">ð¡</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib71">Palmigiano et al., 2020</xref>), and study the effect of input variability <inline-formula><mml:math id="inf135"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> on excitatory variability.</p><p>We quantify levels of E-population variability by studying two emergent properties<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left left left left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="script">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>5</mml:mn><mml:mtext>Hz</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mtext>Hz</mml:mtext></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mspace width="2em"/><mml:mrow><mml:mi mathvariant="script">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>10</mml:mn><mml:mtext>Hz</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mtext>Hz</mml:mtext></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mtext>Hz</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mtext>Hz</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the standard deviation of the stochastic <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-population response about its steady state (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). In the following analyses, we select 1 Hz<sup>2</sup> variance such that the two emergent properties do not overlap in <inline-formula><mml:math id="inf138"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>First, we ran EPI to obtain parameter distribution <inline-formula><mml:math id="inf139"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>Hz</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> producing E-population variability around 5 Hz (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). From the marginal distribution of <inline-formula><mml:math id="inf140"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf141"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, top-left), we can see that <inline-formula><mml:math id="inf142"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is sensitive to various combinations of <inline-formula><mml:math id="inf143"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf144"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula>. Alternatively, both <inline-formula><mml:math id="inf145"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf146"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula> are degenerate with respect to <inline-formula><mml:math id="inf147"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> evidenced by the unexpectedly high variability in those dimensions (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, bottom-right). Together, these observations imply a curved path with respect to <inline-formula><mml:math id="inf148"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of 5 Hz, which is indicated by the modes along <inline-formula><mml:math id="inf149"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3E</xref>).</p><p><xref ref-type="fig" rid="fig3">Figure 3E</xref> suggests a quadratic relationship in E-population fluctuations and the standard deviation of E- and P-population input; as the square of either <inline-formula><mml:math id="inf150"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf151"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> increases, the other compensates by decreasing to preserve the level of <inline-formula><mml:math id="inf152"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This quadratic relationship is preserved at greater level of E-population variability <inline-formula><mml:math id="inf153"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>Hz</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3E</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref>). Indeed, the sum of squares of <inline-formula><mml:math id="inf154"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf155"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> is larger in <inline-formula><mml:math id="inf156"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>Hz</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> than <inline-formula><mml:math id="inf157"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>Hz</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3F</xref>, <inline-formula><mml:math id="inf158"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>), while the sum of squares of <inline-formula><mml:math id="inf159"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf160"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula> are not significantly different in the two EPI distributions (<xref ref-type="fig" rid="fig3s3">Figure 3âfigure supplement 3</xref>, <inline-formula><mml:math id="inf161"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>.40</mml:mn></mml:mrow></mml:math></inline-formula>), in which parameters were bounded from 0 to 0.5. The strong interaction between E- and P-population input variability on excitatory variability is intriguing, since this circuit exhibits a paradoxical effect in the P-population (and no other inhibitory types) (<xref ref-type="fig" rid="fig3s4">Figure 3âfigure supplement 4</xref>), meaning that the E-population is P-stabilized. Future research may uncover a link between the population of network stabilization and compensatory interactions governing excitatory variability.</p><p>EPI revealed the quadratic dependence of excitatory variability on input variability to the E- and P-populations, as well as its independence to input from the other two inhibitory populations. In a simplified model (<inline-formula><mml:math id="inf162"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mtext>noise</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>), it can be shown that surfaces of equal variance are ellipsoids as a function of <inline-formula><mml:math id="inf163"><mml:mi mathvariant="bold-italic">ð</mml:mi></mml:math></inline-formula> (see Section 'Primary visual cortex: Mathematical intuition and challenges'). Nevertheless, the sensitive and degenerate parameters are intractable to predict mathematically, since the covariance matrix depends on the steady-state solution of the network (<xref ref-type="bibr" rid="bib38">Hennequin et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Gardiner, 2009</xref>), and terms in the covariance expression increase quadratically with each additional neuron-type population (see also Section 'Primary visual cortex: Mathematical intuition and challenges'). By pointing out this mathematical complexity, we emphasize the value of EPI for gaining understanding about theoretical models when mathematical analysis becomes onerous or impractical.</p></sec><sec id="s2-5"><title>EPI identifies two regimes of rapid task switching</title><p>It has been shown that rats can learn to switch from one behavioral task to the next on randomly interleaved trials (<xref ref-type="bibr" rid="bib19">Duan et al., 2015</xref>), and an important question is what neural mechanisms produce this computation. In this experimental setup, rats were given an explicit task cue on each trial, either Pro or Anti. After a delay period, rats were shown a stimulus, and made a context (task) dependent response (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). In the Pro task, rats were required to orient toward the stimulus, while in the Anti task, rats were required to orient away from the stimulus. Pharmacological inactivation of the SC impaired rat performance, and time-specific optogenetic inactivation revealed a crucial role for the SC on the cognitively demanding Anti trials (<xref ref-type="bibr" rid="bib20">Duan et al., 2021</xref>). These results motivated a nonlinear dynamical model of the SC containing four functionallyÂ defined neuron-type populations. In <xref ref-type="bibr" rid="bib20">Duan et al., 2021</xref>, a computationally intensive procedure was used to obtain a set of 373 connectivity parameters that qualitatively reproduced these optogenetic inactivation results. To build upon the insights of this previous work, we use the probabilistic tools afforded by EPI to identify and characterize two linked, yet distinct regimes of rapid task switching connectivity.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Inferring rapid task switching networks in superior colliculus.</title><p>(<bold>A</bold>) Rapid task switching behavioral paradigm (see text). (<bold>B</bold>) Model of superior colliculus (SC). Neurons: LP - Left Pro, RP - Right Pro, LA - Left Anti, RA - Right Anti. Parameters: <inline-formula><mml:math id="inf164"><mml:mrow><mml:mi>s</mml:mi><mml:mo mathvariant="normal">â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> - self, <inline-formula><mml:math id="inf165"><mml:mrow><mml:mi>h</mml:mi><mml:mo mathvariant="normal">â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> - horizontal, <inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>v</mml:mi><mml:mo mathvariant="normal">â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> -vertical, <inline-formula><mml:math id="inf167"><mml:mrow><mml:mi>d</mml:mi><mml:mo mathvariant="normal">â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> - diagonal weights. (<bold>C</bold>) The EPI inferred distribution of rapid task switching networks. Red/purple parameters indicate modes <inline-formula><mml:math id="inf168"><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo mathvariant="normal">*</mml:mo></mml:msup><mml:mo mathvariant="normal">â¢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo mathvariant="normal">â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> colored by <inline-formula><mml:math id="inf169"><mml:mrow><mml:mi>s</mml:mi><mml:mo mathvariant="normal">â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>. Sensitivity vectors <inline-formula><mml:math id="inf170"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mn mathvariant="normal">1</mml:mn></mml:msub><mml:mo mathvariant="normal">â¢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo mathvariant="normal">*</mml:mo></mml:msup><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are shown by arrows. (Bottom-left) EPI predictive distribution of task accuracies. (<bold>D</bold>) Mean and standard error (<inline-formula><mml:math id="inf171"><mml:msub><mml:mi>N</mml:mi><mml:mtext mathvariant="normal">test</mml:mtext></mml:msub></mml:math></inline-formula> = 25, bars not visible) of accuracy in Pro (top) and Anti (bottom) tasks after perturbing connectivity away from mode along <inline-formula><mml:math id="inf172"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mn mathvariant="normal">1</mml:mn></mml:msub><mml:mo mathvariant="normal">â¢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo mathvariant="normal">*</mml:mo></mml:msup><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (left), <inline-formula><mml:math id="inf173"><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mtext mathvariant="normal">task</mml:mtext></mml:msub></mml:math></inline-formula> (middle), and <inline-formula><mml:math id="inf174"><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mtext mathvariant="normal">diag</mml:mtext></mml:msub></mml:math></inline-formula> (right).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 1.</label><caption><title>Task accuracy by EPI inferred SC network connectivity.</title><p>(<bold>A</bold>) Same pairplot as <xref ref-type="fig" rid="fig4">Figure 4C</xref> colored by Pro-task accuracy. (<bold>B</bold>) Same as A colored by Anti-task accuracy. (<bold>C</bold>) Connectivity parameters of EPI distributions versus task accuracies. Î² is slope coefficient of linear regression, <inline-formula><mml:math id="inf175"><mml:mi>r</mml:mi></mml:math></inline-formula> is correlation, and <inline-formula><mml:math id="inf176"><mml:mi>p</mml:mi></mml:math></inline-formula> is the two-tailed p-value.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 2.</label><caption><title>SC network simulations by regime.</title><p>(<bold>A</bold>) Simulations in network regime 1: <inline-formula><mml:math id="inf177"><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.75</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>B</bold>) Simulations in network regime 2: <inline-formula><mml:math id="inf178"><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 3.</label><caption><title>Eigenmodes of SC connectivity.</title><p>(<bold>A</bold>) Invariant eigenvectors of connectivity matrix <inline-formula><mml:math id="inf179"><mml:mi>W</mml:mi></mml:math></inline-formula>. (<bold>B</bold>) Accuracies for connectivity perturbations when changing <inline-formula><mml:math id="inf180"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>all</mml:mtext></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf181"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>side</mml:mtext></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="inf182"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>task</mml:mtext></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf183"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>diag</mml:mtext></mml:msub></mml:math></inline-formula> shown in <xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig4-figsupp3-v1.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 4.</label><caption><title>EPI optimization of the SC model producing rapid task switching.</title><p>(<bold>A</bold>) Entropy throughout optimization. (<bold>B</bold>) The emergent property statistic means and variances converge to their constraints at 20,000 iterations following the tenth augmented lagrangian epoch.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig4-figsupp4-v1.tif"/></fig><fig id="fig4s5" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 5.</label><caption><title>SC connectivities obtained through brute-force sampling.</title><p>(<bold>A</bold>) Rapid task switching SC connectivities obtained from random sampling. (<bold>B</bold>) Task accuracies of the inferred distributions from random sampling (top) and EPI (bottom).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig4-figsupp5-v1.tif"/></fig></fig-group><p>In this SC model, there are Pro- and Anti-populations in each hemisphere (left (L) and right (R)) with activity variables <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib20">Duan et al., 2021</xref>). The connectivity of these populations is parameterized by self <inline-formula><mml:math id="inf185"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>, vertical <inline-formula><mml:math id="inf186"><mml:mrow><mml:mi>v</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>, diagonal <inline-formula><mml:math id="inf187"><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> and horizontal <inline-formula><mml:math id="inf188"><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> connections (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). The input <inline-formula><mml:math id="inf189"><mml:mi mathvariant="bold">ð¡</mml:mi></mml:math></inline-formula> is comprised of a positive cue-dependent signal to the Pro- or Anti-populations, a positive stimulus-dependent input to either the Left or Right populations, and a choice-period input to the entire network (see Section 'SC model'). Model responses are bounded from 0 to 1 as a function <inline-formula><mml:math id="inf190"><mml:mi>Ï</mml:mi></mml:math></inline-formula> of an internal variable <inline-formula><mml:math id="inf191"><mml:mi mathvariant="bold">ð®</mml:mi></mml:math></inline-formula><disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Ï</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The model responds to the side with greater Pro neuron activation; forÂ example the response is left if <inline-formula><mml:math id="inf192"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> at the end of the trial. Here, we use EPI to determine the network connectivity <inline-formula><mml:math id="inf193"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> that produces rapid task switching.</p><p>Rapid task switching is formalized mathematically as an emergent property with two statistics: accuracy in the Pro task <inline-formula><mml:math id="inf194"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and Anti task <inline-formula><mml:math id="inf195"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We stipulate that accuracy be on average 0.75 in each task with variance <inline-formula><mml:math id="inf196"><mml:msup><mml:mn>.075</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula><disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="script">ð³</mml:mi></mml:mrow><mml:mo>:</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>.75</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>.75</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msup><mml:mn>.075</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mn>.075</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Seventy-fiveÂ percent accuracy is a realistic level of performance in each task, and with the chosen variance, inferred models will not exhibit fully random responses (50%), nor perfect performance (100%).</p><p>The EPI inferred distribution (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) produces Pro- and Anti-task accuracies (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, bottom-left) consistent with rapid task switching (<xref ref-type="disp-formula" rid="equ9">Equation 9</xref>). This parameter distribution has rich structure that is not captured well by simple linear correlations (<xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref>). Specifically, the shape of the EPI distribution is sharply bent, matching ground truth structure indicated by brute-force sampling (<xref ref-type="fig" rid="fig4s5">Figure 4âfigure supplement 5</xref>). This is most saliently observed in the marginal distribution of <inline-formula><mml:math id="inf197"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>-<inline-formula><mml:math id="inf198"><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4C</xref> top-right), where anticorrelation between <inline-formula><mml:math id="inf199"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf200"><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> switches to correlation with decreasing <inline-formula><mml:math id="inf201"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>. By identifying the modes of the EPI distribution <inline-formula><mml:math id="inf202"><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at different values of <inline-formula><mml:math id="inf203"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4C</xref> red/purple dots), we can quantify this change in distributional structure with the sensitivity dimension <inline-formula><mml:math id="inf204"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4C</xref> red/purple arrows). Note that the directionality of these sensitivity dimensions at <inline-formula><mml:math id="inf205"><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> changes distinctly with <inline-formula><mml:math id="inf206"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>, and are perpendicular to the robust dimensions of the EPI distribution that preserve rapid task switching. These two directionalities of sensitivity motivate the distinction of connectivity into two regimes, which produce different types of responses in the Pro and Anti tasks (<xref ref-type="fig" rid="fig4s2">Figure 4âfigure supplement 2</xref>).</p><p>When perturbing connectivity along the sensitivity dimension away from the modes<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mi>ð³</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>ð³</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>Î´</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>ð¯</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>ð³</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Pro-accuracy monotonically increases in both regimes (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, top-left). However, there is a stark difference between regimes in Anti-accuracy. Anti-accuracy falls in either direction of <inline-formula><mml:math id="inf207"><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> in regime 1, yet monotonically increases along with Pro accuracy in regime 2 (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, bottom-left). The sharp change in local structure of the EPI distribution is therefore explained by distinct sensitivities: Anti-accuracy diminishes in only one or both directions of the sensitivity perturbation.</p><p>To understand the mechanisms differentiating the two regimes, we can make connectivity perturbations along dimensions that only modify a single eigenvalue of the connectivity matrix. These eigenvalues <inline-formula><mml:math id="inf208"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>all</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf209"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>side</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf210"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>task</mml:mtext></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf211"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>diag</mml:mtext></mml:msub></mml:math></inline-formula> correspond to connectivity eigenmodes with intuitive roles in processing in this task (<xref ref-type="fig" rid="fig4s3">Figure 4âfigure supplement 3A</xref>). For example, greater <inline-formula><mml:math id="inf212"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>task</mml:mtext></mml:msub></mml:math></inline-formula> will strengthen internal representations of task, while greater <inline-formula><mml:math id="inf213"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>diag</mml:mtext></mml:msub></mml:math></inline-formula> will amplify dominance of Pro and Anti pairs in opposite hemispheres (Section 'Connectivity eigendecomposition and processing modes'). Unlike the sensitivity dimension, the dimensions <inline-formula><mml:math id="inf214"><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> that perturb isolated connectivity eigenvalues <inline-formula><mml:math id="inf215"><mml:msub><mml:mi>Î»</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> for <inline-formula><mml:math id="inf216"><mml:mrow><mml:mi>a</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mtext>all</mml:mtext><mml:mo>,</mml:mo><mml:mtext>side</mml:mtext><mml:mo>,</mml:mo><mml:mtext>task</mml:mtext><mml:mo>,</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are independent of <inline-formula><mml:math id="inf217"><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (see Section 'Connectivity eigendecomposition and processing modes'), e.g.<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:mi>ð³</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>ð³</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>Î´</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>ð¯</mml:mi><mml:mtext>task</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Connectivity perturbation analyses reveal that decreasing <inline-formula><mml:math id="inf218"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>task</mml:mtext></mml:msub></mml:math></inline-formula> has a very similar effect on Anti accuracy as perturbations along the sensitivity dimension (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, middle). The similar effects of perturbations along the sensitivity dimension <inline-formula><mml:math id="inf219"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and reduction of task eigenvalue (via perturbations along <inline-formula><mml:math id="inf220"><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mtext>task</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>) suggest that there is a carefully tuned strength of task representation in connectivity regime 1, which if disturbed results in random Anti-trial responses. Finally, we recognize that increasing <inline-formula><mml:math id="inf221"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>diag</mml:mtext></mml:msub></mml:math></inline-formula> has opposite effects on Anti-accuracy in each regime (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, right). In the next section, we build on these mechanistic characterizations of each regime by examining their resilience to optogenetic inactivation.</p></sec><sec id="s2-6"><title>EPI inferred SC connectivities reproduce results from optogenetic inactivation experiments</title><p>During the delay period of this task, the circuit must prepare to execute the correct task according to the presented cue. The circuit must then maintain a representation of task throughout the delay period, which is important for correct execution of the Anti-task. Duan et al. found that bilateral optogenetic inactivation of SC during the delay period consistently decreased performance in the Anti-task, but had no effect on the Pro-task (<xref ref-type="fig" rid="fig5">Figure 5A</xref>; <xref ref-type="bibr" rid="bib20">Duan et al., 2021</xref>). The distribution of connectivities inferred by EPI exhibited this same effect in simulation at high optogenetic strengths Î³, which reduce the network activities <inline-formula><mml:math id="inf222"><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by a factor <inline-formula><mml:math id="inf223"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>Î³</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) (see Section 'Modeling optogenetic silencing').</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Responses to optogenetic perturbation by connectivity regime.</title><p>(<bold>A</bold>) Mean and standard error (bars) across recording sessions of task error following delay period optogenetic inactivation in rats. (<bold>B</bold>)Â Mean and standard deviation (bars) of task error induced by delay period inactivation of varying optogenetic strength Î³ across the EPI distribution. (<bold>C</bold>) (Left) Mean and standard error of Pro and Anti error from regime 1 to regime 2 at <inline-formula><mml:math id="inf224"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:mn>0.675</mml:mn></mml:mrow></mml:math></inline-formula>. (Right) Correlations of connectivity eigenvalues with Anti error from regime 1 to regime 2 at <inline-formula><mml:math id="inf225"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:mn>0.675</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>D</bold>) (Left) Mean and standard deviation (shading) of responses of the SC model at the mode of the EPI distribution to delay period inactivation at <inline-formula><mml:math id="inf226"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:mn>0.85</mml:mn></mml:mrow></mml:math></inline-formula>. Accuracy in Pro (top) and Anti (bottom) task is shown as a percentage. (Right) Anti-accuracy following delay period inactivation at <inline-formula><mml:math id="inf227"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:mn>0.85</mml:mn></mml:mrow></mml:math></inline-formula> versus accuracy in the Pro-task across connectivities in the EPI distribution.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 1.</label><caption><title>SC responses to delay period inactivation at Anti error saturating levels.</title><p>(Left) Mean and standard error of Pro- and Anti-error from regime 1 to regime 2 at <inline-formula><mml:math id="inf228"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:mn>0.85</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>(Right) Correlations of connectivity eigenvalues with Anti-error from regime 1 to regime 2 at <inline-formula><mml:math id="inf229"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:mn>0.85</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 2.</label><caption><title>SC responses to delay period inactivation at experiment matching levels.</title><p>(Left) Mean and standard deviation (shading) of responses of the SC model at the mode of the EPI distribution to delay period inactivation at <inline-formula><mml:math id="inf230"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:mn>0.675</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>Accuracy in Pro (top) and Anti (bottom) task is shown as a percentage. (Right) Anti-accuracy following delay period inactivation at <inline-formula><mml:math id="inf231"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:mn>0.675</mml:mn></mml:mrow></mml:math></inline-formula> versus accuracy in the Pro-task across connectivities in the EPI distribution.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56265-fig5-figsupp2-v1.tif"/></fig></fig-group><p>To examine how connectivity affects response to delay period inactivation, we grouped connectivities of the EPI distribution along the continuum linking regimes 1 and 2 of Section 'EPI identifies two regimes of rapid task switching'. <inline-formula><mml:math id="inf232"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the set of EPI samples for which the closest mode was <inline-formula><mml:math id="inf233"><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (see Section 'Mode identification with EPI'). In the following analyses, we examine how error, and the influence of connectivity eigenvalue on Anti-error change along this continuum of connectivities. Obtaining the parameter samples for these analysis with the learned EPI distribution was more than 20,000 times faster than a brute force approach (see Section 'Sample grouping by mode').</p><p>The mean increase in Anti-error of the EPI distribution is closest to the experimentally measured value of 7% at <inline-formula><mml:math id="inf234"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:mn>0.675</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, black dot). At this level of optogenetic strength, regime 1 exhibits an increase in Anti-error with delay period silencing (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, left), while regime 2 does not. In regime 1, greater <inline-formula><mml:math id="inf235"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>task</mml:mtext></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf236"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>diag</mml:mtext></mml:msub></mml:math></inline-formula> decrease Anti-error (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, right). In other words, stronger task representations and diagonal amplification make the SC model more resilient to delay period silencing in the Anti-task. This complements the finding from <xref ref-type="bibr" rid="bib20">Duan et al., 2021</xref> (<xref ref-type="bibr" rid="bib20">Duan et al., 2021</xref>) that <inline-formula><mml:math id="inf237"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>task</mml:mtext></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf238"><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>diag</mml:mtext></mml:msub></mml:math></inline-formula> improve Anti accuracy.</p><p>At roughly <inline-formula><mml:math id="inf239"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:mn>0.85</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, gray dot), the Anti-error saturates, while Pro-error remains at zero. Following delay period inactivation at this optogenetic strength, there are strong similarities in the responses of Pro- and Anti-trials during the choice period (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, left). We interpreted these similarities to suggest that delay period inactivation at this saturated level flips the internal representation of task (from Anti to Pro) in the circuit model. A flipped task representation would explain why the Anti-error saturates at 50%: the average Anti-accuracy in EPI inferred connectivities is 75%, but average Anti accuracy would be 25% (100% - <inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) if the internal representation of task is flipped during the delay period.This hypothesis prescribes a model of Anti-accuracy during delay period silencing of <inline-formula><mml:math id="inf241"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mtext>opto</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>100</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, which is fit closely across both regimes of the EPI inferred connectivities (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, right). Similarities between Pro- and Anti-trial responses were not present at the experiment-matching level of <inline-formula><mml:math id="inf242"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:mn>0.675</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref> left) and neither was anticorrelation in <inline-formula><mml:math id="inf243"><mml:msub><mml:mi>p</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf244"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mtext>opto</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref> right).</p><p>In summary, the connectivity inferred by EPI to perform rapid task switching replicated results from optogenetic silencing experiments. We found that at levels of optogenetic strength matching experimental levels of Anti-error, only one regime actually exhibited the effect. This connectivity regime is less resilient to optogenetic perturbation, and perhaps more biologically realistic. Finally, we characterized the pathology in Anti-error that occurs in both regimes when optogenetic strength is increased to high levels, leading to a mechanistic hypothesis that is experimentally testable. The probabilistic tools afforded by EPI yielded this insight: we identified two regimes and the continuum of connectivities between them by taking gradients of parameter probabilities in the EPI distribution, we identified sensitivity dimensions by measuring the Hessian of the EPI distribution, and we obtained many parameter samples at each step along the continuum at an efficient rate.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In neuroscience, machine learning has primarily been used to reveal structure in neural datasets (<xref ref-type="bibr" rid="bib72">Paninski and Cunningham, 2018</xref>). Careful inference procedures are developed for these statistical models allowing precise, quantitative reasoning, which clarifies the way data informs beliefs about the model parameters. However, these statistical models often lack resemblance to the underlying biology, making it unclear how to go from the structure revealed by these methods, to the neural mechanisms giving rise to it. In contrast, theoretical neuroscience has primarily focused on careful models of neural circuits and the production of emergent properties of computation, rather than measuring structure in neural datasets. In this work, we improve upon parameter inference techniques in theoretical neuroscience with emergent property inference, harnessing deep learning towards parameter inference in neural circuit models (see Section 'Related approaches').</p><p>Methodology for statistical inference in circuit models has evolved considerably in recent years. Early work used rejection sampling techniques (<xref ref-type="bibr" rid="bib5">Beaumont et al., 2002</xref>; <xref ref-type="bibr" rid="bib57">Marjoram et al., 2003</xref>; <xref ref-type="bibr" rid="bib88">Sisson et al., 2007</xref>), but EPI and another recently developed methodology (<xref ref-type="bibr" rid="bib31">GonÃ§alves et al., 2019</xref>) employ deep learning to improve efficiency and provide flexible approximations. SNPE has been used for posterior inference of parameters in circuit models conditioned upon exemplar data used to represent computation, but it does not infer parameter distributions that only produce the computation of interest like EPI (see Section 'Scaling inference of recurrent neural network connectivity with EPI'). When strict control over the predictions of the inferred parameters is necessary, EPI uses a constrained optimization technique (<xref ref-type="bibr" rid="bib51">Loaiza-Ganem et al., 2017</xref>) (see Section 'Augmented lagrangian optimization') to make inference conditioned on the emergent property possible.</p><p>A key difference between EPI and SNPE, is that EPI uses gradients of the emergent property throughout optimization. In Section 'Scaling inference of recurrent neural network connectivity with EPI', we showed that such gradients confer beneficial scaling properties, but a concern remains that emergent property gradients may be too computationally intensive. Even in a case of close biophysical realism with an expensive emergent property gradient, EPI was run successfully on intermediate hub frequency in a five-neuron subcircuit model of the STG (Section 'Motivating emergent property inference of theoretical models'). However, conditioning on the pyloric rhythm (<xref ref-type="bibr" rid="bib55">Marder and Selverston, 1992</xref>) in a model of the pyloric subnetwork model (<xref ref-type="bibr" rid="bib78">Prinz et al., 2004</xref>) proved to be prohibitive with EPI. The pyloric subnetwork requires many time steps for simulation and many key emergent property statistics (e.g. burst duration and phase gap) are not calculable or easily approximated with differentiable functions. In such cases, SNPE, which does not require differentiability of the emergent property, has proven useful (<xref ref-type="bibr" rid="bib31">GonÃ§alves et al., 2019</xref>). In summary, choice of deep inference technique should consider emergent property complexity and differentiability, dimensionality of parameter space, and the importance of constraining the model behavior predicted by the inferred parameter distribution.</p><p>In this paper, we demonstrate the value of deep inference for parameter sensitivity analyses at both the local and global level. With these techniques, flexible deep probability distributions are optimized to capture global structure by approximating the full distribution of suitable parameters. Importantly, the local structure of this deep probability distribution can be quantified at any parameter choice, offering instant sensitivity measurements after fitting. For example, the global structure captured by EPI revealed two distinct parameter regimes, which had different local structure quantified by the deep probability distribution (see Section 'Superior colliculus'). In comparison, bayesian MCMC is considered a popular approach for capturing global parameter structure (<xref ref-type="bibr" rid="bib27">Girolami and Calderhead, 2011</xref>), but there is no variational approximation (the deep probability distribution in EPI), so sensitivity information is not queryable and sampling remains slow after convergence. Local sensitivity analyses (e.g. <xref ref-type="bibr" rid="bib80">Raue et al., 2009</xref>) may be performed independently at individual parameter samples, but these methods alone do not capture the full picture in nonlinear, complex distributions. In contrast, deep inference yields a probability distribution that produces a wholistic assessment of parameter sensitivity at the local and global level, which we used in this study to make novel insights into a range of theoretical models. Together, the abilities to condition upon emergent properties, the efficient inference algorithm, and the capacity for parameter sensitivity analyses make EPI a useful method for addressing inverse problems in theoretical neuroscience.</p><sec id="s3-1"><title>Code availability statement</title><p>All software written for this study is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/cunningham-lab/epi">https://github.com/cunningham-lab/epi</ext-link>Â (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:ab67f15d843abeb69aa4f1c946310a450f2a7f73;origin=https://github.com/cunningham-lab/epi;visit=swh:1:snp:dff596c83aa72d722930f6495d937774d352555f;anchor=swh:1:rev:38febae7035ca921334a616b0f396b3767bf18d4">swh:1:rev:38febae7035ca921334a616b0f396b3767bf18d4</ext-link>),Â <xref ref-type="bibr" rid="bib9">Bittner, 2021</xref>.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Emergent property inference (EPI)</title><p>Solving inverse problems is an important part of theoretical neuroscience, since we must understand how neural circuit models and their parameter choices produce computations. Recently, research on machine learning methodology for neuroscience has focused on finding latent structure in large-scale neural datasets, while research in theoretical neuroscience generally focuses on developing precise neural circuit models that can produce computations of interest. By quantifying computation into an <italic>emergent property</italic> through statistics of the emergent activity of neural circuit models, we can adapt the modern technique of deep probabilistic inference towards solving inverse problems in theoretical neuroscience. Here, we introduce a novel method for statistical inference, which uses deep networks to learn parameter distributions constrained to produce emergent properties of computation.</p><p>Consider model parameterization <inline-formula><mml:math id="inf245"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula>, which is a collection of scientifically meaningful variables that govern the complex simulation of data <inline-formula><mml:math id="inf246"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula>. For example (see Section 'Motivating emergent property inference of theoretical models'), <inline-formula><mml:math id="inf247"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> may be the electrical conductance parameters of an STG subcircuit, and <inline-formula><mml:math id="inf248"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula> the evolving membrane potentials of the five neurons. In terms of statistical modeling, this circuit model has an intractable likelihood <inline-formula><mml:math id="inf249"><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which is predicated by the stochastic differential equations that define the model. From a theoretical perspective, we are less concerned about the likelihood of an exemplar dataset <inline-formula><mml:math id="inf250"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula>, but rather the emergent property of intermediate hub frequency (which implies a consistent dataset <inline-formula><mml:math id="inf251"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula>).</p><p>In this work, emergent properties <inline-formula><mml:math id="inf252"><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:math></inline-formula> are defined through the choice of emergent property statistic <inline-formula><mml:math id="inf253"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (which is a vector of one or more statistics), and its means <inline-formula><mml:math id="inf254"><mml:mi mathvariant="bold-italic">ð</mml:mi></mml:math></inline-formula>, and variances <inline-formula><mml:math id="inf255"><mml:msup><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi><mml:mo rspace="5.8pt">:</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð³</mml:mi><mml:mo>,</mml:mo><mml:mi>ð±</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>ð</mml:mi></mml:mrow><mml:mo rspace="9.1pt">,</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mi>ð³</mml:mi><mml:mo>,</mml:mo><mml:mi>ð±</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>ð</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In general, an emergent property may be a collection of first-, second-, or higher-order moments of a group of statistics, but this study focuses on the case written in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>. In the STG example, intermediate hub frequency is defined by mean and variance constraints on the statistic of hub neuron frequency <inline-formula><mml:math id="inf256"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mtext>hub</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ2 equ3">Equations 2 and 3)</xref>. Precisely, the emergent property statistics <inline-formula><mml:math id="inf257"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> must have means <inline-formula><mml:math id="inf258"><mml:mi mathvariant="bold-italic">ð</mml:mi></mml:math></inline-formula> and variances <inline-formula><mml:math id="inf259"><mml:msup><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> over the EPI distribution of parameters (<inline-formula><mml:math id="inf260"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) and the data produced by those parameters (<inline-formula><mml:math id="inf261"><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>), where the inferred parameter distribution <inline-formula><mml:math id="inf262"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> itself is parameterized by deep network weights and biases <inline-formula><mml:math id="inf263"><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:math></inline-formula>.</p><p>In EPI, a deep probability distribution <inline-formula><mml:math id="inf264"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is optimized to approximate the parameter distribution producing the emergent property <inline-formula><mml:math id="inf265"><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:math></inline-formula>. In contrast to simpler classes of distributions like the gaussian or mixture of gaussians, deep probability distributions are far more flexible and capable of fitting rich structure (<xref ref-type="bibr" rid="bib81">Rezende and Mohamed, 2015</xref>; <xref ref-type="bibr" rid="bib74">Papamakarios et al., 2019a</xref>). In deep probability distributions, a simple random variable <inline-formula><mml:math id="inf266"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (we choose an isotropic gaussian) is mapped deterministically via a sequence of deep neural network layers (<italic>g</italic><sub>1</sub>, . <italic>g</italic><sub><italic>l</italic></sub>) parameterized by weights and biases <inline-formula><mml:math id="inf267"><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:math></inline-formula> to the support of the distribution of interest:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>â¼</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Such deep probability distributions embed the inferred distribution in a deep network. Once optimized, this deep network representation of a distribution has remarkably useful properties: fast sampling and probability evaluations. Importantly, fast probability evaluations confer fast gradient and Hessian calculations as well.</p><p>Given this choice of circuit model and emergent property <inline-formula><mml:math id="inf268"><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf269"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is optimized via the neural network parameters <inline-formula><mml:math id="inf270"><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:math></inline-formula> to find a maximally entropic distribution <inline-formula><mml:math id="inf271"><mml:msubsup><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> within the deep variational family <inline-formula><mml:math id="inf272"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð¬</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">ð½</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">Î</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that produces the emergent property <inline-formula><mml:math id="inf273"><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:math></inline-formula>:<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>â£</mml:mo><mml:mrow><mml:mi mathvariant="script">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:munder><mml:mtext>Â </mml:mtext><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>s.t.Â </mml:mtext><mml:mrow><mml:mi mathvariant="script">ð³</mml:mi></mml:mrow><mml:mtext>Â </mml:mtext><mml:mrow/><mml:mtext>Â </mml:mtext><mml:mrow/><mml:mo>:</mml:mo><mml:mtext>Â </mml:mtext><mml:mrow/><mml:mtext>Â </mml:mtext><mml:mrow/><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">Î¼</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf274"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mi mathvariant="bold">ð³</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is entropy. By maximizing the entropy of the inferred distribution <inline-formula><mml:math id="inf275"><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub></mml:math></inline-formula>, we select the most random distribution in family <inline-formula><mml:math id="inf276"><mml:mi class="ltx_font_mathcaligraphic">ð¬</mml:mi></mml:math></inline-formula> that satisfies the constraints of the emergent property. Since entropy is maximized in <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>, EPI is equivalent to bayesian variational inference (see Section 'EPI as variational inference'), which is why we specify the inferred distribution of EPI as conditioned upon emergent property <inline-formula><mml:math id="inf277"><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:math></inline-formula> with the notation <inline-formula><mml:math id="inf278"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To run this constrained optimization, we use an augmented lagrangian objective, which is the standard approach for constrained optimization (<xref ref-type="bibr" rid="bib6">Bertsekas, 2014</xref>), and the approach taken to fit Maximum Entropy Flow Networks (MEFNs) (<xref ref-type="bibr" rid="bib51">Loaiza-Ganem et al., 2017</xref>). This procedure is detailed in Section 'Augmented lagrangian optimization' and the pseudocode in Algorithm 'Augmented lagrangian optimization'.</p><p>In the remainder of Section 'Emergent property inference (EPI)', we will explain the finer details and motivation of the EPI method. First, we explain related approaches and what EPI introduces to this domain (Section 'Related approaches'). Second, we describe the special class of deep probability distributions used in EPI called normalizing flows (Section 'Deep probability distributions and normalizing flows'). Then, we establish the known relationship between maximum entropy distributions and exponential families (Section 'Maximum entropy distributions and exponential families'). Next, we explain the constrained optimization technique used to solve <xref ref-type="disp-formula" rid="equ14">Equation 14</xref> (Section 'Augmented lagrangian optimization'). Then, we demonstrate the details of this optimization in a toy example (Section 'Example: 2D LDS'). Finally, we explain how EPI is equivalent to variational inference (Section 'EPI as variational inference').</p><sec id="s4-1-1"><title>Related approaches</title><p>When bayesian inference problems lack conjugacy, scientists use approximate inference methods like variational inference (VI) (<xref ref-type="bibr" rid="bib86">Saul and Jordan, 1998</xref>) and Markov chain Monte Carlo (MCMC) (<xref ref-type="bibr" rid="bib59">Metropolis et al., 1953</xref>; <xref ref-type="bibr" rid="bib35">Hastings, 1970</xref>). After optimization, variational methods return a parameterized posterior distribution, which we can analyze. Also, the variational approximation is often chosen such that it permits fast sampling. In contrast MCMC methods only produce samples from the approximated posterior distribution. No parameterized distribution is estimated, and additional samples are always generated with the same sampling complexity. Inference in models defined by systems of differential has been demonstrated with MCMC (<xref ref-type="bibr" rid="bib27">Girolami and Calderhead, 2011</xref>), although this approach requires tractable likelihoods. Advancements have introduced sampling (<xref ref-type="bibr" rid="bib13">Calderhead and Girolami, 2011</xref>), likelihood approximation (<xref ref-type="bibr" rid="bib30">Golightly and Wilkinson, 2011</xref>), and uncertainty quantification techniques (<xref ref-type="bibr" rid="bib15">Chkrebtii et al., 2016</xref>) to make MCMC approaches more efficient and expand the class of applicable models.</p><p>Simulation-based inference (<xref ref-type="bibr" rid="bib17">Cranmer et al., 2020</xref>) is model parameter inference in the absence of a tractable likelihood function. The most prevalent approach to simulation-based inference is approximate bayesian computation (ABC) (<xref ref-type="bibr" rid="bib5">Beaumont et al., 2002</xref>), in which satisfactory parameter samples are kept from random prior sampling according to a rejection heuristic. The obtained set of parameters do not have a probabilities, and further insight about the model must be gained from examination of the parameter set and their generated activity. Methodological advances to ABC methods have come through the use of Markov chain Monte Carlo (MCMC-ABC) (<xref ref-type="bibr" rid="bib57">Marjoram et al., 2003</xref>) and sequential Monte Carlo (SMC-ABC) (<xref ref-type="bibr" rid="bib88">Sisson et al., 2007</xref>) sampling techniques. SMC-ABC is considered state-of-the-art ABC, yet this approach still struggles to scale in dimensionality (<xref ref-type="bibr" rid="bib89">Sisson et al., 2018</xref>; <xref ref-type="fig" rid="fig2">Figure 2</xref>). Still, this method has enjoyed much success in systems biology (<xref ref-type="bibr" rid="bib49">Liepe et al., 2014</xref>). Furthermore, once a parameter set has been obtained by SMC-ABC from a finite set of particles, the SMC-ABC algorithm must be run again from scratch with a new population of initialized particles to obtain additional samples.</p><p>For scientific model analysis, we seek a parameter distribution represented by an approximating distribution as in variational inference (<xref ref-type="bibr" rid="bib86">Saul and Jordan, 1998</xref>): a variational approximation that once optimized yields fast analytic calculations and samples. For the reasons described above, ABC and MCMC techniques are not suitable, because they only produce a set of parameter samples lacking probabilities and have unchanging sampling rate. EPI infers parameters in circuit models using the MEFN (<xref ref-type="bibr" rid="bib51">Loaiza-Ganem et al., 2017</xref>) algorithm with a deep variational approximation. The deep neural network of EPI (<xref ref-type="fig" rid="fig1">Figure 1E</xref>) defines the parametric form (with weights and biases as variational parameters <inline-formula><mml:math id="inf279"><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:math></inline-formula>) of the variational approximation of the inferred parameter distribution <inline-formula><mml:math id="inf280"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The EPI optimization is enabled using stochastic gradient techniques in the spirit of likelihood-free variational inference (<xref ref-type="bibr" rid="bib92">Tran et al., 2017</xref>). The analytic relationship between EPI and variational inference is explained in Section 'EPI as variational inference'.</p><p>We note that, during our preparation and early presentation of this work (<xref ref-type="bibr" rid="bib7">Bittner et al., 2019a</xref>; <xref ref-type="bibr" rid="bib8">Bittner et al., 2019b</xref>), another work has arisen with broadly similar goals: bringing statistical inference to mechanistic models of neural circuits (<xref ref-type="bibr" rid="bib66">Nonnenmacher et al., 2018</xref>; <xref ref-type="bibr" rid="bib60">Michael et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">GonÃ§alves et al., 2019</xref>). We are encouraged by this general problem being recognized by others in the community, and we emphasize that these works offer complementary neuroscientific contributions (different theoretical models of focus) and use different technical methodologies (ours is built on our prior work [<xref ref-type="bibr" rid="bib51">Loaiza-Ganem et al., 2017</xref>], theirs similarly [<xref ref-type="bibr" rid="bib52">Lueckmann et al., 2017</xref>]).</p><p>The method EPI differs from SNPE in some key ways. SNPE belongs to a âsequentialâ class of recently developed simulation-based inference methods in which two neural networks are used for posterior inference. This first neural network is a deep probability distribution (normalizing flow) used to estimate the posterior <inline-formula><mml:math id="inf281"><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (SNPE) or the likelihood <inline-formula><mml:math id="inf282"><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (sequential neural likelihood (SNL) [<xref ref-type="bibr" rid="bib75">Papamakarios et al., 2019b</xref>]). A recent approach uses an unconstrained neural network to estimate the likelihood ratio (sequential neural ratio estimation (SNRE) [<xref ref-type="bibr" rid="bib39">Hermans et al., 2020</xref>]). In SNL and SNRE, MCMC sampling techniques are used to obtain samples from the approximated posterior. This contrasts with EPI and SNPE, which use deep probability distributions to model parameters, which facilitates immediate measurements of sample probability, gradient, or Hessian for system analysis. The second neural network in this sequential class of methods is the amortizer. This unconstrained deep network maps data <inline-formula><mml:math id="inf283"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula> (or statistics <inline-formula><mml:math id="inf284"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> or model parameters <inline-formula><mml:math id="inf285"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula>) to the weights and biases of the first neural network. These methods are optimized on a conditional density (or ratio) estimation objective. The data used to optimize this objective are generated via an adaptive procedure, in which training data pairs (<inline-formula><mml:math id="inf286"><mml:msub><mml:mi mathvariant="bold">ð±</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf287"><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>) become sequentially closer to the true data and posterior.</p><p>The approximating fidelity of the deep probability distribution in sequential approaches is optimized to generalize across the training distribution of the conditioning variable. This generalization property of the sequential methods can reduce the accuracy at the singular posterior of interest. Whereas in EPI, the entire expressivity of the deep probability distribution is dedicated to learning a single distribution as well as possible. The well-known inverse mapping problem of exponential families (<xref ref-type="bibr" rid="bib95">Wainwright and Jordan, 2008</xref>) prohibits an amortization-based approach in EPI, since EPI learns an exponential family distribution parameterized by its mean (in contrast to its natural parameter, see Section 'Maximum entropy distributions and exponential families'). However, we have shown that the same two-network architecture of the sequential simulation-based inference methods can be used for amortized inference in intractable exponential family posteriors when using their natural parameterization (<xref ref-type="bibr" rid="bib10">Bittner and Cunningham, 2019</xref>).</p><p>Finally, one important differentiating factor between EPI and sequential simulation-based inference methods is that EPI leverages gradients <inline-formula><mml:math id="inf288"><mml:mrow><mml:mrow><mml:msub><mml:mo>â</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi></mml:msub><mml:mo>â¡</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> during optimization. These gradients can improve convergence time and scalability, as we have shown on an example conditioning low-rank RNN connectivity on the property of stable amplification (see Section 'Scaling inference of recurrent neural network connectivity with EPI'). With EPI, we prove out the suggestion that a deep inference technique can improve efficiency by leveraging these emergent property gradients when they are tractable. Sequential simulation-based inference techniques may be better suited for scientific problems where <inline-formula><mml:math id="inf289"><mml:mrow><mml:mrow><mml:msub><mml:mo>â</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi></mml:msub><mml:mo>â¡</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is intractable or unavailable, like when there is a nondifferentiable emergent property. However, the sequential simulation-based inference techniques cannot constrain the predictions of the inferred distribution in the manner of EPI.</p><p>Structural identifiability analysis involves the measurement of sensitivity and unidentifiabilities in scientific models. Around a single parameter choice, one can measure the Jacobian. One approach for this calculation that scales well is EAR (<xref ref-type="bibr" rid="bib42">Karlsson et al., 2012</xref>). A popular efficient approach for systems of ODEs has been neural ODE adjoint (<xref ref-type="bibr" rid="bib14">Chen et al., 2018</xref>) and its stochastic adaptation (<xref ref-type="bibr" rid="bib48">Li et al., 2020</xref>). Casting identifiability as a statistical estimation problem, the profile likelihood works via iterated optimization while holding parameters fixed (<xref ref-type="bibr" rid="bib80">Raue et al., 2009</xref>). An exciting recent method is capable of recovering the functional form of such unidentifiabilities away from a point by following degenerate dimensions of the fisher information matrix (<xref ref-type="bibr" rid="bib79">Raman et al., 2017</xref>). Global structural non-identifiabilities can be found for models with polynomial or rational dynamics equations using DAISY (<xref ref-type="bibr" rid="bib77">Pia Saccomani et al., 2003</xref>), or through mean optimal transformations (<xref ref-type="bibr" rid="bib36">Hengl et al., 2007</xref>). With EPI, we have all the benefits given by a statistical inference method plus the ability to query the first- or second-order gradient of the probability of the inferred distribution at any chosen parameter value. The second-order gradient of the log probability (the Hessian), which is directly afforded by EPI distributions, produces quantified information about parametric sensitivity of the emergent property in parameter space (see Section 'Emergent property inference via deep generative models').</p></sec><sec id="s4-1-2"><title>Deep probability distributions and normalizing flows</title><p>Deep probability distributions are comprised of multiple layers of fully connected neural networks (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>). When each neural network layer is restricted to be a bijective function, the sample density can be calculated using the change of variables formula at each layer of the network. For <inline-formula><mml:math id="inf290"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mrow><mml:mi mathvariant="bold">ð¢</mml:mi><mml:mo>-</mml:mo><mml:mn>ð</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>,<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ð³</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ð³</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo movablelimits="false">det</mml:mo><mml:mo>â¡</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mo>â¡</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ð³</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mo>â¡</mml:mo><mml:msub><mml:mi>ð³</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ð³</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â¢</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo movablelimits="false">det</mml:mo><mml:mo>â¡</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mo>â¡</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ð³</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mo>â¡</mml:mo><mml:msub><mml:mi>ð³</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>However, this computation has cubic complexity in dimensionality for fully connected layers. By restricting our layers to normalizing flows (<xref ref-type="bibr" rid="bib81">Rezende and Mohamed, 2015</xref>; <xref ref-type="bibr" rid="bib74">Papamakarios et al., 2019a</xref>) â bijective functions with fast log determinant Jacobian computations, which confer a fast calculation of the sample log probability. Fast log probability calculation confers efficient optimization of the maximum entropy objective (see Section 'Augmented lagrangian optimization').</p><p>We use the real NVP (<xref ref-type="bibr" rid="bib18">Dinh et al., 2017</xref>) normalizing flow class, because its coupling architecture confers both fast sampling (forward) and fast log probability evaluation (backward). Fast probability evaluation facilitates fast gradient and Hessian evaluation of log probability throughout parameter space. Glow permutations were used in between coupling stages (<xref ref-type="bibr" rid="bib45">Kingma and Dhariwal, 2018</xref>). This is in contrast to autoregressive architectures (<xref ref-type="bibr" rid="bib73">Papamakarios et al., 2017</xref>; <xref ref-type="bibr" rid="bib43">Kingma et al., 2016</xref>), in which only one of the forward or backward passes can be efficient. In this work, normalizing flows are used as flexible parameter distribution approximations <inline-formula><mml:math id="inf291"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> having weights and biases <inline-formula><mml:math id="inf292"><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:math></inline-formula>. We specify the architecture used in each application by the number of real NVP affine coupling stages, and the number of neural network layers and units per layer of the conditioning functions.</p><p>When calculating Hessians of log probabilities in deep probability distributions, it is important to consider the normalizing flow architecture. With autoregressive architectures (<xref ref-type="bibr" rid="bib43">Kingma et al., 2016</xref>; <xref ref-type="bibr" rid="bib73">Papamakarios et al., 2017</xref>), fast sampling and fast log probability evaluations are mutually exclusive. That makes these architectures undesirable for EPI, where efficient sampling is important for optimization, and log probability evaluation speed predicates the efficiency of gradient and Hessian calculations. With real NVP coupling architectures, we get both fast sampling and fast Hessians making both optimization and scientific analysis efficient.</p></sec><sec id="s4-1-3"><title>Maximum entropy distributions and exponential families</title><p>The inferred distribution of EPI is a maximum entropy distribution, which have fundamental links to exponential family distributions. A maximum entropy distribution of form:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="script">â</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mtext>Â </mml:mtext><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>s.t.Â </mml:mtext><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>â¼</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Î¼</mml:mi><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf293"><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the sufficient statistics vector and <inline-formula><mml:math id="inf294"><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula> a vector of their mean values, will have probability density in the exponential family:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>ð¼</mml:mi><mml:mo>â¤</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>ð³</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The mappings between the mean parameterization <inline-formula><mml:math id="inf295"><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula> and the natural parameterization <inline-formula><mml:math id="inf296"><mml:mi mathvariant="bold-italic">ð¼</mml:mi></mml:math></inline-formula> are formally hard to identify except in special cases (<xref ref-type="bibr" rid="bib95">Wainwright and Jordan, 2008</xref>).</p><p>In this manuscript, emergent properties are defined by statistics <inline-formula><mml:math id="inf297"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> having a fixed mean <inline-formula><mml:math id="inf298"><mml:mi mathvariant="bold-italic">ð</mml:mi></mml:math></inline-formula> and variance <inline-formula><mml:math id="inf299"><mml:msup><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> as in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>. The variance constraint is a second moment constraint on <inline-formula><mml:math id="inf300"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mi>ð³</mml:mi><mml:mo>,</mml:mo><mml:mi>ð±</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð³</mml:mi><mml:mo>,</mml:mo><mml:mi>ð±</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>ð</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>As a general maximum entropy distribution (<xref ref-type="disp-formula" rid="equ16">Equation 16</xref>), the sufficient statistics vector contains both first and second order moments of <inline-formula><mml:math id="inf301"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð±</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi>ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð±</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi>ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>ð</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>which are constrained to the chosen means and variances<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mrow><mml:msub><mml:mi>ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mi>ð</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mi>ð</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Thus, <inline-formula><mml:math id="inf302"><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula> is used to denote the mean parameter of the maximum entropy distribution defined by the emergent property (all constraints), while <inline-formula><mml:math id="inf303"><mml:mi mathvariant="bold-italic">ð</mml:mi></mml:math></inline-formula> is only the mean of <inline-formula><mml:math id="inf304"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The subscript âoptâ of <inline-formula><mml:math id="inf305"><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula> is chosen since it contains all the constraint values to which the EPI optimization algorithm must adhere.</p></sec><sec id="s4-1-4"><title>Augmented lagrangian optimization</title><p>To optimize <inline-formula><mml:math id="inf306"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>, the constrained maximum entropy optimization is executed using the augmented lagrangian method. The following objective is minimized:<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð½</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>ð¼</mml:mi><mml:mtext>opt</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mi>ð¼</mml:mi><mml:mtext>opt</mml:mtext><mml:mo>â¤</mml:mo></mml:msubsup><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð½</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>â¢</mml:mo><mml:msup><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð½</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Â where there are average constraint violations<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð½</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð³</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf307"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ð¼</mml:mi><mml:mtext>opt</mml:mtext></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mi>â</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> are the lagrange multipliers where <inline-formula><mml:math id="inf308"><mml:mi>m</mml:mi></mml:math></inline-formula> is the number of total constraints<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and <inline-formula><mml:math id="inf309"><mml:mi>c</mml:mi></mml:math></inline-formula> is the penalty coefficient. The mean parameter <inline-formula><mml:math id="inf310"><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula> and sufficient statistics <inline-formula><mml:math id="inf311"><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are determined by the means <inline-formula><mml:math id="inf312"><mml:mi mathvariant="bold-italic">ð</mml:mi></mml:math></inline-formula> and variances <inline-formula><mml:math id="inf313"><mml:msup><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> of the emergent property statistics <inline-formula><mml:math id="inf314"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> defined in <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>. Specifically, <inline-formula><mml:math id="inf315"><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a concatenation of the first and second moments (<xref ref-type="disp-formula" rid="equ19">Equation 19</xref>) and <inline-formula><mml:math id="inf316"><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula> is a concatenation of their constraints <inline-formula><mml:math id="inf317"><mml:mi mathvariant="bold-italic">ð</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf318"><mml:msup><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ20">Equation 20</xref>). (Although, note that this algorithm is written for general <inline-formula><mml:math id="inf319"><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf320"><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula> to satisfy the more general class of emergent properties.) The lagrange multipliers <inline-formula><mml:math id="inf321"><mml:msub><mml:mi mathvariant="bold-italic">ð¼</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula> are closely related to the natural parameters <inline-formula><mml:math id="inf322"><mml:mi mathvariant="bold-italic">ð¼</mml:mi></mml:math></inline-formula> of exponential families (see Section 'EPI as variational inference'). Weights and biases <inline-formula><mml:math id="inf323"><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:math></inline-formula> of the deep probability distribution are optimized according to <xref ref-type="disp-formula" rid="equ21">Equation 21</xref> using the Adam optimizer with learning rate 10<sup>â3</sup> (<xref ref-type="bibr" rid="bib44">Kingma and Ba, 2015</xref>).</p><p>The gradient with respect to entropy <inline-formula><mml:math id="inf324"><mml:mrow><mml:mi>H</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be expressed using the reparameterization trick as an expectation of the negative log density of parameter samples <inline-formula><mml:math id="inf325"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> over the randomness in the parameterless initial distribution <inline-formula><mml:math id="inf326"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>):<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">â«</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>ð³</mml:mi></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð³</mml:mi><mml:mo>â¼</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:msub><mml:mi>ð³</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>â¼</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ð³</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Thus, the gradient of the entropy of the deep probability distribution can be estimated as an average of gradients with respect to the base distribution <inline-formula><mml:math id="inf327"><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>:<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo>â</mml:mo><mml:mi>ð½</mml:mi></mml:msub><mml:mo>â¡</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:msub><mml:mi>ð³</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>â¼</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mo>â</mml:mo><mml:mi>ð½</mml:mi></mml:msub><mml:mo>â¡</mml:mo><mml:mi>log</mml:mi></mml:mrow><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ð³</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The gradients of the log density of the deep probability distribution are tractable through the use of normalizing flows (see Section 'Deep probability distributions and normalizing flows').</p><p>The full EPI optimization algorithm is detailed in Algorithm 1. The lagrangian parameters <inline-formula><mml:math id="inf328"><mml:msub><mml:mi mathvariant="bold-italic">ð¼</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula> are initialized to zero and adapted following each augmented lagrangian epoch, which is a period of optimization with fixed (<inline-formula><mml:math id="inf329"><mml:msub><mml:mi mathvariant="bold-italic">ð¼</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf330"><mml:mi>c</mml:mi></mml:math></inline-formula>) for a given number of stochastic gradient descent (SGD) iterations. A low value of <inline-formula><mml:math id="inf331"><mml:mi>c</mml:mi></mml:math></inline-formula> is used initially, and conditionally increased after each epoch based on constraint error reduction. The penalty coefficient is updated based on the result of a hypothesis test regarding the reduction in constraint violation. The p-value of <inline-formula><mml:math id="inf332"><mml:mrow><mml:mrow><mml:mi>ð¼</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ð½</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>â¢</mml:mo><mml:mi>ð¼</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ð½</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is computed, and <inline-formula><mml:math id="inf333"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is updated to <inline-formula><mml:math id="inf334"><mml:mrow><mml:mi>Î²</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf335"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula>. The other update rule is <inline-formula><mml:math id="inf336"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ð¼</mml:mi><mml:mrow><mml:mtext>opt</mml:mtext><mml:mo>,</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ð¼</mml:mi><mml:mrow><mml:mtext>opt</mml:mtext><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>â¢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> given a batch size <inline-formula><mml:math id="inf337"><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf338"><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>â¼</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Throughout the study, <inline-formula><mml:math id="inf339"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula>, while Î² was chosen to be either 2 or 4. The batch size of EPI also varied according to application.</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th valign="top">Algorithm 1. Emergent property inference</th></tr></thead><tbody><tr><td valign="top">1 initialize <inline-formula><mml:math id="inf340"><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:math></inline-formula> by fitting <inline-formula><mml:math id="inf341"><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub></mml:math></inline-formula> to an isotropic gaussian of mean <inline-formula><mml:math id="inf342"><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>init</mml:mtext></mml:msub></mml:math></inline-formula> and variance <inline-formula><mml:math id="inf343"><mml:msubsup><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>init</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> <break/>2 initialize <inline-formula><mml:math id="inf344"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf345"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ð¼</mml:mi><mml:mrow><mml:mtext>opt</mml:mtext><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn/></mml:mrow></mml:math></inline-formula>. <break/>3 for Augmented lagrangian epoch <inline-formula><mml:math id="inf346"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¦</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> do <break/>4Â Â Â Â Â for SGD iteration <inline-formula><mml:math id="inf347"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¦</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> do <break/>5Â Â Â Â Â Â Â Â Â Â Sample <inline-formula><mml:math id="inf348"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¦</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>â¼</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, get transformed variable <inline-formula><mml:math id="inf349"><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf350"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¦</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> <break/>6Â Â Â Â Â Â Â Â Â Â Update <inline-formula><mml:math id="inf351"><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:math></inline-formula> by descending its stochastic gradient (using ADAM optimizer [<xref ref-type="bibr" rid="bib44">Kingma and Ba, 2015</xref>]). <break/>Â Â Â Â Â Â Â Â Â Â <inline-formula><mml:math id="inf352"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">â</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Î·</mml:mi><mml:mrow><mml:mtext>opt</mml:mtext><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="normal">â</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="normal">â</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Î¼</mml:mi><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">Î·</mml:mi><mml:mrow><mml:mtext>opt</mml:mtext><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfrac><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="normal">â</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Î¼</mml:mi><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Î¼</mml:mi><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula> <break/>7Â Â Â Â Â end <break/>8Â Â Â Â Â Sample <inline-formula><mml:math id="inf353"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¦</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>â¼</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, get transformed variable <inline-formula><mml:math id="inf354"><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf355"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¦</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> <break/>9Â Â Â Â Â Update <inline-formula><mml:math id="inf356"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ð¼</mml:mi><mml:mrow><mml:mtext>opt</mml:mtext><mml:mo>,</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ð¼</mml:mi><mml:mrow><mml:mtext>opt</mml:mtext><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>â¢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. <break/>10Â Â Â Â Â Update <inline-formula><mml:math id="inf357"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (see text for detail). <break/>11 end</td></tr></tbody></table></table-wrap><p>In general, <inline-formula><mml:math id="inf358"><mml:mi>c</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf359"><mml:msub><mml:mi mathvariant="bold-italic">ð¼</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula> should start at values encouraging entropic growth early in optimization. With each training epoch in which the update rule for <inline-formula><mml:math id="inf360"><mml:mi>c</mml:mi></mml:math></inline-formula> is invoked, the constraint satisfaction terms are increasingly weighted, which generally results in decreased entropy (e.g. see <xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1C</xref>). This encourages the discovery of suitable regions of parameter space, and the subsequent refinement of the distribution to produce the emergent property. The momentum parameters of the Adam optimizer are reset at the end of each augmented lagrangian epoch, which proceeds for <inline-formula><mml:math id="inf361"><mml:msub><mml:mi>i</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:math></inline-formula> iterations. In this work, we used a maximum number of augmented lagrangian epochs <inline-formula><mml:math id="inf362"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mo>â¥</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>Rather than starting optimization from some <inline-formula><mml:math id="inf363"><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:math></inline-formula> drawn from a randomized distribution, we found that initializing <inline-formula><mml:math id="inf364"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to approximate an isotropic gaussian distribution conferred more stable, consistent optimization. The parameters of the gaussian initialization were chosen on an application-specific basis. Throughout the study, we chose isotropic Gaussian initializations with mean <inline-formula><mml:math id="inf365"><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>init</mml:mtext></mml:msub></mml:math></inline-formula> at the center of the support of the distribution and some variance <inline-formula><mml:math id="inf366"><mml:msubsup><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>init</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, except for one case, where an initialization informed by random search was used (see Section 'Stomatogastric ganglion'). Deep probability distributions were fit to these gaussian initializations using 10,000 iterations of stochastic gradient descent on the evidence lower bound (as in <xref ref-type="bibr" rid="bib10">Bittner and Cunningham, 2019</xref>) with Adam optimizer and a learning rate of <inline-formula><mml:math id="inf367"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>.</p><p>To assess whether the EPI distribution <inline-formula><mml:math id="inf368"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> produces the emergent property, we assess whether each individual constraint on the means and variances of <inline-formula><mml:math id="inf369"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is satisfied. We consider the EPI to have converged when a null hypothesis test of constraint violations <inline-formula><mml:math id="inf370"><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">ð½</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> being zero is accepted for all constraints <inline-formula><mml:math id="inf371"><mml:mrow><mml:mi>i</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¦</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at a significance threshold <inline-formula><mml:math id="inf372"><mml:mrow><mml:mi>Î±</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>. This significance threshold is adjusted through Bonferroni correction according to the number of constraints <inline-formula><mml:math id="inf373"><mml:mi>m</mml:mi></mml:math></inline-formula>. The p-values for each constraint are calculated according to a two-tailed nonparametric test, where 200 estimations of the sample mean <inline-formula><mml:math id="inf374"><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">ð½</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> are made using <inline-formula><mml:math id="inf375"><mml:msub><mml:mi>N</mml:mi><mml:mtext>test</mml:mtext></mml:msub></mml:math></inline-formula> samples of <inline-formula><mml:math id="inf376"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> at the end of the augmented lagrangian epoch. Of all <inline-formula><mml:math id="inf377"><mml:msub><mml:mi>k</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:math></inline-formula> augmented lagrangian epochs, we select the EPI inferred distribution as that which satisfies the convergence criteria and has greatest entropy.</p><p>When assessing the suitability of EPI for a particular modeling question, there are some important technical considerations. First and foremost, as in any optimization problem, the defined emergent property should always be appropriately conditioned (constraints should not have wildly different units). Furthermore, if the program is underconstrained (not enough constraints), the distribution grows (in entropy) unstably unless mapped to a finite support. If overconstrained, there is no parameter set producing the emergent property, and EPI optimization will fail (appropriately).</p></sec><sec id="s4-1-5"><title>Example: 2D LDS</title><p>To gain intuition for EPI, consider a two-dimensional linear dynamical system (2D LDS) model (<xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1A</xref>):<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>ð±</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>â¢</mml:mo><mml:mi>ð±</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>with<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To run EPI with the dynamics matrix elements as the free parameters <inline-formula><mml:math id="inf378"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (fixing <inline-formula><mml:math id="inf379"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> s), the emergent property statistics <inline-formula><mml:math id="inf380"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> were chosen to contain parts of the primary eigenvalue of <inline-formula><mml:math id="inf381"><mml:mi>A</mml:mi></mml:math></inline-formula>, which predicate frequency, <inline-formula><mml:math id="inf382"><mml:mrow><mml:mtext>imag</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and the growth/decay, <inline-formula><mml:math id="inf383"><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, of the system<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mtext>imag</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf384"><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> is the eigenvalue of greatest real part when the imaginary component is zero, and alternatively that of positive imaginary component when the eigenvalues are complex conjugate pairs. To learn the distribution of real entries of <inline-formula><mml:math id="inf385"><mml:mi>A</mml:mi></mml:math></inline-formula> that produce a band of oscillating systems around 1 Hz, we formalized this emergent property as <inline-formula><mml:math id="inf386"><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> having mean zero with variance <inline-formula><mml:math id="inf387"><mml:msup><mml:mn>0.25</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>, and the oscillation frequency <inline-formula><mml:math id="inf388"><mml:mfrac><mml:mrow><mml:mtext>imag</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> having mean 1 Hz with variance 0.1 Hz<sup>2</sup>:<disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="script">ð³</mml:mi></mml:mrow><mml:mo>:</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mtext>real</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>imag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mi mathvariant="bold-italic">Î¼</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mtext>real</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>imag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msup><mml:mn>0.25</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>Ï</mml:mi><mml:mn>5</mml:mn></mml:mfrac><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To write the emergent property <inline-formula><mml:math id="inf389"><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:math></inline-formula> in the form required for the augmented lagrangian optimization (Section 'Augmented lagrangian optimization'), we concatenate these first and second moment constraints into a vector of sufficient statistics <inline-formula><mml:math id="inf390"><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and constraint values <inline-formula><mml:math id="inf391"><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula>.<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mi>ð³</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="9.1pt">]</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="9.1pt">â</mml:mo><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mi>ð³</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð±</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi>ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð±</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi>ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>imag</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð±</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi>ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð±</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi>ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mtext>imag</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mpadded width="+6.6pt"><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mn>0.25</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>Ï</mml:mi><mml:mn>5</mml:mn></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mpadded><mml:mo rspace="9.1pt">â</mml:mo><mml:msub><mml:mi>ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>From now on in all scientific applications (Sections 'Stomatogastric ganglion',Â 'Scaling EPI for stable amplification in RNNs',Â 'Primary visual cortex',Â 'Superior colliculus'), we specify how the EPI optimization was setup by specifying <inline-formula><mml:math id="inf392"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf393"><mml:mi mathvariant="bold-italic">ð</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf394"><mml:msup><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>.</p><p>Unlike the models we presented in the main text, this model admits an analytical form for the mean emergent property statistics given parameter <inline-formula><mml:math id="inf395"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula>, since the eigenvalues can be calculated using the quadratic formula:<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:mrow><mml:mi>Î»</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>Ï</mml:mi></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>Â±</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>Ï</mml:mi></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mi>Ï</mml:mi></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msqrt></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We study this example, because the inferred distribution is curved and multimodal, and we can compare the result of EPI to analytically derived contours of the emergent property statistics.</p><p>Despite the simple analytic form of the emergent property statistics, the EPI distribution in this example is not simply determined. Although <inline-formula><mml:math id="inf396"><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mi mathvariant="bold">ð³</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is calculable directly via a closed form function, the distribution <inline-formula><mml:math id="inf397"><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> cannot be derived directly. This fact is due to the formally hard problem of the backward mapping: finding the natural parameters <inline-formula><mml:math id="inf398"><mml:mi mathvariant="bold-italic">ð¼</mml:mi></mml:math></inline-formula> from the mean parameters <inline-formula><mml:math id="inf399"><mml:mi mathvariant="bold-italic">ð</mml:mi></mml:math></inline-formula> of an exponential family distribution (<xref ref-type="bibr" rid="bib95">Wainwright and Jordan, 2008</xref>). Instead, we used EPI to approximate this distribution (<xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1B</xref>). We used a real NVP normalizing flow architecture three coupling layers and two-layer neural networks of 50 units per layer, mapped onto a support of <inline-formula><mml:math id="inf400"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (see Section 'Deep probability distributions and normalizing flows').</p><p>Even this relatively simple system has nontrivial (although intuitively sensible) structure in the parameter distribution. To validate our method, we analytically derived the contours of the probability density from the emergent property statistics and values. In the <inline-formula><mml:math id="inf401"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>-<inline-formula><mml:math id="inf402"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> plane, the black line at <inline-formula><mml:math id="inf403"><mml:mrow><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, dashed black line at the standard deviation <inline-formula><mml:math id="inf404"><mml:mrow><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>Â±</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and the dashed gray line at twice the standard deviation <inline-formula><mml:math id="inf405"><mml:mrow><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>Â±</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> follow the contour of probability density of the samples (<xref ref-type="fig" rid="fig1s2">Figure 1âfigure supplement 2A</xref>). The distribution precisely reflects the desired statistical constraints and model degeneracy in the sum of <inline-formula><mml:math id="inf406"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf407"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. Intuitively, the parameters equivalent with respect to emergent property statistic <inline-formula><mml:math id="inf408"><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> have similar log densities.</p><p>To explain the bimodality of the EPI distribution, we examined the imaginary component of <inline-formula><mml:math id="inf409"><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>. When <inline-formula><mml:math id="inf410"><mml:mrow><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (which is the case on average in <inline-formula><mml:math id="inf411"><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:math></inline-formula>), we haveÂ <disp-formula id="equ32"><label>(32)</label><mml:math id="m32"><mml:mrow><mml:mrow><mml:mrow><mml:mtext>imag</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msqrt><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mi>Ï</mml:mi></mml:mfrac></mml:mstyle></mml:msqrt><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>â¢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&lt;</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mtext>otherwise </mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In <xref ref-type="fig" rid="fig1s2">Figure 1âfigure supplement 2B</xref>, we plot the contours of <inline-formula><mml:math id="inf412"><mml:mrow><mml:mtext>imag</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf413"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is fixed to 0 at one standard deviation (<inline-formula><mml:math id="inf414"><mml:mfrac><mml:mi>Ï</mml:mi><mml:mn>5</mml:mn></mml:mfrac></mml:math></inline-formula>, black dashed) and two standard deviations (<inline-formula><mml:math id="inf415"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mn>5</mml:mn></mml:mfrac></mml:math></inline-formula>, gray dashed) from the mean of <inline-formula><mml:math id="inf416"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:math></inline-formula>. This validates the curved multimodal structure of the inferred distribution learned through EPI. Subtler combinations of model and emergent property will have more complexity, further motivating the use of EPI for understanding these systems. As we expect, the distribution results in samples of two-dimensional linear systems oscillating near 1 Hz (<xref ref-type="fig" rid="fig1s3">Figure 1âfigure supplement 3</xref>).</p></sec><sec id="s4-1-6"><title>EPI as variational inference</title><p>In variational inference, a posterior approximation <inline-formula><mml:math id="inf417"><mml:msubsup><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> is chosen from within some variational family <inline-formula><mml:math id="inf418"><mml:mi class="ltx_font_mathcaligraphic">ð¬</mml:mi></mml:math></inline-formula> to be as close as possible to the posterior under the KL divergence criteria<disp-formula id="equ33"><label>(33)</label><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="script">ð¬</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>â£â£</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>â£</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This KL divergence can be written in terms of entropy of the variational approximation:<disp-formula id="equ34"><label>(34)</label><mml:math id="m34"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â£</mml:mo><mml:mo>â£</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo>â£</mml:mo><mml:mi>ð±</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð³</mml:mi><mml:mo>â¼</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð³</mml:mi><mml:mo>â¼</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo>â£</mml:mo><mml:mi>ð±</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ35"><label>(35)</label><mml:math id="m35"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð³</mml:mi><mml:mo>â¼</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>ð½</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi>ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Since the marginal distribution of the data <inline-formula><mml:math id="inf419"><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (or âevidenceâ) is independent of <inline-formula><mml:math id="inf420"><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:math></inline-formula>, variational inference is executed by optimizing the remaining expression. This is usually framed as maximizing the evidence lower bound (ELBO)<disp-formula id="equ36"><label>(36)</label><mml:math id="m36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="script">ð¬</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mo>â£â£</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>â£</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="script">ð¬</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>â¼</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>â£</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Now, we will show how the maximum entropy problem of EPI is equivalent to variational inference. In general, a maximum entropy problem (as in <xref ref-type="disp-formula" rid="equ16">Equation 16</xref>) has an equivalent lagrange dual form:Â <disp-formula id="equ37"><label>(37)</label><mml:math id="m37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="script">ð¬</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mtext>Â </mml:mtext><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">âº</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="script">ð¬</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mtext>Â </mml:mtext><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Î·</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi mathvariant="normal">â¤</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>â¼</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>s.t.Â </mml:mtext><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>â¼</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mstyle></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>with lagrange multipliers <inline-formula><mml:math id="inf421"><mml:msup><mml:mi mathvariant="bold-italic">ð¼</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>. By moving the lagrange multipliers within the expectation<disp-formula id="equ38"><label>(38)</label><mml:math id="m38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>q</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="script">ð¬</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>â¼</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">Î·</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi mathvariant="normal">â¤</mml:mi></mml:mrow></mml:msup><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>inserting a <inline-formula><mml:math id="inf422"><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>â</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> within the expectation,<disp-formula id="equ39"><label>(39)</label><mml:math id="m39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>q</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="script">ð¬</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>â¼</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">Î·</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi mathvariant="normal">â¤</mml:mi></mml:mrow></mml:msup><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>and finally choosing <inline-formula><mml:math id="inf423"><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>â</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to be likelihood averaged statistics as in EPI<disp-formula id="equ40"><label>(40)</label><mml:math id="m40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>q</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="script">ð¬</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>â¼</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">Î·</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi mathvariant="normal">â¤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>â¼</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>â£</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>â¼</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>â£</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>we can compare directly to the objective used in variational inference (<xref ref-type="disp-formula" rid="equ36">Equation 36</xref>). We see that EPI is exactly variational inference with an exponential family likelihood defined by sufficient statistics <inline-formula><mml:math id="inf424"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and where the natural parameter <inline-formula><mml:math id="inf425"><mml:msup><mml:mi mathvariant="bold-italic">ð¼</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> is predicated by the mean parameter <inline-formula><mml:math id="inf426"><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula>. <xref ref-type="disp-formula" rid="equ40">Equation 40</xref> implies that EPI uses an improper (or uniform) prior, which is easily changed.</p><p>This derivation of the equivalence between EPI and variational inference emphasizes why defining a statistical inference program by its mean parameterization <inline-formula><mml:math id="inf427"><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula> is so useful. With EPI, one can clearly define the emergent property <inline-formula><mml:math id="inf428"><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:math></inline-formula> that the model of interest should produce through intuitive selection of <inline-formula><mml:math id="inf429"><mml:msub><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mtext>opt</mml:mtext></mml:msub></mml:math></inline-formula> for a given <inline-formula><mml:math id="inf430"><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Alternatively, figuring out the correct natural parameters <inline-formula><mml:math id="inf431"><mml:msup><mml:mi mathvariant="bold-italic">ð¼</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> for the same <inline-formula><mml:math id="inf432"><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that produces <inline-formula><mml:math id="inf433"><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:math></inline-formula> is a formally hard problem.</p></sec></sec><sec id="s4-2"><title>Stomatogastric ganglion</title><p>In Section 'Motivating emergent property inference of theoretical models' and 'Emergent property inference via deep generative models', we used EPI to infer conductance parameters in a model of the stomatogastric ganglion (STG) (<xref ref-type="bibr" rid="bib34">Gutierrez et al., 2013</xref>). This five-neuron circuit model represents two subcircuits: that generating the pyloric rhythm (fast population) and that generating the gastric mill rhythm (slow population). The additional neuron (the IC neuron of the STG) receives inhibitory synaptic input from both subcircuits, and can couple to either rhythm dependent on modulatory conditions. There is also a parametric regime in which this neuron fires at an intermediate frequency between that of the fast and slow populations (<xref ref-type="bibr" rid="bib34">Gutierrez et al., 2013</xref>), which we infer with EPI as a motivational example. This model is not to be confused with an STG subcircuit model of the pyloric rhythm (<xref ref-type="bibr" rid="bib55">Marder and Selverston, 1992</xref>), which has been statistically inferred in other studies (<xref ref-type="bibr" rid="bib78">Prinz et al., 2004</xref>; <xref ref-type="bibr" rid="bib31">GonÃ§alves et al., 2019</xref>).</p><sec id="s4-2-1"><title>STG model</title><p>We analyze how the parameters <inline-formula><mml:math id="inf434"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mtext>el</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mtext>synA</mml:mtext></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> govern the emergent phenomena of intermediate hub frequency in a model of the stomatogastric ganglion (STG) (<xref ref-type="bibr" rid="bib34">Gutierrez et al., 2013</xref>) shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref> with activity <inline-formula><mml:math id="inf435"><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mtext>f1</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mtext>f2</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mtext>hub</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mtext>s1</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mtext>s2</mml:mtext></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, using the same hyperparameter choices as Gutierrez et al. Each neuronâs membrane potential <inline-formula><mml:math id="inf436"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf437"><mml:mrow><mml:mi>Î±</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mtext>f1</mml:mtext><mml:mo>,</mml:mo><mml:mtext>f2</mml:mtext><mml:mo>,</mml:mo><mml:mtext>hub</mml:mtext><mml:mo>,</mml:mo><mml:mtext>s1</mml:mtext><mml:mo>,</mml:mo><mml:mtext>s2</mml:mtext><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the solution of the following stochastic differential equation:<disp-formula id="equ41"><label>(41)</label><mml:math id="m41"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The input current of each neuron is the sum of the leak, calcium, potassium, hyperpolarization, electrical and synaptic currents. Each current component is a function of all membrane potentials and the conductance parameters <inline-formula><mml:math id="inf438"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula>. Finally, we include gaussian noise <inline-formula><mml:math id="inf439"><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula> to the model of Gutierrez et al. so that the model stochastic, although this is not required by EPI.</p><p>The capacitance of the cell membrane was set to <inline-formula><mml:math id="inf440"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Specifically, the currents are the difference in the neuronâs membrane potential and that current typeâs reversal potential multiplied by a conductance:Â <disp-formula id="equ42"><label>(42)</label><mml:math id="m42"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ43"><label>(43)</label><mml:math id="m43"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mtext>el</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mi>r</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ44"><label>(44)</label><mml:math id="m44"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi mathvariant="normal">â</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mi>r</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ45"><label>(45)</label><mml:math id="m45"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi mathvariant="normal">â</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ46"><label>(46)</label><mml:math id="m46"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mi>N</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ47"><label>(47)</label><mml:math id="m47"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mi>H</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The reversal potentials were set to <inline-formula><mml:math id="inf441"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>40</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf442"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>100</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf443"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>80</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf444"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf445"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>75</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The other conductance parameters were fixed to <inline-formula><mml:math id="inf446"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>â</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mi>Î¼</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. <inline-formula><mml:math id="inf447"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf448"><mml:msub><mml:mi>g</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf449"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> had different values based on fast, intermediate (hub) or slow neuron. The fast conductances had values <inline-formula><mml:math id="inf450"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1.9</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf451"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>3.9</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf452"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>2.5</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The intermediate conductances had values <inline-formula><mml:math id="inf453"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1.7</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf454"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1.9</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf455"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>8.0</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Finally, the slow conductances had values <inline-formula><mml:math id="inf456"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>8.5</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf457"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1.5</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf458"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1.0</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Furthermore, the Calcium, Potassium, and hyperpolarization channels have time-dependent gating dynamics dependent on steady-state gating variables <inline-formula><mml:math id="inf459"><mml:msub><mml:mi>M</mml:mi><mml:mi mathvariant="normal">â</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf460"><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">â</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf461"><mml:msub><mml:mi>H</mml:mi><mml:mi mathvariant="normal">â</mml:mi></mml:msub></mml:math></inline-formula>, respectively:<disp-formula id="equ48"><label>(48)</label><mml:math id="m48"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi mathvariant="normal">â</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>tanh</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ49"><label>(49)</label><mml:math id="m49"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">â</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ50"><label>(50)</label><mml:math id="m50"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">â</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>tanh</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ51"><label>(51)</label><mml:math id="m51"><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mi>cosh</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ52"><label>(52)</label><mml:math id="m52"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi mathvariant="normal">â</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ53"><label>(53)</label><mml:math id="m53"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi mathvariant="normal">â</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ54"><label>(54)</label><mml:math id="m54"><mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>272</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>-</mml:mo><mml:mn>1499</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>7</mml:mn></mml:msub></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>8</mml:mn></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>where we set <inline-formula><mml:math id="inf462"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf463"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf464"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf465"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>15</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf466"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>78.3</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf467"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>10.5</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf468"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>7</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>42.2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf469"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>8</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>87.3</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf470"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>9</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf471"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>25</mml:mn><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Finally, there is a synaptic gating variable as well:<disp-formula id="equ55"><label>(55)</label><mml:math id="m55"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi mathvariant="normal">â</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>9</mml:mn></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>When the dynamic gating variables are considered, this is actually a 15-dimensional nonlinear dynamical system. The gaussian noise <inline-formula><mml:math id="inf472"><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi mathvariant="bold">ð</mml:mi></mml:mrow></mml:math></inline-formula> has variance <inline-formula><mml:math id="inf473"><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> A<sup>2</sup>, and introduces variability in frequency at each parameterization <inline-formula><mml:math id="inf474"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s4-2-2"><title>Hub frequency calculation</title><p>In order to measure the frequency of the hub neuron during EPI, the STG model was simulated for <inline-formula><mml:math id="inf475"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:mrow></mml:math></inline-formula> time steps of <inline-formula><mml:math id="inf476"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>25</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>ms</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>. The chosen <inline-formula><mml:math id="inf477"><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf478"><mml:mi>T</mml:mi></mml:math></inline-formula> were the most computationally convenient choices yielding accurate frequency measurement. We used a basis of complex exponentials with frequencies from 0.0 to 1.0 Hz at 0.01 Hz resolution to measure frequency from simulated time series<disp-formula id="equ56"><label>(56)</label><mml:math id="m56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mn>1.0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â¤</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To measure spiking frequency, we processed simulated membrane potentials with a relu (spike extraction) and low-pass filter with averaging window of size 20, then took the frequency with the maximum absolute value of the complex exponential basis coefficients of the processed time-series. The first 20 temporal samples of the simulation are ignored to account for initial transients.</p><p>To differentiate through the maximum frequency identification, we used a soft-argmax Let <inline-formula><mml:math id="inf479"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> be the complex exponential filter bank dot products with the signal <inline-formula><mml:math id="inf480"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mi>â</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf481"><mml:mrow><mml:mi>Î±</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mtext>f1</mml:mtext><mml:mo>,</mml:mo><mml:mtext>f2</mml:mtext><mml:mo>,</mml:mo><mml:mtext>hub</mml:mtext><mml:mo>,</mml:mo><mml:mtext>s1</mml:mtext><mml:mo>,</mml:mo><mml:mtext>s2</mml:mtext><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The soft-argmax is then calculated using temperature parameter <inline-formula><mml:math id="inf482"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula><disp-formula id="equ57"><label>(57)</label><mml:math id="m57"><mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mtext>softmax</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo>â</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf483"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¦</mml:mi><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The frequency is then calculated as<disp-formula id="equ58"><label>(58)</label><mml:math id="m58"><mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.01</mml:mn><mml:mo>â¢</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mtext>Hz</mml:mtext></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Intermediate hub frequency, like all other emergent properties in this work, is defined by the mean and variance of the emergent property statistics. In this case, we have one statistic, hub neuron frequency, where the mean was chosen to be 0.55 Hz,(<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) and variance was chosen to be 0.025<sup>2</sup> Hz<sup>2</sup> (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>).</p></sec><sec id="s4-2-3"><title>EPI details for the STG model</title><p>EPI was run for the STG model usingÂ <disp-formula id="equ59"><label>(59)</label><mml:math id="m59"><mml:mrow><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mtext>hub</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ60"><label>(60)</label><mml:math id="m60"><mml:mrow><mml:mrow><mml:mi>ð</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="center"><mml:mn>0.55</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and<disp-formula id="equ61"><label>(61)</label><mml:math id="m61"><mml:mrow><mml:msup><mml:mi>ð</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mn>0.025</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>(see Sections 'Maximum entropy distributions and exponential families',Â 'Augmented lagrangian optimization', and example in Section 'Example: 2D LDS'). Throughout optimization, the augmented lagrangian parameters Î· and <inline-formula><mml:math id="inf484"><mml:mi>c</mml:mi></mml:math></inline-formula>, were updated after each epoch of <inline-formula><mml:math id="inf485"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> iterations (see Section 'Augmented lagrangian optimization'). The optimization converged after five epochs (<xref ref-type="fig" rid="fig1s4">Figure 1âfigure supplement 4</xref>).</p><p>For EPI in <xref ref-type="fig" rid="fig1">Figure 1E</xref>, we used a real NVP architecture with three coupling layers and two-layer neural networks of 25 units per layer. The normalizing flow architecture mapped <inline-formula><mml:math id="inf486"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð©</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn/><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to a support of <inline-formula><mml:math id="inf487"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mtext>el</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mtext>synA</mml:mtext></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>Ã</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, initialized to a gaussian approximation of samples returned by a preliminary ABC search. We did not include <inline-formula><mml:math id="inf488"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mtext>synA</mml:mtext></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>, for numerical stability. EPI optimization was run with an augmented lagrangian coefficient of <inline-formula><mml:math id="inf489"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, hyperparameter <inline-formula><mml:math id="inf490"><mml:mrow><mml:mi>Î²</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, a batch size <inline-formula><mml:math id="inf491"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>400</mml:mn></mml:mrow></mml:math></inline-formula>, and we simulated one <inline-formula><mml:math id="inf492"><mml:msup><mml:mi mathvariant="bold">ð±</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> per <inline-formula><mml:math id="inf493"><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>. The architecture converged with criteria <inline-formula><mml:math id="inf494"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>test</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-2-4"><title>Hessian sensitivity vectors</title><p>To quantify the second-order structure of the EPI distribution, we evaluated the Hessian of the log probability <inline-formula><mml:math id="inf495"><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mo>â</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>â¡</mml:mo><mml:mi>log</mml:mi></mml:mrow><mml:mo>â¡</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mo>â¡</mml:mo><mml:msup><mml:mi mathvariant="bold">ð³ð³</mml:mi><mml:mo>â¤</mml:mo></mml:msup></mml:mrow></mml:mfrac></mml:math></inline-formula>. The eigenvector of this Hessian with most negative eigenvalue is defined as the sensitivity dimension <inline-formula><mml:math id="inf496"><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, and all subsequent eigenvectors are ordered by increasing eigenvalue. These eigenvalues are quantifications of how fast the emergent property deteriorates via the parameter combination of their associated eigenvector. In <xref ref-type="fig" rid="fig1">Figure 1D</xref>, the sensitivity dimension <italic>v</italic><sub>1</sub> (solid) and the second eigenvector of the Hessian <italic>v</italic><sub>2</sub> (dashed) are shown evaluated at the mode of the distribution. Since the Hessian eigenvectors have sign degeneracy, the visualized directions in 2-D parameter space were chosen to have positive <inline-formula><mml:math id="inf497"><mml:msub><mml:mi>g</mml:mi><mml:mtext>synA</mml:mtext></mml:msub></mml:math></inline-formula>. The length of the arrows is inversely proportional to the square root of the absolute value of their eigenvalues <inline-formula><mml:math id="inf498"><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>10.7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf499"><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>3.22</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. For the same magnitude perturbation away from the mode, intermediate hub frequency only diminishes along the sensitivity dimension <inline-formula><mml:math id="inf500"><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1EâF</xref>).</p></sec></sec><sec id="s4-3"><title>Scaling EPI for stable amplification in RNNs</title><sec id="s4-3-1"><title>Rank-2 RNN model</title><p>We examined the scaling properties of EPI by learning connectivities of RNNs of increasing size that exhibit stable amplification. Rank-2 RNN connectivity was modeled as <inline-formula><mml:math id="inf501"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mo>â¤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf502"><mml:mrow><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mi>Ï</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf503"><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mi>Ï</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf504"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>â¼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð©</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. This RNN model has dynamics<disp-formula id="equ62"><label>(62)</label><mml:math id="m62"><mml:mrow><mml:mrow><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mover accent="true"><mml:mi>ð±</mml:mi><mml:mo>Ë</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>ð±</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mo>â¢</mml:mo><mml:mi>ð±</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In this analysis, we inferred connectivity parameterizations <inline-formula><mml:math id="inf505"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>1</mml:mn><mml:mo>â¤</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>2</mml:mn><mml:mo>â¤</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>1</mml:mn><mml:mo>â¤</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">ð</mml:mi><mml:mn>2</mml:mn><mml:mo>â¤</mml:mo></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>â¢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> that produced stable amplification using EPI, SMC-ABC (<xref ref-type="bibr" rid="bib88">Sisson et al., 2007</xref>), and SNPE (<xref ref-type="bibr" rid="bib31">GonÃ§alves et al., 2019</xref>) (see Section Related methods).</p></sec><sec id="s4-3-2"><title>Stable amplification</title><p>For this RNN model to be stable, all real eigenvalues of <inline-formula><mml:math id="inf506"><mml:mi>W</mml:mi></mml:math></inline-formula> must be less than 1: <inline-formula><mml:math id="inf507"><mml:mrow><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf508"><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> denotes the greatest real eigenvalue of <inline-formula><mml:math id="inf509"><mml:mi>W</mml:mi></mml:math></inline-formula>. For a stable RNN to amplify at least one input pattern, the symmetric connectivity <inline-formula><mml:math id="inf510"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>W</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mo>â¤</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula> must have an eigenvalue greater than 1: <inline-formula><mml:math id="inf511"><mml:mrow><mml:msubsup><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf512"><mml:msup><mml:mi>Î»</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:math></inline-formula> is the maximum eigenvalue of <inline-formula><mml:math id="inf513"><mml:msup><mml:mi>W</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:math></inline-formula>. These two conditions are necessary and sufficient for stable amplification in RNNs (<xref ref-type="bibr" rid="bib12">Bondanelli and Ostojic, 2020</xref>).</p></sec><sec id="s4-3-3"><title>EPI details for RNNs</title><p>We defined the emergent property of stable amplification with means of these eigenvalues (0.5 and 1.5, respectively) that satisfy these conditions. To complete the emergent property definition, we chose variances (<inline-formula><mml:math id="inf514"><mml:msup><mml:mn>0.25</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>) about those means such that samples rarely violate the eigenvalue constraints. To write the emergent property of <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> in terms of the EPI optimization, we have<disp-formula id="equ63"><label>(63)</label><mml:math id="m63"><mml:mrow><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msubsup><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi></mml:msubsup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ64"><label>(64)</label><mml:math id="m64"><mml:mrow><mml:mrow><mml:mi>ð</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>0.5</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>1.5</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and<disp-formula id="equ65"><label>(65)</label><mml:math id="m65"><mml:mrow><mml:msup><mml:mi>ð</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mn>0.25</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mn>0.25</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>(see Sections 'Maximum entropy distributions and exponential families',Â 'Augmented lagrangian optimization', and example in Section 'Example: 2D LDS'). Gradients of maximum eigenvalues of Hermitian matrices like <inline-formula><mml:math id="inf515"><mml:msup><mml:mi>W</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:math></inline-formula> are available with modern automatic differentiation tools. To differentiate through the <inline-formula><mml:math id="inf516"><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we solved the following equation for eigenvalues of rank-2 matrices using the rank reduced matrix <inline-formula><mml:math id="inf517"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mo>â¤</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula><disp-formula id="equ66"><label>(66)</label><mml:math id="m66"><mml:mrow><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mo>Â±</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>Â±</mml:mo><mml:msqrt><mml:mrow><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo>â¢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>Det</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msqrt></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For EPI in <xref ref-type="fig" rid="fig2">Figure 2</xref>, we used a real NVP architecture with three coupling layers of affine transformations parameterized by two-layer neural networks of 100 units per layer. The initial distribution was a standard isotropic gaussian <inline-formula><mml:math id="inf518"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð©</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn/><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> mapped to the support of <inline-formula><mml:math id="inf519"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We used an augmented lagrangian coefficient of <inline-formula><mml:math id="inf520"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, a batch size <inline-formula><mml:math id="inf521"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf522"><mml:mrow><mml:mi>Î²</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>, and we simulated one <inline-formula><mml:math id="inf523"><mml:msup><mml:mi mathvariant="bold">ð</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> per <inline-formula><mml:math id="inf524"><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>. We chose to use <inline-formula><mml:math id="inf525"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> iterations per augmented lagrangian epoch and emergent property constraint convergence was evaluated at <inline-formula><mml:math id="inf526"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>test</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2B</xref> blue line, and <xref ref-type="fig" rid="fig2">Figure 2CâD</xref> blue). It was fastest to initialize the EPI distribution on a Tesla V100 GPU, and then subsequently optimize it on a CPU with 32 cores. EPI timing measurements accounted for this initialization period.</p></sec><sec id="s4-3-4"><title>Methodological comparison</title><p>We compared EPI to two alternative simulation-based inference techniques, since the likelihood of these eigenvalues given <inline-formula><mml:math id="inf527"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> is not available. Approximate bayesian computation (ABC) (<xref ref-type="bibr" rid="bib5">Beaumont et al., 2002</xref>) is a rejection sampling technique for obtaining sets of parameters <inline-formula><mml:math id="inf528"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> that produce activity <inline-formula><mml:math id="inf529"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula> close to some observed data <inline-formula><mml:math id="inf530"><mml:msub><mml:mi mathvariant="bold">ð±</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>. Sequential Monte Carlo approximate bayesian computation (SMC-ABC) is the state-of-the-art ABC method, which leverages SMC techniques to improve sampling speed. We ran SMC-ABC with the pyABC package (<xref ref-type="bibr" rid="bib46">Klinger et al., 2018</xref>) to infer RNNs with stable amplification: connectivities having eigenvalues within an <inline-formula><mml:math id="inf531"><mml:mi>Ïµ</mml:mi></mml:math></inline-formula>-defined <inline-formula><mml:math id="inf532"><mml:mi>l</mml:mi></mml:math></inline-formula>â2 distance of<disp-formula id="equ67"><label>(67)</label><mml:math id="m67"><mml:mrow><mml:mrow><mml:msub><mml:mi>ð±</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msubsup><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>0.5</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>1.5</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>SMC-ABC was run with a uniform prior over <inline-formula><mml:math id="inf533"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>â¢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a population size of 1000 particles with simulations parallelized over 32 cores, and a multivariate normal transition model.</p><p>SNPE, the next approach in our comparison, is far more similar to EPI. Like EPI, SNPE treats parameters in mechanistic models with deep probability distributions, yet the two learning algorithms are categorically different. SNPE uses a two-network architecture to approximate the posterior distribution of the model conditioned on observed data <inline-formula><mml:math id="inf534"><mml:msub><mml:mi mathvariant="bold">ð±</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>. The amortizing network maps observations <inline-formula><mml:math id="inf535"><mml:msub><mml:mi mathvariant="bold">ð±</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> to the parameters of the deep probability distribution. The weights and biases of the parameter network are optimized by sequentially augmenting the training data with additional pairs (<inline-formula><mml:math id="inf536"><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf537"><mml:msub><mml:mi mathvariant="bold">ð±</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>) based on the most recent posterior approximation. This sequential procedure is important to get training data <inline-formula><mml:math id="inf538"><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> to be closer to the true posterior, and <inline-formula><mml:math id="inf539"><mml:msub><mml:mi mathvariant="bold">ð±</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> to be closer to the observed data. For the deep probability distribution architecture, we chose a masked autoregressive flow with affine couplings (the default choice), three transforms, 50 hidden units, and a normalizing flow mapping to the support as in EPI. This architectural choice closely tracked the size of the architecture used by EPI (<xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>). As in SMC-ABC, we ran SNPE with <inline-formula><mml:math id="inf540"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð±</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>Î¼</mml:mi></mml:mrow></mml:math></inline-formula>. All SNPE optimizations were run for a limit of 1.5 days, or until two consecutive rounds resulted in a validation log probability lower than the maximum observed for that random seed. It was always faster to run SNPE on a CPU with 32 cores rather than on a Tesla V100 GPU.</p><p>To compare the efficiency of these algorithms for inferring RNN connectivity distributions producing stable amplification, we develop a convergence criteria that can be used across methods. While EPI has its own hypothesis testing convergence criteria for the emergent property, it would not make sense to use this criteria on SNPE and SMC-ABC which do not constrain the means and variances of their predictions. Instead, we consider EPI and SNPE to have converged after completing its most recent optimization epoch (EPI) or round (SNPE) in which the distance <inline-formula><mml:math id="inf541"><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="bold-italic">ð</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is less than 0.5. We consider SMC-ABC to have converged once the population produces samples within the <inline-formula><mml:math id="inf542"><mml:mrow><mml:mi>Ïµ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> ball ensuring stable amplification.</p><p>When assessing the scalability of SNPE, it is important to check that alternative hyperparamterizations could not yield better performance. Key hyperparameters of the SNPE optimization are the number of simulations per round <inline-formula><mml:math id="inf543"><mml:msub><mml:mi>n</mml:mi><mml:mtext>round</mml:mtext></mml:msub></mml:math></inline-formula>, the number of atoms used in the atomic proposals of the SNPE-C algorithm (<xref ref-type="bibr" rid="bib32">Greenberg, 2019</xref>), and the batch size <inline-formula><mml:math id="inf544"><mml:mi>n</mml:mi></mml:math></inline-formula>. To match EPI, we used a batch size of <inline-formula><mml:math id="inf545"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf546"><mml:mrow><mml:mi>N</mml:mi><mml:mo>â¤</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula>, however we found <inline-formula><mml:math id="inf547"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to be helpful for SNPE in higher dimensions. While <inline-formula><mml:math id="inf548"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>round</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> yielded SNPE convergence for <inline-formula><mml:math id="inf549"><mml:mrow><mml:mi>N</mml:mi><mml:mo>â¤</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula>, we found that a substantial increase to <inline-formula><mml:math id="inf550"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>round</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>25</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> yielded more consistent convergence at <inline-formula><mml:math id="inf551"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2A</xref>). By increasing <inline-formula><mml:math id="inf552"><mml:msub><mml:mi>n</mml:mi><mml:mtext>round</mml:mtext></mml:msub></mml:math></inline-formula>, we also necessarily increase the duration of each round. At <inline-formula><mml:math id="inf553"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>, we tried two hyperparameter modifications. As suggested in <xref ref-type="bibr" rid="bib32">Greenberg, 2019</xref>, we increased <inline-formula><mml:math id="inf554"><mml:msub><mml:mi>n</mml:mi><mml:mtext>atom</mml:mtext></mml:msub></mml:math></inline-formula> by an order of magnitude to improve gradient quality, but this had little effect on the optimization (much overlap between same random seeds) (<xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2B</xref>). Finally, we increased <inline-formula><mml:math id="inf555"><mml:msub><mml:mi>n</mml:mi><mml:mtext>round</mml:mtext></mml:msub></mml:math></inline-formula> by an order of magnitude, which yielded convergence in one case, but no others. We found no way to improve the convergence rate of SNPE without making more aggressive hyperparameter choices requiring high numbers of simulations. In <xref ref-type="fig" rid="fig2">Figure 2CâD</xref>, we show samples from the random seed resulting in emergent property convergence at greatest entropy (EPI), the random seed resulting in greatest validation log probability (SNPE), and the result of all converged random seeds (SMC).</p></sec><sec id="s4-3-5"><title>Effect of RNN parameters on EPI and SNPE inferred distributions</title><p>To clarify the difference in objectives of EPI and SNPE, we show their results on RNN models with different numbers of neurons <inline-formula><mml:math id="inf556"><mml:mi>N</mml:mi></mml:math></inline-formula> and random strength <inline-formula><mml:math id="inf557"><mml:mi>g</mml:mi></mml:math></inline-formula>. The parameters inferred by EPI consistently produces the same mean and variance of <inline-formula><mml:math id="inf558"><mml:mrow><mml:mtext>real</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf559"><mml:msubsup><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi></mml:msubsup></mml:math></inline-formula>, while those inferred by SNPE change according to the model definition (<xref ref-type="fig" rid="fig2s3">Figure 2âfigure supplement 3A</xref>). For <inline-formula><mml:math id="inf560"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf561"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>, the SNPE posterior has greater concentration in eigenvalues around <inline-formula><mml:math id="inf562"><mml:msub><mml:mi mathvariant="bold">ð±</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> than at <inline-formula><mml:math id="inf563"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, where the model has greater randomness (<xref ref-type="fig" rid="fig2s3">Figure 2âfigure supplement 3B</xref> top, orange). At both levels of <inline-formula><mml:math id="inf564"><mml:mi>g</mml:mi></mml:math></inline-formula> when <inline-formula><mml:math id="inf565"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, the posterior of SNPE has lower entropy than EPI at convergence (<xref ref-type="fig" rid="fig2s3">Figure 2âfigure supplement 3B</xref> top). However at <inline-formula><mml:math id="inf566"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, SNPE results in a predictive distribution of more widely dispersed eigenvalues (<xref ref-type="fig" rid="fig2s3">Figure 2âfigure supplement 3A</xref> bottom), and an inferred posterior with greater entropy than EPI (<xref ref-type="fig" rid="fig2s3">Figure 2âfigure supplement 3B</xref> bottom). We highlight these differences not to focus on an insightful trend, but to emphasize that these methods optimize different objectives with different implications.</p><p>Note that SNPE converges when itâs validation log probability has saturated after several rounds of optimization (<xref ref-type="fig" rid="fig2s3">Figure 2âfigure supplement 3C</xref>), and that EPI converges after several epochs of its own optimization to enforce the emergent property constraints (<xref ref-type="fig" rid="fig2s3">Figure 2âfigure supplement 3D</xref> blue). Importantly, as SNPE optimizes its posterior approximation, the predictive means change, and at convergence may be different than <inline-formula><mml:math id="inf567"><mml:msub><mml:mi mathvariant="bold">ð±</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2s3">Figure 2âfigure supplement 3D</xref> orange, left). It is sensible to assume that predictions of a well-approximated SNPE posterior should closely reflect the data on average (especially given a uniform prior and a low degree of stochasticity); however, this is not a given. Furthermore, no aspect of the SNPE optimization controls the variance of the predictions (<xref ref-type="fig" rid="fig2s3">Figure 2âfigure supplement 3D</xref> orange, right).</p></sec></sec><sec id="s4-4"><title>Primary visual cortex</title><sec id="s4-4-1"><title>V1 model</title><p>E-I circuit models, rely on the assumption that inhibition can be studied as an indivisible unit, despite ample experimental evidence showing that inhibition is instead composed of distinct elements (<xref ref-type="bibr" rid="bib93">Tremblay et al., 2016</xref>). In particular three types of genetically identified inhibitory cell-types â parvalbumin (P), somatostatin (S), VIP (V) â compose 80% of GABAergic interneurons in V1 (<xref ref-type="bibr" rid="bib58">Markram et al., 2004</xref>; <xref ref-type="bibr" rid="bib83">Rudy et al., 2011</xref>; <xref ref-type="bibr" rid="bib93">Tremblay et al., 2016</xref>), and follow specific connectivity patterns (<xref ref-type="fig" rid="fig3">Figure 3A</xref>; <xref ref-type="bibr" rid="bib76">Pfeffer et al., 2013</xref>), which lead to cell-type-specific computations (<xref ref-type="bibr" rid="bib62">Mossing et al., 2021</xref>; <xref ref-type="bibr" rid="bib71">Palmigiano et al., 2020</xref>). Currently, how the subdivision of inhibitory cell-types, shapes correlated variability by reconfiguring recurrent network dynamics is not understood.</p><p>In the stochastic stabilized supralinear network (<xref ref-type="bibr" rid="bib38">Hennequin et al., 2018</xref>), population rate responses <inline-formula><mml:math id="inf568"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula> to mean input <inline-formula><mml:math id="inf569"><mml:mi mathvariant="bold">ð¡</mml:mi></mml:math></inline-formula>, recurrent input <inline-formula><mml:math id="inf570"><mml:mrow><mml:mi>W</mml:mi><mml:mo>â¢</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow></mml:math></inline-formula> and slow noise <inline-formula><mml:math id="inf571"><mml:mi mathvariant="bold-italic">Ïµ</mml:mi></mml:math></inline-formula> are governed by<disp-formula id="equ68"><label>(68)</label><mml:math id="m68"><mml:mrow><mml:mrow><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>ð±</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>ð±</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>â¢</mml:mo><mml:mi>ð±</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>ð¡</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">Ïµ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>whereÂ <inline-formula><mml:math id="inf572"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>â</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mo>â</mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>,Â and the noise is an Ornstein-Uhlenbeck process <inline-formula><mml:math id="inf573"><mml:mrow><mml:mi mathvariant="bold-italic">Ïµ</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mo>â¢</mml:mo><mml:mi>U</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mtext>noise</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula><disp-formula id="equ69"><label>(69)</label><mml:math id="m69"><mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mtext>noise</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>Ïµ</mml:mi><mml:mi>Î±</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>Ïµ</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mtext>noise</mml:mtext></mml:msub></mml:mrow></mml:msqrt><mml:mo>â¢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>Ï</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>Î±</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math id="inf574"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mtext>noise</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>ms</mml:mtext></mml:mrow><mml:mo>&gt;</mml:mo><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>ms</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>. The noisy process is parameterized as<disp-formula id="equ70"><label>(70)</label><mml:math id="m70"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Ï</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>Î±</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mi>Ï</mml:mi><mml:msub><mml:mi>Ï</mml:mi><mml:mtext>noise</mml:mtext></mml:msub></mml:mfrac></mml:mrow></mml:msqrt></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>so that <inline-formula><mml:math id="inf575"><mml:mi mathvariant="bold-italic">ð</mml:mi></mml:math></inline-formula> parameterizes the variance of the noisy input in the absence of recurrent connectivity (<inline-formula><mml:math id="inf576"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mn/></mml:mrow></mml:math></inline-formula>). As contrast <inline-formula><mml:math id="inf577"><mml:mrow><mml:mi>c</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> increases, input to the E- and P-populations increases relative to a baseline input <inline-formula><mml:math id="inf578"><mml:mrow><mml:mi mathvariant="bold">ð¡</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð¡</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi mathvariant="bold">ð¡</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Connectivity (<inline-formula><mml:math id="inf579"><mml:msub><mml:mi>W</mml:mi><mml:mtext>fit</mml:mtext></mml:msub></mml:math></inline-formula>) and input (<inline-formula><mml:math id="inf580"><mml:msub><mml:mi mathvariant="bold">ð¡</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mtext>fit</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf581"><mml:msub><mml:mi mathvariant="bold">ð¡</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mtext>fit</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>) parameters were fit using the deterministic V1 circuit model (<xref ref-type="bibr" rid="bib71">Palmigiano et al., 2020</xref>)<disp-formula id="equ71"><label>(71)</label><mml:math id="m71"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mtext>fit</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>â¢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>â¢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>â¢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>â¢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>â¢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>â¢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>â¢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>â¢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>2.18</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1.19</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.594</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.229</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>1.66</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.651</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.680</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.242</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>.895</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>5.22</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>1.51</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.761</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>3.34</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>2.31</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.254</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2.52</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ72"><label>(72)</label><mml:math id="m72"><mml:mrow><mml:mrow><mml:msub><mml:mi>ð¡</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mtext>fit</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>.416</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>.429</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>.491</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>.486</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and<disp-formula id="equ73"><label>(73)</label><mml:math id="m73"><mml:mrow><mml:mrow><mml:msub><mml:mi>ð¡</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mtext>fit</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>.359</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>.403</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To obtain rates on a realistic scale (100-fold greater), we map these fitted parameters to an equivalence class<disp-formula id="equ74"><label>(74)</label><mml:math id="m74"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>â¢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>â¢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>â¢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>â¢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>â¢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>â¢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>â¢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>â¢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>â¢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>.218</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.119</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.0594</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.0229</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>.166</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.0651</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.068</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.0242</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>.0895</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>5.22</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>1.51</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.0761</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>.334</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.231</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>.0254</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2.52</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ75"><label>(75)</label><mml:math id="m75"><mml:mrow><mml:mrow><mml:msub><mml:mi>ð¡</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>4.16</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>4.29</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>4.91</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>4.86</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and<disp-formula id="equ76"><label>(76)</label><mml:math id="m76"><mml:mrow><mml:mrow><mml:msub><mml:mi>ð¡</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>3.59</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>4.03</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Circuit responses are simulated using <inline-formula><mml:math id="inf582"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> time steps at <inline-formula><mml:math id="inf583"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>ms</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> from an initial condition drawn from <inline-formula><mml:math id="inf584"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>â¼</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>Hz</mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>25</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>Hz</mml:mtext></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Standard deviation of the E-population <inline-formula><mml:math id="inf585"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is calculated as the square root of the temporal variance from <inline-formula><mml:math id="inf586"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>75</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>ms</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf587"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>100</mml:mn><mml:mo>â¢</mml:mo><mml:mtext>ms</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula><disp-formula id="equ77"><label>(77)</label><mml:math id="m77"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-4-2"><title>EPI details for the V1 model</title><p>To write the emergent properties of <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> in terms of the EPI optimization, we haveÂ <disp-formula id="equ78"><label>(78)</label><mml:math id="m78"><mml:mrow><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ79"><label>(79)</label><mml:math id="m79"><mml:mrow><mml:mi>ð</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="center"><mml:mn>5</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>(or <inline-formula><mml:math id="inf588"><mml:mrow><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mn>10</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>), and<disp-formula id="equ80"><label>(80)</label><mml:math id="m80"><mml:mrow><mml:msup><mml:mi>ð</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>(see Sections 'Maximum entropy distributions and exponential families',Â 'Augmented lagrangian optimization', and example in Section 'Example: 2D LDS').</p><p>For EPI in <xref ref-type="fig" rid="fig3">Figure 3DâE</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref>, we used a real NVP architecture with three coupling layers and two-layer neural networks of 50 units per layer. The normalizing flow architecture mapped <inline-formula><mml:math id="inf589"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð©</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn/><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to a support of <inline-formula><mml:math id="inf590"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. EPI optimization was run using three different random seeds for architecture initialization <inline-formula><mml:math id="inf591"><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:math></inline-formula> with an augmented lagrangian coefficient of <inline-formula><mml:math id="inf592"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf593"><mml:mrow><mml:mi>Î²</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, a batch size <inline-formula><mml:math id="inf594"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>, and simulated 100 trials to calculate average <inline-formula><mml:math id="inf595"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for each <inline-formula><mml:math id="inf596"><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>. We used <inline-formula><mml:math id="inf597"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> iterations per epoch. The distributions shown are those of the architectures converging with criteria <inline-formula><mml:math id="inf598"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>test</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> at greatest entropy across three random seeds. Optimization details are shown in <xref ref-type="fig" rid="fig3s2">Figure 3âfigure supplement 2</xref>. The sums of squares of each pair of parameters are shown for each EPI distribution in <xref ref-type="fig" rid="fig3s3">Figure 3âfigure supplement 3</xref>. The plots are histograms of 500 samples from each EPI distribution from which the significance p-values of Section 'EPI reveals how recurrence with multiple inhibitory subtypes governs excitatory variability in a V1 model' are determined.</p></sec><sec id="s4-4-3"><title>Sensitivity analyses</title><p>In <xref ref-type="fig" rid="fig3">Figure 3E</xref>, we visualize the modes of <inline-formula><mml:math id="inf599"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> throughout the <inline-formula><mml:math id="inf600"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula>-<inline-formula><mml:math id="inf601"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> marginal. At each local mode <inline-formula><mml:math id="inf602"><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf603"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> is fixed, we calculated the Hessian and visualized the sensitivity dimension in the direction of positive <inline-formula><mml:math id="inf604"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula>.</p></sec><sec id="s4-4-4"><title>Testing for the paradoxical effect</title><p>The paradoxical effect occurs when a populations steady state rate is decreased (or increased) when an increase (decrease) in current is applied to that population (<xref ref-type="bibr" rid="bib94">Tsodyks et al., 1997</xref>). To see which, if any, populations exhibited a paradoxical effect, we examined responses to changes in input to individual neuron-type populations, where the initial condition was the steadyÂ state response to <inline-formula><mml:math id="inf605"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3s4">Figure 3âfigure supplement 4</xref>). Input magnitudes were chosen so that the effect is salient (0.002 for E and P, but 0.02 for S and V). Only the P-population exhibited the paradoxical effect at this connectivity <inline-formula><mml:math id="inf606"><mml:mi>W</mml:mi></mml:math></inline-formula> and input <inline-formula><mml:math id="inf607"><mml:mi mathvariant="bold">ð¡</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s4-4-5"><title>Primary visual cortex: Mathematical intuition and challenges</title><p>We write the original <xref ref-type="disp-formula" rid="equ68 equ69">Equations 68 and 69</xref> in the following way:<disp-formula id="equ81"><label>(81)</label><mml:math id="m81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Ï</mml:mi></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mo>â</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mi>Ïµ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>d</mml:mi><mml:mi>Ïµ</mml:mi><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:mi>Ïµ</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:msqrt><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:msqrt></mml:mfrac><mml:msub><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:mi>W</mml:mi></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where in this paper we choseÂ <inline-formula><mml:math id="inf608"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Î£</mml:mi><mml:mi>Ïµ</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, the covariance of the noise to be<disp-formula id="equ82"><label>(82)</label><mml:math id="m82"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Î£</mml:mi><mml:mi>Ïµ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mtext>noise</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mover accent="true"><mml:mi>Ï</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mover accent="true"><mml:mi>Ï</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>P</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mover accent="true"><mml:mi>Ï</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>S</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mover accent="true"><mml:mi>Ï</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>V</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf609"><mml:msub><mml:mover accent="true"><mml:mi>Ï</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>Î±</mml:mi></mml:msub></mml:math></inline-formula> is the reparameterized standard deviation of the noise for population Î± from <xref ref-type="disp-formula" rid="equ70">Equation 70</xref>.</p><p>We are interested in computing the covariance of the activity. For that, first we define <inline-formula><mml:math id="inf610"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mi>Ï</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mi>Ïµ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the total input to each cell type, and the matrix <inline-formula><mml:math id="inf611"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the negative JacobianÂ <inline-formula><mml:math id="inf612"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mo>â</mml:mo><mml:mi>Ï</mml:mi><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.Â Then,Â <xref ref-type="disp-formula" rid="equ81">Equation 81</xref>Â can be written as an 8-dimensional system. Linearizing around the fixed point of the system without fluctuations, we find the equations that describe the fluctuations of the input to each cell type:<disp-formula id="equ83"><label>(83)</label><mml:math id="m83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>Î´</mml:mi><mml:mi>v</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Ïµ</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>S</mml:mi></mml:mtd><mml:mtd><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>Ï</mml:mi><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>I</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:mi>I</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>Î´</mml:mi><mml:mi>v</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Ïµ</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mfrac><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:msqrt><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:msqrt></mml:mfrac><mml:msub><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mfrac><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:msqrt><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:msqrt></mml:mfrac><mml:msub><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf613"><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi mathvariant="bold">ð</mml:mi></mml:mrow></mml:math></inline-formula> is a vector with the private noise of each variable. The <inline-formula><mml:math id="inf614"><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi mathvariant="bold">ð</mml:mi></mml:mrow></mml:math></inline-formula> term is multiplied by a non-diagonal matrix,Â because the noise that the voltage receives is the exact same as the one that comes from the OU process and not another process. The covariance of the inputs <inline-formula><mml:math id="inf615"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:mi>Î´</mml:mi><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mi>Î´</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> can be found as the solution the followingÂ Lyapunov equation (<xref ref-type="bibr" rid="bib38">Hennequin et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Gardiner, 2009</xref>):<disp-formula id="equ84"><label>(84)</label><mml:math id="m84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>S</mml:mi></mml:mtd><mml:mtd><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>Ï</mml:mi><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>I</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:mi>I</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>Ï</mml:mi><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>I</mml:mi></mml:mtd><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:mi>I</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac><mml:mn>2</mml:mn><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mfrac><mml:mn>2</mml:mn><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mn>2</mml:mn><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mfrac><mml:mn>2</mml:mn><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf616"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:mi>Î´</mml:mi><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mi>Î´</mml:mi><mml:msup><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> can be eliminated by solving this block matrix multiplication:<disp-formula id="equ85"><label>(85)</label><mml:math id="m85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>â</mml:mo><mml:msup><mml:mi>Ï</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The equation above is another Lyapunov Equation, now in 4 dimensions. In the simplest case in which <inline-formula><mml:math id="inf617"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mtext>noise</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:math></inline-formula>, the voltage is directly driven by white noise, and <inline-formula><mml:math id="inf618"><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math></inline-formula> can be expressed in powers of <inline-formula><mml:math id="inf619"><mml:mi>S</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf620"><mml:msup><mml:mi>S</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:math></inline-formula>. Because <inline-formula><mml:math id="inf621"><mml:mi>S</mml:mi></mml:math></inline-formula> satisfies its own polynomial equation (Cayley Hamilton theorem), there will be four coefficients for the expansion of <inline-formula><mml:math id="inf622"><mml:mi>S</mml:mi></mml:math></inline-formula> and four for <inline-formula><mml:math id="inf623"><mml:msup><mml:mi>S</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:math></inline-formula>, resulting in 16 coefficients that define <inline-formula><mml:math id="inf624"><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math></inline-formula> for a given <inline-formula><mml:math id="inf625"><mml:mi>S</mml:mi></mml:math></inline-formula>. Due to symmetry arguments (<xref ref-type="bibr" rid="bib26">Gardiner, 2009</xref>), in this case the diagonal elements of the covariance matrix of the voltage will have the form:<disp-formula id="equ86"><label>(86)</label><mml:math id="m86"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Î</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>â¢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â¢</mml:mo><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>â¢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>These coefficients <inline-formula><mml:math id="inf626"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are complicated functions of the Jacobian of the system. Although expressions for these coefficients can be found explicitly, only numerical evaluation of those expressions determine which components of the noisy input are going to strongly influence the variability of excitatory population. Showing the generality of this dependence in more complicated noise scenarios (e.g. <inline-formula><mml:math id="inf627"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mtext>noise</mml:mtext></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:math></inline-formula> as in Section 'EPI reveals how recurrence with multiple inhibitory subtypes governs excitatory variability in a V1 model'), is the focus of current research.</p></sec></sec><sec id="s4-5"><title>Superior colliculus</title><sec id="s4-5-1"><title>SC model</title><p>The ability to switch between two separate tasks throughout randomly interleaved trials, or ârapid task switching,â has been studied in rats, and midbrain superior colliculus (SC) has been show to play an important in this computation (<xref ref-type="bibr" rid="bib19">Duan et al., 2015</xref>). Neural recordings in SC exhibited two populations of neurons that simultaneously represented both task context (Pro or Anti) and motor response (contralateral or ipsilateral to the recorded side), which led to the distinction of two functional classes: the Pro/Contra and Anti/Ipsi neurons (<xref ref-type="bibr" rid="bib20">Duan et al., 2021</xref>). Given this evidence, Duan et al. proposed a model with four functionally-defined neuron-type populations: two in each hemisphere corresponding to the Pro/Contra and Anti/Ipsi populations. We study how the connectivity of this neural circuit governs rapid task switching ability.</p><p>The four populations of this model are denoted as left Pro (LP), left Anti (LA), right Pro (RP) and right Anti (RA). Each unit has an activity (<inline-formula><mml:math id="inf628"><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub></mml:math></inline-formula>) and internal variable (<inline-formula><mml:math id="inf629"><mml:msub><mml:mi>u</mml:mi><mml:mi>Î±</mml:mi></mml:msub></mml:math></inline-formula>) related by<disp-formula id="equ87"><label>(87)</label><mml:math id="m87"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>â¢</mml:mo><mml:mrow><mml:mi>tanh</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf630"><mml:mrow><mml:mi>Î±</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf631"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf632"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> control the position and shape of the nonlinearity. We order the neural populations of <inline-formula><mml:math id="inf633"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf634"><mml:mi>u</mml:mi></mml:math></inline-formula> in the following manner<disp-formula id="equ88"><label>(88)</label><mml:math id="m88"><mml:mrow><mml:mrow><mml:mrow><mml:mi>ð±</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mi>ð®</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>which evolve according to<disp-formula id="equ89"><label>(89)</label><mml:math id="m89"><mml:mrow><mml:mrow><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>ð®</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>ð®</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mo>â¢</mml:mo><mml:mi>ð±</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>ð¡</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>ð</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>with time constant <inline-formula><mml:math id="inf635"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.09</mml:mn><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, step size 24 ms and Gaussian noise <inline-formula><mml:math id="inf636"><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi mathvariant="bold">ð</mml:mi></mml:mrow></mml:math></inline-formula> of variance <inline-formula><mml:math id="inf637"><mml:msup><mml:mn>0.2</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>. These hyperparameter values are motivated by modeling choices and results from <xref ref-type="bibr" rid="bib20">Duan et al., 2021</xref>.</p><p>The weight matrix has four parameters for self <inline-formula><mml:math id="inf638"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>, vertical <inline-formula><mml:math id="inf639"><mml:mrow><mml:mi>v</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>, horizontal <inline-formula><mml:math id="inf640"><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>, and diagonal <inline-formula><mml:math id="inf641"><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> connections:<disp-formula id="equ90"><label>(90)</label><mml:math id="m90"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>v</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>v</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>v</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>v</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We study the role of parameters <inline-formula><mml:math id="inf642"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> in rapid task switching.</p><p>The circuit receives four different inputs throughout each trial, which has a total length of 1.8 s.<disp-formula id="equ91"><label>(91)</label><mml:math id="m91"><mml:mrow><mml:mrow><mml:mi>ð¡</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ð¡</mml:mi><mml:mtext>constant</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ð¡</mml:mi><mml:mtext>P,bias</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ð¡</mml:mi><mml:mtext>rule</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ð¡</mml:mi><mml:mtext>choice-period</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ð¡</mml:mi><mml:mtext>light</mml:mtext></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>There is a constant input to every population,<disp-formula id="equ92"><label>(92)</label><mml:math id="m92"><mml:mrow><mml:mrow><mml:msub><mml:mi>ð¡</mml:mi><mml:mtext>constant</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>constant</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>â¤</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>a bias to the Pro populations<disp-formula id="equ93"><label>(93)</label><mml:math id="m93"><mml:mrow><mml:mrow><mml:msub><mml:mi>ð¡</mml:mi><mml:mtext>P,bias</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>P,bias</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>â¤</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>rule-based input depending on the condition<disp-formula id="equ94"><label>(94)</label><mml:math id="m94"><mml:mrow><mml:mrow><mml:msub><mml:mi>ð¡</mml:mi><mml:mtext>P,rule</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>P,rule</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>â¤</mml:mo><mml:mrow><mml:mn>1.2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ95"><label>(95)</label><mml:math id="m95"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ð¡</mml:mi><mml:mtext>A,rule</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>A,rule</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>â¤</mml:mo><mml:mrow><mml:mn>1.2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>a choice-period input<disp-formula id="equ96"><label>(96)</label><mml:math id="m96"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ð¡</mml:mi><mml:mtext>choice</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>choice</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:mn>1.2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and an input to the right or left-side depending on where the light stimulus is delivered<disp-formula id="equ97"><label>(97)</label><mml:math id="m97"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ð¡</mml:mi><mml:mtext>light</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>light</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>â¢</mml:mo><mml:mn>1.2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mrow><mml:mn>1.5</mml:mn><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo> <mml:mtext>and Left</mml:mtext></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>light</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>â¢</mml:mo><mml:mn>1.2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mrow><mml:mn>1.5</mml:mn><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo> <mml:mtext>and Right</mml:mtext></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The input parameterization was fixed to <inline-formula><mml:math id="inf643"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>constant</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf644"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>P,bias</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf645"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>P,rule</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf646"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>A,rule</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf647"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>choice</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf648"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>light</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-5-2"><title>Task accuracy calculation</title><p>The accuracies of the Pro- and Anti-tasks are calculated as<disp-formula id="equ98"><label>(98)</label><mml:math id="m98"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð±</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi>ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>andÂ <disp-formula id="equ99"><label>(99)</label><mml:math id="m99"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi>ð±</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi>ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf649"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf650"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> calculate the decision made in each trial (approximately 1 for correct and 0 for incorrect choices). Specifically,Â <disp-formula id="equ100"><label>(100)</label><mml:math id="m100"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1.8</mml:mn><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1.8</mml:mn><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>in Pro-trials where the stimulus is on the left side, and Î approximates the Heaviside step function. Similarly,<disp-formula id="equ101"><label>(101)</label><mml:math id="m101"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1.8</mml:mn><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1.8</mml:mn><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Â in Anti-trials where the stimulus was on the left side. Our accuracy calculation only considers one stimulus presentation (Left), since the model is left-right symmetric. The accuracy is averaged over 200 independent trials, and the Heaviside step function is approximated as<disp-formula id="equ102"><label>(102)</label><mml:math id="m102"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mtext>sigmoid</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mi mathvariant="normal">Î</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mi>ð±</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf651"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mi mathvariant="normal">Î</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-5-3"><title>EPI details for the SC model</title><p>To write the emergent properties of <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> in terms of the EPI optimization, we have<disp-formula id="equ103"><label>(103)</label><mml:math id="m103"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ð±</mml:mi><mml:mo>;</mml:mo><mml:mi>ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ104"><label>(104)</label><mml:math id="m104"><mml:mrow><mml:mrow><mml:mi>ð</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>.75</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>.75</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and<disp-formula id="equ105"><label>(105)</label><mml:math id="m105"><mml:mrow><mml:msup><mml:mi>ð</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mn>.075</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mn>.075</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>(see Sections 'Maximum entropy distributions and exponential families',Â 'Augmented lagrangian optimization', and example in Section 'Example: 2D LDS').</p><p>Throughout optimization, the augmented lagrangian parameters Î· and <inline-formula><mml:math id="inf652"><mml:mi>c</mml:mi></mml:math></inline-formula>, were updated after each epoch of <inline-formula><mml:math id="inf653"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> iterations (see Section 'Augmented lagrangian optimization'). The optimization converged after ten epochs (<xref ref-type="fig" rid="fig4s4">Figure 4âfigure supplement 4</xref>).</p><p>For EPI in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, we used a real NVP architecture with three coupling layers of affine transformations parameterized by two-layer neural networks of 50 units per layer. The initial distribution was a standard isotropic gaussian <inline-formula><mml:math id="inf654"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð©</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn/><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> mapped to a support of <inline-formula><mml:math id="inf655"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð³</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We used an augmented lagrangian coefficient of <inline-formula><mml:math id="inf656"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, a batch size <inline-formula><mml:math id="inf657"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf658"><mml:mrow><mml:mi>Î²</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. The distribution was the greatest EPI distribution to converge across five random seeds with criteria <inline-formula><mml:math id="inf659"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>test</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>The bend in the EPI distribution is not a spurious result of the EPI optimization. The structure discovered by EPI matches the shape of the set of points returned from brute-force random sampling (<xref ref-type="fig" rid="fig4s5">Figure 4âfigure supplement 5A</xref>) These connectivities were sampled from a uniform distribution over the range of each connectivity parameter, and all parameters producing accuracy in each task within the range of 60% to 90% were kept. This set of connectivities will not match the distribution of EPI exactly, since it is not conditioned on the emergent property. For example, the parameter set returned by the brute-force search is biased toward lower accuracies (<xref ref-type="fig" rid="fig4s5">Figure 4âfigure supplement 5B</xref>).</p></sec><sec id="s4-5-4"><title>Mode identification with EPI</title><p>We found one mode of the EPI distribution for fixed values of <inline-formula><mml:math id="inf660"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> from 1 to â1 in steps of 0.25. To begin, we chose an initial parameter value from 500 parameter samples <inline-formula><mml:math id="inf661"><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="bold-italic">ð½</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">â£</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> that had closest <inline-formula><mml:math id="inf662"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> value to 1. We then optimized this estimate of the mode (for fixed <inline-formula><mml:math id="inf663"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>) using probability gradients of the deep probability distribution for 500 steps of gradient ascent with a learning rate of <inline-formula><mml:math id="inf664"><mml:mrow><mml:mn>5</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The next mode (at <inline-formula><mml:math id="inf665"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math></inline-formula>) was found using the previous mode as the initialization. This and all subsequent optimizations used 200 steps of gradient ascent with a learning rate of <inline-formula><mml:math id="inf666"><mml:mrow><mml:mn>1</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, except at <inline-formula><mml:math id="inf667"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> where a learning rate of <inline-formula><mml:math id="inf668"><mml:mrow><mml:mn>5</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> was used. During all mode identification optimizations, the learning rate was reduced by half (decay = 0.5) after every 100 iterations.</p></sec><sec id="s4-5-5"><title>Sample grouping by mode</title><p>For the analyses in <xref ref-type="fig" rid="fig5">Figure 5C</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>, we obtained parameters for each step along the continuum between regimes 1 and 2 by sampling from the EPI distribution. Each sample was assigned to the closest mode <inline-formula><mml:math id="inf669"><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Sampling continued until 500 samples were assigned to each mode, which took 2.67 s (5.34 ms/sample-per-mode). It took 9.59 min to obtain just five samples for each mode with brute force sampling requiring accuracies between 60% and 90% in each task (115 s/sample-per-mode). This corresponds to a sampling speed increase of roughly 21,500 once the EPI distribution has been learned.</p></sec><sec id="s4-5-6"><title>Sensitivity analysis</title><p>At each mode, we measure the sensitivity dimension (that of most negative eigenvalue in the Hessian of the EPI distribution) <inline-formula><mml:math id="inf670"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To resolve sign degeneracy in eigenvectors, we chose <inline-formula><mml:math id="inf671"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to have negative element in <inline-formula><mml:math id="inf672"><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>. This tells us what parameter combination rapid task switching is most sensitive to at this parameter choice in the regime.</p></sec><sec id="s4-5-7"><title>Connectivity eigendecomposition and processing modes</title><p>To understand the connectivity mechanisms governing task accuracy, we took the eigendecomposition of the connectivity matrices <inline-formula><mml:math id="inf673"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>â¢</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which results in the same eigenmodes <inline-formula><mml:math id="inf674"><mml:msub><mml:mi mathvariant="bold">ðª</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> for all <inline-formula><mml:math id="inf675"><mml:mi>W</mml:mi></mml:math></inline-formula> parameterized by <inline-formula><mml:math id="inf676"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4s3">Figure 4âfigure supplement 3A</xref>). These eigenvectors are always the same, because the connectivity matrix is symmetric and the model also assumes symmetry across hemispheres, but the eigenvalues of connectivity (or degree of eigenmode amplification) change with <inline-formula><mml:math id="inf677"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula>. These basis vectors have intuitive roles in processing for this task, and are accordingly named the <italic>all</italic> eigenmode - all neurons co-fluctuate, <italic>side</italic> eigenmode - one side dominates the other, <italic>task</italic> eigenmode - the Pro or Anti-populations dominate the other, and <italic>diag</italic> mode - Pro- and Anti-populations of opposite hemispheres dominate the opposite pair. Due to the parametric structure of the connectivity matrix, the parameters <inline-formula><mml:math id="inf678"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> are a linear function of the eigenvalues <inline-formula><mml:math id="inf679"><mml:mrow><mml:mi mathvariant="bold-italic">ð</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>all</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>side</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>task</mml:mtext></mml:msub><mml:mo>â¢</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>diag</mml:mtext></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> associated with these eigenmodes.<disp-formula id="equ106"><label>(106)</label><mml:math id="m106"><mml:mrow><mml:mi>ð³</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>â¢</mml:mo><mml:mi>ð</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ107"><label>(107)</label><mml:math id="m107"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We are interested in the effect of raising or lowering the amplification of each eigenmode in the connectivity matrix by perturbing individual eigenvalues Î». To test this, we calculate the unit vector of changes in the connectivity <inline-formula><mml:math id="inf680"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> that result from a change in the associated eigenvalues<disp-formula id="equ108"><label>(108)</label><mml:math id="m108"><mml:mrow><mml:mrow><mml:msub><mml:mi>ð¯</mml:mi><mml:mtext>a</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mfrac><mml:mrow><mml:mo>â</mml:mo><mml:mo>â¡</mml:mo><mml:mi>ð³</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mo>â¡</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>a</mml:mtext></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mfrac><mml:mrow><mml:mo>â</mml:mo><mml:mo>â¡</mml:mo><mml:mi>ð³</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mo>â¡</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>a</mml:mtext></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Â where<disp-formula id="equ109"><label>(109)</label><mml:math id="m109"><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mo>â</mml:mo><mml:mo>â¡</mml:mo><mml:mi>ð³</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mo>â¡</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mtext>a</mml:mtext></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>ð</mml:mi><mml:mtext>a</mml:mtext></mml:msub></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and forÂ example <inline-formula><mml:math id="inf681"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð</mml:mi><mml:mtext>all</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. So <inline-formula><mml:math id="inf682"><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mtext>a</mml:mtext></mml:msub></mml:math></inline-formula> is the normalized column of A corresponding to eigenmode <inline-formula><mml:math id="inf683"><mml:mi>a</mml:mi></mml:math></inline-formula>. The parameter dimension <inline-formula><mml:math id="inf684"><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="inf685"><mml:mrow><mml:mi>a</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mtext>all</mml:mtext><mml:mo>,</mml:mo><mml:mtext>side</mml:mtext><mml:mo>,</mml:mo><mml:mtext>task</mml:mtext><mml:mo>,</mml:mo> <mml:mtext>and diag</mml:mtext><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) that increases the eigenvalue of connectivity <inline-formula><mml:math id="inf686"><mml:msub><mml:mi>Î»</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> is <inline-formula><mml:math id="inf687"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula>-invariant (<xref ref-type="disp-formula" rid="equ109">Equation 109</xref>) and <inline-formula><mml:math id="inf688"><mml:mrow><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>â </mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. By perturbing <inline-formula><mml:math id="inf689"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> along <inline-formula><mml:math id="inf690"><mml:msub><mml:mi mathvariant="bold">ð¯</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula>, we can examine how model function changes by directly modulating the connectivity amplification of specific eigenmodes, which have interpretable roles in processing in each task.</p></sec><sec id="s4-5-8"><title>Modeling optogenetic silencing</title><p>We tested whether the inferred SC model connectivities could reproduce experimental effects of optogenetic inactivation in rats (<xref ref-type="bibr" rid="bib20">Duan et al., 2021</xref>). During periods of simulated optogenetic inactivation, activity was decreased proportional to the optogenetic strength <inline-formula><mml:math id="inf691"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><disp-formula id="equ110"><label>(110)</label><mml:math id="m110"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>Î³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â¢</mml:mo><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>Î±</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Delay period inactivation was from <inline-formula><mml:math id="inf692"><mml:mrow><mml:mn>0.8</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1.2</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was funded by NSF Graduate Research Fellowship, DGE-1644869, McKnight Endowment Fund, NIH NINDS 5R01NS100066, Simons Foundation 542963, NSF NeuroNex Award, DBI-1707398, The Gatsby Charitable Foundation, Simons Collaboration on the Global Brain Postdoctoral Fellowship, Chinese Postdoctoral Science Foundation, and International Exchange Program Fellowship. We also acknowledge the Marine Biological Laboratory Methods in Computational Neuroscience Course, where this work was discussed and explored in its early stages. Helpful conversations were had with Larry Abbott, Stephen Baccus, James Fitzgerald, Gabrielle Gutierrez, Francesca Mastrogiuseppe, Srdjan Ostojic, Liam Paninski, and Dhruva Raman.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-56265-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All software and scripts for emergent property inference and the analyses in this manuscript can be found on the Cunningham Lab github at this link: <ext-link ext-link-type="uri" xlink:href="https://github.com/cunningham-lab/epi">https://github.com/cunningham-lab/epi</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:38febae7035ca921334a616b0f396b3767bf18d4">https://archive.softwareheritage.org/swh:1:rev:38febae7035ca921334a616b0f396b3767bf18d4</ext-link>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Theoretical neuroscience rising</article-title><source>Neuron</source><volume>60</volume><fpage>489</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.019</pub-id><pub-id pub-id-type="pmid">18995824</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Achard</surname> <given-names>P</given-names></name><name><surname>De Schutter</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Complex parameter landscape for a complex neuron model</article-title><source>PLOS Computational Biology</source><volume>2</volume><elocation-id>e94</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.0020094</pub-id><pub-id pub-id-type="pmid">16848639</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alonso</surname> <given-names>LM</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Visualization of currents in neural models with similar behavior and different conductance densities</article-title><source>eLife</source><volume>8</volume><elocation-id>e42722</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.42722</pub-id><pub-id pub-id-type="pmid">30702427</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barak</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Recurrent neural networks as versatile tools of neuroscience research</article-title><source>Current Opinion in Neurobiology</source><volume>46</volume><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.06.003</pub-id><pub-id pub-id-type="pmid">28668365</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beaumont</surname> <given-names>MA</given-names></name><name><surname>Zhang</surname> <given-names>W</given-names></name><name><surname>Balding</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Approximate bayesian computation in population genetics</article-title><source>Genetics</source><volume>162</volume><fpage>2025</fpage><lpage>2035</lpage><pub-id pub-id-type="doi">10.1093/genetics/162.4.2025</pub-id><pub-id pub-id-type="pmid">12524368</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bertsekas</surname> <given-names>DP</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Constrained Optimization and Lagrange Multiplier Methods</source><publisher-name>Academic press</publisher-name><pub-id pub-id-type="doi">10.1016/C2013-0-10366-2</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bittner</surname> <given-names>SR</given-names></name><name><surname>Palmigiano</surname> <given-names>A</given-names></name><name><surname>Miller</surname> <given-names>KD</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Degenerate solution networks for theoretical neuroscience</article-title><conf-name>Computational and Systems Neuroscience Meeting (COSYNE), Lisbon, Portugal</conf-name></element-citation></ref><ref id="bib8"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bittner</surname> <given-names>SR</given-names></name><name><surname>Piet</surname> <given-names>AT</given-names></name><name><surname>Duan</surname> <given-names>CA</given-names></name><name><surname>Palmigiano</surname> <given-names>A</given-names></name><name><surname>Miller</surname> <given-names>KD</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Examining models in theoretical neuroscience with degenerate solution networks</article-title><conf-name>Bernstein Conference 2019 Germany</conf-name></element-citation></ref><ref id="bib9"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bittner</surname> <given-names>SR</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>epi</data-title><source>Software Heritage</source><version designator="swh:1:rev:38febae7035ca921334a616b0f396b3767bf18d4 https://archive.softwareheritage.org/swh:1:rev:38febae7035ca921334a616b0f396b3767bf18d4">swh:1:rev:38febae7035ca921334a616b0f396b3767bf18d4 https://archive.softwareheritage.org/swh:1:rev:38febae7035ca921334a616b0f396b3767bf18d4</version></element-citation></ref><ref id="bib10"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bittner</surname> <given-names>SR</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Approximating exponential family models (not single distributions) with a two-network architecture</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1903.07515">https://arxiv.org/abs/1903.07515</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bondanelli</surname> <given-names>G</given-names></name><name><surname>Deneux</surname> <given-names>T</given-names></name><name><surname>Bathellier</surname> <given-names>B</given-names></name><name><surname>Ostojic</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Population coding and network dynamics during off responses in auditory cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/810655</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bondanelli</surname> <given-names>G</given-names></name><name><surname>Ostojic</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Coding with transient trajectories in recurrent neural networks</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007655</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007655</pub-id><pub-id pub-id-type="pmid">32053594</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calderhead</surname> <given-names>B</given-names></name><name><surname>Girolami</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Statistical analysis of nonlinear dynamical systems using differential geometric sampling methods</article-title><source>Interface Focus</source><volume>1</volume><fpage>821</fpage><lpage>835</lpage><pub-id pub-id-type="doi">10.1098/rsfs.2011.0051</pub-id><pub-id pub-id-type="pmid">23226584</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>RTQ</given-names></name><name><surname>Rubanova</surname> <given-names>Y</given-names></name><name><surname>Bettencourt</surname> <given-names>J</given-names></name><name><surname>Duvenaud</surname> <given-names>DK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural ordinary differential equations</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>6571</fpage><lpage>6583</lpage></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chkrebtii</surname> <given-names>OA</given-names></name><name><surname>Campbell</surname> <given-names>DA</given-names></name><name><surname>Calderhead</surname> <given-names>B</given-names></name><name><surname>Girolami</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Bayesian solution uncertainty quantification for differential equations</article-title><source>Bayesian Analysis</source><volume>11</volume><fpage>1239</fpage><lpage>1267</lpage><pub-id pub-id-type="doi">10.1214/16-BA1017</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname> <given-names>MM</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Sugrue</surname> <given-names>LP</given-names></name><name><surname>Cohen</surname> <given-names>MR</given-names></name><name><surname>Corrado</surname> <given-names>GS</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name><name><surname>Clark</surname> <given-names>AM</given-names></name><name><surname>Hosseini</surname> <given-names>P</given-names></name><name><surname>Scott</surname> <given-names>BB</given-names></name><name><surname>Bradley</surname> <given-names>DC</given-names></name><name><surname>Smith</surname> <given-names>MA</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name><name><surname>Armstrong</surname> <given-names>KM</given-names></name><name><surname>Moore</surname> <given-names>T</given-names></name><name><surname>Chang</surname> <given-names>SW</given-names></name><name><surname>Snyder</surname> <given-names>LH</given-names></name><name><surname>Lisberger</surname> <given-names>SG</given-names></name><name><surname>Priebe</surname> <given-names>NJ</given-names></name><name><surname>Finn</surname> <given-names>IM</given-names></name><name><surname>Ferster</surname> <given-names>D</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Santhanam</surname> <given-names>G</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Stimulus onset quenches neural variability: a widespread cortical phenomenon</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>369</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1038/nn.2501</pub-id><pub-id pub-id-type="pmid">20173745</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cranmer</surname> <given-names>K</given-names></name><name><surname>Brehmer</surname> <given-names>J</given-names></name><name><surname>Louppe</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The frontier of simulation-based inference</article-title><source>PNAS</source><volume>117</volume><fpage>30055</fpage><lpage>30062</lpage><pub-id pub-id-type="doi">10.1073/pnas.1912789117</pub-id><pub-id pub-id-type="pmid">32471948</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dinh</surname> <given-names>L</given-names></name><name><surname>Sohl-Dickstein</surname> <given-names>J</given-names></name><name><surname>Bengio</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Density estimation using real nvp</article-title><conf-name>Proceedings of the 5th International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duan</surname> <given-names>CA</given-names></name><name><surname>Erlich</surname> <given-names>JC</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Requirement of prefrontal and midbrain regions for rapid executive control of behavior in the rat</article-title><source>Neuron</source><volume>86</volume><fpage>1491</fpage><lpage>1503</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.05.042</pub-id><pub-id pub-id-type="pmid">26087166</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duan</surname> <given-names>CA</given-names></name><name><surname>Pagan</surname> <given-names>M</given-names></name><name><surname>Piet</surname> <given-names>AT</given-names></name><name><surname>Kopec</surname> <given-names>CD</given-names></name><name><surname>Akrami</surname> <given-names>A</given-names></name><name><surname>Riordan</surname> <given-names>AJ</given-names></name><name><surname>Erlich</surname> <given-names>JC</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Collicular circuits for flexible sensorimotor routing</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00865-x</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elsayed</surname> <given-names>GF</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Structure in neural population recordings: an expected byproduct of simpler phenomena?</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1310</fpage><lpage>1318</lpage><pub-id pub-id-type="doi">10.1038/nn.4617</pub-id><pub-id pub-id-type="pmid">28783140</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erguler</surname> <given-names>K</given-names></name><name><surname>Stumpf</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Practical limits for reverse engineering of dynamical systems: a statistical analysis of sensitivity and parameter inferability in systems biology models</article-title><source>Molecular BioSystems</source><volume>7</volume><fpage>1593</fpage><lpage>1602</lpage><pub-id pub-id-type="doi">10.1039/c0mb00107d</pub-id><pub-id pub-id-type="pmid">21380410</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname> <given-names>DJ</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1</pub-id><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname> <given-names>D</given-names></name><name><surname>Olasagasti</surname> <given-names>I</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name><name><surname>Aksay</surname> <given-names>ERF</given-names></name><name><surname>Goldman</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A modeling framework for deriving the structural and functional architecture of a Short-Term memory microcircuit</article-title><source>Neuron</source><volume>79</volume><fpage>987</fpage><lpage>1000</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.06.041</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname> <given-names>WR</given-names></name><name><surname>Ungar</surname> <given-names>LH</given-names></name><name><surname>Schwaber</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Significance of conductances in Hodgkin-Huxley models</article-title><source>Journal of Neurophysiology</source><volume>70</volume><fpage>2502</fpage><lpage>2518</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.70.6.2502</pub-id><pub-id pub-id-type="pmid">7509859</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gardiner</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Stochastic Methods: A Handbook for the Natural and Social Sciences</source><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girolami</surname> <given-names>M</given-names></name><name><surname>Calderhead</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Riemann manifold Langevin and hamiltonian monte carlo methods</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>73</volume><fpage>123</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9868.2010.00765.x</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman</surname> <given-names>MS</given-names></name><name><surname>Golowasch</surname> <given-names>J</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Global structure, robustness, and modulation of neuronal models</article-title><source>The Journal of Neuroscience</source><volume>21</volume><fpage>5229</fpage><lpage>5238</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.21-14-05229.2001</pub-id><pub-id pub-id-type="pmid">11438598</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Memory without feedback in a neural network</article-title><source>Neuron</source><volume>61</volume><fpage>621</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.12.012</pub-id><pub-id pub-id-type="pmid">19249281</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golightly</surname> <given-names>A</given-names></name><name><surname>Wilkinson</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Bayesian parameter inference for stochastic biochemical network models using particle markov chain monte carlo</article-title><source>Interface Focus</source><volume>1</volume><fpage>807</fpage><lpage>820</lpage><pub-id pub-id-type="doi">10.1098/rsfs.2011.0047</pub-id><pub-id pub-id-type="pmid">23226583</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>GonÃ§alves</surname> <given-names>PJ</given-names></name><name><surname>Lueckmann</surname> <given-names>J-M</given-names></name><name><surname>Deistler</surname> <given-names>M</given-names></name><name><surname>Nonnenmacher</surname> <given-names>M</given-names></name><name><surname>Ãcal</surname> <given-names>K</given-names></name><name><surname>Bassetto</surname> <given-names>G</given-names></name><name><surname>Chintaluri</surname> <given-names>C</given-names></name><name><surname>Podlaski</surname> <given-names>WF</given-names></name><name><surname>Haddad</surname> <given-names>SA</given-names></name><name><surname>Vogels</surname> <given-names>TP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Training deep neural density estimators to identify mechanistic models of neural dynamics</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/838383</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Greenberg</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Marcel Nonnenmacher, and Jakob H Macke. automatic posterior transformation for likelihood-free inference</article-title><conf-name>International Conference OnÂ Machine Learning</conf-name></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutenkunst</surname> <given-names>RN</given-names></name><name><surname>Waterfall</surname> <given-names>JJ</given-names></name><name><surname>Casey</surname> <given-names>FP</given-names></name><name><surname>Brown</surname> <given-names>KS</given-names></name><name><surname>Myers</surname> <given-names>CR</given-names></name><name><surname>Sethna</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Universally sloppy parameter sensitivities in systems biology models</article-title><source>PLOS Computational Biology</source><volume>3</volume><elocation-id>e189</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.0030189</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutierrez</surname> <given-names>GJ</given-names></name><name><surname>O'Leary</surname> <given-names>T</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Multiple mechanisms switch an electrically coupled, synaptically inhibited neuron between competing rhythmic oscillators</article-title><source>Neuron</source><volume>77</volume><fpage>845</fpage><lpage>858</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.016</pub-id><pub-id pub-id-type="pmid">23473315</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hastings</surname> <given-names>WK</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>Monte carlo sampling methods using markov chains and their applications</article-title><source>Biometrika</source><volume>57</volume><fpage>97</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1093/biomet/57.1.97</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hengl</surname> <given-names>S</given-names></name><name><surname>Kreutz</surname> <given-names>C</given-names></name><name><surname>Timmer</surname> <given-names>J</given-names></name><name><surname>Maiwald</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Data-based identifiability analysis of non-linear dynamical models</article-title><source>Bioinformatics</source><volume>23</volume><fpage>2612</fpage><lpage>2618</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btm382</pub-id><pub-id pub-id-type="pmid">17660526</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname> <given-names>G</given-names></name><name><surname>Vogels</surname> <given-names>TP</given-names></name><name><surname>Gerstner</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimal control of transient dynamics in balanced networks supports generation of complex movements</article-title><source>Neuron</source><volume>82</volume><fpage>1394</fpage><lpage>1406</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.045</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname> <given-names>G</given-names></name><name><surname>Ahmadian</surname> <given-names>Y</given-names></name><name><surname>Rubin</surname> <given-names>DB</given-names></name><name><surname>Lengyel</surname> <given-names>M</given-names></name><name><surname>Miller</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The dynamical regime of sensory cortex: stable dynamics around a single Stimulus-Tuned attractor account for patterns of noise variability</article-title><source>Neuron</source><volume>98</volume><fpage>846</fpage><lpage>860</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.04.017</pub-id><pub-id pub-id-type="pmid">29772203</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hermans</surname> <given-names>J</given-names></name><name><surname>Begy</surname> <given-names>V</given-names></name><name><surname>Louppe</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Likelihood-free mcmc with amortized approximate ratio estimators</article-title><conf-name>International Conference on Machine Learning PMLR</conf-name><fpage>4239</fpage><lpage>4248</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname> <given-names>KE</given-names></name><name><surname>Middendorf</surname> <given-names>TR</given-names></name><name><surname>Aldrich</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Determination of parameter identifiability in nonlinear biophysical models: a bayesian approach</article-title><source>Journal of General Physiology</source><volume>143</volume><fpage>401</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1085/jgp.201311116</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Neural networks and physical systems with emergent collective computational abilities</article-title><source>PNAS</source><volume>79</volume><fpage>2554</fpage><lpage>2558</lpage><pub-id pub-id-type="doi">10.1073/pnas.79.8.2554</pub-id><pub-id pub-id-type="pmid">6953413</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karlsson</surname> <given-names>J</given-names></name><name><surname>Anguelova</surname> <given-names>M</given-names></name><name><surname>Jirstrand</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>An efficient method for structural identifiability analysis of large dynamic systems*</article-title><source>IFAC Proceedings Volumes</source><volume>45</volume><fpage>941</fpage><lpage>946</lpage><pub-id pub-id-type="doi">10.3182/20120711-3-BE-2027.00381</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Salimans</surname> <given-names>T</given-names></name><name><surname>Jozefowicz</surname> <given-names>R</given-names></name><name><surname>Chen</surname> <given-names>X</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Welling</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Improved variational inference with inverse autoregressive flow</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>4743</fpage><lpage>4751</lpage></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Ba</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Adam: a method for stochastic optimization</article-title><conf-name>International Conference OnÂ Learning Representations</conf-name></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Dhariwal</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Glow: generative flow with invertible 1x1 convolutions</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>10215</fpage><lpage>10224</lpage></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klinger</surname> <given-names>E</given-names></name><name><surname>Rickert</surname> <given-names>D</given-names></name><name><surname>Hasenauer</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>pyABC: distributed, likelihood-free inference</article-title><source>Bioinformatics</source><volume>34</volume><fpage>3591</fpage><lpage>3593</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bty361</pub-id><pub-id pub-id-type="pmid">29762723</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kopell</surname> <given-names>N</given-names></name><name><surname>Ermentrout</surname> <given-names>GB</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Coupled oscillators and the design of central pattern generators</article-title><source>Mathematical Biosciences</source><volume>90</volume><fpage>87</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1016/0025-5564(88)90059-4</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>X</given-names></name><name><surname>Wong</surname> <given-names>T-KL</given-names></name><name><surname>Chen</surname> <given-names>RTQ</given-names></name><name><surname>Duvenaud</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Scalable gradients for stochastic differential equations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2001.01328">https://arxiv.org/abs/2001.01328</ext-link></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liepe</surname> <given-names>J</given-names></name><name><surname>Kirk</surname> <given-names>P</given-names></name><name><surname>Filippi</surname> <given-names>S</given-names></name><name><surname>Toni</surname> <given-names>T</given-names></name><name><surname>Barnes</surname> <given-names>CP</given-names></name><name><surname>Stumpf</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A framework for parameter estimation and model selection from experimental data in systems biology using approximate bayesian computation</article-title><source>Nature Protocols</source><volume>9</volume><fpage>439</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1038/nprot.2014.025</pub-id><pub-id pub-id-type="pmid">24457334</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname> <given-names>A</given-names></name><name><surname>Rosenbaum</surname> <given-names>R</given-names></name><name><surname>Doiron</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Inhibitory stabilization and visual coding in cortical circuits with multiple interneuron subtypes</article-title><source>Journal of Neurophysiology</source><volume>115</volume><fpage>1399</fpage><lpage>1409</lpage><pub-id pub-id-type="doi">10.1152/jn.00732.2015</pub-id><pub-id pub-id-type="pmid">26740531</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Loaiza-Ganem</surname> <given-names>G</given-names></name><name><surname>Gao</surname> <given-names>Y</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Maximum entropy flow networks</article-title><conf-name>International Conference OnÂ Learning Representations</conf-name></element-citation></ref><ref id="bib52"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lueckmann</surname> <given-names>J-M</given-names></name><name><surname>Goncalves</surname> <given-names>PJ</given-names></name><name><surname>Bassetto</surname> <given-names>G</given-names></name><name><surname>Ãcal</surname> <given-names>K</given-names></name><name><surname>Nonnenmacher</surname> <given-names>M</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Flexible statistical inference for mechanistic models of neural dynamics</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1289</fpage><lpage>1299</lpage></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mannakee</surname> <given-names>BK</given-names></name><name><surname>Ragsdale</surname> <given-names>AP</given-names></name><name><surname>Transtrum</surname> <given-names>MK</given-names></name><name><surname>Gutenkunst</surname> <given-names>RN</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>Sloppiness and the geometry of parameter space</chapter-title><person-group person-group-type="editor"><name><surname>Mannakee</surname> <given-names>B. K</given-names></name></person-group><source>Uncertainty in Biology</source><publisher-name>Springer</publisher-name><fpage>271</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-21296-8_11</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>From biophysics to models of network function</article-title><source>Annual Review of Neuroscience</source><volume>21</volume><fpage>25</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.21.1.25</pub-id><pub-id pub-id-type="pmid">9530490</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marder</surname> <given-names>E</given-names></name><name><surname>Selverston</surname> <given-names>AI</given-names></name></person-group><year iso-8601-date="1992">1992</year><source>Dynamic Biological Networks: The Stomatogastric Nervous System</source><publisher-name>MIT press</publisher-name><pub-id pub-id-type="doi">10.1016/0166-2236(93)90153-D</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marder</surname> <given-names>E</given-names></name><name><surname>Thirumalai</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Cellular, synaptic and network effects of neuromodulation</article-title><source>Neural Networks</source><volume>15</volume><fpage>479</fpage><lpage>493</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(02)00043-6</pub-id><pub-id pub-id-type="pmid">12371506</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marjoram</surname> <given-names>P</given-names></name><name><surname>Molitor</surname> <given-names>J</given-names></name><name><surname>Plagnol</surname> <given-names>V</given-names></name><name><surname>Tavare</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Markov chain monte carlo without likelihoods</article-title><source>PNAS</source><volume>100</volume><fpage>15324</fpage><lpage>15328</lpage><pub-id pub-id-type="doi">10.1073/pnas.0306899100</pub-id><pub-id pub-id-type="pmid">14663152</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname> <given-names>H</given-names></name><name><surname>Toledo-Rodriguez</surname> <given-names>M</given-names></name><name><surname>Wang</surname> <given-names>Y</given-names></name><name><surname>Gupta</surname> <given-names>A</given-names></name><name><surname>Silberberg</surname> <given-names>G</given-names></name><name><surname>Wu</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Interneurons of the neocortical inhibitory system</article-title><source>Nature Reviews Neuroscience</source><volume>5</volume><fpage>793</fpage><lpage>807</lpage><pub-id pub-id-type="doi">10.1038/nrn1519</pub-id><pub-id pub-id-type="pmid">15378039</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metropolis</surname> <given-names>N</given-names></name><name><surname>Rosenbluth</surname> <given-names>AW</given-names></name><name><surname>Rosenbluth</surname> <given-names>MN</given-names></name><name><surname>Teller</surname> <given-names>AH</given-names></name><name><surname>Teller</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1953">1953</year><article-title>Equation of state calculations by fast computing machines</article-title><source>The Journal of Chemical Physics</source><volume>21</volume><fpage>1087</fpage><lpage>1092</lpage><pub-id pub-id-type="doi">10.1063/1.1699114</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Michael</surname> <given-names>D</given-names></name><name><surname>Goncalves</surname> <given-names>PJ</given-names></name><name><surname>Oecal</surname> <given-names>K</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Statistical inference for analyzing sloppiness in neuroscience models</article-title><conf-name>Bernstein Conference 2019 Germany</conf-name></element-citation></ref><ref id="bib61"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>MÅynarski</surname> <given-names>W</given-names></name><name><surname>HledÃ­k</surname> <given-names>M</given-names></name><name><surname>Sokolowski</surname> <given-names>TR</given-names></name><name><surname>TkaÄik</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Statistical analysis and optimality of neural systems</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/848374</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mossing</surname> <given-names>DP</given-names></name><name><surname>Veit</surname> <given-names>J</given-names></name><name><surname>Palmigiano</surname> <given-names>A</given-names></name><name><surname>Miller</surname> <given-names>KD</given-names></name><name><surname>Adesnik</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Antagonistic inhibitory subnetworks control cooperation and competition across cortical space</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.03.31.437953</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname> <given-names>BK</given-names></name><name><surname>Miller</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Balanced amplification: a new mechanism of selective amplification of neural activity patterns</article-title><source>Neuron</source><volume>61</volume><fpage>635</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.005</pub-id><pub-id pub-id-type="pmid">19249282</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname> <given-names>S</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Juavinett</surname> <given-names>AL</given-names></name><name><surname>Gluf</surname> <given-names>S</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id><pub-id pub-id-type="pmid">31551604</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname> <given-names>CM</given-names></name><name><surname>Stryker</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title><source>Neuron</source><volume>65</volume><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id><pub-id pub-id-type="pmid">20188652</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nonnenmacher</surname> <given-names>M</given-names></name><name><surname>Goncalves</surname> <given-names>PJ</given-names></name><name><surname>Bassetto</surname> <given-names>G</given-names></name><name><surname>Lueckmann</surname> <given-names>J-M</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Robust statistical inference for simulation-based models in neuroscience</article-title><conf-name>Bernstein Conference 2018 Germany</conf-name></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Leary</surname> <given-names>T</given-names></name><name><surname>Williams</surname> <given-names>AH</given-names></name><name><surname>Franci</surname> <given-names>A</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cell types, network homeostasis, and pathological compensation from a biologically plausible ion channel expression model</article-title><source>Neuron</source><volume>82</volume><fpage>809</fpage><lpage>821</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.002</pub-id><pub-id pub-id-type="pmid">24853940</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Leary</surname> <given-names>T</given-names></name><name><surname>Sutton</surname> <given-names>AC</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Computational models in the age of large datasets</article-title><source>Current Opinion in Neurobiology</source><volume>32</volume><fpage>87</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.01.006</pub-id><pub-id pub-id-type="pmid">25637959</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olypher</surname> <given-names>AV</given-names></name><name><surname>Calabrese</surname> <given-names>RL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Using constraints on neuronal activity to reveal compensatory changes in neuronal parameters</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>3749</fpage><lpage>3758</lpage><pub-id pub-id-type="doi">10.1152/jn.00842.2007</pub-id><pub-id pub-id-type="pmid">17855581</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozeki</surname> <given-names>H</given-names></name><name><surname>Finn</surname> <given-names>IM</given-names></name><name><surname>Schaffer</surname> <given-names>ES</given-names></name><name><surname>Miller</surname> <given-names>KD</given-names></name><name><surname>Ferster</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Inhibitory stabilization of the cortical network underlies visual surround suppression</article-title><source>Neuron</source><volume>62</volume><fpage>578</fpage><lpage>592</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.03.028</pub-id><pub-id pub-id-type="pmid">19477158</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Palmigiano</surname> <given-names>A</given-names></name><name><surname>Fumarola</surname> <given-names>F</given-names></name><name><surname>Mossing</surname> <given-names>DP</given-names></name><name><surname>Kraynyukova</surname> <given-names>N</given-names></name><name><surname>Adesnik</surname> <given-names>H</given-names></name><name><surname>Miller</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Structure and variability of optogenetic responses identify the operating regime of cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.11.11.378729</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural data science: accelerating the experiment-analysis-theory cycle in large-scale neuroscience</article-title><source>Current Opinion in Neurobiology</source><volume>50</volume><fpage>232</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.04.007</pub-id><pub-id pub-id-type="pmid">29738986</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Papamakarios</surname> <given-names>G</given-names></name><name><surname>Pavlakou</surname> <given-names>T</given-names></name><name><surname>Murray</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Masked autoregressive flow for density estimation</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>2338</fpage><lpage>2347</lpage></element-citation></ref><ref id="bib74"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Papamakarios</surname> <given-names>G</given-names></name><name><surname>Nalisnick</surname> <given-names>E</given-names></name><name><surname>Rezende</surname> <given-names>DJ</given-names></name><name><surname>Mohamed</surname> <given-names>S</given-names></name><name><surname>Lakshminarayanan</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Normalizing flows for probabilistic modeling and inference</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1912.02762">https://arxiv.org/abs/1912.02762</ext-link></element-citation></ref><ref id="bib75"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Papamakarios</surname> <given-names>G</given-names></name><name><surname>Sterratt</surname> <given-names>D</given-names></name><name><surname>Murray</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Sequential neural likelihood: fast likelihood-free inference with autoregressive flows</article-title><conf-name>The 22nd International Conference on Artificial Intelligence and Statistics</conf-name><fpage>837</fpage><lpage>848</lpage></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeffer</surname> <given-names>CK</given-names></name><name><surname>Xue</surname> <given-names>M</given-names></name><name><surname>He</surname> <given-names>M</given-names></name><name><surname>Huang</surname> <given-names>ZJ</given-names></name><name><surname>Scanziani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Inhibition of inhibition in visual cortex: the logic of connections between molecularly distinct interneurons</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1068</fpage><lpage>1076</lpage><pub-id pub-id-type="doi">10.1038/nn.3446</pub-id><pub-id pub-id-type="pmid">23817549</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pia Saccomani</surname> <given-names>M</given-names></name><name><surname>Audoly</surname> <given-names>S</given-names></name><name><surname>D'AngiÃ²</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Parameter identifiability of nonlinear systems: the role of initial conditions</article-title><source>Automatica</source><volume>39</volume><fpage>619</fpage><lpage>632</lpage><pub-id pub-id-type="doi">10.1016/S0005-1098(02)00302-3</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prinz</surname> <given-names>AA</given-names></name><name><surname>Bucher</surname> <given-names>D</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Similar network activity from disparate circuit parameters</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>1345</fpage><lpage>1352</lpage><pub-id pub-id-type="doi">10.1038/nn1352</pub-id><pub-id pub-id-type="pmid">15558066</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raman</surname> <given-names>DV</given-names></name><name><surname>Anderson</surname> <given-names>J</given-names></name><name><surname>Papachristodoulou</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Delineating parameter unidentifiabilities in complex models</article-title><source>Physical Review E</source><volume>95</volume><elocation-id>032314</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.95.032314</pub-id><pub-id pub-id-type="pmid">28415348</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raue</surname> <given-names>A</given-names></name><name><surname>Kreutz</surname> <given-names>C</given-names></name><name><surname>Maiwald</surname> <given-names>T</given-names></name><name><surname>Bachmann</surname> <given-names>J</given-names></name><name><surname>Schilling</surname> <given-names>M</given-names></name><name><surname>KlingmÃ¼ller</surname> <given-names>U</given-names></name><name><surname>Timmer</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Structural and practical identifiability analysis of partially observed dynamical models by exploiting the profile likelihood</article-title><source>Bioinformatics</source><volume>25</volume><fpage>1923</fpage><lpage>1929</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btp358</pub-id><pub-id pub-id-type="pmid">19505944</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rezende</surname> <given-names>DJ</given-names></name><name><surname>Mohamed</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Variational inference with normalizing flows</article-title><conf-name>International Conference onÂ Machine Learning</conf-name></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname> <given-names>DB</given-names></name><name><surname>Van Hooser</surname> <given-names>SD</given-names></name><name><surname>Miller</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The stabilized supralinear network: a unifying circuit motif underlying multi-input integration in sensory cortex</article-title><source>Neuron</source><volume>85</volume><fpage>402</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.026</pub-id><pub-id pub-id-type="pmid">25611511</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudy</surname> <given-names>B</given-names></name><name><surname>Fishell</surname> <given-names>G</given-names></name><name><surname>Lee</surname> <given-names>S</given-names></name><name><surname>Hjerling-Leffler</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Three groups of interneurons account for nearly 100% of neocortical GABAergic neurons</article-title><source>Developmental Neurobiology</source><volume>71</volume><fpage>45</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1002/dneu.20853</pub-id><pub-id pub-id-type="pmid">21154909</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russo</surname> <given-names>AA</given-names></name><name><surname>Bittner</surname> <given-names>SR</given-names></name><name><surname>Perkins</surname> <given-names>SM</given-names></name><name><surname>Seely</surname> <given-names>JS</given-names></name><name><surname>London</surname> <given-names>BM</given-names></name><name><surname>Lara</surname> <given-names>AH</given-names></name><name><surname>Miri</surname> <given-names>A</given-names></name><name><surname>Marshall</surname> <given-names>NJ</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name><name><surname>Jessell</surname> <given-names>TM</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Churchland</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Motor cortex embeds Muscle-like commands in an untangled population response</article-title><source>Neuron</source><volume>97</volume><fpage>953</fpage><lpage>966</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.004</pub-id><pub-id pub-id-type="pmid">29398358</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saleem</surname> <given-names>AB</given-names></name><name><surname>Ayaz</surname> <given-names>A</given-names></name><name><surname>Jeffery</surname> <given-names>KJ</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Integration of visual motion and locomotion in mouse visual cortex</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1864</fpage><lpage>1869</lpage><pub-id pub-id-type="doi">10.1038/nn.3567</pub-id><pub-id pub-id-type="pmid">24185423</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Saul</surname> <given-names>L</given-names></name><name><surname>Jordan</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><chapter-title>A mean field learning algorithm for unsupervised neural networks</chapter-title><person-group person-group-type="editor"><name><surname>Jordan</surname> <given-names>M. I</given-names></name></person-group><source>Learning in Graphical Models</source><publisher-name>Springer</publisher-name><fpage>541</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.1007/978-94-011-5014-9_20</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savin</surname> <given-names>C</given-names></name><name><surname>TkaÄik</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Maximum entropy models as a tool for building precise neural controls</article-title><source>Current Opinion in Neurobiology</source><volume>46</volume><fpage>120</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.08.001</pub-id><pub-id pub-id-type="pmid">28869818</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sisson</surname> <given-names>SA</given-names></name><name><surname>Fan</surname> <given-names>Y</given-names></name><name><surname>Tanaka</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Sequential monte carlo without likelihoods</article-title><source>PNAS</source><volume>104</volume><fpage>1760</fpage><lpage>1765</lpage><pub-id pub-id-type="doi">10.1073/pnas.0607208104</pub-id><pub-id pub-id-type="pmid">17264216</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sisson</surname> <given-names>SA</given-names></name><name><surname>Fan</surname> <given-names>Y</given-names></name><name><surname>Beaumont</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Handbook of Approximate Bayesian Computation</source><publisher-name>CRC Press</publisher-name><pub-id pub-id-type="doi">10.1002/bimj.201900141</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sompolinsky</surname> <given-names>H</given-names></name><name><surname>Crisanti</surname> <given-names>A</given-names></name><name><surname>Sommers</surname> <given-names>HJ</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Chaos in random neural networks</article-title><source>Physical Review Letters</source><volume>61</volume><fpage>259</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.61.259</pub-id><pub-id pub-id-type="pmid">10039285</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural circuits as computational dynamical systems</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><fpage>156</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.01.008</pub-id><pub-id pub-id-type="pmid">24509098</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tran</surname> <given-names>D</given-names></name><name><surname>Ranganath</surname> <given-names>R</given-names></name><name><surname>Blei</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hierarchical implicit models and likelihood-free variational inference</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>5523</fpage><lpage>5533</lpage></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tremblay</surname> <given-names>R</given-names></name><name><surname>Lee</surname> <given-names>S</given-names></name><name><surname>Rudy</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>GABAergic interneurons in the neocortex: from cellular properties to circuits</article-title><source>Neuron</source><volume>91</volume><fpage>260</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.06.033</pub-id><pub-id pub-id-type="pmid">27477017</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsodyks</surname> <given-names>MV</given-names></name><name><surname>Skaggs</surname> <given-names>WE</given-names></name><name><surname>Sejnowski</surname> <given-names>TJ</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Paradoxical effects of external modulation of inhibitory interneurons</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>4382</fpage><lpage>4388</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04382.1997</pub-id><pub-id pub-id-type="pmid">9151754</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wainwright</surname> <given-names>MJ</given-names></name><name><surname>Jordan</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Graphical models, exponential families, and variational inference</article-title><source>Foundations and Trends in Machine Learning</source><volume>1</volume><fpage>1</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1561/2200000001</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neurophysiological and computational principles of cortical rhythms in cognition</article-title><source>Physiological Reviews</source><volume>90</volume><fpage>1195</fpage><lpage>1268</lpage><pub-id pub-id-type="doi">10.1152/physrev.00035.2008</pub-id><pub-id pub-id-type="pmid">20664082</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname> <given-names>KF</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A recurrent network mechanism of time integration in perceptual decisions</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>1314</fpage><lpage>1328</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3733-05.2006</pub-id><pub-id pub-id-type="pmid">16436619</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56265.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>O'Leary</surname><given-names>Timothy</given-names></name><role>Reviewing Editor</role><aff><institution>University of Cambridge</institution><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Goldman</surname><given-names>Mark S</given-names></name><role>Reviewer</role><aff><institution>University of California at Davis</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Progress in understanding cellular and circuit mechanisms that underpin neural function depend on us being able to reconstitute key computations and high level functions from models that consist of simpler, biologically motivated building blocks. This requires choosing parameters in a model, yet even in the simplest models there tends to be a degenerate relationship between model parameters and its overall emergent function. Bittner and colleagues present a computationally tractable method for inferring parameter distributions that are consistent with an emergent property of interest and demonstrate its use in a variety of well-known circuit models, and show that the method compares favourably with state of the art sampling and parameter identification methods.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Interrogating theoretical models of neural computation with deep inference&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and John Huguenard as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Mark S. Goldman (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>The editors have judged that your manuscript is of potential interest, but as described below significant additional work is required before it can be considered for publication. Adequate revisions may require extensive work and we ordinarily avoid passing a manuscript to a second round of review on this basis. However, we have made changes in our revision policy in response to COVID-19 (https://elifesciences.org/articles/57162). First, because many researchers have temporarily lost access to the labs, we will give authors as much time as they need to submit revised manuscripts. We are also offering, if you choose, to post the manuscript to bioRxiv (if it is not already there) along with this decision letter and a formal designation that the manuscript is &quot;in revision at <italic>eLife</italic>&quot;. Please let us know if you would like to pursue this option. (If your work is more suitable for medRxiv, you will need to post the preprint yourself, as the mechanisms for us to do so are still in development.)</p><p>Summary:</p><p>Bittner and colleagues introduce a machine learning framework for maximum entropy inference of model parameter distributions that are consistent with certain emergent model properties, specified by the investigator. The approach is illustrated on several models of potential interest.</p><p>Reviewers were broadly enthusiastic about the potential usefulness of this methodology. However, the reviews and ensuing discussion revealed several points of concern about the manuscript and the approach. The full reviewer comments are included below. The main concerns are summarized as follows.</p><p>Essential revisions:</p><p>1. The methodology is not adequately explained. Both the body text and methods section present a somewhat selective description that is very hard to follow in places and should be checked and rewritten for clarity, completeness, notational consistency and correctness.</p><p>2. The computational resources required to use this method are not adequately benchmarked. For example, the cosubmission (Macke et al) reported wall clock time, required hardware and iterations required to produce results by directly comparing to existing methods (approximate bayesian computation, naive sampling, etc.) Without transparent benchmarks it is not possible to assess the advance offered by this method.</p><p>3. The extent to which this method is generally/straightforwardly applicable was in doubt. It seemed as though a significant amount of computation was required to do inference on one specified property and that the computation would need to be run afresh to query a new property. The methodology in the cosubmission (Macke) made clear that computation required for successive inferences is 'amortized' during training on random parameters. Moreover, EPI seemed less flexible than the cosubmission's approach in that it required a differentiable loss function. The complementarity and advantages of this approach as opposed to the cosubmission's approach are therefore unclear.</p><p>4. Some examples lack depth in their treatment (see reviewer comments) and in some cases the presentation is somewhat misleading. The STG example is not in fact a model of the STG. The cosubmission (Macke) uses a model close to the original Prinz et al. model, which is a model of the pyloric subnetwork. It would be instructive to benchmark against this same model, including computation time/resources required. Secondly, the subsequent example (input-responsivity in a nonlinear sensory system) appeared to imply that EPI permits 'generation' and testing of hypotheses in a way that other methods do not. All the method really does is estimate a joint distribution of feasible parameters in a specific model which is manually inspected to propose hypotheses. Any other method (including brute force sampling) could be used in a similar way, so any claim that this is an integral advantage of EPI would be spurious. Indeed, one reviewer was confused about the origin of these hypotheses. While it is helpful to illustrate how EPI (and other methods) could be used, the writing needs to be far clearer in general and should clarify that EPI does not offer any new specific means of generating or evaluating hypotheses.</p><p>5. There is a substantial literature on parameter sensitivity analysis and inference in systems biology, applied dynamical systems and control that has been neglected in this manuscript. The manuscript needs to acknowledge, draw parallels and explain distinctions from current methods (ABC, profile likelihood, deep learning approaches, gaussian processes, etc). The under-referencing of this literature deepened concerns about whether this approach represented an advance. DOIs for a small subset of potentially relevant papers include:</p><p>https://doi.org/10.1038/nprot.2014.025</p><p>http://doi.org/10.1085/jgp.201311116</p><p>http://doi.org/10.1016/j.tcs.2008.07.005</p><p>http://doi.org/10.3182/20120711-3-BE-2027.00381</p><p>http://doi.org/10.1093/bioinformatics/btm382</p><p>http://doi.org/10.1111/j.1467-9868.2010.00765.x</p><p>http://doi.org/10.1098/rsfs.2011.0051</p><p>https://doi.org/10.1098/rsfs.2011.0047</p><p>http://doi.org/10.1214/16-BA1017</p><p>https://doi.org/10.1039/C0MB00107D</p><p>6. One of the reviewers expressed concern that the work might have had significant input from a senior colleague during its early stages, and that it might be worth discussing with the senior colleague whether their contribution was worthy of authorship. The authors may take this concern into account in revising the manuscript.</p><p>7. Finally, please also address specific points raised by the reviewers, included below.</p><p><italic>Reviewer #1:</italic></p><p>General Assessment: The authors introduce a machine learning framework for maximum entropy inference of model parameters which are consistent with certain emergent properties, which they call 'emergent property inference'. I think this is an interesting and direction, and this looks like a useful step towards this program. I think the paper could be improved with a more thorough discussion both of the broad principles their black box approach seeks to optimize, as well as the details of its implementation. I also think the detailed examples should be more self-contained. Finally I find this work to be somewhat misrepresented as a key to all of theoretical neuroscience. This approach may have some things to offer to the interesting problem of finding parameter regions of models, but this is not the entirety of, nor really a major part of theoretical neuroscience as I see it.</p><p>Other concerns:</p><p>(1) Maximizing the entropy of the distribution is not a reparameterization invariant exercise. That is, results depend on whether the model parameters contains rates or time constants, for example. I wonder if this approach is attempting to use a 'flat prior' in some sense which has the same reparameterization issue? Can the authors comment?</p><p>(2) I don't think this is a criticism of the work, but instead of the writing about it: I find the introductory paragraphs to give a rather limited overview of theory as finding parameters of models which contain the right phenomenology.</p><p>(3) I am somewhat familiar with the stomatognathic circuit model, and so that is where I think I understand what they have done best. I don't understand what I should take away from their paper with regards to this model. Are there any findings that hadn't been appreciated before? What does this method tell us about the system and or its model?</p><p>(4) I don't follow the other examples. Ideally more details should be given so that readers like myself who don't already know these systems can understand what's been done.</p><p>(5) In figure 2C, the difference between the confidence interval between linear and nonlinear predictions is huge! How much of this is due to nonlinearities, and how much is due to differences in the way these models are being evaluated?</p><p><italic>Reviewer #2:</italic></p><p>This is a very interesting approach to an extremely important question in theoretical neuroscience, and the mathematics and algorithms appear to be very rigorous. The complexities in using this in practice make me wonder if it will find wide application though: setting up the objective to be differentiable, tweaking hyperparameters for training, and interpreting the results; all seem to require a lot from the user. On the other hand, the authors are to be congratulated on providing high quality open source code including clear tutorials on how to use it.</p><p>1. Training deep networks is hard. Indeed the authors devote a substantial amount of the manuscript to techniques for training them, and note that different hyperparameters were necessary for each of the different studies. Can the authors be confident that they have found the network which gives maximum entropy or close to it? If so, how. If not, how does that affect the conclusions?</p><p>2. Interpreting the results still seems to require quite a lot of work. For example, from inspecting Figure 2 the authors extract four hypotheses. Why these four? Are there other hypotheses that could be extracted and if not how do we know there aren't? Could something systematic be said here?</p><p>3. Scalability. The authors state that the method should in principle be scalable, but does that apply to interpreting the results? For example, for the V1 model it seems that you need to look at 48 figures for 4 variables, and I believe this would scale as O(n<sup>2</sup>) with n variables. This seems to require an unsustainable amount of manual work?</p><p>4. There are some very particular choices made in the applications and I wonder how general the conclusions are as a consequence. For example, in Equation 5 the authors choose an arbitrary amount of variance 0.01<sup>2</sup> â why? In the same example, why look at y=0.1 and 0.5?</p><p><italic>Reviewer #3:</italic></p><p>This paper addresses a major issue in fitting neuroscience models: how to identify the often degenerate, or nearly degenerate, set of parameters that can underlie a set of experimental observations. Whereas previous techniques often depended upon brute force explorations or special parametric forms or local linearizations to generate sets of parameters consistent with measured properties, the authors take advantage of deep generative networks to address this problem. Overall, I think this paper and the complementary submission have the potential to be transformative contributions to model fitting efforts in neuroscience. That being said, since the primary contribution is the methodology, I think the paper requires more systematic comparisons to ground truth examples to demonstrate potential strengths and weaknesses, and more focus on methodology rather than applications.</p><p>1) The authors only have a single ground-truth example where they compare to a known result (a 2x2 linear dynamical system). It would be good to show how well this method compares to results from, for example, a direct brute force grid search of a system with a strongly non-elliptical (e.g. sharply bent) shaped parameter regime and a reasonably large (e.g. 5?) number of parameters corresponding to a particular property, to see how well the derived probability distribution overlaps the brute force grid search parameters (perhaps shown via several 2-D projections).</p><p>2) It was not obvious whether EPI actually scales well to higher dimensions and how much computation it would take (there is one claim that it 'should scale reasonably'). While I agree that examples with a small number of parameters is nice for illustration, a major issue is how to develop techniques that can handle large numbers of parameters (brute force, while inelegant, inefficient, and not producing an explicit probability distribution can do a reasonable job for small #'s of parameters). The authors should show some example of extending to larger number of parameters and do some checks to show that it appears to work. As a methodological contribution, the authors should also give some sense of how computationally intensive the method is and some sense of how it scales with size. This seems particularly relevant to, for example, trying to infer uncertainties in a large weight matrix or a non-parametric description of spatial or temporal responses or a sensory neuron (which I'm assuming this technique is not appropriate for? See point #4 below).</p><p>3) For the STG-like example, this was done for a very simple model that was motivated by the STG but isn't based on experimental recordings. Most of the brute force models of the STG seek to fit various waveform properties of neurons and relative phases. Could the model handle these types of analyses, or would it run into problems due to either needing to specify too many properties or because properties like &quot;number of spikes per burst&quot; are discrete rather than continuous? This isn't fatal, but would be good to consider and/or note explicitly.</p><p>4) The discussion should be expanded to be more specific about what problems the authors think the model is, or is not, appropriate for. Comparisons to the Goncalves article would also be helpful since users will want to know the comparative advantages/disadvantages of each method. (if the authors could coordinate running their methods on a common illustrative example, that would be cool, but not required).</p><p>5) Given that the paper is heavily a (very valuable!) methods paper for a general audience, the method should be better explained both in the main text and the supplement. Some specific ones are below, but the authors should more generally send the paper to naÃ¯ve readers to check what is/is not well explained.</p><p>â Figure 1 is somewhat opaque and also has notational issues (e.g. omega is the frequency but also appears to be the random input sample).</p><p>â For the general audience of <italic>eLife</italic>, panels C and D are not well described individually or well connected to each other and don't illustrate or describe all of the relevant variables (including what q0 is and what x is).</p><p>â In Equation 2 (and also in the same equation in the supplement), it was not immediately obvious what the expectation was taken over.</p><p>â The authors don't specific the distribution of w (it's referred to only as 'a simple random variable', which is not clear).</p><p>â It was also sometimes hard to quickly find in the text basic, important quantities like what z was for a given simulation.</p><p>â The augmented Lagrangian optimization was not well explained or motivated. There is a reference to m=absolute value(mu) but I didn't see m in the above equation.</p><p>â Using mu to describe a vector that includes means and variances is confusing notation since mu often denotes means.</p><p>â It would be helpful to have a pseudo-code 'Algorithm' figure or section of the text.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56265.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1. The methodology is not adequately explained. Both the body text and methods section present a somewhat selective description that is very hard to follow in places and should be checked and rewritten for clarity, completeness, notational consistency and correctness.</p></disp-quote><p>Agreed. In the introduction, we present our method by focusing on the key aspect of EPI that differentiates it from other approaches to inverse problems. Specifically, we explain how current methodology infers the parameters producing computation by conditioning on <italic>exemplar datasets</italic> (real or simulated), whereas in EPI we condition directly on the <italic>emergent properties</italic> that define the computation. We also note (and have an appendix detailing) how EPI is in fact doing variational bayesian inference, but that the parameterization of the problem as we solve it is more natural given the goal of reproducing a computation. In other words, the methodology is mathematically sound and directly connected to well-worn techniques, but the specific form we solve is the appropriate choice for the motivating problem.</p><p>Lines 44-52</p><p>âStatistical inference, of course, requires quantification of the sometimes vague term <italic>computation</italic>. [â¦] Related to this point, use of a conventional dataset encourages conventional data likelihoods or loss functions, which focus on some global metric like squared error or marginal evidence, rather than the computation itself.â</p><p>We now frame the method within the more general context of parameter inference techniques in neuroscience, rather than the context of recent advancements in machine learning. We have made concentrated efforts to simplify language and to relate to all relevant existing methodology.</p><p>Lines 53-63</p><p>âAlternatively, researchers often quantify an <italic>emergent property</italic> (EP): a statistic of data that directly quantifies the computation of interest, wherein the dataset is implicit. [â¦] This statistical framework is not new: it is intimately connected to the literature on approximate bayesian computation [24, 25, 26], parameter sensitivity analyses [27, 28, 29, 30], maximum entropy modeling [31, 32, 33], and approximate bayesian inference [34, 35]; we detail these connections in Section 5.1.1.â</p><p>To improve clarity, we have overhauled and simplified the notation and presentation of emergent properties. Emergent properties are now denoted with X: rather than a vector of first- and second-moment constraints, we present the emergent property more readably (and equivalently) as mean and variance constraints on emergent property statistics:</p><p>Line 742</p><p>â<inline-formula><mml:math id="inf693"><mml:mrow><mml:mi>X</mml:mi><mml:mspace width="0.222em"/><mml:mo>:</mml:mo><mml:mspace width="0.222em"/><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo form="prefix" stretchy="true">[</mml:mo><mml:mi>f</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>z</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo form="postfix" stretchy="true">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.222em"/><mml:mi>Î¼</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222em"/><mml:msub><mml:mtext mathvariant="normal">Var</mml:mtext><mml:mrow><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo form="prefix" stretchy="true">[</mml:mo><mml:mi>f</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>z</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo form="postfix" stretchy="true">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.222em"/><mml:msup><mml:mi>Ï</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> (12)â</p><p>Emergent properties and EPI are introduced and explained with this revised notation in Section 'Emergent property inference via deep generative models', and we have completely redone the Methods Section 'Emergent property inference (EPI)' to improve clarity, precisely explain EPI optimization (Section 'Augmented lagrangian optimization'), and derive equivalence to established bayesian inference techniques (Section 'EPI as variational inference').</p><p>In our revised writing, we deemphasize the role of maximum entropy in the EPI algorithm, because this has largely served as a distraction in our experience. We show in Section 5.1.6 that EPI is exactly variational inference (this was mentioned in the previous draft but not as a point of connection and focus, as it should be and now is). Therefore, it makes sense to present EPI in the main text as a statistical inference technique that constrains the predictions of the inferred parameters to be an emergent property, and leave the details of the maximum entropy to the technically proficient in Section 5.1.</p><p>Lines 152-154</p><p>âMany distributions in Q will respect the emergent property constraints, so we select the most random (highest entropy) distribution, which also means this approach is equivalent to bayesian variational inference (see Section 5.1.6).â</p><p>Figure 1 has been largely redone to reflect the modified presentation, and improve the pictorial representation of the method. In Section 3.3, we now show that EPI is the only simulation-based inference method that controls the predictions of its inferred distribution (Figure 2C-D).</p><p>Finally, we emphasize the utility of this deep inference technique for scientific inquiry in a new paragraph at the end of Section 3.2.</p><p>Lines 159-175</p><p>âThe structure of the inferred parameter distributions of EPI can be analyzed to reveal key information about how the circuit model produces the emergent property.</p><p>[â¦] Importantly and unlike alternative techniques, once an EPI distribution has been learned, the modes and Hessians of the distribution can be measured with trivial computation (see Section 5.1.2).â</p><disp-quote content-type="editor-comment"><p>2. The computational resources required to use this method are not adequately benchmarked. For example, the cosubmission (Macke et al) reported wall clock time, required hardware and iterations required to produce results by directly comparing to existing methods (approximate bayesian computation, naive sampling, etc.) Without transparent benchmarks it is not possible to assess the advance offered by this method.</p></disp-quote><p>We thank the reviewers for emphasizing the importance of this methodological comparison. In Section 3.3, we provide a direct comparison of EPI to alternative simulation-based inference techniques SMC-ABC and SNPE by inferring RNN connectivities that exhibit stable amplification. These comparisons evaluate both wall time (Figure 2A) and simulation count (Figure 2B), and we explain how each algorithm was run in its preferred hardware setting in Section 5.3.4.</p><p>In this analysis, we demonstrate the improved scalability of deep inference techniques (EPI and SNPE) with respect to the state-of-the-art approximate bayesian computation technique (SMC-ABC). While controlling for architecture size (Figure 2âfigure supplement 1), we push the limits of SNPE through targeted hyperparameter modifications (Figure 2âfigure supplement 2), and show that EPI scales to higher dimensional RNN connectivities producing stable amplification than SNPE. Furthermore, we emphasize that SNPE does not constrain the properties of the inferred parameters; many connectivities inferred by SNPE result in unstable or nonamplified models (Figure 2C, Figure 2âfigure supplement 3).</p><disp-quote content-type="editor-comment"><p>3. The extent to which this method is generally/straightforwardly applicable was in doubt. It seemed as though a significant amount of computation was required to do inference on one specified property and that the computation would need to be run afresh to query a new property. The methodology in the cosubmission (Macke) made clear that computation required for successive inferences is 'amortized' during training on random parameters. Moreover, EPI seemed less flexible than the cosubmission's approach in that it required a differentiable loss function. The complementarity and advantages of this approach as opposed to the cosubmission's approach are therefore unclear.</p></disp-quote><p>We respectfully disagree with this characterization, but we take responsibility here: we certainly needed to improve our exposition and comparisons to clarify the general applicability of EPI. We have done so, and what results is a meaningful comparison with the cosubmission (SNPE, Macke et al., now published) and other methods. Of course, there is a tradeoff (no free lunch, as usual), and these new analyses clarify when EPI is preferable to SNPE, and vice versa.</p><p>First, unlike SNPE, EPI leverages gradients of the emergent property throughout optimization, which leads to better efficiency and scalability (Section 3.3). This is the usual tradeoff of gradient-based vs sampling-based methods. The emergent properties of many models in neuroscience are tractably differentiable, four of which we analyze in this manuscript ranging across levels of biological realism, network size, and computational function. This tradeoff is now explained in the Discussion (Section 4).</p><p>Second, of course the reviewers are right to point out EPI does not amortize across emergent properties, and it requires differentiability of the emergent properties of the model. SNPE is more suitable for inference with nondifferentiable mechanistic models and scientific problems requiring many inferred parameter distributions. However, these relative drawbacks of EPI with respect to SNPE can be considered choices made in a tradeoff between simulation-based inference approaches.</p><p>On the balance, these two methods occupy different areas of use, and both are equivalently âgenerally/straightforwardly applicableâ; we believe the new analyses and discussion in the manuscript clarify that, as it is a key point for practitioners going forward.</p><p>Lines 435-447</p><p>âA key difference between EPI and SNPE, is that EPI uses gradients of the emergent property throughout optimization. [â¦] In summary, choice of deep inference technique should consider emergent property complexity and differentiability, dimensionality of parameter space, and the importance of constraining the model behavior predicted by the inferred parameter distribution.â</p><p>Furthermore, EPI focuses the entire expressivity of the approximating deep probability distribution on a single distribution, rather than spreading this expressivity to some uncharacterized degree across the chosen training distribution of amortized posteriors in SNPE (Section 5.1.1 Related Approaches).</p><p>Lines 832-838</p><p>âThe approximating fidelity of the deep probability distribution in sequential approaches is optimized to generalize across the training distribution of the conditioning variable. [â¦] The well-known inverse mapping problem of exponential families [85] prohibits an amortization-based approach in EPI, since EPI learns an exponential family distribution parameterized by its mean (in contrast to its natural parameter, see Section 5.1.3).â</p><p>Finally, we emphasize that EPI does something fundamental that SNPE and other inference techniques cannot. EPI learns parameter distributions whose predictions are constrained to produce the emergent property. We show in Figure 2 and Supplementary Figure 2âfigure supplement 3 that SNPE does not control the statistical properties of its predictions, resulting in the inference of many parameters that are not consistent with the desired emergent property.</p><p>Lines 228-238</p><p>âNo matter the number of neurons, EPI always produces connectivity distributions with mean and variance of real(<italic>Î»</italic><sub>1</sub>) and <inline-formula><mml:math id="inf694"><mml:msubsup><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi></mml:msubsup></mml:math></inline-formula> according to X (Figure 2C, blue). [â¦] Even for moderate neuron counts, the predictions of the inferred distribution of SNPE are highly dependent on <italic>N</italic> and <italic>g</italic>, while EPI maintains the emergent property across choices of RNN (see Section 5.3.5).â</p><disp-quote content-type="editor-comment"><p>4. Some examples lack depth in their treatment (see reviewer comments) and in some cases the presentation is somewhat misleading. The STG example is not in fact a model of the STG. The cosubmission (Macke) uses a model close to the original Prinz et al. model, which is a model of the pyloric subnetwork. It would be instructive to benchmark against this same model, including computation time/resources required. Secondly, the subsequent example (input-responsivity in a nonlinear sensory system) appeared to imply that EPI permits 'generation' and testing of hypotheses in a way that other methods do not. All the method really does is estimate a joint distribution of feasible parameters in a specific model which is manually inspected to propose hypotheses. Any other method (including brute force sampling) could be used in a similar way, so any claim that this is an integral advantage of EPI would be spurious. Indeed, one reviewer was confused about the origin of these hypotheses. While it is helpful to illustrate how EPI (and other methods) could be used, the writing needs to be far clearer in general and should clarify that EPI does not offer any new specific means of generating or evaluating hypotheses.</p></disp-quote><p>We thank the reviewers for explaining how they found some of the presentation misleading. We have taken serious care in this manuscript to clarify (a) what is novel, appreciable scientific insight provided by EPI, as well as (b) which scientific analyses are made possible by EPI.</p><p>(a) In the revised manuscript we clarify that novel theoretical insights are not being made into the STG subcircuit model or the recurrent neural network models. The STG subcircuit serves as a motivational example to explain how EPI works, and we use RNNs exhibiting stable amplification as a substrate for scalability analyses.</p><p>Lines 75-79</p><p>âFirst, we show EPIâs ability to handle biologically realistic circuit models using a five-neuron model of the stomatogastric ganglion [41]: a neural circuit whose parametric degeneracy is closely studied [42]. Then, we show EPIâs scalability to high dimensional parameter distributions by inferring connectivities of recurrent neural networks that exhibit stable, yet amplified responses â a hallmark of neural responses throughout the brain [43, 44, 45].â</p><p>We do produce strong theoretical insights into a model of primary visual cortex (Section 3.4) and superior colliculus (Section 3.5). These analyses have substantially more depth than the previous manuscript.</p><p>Lines 79-87</p><p>âIn a model of primary visual cortex [46, 47], EPI reveals how the recurrent processing across different neuron-type populations shapes excitatory variability: a finding that we show is analytically intractable. [â¦] Intriguingly, the inferred connectivities of each regime reproduced results from optogenetic inactivation experiments in markedly different ways.â</p><p>(b) The ability to infer a flexible approximation to a probability distribution constrained to produce an emergent property is novel in its own right (Figure 2). The deep probability distribution fit by EPI facilitates the mode identification (via gradient ascent of the parameter log probability) and sensitivity measurement (via the measurement of the eigenvector of the Hessian at a parameter value). These mode identifications and sensitivity measurements are done in Sections 3.1 (Figure 1E), 3.4 (Figure 3E), and 3.5 (Figure 4C). By using this mode identification technique along the ridges of high parameter probability in the SC model, we identify the parameters transitioning between the two regimes. Finally, the sensitivity dimensions of the SC model identified by EPI facilitated regime characterization through perturbation analyses (Figure 4D).</p><p>Importantly, we do not claim that these theoretical insights were necessarily dependent on using the techniques in (b). One could have come to these conclusions via various combinations of techniques mentioned in Section 5.1.1 Related Methods. In the case of the V1 model inference, the main point is to indicate that such insight can be afforded by EPI and its related methods, in contrast to the analytic derivations emblematic of practice in theoretical neuroscience.</p><p>Lines 297-305</p><p>âEPI revealed the quadratic dependence of excitatory variability on input variability to the E- and P-populations, as well as its independence to input from the other two inhibitory populations. [â¦] By pointing out this mathematical complexity, we emphasize the value of EPI for gaining understanding about theoretical models when mathematical analysis becomes onerous or impractical.â</p><p>In the case of the SC model inference, random sampling would have taken prohibitively long, and it is unclear how the continuum between the two connectivity regimes would have been identified with alternative techniques:</p><p>Lines 412-415</p><p>âThe probabilistic tools afforded by EPI yielded this insight: we identified two regimes and the continuum of connectivities between them by taking gradients of parameter probabilities in the EPI distribution, we identified sensitivity dimensions by measuring the Hessian of the EPI distribution, and we obtained many parameter samples at each step along the continuum at an efficient rate.â</p><p>As the reviewers indicate, the STG model analyzed in our manuscript is not that of Prinz et al. 2004, and thus not the model analyzed by the cosubmission (Macke et al). We chose an alternative model, believing it important to demonstrate inference in biophysically realistic Morris-Lecar models when gradients are tractable, which is the case of the 5-neuron STG model we analyzed from Gutierrez et al. 2013. This 5-neuron model represents the IC neuron (hub) and its coupling to the pyloric (fast) or gastric mill (slow) subcircuit rhythms. In the introductory text, we refer to this model as the âSTG subcircuitâ model (rather than âSTG modelâ), and we better clarify what aspect of the STG is being modeled in Results section 3.1.</p><p>Lines 98-103</p><p>âA subcircuit model of the STG [41] is shown schematically in Figure 1A. The fast population (f1 and f2) represents the subnetwork generating the pyloric rhythm and the slow population (s1 and s2) represents the subnetwork of the gastric mill rhythm. [â¦] The hub neuron couples with either the fast or slow population, or both depending on modulatory conditions.â</p><p>The difference between this model and the STG model of the pyloric subnetwork is emphasized in Discussion:</p><p>Lines 440-443</p><p>âHowever, conditioning on the pyloric rhythm [68] in a model of the pyloric subnetwork model [15] proved to be prohibitive with EPI. The pyloric subnetwork requires many time steps for simulation and many key emergent property statistics (e.g. burst duration and phase gap) are not calculable or easily approximated with differentiable functions.â</p><disp-quote content-type="editor-comment"><p>5. There is a substantial literature on parameter sensitivity analysis and inference in systems biology, applied dynamical systems and control that has been neglected in this manuscript. The manuscript needs to acknowledge, draw parallels and explain distinctions from current methods (ABC, profile likelihood, deep learning approaches, gaussian processes, etc). The under-referencing of this literature deepened concerns about whether this approach represented an advance. DOIs for a small subset of potentially relevant papers include:</p><p>https://doi.org/10.1038/nprot.2014.025</p><p>http://doi.org/10.1085/jgp.201311116</p><p>http://doi.org/10.1016/j.tcs.2008.07.005</p><p>http://doi.org/10.3182/20120711-3-BE-2027.00381</p><p>http://doi.org/10.1093/bioinformatics/btm382</p><p>http://doi.org/10.1111/j.1467-9868.2010.00765.x</p><p>http://doi.org/10.1098/rsfs.2011.0051</p><p>https://doi.org/10.1098/rsfs.2011.0047</p><p>http://doi.org/10.1214/16-BA1017</p><p>https://doi.org/10.1039/C0MB00107D</p></disp-quote><p>Thank you for pointing us to these references on sensitivity analyses, MCMC inference, and applied dynamical systems. We have incorporated most of them into the current manuscript and explain EPIâs relation to each class of these techniques throughout Section 5.1.1 Related approaches.</p><disp-quote content-type="editor-comment"><p>6. One of the reviewers expressed concern that the work might have had significant input from a senior colleague during its early stages, and that it might be worth discussing with the senior colleague whether their contribution was worthy of authorship. The authors may take this concern into account in revising the manuscript.</p></disp-quote><p>Thank you for pointing this out. We take authorship and scientific contribution very seriously (it is always part of our manuscript submission process, as it was here). We have gone back and discussed every researcher who was in any way involved in this work, and to our best estimation, we think this comment references Woods Hole Course Project mentors where this work was discussed and explored in its early stages. We have had explicit follow-up conversations with James Fitzgerald and Dhruva Raman and noted that of course we would be happy to have them involved at any level (great colleagues!). They both reported they are happy with their acknowledgement in the paper and donât feel that there is a justification for authorship (again we were very welcoming of this possibility and maintain positive enthusiasm for both). Furthermore, Stephen Baccus has requested that we acknowledge the summer course, which we have done. If there is any other âsenior colleagueâ the reviewer had in mind, please let us know and we will of course gladly pursue the matter.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>General Assessment: The authors introduce a machine learning framework for maximum entropy inference of model parameters which are consistent with certain emergent properties, which they call 'emergent property inference'. I think this is an interesting and direction, and this looks like a useful step towards this program. I think the paper could be improved with a more thorough discussion both of the broad principles their black box approach seeks to optimize, as well as the details of its implementation. I also think the detailed examples should be more self-contained. Finally I find this work to be somewhat misrepresented as a key to all of theoretical neuroscience. This approach may have some things to offer to the interesting problem of finding parameter regions of models, but this is not the entirety of, nor really a major part of theoretical neuroscience as I see it.</p></disp-quote><p>We thank the reviewer for their positive comments and thoughtful feedback. We have made serious effort to improve our presentation and explanation of EPI (see response to main concern #1). Furthermore, we have focused on clearly motivating and describing each neural circuit model studied in this manuscript. Sufficient mathematical detail is written in each Results section, while full details are presented in Methods. All code for training EPI on these models and their analysis are available in well-documented scripts and notebooks in our github repository.</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/cunningham-lab/epi">https://github.com/cunningham-lab/epi</ext-link></p><p>We modify our writing to clarify that EPI is not a key to all of theoretical neuroscience, but rather a powerful solution to inverse problems in neural circuit modeling. Inverse problems are indeed a major part of theoretical neuroscience, and their solutions are important for the evaluation of neural circuit models. We clarify this point in the introduction:</p><p>Lines 26-35</p><p>âThe fundamental practice of theoretical neuroscience is to use a mathematical model to understand neural computation, whether that computation enables perception, action, or some intermediate processing. [â¦] The inverse problem is crucial for reasoning about likely parameter values, uniquenesses and degeneracies, and predictions made by the model [6, 7, 8].â</p><disp-quote content-type="editor-comment"><p>Other concerns:</p><p>(1) Maximizing the entropy of the distribution is not a reparameterization invariant exercise. That is, results depend on whether the model parameters contains rates or time constants, for example. I wonder if this approach is attempting to use a 'flat prior' in some sense which has the same reparameterization issue? Can the authors comment?</p></disp-quote><p>The reviewer is correct to point out that maximum entropy solutions are not reparameterization invariant, and indeed the units matter. The reviewerâs suggestion that the method is in some sense using a flat prior is also correct. To clarify, EPI does not execute posterior inference, because there is no explicit dataset or specified prior belief in the EPI framework. However, we derive the relation of EPI to bayesian variational inference in Section 5.1.6, which shows EPI uses a uniform prior when framed as variational inference.</p><p>Lines 1027-1031</p><p>âWe see that EPI is exactly variational inference with an exponential family likelihood defined by sufficient statistics <italic>T</italic>(z<sup>) =</sup> Ex<sub>â¼<italic>p</italic>(x|z)</sub> [<italic>Ï</italic>(x;z)], and where the natural parameter <italic>Î·</italic><sup>â</sup> is predicated by the mean parameter <italic>Âµ</italic><sub>opt</sub>. Equation 40 implies that EPI uses an improper (or uniform) prior, which is easily changed.â</p><p>In our examples, we only infer distributions of parameters with the same units, so this issue should not draw concern over the validity of our model analyses. As suggested, the EPI solution will differ according to relative scaling of parameter values under the maximum entropy selection principle. Thus, an important clarification is that sensitivity quantifications are made in the context of the chosen parameter scalings.</p><disp-quote content-type="editor-comment"><p>(2) I don't think this is a criticism of the work, but instead of the writing about it: I find the introductory paragraphs to give a rather limited overview of theory as finding parameters of models which contain the right phenomenology.</p></disp-quote><p>We appreciate this feedback. We have adapted the introduction to clarify that we are focusing on solving inverse problems in theoretical neuroscience.</p><disp-quote content-type="editor-comment"><p>(3) I am somewhat familiar with the stomatognathic circuit model, and so that is where I think I understand what they have done best. I don't understand what I should take away from their paper with regards to this model. Are there any findings that hadn't been appreciated before? What does this method tell us about the system and or its model?</p></disp-quote><p>We clarify in point 4 above that we are not claiming to produce novel, appreciable scientific insight about the STG subcircuit model, which is used as a motivation example. The takeaway is that the conductance parameters producing intermediate hub frequency belong to a complex 2-D distribution, which EPI captures accurately, and that EPI can tell us the parameter changes away from the prototypical configuration that change hub frequency the most or least. For example, for increases in <italic>g</italic><sub>el</sub> and <italic>g</italic><sub>synA</sub> according to the proportions of the degenerate dimension of parameter space, intermediate hub neuron frequency will be preserved in this model. This suggests that in the real STG neural circuit, the IC neuron will remain at an intermediate frequency between the pyloric and gastric mill rhythms if parameter changes are made along such a dimension.</p><p>Lines 171 -173</p><p>âThe directionality of v<sub>2</sub> suggests that changes in conductance along this parameter combination will most preserve hub neuron firing between the intrinsic rates of the pyloric and gastric mill rhythms.â</p><disp-quote content-type="editor-comment"><p>(4) I don't follow the other examples. Ideally more details should be given so that readers like myself who don't already know these systems can understand what's been done.</p></disp-quote><p>Thank you for the feedback. We have taken care to give more general context and motivation for each neural circuit model.</p><disp-quote content-type="editor-comment"><p>(5) In figure 2C, the difference between the confidence interval between linear and nonlinear predictions is huge! How much of this is due to nonlinearities, and how much is due to differences in the way these models are being evaluated?</p></disp-quote><p>In the current manuscript, we do not examine the difference between linear and nonlinear predictions of the V1 model.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>This is a very interesting approach to an extremely important question in theoretical neuroscience, and the mathematics and algorithms appear to be very rigorous. The complexities in using this in practice make me wonder if it will find wide application though: setting up the objective to be differentiable, tweaking hyperparameters for training, and interpreting the results; all seem to require a lot from the user. On the other hand, the authors are to be congratulated on providing high quality open source code including clear tutorials on how to use it.</p><p>1. Training deep networks is hard. Indeed the authors devote a substantial amount of the manuscript to techniques for training them, and note that different hyperparameters were necessary for each of the different studies. Can the authors be confident that they have found the network which gives maximum entropy or close to it? If so, how. If not, how does that affect the conclusions?</p></disp-quote><p>We agree with the reviewer that hyperparameter sensitivity and global optima are important considerations of any optimization algorithm using deep neural networks. To draw a parallel, training deep networks for visual processing used to be considered infeasible, but became easier through iterative improvements in architectural and hyperparameter choices that spread across the field. Similarly, training deep networks via EPI to learn parameter distributions became easier throughout this research project as we learned through trial and error what works well. In fact, there has been extraordinary progress in the field of deep probability distributions (specifically normalizing flows), that have allowed EPI to converge regularly while capturing complex structure (e.g. Dinh et al. 2017 and Kingma et al. 2018). This manuscriptâs explanation of hyperparameter choices and the extensive set of examples in the online code will serve as valuable guidelines for future research using this method. Every figure of this paper is reproducible with the jupyter notebooks, and there are several tutorials for understanding the most consequential hyperparameters: the augmented lagrangian constant and deep probability distribution architecture.</p><p>In general, we cannot know if we have obtained the global maximum entropy distribution for a given emergent property. The reviewer is correct to point out that multiple distributions may satisfy the emergent property and have different levels of entropy. In the new manuscript, we present the method as an inference technique without focusing very greatly on maximum entropy, since it tends to distract and confuse the reader. We derive an analytic equivalence to variational inference (Section 5.1.6) showing that (a) EPI is a valid inference method, and (b) to emphasize that maximum entropy is the normative selection principle of bayesian inference methods in general. Thus, the concern of not having the globally optimal inferred distribution is the same that applies to all other statistical inference techniques.</p><p>Practically, this has scientific implications. It means that we may be missing important structure in the inferred distribution, or we may be missing additional modes in parameter space. To handle this methodologically, we run EPI with multiple random seeds, and select the distribution that has converged with the greatest entropy for scientific analysis. Throughout the manuscript, we compare to analytic, error contour, and brute-force ground truth to ensure we are capturing the correct distribution with EPI (see response to R3 concern 1).</p><disp-quote content-type="editor-comment"><p>2. Interpreting the results still seems to require quite a lot of work. For example, from inspecting Figure 2 the authors extract four hypotheses. Why these four? Are there other hypotheses that could be extracted and if not how do we know there aren't? Could something systematic be said here?</p></disp-quote><p>This analysis is no longer in the manuscript.</p><disp-quote content-type="editor-comment"><p>3. Scalability. The authors state that the method should in principle be scalable, but does that apply to interpreting the results? For example, for the V1 model it seems that you need to look at 48 figures for 4 variables, and I believe this would scale as O(n<sup>2</sup>) with n variables. This seems to require an unsustainable amount of manual work?</p></disp-quote><p>We refer the reviewer to Figure 2 and Section 3.3 for scalability analysis. The scaling analysis addresses the question of the issue of parameter discovery with EPI in high-dimensional parameter spaces.</p><p>Another important question the reviewer brings up is how well one can analyze the high-dimensional parameter distributions that EPI produces? Indeed, these distributions become more challenging to understand and visualize in high dimensions. This is where the sensitivity measurements appearing in sections 3.1, 3.4, and 3.5 can be particularly useful. Even in high dimensions, trained deep probability distributions offer tractable quantitative assessments of how parametric combinations affect the emergent property that was conditioned upon.</p><disp-quote content-type="editor-comment"><p>4. There are some very particular choices made in the applications and I wonder how general the conclusions are as a consequence. For example, in Equation 5 the authors choose an arbitrary amount of variance 0.01<sup>2</sup> â why? In the same example, why look at y=0.1 and 0.5?</p></disp-quote><p>In the current manuscript, we make sure to explain all choices of the emergent property constraints Here, we show the description of each emergent property with equations omitted.</p><p>Section 3.2, Lines 136-140</p><p>âWe stipulate that the hub neuronâs spiking frequency â denoted by statistic <italic>Ï</italic><sub>hub</sub>(<bold>x</bold>)</p><p>â is close to a frequency of 0.55Hz, between that of the slow and fast frequencies. Mathematically, we define this emergent property with two constraints: that the mean hub frequency is 0.55Hz, and that the variance of the hub frequency is moderate.â</p><p>Section 3.3, Lines 200-206</p><p>âTwo conditions are necessary and sufficient for RNNs to exhibit stable amplification [51]: real(<italic>Î»</italic><sub>1</sub>) <italic>&lt;</italic> 1 and <inline-formula><mml:math id="inf695"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>Î»</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> .[â¦] EPI can naturally condition on this emergent property Variance constraints predicate that the majority of the distribution (within two standard deviations) are within the specified ranges.â</p><p>Section 3.4, Lines 275-278</p><p>âWe quantify levels of E-population variability by studying two emergent properties where <italic>s<sub>E</sub></italic>(x;z) is the standard deviation of the stochastic <italic>E</italic>-population response about its steady state (Figure 3C). In the following analyses, we select 1Hz<sup>2</sup> variance such that the two emergent properties do not overlap in <italic>s<sub>E</sub></italic>(z;x).â</p><p>Section 3.5, Lines 331-334</p><p>âWe stipulate that accuracy be on average.75 in each task with variance.075<sup>2</sup>. 75% accuracy is a realistic level of performance in each task, and with the chosen variance, inferred models will not exhibit fully random responses (50%), nor perfect performance (100%).â</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>This paper addresses a major issue in fitting neuroscience models: how to identify the often degenerate, or nearly degenerate, set of parameters that can underlie a set of experimental observations. Whereas previous techniques often depended upon brute force explorations or special parametric forms or local linearizations to generate sets of parameters consistent with measured properties, the authors take advantage of deep generative networks to address this problem. Overall, I think this paper and the complementary submission have the potential to be transformative contributions to model fitting efforts in neuroscience. That being said, since the primary contribution is the methodology, I think the paper requires more systematic comparisons to ground truth examples to demonstrate potential strengths and weaknesses, and more focus on methodology rather than applications.</p><p>1) The authors only have a single ground-truth example where they compare to a known result (a 2x2 linear dynamical system). It would be good to show how well this method compares to results from, for example, a direct brute force grid search of a system with a strongly non-elliptical (e.g. sharply bent) shaped parameter regime and a reasonably large (e.g. 5?) number of parameters corresponding to a particular property, to see how well the derived probability distribution overlaps the brute force grid search parameters (perhaps shown via several 2-D projections).</p></disp-quote><p>We thank the reviewer for pointing out the importance of ground truth comparisons in this manuscript. In this revision, we make ground truth comparisons via analytic derivations, empirical error contours, and brute-force sampling.</p><p>Analytic comparisons: The 2x2 linear dynamical system is chosen as a worked example because it has multi-modal non-elliptical structure (Figure 1âfigure supplement 1), and its contours can be derived analytically (Figure 1âfigure supplement 2). Similarly, in Section 5.4.5, we derive the quadratic relationship between excitatory variability and input noise variability (in a simplified model) suggesting that the quadratic relationship uncovered by EPI (see Section 3.4) is correct.</p><p>Error contours: In the motivation example, we compare the EPI inferred distribution of STG conductances to hub frequency contours (Figure 1E), which show that the non-elliptical parametric structure captured by EPI is in agreement with these contours. This general region of parameter space was labeled following grid search analyses in a previous study (Gutierrez et al. 2013, Figure 2, parameter regime G).</p><p>Brute-force: The EPI inferred distribution for rapid task switching in the SC model is sharply bent (Figure 4), and matches the parameter set returned from random sampling (Figure 4âfigure supplement 5A). We note that the brute-force parameter set is actually not the ground-truth solution, because it does not obey the constraints of the emergent property as the EPI distribution does (Figure 4âfigure supplement 5B). This can explain the spurious samples in the brute-force set that are not in the EPI inferred distribution.</p><p>All EPI distributions shown in this manuscript are âvalidatedâ in the sense that they pass a hypothesis testing criteria for emergent property convergence; all EPI distributions produce their emergent properties. Finally, the underlying maximum entropy flow network (MEFN) algorithm is compared to a ground truth solution (Loaiza Ganem et al. 2017, Figure 2) by deriving ground truth from the duality of maximum entropy distributions and exponential families (see Section 5.1.3).</p><disp-quote content-type="editor-comment"><p>2) It was not obvious whether EPI actually scales well to higher dimensions and how much computation it would take (there is one claim that it 'should scale reasonably'). While I agree that examples with a small number of parameters is nice for illustration, a major issue is how to develop techniques that can handle large numbers of parameters (brute force, while inelegant, inefficient, and not producing an explicit probability distribution can do a reasonable job for small #'s of parameters). The authors should show some example of extending to larger number of parameters and do some checks to show that it appears to work. As a methodological contribution, the authors should also give some sense of how computationally intensive the method is and some sense of how it scales with size. This seems particularly relevant to, for example, trying to infer uncertainties in a large weight matrix or a non-parametric description of spatial or temporal responses or a sensory neuron (which I'm assuming this technique is not appropriate for? See point #4 below).</p></disp-quote><p>The reviewer is right to point out the importance of a scaling analysis. Please see response to Main concern #2.</p><disp-quote content-type="editor-comment"><p>3) For the STG-like example, this was done for a very simple model that was motivated by the STG but isn't based on experimental recordings. Most of the brute force models of the STG seek to fit various waveform properties of neurons and relative phases. Could the model handle these types of analyses, or would it run into problems due to either needing to specify too many properties or because properties like &quot;number of spikes per burst&quot; are discrete rather than continuous? This isn't fatal, but would be good to consider and/or note explicitly.</p></disp-quote><p>The STG subcircuit model of Gutierrez et al. 2013 is certainly less complex than other models of the STG, yet 5 connected Morris-Lecar neurons is certainly a nontrivial system. We clarify why this model is analyzed in Section 3.1 instead of more complex STG models when discussing the differences between EPI and SNPE in Discussion:</p><p>Lines 435-447</p><p>âA key difference between EPI and SNPE, is that EPI uses gradients of the emergent property throughout optimization. [â¦] In summary, choice of deep inference technique should consider emergent property complexity and differentiability, dimensionality of parameter space, and the importance of constraining the model behavior predicted by the inferred parameter distribution.â</p><disp-quote content-type="editor-comment"><p>4) The discussion should be expanded to be more specific about what problems the authors think the model is, or is not, appropriate for. Comparisons to the Goncalves article would also be helpful since users will want to know the comparative advantages/disadvantages of each method. (if the authors could coordinate running their methods on a common illustrative example, that would be cool, but not required).</p></disp-quote><p>Thank you for this recommendation. We now include substantial text in discussion devoted to this topic in addition to that quoted in the previous response (R3 concern #3).</p><p>Lines 427-447</p><p>âMethodology for statistical inference in circuit models has evolved considerably in recent years. [â¦] In summary, choice of deep inference technique should consider emergent property complexity and differentiability, dimensionality of parameter space, and the importance of constraining the model behavior predicted by the inferred parameter distribution.â</p><disp-quote content-type="editor-comment"><p>5) Given that the paper is heavily a (very valuable!) methods paper for a general audience, the method should be better explained both in the main text and the supplement. Some specific ones are below, but the authors should more generally send the paper to naÃ¯ve readers to check what is/is not well explained.</p><p>â Figure 1 is somewhat opaque and also has notational issues (e.g. omega is the frequency but also appears to be the random input sample).</p></disp-quote><p>Figure 1 has been completely revised.</p><disp-quote content-type="editor-comment"><p>â For the general audience of eLife, panels C and D are not well described individually or well connected to each other and don't illustrate or describe all of the relevant variables (including what q0 is and what x is).</p></disp-quote><p>Agreed. In the new figure, we clearly depict <bold>z</bold> as parameters and <bold>x</bold> as circuit activity. We keep the vertical directionality of panel D exclusively for the deep generative process of the deep probability distribution (where <italic>q</italic><sub>0</sub> is replaced and shown explicitly as an isotropic gaussian input to the deep network). The horizontal directionality shared between panels D and E reflect the procedure of theoretical neuroscience described in Section 3.1.</p><disp-quote content-type="editor-comment"><p>â In Equation 2 (and also in the same equation in the supplement), it was not immediately obvious what the expectation was taken over.</p></disp-quote><p>We take care to always indicate the variables over which expectations are taken in the updated manuscript.</p><disp-quote content-type="editor-comment"><p>â The authors don't specific the distribution of w (it's referred to only as 'a simple random variable', which is not clear).</p></disp-quote><p>It is clarified in the text that this simple initial distribution transformed by the deep neural network is an isotropic gaussian.</p><disp-quote content-type="editor-comment"><p>â It was also sometimes hard to quickly find in the text basic, important quantities like what z was for a given simulation.</p></disp-quote><p>We have made sure to make this explicit in each section of the paper.</p><disp-quote content-type="editor-comment"><p>â The augmented Lagrangian optimization was not well explained or motivated. There is a reference to m=absolute value(mu) but I didn't see m in the above equation.</p></disp-quote><p>Lines 767-769</p><p>âTo run this constrained optimization, we use an augmented lagrangian objective, which is the standard approach for constrained optimization [70], and the approach taken to fit Maximum Entropy Flow Networks (MEFNs) [38].&quot;</p><disp-quote content-type="editor-comment"><p>â Using mu to describe a vector that includes means and variances is confusing notation since mu often denotes means.</p></disp-quote><p>Agreed. Throughout the updated main text, we only describe emergent properties through explicit mean and variance constraints as in Equation 11. We reserve the mean parameterization <italic>Âµ</italic><sub>opt</sub> of the maximum entropy solution of the EPI optimization for technical details in methods. The difference between <italic>Âµ</italic> and <italic>Âµ</italic><sub>opt</sub> is described in Section 5.1.3.</p><disp-quote content-type="editor-comment"><p>â It would be helpful to have a pseudo-code 'Algorithm' figure or section of the text.</p></disp-quote><p>We provide pseudocode for the EPI optimization in Algorithm 1, which mirrors the pseudocode found in the paper describing the underlying algorithm for MEFN [38].</p></body></sub-article></article>