<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">90841</article-id><article-id pub-id-type="doi">10.7554/eLife.90841</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.90841.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Multisensory integration operates on correlated input from unimodal transient channels</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Parise</surname><given-names>Cesare V</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0000-6092-561X</contrib-id><email>cesare.parise@liverpool.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Ernst</surname><given-names>Marc O</given-names></name><email>marc.ernst@uni-ulm.de</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xs57h96</institution-id><institution>Department of Psychology, University of Liverpool</institution></institution-wrap><addr-line><named-content content-type="city">Liverpool</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02hpadn98</institution-id><institution>Cognitive Neuroscience Department, University of Bielefeld (DE)</institution></institution-wrap><addr-line><named-content content-type="city">Bielefeld</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/032000t02</institution-id><institution>Applied Cognitive Psychology, University of Ulm (DE)</institution></institution-wrap><addr-line><named-content content-type="city">Ulm</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Groh</surname><given-names>Jennifer M</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00py81415</institution-id><institution>Duke University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>22</day><month>01</month><year>2025</year></pub-date><volume>12</volume><elocation-id>RP90841</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-08-01"><day>01</day><month>08</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-07-07"><day>07</day><month>07</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.07.07.548109"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-12-15"><day>15</day><month>12</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.90841.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-09-05"><day>05</day><month>09</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.90841.2"/></event></pub-history><permissions><copyright-statement>© 2023, Parise and Ernst</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Parise and Ernst</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-90841-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-90841-figures-v2.pdf"/><abstract><p>Audiovisual information reaches the brain via both sustained and transient input channels, representing signals’ intensity over time or changes thereof, respectively. To date, it is unclear to what extent transient and sustained input channels contribute to the combined percept obtained through multisensory integration. Based on the results of two novel psychophysical experiments, here we demonstrate the importance of the transient (instead of the sustained) channel for the integration of audiovisual signals. To account for the present results, we developed a biologically inspired, general-purpose model for multisensory integration, the multisensory correlation detectors, which combines correlated input from unimodal transient channels. Besides accounting for the results of our psychophysical experiments, this model could quantitatively replicate several recent findings in multisensory research, as tested against a large collection of published datasets. In particular, the model could simultaneously account for the perceived timing of audiovisual events, multisensory facilitation in detection tasks, causality judgments, and optimal integration. This study demonstrates that several phenomena in multisensory research that were previously considered unrelated, all stem from the integration of correlated input from unimodal transient channels.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>sensory processing</kwd><kwd>multisensory integration</kwd><kwd>psychophysics</kwd><kwd>computational modeling</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>251654672-TRR 161</award-id><principal-award-recipient><name><surname>Ernst</surname><given-names>Marc O</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Psychophysical experiments and computational modeling demonstrate the importance of transient, instead of sustained, channels for the integration of audiovisual signals.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Audiovisual stimuli naturally unfold over time, and their structures alternate intervals during which the signals remain relatively constant (such as the steady luminance of a light bulb) to sudden moments of change (when the bulb lights up). To efficiently process the temporal structure of incoming signals, the sensory systems of mammals (and other animal classes) rely on separate channels, encoding stimulus intensity through either sustained or transient responses (<xref ref-type="bibr" rid="bib6">Benucci et al., 2007</xref>; <xref ref-type="bibr" rid="bib20">Kim et al., 2011</xref>; <xref ref-type="bibr" rid="bib7">Breitmeyer and Ganz, 1976</xref>; <xref ref-type="bibr" rid="bib34">Qin et al., 2007</xref>; <xref ref-type="bibr" rid="bib35">Recanzone, 2000</xref>; <xref ref-type="bibr" rid="bib19">Ikeda and Wright, 1972</xref>). These channels are known to originate early in the processing hierarchy (<xref ref-type="bibr" rid="bib35">Recanzone, 2000</xref>; <xref ref-type="bibr" rid="bib19">Ikeda and Wright, 1972</xref>), with sustained ones responding with constant neural firing to static input intensity, whereas transient channels respond with increased firing to any variations in input intensity (i.e., both increments and decrements, <xref ref-type="fig" rid="fig1">Figure 1a</xref>). On a functional level, the response of sustained and transient channels represents distinct dynamic stimulus information. Specifically, sustained responses represent the intensity of the stimulus, while transient responses represent changes in stimulus intensity. In terms of frequency response, sustained channels can be characterized as low-pass temporal filters (<xref ref-type="disp-formula" rid="equ11">Equation 10</xref>), highlighting the low-frequency signal components. Transient channels, on the other hand, have a higher spectral tuning and can be characterized as band-pass temporal filters (<xref ref-type="disp-formula" rid="equ2">Equation 1</xref>), signaling events, or moments of stimulus change (<xref ref-type="bibr" rid="bib40">Stigliani et al., 2017</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Sustained vs. transient channels.</title><p>(<bold>a</bold>) Responses of sustained and transient channels to onset and offset step stimuli, and periodic signals comprising sequences of onsets and offsets. Note that while the sustained channels closely follow the intensity profile of the input stimuli, transient channels only respond to changes in stimulus intensity, and such a response is always positive, irrespective of whether stimulus intensity increases or decreases. Therefore, when presented with periodic signals, while the sustained channels respond at the same frequency as the input stimulus (frequency following), transient channels respond at a frequency that is twice that of the input (frequency doubling). (<bold>b</bold>) Synchrony as measured from cross-correlation between pairs of step stimuli, as seen through sustained (top) and transient (bottom) channels (transient and sustained channels are simulated using <xref ref-type="disp-formula" rid="equ2 equ11">Equations 1; 10</xref>, respectively). Note how synchrony (i.e., correlation) for sustained channels peaks at zero lag when the intensity of the input stimuli changes in the same direction, whereas it is minimal at zero lag when the steps have opposite polarities (negatively correlated stimuli). Conversely, being insensitive to the polarity of intensity changes, synchrony for transient channels always peaks at zero lag. (<bold>c</bold>) Synchrony (i.e., cross-correlation) of periodic onsets and offset stimuli as seen from sustained and transient channels. While synchrony peaks once (at zero phase shift) for sustained channels, it peaks twice for transient channels (at zero and pi radians phase shift), as a consequence of its frequency-doubling response characteristic. (<bold>d</bold>) Experimental apparatus: participants sat in front of a black cardboard panel with a circular opening, through which audiovisual stimuli were delivered by a white LED and a loudspeaker. (<bold>e</bold>) Predicted effects of experiments 1 and 2 depending on whether audiovisual integration relies on transient or sustained input channels. The presence of the effects of interest in both experiments or the lack thereof indicates an inconclusive result, not interpretable in the light of our hypotheses.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-fig1-v2.tif"/></fig><p>Changing information over time is also critical for multisensory perception (<xref ref-type="bibr" rid="bib39">Stein, 2012</xref>): when two signals from different modalities are caused by the same underlying event, they usually covary over time (like firecrackers’ pops and blazes). A growing body of literature has now investigated human sensitivity and adaptation to temporal lags across the senses (<xref ref-type="bibr" rid="bib43">Vroomen and Keetels, 2010</xref>), and it is well established that both multisensory illusions (<xref ref-type="bibr" rid="bib37">Sekuler et al., 1997</xref>; <xref ref-type="bibr" rid="bib36">Samad et al., 2018</xref>; <xref ref-type="bibr" rid="bib42">van Wassenhove et al., 2007</xref>) and Bayesian-optimal cue integration (e.g., <xref ref-type="bibr" rid="bib28">Parise et al., 2012</xref>) critically depend on synchrony and temporal correlation across the senses. However, multisensory integration does not operate on the raw physical signals; these are systematically transformed during transduction and early neural processing. Therefore to understand multisensory integration, it is critical to figure out how unisensory signals are processed before feeding into the integration stage. Surprisingly, this fundamental question has received little attention in multisensory research.</p><p>Current evidence, however, suggests a prominence of transient channels in the percept resulting from multisensory integration. For example, <xref ref-type="bibr" rid="bib4">Andersen and Mamassian, 2008</xref> found that task-irrelevant increments or decrements in sound intensity equally facilitated the detection of both increments and decrements in the lightness of a visual display. Critically, such an effect only occurred when changes in the two modalities occurred in approximate temporal synchrony. Based on the independence of polarity of this crossmodal facilitation, where intensity increments and decrements produced similar perceptual benefits, the authors concluded that audiovisual integration relies primarily on unsigned transient stimulus information. The role of transient channels in audiovisual perception is further supported by fMRI evidence: <xref ref-type="bibr" rid="bib46">Werner and Noppeney, 2011</xref> found that audiovisual interactions in the human brain only occurred during stimulus transitions and demonstrated that transient onset and offset responses could be dissociated both anatomically and functionally (see also <xref ref-type="bibr" rid="bib18">Herdener et al., 2009</xref>). While these studies demonstrate the dominance of transient over sustained temporal channels in, for example, detection tasks as studied by <xref ref-type="bibr" rid="bib4">Andersen and Mamassian, 2008</xref>, or the pattern of neural responses during passive observation of audiovisual stimuli as in <xref ref-type="bibr" rid="bib18">Herdener et al., 2009</xref>, to date, it is still unknown to what extent transient and sustained channels affect the perceived timing of audiovisual events – such as the subjective synchrony of visual and auditory signals, which is arguably the primary determinant of multisensory integration.</p><p>To understand the effect of transient versus sustained channels in multisensory perception, we must first focus on the difference between their unimodal responses. For that, we can consider stimuli consisting of steps in stimulus intensity (e.g., <xref ref-type="bibr" rid="bib4">Andersen and Mamassian, 2008</xref>). A schematic representation is shown in <xref ref-type="fig" rid="fig1">Figure 1a</xref>: while onset and offset step stimuli trigger identical unsigned transient responses, sustained responses differ across conditions. That is, given that sustained channels represent the magnitude of the stimulus, responses to onset and offset stimuli are negatively correlated. In signal processing, Pearson correlation is commonly used to assess the synchrony of two related signals, with a higher correlation representing higher synchrony (and similarity, <xref ref-type="bibr" rid="bib44">Wei, 2006</xref>). Therefore, we can hypothesize that if multisensory time perception relies on sustained input channels, positively correlated audiovisual stimuli (e.g., onset or offset stimuli in both modalities) should be perceived as more synchronous than negatively correlated stimuli (i.e., onset in one modality, offset in the other). Conversely, if audiovisual synchrony relies on unsigned transients, positively and negatively correlated stimuli should appear equally synchronous (<xref ref-type="fig" rid="fig1">Figure 1b</xref>).</p><p>This hypothesis will be tested in experiment 1, where systematic differences in perceived synchrony based on whether audiovisual signals are positively or negatively correlated would provide evidence for a dominant role of sustained input channels in audiovisual temporal processing. A lack of systematic differences driven by stimulus correlation, however, would not necessarily imply a transient nature of audiovisual temporal processing: this would require additional evidence from an inverse experiment, one in which the transient hypothesis predicts an effect that the sustained hypothesis does not (<xref ref-type="fig" rid="fig1">Figure 1e</xref>). For that, we can consider audiovisual stimuli consisting of periodic onsets and offsets (e.g., square-wave amplitude modulation, see <xref ref-type="fig" rid="fig1">Figure 1c</xref>). While sustained channels respond at the same rate as the input, transient responses have a rate double that of the input signals. This phenomenon, known as frequency doubling, is commonly considered a hallmark of the contribution of transient channels in sensory neuroscience (e.g., <xref ref-type="bibr" rid="bib20">Kim et al., 2011</xref>).</p><p>Frequency doubling, therefore, is a handle to assess the contribution of transient and sustained channels in audiovisual perception. Consider audiovisual stimuli defined by square-wave amplitude modulations with a parametric manipulation of crossmodal phase shift (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). If audiovisual temporal integration relies on the correlation between sustained input channels, we can predict perceived audiovisual synchrony (i.e., correlation, <xref ref-type="bibr" rid="bib44">Wei, 2006</xref>) to peak just once: at zero phase shifts. Conversely, if multisensory temporal integration relies on transient channels, perceived audiovisual synchrony should peak twice: at 0 and 180° phase lag (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). Such a frequency-doubling phenomenon can be easily assessed psychophysically by measuring reported simultaneity as a function of audiovisual lags. Therefore, in a second psychophysical study, we rely on frequency doubling in audiovisual synchrony perception to assess whether multisensory integration relies on transient or sustained input channels.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Experiment 1: Step stimuli</title><p>To probe the effect of the correlation between unimodal step stimuli on the perceived timing of audiovisual events, eight participants (age range 22–35 years, four females) observed audiovisual signals consisting of lightness and acoustic intensity increments (on-step) and decrements (off-step). On-steps and off-steps were paired in all possible combinations, giving rise to four experimental conditions (both modalities on, both off, vision on with audio off, and vision off with audio on, see <xref ref-type="fig" rid="fig1">Figure 1b</xref>). The lag between visual and acoustic step events was parametrically manipulated using the method of constant stimuli (15 steps, between –0.4 and 0.4 s). After the stimulus presentation, participants performed a temporal order judgment (TOJ, which event came first, sound or light?) or a simultaneity judgment (SJ, were the stimuli synchronous or not?). TOJ and SJ tasks were run in different sessions occurring on different days. Although our original hypotheses (see, <xref ref-type="fig" rid="fig1">Figure 1b–e</xref>) do not make specific predictions for the TOJ task, for completeness we run such an experiment anyway, as its inclusion provides a more stringent test for our model (see ‘Modeling’ section).</p><p>The audiovisual display used to deliver the stimuli consisted of a white LED in an on- (high-luminance) and off-state (low-luminance), and acoustic white noise, also in either on- (quiet) or off-state (loud). Sounds came from a speaker located behind the LED, and both the speaker and the LED were controlled using an audio interface to minimize system delay (<xref ref-type="fig" rid="fig1">Figure 1d</xref>, see <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>). A white, sound transparent cloth was placed in front of the LED so that when the light was on, participants saw a white disk of 13° in diameter. Overall, each participant provided 600 responses in the TOJ task (15 lags, 10 repetitions, and 4 conditions) and additional 600 responses in the SJ task (again, 15 lags, 10 repetitions, and 4 conditions). The experiment was run in a dark, sound-attenuated booth, and the position of participants’ heads was controlled using a chin- and a headrest. Participants were paid 8 euros/hr. The experimental procedure was approved by the ethics committee of the University of Bielefeld (ref. no. 2015-136) and was conducted in accordance with the Declaration of Helsinki.</p></sec><sec id="s2-2"><title>Results</title><p>To assess whether different combinations of on-step and off-step stimuli (and correlation thereof) elicit measurable psychophysical effects, we estimated both the point and the window of subjective simultaneity (PSS and WSS, i.e., the delay at which audiovisual stimuli appear simultaneous, and the width of the window of simultaneity). For that, we fitted psychometric curves to both TOJ and SJ data, independently for each condition. Specifically, following standard procedures (<xref ref-type="bibr" rid="bib27">Parise and Spence, 2009</xref>), TOJs were statistically modeled as cumulative Gaussians, with four free parameters (intercept, slope, and two asymptotes, <xref ref-type="fig" rid="fig2">Figure 2a</xref>). The PSS was calculated as the lag at which TOJs were at chance level, whereas the WSS was calculated as the half-difference between the lags eliciting 0.75 and 0.25 probability of audio-first responses. The SJs data, instead, were modeled as the difference of two cumulative Gaussians (<xref ref-type="bibr" rid="bib47">Yarrow et al., 2011</xref>), leading to asymmetric bell-shaped psychometric functions (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). The PSS was calculated as the lag at which perceived simultaneity was maximal, whereas the WSS was calculated as the half-width at half-maximum. Results are summarized in <xref ref-type="fig" rid="fig2">Figure 2c and d</xref> (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for individual data, and ‘Materials and methods’ for a correlation analysis of the PSS and WSS measured from TOJs vs. SJs).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Experiment 1: results.</title><p>(<bold>a</bold>) Responses in the temporal order judgment (TOJ) task and psychometric fits (averaged across participants) for the four experimental conditions. (<bold>b</bold>) Responses in the simultaneity judgment (SJ) task and psychometric fits (averaged across participants) for the four experimental conditions. Each dot in (a, b) corresponds to 80 trials. (<bold>c</bold>) Window of subjective simultaneity for each condition and task. (<bold>d</bold>) Point of subjective simultaneity for each condition and task.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Data of the temporal order judgment (TOJ) task.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-90841-fig2-data1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig2sdata2"><label>Figure 2—source data 2.</label><caption><title>Data of the simultaneity judgment (SJ) task.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-90841-fig2-data2-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Results and psychometric fits of experiment 1.</title><p>Data from different observers are represented in different rows. (<bold>a</bold>) represents the data of the temporal order judgment (TOJ) task. (<bold>b</bold>) represents the data of the simultaneity judgment (SJ) task. The icons on top represent the different conditions. Each dot corresponds to 10 trials. (<bold>c</bold>) represents the scatterplot of the point of subjective simultaneity (PSS) measured with the TOJ plotted against the PSS measured with the SJ. (<bold>d</bold>) represents the scatterplot of the window of subjective simultaneity (WSS) measured with the TOJ plotted against the PSS measured with the SJ. Each dot in (c, d) represents the PSS or WSS from one condition and observer (i.e., there are four dots for each observer, one per condition); datapoints from the same participant are linked by a gray line.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-fig2-figsupp1-v2.tif"/></fig></fig-group><p>To assess whether the four experimental conditions differ in the PSS and WSS, we run both nonparametric Friedman tests, with Bonferroni correction for multiple testing, and Bayesian repeated-measures ANOVA. Neither PSS nor WSS statistically differed across conditions, and this was true for both the TOJ and SJ data. Results are summarized in <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>.</p></sec><sec id="s2-3"><title>Conclusion</title><p>The lack of a difference across conditions found in experiment 1 implies that the on-step and off-step stimuli induced similar perceptual responses. Based on our original hypothesis, the present results argue against the dominance of sustained input channels in the perception of audiovisual events, as the synchrony (i.e., Pearson correlation, <xref ref-type="bibr" rid="bib44">Wei, 2006</xref>) of sustained signals would otherwise have been affected by the crossmodal combination of on-step and off-step stimuli. As previously mentioned, while a lack of difference across conditions in experiment 1 is necessary to infer a dominance of transient channels in audiovisual time perception, this evidence is not sufficient on its own. For that, we would need additional evidence in the form of the presence of systematic effects, which are predicted from the operating principles of the transient channels, but not for sustained ones. Experiment 2 was designed to fulfill such a requirement as it predicts a frequency-doubling effect in perceived synchrony for transient but not for sustained input channels.</p></sec><sec id="s2-4"><title>Experiment 2: Periodic stimuli</title><p>Participants observed a periodic audiovisual stimulus consisting of a square-waved intensity envelope and performed a force-choice SJ task. The carrier visual and auditory stimuli consisted of pink noise, delivered by a speaker and an LED (<xref ref-type="fig" rid="fig1">Figure 1d</xref>), which were switched on and off periodically in a square-wave fashion with a period of 2 s for a total duration of 6 s (so that three full audiovisual cycles were presented on each trial, <xref ref-type="fig" rid="fig3">Figure 3a</xref>). To prevent participants from just focusing on the start and endpoint of the signals, during the intertrial interval, the visual and auditory stimuli were set to a pedestal intensity level, so that during the trial the square-wave modulation was gradually ramped on and off, following a raised cosine profile with a duration of 6 s (<xref ref-type="fig" rid="fig3">Figure 3a</xref>; see also <xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5C</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Experiment 2: stimuli and results.</title><p>(<bold>a</bold>) Schematic representation of the periodic stimuli. (<bold>b</bold>) Frequency domain representation of the psychometric function: note the amplitude peak at two cycles per period. Errorbars represent the 99% CIs. The inset represents the phase angle of the two cycles per period frequency component for each participant (thin lines) and the average phase (arrow). (<bold>c</bold>) Results of experiment 2 and psychometric fit, averaged across all participants. Each dot corresponds to 75 trials.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Data of the simultaneity judgment (SJ) task.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-90841-fig3-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Results and psychometric fits of experiment 2.</title><p>Data from different observers are represented in different rows. The first four columns represent the data in polar coordinates, whereas the second column represent the same dataset in Cartesian coordinates. The counter-clockwise rotation of the polar psychometric functions indicates that maximum perceived synchrony across the senses occurs when vision changes slightly before audition. Each dot corresponds to 15 trials. The last column represents the psychometric curve in the frequency domain: note the peak at 2 cpp in every participant, indicating the frequency-doubling effect. The errorbars in the last column represent the 99% CIs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-fig3-figsupp1-v2.tif"/></fig></fig-group><p>The lag across the sensory signals consisted of relative phase shifts of the two square waves, while the raised cosine window remained constant (and synchronous across the senses). Audiovisual phase shift was manipulated according to the method of constant stimuli, and a full cycle was sampled in 40 steps. Each lag was presented 15 times to yield a total of 600 trials per participant. Besides the relative phase shifts across the multisensory signals, also the phase offset of the stimulus as a whole varied pseudorandomly across trials (spanning a full period sampled in 15 steps, one per repetition). Therefore, each time a given lag (phase shift) was tested, also the phase offset of the signals changed. Given that we expect the frequency-doubling effect to be strong in size (i.e., synchrony at pi-phase shift should approach 0 under the sustained hypothesis, and 1 under the transient hypothesis), and that we collected a large number of responses (n = 600 per observer), a pool of five participants (age range 25–35 years, three females) was large enough to reliably assess its presence or lack thereof. Participants were paid 8 euros/hr, and the experimental procedure was approved by the ethics committee of the University of Bielefeld and was conducted in accordance with the Declaration of Helsinki.</p></sec><sec id="s2-5"><title>Results</title><p>Due to the periodic nature of the stimuli and hence of the experimental manipulation of lag, the resulting psychometric functions are also expected to be periodic, with an alternation of phase shifts yielding higher and lower reported synchrony. In this context, the evidence of frequency doubling can be measured from the number of oscillations in perceived simultaneity for a full cycle of phase shifts between the audiovisual stimuli.</p><p>Given the periodicity of the stimuli, it is natural to analyze the psychometric functions in the frequency domain. Therefore, to get a nonparametric and assumption-free estimate of the frequency of oscillations in the data, we ran a Fourier analysis on the empirical psychometric curves. If human responses are driven by transient input channels, we predict a peak at two cycles per period (cpp., i.e., frequency doubling), otherwise, there should be a single peak at 1 cpp. The power spectrum of the psychometric functions shows a sharp peak at a frequency of 2 cpp in all observers, thereby indicating the presence of the hypothesized frequency-doubling effect (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). To assess whether the amplitude at 1 and 2 cpp is statistically different at the individual observer level, we used a bootstrap procedure to estimate the CIs of the response spectrum. For that, we used the binomial distribution and simulated 50,000 psychometric functions, on which we performed a frequency analysis to obtain the 99% CIs of the amplitudes. The results for both the aggregate observer and the individual data show a clear separation between the CIs of the amplitude at the frequencies of 1 and 2 cpp and demonstrate that the 2 cpp is indeed the dominant frequency, whereas the amplitude at 1 cpp is close to 0 and it is no different from the background noise (i.e., higher harmonics; see <xref ref-type="fig" rid="fig3">Figure 3b</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>Given that the frequency analyses revealed a 2 cpp peak for all participants, we used this information to fit a sinusoidal psychometric function to the psychophysical data. Under the assumption of late Gaussian noise, the psychometric function can be written as<disp-formula id="equ1"><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>c</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>⋅</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⋅</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>+</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle p\left (synch \, \big | \, \phi \right)=normcdf\left (\alpha +\beta \cdot sin\left (f\cdot \phi +\theta \right)\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>normcdf</italic> represents the cumulative normal distribution function, the parameter <inline-formula><alternatives><mml:math id="inf1"><mml:mi>α</mml:mi></mml:math><tex-math id="inft1">\begin{document}$\alpha $\end{document}</tex-math></alternatives></inline-formula> is the bias term, <inline-formula><alternatives><mml:math id="inf2"><mml:mi>β</mml:mi></mml:math><tex-math id="inft2">\begin{document}$\beta $\end{document}</tex-math></alternatives></inline-formula> the sensitivity, <inline-formula><alternatives><mml:math id="inf3"><mml:mi>ϕ</mml:mi></mml:math><tex-math id="inft3">\begin{document}$\phi $\end{document}</tex-math></alternatives></inline-formula> the phase-lag (range = [0, 2<inline-formula><alternatives><mml:math id="inf4"><mml:mi>π</mml:mi></mml:math><tex-math id="inft4">\begin{document}$\pi $\end{document}</tex-math></alternatives></inline-formula>]), <italic>f</italic> the frequency of oscillations, and <inline-formula><alternatives><mml:math id="inf5"><mml:mi>θ</mml:mi></mml:math><tex-math id="inft5">\begin{document}$\theta $\end{document}</tex-math></alternatives></inline-formula> is the phase offset (this shifts peak synchrony toward either positive or negative phases). Based on the Fourier analyses, we fixed the frequency (<inline-formula><alternatives><mml:math id="inf6"><mml:mi>f</mml:mi></mml:math><tex-math id="inft6">\begin{document}$f$\end{document}</tex-math></alternatives></inline-formula>) to 2 cpp, <inline-formula><alternatives><mml:math id="inf7"><mml:mi>θ</mml:mi></mml:math><tex-math id="inft7">\begin{document}$\theta $\end{document}</tex-math></alternatives></inline-formula> to –0.441 radians, and kept the linear coefficients – <inline-formula><alternatives><mml:math id="inf8"><mml:mi>α</mml:mi></mml:math><tex-math id="inft8">\begin{document}$\alpha $\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf9"><mml:mi>β</mml:mi></mml:math><tex-math id="inft9">\begin{document}$\beta $\end{document}</tex-math></alternatives></inline-formula> – as free parameters, which were fitted to the psychophysical data using an adaptive Bayesian algorithm (<xref ref-type="bibr" rid="bib1">Acerbi and Ma, 2017</xref>).</p><p>Overall, the fitted psychometric curves match the empirical data with a high goodness of fit (median r<sup>2</sup> = 0.9321). Such an agreement between psychometric curves and empirical data achieved with the frequency parameter constrained by the Fourier analyses further indicates a reliable frequency-doubling effect in all our participants.</p></sec><sec id="s2-6"><title>Conclusion</title><p>The results of experiment 2 clearly demonstrate the existence of a frequency-doubling effect in the perceived simultaneity of periodic audiovisual stimuli. Given that the frequency-doubling effect is only expected to occur if audiovisual synchrony is computed over unsigned transient input channels, the present results support a dominance of transient input channels in multisensory time perception. These results complement the conclusions of experiment 1 and together demonstrate the key role of transient input channels in audiovisual integration (see <xref ref-type="fig" rid="fig1">Figure 1E</xref>).</p></sec><sec id="s2-7"><title>Modeling</title><p>To account for multisensory integration, we have previously proposed a computational model, the multisensory correlation detector (MCD, <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>) that exploits the temporal correlation between the senses to solve the correspondence problem, detect simultaneity and lag across the senses, and perform Bayesian-optimal multisensory integration. Based on the Hassenstein–Reichardt motion detector (<xref ref-type="bibr" rid="bib17">Hassenstein and Reichardt, 1956</xref>; <xref ref-type="bibr" rid="bib14">Fujisaki and Nishida, 2005</xref>; <xref ref-type="bibr" rid="bib15">Fujisaki and Nishida, 2007</xref>), the core of the MCD is composed of two mirror-symmetric subunits, each multiplying visual and auditory input after applying a low-pass filter to each of them. As a consequence of this asymmetric filtering, each subunit is selectively tuned to different temporal order of the signals (i.e., vision vs. audition lead). The outputs of the two subunits are then combined in different ways to detect the correlation and lag of multisensory signals, respectively. Specifically, correlation is calculated by multiplying the outputs of the subunits, hence producing an output (<inline-formula><alternatives><mml:math id="inf10"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft10">\begin{document}$MCD_{Corr}$\end{document}</tex-math></alternatives></inline-formula>) whose magnitude represents the correlation between the signals (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). Temporal lag is instead detected by subtracting the outputs of the subunits, like in the classic Hassenstein–Reichardt detector (<xref ref-type="bibr" rid="bib17">Hassenstein and Reichardt, 1956</xref>). This yields an output (<inline-formula><alternatives><mml:math id="inf11"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft11">\begin{document}$MCD_{Lag}$\end{document}</tex-math></alternatives></inline-formula>) with a sign that represents the temporal order of the signals (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). While a single MCD unit can only perform temporal integration of multisensory input, a population of MCD units, each receiving input from spatially tuned receptive fields (<xref ref-type="fig" rid="fig4">Figure 4d</xref>), followed by divisive normalization (<xref ref-type="fig" rid="fig4">Figure 4e</xref>, see <xref ref-type="bibr" rid="bib26">Ohshiro et al., 2011</xref>), can perform Bayesian-optimal spatial cue integration (e.g., see <xref ref-type="bibr" rid="bib3">Alais and Burr, 2004</xref>) for audiovisual source localization (<xref ref-type="fig" rid="fig4">Figure 4f</xref>, see <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref> and <xref ref-type="bibr" rid="bib32">Parise, 2025</xref>, for details).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Multisensory correlation detector (MCD) model.</title><p>(<bold>a</bold>) Model schematics: the impulse-response functions of the channels are represented in the small boxes, and call-outs represent the transfer functions. (<bold>b</bold>) Lag detector step responses as a function of the lag between visual and acoustic steps. (<bold>c</bold>) Correlation detector responses as a function of the lag between visual and acoustic steps. (<bold>d</bold>) Population of MCD units, each receiving input from spatiotopic receptive fields. (<bold>e</bold>) Normalization, where the output of each unit is divided by the sum of the activity of all units. (<bold>f</bold>) Optimal integration of audiovisual spatial cues, as achieved using a population of MCDs with divisive normalization. Lines represent the likelihood functions for the unimodal and bimodal stimuli; dots represent the response of the MCD model, which is indistinguishable from the bimodal likelihood function.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-fig4-v2.tif"/></fig><p>In its original form, the input to the MCD consisted of sustained unimodal channels, modeled as low-pass temporal filters (see also, <xref ref-type="bibr" rid="bib8">Burr et al., 2009</xref>; <xref ref-type="bibr" rid="bib47">Yarrow et al., 2011</xref>). Although such a model could successfully account for the integration of trains of audiovisual impulses, the MCD cannot replicate the present results: for that, we first need to feed the model with unsigned transient unimodal input channels. Therefore, we replaced the front-end low-pass filters with band-pass temporal filters (to detect transients) followed by a squaring non-linearity (to get the unsigned transient; <xref ref-type="bibr" rid="bib40">Stigliani et al., 2017</xref>), and tested the model against the results of both experiments 1 and 2, plus a variety of previously published psychophysical studies.</p><p>The equations of the revised MCD model are reported in the ‘Materials and methods’. Just like the original version, the revised MCD model has three free parameters, representing the time constants of the two band-pass filters (one per modality) and that of the low-pass temporal filters of the subunits of the detector. Given that the tuning of the time constants of the model depends both on physiological constraints and on the temporal profile of the input stimuli (see <xref ref-type="bibr" rid="bib33">Pesnot et al., 2022</xref>), here we used the data from experiments 1 and 2 to constrain the temporal constants for the simulation of datasets consisting of either step stimuli or variations thereof (e.g., periodic stimuli), while for the simulation of experiments relying on stimuli with faster temporal rates, we constrained the temporal constants using data from <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>. Details on parameter fitting are reported in the ‘Materials and methods’. Given that in a previous study (<xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>) we have already shown that alternative models for audiovisual integration are not flexible enough to reproduce the wide gamut of psychophysical data successfully accounted for by our model, here we only consider the MCD model (in its revised form). A MATLAB script with the full implementation of the revised MCD model is available in the ‘Materials and methods’.</p><sec id="s2-7-1"><title>Simulation of experiments 1 and 2</title><p>To test whether an MCD that receives input from unimodal transient channels could replicate the results of experiments 1 and 2, we fed the stimuli to the detector and used the <inline-formula><alternatives><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math><tex-math id="inft12">\begin{document}$\overline{MCD_{corr}}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ9">Equation 8</xref>) output to model SJs, and the <inline-formula><alternatives><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math><tex-math id="inft13">\begin{document}$\overline{MCD_{lag}}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ10">Equation 9</xref>) for the TOJs (see <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>). Given that the psychophysical data consisted of response probabilities (i.e., probability of ‘synchronous’ responses for the SJ and probability of ‘audio first’ responses in the TOJ), whereas the outputs of the model are continuous variables expressed in arbitrary units, we used a GLM with a probit link function to transform the output of the MCD into probabilities (<xref ref-type="fig" rid="fig5">Figure 5a</xref>, see also <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref> for a detailed description of the approach). The linear coefficients (i.e., slope and intercept) were fitted separately for each condition and task so that each psychometric curve had two free parameters for the GLM. The three parameters defining the temporal constants of the MCD model, instead, were fitted using all data from experiments 1 and 2 combined using an adaptive Bayesian algorithm (<xref ref-type="bibr" rid="bib1">Acerbi and Ma, 2017</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Multisensory correlation detector (MCD) simulations of experiments 1 and 2.</title><p>(<bold>a</bold>) Schematics of the observer model, which receives input from one MCD unit to generate a behavioral response. The output of the MCD unit is integrated over a given temporal window (whose width depends on the duration of the stimuli) and corrupted by late additive noise before being compared to an internal criterion to generate a binary response. Such a perceptual decision-making process is modeled using a generalized linear model (GLM), depending on the task, the predictors of the GLM were either <inline-formula><alternatives><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math><tex-math id="inft14">\begin{document}$\overline{MCD_{corr}}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ9">Equation 8</xref>) or <inline-formula><alternatives><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math><tex-math id="inft15">\begin{document}$\overline{MCD_{lag}}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ10">Equation 9</xref>). (<bold>b</bold>) Responses for the temporal order judgment (TOJ) task of experiment 1 (dots) and model responses (red curves). (<bold>c</bold>) Responses for the simultaneity judgment (SJ) task of experiment 1 (dots) and model responses (blue curves). (<bold>d</bold>) Experiment 2 human (dots) and model responses (blue curve). (<bold>e</bold>) Scatterplot of human vs. model responses for both experiments.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-fig5-v2.tif"/></fig><p>Overall, the model could tightly replicate the results of our experiments (<xref ref-type="fig" rid="fig5">Figure 5b–d</xref>): the Pearson correlation between the psychophysical data and model responses computed across all conditions and participants was 0.99 for the SJ task of experiment 1, 0.99 for the TOJ task of experiment 1, and 0.97 for experiment 2 (<xref ref-type="fig" rid="fig5">Figure 5e</xref>, see <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>). For comparison, the correlation between the data and the psychometric fits for experiment 1 (i.e., cumulative Gaussians for the TOJs and difference of two cumulative Gaussians for the SJ) was 0.97 and 0.98 for experiment 2; however, the psychometric fits required nearly twice as many free parameters compared to the model fits and do not account for the generative process that give rise to the observed data. Importantly, just like human responses, the responses of the revised MCD model were nearly identical across all conditions in experiment 1, whereas in experiment 2 they displayed a clear frequency-doubling effect. Furthermore, unlike the psychometric fits, which require to specify a priori the shape of the fitting function (e.g., sigmoid, bell, or periodic), the MCD model provides an output without specifying a priori any shape for the psychometric function. As shown in <xref ref-type="fig" rid="fig5">Figure 5b–d</xref>, the model naturally captures the shape of the psychometric data. For instance, the same <inline-formula><alternatives><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math><tex-math id="inft16">\begin{document}$\overline{MCD_{corr}}$\end{document}</tex-math></alternatives></inline-formula> output could generate bell-shaped response distributions in experiment 1 and sinusoidal responses in experiment 2, purely based on the features of the input signals (such as its periodicity, or lack thereof). Therefore, taken together, the present simulations demonstrate that multisensory perception does indeed operate on correlated input from unimodal transient channels.</p></sec></sec><sec id="s2-8"><title>Validation of the MCD through simulation of published results</title><p>Although the simulations of experiments 1 and 2 demonstrate that an MCD unit that receives input from transient unimodal channels is capable of reproducing the present psychophysical results, it is important to assess the generalizability of this approach. Therefore, we relied on the previous literature on multisensory perception of correlation, simultaneity, and lag, as well as on the performance on crossmodal detection tasks, to validate our computational framework and assess its generalizability (by comparing the MCD responses against human performance on tasks not originally designed around our model). To this end, we selected a series of studies that employed parametric manipulations of the temporal structure of the signals, simulated the stimuli, and used the MCD model to predict human performance. If an MCD unit that receives input from transient unimodal channels is indeed the elementary computational unit for multisensory temporal processing, we should be able to reproduce all of the earlier findings on multisensory perception with our revised MCD model. A summary of our simulations, listing the sample size, number of observers, and Pearson correlation of MCD and human responses, is reported in <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>.</p><sec id="s2-8-1"><title>Causality and temporal order judgments for random sequences of audiovisual impulses</title><p>When we first proposed the MCD model (<xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>), we tested it against a psychophysical experiment in which participants were presented with a random sequence of five clicks and five flashes over an interval of 1 s (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1a</xref>), and had to report whether the signals appeared to share a common cause (causality judgments) and which modality came first (TOJ). To test whether the revised MCD model could also account for these previous findings, we fed the same stimuli to the model and used the <inline-formula><alternatives><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math><tex-math id="inft17">\begin{document}$\overline{MCD_{corr}}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ9">Equation 8</xref>) output to simulate causality judgments, and <inline-formula><alternatives><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math><tex-math id="inft18">\begin{document}$\overline{MCD_{lag}}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ10">Equation 9</xref>) to simulate TOJs. Given that the stimuli in this experiment had a much higher temporal rate than the stimuli used in experiments 1 and 2, the temporal constants of the MCD were set as free parameters that we fitted to the experimental data. Due to the stochastic nature of the stimuli, we analyzed the data using reverse correlation (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1b</xref>). The temporal constants of the MCD were fitted to maximize the Pearson correlation between the empirical and simulated responses, for both causality and TOJs (for details on modeling and reverse correlation analyses, see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1b</xref> and <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>).</p><p>Overall, the model faithfully reproduced the experimental data, and the Pearson correlation between empirical and simulated classification images was 0.99 (<xref ref-type="fig" rid="fig6">Figure 6a and b</xref>). This result closely replicates the original simulations (<xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>) performed with a version of the MCD that instead received input from sustained unimodal temporal channels (not transient as the current model). Given the differences between transient and sustained temporal channels, it may seem surprising that the two models are in such close agreement in the current simulation. However, it is important to remember that the stimuli in these experiments consisted of impulses (clicks and flashes), and the impulse response function of the sustained and transient channels as modeled in this study is indeed very similar. Therefore, such an agreement between the responses of the original and the revised versions of the MCD is expected. Besides proving that the revised model can replicate previous results, this simulation allowed us to estimate the temporal constants of the MCD for trains of clicks and flashes, which was necessary for the next simulations.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Multisensory correlation detector (MCD) simulations of published results.</title><p>(<bold>a</bold>) Results of the causality judgment task of <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>. The left panel represents the empirical classification image (gray) and the one obtained using the MCD model (blue). The right panel represents the output of the model plotted against human responses. Each dot corresponds to 315 responses. (<bold>b</bold>) Results of the temporal order judgment task of <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>. The left panel represents the empirical classification image (gray) and the one obtained using the MCD model (red). The right panel represents the output of the model plotted against human responses. Each dot corresponds to 315 responses. (<bold>c</bold>) Results of the causality judgment task of <xref ref-type="bibr" rid="bib22">Locke and Landy, 2017</xref>. The left panel represents the empirical classification image (gray) and the one obtained using the MCD model (blue). The right panel represents the effect of maximum audiovisual lag on perceived causality. Each dot represents on average 876 trials (range = [540, 1103]). (<bold>d</bold>) Results of the temporal order judgment task of <xref ref-type="bibr" rid="bib45">Wen et al., 2020</xref>. Squares represent the onset condition, whereas circles represent the offset condition. Each dot represents ≈745 trials. (<bold>e</bold>) Results of the detection task of <xref ref-type="bibr" rid="bib4">Andersen and Mamassian, 2008</xref>, showing auditory facilitation of visual detection task. Each dot corresponds to 336 responses. (<bold>f</bold>) Results of the audiovisual amplitude modulation detection task of <xref ref-type="bibr" rid="bib24">Nidiffer et al., 2018</xref>, where the audiovisual correlation was manipulated by varying the frequency and phase of the modulation signals. Each dot represents ≈140 trials. The datapoint represented by a star corresponds to the stimuli displayed in <xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Stimuli and reverse correlation analyses of <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>.</title><p>(<bold>a</bold>) represents three pairs of audiovisual stimuli (left) and their cross-correlation (right). (<bold>b</bold>) shows a schematic representation of the reverse correlation analyses. Adapted from Figure 2 of <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Stimuli of <xref ref-type="bibr" rid="bib22">Locke and Landy, 2017</xref>.</title><p>Three example pairs of audiovisual stimuli, and their cross-correlogram. Note how the stimuli vary in both terms of temporal rate (i.e., total number of clicks and flashes) and audiovisual correlation (with the bottom one being fully correlated).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-fig6-figsupp2-v2.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 3.</label><caption><title>Stimuli of <xref ref-type="bibr" rid="bib45">Wen et al., 2020</xref>.</title><p>Stimuli consisted of rectangular temporal envelopes, with a variable audiovisual lag either at the onset (top) or offset (bottom). The amount of audiovisual lag varied across trials, following a staircase procedure.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-fig6-figsupp3-v2.tif"/></fig><fig id="fig6s4" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 4.</label><caption><title>Stimuli of <xref ref-type="bibr" rid="bib4">Andersen and Mamassian, 2008</xref>.</title><p>Stimuli consisted of on-steps, with a parametrical manipulation of the lag between vision and audition, determined using the method of constant stimuli.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-fig6-figsupp4-v2.tif"/></fig><fig id="fig6s5" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 5.</label><caption><title>Stimuli of <xref ref-type="bibr" rid="bib24">Nidiffer et al., 2018</xref>.</title><p>Stimuli consisted of a sinusoidal amplitude modulation over a pedestal intensity (a). The pedestal had a 10 ms linear ramp at onset and offset. While the visual sinusoidal amplitude modulation was constant throughout the experiment (frequency = 6 Hz, zero phase offset), the auditory amplitude modulation varied in both frequency (from 6 Hz to 7 Hz, in five steps) and phase offset (from 0 to 360°, in eight steps). Next to each pair of stimuli, we represent the scatterplot of the visual and auditory envelopes, which highlight the audiovisual correlation. The call-out boxes zoom in on the correlation between the audiovisual sinusoidal amplitude modulation (excluding the linear onset and offset ramps), whereas the main scatterplots also display the ramps. (b) represents the audiovisual Pearson correlation for all the stimuli used by Nidiffer et al. The left panel shows the total audiovisual correlation, which was calculated while also including the onset and offset ramps; the right panel represents the partial correlation, calculated by only considering the sinusoidal amplitude modulation (i.e., the area inside the call-outs in a) without considering the onset and offset linear ramps as done in <xref ref-type="bibr" rid="bib24">Nidiffer et al., 2018</xref>. Note that once the ramps are included in the analyses, all audiovisual stimuli are strongly correlated (with only minor differences across conditions). The stars in the correlation matrices mark the cells corresponding to the four stimuli represented in (a), and the datapoints represented by a star in <xref ref-type="fig" rid="fig5">Figure 5f</xref>. (c) displays a comparison between the stimuli used by <xref ref-type="bibr" rid="bib24">Nidiffer et al., 2018</xref> and our experiment 2. Both the time axis (abscissa) and the intensity envelope (ordinate) are here drawn to scale. Although both experiments consist of stimuli with periodic amplitude modulations, there are key important differences. First off, while between two consecutive trials the visual and auditory stimuli were completely off in Nidiffer, in our study the pedestal was always present (without any interruptions across consecutive trials). That introduces transients in Nidiffer’s study both at the beginning and the end of each stimulus, which are absent in our stimuli. The relative magnitude of such transients to the comparatively low depth of amplitude modulation (of the sinusoidal component) is the ultimate reason for the absence of frequency doubling in <xref ref-type="bibr" rid="bib24">Nidiffer et al., 2018</xref>. In our study, transients at the beginning and end of each trial are prevented by playing a constant pedestal stimulus level across trials, and by applying a Gaussian envelope to the depth of our square-wave modulation. Additionally, it is important to stress the obvious difference in the duration and frequency of the stimuli used in the two studies. Specifically, our stimuli were 12 times longer than the stimuli of Nidiffer, but the frequency of amplitude modulation of our study was about 12 times lower (varying depending on the various conditions of Nidiffer et al.). Finally, note the difference in the depth of amplitude modulation across the two experiments.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-fig6-figsupp5-v2.tif"/></fig><fig id="fig6s6" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 6.</label><caption><title>Effect of unimodal temporal constants on the goodness of fit (Pearson correlation) between the multisensory correlation detector (MCD) model and the data from our experiments 1 and 2.</title><p>(a) represents the simplified model, with a single biphasic temporal filter. (b) represents the full model, with unimodal temporal filters in quadrature pairs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-fig6-figsupp6-v2.tif"/></fig></fig-group></sec><sec id="s2-8-2"><title>Causality judgments for random sequences of audiovisual impulses with high temporal rates</title><p>To identify the temporal features of the stimuli that promote audiovisual integration, <xref ref-type="bibr" rid="bib22">Locke and Landy, 2017</xref>, experiment (2) ran a psychophysical experiment that was very similar to the causality judgment task described in the previous section. Although the stimuli in the two studies were nearly identical, the temporal sequences used by <xref ref-type="bibr" rid="bib22">Locke and Landy, 2017</xref> had a considerably higher temporal rate (range 8–14 impulses/s), longer duration (2 s), and a more controlled temporal structure (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>). Participants observed the stimuli and performed a causality judgment task, whose results demonstrated that the perception of a common cause depended both on the correlation in the temporal structure of auditory and visual sequences (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, left) and the maximum lag between individual clicks and flashes (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, right).</p><p>Given the similarity of this study with the causality judgment of <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>, we followed the same logic described above to perform reverse correlation analyses (without smoothing the cross-correlograms). Moreover, the MCD simulations were performed with the same temporal constants used for the simulations of <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>, so that this experiment is simulated with a fully constrained model, with zero free parameters. Unlike our previous experiment, however, the stimuli used by <xref ref-type="bibr" rid="bib22">Locke and Landy, 2017</xref> varied in temporal rate, and hence also the number of clicks and flashes differed across trials. Given that the MCD is sensitive to the total stimulus energy, we normalized the model responses by dividing the <inline-formula><alternatives><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math><tex-math id="inft19">\begin{document}$\overline{MCD_{corr}}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ9">Equation 8</xref>) output by the rate of the stimuli.</p><p>Reverse correlation analyses were performed at the single subject level using both participants and model responses (see <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref> for details on how the continuous model output of the MCD was discretized into a dichotomous variable), and the average data is shown in <xref ref-type="fig" rid="fig6">Figure 6c</xref> (left panel). Overall, the MCD model could near-perfectly predict the empirical classification image (Pearson correlation &gt;0.99). Besides the reverse correlation analyses, <xref ref-type="bibr" rid="bib22">Locke and Landy, 2017</xref> measured how the maximum lag between individual clicks and flashes affected the perceived common cause of audiovisual sequences. The results, shown in <xref ref-type="fig" rid="fig6">Figure 6c</xref> (right panel), demonstrate that the perception of a common cause decreased with increasing maximum audiovisual lag. Once again, the MCD model accurately predicts this finding (Pearson correlation = 0.98) without any free parameters. Taken together, the present simulations demonstrate that the MCD model can account for the perception of a common cause between stochastic audiovisual sequences, even when the stimuli have a high temporal rate, and highlight the importance of both similarity in the temporal structure and crossmodal lag in audiovisual integration.</p></sec><sec id="s2-8-3"><title>Temporal order judgment for onset and offset stimuli</title><p>To investigate the perceived timing of auditory and visual on- and offsets, <xref ref-type="bibr" rid="bib45">Wen et al., 2020</xref> presented continuous audiovisual noise stimuli with step on- and offsets (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>). The authors parametrically manipulated the audiovisual lag between either of the onset or the offset of the audiovisual stimulus and asked participants to report the perceived temporal order of the corresponding on- or offset, respectively. Despite large variability across participants, the point of subjective simultaneity systematically differed across conditions, with acoustic stimuli more likely appearing to change before the visual stimuli in the onset compared to the offset condition.</p><p>To test whether the model could replicate this finding, we generated audiovisual signals with the same temporal manipulations of audiovisual lag and fed them to the MCD. To minimize the number of free parameters, we ran the simulation using the temporal constants of the filters fitted using the data from experiments 1 and 2 (see above). Moreover, to test whether the MCD alone could predict the PSS shift between the onset and offset condition, we combined the data from all participants and used a single GLM to link <inline-formula><alternatives><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math><tex-math id="inft20">\begin{document}$\overline{MCD_{lag}}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ10">Equation 9</xref>) to the TOJ data, irrespective of condition. Therefore, this simulation consisted of just two free parameters (i.e., the slope and intercept of the GLM). Overall, the responses of the MCD were in excellent agreement with the empirical data (Pearson correlation = 0.97) and successfully captured the difference across onset and offset condition (<xref ref-type="fig" rid="fig6">Figure 6d</xref>). For comparison, the Pearson correlation between the data and the psychometric functions (modeled as cumulative Gaussians, fitted independently for onset and offset conditions) was also 0.97, but required twice as many free parameters (two per condition), and it does not account for the underlying sensory information processing.</p></sec><sec id="s2-8-4"><title>Acoustic facilitation of visual transients’ detection</title><p>To study the temporal dynamic of audiovisual integration of unimodal transients, <xref ref-type="bibr" rid="bib4">Andersen and Mamassian, 2008</xref>, experiment (2) asked participants to detect a visual transient (i.e., a luminance increment) presented slightly before or after a task-irrelevant acoustic transient (i.e., a sound intensity increment, <xref ref-type="fig" rid="fig6s4">Figure 6—figure supplement 4</xref>). The task consisted of a two-interval forced-choice, with the visual stimuli set at 75% detection threshold, and the asynchrony between visual and acoustic transients parametrically manipulated using the method of constant stimuli.</p><p>To simulate this experiment, we fed the stimuli to the MCD model and used a GLM to link the <inline-formula><alternatives><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math><tex-math id="inft21">\begin{document}$\overline{MCD_{corr}}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ9">Equation 8</xref>) responses to the proportion of correct responses. The temporal constants of the model were constrained based on the results of experiments 1 and 2, so that this simulation had two free parameters: the slope and the intercept of the GLM. Overall, the MCD could reproduce the data of <xref ref-type="bibr" rid="bib4">Andersen and Mamassian, 2008</xref>, and the Pearson correlation between data and model responses was 0.91 (<xref ref-type="fig" rid="fig6">Figure 6e</xref>).</p></sec><sec id="s2-8-5"><title>Detection of sinusoidal amplitude modulation</title><p>To test whether audiovisual amplitude modulation detection depends on the correlation between the senses, <xref ref-type="bibr" rid="bib24">Nidiffer et al., 2018</xref> (experiment 2) asked the participants to detect audiovisual amplitude modulation. Stimuli consisted of a pedestal intensity to which (in some trials) the authors added a near-threshold sinusoidal amplitude modulation (<xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5a</xref>). To manipulate audiovisual correlation, the authors varied the frequency and phase of the sinusoidal modulation signal. Specifically, the frequency of auditory amplitude modulation varied between 6 Hz and 7 Hz (five steps), while the phase shift varied between 0 and 360° (eight steps). The frequency and phase shift of visual amplitude modulation were instead constant and set to 6 Hz and 0°, respectively.</p><p>Both phase and frequency systematically affected participants’ sensitivity; however, the results do not show any evidence of a frequency-doubling effect. This may appear as a surprising finding: given the analogy between this study and our experiment 2, a frequency-doubling effect should be intuitively expected (if, as we claim, correlation detection relied on transient input channels; though see <xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5a</xref>, for a visualization of the difference between these studies). In theory, when the amplitude modulations in the two modalities are 180° out of phase, the modulation signals are negatively correlated, and negatively correlated signals become positively correlated once fed to unsigned transient channels (<xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5a</xref>, bottom-left stimuli). When calculating the Pearson correlation of pairs of audiovisual signals, however, we should consider the whole stimuli: including the pedestal, not just the amplitude modulations. Indeed, the stimuli used by <xref ref-type="bibr" rid="bib24">Nidiffer et al., 2018</xref> also consisted of linear ramps at onset and offset and a pedestal intensity level, compared with which the depth of the amplitude modulation was barely noticeable (i.e., the modulation depth was about 6% of the pedestal level, see <xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5</xref>). Critically, Nidiffer and colleagues did not account for pedestals or onset and offset ramps when computing audiovisual correlation in their original paper. Once the audiovisual correlation is computed while also considering the ramps (and pedestals, see scatterplots in <xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5a</xref>), the lack of a frequency-doubling effect becomes apparent: all stimuli used by <xref ref-type="bibr" rid="bib24">Nidiffer et al., 2018</xref> are strongly positively correlated, though such a correlation slightly varied across conditions (being 1 when amplitude modulation had the same frequency in both modalities and zero phase shift, and 0.8 in the least correlated condition, see <xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5a and b</xref>).</p><p>To simulate the experiment of <xref ref-type="bibr" rid="bib24">Nidiffer et al., 2018</xref>, we fed the stimuli (including pedestals and onset and offset ramps) to the model and used the <inline-formula><alternatives><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math><tex-math id="inft22">\begin{document}$\overline{MCD_{corr}}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ9">Equation 8</xref>) output and a GLM to obtain the hit rate for the detection task. Given that neither the temporal constants of the MCD optimized for the experiments 1 and 2, nor those optimized for <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>, provided a good fit for Nidiffer’s data, we set both the temporal constants of the MCD and the slope and intercept of the GLM as free parameters. Hence, the model could replicate the results of <xref ref-type="bibr" rid="bib24">Nidiffer et al., 2018</xref>, and the Pearson correlation between the model and human responses was 0.89.</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Taken together, the present results demonstrate the dominance of transient over sustained channels in audiovisual integration. Based on the assumption that perceived synchrony across the senses depends on the (Pearson) correlation of the unimodal signals (<xref ref-type="bibr" rid="bib44">Wei, 2006</xref>), we generated specific hypotheses for perceived audiovisual synchrony—depending on whether such a correlation is computed over transient or sustained inputs. Such hypotheses were then tested against the results of two novel psychophysical experiments, jointly showing that Pearson correlation between transient input signals systematically determines the perceived timing of audiovisual events. Based on that, we revised a general model for audiovisual integration, the MCD, to selectively receive input from unimodal transient instead of sustained channels. Inspired by the motion detectors originally proposed for insect vision, such a biologically plausible model integrates audiovisual signals through correlation detection and could successfully account for the results of our psychophysical experiments along with a variety of recent findings in multisensory research. Specifically, once fed with transient input channels, the model could replicate human TOJs, SJs, causality judgments and crossmodal signal detection under a broad manipulation of input signals (i.e., step stimuli, trains of impulses, sinusoidal envelopes, etc.).</p><p>Previous research has already proposed a dominance of transient over sustained channels in audiovisual perception, with evidence coming from both psychophysical detection tasks and neuroimaging studies (<xref ref-type="bibr" rid="bib4">Andersen and Mamassian, 2008</xref>; <xref ref-type="bibr" rid="bib46">Werner and Noppeney, 2011</xref>). However, the role of transient and sustained channels on the perceived timing of audiovisual events (arguably the primary determinant of multisensory integration) has never been previously addressed, let alone computationally framed within a general model of multisensory integration. This study fills this obvious gap and explains all such previous findings in terms of the response dynamics of the MCD model, thereby comprehensively demonstrating that audiovisual integration relies on correlated input from unimodal transient channels. Recent research, however, criticized the MCD model for its alleged inability to either process audiovisual stimuli with a high temporal rate or detect signal correlation for short temporal intervals (<xref ref-type="bibr" rid="bib22">Locke and Landy, 2017</xref>; <xref ref-type="bibr" rid="bib9">Colonius and Diederich, 2020</xref>). By receiving inputs from transient unimodal channels, this revised version of the model fully addresses such criticisms. Indeed, the new MCD can near perfectly predict human performance in a causality judgment task with stimuli with a high temporal rate (up to 18 Hz) using the same set of parameters optimized for stimuli with a much lower rate (5 Hz), with a Pearson correlation coefficient of 0.99. Interestingly, recent results <xref ref-type="bibr" rid="bib31">Parise, 2024</xref> demonstrate that a population of spatially tuned MCD units can also account for the integration of ecological audiovisual stimuli over time and space, thereby replicating phenomena such as the McGurk Illusion, the Ventriloquist Illusion and even attentional orienting.</p><p>While MCD simulations could replicate the shape of the empirical psychometric curves, standard psychometric functions can also fit the same datasets, sometimes with even higher goodness of fit. Hence, one might wonder what the advantage of the current modeling approach is. When comparing MCD simulations with psychometric fits, it is important to focus on the differences between these two modeling approaches. Psychometric functions usually relate some physical parameter (the independent variable) to a measure of performance (the dependent variable) based on statistical considerations regarding the stimuli and the underlying perceptual decision-making process. For example, based on assumptions on the nature of the bell-shaped distribution of audiovisual SJs, Yarrow and colleagues (<xref ref-type="bibr" rid="bib47">Yarrow et al., 2011</xref>) proposed a model for SJs that fits the SJ data of our experiment 1 just as well as the MCD model (though with a larger number of free parameters). Psychometric curves for SJs, however, are not always bell-shaped: for example, in experiment 2, they are sinusoidal. This finding is naturally captured by the MCD model but not by models that enforce some specific shape for the psychometric functions. The reason is that statistical approaches to psychometric curve fitting are usually blind to the stimuli and agnostic as to how the raw input signals are transformed into evidence for perceptual decision-making. Indeed, while the inputs for psychometric fits are some parameters of the stimuli (e.g., the amount of audiovisual lag), the inputs for the MCD simulations are the actual stimuli themselves. That is, the MCD is a stimulus computable model that extracts from the raw signal the evidence that is then fed into the perceptual decision-making process (i.e., the observer model, <xref ref-type="fig" rid="fig5">Figure 5a</xref>). By making explicit all the processing steps that link the input stimuli to a button press, the MCD model can tailor its predictions to any input stimuli, with the shape of the psychometric curves being unconstrained in principle, yet fully predictable based only on the input signals and the experimental task. This is why the same MCD model can predict bell-shaped psychometric functions in the SJ task of experiment 1, sigmoidal functions in the TOJ task of experiment 1, and sinusoidal functions in experiment 2, whereas three different functions were necessary for the psychometric fits of the same experiments. Moreover, unlike psychometric fits, the MCD model is a biologically inspired neural model; as such, it not only allows one to account for behavioral responses, but also for neurophysiological data, as recently shown through magnetoencephalography (<xref ref-type="bibr" rid="bib33">Pesnot et al., 2022</xref>).</p><p>Previous research attributed the multisensory benefits on perceptual decision-making tasks to ‘late’, post-sensory changes occurring at the level of the decision dynamics, with evidence stemming from EEG activity in brain regions commonly considered to be involved in ‘high-level’, decision-making (<xref ref-type="bibr" rid="bib13">Franzen et al., 2020</xref>). While the present study cannot directly address such neurophysiological considerations, the computational framework proposed here, however, challenges any post-sensory interpretations. Indeed, the model proposed in this study can be divided into two separate components: the MCD, which handles the ‘early’ sensory processing stage, and a ‘late’ observer model, which receives input from the MCD and uses this information (along with the task demands) to generate a perceptual classification response (i.e., a button-press). In this context, it is important to note how multisensory benefits in detection tasks critically depend on the correlation and timing of audiovisual events: namely, the two factors that mostly affect the responses of the MCD model (e.g., see <xref ref-type="fig" rid="fig6">Figure 6e and f</xref>). Therefore, it is not surprising to see that the current framework can account for the effects of stimulus timing on performance purely based on the dynamics of the MCD responses, with no need for ad hoc adjustments in the perceptual decision-making process. Further studies will hopefully reconcile such discrepant interpretations on the origins of multisensory benefits on perceptual decision-making tasks, possibly through a better understanding of the computations underlying the electrophysiological correlates recorded with modern imaging techniques.</p><p>Although the present study unequivocally supports a dominance of transient channels in multisensory integration, it is still necessary to consider which role, if any, sustained channels may play in crossmodal perception. Indeed, earlier studies have shown that sustained information, such as the intensity of visual and acoustic stimuli, does indeed systematically affect performance in behavioral tasks (<xref ref-type="bibr" rid="bib38">Stein et al., 1996</xref>; <xref ref-type="bibr" rid="bib25">Odgaard et al., 2004</xref>), and can even elicit phenomena known as crossmodal correspondences (<xref ref-type="bibr" rid="bib29">Parise and Spence, 2013</xref>). The mapping of intensity between vision and audition, however, is a somewhat peculiar one as it is not obvious whether increasing intensity in one modality is mapped to higher or lower intensity in the other (i.e., an obvious mapping between acoustic and lightness intensity, rooted in natural scene statistics, has not been reported, yet). A perhaps more profound mapping between sustained audiovisual information relates to redundant cues, that is, properties or a physical stimulus that can be jointly estimated via two or more senses; such as the size of an object, which can be simultaneously estimated through vision and touch (<xref ref-type="bibr" rid="bib12">Ernst and Banks, 2002</xref>; <xref ref-type="bibr" rid="bib41">van Dam et al., 2014</xref>). This is a particularly prominent aspect of multisensory perception, and the estimated size of an object is arguably sustained, rather than transient stimulus information. While the present study cannot directly address such a question, we propose that the correlation between visual and tactile transients, like those occurring when we reach and make contact with an object, is what the brain needs to solve the correspondence problem, and that hence let us infer that what we see and touch are indeed coming from the same distal stimulus. Then, once made sure that the spatial cues that we get from vision and touch are redundant, size information can be optimally integrated. Testing such a hypothesis is surely a fertile subject for future research.</p><p>Finally, it is important to consider what the advantages of transient over sustained stimulus information for multisensory perception are. An obvious one is parsimony as dropping information that does not change over time entails lower transmission bandwidth by minimizing redundancy through efficient input coding (<xref ref-type="bibr" rid="bib5">Barlow, 1961</xref>). Therefore, by only representing input variations, transient channels operate as event detectors, signaling the system of potentially relevant changes in the surrounding. Interestingly, a similar approach has grown in popularity in novel technological applications, such as neuromorphic circuits and event cameras. Indeed, while traditional camera sensors operate on a frame-based approach, whereby inputs are periodically sampled based on an internal clock, event cameras only respond to external changes in brightness as they occur, thereby reducing transmission bandwidth and maximizing dynamic range. Interestingly, recent work has even successfully exploited Hassenstein–Reichardt detectors as a biologically inspired solution for detecting motion with event cameras (<xref ref-type="bibr" rid="bib10">D’Angelo et al., 2020</xref>). Hence, considering the mathematical equivalence of the MCD and the Hassenstein–Reichardt detectors, the present study suggests the intriguing possibility of using the revised MCD model as a biologically inspired solution for sensory fusion in future multimodal neuromorphic systems.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>The MCD model</title><p>The modified MCD model closely resembles the original model, but it takes input from unimodal transient (instead of sustained) input channels. That is, rather than being simply low-pass filtered, time-varying visual and auditory signals (<inline-formula><alternatives><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft23">\begin{document}$S_{V}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft24">\begin{document}$S_{A}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula>) are independently filtered by band-pass filters (<inline-formula><alternatives><mml:math id="inf25"><mml:mi>f</mml:mi></mml:math><tex-math id="inft25">\begin{document}$f$\end{document}</tex-math></alternatives></inline-formula>). Following <xref ref-type="bibr" rid="bib2">Adelson and Bergen, 1985</xref>, band-pass filters are modeled as biphasic impulse response functions defined as follows:<disp-formula id="equ2"><label>(1)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>⋅</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>⋅</mml:mo><mml:mi>n</mml:mi><mml:mo>!</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle f_{mod}(t)=\frac{t}{\tau_{mod}}\cdot e^{\frac{-t}{\tau_{mod}} }\cdot \left(1-\frac{t^2}{\tau^2_{mod}}\cdot n!\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>In this equation, <inline-formula><alternatives><mml:math id="inf26"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft26">\begin{document}$\tau _{mod}$\end{document}</tex-math></alternatives></inline-formula> is the modality-dependent temporal constant of the filter (mod = [a,v]). Based on the previous results of experiments 1 and 2, we set these constants to be  <inline-formula><alternatives><mml:math id="inf27"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft27">\begin{document}$\tau _{V}$\end{document}</tex-math></alternatives></inline-formula> = 0.070 s and  <inline-formula><alternatives><mml:math id="inf28"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft28">\begin{document}$\tau _{A}$\end{document}</tex-math></alternatives></inline-formula> = 0.055 s for the visual and auditory filters, respectively. In line with <xref ref-type="bibr" rid="bib2">Adelson and Bergen, 1985</xref>, the parameter <inline-formula><alternatives><mml:math id="inf29"><mml:mi>n</mml:mi></mml:math><tex-math id="inft29">\begin{document}$n$\end{document}</tex-math></alternatives></inline-formula> (which controls the negative lobe of the impulse response) is set to 3.</p><p>As in the original implementation of the MCD model, the low-pass temporal filter of the correlation unit was<disp-formula id="equ3"><label>(2)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>⋅</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle f_{av}(t)=\frac{t}{\tau_{av}}\cdot e^{\frac{-t}{\tau_{av}} } $$\end{document}</tex-math></alternatives></disp-formula></p><p>The temporal constant <inline-formula><alternatives><mml:math id="inf30"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft30">\begin{document}$\tau _{av}$\end{document}</tex-math></alternatives></inline-formula> was set to 0.674 s. Although the bimodal temporal constant <inline-formula><alternatives><mml:math id="inf31"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft31">\begin{document}$\tau _{av}$\end{document}</tex-math></alternatives></inline-formula> has a broad temporal tuning, the unimodal temporal constants <inline-formula><alternatives><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft32">\begin{document}$\tau _{V}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft33">\begin{document}$\tau _{A}$\end{document}</tex-math></alternatives></inline-formula> have systematic effects on the goodness of fit of the MCD model to our new psychophysical data. Therefore, in <xref ref-type="fig" rid="fig6s6">Figure 6—figure supplement 6</xref> we show how the correlation between the MCD model and empirical data varies as a function of the temporal tuning of the visual and auditory transient detectors.</p><p>The filtered unisensory stimuli <inline-formula><alternatives><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft34">\begin{document}$Sf_{mod}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> feeding into the correlation unit were obtained as follows:<disp-formula id="equ4"><label>(3)</label><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle Sf_{mod}(t)=[S_{mod}(t)*f_{mod}(t)]^2$$\end{document}</tex-math></alternatives></disp-formula></p><p>where (*) represents the convolution operator. Filtered signals are squared before being summed to render the responses insensitive to the polarity of changes in intensity (<xref ref-type="bibr" rid="bib40">Stigliani et al., 2017</xref>).</p><p>Like in the original MCD model, each subunit (<inline-formula><alternatives><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft35">\begin{document}$u_{1}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft36">\begin{document}$u_{2}$\end{document}</tex-math></alternatives></inline-formula>) of the detector independently combines filtered visual and auditory signals as follows:<disp-formula id="equ5"><label>(4)</label><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle u_1(t)=Sf_v(t).[Sf_a(t)*f_{av}(t)]$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ6"><label>(5)</label><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle u_2(t)=Sf_a(t).[Sf_v(t)*f_{av}(t)]$$\end{document}</tex-math></alternatives></disp-formula></p><p>To this end, the signals are convolved (*) with the low-pass temporal filters. The response of the subunits is eventually multiplied or subtracted.<disp-formula id="equ7"><label>(6)</label><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle MCD_{corr}(t)=\sqrt{u_1(t)\cdot u_2(t)} $$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ8"><label>(7)</label><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle MCD_{lag}(t)= u_2(t)-u_1(t)$$\end{document}</tex-math></alternatives></disp-formula></p><p>The resulting time-varying responses represent the local temporal correlation (<inline-formula><alternatives><mml:math id="inf37"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft37">\begin{document}$MCD_{Corr}$\end{document}</tex-math></alternatives></inline-formula>) and lag (<inline-formula><alternatives><mml:math id="inf38"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft38">\begin{document}$MCD_{Lag}$\end{document}</tex-math></alternatives></inline-formula>) across the signals. To reduce such time-varying responses into a single summary variable representing the total amount of evidence from each trial, we simply averaged the output of the detectors over a given temporal window of N samples:<disp-formula id="equ9"><label>(8)</label><alternatives><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t9">\begin{document}$$\displaystyle  \overline{MCD_{corr}}=\frac{1}{N} \sum\limits_{t=1}^{N}{MCD_{corr}(t)} $$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ10"><label>(9)</label><alternatives><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t10">\begin{document}$$\displaystyle  \overline{MCD_{lag}}=\frac{1}{N} \sum\limits_{t=1}^{N}{MCD_{lag}(t)} $$\end{document}</tex-math></alternatives></disp-formula></p><p>In the present simulations, the width of the temporal window varies across experiments due to the variable duration of the audiovisual stimuli. The output of the MCD model is eventually transformed into probabilities using a general linear model with a probit link function (assuming additive Gaussian noise; see <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>, for a similar approach). A Matlab implementation of the model is available as <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>.</p></sec><sec id="s4-2"><title>Modeling sustained input channels</title><p>In line with previous work (<xref ref-type="bibr" rid="bib8">Burr et al., 2009</xref>; <xref ref-type="bibr" rid="bib40">Stigliani et al., 2017</xref>; <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>), here we model sustained input channels as low-pass temporal filters with the following impulse response function:<disp-formula id="equ11"><label>(10)</label><alternatives><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>⋅</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="t11">\begin{document}$$\displaystyle f_{mod}(t)=\frac{t}{\tau_{mod}}\cdot e^\frac{-t}{\tau_{mod}} $$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft39">\begin{document}$\tau _{mod}$\end{document}</tex-math></alternatives></inline-formula> is the modality-dependent temporal constant of the filter (mod = [a,v]). Such a low-pass filter has the same shape as the bimodal filters of the MCD (<xref ref-type="disp-formula" rid="equ3">Equation 2</xref>) and of the unimodal filter of the original MCD model (<xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>).</p></sec><sec id="s4-3"><title>Experiment 1: The relationship between the PSS and WSS measured using TOJs vs. SJs</title><p>TOJs and SJs are the two main psychophysical tasks to measure sensitivity to lags across the senses. With both tasks it is possible to estimate the PSS and WSS; however, when measured on the same subjects, the PSS and WSS measured from the two tasks are often not correlated (<xref ref-type="bibr" rid="bib16">García-Pérez and Alcalá-Quintana, 2012</xref>; <xref ref-type="bibr" rid="bib21">Linares and Holcombe, 2014</xref>; <xref ref-type="bibr" rid="bib23">Machulla et al., 2016</xref>). This finding has been sometimes considered evidence for independent underlying neural mechanisms. Given that in experiment 1 we estimated the PSS and WSS with both TOJs and SJs, we can repeat the same analyses on our dataset. <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> shows the scatterplot of the PSS measured with the TOJ against the PSS measured with the SJ, and Figure S1D the scatterplot of the WSS measured with the TOJ against the SJ. Each point corresponds to one psychometric function in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1a</xref>. As in previous studies, the PSS and WSS measured with the two tasks are not significantly correlated (PSS: <italic>r</italic> = −0.16, p=0.38; WSS: <italic>r</italic> = 0.16, p=0.39).</p><p>While such a finding intuitively suggests the existence of independent mechanisms underlying the two tasks, our model clearly suggests otherwise. Indeed, the MCD model provides two outputs: one representing the decision variable for the SJ (<xref ref-type="disp-formula" rid="equ9">Equation 8</xref>) and the other for the TOJ (<xref ref-type="disp-formula" rid="equ10">Equation 9</xref>). Therefore, we propose that TOJs and SJs share a common mechanism for sensory processing: the MCD model (<xref ref-type="disp-formula" rid="equ2 equ3 equ4 equ5 equ6 equ7 equ8">Equations 1–7</xref>). However, the following decision-making processes (see <xref ref-type="fig" rid="fig5">Figure 5a</xref>) are independent across the two tasks, hence the lack of correlation between the PSS and WSS estimated using SJs vs. TOJs.</p></sec><sec id="s4-4"><title>Sample size</title><p>To properly substantiate our claims and quantitatively test our model, this study relies on a large collection of three novel psychophysical datasets and six previously published ones. These consisted of behavioral responses from a variety of tasks such as TOJs, SJs, causality judgments and detection tasks for a total of 68,693 trials. Our experiments 1 and 2 alone consisted of 12,600 trials (4800 for the TOJ task in experiment 1, 4800 for the SJ task in experiment 1, and 3000 for experiment 2). Given the nature of the present study, we were especially interested in determining the shapes of the psychometric functions, hence we prioritized collecting a large number of trials per observer (over a large pool of observers). Considering that we expected both large effect sizes (see <xref ref-type="fig" rid="fig1">Figure 1b and c</xref>) and low individual variability, the sample size of our new experiments is more than sufficient to draw reliable conclusions. Single-subject analyses (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) show consistent behavior across participants support our original assumptions.</p><p>Nevertheless, it is important to stress that our psychophysical experiments only represent a small fraction of the overall dataset used in this study to assess and model the contribution of transient and sustained channels in multisensory integration. Indeed, when calculating the sample size of this study, we must also include all the previously published datasets that were reanalyzed and simulated with the MCD model. Hence, our conclusions are supported by a large-scale analysis and computational modeling of a vast set of behavioral data, consisting of 68,693 trials from a sample of 110 observers, collectively providing strong converging evidence for the dominance of transient over sustained input channels in multisensory integration (see <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>).</p><p>The data from experiments 1 and 2 are available as <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>, <xref ref-type="supplementary-material" rid="fig2sdata2">Figure 2—source data 2</xref>, and <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref>.</p></sec><sec id="s4-5"><title>The quadrature MCD model</title><p>The MCD units used so far receive input from unimodal transient channels modeled as a single biphasic temporal filter followed by squaring nonlinearities. In the original version of <xref ref-type="bibr" rid="bib2">Adelson and Bergen, 1985</xref>, however, such a transient detection unit consisted of two biphasic temporal filters applied in parallel to the input, and the resulting signals are then squared and summed to each other. Although for the present simulations such a simplified version of the transient detector was sufficient, this may not be the case for different and more complex sets of stimuli. Hence, for completeness, here we also describe the full transient detector model.</p><p>Just like the simplified transient detector (<xref ref-type="disp-formula" rid="equ2 equ4">Equations 1 and 3</xref>), also the full transient detector consists of biphasic temporal filters. However, instead of passing the signal through a single biphasic filter, the full transient detector consists of two biphasic temporal filters 90° out of phase, applied in parallel to the incoming signals. Following Adelson and Bergen, these quadrature filters are modeled as follows:<disp-formula id="equ12"><label>(11)</label><alternatives><mml:math id="m12"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>∙</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>∙</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfenced><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo>∙</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><tex-math id="t12">\begin{document}$$\displaystyle f_{n}\left (t\right)=\left (\frac{t}{\tau _{bp}}\right)^{n}\cdot e^{\frac{- t}{\tau _{bp}}}\cdot \left [\frac{1}{n!}- \frac{1}{\left (n+2\right)!}\cdot \left (\frac{t}{\tau _{bp}}\right)^{2}\right ]$$\end{document}</tex-math></alternatives></disp-formula></p><p>The phase of the filter is determined by <inline-formula><alternatives><mml:math id="inf40"><mml:mi>n</mml:mi></mml:math><tex-math id="inft40">\begin{document}$n$\end{document}</tex-math></alternatives></inline-formula>, which, based on <xref ref-type="bibr" rid="bib11">Emerson et al., 1992</xref> takes the values of 6 for the fast filter and 9 for the slow one. The temporal constant of the filters is determined by the parameter <inline-formula><alternatives><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft41">\begin{document}$\tau _{bp}$\end{document}</tex-math></alternatives></inline-formula>.</p><p>Fast and slow filters are applied to each unimodal input signal and the two resulting signals are squared and then summed. Then, a compressive nonlinearity (square root) is applied to the output to constrain it within a reasonable range. Therefore, the output of each unimodal unit feeding into the correlation detector takes the following form:<disp-formula id="equ13"><label>(12)</label><alternatives><mml:math id="m13"><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>∗</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>∗</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:math><tex-math id="t13">\begin{document}$$\displaystyle Sf_{mod}\left (t\right)=\sqrt{\left [S_{mod}\left (t\right)\ast f_{6}\left (t\right)\right ]^{2}+\left [S_{mod}\left (t\right)\ast f_{9}\left (t\right)\right ]^{2}}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf42"><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:math><tex-math id="inft42">\begin{document}$mod=vid,aud$\end{document}</tex-math></alternatives></inline-formula> represents the sensory modality and <inline-formula><alternatives><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft43">\begin{document}$*$\end{document}</tex-math></alternatives></inline-formula> is the convolution operator. These filtered signals are then multiplied in the two subunits of the MCD following the same logic as the simplified model (<xref ref-type="disp-formula" rid="equ5 equ6 equ7 equ8 equ9 equ10">Equations 4–9</xref>).</p><p><xref ref-type="fig" rid="fig6s6">Figure 6—figure supplement 6</xref> displays how the temporal tuning of the unimodal temporal filter in the quadrature model affects the goodness of fit of the simulation of our experiments 1 and 2. Clearly, the temporal tuning of the unimodal filters strongly affects the goodness of fit of the model: although overall a tuning below 0.06 s is required for a good fit, the resulting landscape is highly irregular and overall favors slightly faster temporal constants for audition than vision. The parameter defining the low-pass temporal filter of the MCD subunits, instead, is less sensitive to the exact tuning. Hence, for this figure its value is arbitrarily set to 0.674 s. The psychometric functions generated by the full quadrature model are very similar to the ones generated by the simplified model (<xref ref-type="fig" rid="fig5">Figure 5</xref>). A MATLAB implementation is available as <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Methodology, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The experiment was conducted in accordance to the Declaration of Helsinki and was approved by the ethics committee of the University of Bielefeld. Participants received 8 euros per hour and provided written informed consent before participating to the experiment.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-90841-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="scode1"><label>Source code 1.</label><caption><title>MATLAB implementations of the MCD model and the quadrature MCD model.</title></caption><media xlink:href="elife-90841-code1-v2.zip" mimetype="application" mime-subtype="zip"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data is included as source data files for Figures 2 and 3.</p></sec><ack id="ack"><title>Acknowledgements</title><p>We extend gratitude to Prof. Marty Banks, whose early discussions with us helped shape the foundation of this study, and Dr. Irene Senna for her insightful comments throughout. This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation), Project ID 251654672-TRR 161 (Project C05).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Acerbi</surname><given-names>L</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Practical Bayesian optimization for model fitting with bayesian adaptive direct search</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1705.04405">https://arxiv.org/abs/1705.04405</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname><given-names>EH</given-names></name><name><surname>Bergen</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Spatiotemporal energy models for the perception of motion</article-title><source>Journal of the Optical Society of America A</source><volume>2</volume><elocation-id>284</elocation-id><pub-id pub-id-type="doi">10.1364/JOSAA.2.000284</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alais</surname><given-names>D</given-names></name><name><surname>Burr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The ventriloquist effect results from near-optimal bimodal integration</article-title><source>Current Biology</source><volume>14</volume><fpage>257</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2004.01.029</pub-id><pub-id pub-id-type="pmid">14761661</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>TS</given-names></name><name><surname>Mamassian</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Audiovisual integration of stimulus transients</article-title><source>Vision Research</source><volume>48</volume><fpage>2537</fpage><lpage>2544</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.08.018</pub-id><pub-id pub-id-type="pmid">18801382</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>Possible principles underlying the transformation of sensory messages</article-title><source>Sensory Communication</source><volume>1</volume><fpage>217</fpage><lpage>233</lpage></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benucci</surname><given-names>A</given-names></name><name><surname>Frazor</surname><given-names>RA</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Standing waves and traveling waves distinguish two circuits in visual cortex</article-title><source>Neuron</source><volume>55</volume><fpage>103</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.017</pub-id><pub-id pub-id-type="pmid">17610820</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breitmeyer</surname><given-names>BG</given-names></name><name><surname>Ganz</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Implications of sustained and transient channels for theories of visual pattern masking, saccadic suppression, and information processing</article-title><source>Psychological Review</source><volume>83</volume><fpage>1</fpage><lpage>36</lpage><pub-id pub-id-type="pmid">766038</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burr</surname><given-names>D</given-names></name><name><surname>Silva</surname><given-names>O</given-names></name><name><surname>Cicchini</surname><given-names>GM</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name><name><surname>Morrone</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Temporal mechanisms of multimodal binding</article-title><source>Proceedings of the Royal Society B</source><volume>276</volume><fpage>1761</fpage><lpage>1769</lpage><pub-id pub-id-type="doi">10.1098/rspb.2008.1899</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colonius</surname><given-names>H</given-names></name><name><surname>Diederich</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Formal models and quantitative measures of multisensory integration: A selective overview</article-title><source>The European Journal of Neuroscience</source><volume>51</volume><fpage>1161</fpage><lpage>1178</lpage><pub-id pub-id-type="doi">10.1111/ejn.13813</pub-id><pub-id pub-id-type="pmid">29285815</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Angelo</surname><given-names>G</given-names></name><name><surname>Janotte</surname><given-names>E</given-names></name><name><surname>Schoepe</surname><given-names>T</given-names></name><name><surname>O’Keeffe</surname><given-names>J</given-names></name><name><surname>Milde</surname><given-names>MB</given-names></name><name><surname>Chicca</surname><given-names>E</given-names></name><name><surname>Bartolozzi</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Event-based eccentric motion detection exploiting time difference encoding</article-title><source>Frontiers in Neuroscience</source><volume>14</volume><elocation-id>451</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2020.00451</pub-id><pub-id pub-id-type="pmid">32457575</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emerson</surname><given-names>RC</given-names></name><name><surname>Bergen</surname><given-names>JR</given-names></name><name><surname>Adelson</surname><given-names>EH</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Directionally selective complex cells and the computation of motion energy in cat visual cortex</article-title><source>Vision Research</source><volume>32</volume><fpage>203</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(92)90130-b</pub-id><pub-id pub-id-type="pmid">1574836</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname><given-names>MO</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title><source>Nature</source><volume>415</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/415429a</pub-id><pub-id pub-id-type="pmid">11807554</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franzen</surname><given-names>L</given-names></name><name><surname>Delis</surname><given-names>I</given-names></name><name><surname>De Sousa</surname><given-names>G</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Philiastides</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Auditory information enhances post-sensory visual evidence during rapid multisensory decision-making</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>5440</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-19306-7</pub-id><pub-id pub-id-type="pmid">33116148</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujisaki</surname><given-names>W</given-names></name><name><surname>Nishida</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Temporal frequency characteristics of synchrony–asynchrony discrimination of audio-visual signals</article-title><source>Experimental Brain Research</source><volume>166</volume><fpage>455</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1007/s00221-005-2385-8</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujisaki</surname><given-names>W</given-names></name><name><surname>Nishida</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Feature-based processing of audio-visual synchrony perception revealed by random pulse trains</article-title><source>Vision Research</source><volume>47</volume><fpage>1075</fpage><lpage>1093</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2007.01.021</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>García-Pérez</surname><given-names>MA</given-names></name><name><surname>Alcalá-Quintana</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>On the discrepant results in synchrony judgment and temporal-order judgment tasks: A quantitative model</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>19</volume><fpage>820</fpage><lpage>846</lpage><pub-id pub-id-type="doi">10.3758/s13423-012-0278-y</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassenstein</surname><given-names>B</given-names></name><name><surname>Reichardt</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1956">1956</year><article-title>Systemtheoretische analyse der zeit-, reihenfolgen- und vorzeichenauswertung bei der bewegungsperzeption des rüsselkäfers chlorophanus</article-title><source>Zeitschrift Für Naturforschung B</source><volume>11</volume><fpage>513</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1515/znb-1956-9-1004</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herdener</surname><given-names>M</given-names></name><name><surname>Lehmann</surname><given-names>C</given-names></name><name><surname>Esposito</surname><given-names>F</given-names></name><name><surname>di Salle</surname><given-names>F</given-names></name><name><surname>Federspiel</surname><given-names>A</given-names></name><name><surname>Bach</surname><given-names>DR</given-names></name><name><surname>Scheffler</surname><given-names>K</given-names></name><name><surname>Seifritz</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Brain responses to auditory and visual stimulus offset: shared representations of temporal edges</article-title><source>Human Brain Mapping</source><volume>30</volume><fpage>725</fpage><lpage>733</lpage><pub-id pub-id-type="doi">10.1002/hbm.20539</pub-id><pub-id pub-id-type="pmid">18266216</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ikeda</surname><given-names>H</given-names></name><name><surname>Wright</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>Receptive field organization of ‘sustained’ and ‘transient’ retinal ganglion cells which subserve different functional roles</article-title><source>The Journal of Physiology</source><volume>227</volume><fpage>769</fpage><lpage>800</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1972.sp010058</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>Y-J</given-names></name><name><surname>Grabowecky</surname><given-names>M</given-names></name><name><surname>Paller</surname><given-names>KA</given-names></name><name><surname>Suzuki</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Differential roles of frequency-following and frequency-doubling visual responses revealed by evoked neural harmonics</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>1875</fpage><lpage>1886</lpage><pub-id pub-id-type="doi">10.1162/jocn.2010.21536</pub-id><pub-id pub-id-type="pmid">20684661</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linares</surname><given-names>D</given-names></name><name><surname>Holcombe</surname><given-names>AO</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Differences in perceptual latency estimated from judgments of temporal order, simultaneity and duration are inconsistent</article-title><source>I-Perception</source><volume>5</volume><fpage>559</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1068/i0675</pub-id><pub-id pub-id-type="pmid">26034565</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Locke</surname><given-names>SM</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Temporal causal inference with stochastic audiovisual sequences</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0183776</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0183776</pub-id><pub-id pub-id-type="pmid">28886035</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Machulla</surname><given-names>T-K</given-names></name><name><surname>Di Luca</surname><given-names>M</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The consistency of crossmodal synchrony perception across the visual, auditory, and tactile senses</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>42</volume><fpage>1026</fpage><lpage>1038</lpage><pub-id pub-id-type="doi">10.1037/xhp0000191</pub-id><pub-id pub-id-type="pmid">27045320</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nidiffer</surname><given-names>AR</given-names></name><name><surname>Diederich</surname><given-names>A</given-names></name><name><surname>Ramachandran</surname><given-names>R</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multisensory perception reflects individual differences in processing temporal correlations</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>14483</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-32673-y</pub-id><pub-id pub-id-type="pmid">30262826</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Odgaard</surname><given-names>EC</given-names></name><name><surname>Arieh</surname><given-names>Y</given-names></name><name><surname>Marks</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Brighter noise: Sensory enhancement of perceived loudness by concurrent visual stimulation</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>4</volume><fpage>127</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.3758/CABN.4.2.127</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohshiro</surname><given-names>T</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A normalization model of multisensory integration</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>775</fpage><lpage>782</lpage><pub-id pub-id-type="doi">10.1038/nn.2815</pub-id><pub-id pub-id-type="pmid">21552274</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parise</surname><given-names>CV</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>“When birds of a feather flock together”: synesthetic correspondences modulate audiovisual integration in non-synesthetes</article-title><source>PLOS ONE</source><volume>4</volume><elocation-id>e5664</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0005664</pub-id><pub-id pub-id-type="pmid">19471644</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parise</surname><given-names>CV</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>When correlation implies causation in multisensory integration</article-title><source>Current Biology</source><volume>22</volume><fpage>46</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.11.039</pub-id><pub-id pub-id-type="pmid">22177899</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Parise</surname><given-names>C</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Audiovisual Cross-Modal Correspondences in the General Population</source><publisher-name>Oxford Handbooks Online</publisher-name><pub-id pub-id-type="doi">10.1093/oxfordhb/9780199603329.013.0039</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parise</surname><given-names>CV</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Correlation detection as a general mechanism for multisensory integration</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>11543</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms11543</pub-id><pub-id pub-id-type="pmid">27265526</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Parise</surname><given-names>CV</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Spatiotemporal Models for Multisensory Integration</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.12.29.573621</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parise</surname><given-names>CV</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>A stimulus-computable model for audiovisual perception and spatial orienting in mammals</article-title><source>eLife</source><volume>14</volume><elocation-id>RP106122</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.106122</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pesnot</surname><given-names>LJ</given-names></name><name><surname>Parise</surname><given-names>CV</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name><name><surname>van Wassenhove</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Multisensory correlation computations in the human brain identified by a time-resolved encoding model</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>2489</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-29687-6</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname><given-names>L</given-names></name><name><surname>Chimoto</surname><given-names>S</given-names></name><name><surname>Sakai</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Sato</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Comparison between offset and onset responses of primary auditory cortex ON-OFF neurons in awake cats</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>3421</fpage><lpage>3431</lpage><pub-id pub-id-type="doi">10.1152/jn.00184.2007</pub-id><pub-id pub-id-type="pmid">17360820</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Recanzone</surname><given-names>GH</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Response profiles of auditory cortical neurons to tones and noise in behaving macaque monkeys</article-title><source>Hearing Research</source><volume>150</volume><fpage>104</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1016/s0378-5955(00)00194-5</pub-id><pub-id pub-id-type="pmid">11077196</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samad</surname><given-names>M</given-names></name><name><surname>Parise</surname><given-names>C</given-names></name><name><surname>Keller</surname><given-names>S</given-names></name><name><surname>Di Luca</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A common cause in the phenomenological and sensorimotor correlates of body ownership</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>1230</elocation-id><pub-id pub-id-type="doi">10.1167/18.10.1230</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sekuler</surname><given-names>R</given-names></name><name><surname>Sekuler</surname><given-names>AB</given-names></name><name><surname>Lau</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Sound alters visual motion perception</article-title><source>Nature</source><volume>385</volume><elocation-id>308</elocation-id><pub-id pub-id-type="doi">10.1038/385308a0</pub-id><pub-id pub-id-type="pmid">9002513</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>London</surname><given-names>N</given-names></name><name><surname>Wilkinson</surname><given-names>LK</given-names></name><name><surname>Price</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Enhancement of perceived visual intensity by auditory stimuli: A psychophysical analysis</article-title><source>Journal of Cognitive Neuroscience</source><volume>8</volume><fpage>497</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1162/jocn.1996.8.6.497</pub-id><pub-id pub-id-type="pmid">23961981</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>The New Handbook of Multisensory Processing</source><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/8466.001.0001</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stigliani</surname><given-names>A</given-names></name><name><surname>Jeska</surname><given-names>B</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Encoding model of temporal processing in human visual cortex</article-title><source>PNAS</source><volume>114</volume><fpage>E11047</fpage><lpage>E11056</lpage><pub-id pub-id-type="doi">10.1073/pnas.1704877114</pub-id><pub-id pub-id-type="pmid">29208714</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>van Dam</surname><given-names>CJL</given-names></name><name><surname>Parise</surname><given-names>CV</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Modeling multisensory integration</chapter-title><person-group person-group-type="editor"><name><surname>Bennett</surname><given-names>DJ</given-names></name><name><surname>Hill</surname><given-names>CS</given-names></name></person-group><source>Sensory Integration and the Unity of Consciousness</source><publisher-name>MIT Press</publisher-name><fpage>209</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.7551/mitpress/9780262027786.003.0010</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Wassenhove</surname><given-names>V</given-names></name><name><surname>Grant</surname><given-names>KW</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Temporal window of integration in auditory-visual speech perception</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>598</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.01.001</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vroomen</surname><given-names>J</given-names></name><name><surname>Keetels</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Perception of intersensory synchrony: A tutorial review</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>72</volume><fpage>871</fpage><lpage>884</lpage><pub-id pub-id-type="doi">10.3758/APP.72.4.871</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>WWS</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Time Series Analysis: Univariate and Multivariate Methods</source><publisher-name>Pearson College Division</publisher-name></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname><given-names>P</given-names></name><name><surname>Opoku-Baah</surname><given-names>C</given-names></name><name><surname>Park</surname><given-names>M</given-names></name><name><surname>Blake</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Judging relative onsets and offsets of audiovisual events</article-title><source>Vision</source><volume>4</volume><elocation-id>17</elocation-id><pub-id pub-id-type="doi">10.3390/vision4010017</pub-id><pub-id pub-id-type="pmid">32138261</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Werner</surname><given-names>S</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The contributions of transient and sustained response codes to audiovisual integration</article-title><source>Cerebral Cortex</source><volume>21</volume><fpage>920</fpage><lpage>931</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq161</pub-id><pub-id pub-id-type="pmid">20810622</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarrow</surname><given-names>K</given-names></name><name><surname>Jahn</surname><given-names>N</given-names></name><name><surname>Durant</surname><given-names>S</given-names></name><name><surname>Arnold</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Shifts of criteria or neural timing? The assumptions underlying timing perception studies</article-title><source>Consciousness and Cognition</source><volume>20</volume><fpage>1518</fpage><lpage>1531</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2011.07.003</pub-id><pub-id pub-id-type="pmid">21807537</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Sample size of the datasets modeled and analyzed in the present study.</title><p>Two of the datasets listed here (i.e., experiment 1 and <xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref>) consisted of two tasks, each tested on the same pool of observers. The last row represents the total number of observers and trials, the average number of trials per participant, and the average correlation between multisensory correlation detector (MCD) simulations and human data. Note how the revised MCD tightly replicated human responses in all of the datasets included in this study, despite major differences in stimuli, tasks, and sample sizes of the individual studies (see last column).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dataset</th><th align="left" valign="bottom">Task</th><th align="left" valign="bottom">Number of observers</th><th align="left" valign="bottom">Number of trials</th><th align="left" valign="bottom">Trials for observer</th><th align="left" valign="bottom">MCD-data correlation</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="2"><bold>Experiment 1</bold></td><td align="left" valign="bottom">Simultaneity judgment</td><td align="char" char="." valign="bottom" rowspan="2">8</td><td align="char" char="." valign="bottom">4800</td><td align="char" char="." valign="bottom">600</td><td align="char" char="." valign="bottom">0.99</td></tr><tr><td align="left" valign="bottom">Temporal order judgment</td><td align="char" char="." valign="bottom">4800</td><td align="char" char="." valign="bottom">600</td><td align="char" char="." valign="bottom">0.99</td></tr><tr><td align="left" valign="bottom"><bold>Experiment 2</bold></td><td align="left" valign="bottom">Simultaneity judgment</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">3000</td><td align="char" char="." valign="bottom">600</td><td align="char" char="." valign="bottom">0.98</td></tr><tr><td align="left" valign="bottom" rowspan="2"><xref ref-type="bibr" rid="bib30">Parise and Ernst, 2016</xref></td><td align="left" valign="bottom">Causality judgment</td><td align="char" char="." valign="bottom" rowspan="2">5</td><td align="char" char="." valign="bottom">9300</td><td align="char" char="." valign="bottom">1860</td><td align="char" char="." valign="bottom">0.98</td></tr><tr><td align="left" valign="bottom">Temporal order judgment</td><td align="char" char="." valign="bottom">9300</td><td align="char" char="." valign="bottom">1860</td><td align="char" char="." valign="bottom">0.99</td></tr><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib22">Locke and Landy, 2017</xref></td><td align="left" valign="bottom">Causality judgment</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">7200</td><td align="char" char="." valign="bottom">720</td><td align="char" char="." valign="bottom">0.99</td></tr><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib45">Wen et al., 2020</xref></td><td align="left" valign="bottom">Temporal order judgment</td><td align="char" char="." valign="bottom">57</td><td align="char" char="." valign="bottom">22,337</td><td align="left" valign="bottom">≈392</td><td align="char" char="." valign="bottom">0.97</td></tr><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib4">Andersen and Mamassian, 2008</xref></td><td align="left" valign="bottom">Transient detection</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">2352</td><td align="left" valign="bottom">≈181</td><td align="char" char="." valign="bottom">0.91</td></tr><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib24">Nidiffer et al., 2018</xref></td><td align="left" valign="bottom">Modulation detection</td><td align="char" char="." valign="bottom">12</td><td align="char" char="." valign="bottom">5604</td><td align="char" char="." valign="bottom">467</td><td align="char" char="." valign="bottom">0.89</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Overall</td><td align="char" char="." valign="bottom">110</td><td align="char" char="." valign="bottom">68,693</td><td align="left" valign="bottom">≈624</td><td align="char" char="." valign="bottom">0.97</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Results of Friedman test and Bayesian repeated measures ANOVA for experiment 1.</title><p>Four separate Friedman tests were used to assess whether the four experimental conditions differed in terms of point of subjective simultaneity (PSS) and window of subjective simultaneity (WSS) in the temporal order judgment (TOJ) and simultaneity judgment (SJ) tasks. The first column represents the variables, the second the <inline-formula><alternatives><mml:math id="inf44"><mml:msup><mml:mrow><mml:mi>χ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><tex-math id="inft44">\begin{document}$\chi ^{2}$\end{document}</tex-math></alternatives></inline-formula> value and the degrees of freedom (in brackets), and the third the p-value. Given that we ran four tests on the same dataset, statistical significance should be computed by comparing the p-value against a Bonferroni-adjusted alpha level of 0.0125 (i.e., 0.05/4). The last column represents the Bayes factor BF<sub>01</sub> in favor of the null hypothesis as calculated using Bayesian repeated measures ANOVA using the statistical software JASP (JASP Team 2024; version 0.18.3) with the default settings. Together with the Friedman test, the present analyses provide further converging evidence for the lack of meaningful differences across the two tasks and four conditions of experiment 1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="2"/><th align="left" valign="top" colspan="2">Friedman test</th><th align="left" valign="top">Bayesian ANOVA</th></tr><tr><th align="left" valign="top"><inline-formula><alternatives><mml:math id="inf45"><mml:msup><mml:mrow><mml:mi>χ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><tex-math id="inft45">\begin{document}$\chi ^{2}$\end{document}</tex-math></alternatives></inline-formula>(df)</th><th align="left" valign="top">p-Value</th><th align="left" valign="top">BF<sub>01</sub></th></tr></thead><tbody><tr><td align="left" valign="top">PSS (TOJ)</td><td align="char" char="." valign="top">6.15 (3)</td><td align="char" char="." valign="top">0.1045</td><td align="char" char="." valign="top">2.833</td></tr><tr><td align="left" valign="top">PSS (SJ)</td><td align="char" char="." valign="top">2.85 (3)</td><td align="char" char="." valign="top">0.4153</td><td align="char" char="." valign="top">2.237</td></tr><tr><td align="left" valign="top">WSS (TOJ)</td><td align="char" char="." valign="top">2.55 (3)</td><td align="char" char="." valign="top">0.4663</td><td align="char" char="." valign="top">2.570</td></tr><tr><td align="left" valign="top">WSS (SJ)</td><td align="char" char="." valign="top">5.55 (3)</td><td align="char" char="." valign="top">0.1357</td><td align="char" char="." valign="top">1.452</td></tr></tbody></table></table-wrap></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90841.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Groh</surname><given-names>Jennifer M</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Duke University</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study evaluates a model for multisensory correlation detection, focusing on the detection of correlated transients in visual and auditory stimuli. Overall, the experimental design is sound and the evidence is <bold>compelling</bold>. The synergy between the experimental and theoretical aspects of the article is strong, and the work will be of interest to both neuroscientists and psychologists working in the domain of sensory processing and perception.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90841.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The authors present a model for multisensory correlation detection that is based on the neurobiologically plausible Hassenstein Reichardt detector (Parise &amp; Ernst, 2016). They demonstrate that this model can account for human behaviour in synchrony or temporal order judgements and related temporal tasks in two new data sets (acquired in this study) and a range of previous data sets. While the current study is limited to the model assessment for relatively simple audiovisual signals, in future communications, the authors demonstrate that the model can also account for audiovisual integration of complex naturalistic signals such as speech and music.</p><p>The significance of this work lies in its ability to explain multisensory perception using fundamental neural mechanisms previously identified in insect motion processing.</p><p>Strengths:</p><p>(1) The model goes beyond descriptive models such as cumulative Gaussians for TOJ and differences in cumulative Gaussians for SJ tasks by providing a mechanism that builds on the neurobiologically plausible Hassenstein-Reichardt detector.</p><p>(2) This model can account for results from two new experiments that focus on the detection of correlated transients and frequency doubling. The model also accounts for several behavioural results from experiments including stochastic sequences of A/V events and sine wave modulations (and naturalistic Av signals such as speech and music as shown in future communications).</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90841.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This is an interesting and well-written manuscript that seeks to detail performance on two human psychophysical experiments designed to look at the relative contributions of transient and sustained components of a multisensory (i.e., audiovisual) stimulus to their integration. The work is framed within the context of a model previously developed by the authors and now somewhat revised to better incorporate the experimental findings. The major takeaway from the paper is that transient signals carry the vast majority of the information related to the integration of auditory and visual cues, and that the Multisensory Correlation Detector (MCD) model not only captures the results of the current study, but is also highly effective in capturing the results of prior studies focused on temporal and causal judgments.</p><p>Strengths:</p><p>Overall the experimental design is sound and the analyses well performed. The extension of the MCD model to better capture transients make a great deal of sense in the current context, and it is very nice to see the model applied to a variety of previous studies.</p><p>Comments on the revised version:</p><p>In the revised manuscript, the authors have done an excellent job of responding to the prior critiques. I have no additional concerns or comments.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90841.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Parise</surname><given-names>Cesare V</given-names></name><role specific-use="author">Author</role><aff><institution>University of Liverpool</institution><addr-line><named-content content-type="city">Liverpool</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Ernst</surname><given-names>Marc O</given-names></name><role specific-use="author">Author</role><aff><institution>ULM University</institution><addr-line><named-content content-type="city">Ulm</named-content></addr-line><country>Germany</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><p><bold>Public Reviews:</bold></p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p><p>The authors present a model for multisensory correlation detection that is based on the neurobiologically plausible Hassenstein Reichardt detector. It modifies their previously reported model (Parise &amp; Ernst, 2016) in two ways: a bandpass (rather than lowpass) filter is initially applied and the filtered signals are then squared. The study shows that this model can account for synchrony judgement, temporal order judgement, etc in two new data sets (acquired in this study) and a range of previous data sets.</p><p>Strengths:</p><p>(1) The model goes beyond descriptive models such as cumulative Gaussians for TOJ and differences in cumulative Gaussians for SJ tasks by providing a mechanism that builds on the neurobiologically plausible Hassenstein-Reichardt detector.</p><p>(2) This modified model can account for results from two new experiments that focus on the detection of correlated transients and frequency doubling. The model also accounts for several behavioural results from experiments including stochastic sequences of A/V events and sine wave modulations.</p><p>Additional thoughts:</p><p>(1) The model introduces two changes: bandpass filtering and squaring of the inputs. The authors emphasize that these changes allow the model to focus selectively on transient rather than sustained channels. But shouldn't the two changes be introduced separately? Transients may also be detected for signed signals.</p></disp-quote><p>We updated the original model because our new psychophysical evidence demonstrates the fundamental role of unsigned transient for multisensory perception. While the original model received input from sustained unimodal channels (low-pass filters), the new version receives input from unsigned unimodal transient channels. Transient channels are normally modelled through bandpass filters (to remove the DC and high-frequency signal components) and squaring (to remove the sign). While these may appear as two separate changes in the model, they are, in fact, a single one: the substitution of sustained with unsigned transient channels (for a similar approach, see Stigliani et al. 2017, PNAS). Either change alone would not be sufficient to implement a transient channel that accounts for the present results.</p><p>That said, we were also concerned with introducing too many changes in the model at once. Indeed, we simply modelled the unimodal transient channels as a single band-pass filter followed by squaring. This is already a stripped-down version of the unsigned transient detectors proposed by Adelson and Bergen in their classic Motion Energy model. The original model consisted of two biphasic temporal filters 90 degrees out of phase (i.e., quadrature filters), whose output is later combined. While a simpler implementation of the transient channels was sufficient in the present study, the full model may be necessary for other classes of stimuli (including speech, Parise, 2024, BiorXiv). Therefore, for completeness, we now include in the Supplementary Information a formal description of the full model, and validate it by simulating our two novel psychophysical studies. See Supplementary Information “The quadrature MCD model” section and Supplementary Figure S8.</p><disp-quote content-type="editor-comment"><p>(2) Because the model is applied only to rather simple artificial signals, it remains unclear to what extent it can account for AV correlation detection for naturalistic signals. In particular, speech appears to rely on correlation detection of signed signals. Can this modified model account for SJ or TOJ judgments for naturalistic signals?</p></disp-quote><p>It can. In a recent series of studies we have demonstrated that a population of spatially-tuned MCD units can account for audiovisual correlation detection for naturalistic stimuli, including speech (e.g. the McGurk Illusion). Once again, unsigned transients were sufficient to replicate a variety of previous findings. We have now extended the discussion to cover this recent research: Parise, C. V. (2024). Spatiotemporal models for multisensory integration. bioRxiv, 2023-12.</p><disp-quote content-type="editor-comment"><p>Even Nidiffer et al. (2018) which is explicitly modelled by the authors report a significant difference in performance for correlated and anti-correlated signals. This seems to disagree with the results of study 1 reported in the current paper and the model's predictions. How can these contradicting results be explained? If the brain detects correlation on signed and unsigned signals, is a more complex mechanism needed to arbitrate between those two?</p></disp-quote><p>We believe the reviewer here refers to our Experiment 2 (where, like Nidiffer at al. (2018) we used periodic stimuli, not Experiment 1, which consists of step stimuli). We were also puzzled by the difference between our Experiment 2 and Nidiffer et al. (2018): we induced frequency doubling, Nidiffer did not. Based on quantitative simulations, we concluded that this difference could be attributed to the fact that while Nidiffer included on each trial an intensity ramp in their periodic audiovisual stimuli, we did not. As a result, when considering the ramp (unlike in Nidiffer’s analyses), all audiovisual signals used by Nidiffer were positively correlated (irrespective of frequency and phase offset), while our signals in Experiment 2 were sometimes correlated and other times not (depending on the phase offset). This important simulation is included in Supplementary Figure S7; we also have now updated the text to better highlight the role of the pedestal in determining the direction of the correlation.</p><disp-quote content-type="editor-comment"><p>(3) The number of parameters seems quite comparable for the authors' model and descriptive models (e.g. PSF models). This is because time constants require refitting (at least for some experimental data sets) and the correlation values need to be passed through a response mode (i.e. probit function) to account for behavioural data. It remains unclear how the brain adjusts the time constants to different sensory signals.</p></disp-quote><p>This is a deep question. For simplicity, here the temporal constants were fitted to the empirical psychometric functions. To avoid overfitting, whenever possible we fitted such parameters over some training datasets, while trying to predict others. However, in some cases, it was necessary to fit the temporal constants to specific datasets. This may suggest that the temporal tuning of those units is not crystalised to some pre-defined values, but is adjusted based on recent perceptual history (e.g., the sequence of trials and stimuli participants are exposed to during the various experiments).</p><p>For transparency, here we show how varying the tuning of the temporal constants of the filters affects the goodness of fit of our new psychophysical experiments (Supplementary Figure S8). As it can be readily appreciated, the relative temporal tuning of the unimodal transient detector was critical, though their absolute values could vary over a range of about 15 to over 100ms. The tuning of the low-pass filters of the correlation detector (not shown here) displayed much lower temporal sensitivity over a range between 0.1s to over 1s.</p><p>This simulation shows the impact of temporal tuning in our simulations, however, the question remains as to how such a tuning gets selected in the first place. An appealing explanation relies on natural scene statistics: units are temporally tuned to the most common audiovisual stimuli. Although our current empirical evidence does not allow us to quantitatively address this question, in previous simulations (see Parise &amp; Ernst, 2016, Supplementary Figure 8), by analogy with visual motion adaptation, we show how the temporal constants of our model can dynamically adjust and adapt to recent perceptual history. We hope these new and previous simulations address the question about the nature of the temporal tuning of the MCD units.</p><disp-quote content-type="editor-comment"><p>(4) Fujisaki and Nishida (2005, 2006) proposed mechanisms for AV correlation detection based on the Hassenstein-Reichardt motion detector (though not formalized as a computational model).</p></disp-quote><p>This is correct, Fujisaki and Nishida (2005, 2007) also hypothesized that AV synchrony could be detected using a mechanism analogous to motion detection. Interestingly, however, they ruled out such a hypothesis, as their “data do not support the existence of specialized low-level audio-visual synchrony detectors”. Yet, along with our previous work (Parise &amp; Ernst, 2016, where we explicitly modelled the experiments of Fujisaki and Nishida), the present simulations quantitatively demonstrate that a low-level AV synchrony detector is instead sufficient to account for audiovisual synchrony perception and correlation detection. We now credit Fujusaki and Nishida in the modelling section for proposing that AV synchrony can be detected by a cross-correlator.</p><p>Finally, we believe the reviewer is referring to the 2005 and 2007 studies of Fujisaki and Nishida (not 2006); here are the full references of the two articles we are referring to:</p><p>Fujisaki, W., &amp; Nishida, S. Y. (2005). Temporal frequency characteristics of synchrony–asynchrony discrimination of audio-visual signals. Experimental Brain Research, 166, 455-464.</p><p>Fujisaki, W., &amp; Nishida, S. Y. (2007). Feature-based processing of audio-visual synchrony perception revealed by random pulse trains. Vision Research, 47(8), 1075-1093.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>This is an interesting and well-written manuscript that seeks to detail the performance of two human psychophysical experiments designed to look at the relative contributions of transient and sustained components of a multisensory (i.e., audiovisual) stimulus to their integration. The work is framed within the context of a model previously developed by the authors and is now somewhat revised to better incorporate the experimental findings. The major takeaway from the paper is that transient signals carry the vast majority of the information related to the integration of auditory and visual cues, and that the Multisensory Correlation Detector (MCD) model not only captures the results of the current study but is also highly effective in capturing the results of prior studies focused on temporal and causal judgments.</p><p>Strengths:</p><p>Overall the experimental design is sound and the analyses are well performed. The extension of the MCD model to better capture transients makes a great deal of sense in the current context, and it is very nice to see the model applied to a variety of previous studies.</p><p>Weaknesses:</p><p>My one major issue with the paper revolves around its significance. In the context of a temporal task(s), is it in any way surprising that the important information is carried by stimulus transients? Stated a bit differently, isn't all of the important information needed to solve the task embedded in the temporal dimension? I think the authors need to better address this issue to punch up the significance of their work.</p></disp-quote><p>In hindsight, it may appear unsurprising that transient signals carry most information for audiovisual integration. Yet, so somewhat unexpectedly, this has never been investigated using perhaps the most diagnostic psychophysical tools for perceived crossmodal timing; namely temporal order and simultaneity judgments–along with carefully designed experiments with quantitative predictions for the effect of either channel. The fact that the results conform to intuitive expectations further supports the value of the present work: grounding empirically with what is intuitively expected. This offers solid psychophysical evidence that one can build on for future advancements. Importantly, developing a model that builds on our new results and uses the same parameters to predict a variety of classic experiments in the field, further supports the current approach.</p><p>If “significance” is intended as shaking previous intuitions or theories, then no: this is not a significant contribution. If instead, by significance we intend to build a solid empirical and theoretical ground for future work, then we believe this study is not significant, it is foundational. We hope that this work's significance is better captured in our discussion.</p><p>On a side note, there is an intriguing factor around transient vs. sustained channels: what matters is the amount of change, not the absolute stimulus intensity. Previous studies, for example, have suggested a positive cross modal mapping between auditory loudness and visual lightness or brightness [Odegaard et al., 2004]. This study, conversely, challenges this view and demonstrates that what matters for multisensory integration in time is not the intensity of a stimulus, but changes thereof.</p><disp-quote content-type="editor-comment"><p>In a more minor comment, I think there also needs to be a bit more effort into articulating the biological plausibility/potential instantiations of this sustained versus transient dichotomy. As written, the paper suggests that these are different &quot;channels&quot; in sensory systems, when in reality many neurons (and neural circuits) carry both on the same lines.</p></disp-quote><p>The reviewer is right, in our original manuscript we glossed over this aspect. We have now expanded the introduction to discuss their anatomical basis. However, we are not assuming any strict dichotomy between transient and sustained channels; rather, our results and simulations demonstrate that transient information is sufficient to account for audiovisual temporal integration.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>(1) Related to point 2 of the public review, can the authors provide additional results showing that the model can also account for naturalistic signals and more complex stochastic signals?</p></disp-quote><p>While working on this manuscript, we were also working in parallel on a project related to audiovisual integration of naturalistic signals. A pre-print is available online [Parise, 2024, BiorXiv], and the related study is now discussed in the conclusions.</p><disp-quote content-type="editor-comment"><p>(2) As noted in the public review, Fujisaki and Nishida (2005, 2006) already proposed mechanisms for AV correlation detection based on the Hassenstein-Reichardt motion detector. Their work should be referenced and discussed.</p></disp-quote><p>We have now acknowledged the contribution of Fujisaki and Nishida in the modelling section, when we first introduce the link between our model and the Hassenstein-Reichardt detectors.</p><disp-quote content-type="editor-comment"><p>(3) Experimental parameters: Was the phase shift manipulated in blocks? If yes, what about temporal recalibration?</p></disp-quote><p>To minimise the effect of temporal recalibration, the order of trials in our experiments was randomised. Nonetheless, we can directly assess potential short-term recalibration effects by plotting our psychophysical responses against both the current SOA, and that of the previous trials. The resulting (raw) psychometric surfaces below are averaged across observers (and conditions for Experiment 1). In all our experiments, responses are obviously dependent on the current SOA (x-axis). However, the SOA of the previous trials (y-axis) does not seem to meaningfully affect simultaneity and temporal order judgments. The psychometric curves above the heatmaps represent the average psychometric functions (marginalized over the SOA of the previous trial).</p><p>All in all, the present analyses demonstrate negligible temporal recalibration across trials, likely induced by a random sequence of lags or phase shifts. Therefore, when estimating the temporal constants of the model, it seems reasonable to ignore the potential effects of temporal recalibration. To avoid increasing the complexity of the present manuscript, we would prefer not to include the present analyses in the revised version.</p><fig id="sa3fig1" position="float"><label>Author response image 1.</label><caption><title>Effect of previous trial.</title><p>Psychometric surfaces for Experiments 1 and 2 plotted against the lag in the current vs. the previous trial. While psychophysical responses are strongly modulated by the lag in the last trial (horizontal axis), they are relatively unaffected by the lag in the previous trial (vertical axis).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90841-sa3-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>(4) The model predicts no differences for experiment 1 and this is what is empirically observed. Can the authors support these null results with Bayes factors?</p></disp-quote><p>This is a good suggestion: we have now included a Bayesian repeated measures ANOVA to the analyses of Experiment 1. As expected, these analyses provide further, though mild evidence in support for the null hypothesis (See Table S2). For completeness, the new Bayesian analyses are presented alongside the previous frequentist ones in the revised manuscript.</p></body></sub-article></article>