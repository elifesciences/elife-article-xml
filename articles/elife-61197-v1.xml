<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">61197</article-id><article-id pub-id-type="doi">10.7554/eLife.61197</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Shape-invariant encoding of dynamic primate facial expressions in human perception</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-199793"><name><surname>Taubert</surname><given-names>Nick</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5742-3889</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-198361"><name><surname>Stettler</surname><given-names>Michael</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5913-9547</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-167357"><name><surname>Siebert</surname><given-names>Ramona</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5027-8252</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-144331"><name><surname>Spadacenta</surname><given-names>Silvia</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7426-1022</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-199794"><name><surname>Sting</surname><given-names>Louisa</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-13879"><name><surname>Dicke</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-10206"><name><surname>Thier</surname><given-names>Peter</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5909-4222</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-88900"><name><surname>Giese</surname><given-names>Martin A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1178-2768</contrib-id><email>martin.giese@uni-tuebingen.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Section for Computational Sensomotorics, Centre for Integrative Neuroscience &amp; Hertie Institute for Clinical Brain Research, University Clinic Tübingen</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>International Max Planck Research School for Intelligent Systems (IMPRS-IS)</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution>Department of Cognitive Neurology, Hertie Institute for Clinical Brain Research, University of Tübingen</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kok</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>11</day><month>06</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e61197</elocation-id><history><date date-type="received" iso-8601-date="2020-07-20"><day>20</day><month>07</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-04-22"><day>22</day><month>04</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Taubert et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Taubert et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-61197-v1.pdf"/><abstract><p>Dynamic facial expressions are crucial for communication in primates. Due to the difficulty to control shape and dynamics of facial expressions across species, it is unknown how species-specific facial expressions are perceptually encoded and interact with the representation of facial shape. While popular neural network models predict a joint encoding of facial shape and dynamics, the neuromuscular control of faces evolved more slowly than facial shape, suggesting a separate encoding. To investigate these alternative hypotheses, we developed photo-realistic human and monkey heads that were animated with motion capture data from monkeys and humans. Exact control of expression dynamics was accomplished by a Bayesian machine-learning technique. Consistent with our hypothesis, we found that human observers learned cross-species expressions very quickly, where face dynamics was represented largely independently of facial shape. This result supports the co-evolution of the visual processing and motor control of facial expressions, while it challenges appearance-based neural network theories of dynamic expression recognition.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>dynamic faces</kwd><kwd>social communication</kwd><kwd>emotion expression</kwd><kwd>cross-species recognition</kwd><kwd>avatar</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004412</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>RGP0036/2016</award-id><principal-award-recipient><name><surname>Giese</surname><given-names>Martin A</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002347</institution-id><institution>Bundesministerium für Bildung und Forschung</institution></institution-wrap></funding-source><award-id>BMBF FKZ 01GQ1704</award-id><principal-award-recipient><name><surname>Stettler</surname><given-names>Michael</given-names></name><name><surname>Sting</surname><given-names>Louisa</given-names></name><name><surname>Giese</surname><given-names>Martin A</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100008316</institution-id><institution>Baden-Württemberg Stiftung</institution></institution-wrap></funding-source><award-id>NEU007/1 KONSENS-NHE</award-id><principal-award-recipient><name><surname>Taubert</surname><given-names>Nick</given-names></name><name><surname>Stettler</surname><given-names>Michael</given-names></name><name><surname>Giese</surname><given-names>Martin A</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>TH 425/12-2</award-id><principal-award-recipient><name><surname>Siebert</surname><given-names>Ramona</given-names></name><name><surname>Spadacenta</surname><given-names>Silvia</given-names></name><name><surname>Dicke</surname><given-names>Peter</given-names></name><name><surname>Thier</surname><given-names>Peter</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007065</institution-id><institution>Nvidia</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Taubert</surname><given-names>Nick</given-names></name><name><surname>Stettler</surname><given-names>Michael</given-names></name><name><surname>Giese</surname><given-names>Martin A</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>2019-SyG-RELEVANCE-856495</award-id><principal-award-recipient><name><surname>Taubert</surname><given-names>Nick</given-names></name><name><surname>Stettler</surname><given-names>Michael</given-names></name><name><surname>Giese</surname><given-names>Martin A</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution>EC CogIMon H2020</institution></institution-wrap></funding-source><award-id>ICT-23-2014/644727</award-id><principal-award-recipient><name><surname>Taubert</surname><given-names>Nick</given-names></name><name><surname>Giese</surname><given-names>Martin A</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Contrasting with neural network theories, a study of the cross-species perception of dynamic faces with highly realistic human and monkey avatars reveals independent perceptual encoding of facial shape and expression.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Facial expressions are crucial for social communication of human as well as non-human primates (<xref ref-type="bibr" rid="bib5">Calder, 2011</xref>; <xref ref-type="bibr" rid="bib14">Darwin, 1872</xref>; <xref ref-type="bibr" rid="bib23">Jack and Schyns, 2017</xref>; <xref ref-type="bibr" rid="bib10">Curio et al., 2010</xref>), and humans can learn facial expressions even of other species (<xref ref-type="bibr" rid="bib33">Nagasawa et al., 2015</xref>). While facial expressions in everyday life are dynamic, specifically, expression recognition across different species has been studied mainly using static pictures of faces (<xref ref-type="bibr" rid="bib6">Campbell et al., 1997</xref>; <xref ref-type="bibr" rid="bib12">Dahl et al., 2013</xref>; <xref ref-type="bibr" rid="bib41">Sigala et al., 2011</xref>; <xref ref-type="bibr" rid="bib20">Guo et al., 2019</xref>; <xref ref-type="bibr" rid="bib11">Dahl et al., 2009</xref>). A few studies have compared the perception of human and monkey expressions using movie stimuli, finding overlaps in the brain activation patterns induced by within- and cross-species expression observation in humans as well as monkeys (<xref ref-type="bibr" rid="bib49">Zhu et al., 2013</xref>; <xref ref-type="bibr" rid="bib36">Polosecki et al., 2013</xref>). Since natural video stimuli provide no accurate control of the dynamics and form features of facial expressions, it is unknown how expression dynamics is perceptually encoded across different primate species and how it interacts with the representation of facial shape.</p><p>In primate phylogenesis, the visual processing of dynamic facial expressions has co-evolved with the neuromuscular control of faces (<xref ref-type="bibr" rid="bib38">Schmidt and Cohn, 2001</xref>). Remarkably, the structure and arrangement of facial muscles is highly similar across different primate species (<xref ref-type="bibr" rid="bib45">Vick et al., 2007</xref>; <xref ref-type="bibr" rid="bib34">Parr et al., 2010</xref>), while face shapes differ considerably, for example, between humans, apes, and monkeys. This motivates the following two hypotheses: (1) The phylogenetic continuity in motor control should facilitate fast learning of dynamic expressions across primate species and (2) the different speeds of the phylogenetic development of the facial shape and its motor control should potentially imply a separate visual encoding of expression dynamics and basic face shape. The second hypothesis seems consistent with a variety of data in functional imaging, which suggests a partial separation of the anatomical structures processing changeable and non-changeable aspects of faces (<xref ref-type="bibr" rid="bib21">Haxby et al., 2000</xref>; <xref ref-type="bibr" rid="bib2">Bernstein and Yovel, 2015</xref>).</p><p>We investigated these hypotheses, exploiting advanced methods from computer animation and machine learning, combined with motion capture in monkeys and humans. We designed highly realistic three-dimensional (3D) human and monkey avatar heads by combining structural information derived from 3D scans, multi-layer texture models for the reflectance properties of the skin, and hair animation. Expression dynamics was derived from motion capture recordings on monkeys and humans, exploiting a hierarchical generative Bayesian model to generate a continuous motion style space. This space includes continuous interpolations between two expression types (‘anger’ vs. ‘fear’), and human- and monkey-specific motions. Human observers categorized these dynamic expressions, presented on the human or the monkey head model, in terms of the perceived expression type and species-specificity of the motion (human vs. monkey expression).</p><p>Consistent with our hypotheses, we found very fast cross-species learning of expression dynamics with a typically narrower tuning for human- compared to monkey-specific expressions. Most importantly, the perceptual categorization of expression dynamics was largely independent of the facial shape (human vs. monkey). In particular, the accuracy of the categorization of species-specific dynamic facial expressions did not show a dependence on whether the species-specific expressive motion and the avatar species were matching (e.g., monkey expressions being recognized more accurately on a monkey avatar). Our results were highly robust against substantial variations in the expressive stimulus features. They specify fundamental constraints for the computational neural mechanisms of dynamic face processing and challenge popular neural network models, accounting for expression recognition by the learning of sequences of key shapes (e.g. <xref ref-type="bibr" rid="bib10">Curio et al., 2010</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In this section, we briefly sketch the methodology of our experiments; whereas many other important details can be found in 'Materials and methods' section and 'Appendix 1'. Then, we describe in detail the results of the three main experiments, which we realized (further control experiments are described in 'Appendix 1').</p><p>Our studies investigated the perceptual representations of dynamic human and monkey facial expressions in human observers, exploiting photo-realistic human and monkey face avatars (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The motion of the avatars was generated exploiting motion capture data of both primate species (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), which were used to compute the corresponding deformation of the surface mesh of the face, exploiting a model based on elastic ribbon structures that were modeled after the main facial muscles of humans and monkeys (<xref ref-type="fig" rid="fig1">Figure 1C</xref> and Appendix 1).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Stimulus generation and paradigm.</title><p>(<bold>A</bold>) Frame sequence of a monkey and a human facial expression. (<bold>B</bold>) Monkey motion capture with 43 reflecting facial markers. (<bold>C</bold>) Regularized face mesh, whose deformation is controlled by an embedded elastic ribbon-like control structure that is optimized for animation. (<bold>D</bold>) Stimulus consisting of 25 motion patterns, spanning up a two-dimensional style space with the dimensions ‘expression’ and ‘species’, generated by interpolation between two expressions (‘anger/threat’ and ‘fear’) and the two species (‘monkey’ and ‘human’). Each motion pattern was used to animate a monkey and a human avatar model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61197-fig1-v1.tif"/></fig><p>In order to realize a full parametric control of motion style, we exploited a Bayesian motion morphing technique ('Materials and methods') to create a continuous expression space that smoothly interpolates between human and monkey expressions. We used two human expressions and two monkey expressions as basic patterns, which represented corresponding emotional states (‘fear’ and ‘anger/threat’). Interpolating between these four prototypical motions in five equidistant steps, we generated a set of 25 facial movements that vary in five steps along two dimensions, the expression type, and the species, as illustrated in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. Each generated motion pattern can be parameterized by a two-dimensional style vector (<italic>e</italic>, <italic>s</italic>), where the first component <italic>e</italic> specifies the expression type (<inline-formula><mml:math id="inf1"><mml:mi>e</mml:mi><mml:mi/><mml:mo>=</mml:mo><mml:mi/><mml:mn>0</mml:mn></mml:math></inline-formula>: expression 1 (‘fear’) and <inline-formula><mml:math id="inf2"><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>: expression 2 (‘anger/threat’)), and where the second variable <italic>s</italic> defines the species-specificity of the motion (<inline-formula><mml:math id="inf3"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>: monkey and <inline-formula><mml:math id="inf4"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>: human). The dynamic expressions were used to animate a highly realistic monkey as well as a human avatar model (generation; 'Materials and methods'). In order to vary the two-dimensional stimulus features, we rendered the avatars from two different view angles: from the front view and from the view that was rotated by 30 degrees about the vertical axis. This rotated view maximized the differences of the two-dimensional appearance relative to the front view, while avoiding strong salient changes, such as occlusions of face parts. The following sections describe the results of the three main experiments of our study.</p><sec id="s2-1"><title>Dynamic expression perception is largely independent of facial shape</title><p>In our first experiment, we used the original dynamic expressions of humans and monkeys as prototypes and presented morphs between them, separately, on the human and the monkey avatar faces, with two different view angles (0 and 30 degrees rotation about the vertical axis). Facial movements of humans and monkeys are quite different (<xref ref-type="bibr" rid="bib45">Vick et al., 2007</xref>), so that our participants, all of whom had no prior experience with macaque monkeys, needed to be familiarized briefly with the monkey expressions prior to the main experiment. During the familiarization, participants learned to recognize the four prototypical expressions perfectly, always with maximally four stimulus repetitions. During the main experiment, motions were presented in a block-randomized order, and in separate blocks for the two avatars and for the two tested views. The expression movies with a duration of 5 s showed the face going from a neutral expression to the extreme expression and back to neutral (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Participants observed 10 repetitions of each stimulus. They had to decide whether the observed stimulus was looking more like a human or a monkey expression (independent of the avatar shape and view), and whether the expression was rather ‘anger/threat’ or ‘fear’. The resulting two binary responses in each trial can be interpreted as assignment of one out of four classes to the perceived expression of the stimulus, independent of avatar type and view (1: human-angry, 2: human-fear, 3: monkey-threat, and 4: monkey-fear).</p><p><xref ref-type="fig" rid="fig2">Figure 2A</xref> shows the raw classification data as histograms of the relative frequencies of the four classes <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, as a function of the style parameters <italic>e</italic> and <italic>s</italic> for the four tested classes. The class probabilities <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> were modeled by a logistic multinomial regression model ('Materials and methods'), resulting in the fitted discriminant functions that are shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref> for the different classes. Comparing regression models with different sets of predictor variables, we found that in almost all cases, a model of the form that contains the two style variables for expression (<inline-formula><mml:math id="inf7"><mml:mi>e</mml:mi></mml:math></inline-formula>) and the species (<inline-formula><mml:math id="inf8"><mml:mi>s</mml:mi></mml:math></inline-formula>) as predictors (in addition to a constant predictor) was the simplest model that provided good fits of the data. <xref ref-type="fig" rid="fig2">Figure 2C</xref> shows the prediction accuracy of regression models with different sets of predictors for the monkey avatar stimulus (data from the other conditions are presented in Appendix 1). The different models were compared quantitatively using prediction accuracy and the Bayesian Information Criterion (BIC). Specifically, a model that also included the product <italic>e</italic>⋅<italic>s</italic> did not provide significantly better prediction results, except for a very small improvement of the prediction accuracy for the rotated view conditions. Models only including one of the predictors, <italic>e</italic> or s, provided significantly worse fits. Likewise, models that contained the average amount of optic flow as the additional predictor did not result in higher prediction accuracy (see Appendix 1 for details.). This implies that simple motion features, such as the amount of optic flow, do not provide a trivial explanation of our results. Summarizing, both style variables <italic>e</italic> and <italic>s</italic> are necessary as predictors, and there is no strong interaction between them. This motivated us to use the model with these two predictor variables for our further analyses.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Raw data and statistical analysis.</title><p>(<bold>A</bold>) Histograms of the classification data for the four classes (see text) as functions of the style parameters e and s. Data is shown for the human avatar, front view, using the original motion-captured expressions as prototypes. (<bold>B</bold>) Fitted discriminant functions using a logistic multinomial regression model (see 'Materials and methods'). Data is shown for the human avatar, front view, using the original motion-captured expressions as prototypes. (<bold>C</bold>) Prediction accuracy of the multinomial regression models with different numbers of predictors (constant predictor, only style variable <italic>e</italic> or <italic>s</italic>, and both of them).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61197-fig2-v1.tif"/></fig><p><xref ref-type="fig" rid="fig3">Figure 3A</xref> shows a comparison of all fitted discriminant functions, shown separately for the two avatar types and for the two tested view conditions. These functions show the predicted class probabilities as functions of the two style parameters <italic>e</italic> and <italic>s</italic>. The form of these discriminant functions is highly similar between the two avatar types and also between the view conditions. This is confirmed by the fact that the fraction of the variance that is different between these functions divided by the one that is shared between them does not exceed 3% (<inline-formula><mml:math id="inf9"><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>2.75</mml:mn><mml:mi>%</mml:mi></mml:math></inline-formula>; see 'Materials and methods'). The same conclusion is also supported by a comparison of the multinomially distributed classification responses using a contingency table analysis (see 'Materials and methods'), across the four conditions (avatar types and views), separately for the different points in morphing space and across participants. This analysis revealed that only for three stimuli (12%) of the style space, the classification responses were significantly different (<inline-formula><mml:math id="inf10"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:math></inline-formula>, Bonferroni-corrected). Differences tended to be larger especially for intermediate values of the style space coordinates <italic>e</italic> and <italic>s</italic>, thus for the stimuli with high perceptual ambiguity (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). This result implies that primate facial expressions are perceptually encoded largely independently of the head shape (human vs. monkey) and of the stimulus view. Especially, this implies substantial independence of this encoding of the two-dimensional image features, which vary substantially between the view conditions, and even more between the human and the monkey avatar model. This observed independence might also explain why many of our subjects were able to recognize <italic>human</italic> facial expressions on the monkey avatar face, even without any familiarization. This matches the common experience that humans can recognize dynamic facial expressions spontaneously even from non-human comic figures, which often are highly unnatural.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Fitted discriminant functions P<sub>i</sub>(e,s) for the original stimuli.</title><p>Classes correspond to the four prototype motions, as specified in <xref ref-type="fig" rid="fig1">Figure 1D</xref> (i = 1: human-angry, 2: human-fear, 3: monkey-threat, 4: monkey-fear). (<bold>A</bold>) Results for the stimuli generated using original motion-captured expressions of humans and monkeys as prototypes, for presentation on a monkey and a human avatar. (<bold>B</bold>) Significance levels (Bonferroni-corrected) of the differences between the multinomially distributed classification responses for the 25 motion patterns, presented on the monkey and human avatar.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61197-fig3-v1.tif"/></fig></sec><sec id="s2-2"><title>Tuning is narrower for human-specific than for monkey-specific dynamic expressions</title><p>A biologically important question is whether expressions of one’s own species are processed differently from those of other primate species, potentially supporting an <italic>own-species advantage</italic> in the processing of dynamic facial expressions (<xref ref-type="bibr" rid="bib13">Dahl et al., 2014</xref>). In order to characterize the tuning of the perceptual representation for monkey vs. human expressions, we computed tuning functions, by marginalizing the discriminant functions belonging to the same species category (<italic>P</italic><sub>1</sub> and <italic>P</italic><sub>2</sub> belonging to the human, and <italic>P</italic><sub>3</sub> and <italic>P</italic><sub>4</sub> to the monkey expressions) over the expression dimension <italic>e</italic> (see 'Materials and methods' for details). <xref ref-type="fig" rid="fig4">Figure 4A</xref> shows the resulting two species-tuning functions <italic>D</italic><sub>H</sub>(<italic>s</italic>) and <italic>D</italic><sub>M</sub>(<italic>s</italic>), revealing a smaller tuning width for the human than for the monkey expressions for all stimulus types, except for the 30 degrees rotated human condition.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Tuning functions.</title><p>(<bold>A</bold>) Fitted species-tuning functions <italic>D</italic><sub>H</sub>(<italic>s</italic>) (solid lines) and <italic>D</italic><sub>M</sub>(<italic>s</italic>) (dashed lines) for the categorization of patterns as monkey vs. human expressions, separately for the two avatar types (human and monkey) and the two view conditions. Different line styles indicate the experiments using original motion-captured motion, stimuli with occluded ears, and the experiment using prototype motions that were equilibrated for the amount of motion/deformation across prototypes. (<bold>B</bold>) Thresholds of the tuning functions for the three experiments for presentation on the two avatar types and the two view angles. (<bold>C</bold>) Steepness of the tuning functions at the threshold points for the experiments with and without equilibration of the prototype motions, and with occlusion of the ears.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61197-fig4-v1.tif"/></fig><p>The fitted threshold values are given by the conditions <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi/><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi/><mml:mn>0.5</mml:mn></mml:math></inline-formula> and are shown in <xref ref-type="fig" rid="fig4">Figure 4B</xref> for the monkey- and the human-specific motion (solid vs. dashed lines). This observation is confirmed by computing the threshold values of the tuning functions by fitting them with a sigmoidal function (see 'Materials and methods'). Comparing the threshold values by running separate ANOVAs for the four stimulus types (monkey and human front view, and monkey and human rotated view), we found significantly narrower tuning for the human than for the monkey expression for all tested conditions, except for the human avatar in the 30 degrees condition. These two-way mixed-model ANOVAs include the expression type (human vs. monkey motion) as within-subject factor and the stimulus type (original motion, stimuli with occluded ears, or animated with equilibrated motion; see below) as between-subject factor. The ANOVAs reveal a strong effect of the expression type (<italic>F</italic><inline-formula><mml:math id="inf12"><mml:mfenced separators="|"><mml:mrow><mml:mn>1,66</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>188.82</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf13"><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi/><mml:mn>66</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>46.39</mml:mn></mml:math></inline-formula>, and <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>40</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>127.35</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively), except for the human 30 degrees condition, where the influence of this factor did not reach significance (<inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>40</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.43</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.23</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). In all cases, we failed to find a significant influence of the stimulus type (<inline-formula><mml:math id="inf16"><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>2,66</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.0</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf17"><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>2,66</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf18"><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>1,40</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.002</mml:mn></mml:math></inline-formula>, and <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>40</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.014</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.91</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively). Interactions between stimulus type and expression type were found for all conditions (<inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>66</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>4.51</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.015</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf21"><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>2,66</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>3.15</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.049</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>40</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>8.31</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.007</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively), but not for the human 30 degrees condition (<inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>40</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.735</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.39</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>Summarizing, there is a strong tendency of the species-specific expression tuning to be narrower for the human ‘own-species’ expressions, while this tendency is not as prominent in rotated views.</p></sec><sec id="s2-3"><title>Robustness of results against variations of species-specific features</title><p>One may ask whether the previous observations are robust with respect to variations of the chosen stimuli. For example, monkey facial movements include <italic>species-specific features</italic>, such as ear motion, that are not present in human expressions. Do the observed differences between the recognition of human and monkey expressions depend on these features? We investigated this question by repeating the original experiment, presenting only the front view, with a new set of participants, using stimuli for which the ear region was occluded. <xref ref-type="fig" rid="fig5">Figure 5A</xref> depicts the corresponding fitted discriminant functions, which are quite similar to the ones without occlusion, characterized again by a high similarity in shape between the human and monkey avatar (ratio of different vs. shared variance: <inline-formula><mml:math id="inf24"><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>1.44</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:math></inline-formula>; only 12% of the categorization responses over the 25 points in morphing space were significantly different between the two avatar types; <inline-formula><mml:math id="inf25"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:math></inline-formula>; <xref ref-type="fig" rid="fig5">Figure 5B</xref>). <xref ref-type="fig" rid="fig4">Figure 4A</xref> also shows that the corresponding tuning functions <italic>D</italic><sub>M</sub> and <italic>D</italic><sub>H</sub> are very similar to the ones for the non-occluded stimuli, and the associated threshold values (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) are not significantly different from the one for non-occluded stimuli (see 'ANOVA analysis'). Summarizing, the elimination of ear motion as a monkey-specific feature did not have a major influence on the main results of the original experiment.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Fitted discriminant functions P<sub>i</sub>(e,s) for the condition with occlusions of the ears.</title><p>Classes correspond to the four prototype motions, as specified in <xref ref-type="fig" rid="fig1">Figure 1D</xref> (i = 1: human-angry, 2: human-fear, 3: monkey-threat, 4: monkey-fear). (<bold>A</bold>) Results for the stimuli generated using original motion-captured expressions of humans and monkeys as prototypes but with occluded ears, for presentation on a monkey and a human avatar (only using the front view). (<bold>B</bold>) Significance levels (Bonferroni-corrected) of the differences between the multinomially distributed classification responses for the 25 motion patterns, presented on the monkey and human avatar.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61197-fig5-v1.tif"/></fig></sec><sec id="s2-4"><title>Robustness against variations of expressivity</title><p>A further possible concern might be that the chosen prototypical expressions might specify different amounts of salient low-level features, for example, due to species differences in the motion, or because of differences between the anatomies of the human and the monkey face. In order to control for the influence of such expressive low-level information, we re-ran the main conditions of the experiment with stimuli that were equilibrated (balanced) for the amount of such expressive information.</p><p>Our equilibration procedure was based on a pilot experiment that compared equilibration methods based on different types of measures for low-level information. This included the total amount of optic flow (OF), the maximum deformation of the polygon mesh during the expression (DF), and the total motion flow of the polygon mesh during the expression (MF) (see 'Materials and methods' for details). In the control experiment, nine participants rated these equilibrated stimulus sets in terms of the perceived expressivity of their motion (independent of avatar type). Perceived expressivity was assessed by ratings using a nine-point Likert scale (1: non-expressive, 9: very expressive), presenting each stimulus in a block-randomized manner for four times.</p><p>The averages of these ratings, comparing the different low-level measures, are shown in <xref ref-type="fig" rid="fig6">Figure 6A</xref>. In addition, this figure also shows the ratings for the neutral expression, which are very low, and the ratings for the original non-equilibrated expressions. It turns out that balancing the amount of polygon motion (MF) resulted in the lowest standard deviation of the expressivity ratings after equilibration (except for the neutral condition <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1.479</mml:mn><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.021</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Equilibration of low-level expressive information.</title><p>(<bold>A</bold>) Mean perceived expressivity ratings for stimulus sets that were equilibrated using different types of measures for the amount of expressive low-level information: OF: optic flow computed with an optic flow algorithm; DF: shape difference compared to the neutral face (measured by the 2D distance in polygon vertex space); MF: two-dimensional motion of the polygons on the surface of the face. In addition, the ratings for a static neutral face are shown as reference point for the rating (neutral). (<bold>B</bold>) Extreme frames of the monkey threat prototype before and after equilibration using the MF measure (<bold>C</bold>) 2D polygon motion flow (MF) computed for the 25 stimuli in our expression style space for the monkey avatar for the front view (similar results were obtained for the other stimulus types).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61197-fig6-v1.tif"/></fig><p>More specifically, perceived expressivity showed smaller variance for the MF condition than for the DF conditions for the human avatar (<inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>142</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.479</mml:mn><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.021</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Also, for the monkey avatar, this variance was smaller than for all other conditions (<inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>1.403</mml:mn><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.045</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), except for the DF condition (<inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>142</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.869</mml:mn><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.407</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Moreover, the difference of the perceived expressiveness between the two avatars was non-significant (<inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>283</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.937</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.349</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) for equilibration with the DF measure. For these reasons, and also because it resulted in the equilibrated stimuli with the highest expressivity, we decided to use MF as a measure of the equilibration of the prototype motion in our main experiment (a more extensive analysis of these data and additional tested measures for low-level expressive information are discussed in Appendix 1).</p><p>Equilibration was based on creating morphs between the original motion-captured expressions and a neutral expression, varying the morphing weight of the neutral expression in a way that resulted in a matching of the amount of motion flow (see 'Materials and methods'). Equilibration was realized separately for the two avatars and also for the different view conditions. <xref ref-type="fig" rid="fig6">Figure 6B</xref> shows an example of the effect of equilibration on the extreme frames of a monkey-threat expression. The equilibration also reduces the very salient mouth opening motion of the monkey, which, due to anatomical differences, cannot be realized by a real human face. The efficiency of the procedure in balancing the amount of motion information is illustrated in <xref ref-type="fig" rid="fig6">Figure 6C</xref>. It illustrates the motion flow before and after equilibration for the different points of our motion style space for the front view. The standard deviation of the motion flow across the 25 conditions in style space is reduced by 83% for the monkey avatar and by 54% for the human avatar by the equilibration. Constraining the flow analysis to the mouth region, we found that the standard deviation of the corresponding motion flow across conditions was reduced by 79% for the monkey avatar and by 59% for the human avatar (results for the other view conditions are similar).</p><p>The fitted discriminant functions for the data from the repetition of the experiment with equilibrated stimuli are shown in <xref ref-type="fig" rid="fig7">Figure 7A</xref>. These functions are more symmetrical along the axes of the morphing space than for the original stimuli (for example, this reduces the amount of confusions of human anger and monkey fear expressions that occurs for intermediate levels of the style parameters, especially for the human avatar, potentially due to the subtlety of the monkey fear expression). This is corroborated by the fact that an asymmetry index (AI) that measures the deviation from a perfect symmetry with respect to the <italic>e</italic> and <italic>s</italic> axes (see Appendix 1) is significantly reduced for the data from the experiment with equilibrated stimuli compared to the data from the experiment using the original motion prototypes (<inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.624</mml:mn><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/></mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>b</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.504</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), the difference being significant according to the Wilcoxon signed-rank test (<inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mn>2.49</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.013</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Compared to the original stimuli, we found an even higher similarity of the discriminant functions between the two avatar types and the different view conditions. This is corroborated by the small ratios of different vs. shared variance between the conditions (<inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>4.01</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), where only 4% of the categorization responses across the 25 points in morphing space were significantly different between the avatar types and view conditions, according to a contingency table analysis (<xref ref-type="fig" rid="fig7">Figure 7B</xref>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Fitted discriminant functions P<sub>i</sub>(e,s) for the experiment with equilibration of expressive information.</title><p>Classes correspond to the four prototype motions, as specified in <xref ref-type="fig" rid="fig1">Figure 1D</xref> (i = 1: human-angry, 2: human-fear, 3: monkey-threat, 4: monkey-fear). (<bold>A</bold>) Results for the stimuli set derived from prototype motions that were equilibrated with respect to the amount of local motion/deformation information, for presentation on a monkey and a human avatar. (<bold>B</bold>) Significance levels (Bonferroni-corrected) of the differences between the multinomially distributed classification responses for the 25 motion patterns, presented on the monkey and human avatar.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61197-fig7-v1.tif"/></fig><p>Most importantly, also for these equilibrated stimulus sets, we found a narrower tuning for the human than for the monkey dynamic expressions (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). This is confirmed by the results of the ANOVA for the threshold points of the tuning functions <italic>D</italic><sub>M</sub>(<italic>s</italic>) and <italic>D</italic><sub>H</sub>(<italic>s</italic>) (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), which failed to show a significant influence of the factor stimulus type (original vs. occluded vs. equilibrated stimuli) (see above).</p><p>An analysis of the steepness of the fitted threshold functions is shown in <xref ref-type="fig" rid="fig4">Figure 4C</xref>. This analysis shows that the equilibration procedure effectively balances the steepness of the tuning functions between the human and the monkey expressions, which is apparent in the non-equilibrated stimuli. This observation is confirmed by two-way ANOVAs for the original motion stimuli and the ones with occluded ears, which show significant influences of the factor avatar type/view (<inline-formula><mml:math id="inf34"><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>3,83</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>12.76</mml:mn><mml:mo>;</mml:mo></mml:math></inline-formula> <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.006</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>39</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>3.33</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.077</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively), but not of the expression type (<inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>83</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>39</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.92</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), and no interactions. Contrasting with this result, the ANOVA for the stimuli with equilibrated motion does not show any significant effects, neither of the avatar type and view (<inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>87</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.27</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.26</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), nor of the expression type (<inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>87</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.03</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.86</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), nor of an interaction (full ANOVAS' results in <xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref>).</p><p>Summarizing, these results show that the high similarity of the classification data of the stimuli between the two different avatar types, and between the different view conditions, was not fundamentally changing if the expressiveness of the stimuli was controlled. Also, the tendency for a narrower tuning for human own-species expressions was robust against this manipulation. However, balancing expressiveness leveled out the differences in the steepness of the computed species-tuning functions. This rules out the objection that the observed effects are just an implication of differences in the amount of low-level salient features of the chosen prototypical motion patterns.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Due to the technical difficulties of an exact control of dynamics of facial expressions (<xref ref-type="bibr" rid="bib25">Knappmeyer et al., 2003</xref>; <xref ref-type="bibr" rid="bib22">Hill et al., 2005</xref>), in particular of animals, the computational principles of the perceptual representation of dynamic facial expressions remain largely unknown. Exploiting advanced methods from computer animation with motion capture across species and machine-learning methods for motion interpolation, our study reveals fundamental insights about the perceptual encoding of dynamic facial expressions across primate species. At the same time, the developed technology lays the ground for physiological studies with highly controlled stimuli on the neural encoding of such dynamic patterns (<xref ref-type="bibr" rid="bib36">Polosecki et al., 2013</xref>; <xref ref-type="bibr" rid="bib8">Chandrasekaran et al., 2013</xref>; <xref ref-type="bibr" rid="bib1">Barraclough et al., 2005</xref>; <xref ref-type="bibr" rid="bib16">Furl et al., 2012</xref>).</p><p>Our first key observation was that facial expressions of macaque monkeys were learned very quickly by human observers. This was the case even though monkey expressions are quite different from human expressions, so that naive observers could not interpret them spontaneously. This fast learning might be a consequence of the high similarity of the neuromuscular control of facial movements in humans and macaques (<xref ref-type="bibr" rid="bib34">Parr et al., 2010</xref>), resulting in a high similarity of the structural properties of the expression dynamics that can be exploited by the visual system for fast learning.</p><p>Secondly, and unexpectedly from shape-based accounts for dynamic expression recognition, we found that the categorization of dynamic facial expressions was astonishingly independent of the type of primate face (human vs. monkey) and of the stimulus view (0 vs. 30 degrees of rotation of the head about the vertical axis). Clearly, this shows a substantial degree of invariance against changes of the two-dimensional image features. More specifically, we neither found strong differences between categorization responses dependent of these parameters, nor did we find a better perceptual representation of species-specific dynamic expressions that match the species of the avatar (e.g., a more accurate representation of human expressions on the human avatar or of monkey expressions on the monkey avatar). Facial expression dynamics seems thus represented independently of the detailed shape features of the primate head and of the stimulus view.</p><p>Yet, we found a clear and highly robust own-species advantage (<xref ref-type="bibr" rid="bib39">Scott and Fava, 2013</xref>; <xref ref-type="bibr" rid="bib35">Pascalis et al., 2005</xref>) in terms of the accuracy of the tuning for expression dynamics: the tuning along the species axis of our motion style space was narrower for human than for monkey expressions. This remained true even for stimuli that eliminated species-specific features, such as ear motion, or which were carefully equilibrated in terms of the amount of low-level information.</p><p>Both key results support our initial hypotheses: perception can exploit the similarity of the structure of dynamic expressions across different primate species for fast learning. At the same time, and consistent with a co-evolution of the visual processing of dynamic facial expressions with their motor control, we found a largely independent encoding of facial expression dynamics from a basic facial shape in primate expressions. This observed independence has to be further confirmed in more extended experiments, including a bigger spectrum of facial shapes and, probably, even faces from non-primate species. In fact, the observation that humans observe facial expressions readily from comic characters, which do not even correspond to existing species, suggests that the observed invariance goes far beyond primate faces. However, further experiments including a much wider spectrum of facial shapes will be required to confirm this more general hypothesis.</p><p>The observed independence of basic facial shape and expression encoding seems in-line with results from functional imaging studies that suggest a modular representation of different aspects of faces, such as changeable and non-changeable ones (<xref ref-type="bibr" rid="bib21">Haxby et al., 2000</xref>; <xref ref-type="bibr" rid="bib2">Bernstein and Yovel, 2015</xref>; <xref ref-type="bibr" rid="bib15">Dobs et al., 2019</xref>). At the same time, it is difficult to reconcile our experiments with several popular (recurrent) neural network models that represent facial expressions in terms of sequences of learned key shapes (<xref ref-type="bibr" rid="bib10">Curio et al., 2010</xref>; <xref ref-type="bibr" rid="bib29">Li and Deng, 2020</xref>). Since the shape differences between human and monkey faces are much larger than the ones between the keyframes from the same expression, the observed spontaneous generalization of dynamic expressions to faces from a different primate species seems difficult to account for by such models.</p><p>Concrete circuits for a shape-independent encoding of expression dynamics still have to be discovered. One possibility is that they might exploit optic-flow analysis (<xref ref-type="bibr" rid="bib19">Giese and Poggio, 2003</xref>; <xref ref-type="bibr" rid="bib24">Jhuang et al., 2007</xref>), since the optic flow of the expressions on different head models might be similar. Another possibility would be mechanisms that are based on ‘vectorized encoding’, where the face shape in individual stimulus frames is encoded in terms of their differences in feature space from a ‘reference’ or ‘norm face’ (<xref ref-type="bibr" rid="bib17">Giese, 2016</xref>; <xref ref-type="bibr" rid="bib28">Leopold et al., 2006</xref>; <xref ref-type="bibr" rid="bib37">Rhodes and Jeffery, 2006</xref>; <xref ref-type="bibr" rid="bib3">Beymer and Poggio, 1996</xref>). We have demonstrated elsewhere that a very robust recognition of dynamic facial expressions can be accomplished by a neural recognition model that is based on this encoding principle (<xref ref-type="bibr" rid="bib43">Stettler, 2020</xref>), where norm-referenced encoding had been shown to account for the identity tuning of face-selective neurons in area IT (<xref ref-type="bibr" rid="bib28">Leopold et al., 2006</xref>; <xref ref-type="bibr" rid="bib18">Giese and Leopold, 2005</xref>). The presented novel technology for the generation of highly realistic dynamic face avatars of humans and monkeys enables electrophysiological studies that clarify the exact underlying neural mechanisms. A similar methodological approach was quite successful for discovering of the neural mechanisms of the identity of static faces (e.g., <xref ref-type="bibr" rid="bib28">Leopold et al., 2006</xref>; <xref ref-type="bibr" rid="bib32">Murphy and Leopold, 2019</xref>; <xref ref-type="bibr" rid="bib9">Chang and Tsao, 2017</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th>Reagent type <break/>(species) or resource</th><th>Designation</th><th>Source or reference</th><th>Identifiers</th><th>Additional information</th></tr></thead><tbody><tr><td valign="top">Software, algorithm</td><td valign="top">Custom-written software written in C#</td><td valign="top">This study</td><td valign="top"><ext-link ext-link-type="uri" xlink:href="https://hih-git.neurologie.uni-tuebingen.de/ntaubert/FacialExpressions">https://hih-git.neurologie.uni-tuebingen.de/ntaubert/FacialExpressions</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:a1b3bc71b83e80f5d5ea29646533caf2ab65b0ee;origin=https://hih-git.neurologie.uni-tuebingen.de/ntaubert/FacialExpressions;visit=swh:1:snp:1a7157ee21eb78b289fee45d5fa0d738f0196a12;anchor=swh:1:rev:6d041a0a0cc7055618f85891b85d76e0e7f80eed">swh:1:rev:6d041a0a0cc7055618f85891b85d76e0e7f80eed</ext-link>; <xref ref-type="bibr" rid="bib44">Taubert, 2021</xref>)</td><td/></tr><tr><td valign="top">Software, algorithm</td><td valign="top">C3Dserver</td><td valign="top">Website</td><td valign="top"><ext-link ext-link-type="uri" xlink:href="https://www.c3dserver.com">https://www.c3dserver.com</ext-link></td><td/></tr><tr><td valign="top">Software, algorithm</td><td valign="top">Visual C++ Redistributable for Visual Studio 2012 Update4 × 86 and x64</td><td valign="top">Website</td><td valign="top"><ext-link ext-link-type="uri" xlink:href="https://www.microsoft.com/en-US/download/details.aspx?id=30679">https://www.microsoft.com/en-US/download/details.aspx?id=30679</ext-link></td><td/></tr><tr><td valign="top">Software, algorithm</td><td valign="top">AssimpNet</td><td valign="top">Website</td><td valign="top"><ext-link ext-link-type="uri" xlink:href="https://www.nuget.org/packages/AssimpNet">https://www.nuget.org/packages/AssimpNet</ext-link></td><td/></tr><tr><td valign="top">Software, algorithm</td><td valign="top">Autodesk Maya 2018</td><td valign="top">Website</td><td valign="top"><ext-link ext-link-type="uri" xlink:href="https://www.autodesk.com/education/free-software/maya">https://www.autodesk.com/education/free-software/maya</ext-link></td><td/></tr><tr><td valign="top">Software, algorithm</td><td valign="top">MATLAB 2019b</td><td valign="top">Website</td><td valign="top"><ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/products/matlab.html">https://www.mathworks.com/products/matlab.html</ext-link></td><td/></tr><tr><td valign="top">Software, algorithm</td><td valign="top">Psychophysics toolbox 3.0.15</td><td valign="top">Website</td><td valign="top"><ext-link ext-link-type="uri" xlink:href="http://psychtoolbox.org/">http://psychtoolbox.org/</ext-link></td><td valign="top"/></tr><tr><td valign="top">Software, algorithm</td><td valign="top">R 3.6</td><td valign="top">Website</td><td valign="top"><ext-link ext-link-type="uri" xlink:href="https://www.r-project.org/">https://www.r-project.org/</ext-link></td><td/></tr><tr><td valign="top">Other</td><td valign="top">Training data for interpolation algorithm</td><td valign="top">This study</td><td valign="top"><ext-link ext-link-type="uri" xlink:href="https://hih-git.neurologie.uni-tuebingen.de/ntaubert/FacialExpressions/tree/master/Data/MonkeyHumanFaceExpression">https://hih-git.neurologie.uni-tuebingen.de/ntaubert/FacialExpressions/tree/master/Data/MonkeyHumanFaceExpression</ext-link></td><td/></tr><tr><td valign="top">Other</td><td valign="top">Stimuli for experiments</td><td valign="top">This study</td><td valign="top"><ext-link ext-link-type="uri" xlink:href="https://hih-git.neurologie.uni-tuebingen.de/ntaubert/FacialExpressions/tree/master/Stimuli">https://hih-git.neurologie.uni-tuebingen.de/ntaubert/FacialExpressions/tree/master/Stimuli</ext-link></td><td/></tr></tbody></table></table-wrap><sec id="s4-1"><title>Human participants</title><p>In total, 78 human participants (42 females) participated in the psychophysical studies. The age range was 21–53 years (mean 26.2, standard deviation 4.71). All participants had no prior experience with macaque monkeys and normal or to-normal corrected <bold>v</bold>ision. Participants gave written informed consent and were reimbursed with 10 EUR per hour for the experiment. In total, 31 participants (16 females) were taking part in the first experiment using stimuli based on the original motion capture data and the experiment with occlusion of the ears. 22 participants (13 females) took part in the experiment with equilibrated motion of the prototypes. In addition, 16 participants (eight females) took part in a Turing test control experiment (see below), and nine (five females) participants took part in a control experiment to identify features that influence perceived expressiveness of the stimuli. All psychophysical experiments were approved by the Ethics Board of the University Clinic Tübingen and were consistent with the rules of the Declaration of Helsinki.</p></sec><sec id="s4-2"><title>Stimulus presentation</title><p>Subjects were presented the stimuli watching a computer screen at a distance of 70 cm in a dark room (view angle about 12 degrees), with a resolution of 720 × 720 pixels using <italic>MATLAB</italic> and the <italic>Psychotoolbox (3.0.15)</italic> library for stimulus presentation. Each stimulus was repeated for a maximum of three times before asking for the responses, but participants could skip after the first presentation if they were certain about their responses. Participants were first asked whether the perceived expression was rather from a human or a monkey, and whether it was rather the first or the second expression. Responses were given by key presses. Stimuli for the two different avatar types were presented in different blocks, with 10 repeated blocks per avatar type.</p></sec><sec id="s4-3"><title>Dynamic monkey and human head model</title><p>For our experiments, we exploited a monkey and a human dynamic face avatar with a very high degree of realism. The monkey head model was derived from a structural magnetic resonance scan of a rhesus monkey (9 years old, male). The surface of the face was modeled by an elastic mesh structure (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) that imitates the deformations induced by the major face muscles of macaque monkeys (<xref ref-type="bibr" rid="bib34">Parr et al., 2010</xref>). To accomplish a highly realistic appearance, special methods for hair animation and an appropriate modeling of skin reflectance were applied (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The human head was based on a scan-based commercial avatar with blend-shape animation, exploiting a multi-channel texture simulation software. Mesh deformations compatible with the human face muscle structure were computed from motion capture data in the same way as for the monkey face. Further technical details about the creation of these head models are described in Appendix 1.</p><p>The used dynamic head models achieve state-of-the-art degree of realism for the human head, and to our knowledge, we present the only highly realistic monkey avatar that is animated with motion capture data from real animals used in physiology so far. It has been demonstrated by a recent study of our lab that our dynamic monkey avatar induces behavioral reactions of macaque monkeys that are very similar to ones elicited by real movies, reaching the ‘good side’ of the uncanny valley (<xref ref-type="bibr" rid="bib40">Siebert et al., 2020</xref>), contrasting with previous studies using avatars in experiments with monkeys (<xref ref-type="bibr" rid="bib8">Chandrasekaran et al., 2013</xref>; <xref ref-type="bibr" rid="bib7">Campbell et al., 2009</xref>; <xref ref-type="bibr" rid="bib42">Steckenfinger and Ghazanfar, 2009</xref>). A related result has been obtained recently for static pictures of monkeys, demonstrating comparable looking times for the avatar and real pictures of monkey expressions, but without expressive motion of the face (<xref ref-type="bibr" rid="bib4">Bilder and Lauhin, 2014</xref>).</p></sec><sec id="s4-4"><title>Motion generation and style space</title><p>The animation of our avatars was based on motion capture data of two real monkey and human expressions. For motion capture, we used a VICON motion capture system with a marker set of 43 markers that were placed on the face of a monkey and a human participant. Facial expressions were elicited by instructions, or by having the animal interact with an experimenter, respectively. For this study, we exploited multiple repetitions of two human and two monkey expressions (<italic>anger/threat</italic> and <italic>fear)</italic>, and additional trials with neutral expressions. Further details about motion capture and the transfer of the motion to the head models are given in Appendix 1.</p><p>In order to control the information content and the expressivity of the dynamic face stimuli, we created motion morphs between these prototypical expressions. For this purpose, we exploited a method that is based on a generative Bayesian model of the trajectories of the control points of the face. This algorithm allows to create linear combinations in space-time between these prototypical motions, controlling smoothly the expressiveness and the style of the created facial motion. We verified in an additional control experiment (Turing test) that animations created with the original motion capture data were indistinguishable from the ones generated with motion trajectories generated with this Bayesian algorithm (reproducing the prototypes by the generative Bayesian model) (see Appendix 1 about details concerning this algorithm and the Turing test experiment).</p></sec><sec id="s4-5"><title>Modeling of the classification responses</title><p>Using a multinomial logistic regression analysis, the relative frequencies of the four classes <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> were approximated by class probabilities <inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi/><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> for the four classes that were modeled by a generalized linear model (GLM) of the form<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The variables <italic>y<sub>j</sub></italic> were given by linear combinations of predictor variables <italic>X<sub>i</sub></italic> in the form<disp-formula id="equ2"><mml:math id="m2"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal"/><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal"/></mml:math></disp-formula></p><p>We compared a multitude of models, including different sets of predictors. The most compact model was linear in the style space variables <italic>e</italic> and <italic>s</italic> and was given by the equation<disp-formula id="equ3"><mml:math id="m3"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal"/></mml:math></disp-formula></p><p>We also tested variants of linear models that included the predictor variable <inline-formula><mml:math id="inf43"><mml:mi>e</mml:mi><mml:mi>*</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula> and a predictor variable that is proportional to the total amount of optical flow, computed using a Horn-Schunck algorithm (CV Toolbox) from the stimulus movies. The different versions of the model were compared exploiting their prediction accuracy and the BIC. We discarded the models if, after addition of a new predictor, either their accuracy was decreasing or the BIC showed a decrease. Further details of the model fitting procedure are described in Appendix 1.</p></sec><sec id="s4-6"><title>Computation of the tuning functions</title><p>The species-tuning functions were computed by marginalization of the discriminant functions belonging to the same species category along the variable <italic>e</italic>. The tuning function to monkey expressions as a function of the species parameter <italic>s</italic> was defined as <inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mi/><mml:mi mathvariant="normal">d</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Similarly, the tuning function for human expressions was given by <inline-formula><mml:math id="inf45"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mi/><mml:mi mathvariant="normal">d</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. For this function, the direction of the <italic>s-</italic>axis was flipped, so that the category center also appears for <italic>s =</italic> 0, just as for the function <italic>D</italic><sub>M</sub>(<italic>s</italic>).</p></sec><sec id="s4-7"><title>Equilibration of stimuli for amount of motion/deformation</title><p>In order to control the amount of expressive low-level information, that is, the total amount of motion or shape deformation, we generated sets of equilibrated stimuli. For this purpose, we first defined different measures for the low-level information content and balanced the stimuli by equilibrating these measures. Tested measures included (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) optic flow (computed with an optic flow algorithm) (OF), the maximum amount of deformation (projected to the plane) of the polygon mesh relative to the neutral pose (DF), and the (two-dimensional) motion flow of the polygon mesh integrated over time (MF). To control the information content of the stimuli, we generated morphs between the original motion and the trajectories of a neutral expression using our motion morphing technique. In these morphs, the original expression was weighted with the morph level <inline-formula><mml:math id="inf46"><mml:mi>λ</mml:mi></mml:math></inline-formula> and the neutral expression with the weight <inline-formula><mml:math id="inf47"><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi/><mml:mi>λ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. The parameter <inline-formula><mml:math id="inf48"><mml:mi>λ</mml:mi></mml:math></inline-formula> was chosen to equate the low-level measures of all four prototypical stimuli, separately for the two avatar models (for the front view). For this purpose, we fitted the relationship between the individual measures <italic>M</italic> for the low-level information and the morphing parameter <inline-formula><mml:math id="inf49"><mml:mi>λ</mml:mi></mml:math></inline-formula> by a logistic function of the form (<inline-formula><mml:math id="inf50"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> signifying constants)<disp-formula id="equ4"><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The inverse of this function was used to determine the values of the morph parameter <inline-formula><mml:math id="inf51"><mml:mi>λ</mml:mi></mml:math></inline-formula> that matched the value <italic>M</italic> of the most expressive prototype motion. The MF measure resulted in the least variability of the perceived expressiveness of the equilibrated stimuli (see 'Results'), and thus was used to equilibrate the stimuli for all experimental conditions.</p></sec><sec id="s4-8"><title>Statistical analysis</title><p>Statistical analyses were implemented using <italic>MATLAB</italic> and RStudio (3.6.2), using R and the package <italic>lme4</italic> for the mixed models of ANOVA. We used G*Power 1.3 software to compute a prior rough estimate of the minimum required number of participants for medium effect size.</p><p>Different GLMs for the modeling of the categorization data were fitted using the <italic>MATLAB Statistics Toolbox.</italic> Models for the discriminant functions, including different sets of predictors, were compared using a step-wise regression approach. Models of different complexity were compared based on the prediction accuracy and by exploiting the BIC.</p><p>Two statistical measures were applied in order to compare the similarity of the categorization responses for the two avatar types. First, we computed the ratio of the different vs. shared variance between the fitted discriminant functions. For this purpose, we first computed the average discriminant function across both the avatar types and the two view conditions, and separately for the different classes (the index <italic>k</italic> running over the avatar types and view conditions, and <italic>j</italic> indicating the class number):<disp-formula id="equ5"><mml:math id="m5"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The ratio of the variance that is different and shared between the four conditions (avatars and views) is then given by the expression<disp-formula id="equ6"><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msubsup><mml:mo>∬</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mover><mml:mi>P</mml:mi><mml:mo>−</mml:mo></mml:mover><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:munder><mml:mo>∑</mml:mo><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:munder><mml:msubsup><mml:mo>∬</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mover><mml:mi>P</mml:mi><mml:mo>−</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This ratio is zero if the discriminant functions across all four conditions are identical.</p><p>As second statistical analysis, we compared the multinomially distributed four-class classification responses across the participants for the individual points in morphing space using a contingency table analysis that tested for the independence of the class probabilities from the avatar types and the two view conditions. Statistical differences were evaluated using a <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>-test and, for cases for which predicted frequencies were lower than 5, we exploited a bootstrapping approach (<xref ref-type="bibr" rid="bib48">Wilson et al., 2020</xref>).</p><p>The species-tuning functions, <italic>D</italic><sub>H</sub>(<italic>s</italic>) and <italic>D</italic><sub>M</sub>(<italic>s</italic>), were fitted by the sigmoidal function <inline-formula><mml:math id="inf53"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi/><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi/><mml:mo>(</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">ω</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula>, with the parameter <inline-formula><mml:math id="inf54"><mml:mi>θ</mml:mi></mml:math></inline-formula> determining the threshold and <inline-formula><mml:math id="inf55"><mml:mi>ω</mml:mi></mml:math></inline-formula>, the steepness. Differences of the tuning parameters <inline-formula><mml:math id="inf56"><mml:mi>θ</mml:mi></mml:math></inline-formula> were tested using two-factor mixed-model ANOVAs (species-specific of motion (monkey vs. human) as the within-subject factor and experiment (original motion, occlusion of the ears, and equilibrated motion) as the between-subject factor). Differences of the steepness parameters <inline-formula><mml:math id="inf57"><mml:mi>ω</mml:mi></mml:math></inline-formula> were tested using within-subject two-factor ANOVAs.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>Special thanks to Tjeerd Dijkstra for the help with advanced statistical analysis techniques. We thank H and I Bülthoff for helpful comments. This work was supported by HFSP RGP0036/2016 and EC CogIMon H2020 ICT-23-2014/644727,and European Research Council ERC 2019-SYG under EU Horizon 2020 research and innovation programme (grant agreement No. 856495, RELEVANCE),. MG was also supported by BMBF FKZ 01GQ1704 and BMG: SSTeP-KiZ (grant no. ZMWI1-2520DAT700). RS, SS, PD, and PT were supported by a grant from the DFG (TH 425/12–2). Further support by NVIDIA Corp.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Supervision, Validation, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Supervision, Validation, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Data curation, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Supervision, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Funding acquisition, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Supervision, Funding acquisition, Validation, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: All participants participated voluntarily in our study and gave written informed consent about this, and about the use of their data in a scientific study. Data was used in a fully anonymised fashion, nowhere we published data from individual participants. All psychophysical experiments were approved by the Ethics Board of the University Clinic Tübingen (886/2018BO2) and were conducted consistently with the rules of the Declaration of Helsinki.</p></fn><fn fn-type="other" id="fn2"><p>Animal experimentation: All animal procedures were approved by the local animal care committee (Regierungspräsidium Tübingen, Abteilung Tierschutz, permit number: N4/14) and fully complied with German law and the National Institutes of Health's Guide for the Care and Use of Laboratory Animals.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-61197-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Motion Capture data use to train our Bayesian Algorithm, all the rendered stimuli sequences to reproduce our experiment, as well as the raw participants' answers with the source code to reproduce our figures are available on GitLab <ext-link ext-link-type="uri" xlink:href="https://hih-git.neurologie.uni-tuebingen.de/ntaubert/FacialExpressions">https://hih-git.neurologie.uni-tuebingen.de/ntaubert/FacialExpressions</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:6d041a0a0cc7055618f85891b85d76e0e7f80eed">https://archive.softwareheritage.org/swh:1:rev:6d041a0a0cc7055618f85891b85d76e0e7f80eed</ext-link>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barraclough</surname> <given-names>NE</given-names></name><name><surname>Xiao</surname> <given-names>D</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name><name><surname>Oram</surname> <given-names>MW</given-names></name><name><surname>Perrett</surname> <given-names>DI</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Integration of visual and auditory information by superior temporal sulcus neurons responsive to the sight of actions</article-title><source>Journal of Cognitive Neuroscience</source><volume>17</volume><fpage>377</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1162/0898929053279586</pub-id><pub-id pub-id-type="pmid">15813999</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname> <given-names>M</given-names></name><name><surname>Yovel</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Two neural pathways of face processing: a critical evaluation of current models</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>55</volume><fpage>536</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2015.06.010</pub-id><pub-id pub-id-type="pmid">26067903</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beymer</surname> <given-names>D</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Image representations for visual learning</article-title><source>Science</source><volume>272</volume><fpage>1905</fpage><lpage>1909</lpage><pub-id pub-id-type="doi">10.1126/science.272.5270.1905</pub-id><pub-id pub-id-type="pmid">8658162</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bilder</surname> <given-names>CR</given-names></name><name><surname>Lauhin</surname> <given-names>TM</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Analysis of Categorial Data with R</source><publisher-name>CRC Press</publisher-name></element-citation></ref><ref id="bib5"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Calder</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>The Oxford Handbook of Face Perception</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/oxfordhb/9780199559053.001.0001</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname> <given-names>R</given-names></name><name><surname>Pascalis</surname> <given-names>O</given-names></name><name><surname>Coleman</surname> <given-names>M</given-names></name><name><surname>Wallace</surname> <given-names>SB</given-names></name><name><surname>Benson</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Are faces of different species perceived categorically by human observers?</article-title><source>Proceedings of the Royal Society of London. Series B: Biological Sciences</source><volume>264</volume><fpage>1429</fpage><lpage>1434</lpage><pub-id pub-id-type="doi">10.1098/rspb.1997.0199</pub-id><pub-id pub-id-type="pmid">9364783</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname> <given-names>MW</given-names></name><name><surname>Carter</surname> <given-names>JD</given-names></name><name><surname>Proctor</surname> <given-names>D</given-names></name><name><surname>Eisenberg</surname> <given-names>ML</given-names></name><name><surname>de Waal</surname> <given-names>FB</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Computer animations stimulate contagious yawning in chimpanzees</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>276</volume><fpage>4255</fpage><lpage>4259</lpage><pub-id pub-id-type="doi">10.1098/rspb.2009.1087</pub-id><pub-id pub-id-type="pmid">19740888</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname> <given-names>C</given-names></name><name><surname>Lemus</surname> <given-names>L</given-names></name><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic faces speed up the onset of auditory cortical spiking responses during vocal detection</article-title><source>PNAS</source><volume>110</volume><fpage>E4668</fpage><lpage>E4677</lpage><pub-id pub-id-type="doi">10.1073/pnas.1312518110</pub-id><pub-id pub-id-type="pmid">24218574</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>L</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The code for facial identity in the primate brain</article-title><source>Cell</source><volume>169</volume><fpage>1013</fpage><lpage>1028</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2017.05.011</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Curio</surname> <given-names>C</given-names></name><name><surname>Bülthoff</surname> <given-names>HH</given-names></name><name><surname>Giese</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Dynamic Faces: Insights From Experiments and Computation</source><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dahl</surname> <given-names>CD</given-names></name><name><surname>Wallraven</surname> <given-names>C</given-names></name><name><surname>Bülthoff</surname> <given-names>HH</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Humans and macaques employ similar face-processing strategies</article-title><source>Current Biology</source><volume>19</volume><fpage>509</fpage><lpage>513</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.01.061</pub-id><pub-id pub-id-type="pmid">19249210</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dahl</surname> <given-names>CD</given-names></name><name><surname>Rasch</surname> <given-names>MJ</given-names></name><name><surname>Tomonaga</surname> <given-names>M</given-names></name><name><surname>Adachi</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The face inversion effect in non-human primates revisited - an investigation in chimpanzees (Pan Troglodytes)</article-title><source>Scientific Reports</source><volume>3</volume><elocation-id>2504</elocation-id><pub-id pub-id-type="doi">10.1038/srep02504</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dahl</surname> <given-names>CD</given-names></name><name><surname>Chen</surname> <given-names>CC</given-names></name><name><surname>Rasch</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Own-race and own-species advantages in face perception: a computational view</article-title><source>Scientific Reports</source><volume>4</volume><elocation-id>6654</elocation-id><pub-id pub-id-type="doi">10.1038/srep06654</pub-id><pub-id pub-id-type="pmid">25323815</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Darwin</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1872">1872</year><source>The Expression of the Emotions in Man and Animals</source><publisher-name>John Murray</publisher-name></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobs</surname> <given-names>K</given-names></name><name><surname>Isik</surname> <given-names>L</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>How face perception unfolds over time</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>1258</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-09239-1</pub-id><pub-id pub-id-type="pmid">30890707</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furl</surname> <given-names>N</given-names></name><name><surname>Hadj-Bouziane</surname> <given-names>F</given-names></name><name><surname>Liu</surname> <given-names>N</given-names></name><name><surname>Averbeck</surname> <given-names>BB</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Dynamic and static facial expressions decoded from motion-sensitive Areas in the macaque monkey</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>15952</fpage><lpage>15962</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1992-12.2012</pub-id><pub-id pub-id-type="pmid">23136433</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giese</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Face recognition: canonical mechanisms at multiple timescales</article-title><source>Current Biology</source><volume>26</volume><fpage>R534</fpage><lpage>R537</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.05.045</pub-id><pub-id pub-id-type="pmid">27404241</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giese</surname> <given-names>MA</given-names></name><name><surname>Leopold</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Physiologically inspired neural model for the encoding of face spaces</article-title><source>Neurocomputing</source><volume>65-66</volume><fpage>93</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2004.10.060</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giese</surname> <given-names>MA</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neural mechanisms for the recognition of biological movements</article-title><source>Nature Reviews Neuroscience</source><volume>4</volume><fpage>179</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1038/nrn1057</pub-id><pub-id pub-id-type="pmid">12612631</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname> <given-names>K</given-names></name><name><surname>Li</surname> <given-names>Z</given-names></name><name><surname>Yan</surname> <given-names>Y</given-names></name><name><surname>Li</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Viewing heterospecific facial expressions: an eye-tracking study of human and monkey viewers</article-title><source>Experimental Brain Research</source><volume>237</volume><fpage>2045</fpage><lpage>2059</lpage><pub-id pub-id-type="doi">10.1007/s00221-019-05574-3</pub-id><pub-id pub-id-type="pmid">31165915</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Hoffman</surname> <given-names>EA</given-names></name><name><surname>Gobbini</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The distributed human neural system for face perception</article-title><source>Trends in Cognitive Sciences</source><volume>4</volume><fpage>223</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01482-0</pub-id><pub-id pub-id-type="pmid">10827445</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hill</surname> <given-names>HC</given-names></name><name><surname>Troje</surname> <given-names>NF</given-names></name><name><surname>Johnston</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Range- and domain-specific exaggeration of facial speech</article-title><source>Journal of Vision</source><volume>5</volume><fpage>793</fpage><lpage>807</lpage><pub-id pub-id-type="doi">10.1167/5.10.4</pub-id><pub-id pub-id-type="pmid">16441186</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jack</surname> <given-names>RE</given-names></name><name><surname>Schyns</surname> <given-names>PG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Toward a social psychophysics of face communication</article-title><source>Annual Review of Psychology</source><volume>68</volume><fpage>269</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-010416-044242</pub-id><pub-id pub-id-type="pmid">28051933</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jhuang</surname> <given-names>H</given-names></name><name><surname>Serre</surname> <given-names>T</given-names></name><name><surname>Wolf</surname> <given-names>L</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>IEEE 11th international conference on computer vision (ICCV)</article-title><conf-name>Rio De Janeiro</conf-name></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knappmeyer</surname> <given-names>B</given-names></name><name><surname>Thornton</surname> <given-names>IM</given-names></name><name><surname>Bülthoff</surname> <given-names>HH</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The use of facial motion and facial form during the processing of identity</article-title><source>Vision Research</source><volume>43</volume><fpage>1921</fpage><lpage>1936</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(03)00236-0</pub-id><pub-id pub-id-type="pmid">12831755</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lawrence</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning for larger datasets with the gaussian process latent variable model</article-title><conf-name>Proceedings of the Eleventh International Workshop on Artificial Intelligence and Statistics</conf-name><fpage>243</fpage><lpage>250</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lawrence</surname> <given-names>ND</given-names></name><name><surname>Moore</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Hierarchical gaussian process latent variable models</article-title><conf-name>International Conference in Machine Learning</conf-name><fpage>481</fpage><lpage>488</lpage></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leopold</surname> <given-names>DA</given-names></name><name><surname>Bondar</surname> <given-names>IV</given-names></name><name><surname>Giese</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Norm-based face encoding by single neurons in the monkey inferotemporal cortex</article-title><source>Nature</source><volume>442</volume><fpage>572</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1038/nature04951</pub-id><pub-id pub-id-type="pmid">16862123</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>S</given-names></name><name><surname>Deng</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep facial expression recognition: a survey</article-title><conf-name>IEEE Transactions on Affective Computing</conf-name><pub-id pub-id-type="doi">10.1109/TAFFC.2020.2981446</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Minty</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dawn of the planet of the apes in ACM SIGGRAPH 2014 computer animation festival</article-title><conf-name>Association for Computing Machinery, Vancouver, Canada</conf-name></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Møller</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>A scaled conjugate gradient algorithm for fast supervised learning</article-title><source>Neural Networks</source><volume>6</volume><fpage>525</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(05)80056-5</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname> <given-names>AP</given-names></name><name><surname>Leopold</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A parameterized digital 3D model of the rhesus macaque face for investigating the visual processing of social cues</article-title><source>Journal of Neuroscience Methods</source><volume>324</volume><elocation-id>108309</elocation-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.06.001</pub-id><pub-id pub-id-type="pmid">31229584</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagasawa</surname> <given-names>M</given-names></name><name><surname>Mitsui</surname> <given-names>S</given-names></name><name><surname>En</surname> <given-names>S</given-names></name><name><surname>Ohtani</surname> <given-names>N</given-names></name><name><surname>Ohta</surname> <given-names>M</given-names></name><name><surname>Sakuma</surname> <given-names>Y</given-names></name><name><surname>Onaka</surname> <given-names>T</given-names></name><name><surname>Mogi</surname> <given-names>K</given-names></name><name><surname>Kikusui</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Social evolution Oxytocin-gaze positive loop and the coevolution of human-dog bonds</article-title><source>Science</source><volume>348</volume><fpage>333</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1126/science.1261022</pub-id><pub-id pub-id-type="pmid">25883356</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parr</surname> <given-names>LA</given-names></name><name><surname>Waller</surname> <given-names>BM</given-names></name><name><surname>Burrows</surname> <given-names>AM</given-names></name><name><surname>Gothard</surname> <given-names>KM</given-names></name><name><surname>Vick</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Brief communication: maqfacs: a muscle-based facial movement coding system for the rhesus macaque</article-title><source>American Journal of Physical Anthropology</source><volume>143</volume><fpage>625</fpage><lpage>630</lpage><pub-id pub-id-type="doi">10.1002/ajpa.21401</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pascalis</surname> <given-names>O</given-names></name><name><surname>Scott</surname> <given-names>LS</given-names></name><name><surname>Kelly</surname> <given-names>DJ</given-names></name><name><surname>Shannon</surname> <given-names>RW</given-names></name><name><surname>Nicholson</surname> <given-names>E</given-names></name><name><surname>Coleman</surname> <given-names>M</given-names></name><name><surname>Nelson</surname> <given-names>CA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Plasticity of face processing in infancy</article-title><source>PNAS</source><volume>102</volume><fpage>5297</fpage><lpage>5300</lpage><pub-id pub-id-type="doi">10.1073/pnas.0406627102</pub-id><pub-id pub-id-type="pmid">15790676</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polosecki</surname> <given-names>P</given-names></name><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Schweers</surname> <given-names>N</given-names></name><name><surname>Romanski</surname> <given-names>LM</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Faces in motion: selectivity of macaque and human face processing areas for dynamic stimuli</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>11768</fpage><lpage>11773</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5402-11.2013</pub-id><pub-id pub-id-type="pmid">23864665</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rhodes</surname> <given-names>G</given-names></name><name><surname>Jeffery</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Adaptive norm-based coding of facial identity</article-title><source>Vision Research</source><volume>46</volume><fpage>2977</fpage><lpage>2987</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2006.03.002</pub-id><pub-id pub-id-type="pmid">16647736</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname> <given-names>KL</given-names></name><name><surname>Cohn</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Human facial expressions as adaptations: evolutionary questions in facial expression research</article-title><source>American Journal of Physical Anthropology</source><volume>116</volume><fpage>3</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1002/ajpa.20001</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname> <given-names>LS</given-names></name><name><surname>Fava</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The own-species face Bias: a review of developmental and comparative data</article-title><source>Visual Cognition</source><volume>21</volume><fpage>1364</fpage><lpage>1391</lpage><pub-id pub-id-type="doi">10.1080/13506285.2013.821431</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siebert</surname> <given-names>R</given-names></name><name><surname>Taubert</surname> <given-names>N</given-names></name><name><surname>Spadacenta</surname> <given-names>S</given-names></name><name><surname>Dicke</surname> <given-names>PW</given-names></name><name><surname>Giese</surname> <given-names>MA</given-names></name><name><surname>Thier</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A naturalistic dynamic monkey head avatar elicits Species-Typical reactions and overcomes the uncanny valley</article-title><source>Eneuro</source><volume>7</volume><elocation-id>ENEURO.0524-19.2020</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0524-19.2020</pub-id><pub-id pub-id-type="pmid">32513660</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sigala</surname> <given-names>R</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name><name><surname>Rainer</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Own-species Bias in the representations of monkey and human face categories in the primate temporal lobe</article-title><source>Journal of Neurophysiology</source><volume>105</volume><fpage>2740</fpage><lpage>2752</lpage><pub-id pub-id-type="doi">10.1152/jn.00882.2010</pub-id><pub-id pub-id-type="pmid">21430277</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steckenfinger</surname> <given-names>SA</given-names></name><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Monkey visual behavior falls into the uncanny valley</article-title><source>PNAS</source><volume>106</volume><fpage>18362</fpage><lpage>18366</lpage><pub-id pub-id-type="doi">10.1073/pnas.0910063106</pub-id><pub-id pub-id-type="pmid">19822765</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stettler</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><source>Artificial Neural Networks and Machine Learning - ICANN 2020</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-030-61616-8</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Taubert</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Hierarchical GPDM</data-title><source>Software Heritage</source><version designator="swh:1:rev:6d041a0a0cc7055618f85891b85d76e0e7f80eed">swh:1:rev:6d041a0a0cc7055618f85891b85d76e0e7f80eed</version><publisher-loc>https://archive.softwareheritage.org/swh:1:rev:6d041a0a0cc7055618f85891b85d76e0e7f80eed</publisher-loc></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vick</surname> <given-names>SJ</given-names></name><name><surname>Waller</surname> <given-names>BM</given-names></name><name><surname>Parr</surname> <given-names>LA</given-names></name><name><surname>Smith Pasqualini</surname> <given-names>MC</given-names></name><name><surname>Bard</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A Cross-species comparison of facial morphology and movement in humans and chimpanzees using the facial action coding system (FACS)</article-title><source>Journal of Nonverbal Behavior</source><volume>31</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1007/s10919-006-0017-z</pub-id><pub-id pub-id-type="pmid">21188285</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>JM</given-names></name><name><surname>Fleet</surname> <given-names>DJ</given-names></name><name><surname>Hertzmann</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Gaussian process dynamical models for human motion</article-title><conf-name>IEEE Transactions on Pattern Analysis and Machine Intelligence</conf-name><fpage>283</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2007.1167</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>JM</given-names></name><name><surname>Fleet</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Multifactor gaussian process models for style-content separation</article-title><conf-name>Proceedings of the 24th International Conference on Machine Learning (Association for Computing Machinery)</conf-name><fpage>975</fpage><lpage>982</lpage></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname> <given-names>VAD</given-names></name><name><surname>Kade</surname> <given-names>C</given-names></name><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Treue</surname> <given-names>S</given-names></name><name><surname>Kagan</surname> <given-names>I</given-names></name><name><surname>Fischer</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Macaque gaze responses to the primatar: a virtual macaque head for social cognition research</article-title><source>Frontiers in Psychology</source><volume>11</volume><elocation-id>1645</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2020.01645</pub-id><pub-id pub-id-type="pmid">32765373</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname> <given-names>Q</given-names></name><name><surname>Nelissen</surname> <given-names>K</given-names></name><name><surname>Van den Stock</surname> <given-names>J</given-names></name><name><surname>De Winter</surname> <given-names>FL</given-names></name><name><surname>Pauwels</surname> <given-names>K</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name><name><surname>Vanduffel</surname> <given-names>W</given-names></name><name><surname>Vandenbulcke</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dissimilar processing of emotional facial expressions in human and monkey temporal cortex</article-title><source>NeuroImage</source><volume>66</volume><fpage>402</fpage><lpage>411</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.10.083</pub-id><pub-id pub-id-type="pmid">23142071</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec id="s8" sec-type="appendix"><title>Supporting Information</title><boxed-text><sec id="s9"><title>Monkey subject</title><p>The monkey facial movements were recorded from a 9-year-old male rhesus monkey (<italic>Macaca mulatta</italic>), born in captivity and pair-housed. The monkey had previously been implanted with an individually adapted titanium head-post to allow head immobilization in unrelated neurophysiological experiments, and it had been trained to climb into a primate chair and to accept head fixation. The animal was in daily contact with other macaque monkeys and human caretaking personnel. The structural model of the monkey’s head was derived from a T1-weighted MRI-scan with an isotropic resolution of 1 mm. Motion capture recordings were compatible with the guidelines set by the National Institutes of Health and German national law and were approved by the local committee supervising the handling of experimental animals (Regierungspräsidium Tübingen, Abteilung Tierschutz, permit number N4/14). Human movements were recorded from a 40-year-old male human subject.</p></sec><sec id="s10"><title>Monkey avatar</title><p>The highly realistic monkey avatar was generated exploiting state-of-the-art techniques from computer animation. Such techniques have been applied before for the realization of animation movies for cinemas (<xref ref-type="bibr" rid="bib30">Minty, 2014</xref>). However, monkey avatars of high quality have only very recently been developed for studies on static face perception (<xref ref-type="bibr" rid="bib32">Murphy and Leopold, 2019</xref>), and to our knowledge, this work is the first one exploiting motion capture from monkeys for generating such dynamic avatars. The head model was developed based on the MRI scan of an animal (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>). The scan provides a quite detailed model of the basic shape of the head, but it is characterized by a highly irregular polygon structure, which makes it difficult to control the deformation during animation. In order to obtain a mesh model that can be manipulated more easily, we reduced the number of polygons and created a regularized mesh with clean edge loops (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>). This corrected mesh was adjusted for a neutral pose, and control points were specified that control the mesh deformation during animation. For the regularized mesh, the weighting regions that determine the influences of the individual control points on the mesh could be exactly controlled. This developed ‘low-polygon’ model is useful for controlling the animation, but it lacks a lot of high-frequency details that are critical for a realistic appearance of the face. In order to add such details, we imported the model with the low polygon number into <italic>Adobe Autodesk Mudbox</italic>, a software that allows, by subdivision, to generate again a highly regular mesh model with a high number of polygons. This model was further refined by a number of specific editing steps, including clay modeling, in order to improve 3D shape details. Using a special tool (<italic>Alpha Brushes</italic>), additional texture details were added, such as wrinkles and pores (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1C</xref>). To transfer the deformation from the low- to the high-frequency polygon model, we exported displacement maps in <italic>Autodesk Mudbox,</italic> which capture the differences between the low and high polygon-density models.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Details of generation of the monkey head model.</title><p>(<bold>A</bold>) Irregular surface mesh resulting from the magnetic resonance scan of a monkey head. (<bold>B</bold>) Face mesh, the deformation of which is following control points specified by motion-captured markers. (<bold>C</bold>) Surface with a high polygon number derived from the mesh in (<bold>B</bold>), applying displacement texture maps, including high-frequency details such as pores and wrinkles. (<bold>D</bold>) Skin texture maps modeling the epidermal layer (left), the dermal layer (middle), and the subdermal layer (right panel). (<bold>E</bold>) Specularity textures modeling the reflection properties of the skin; overall specularity (left) and the map specifying oily vs. wet appearance (right panel). (<bold>F</bold>) Complete monkey face model, including the modeling of fur and whiskers.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61197-app1-fig1-v1.tif"/></fig><p>A particular challenge was the development of a realistic skin model that specifies believable color and reflectance properties. Skin-surface textures were generated using photographs of a real monkey as the reference and by painting layer-wise color variations of the skin in order to approximate maximally its realistic appearance. Specifically, we used multiple layers of diffusive texture to model the translucent behavior of the skin, separately for the deep layer, subdermal layer, and epidermal layers (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1D</xref>). For the deep layer, we hue-shifted the diffuse texture map toward red colors in order to model the deep vascularization, while the color of the subdermal layers was shifted more toward yellow in order to simulate the fatty parts of the skin. In order to mimic the very thin superficial layer of dead skin, we desaturated the diffuse texture for the epidermal layer. For realistic appearance, it was also important to model the specularity of monkey skin, reproducing how light is reflected from the skin within different facial regions. For this purpose, we created two specular maps for the monkey’s face, one simulating the basic specularity and one describing the oiliness vs. wetness of the skin (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1E</xref>). Both material channels have an Index of Refraction (IOR) of 1.375, corresponding to the IOR of water.</p><p>A final element that was essential for a realistic appearance was the realistic modeling of the fur. For this purpose, we exploited the built-in <italic>XGen Interactive Groom</italic> feature for hair creation of the animation software <italic>Maya</italic>. The overall appearance of the hair was controlled exploiting three control levels. The first level models the base of the fur, defining the direction of the hairs by control splines and adding some noise to model texture fluctuations and the matting of the fur. The second level models structures consisting of long hair, including the whiskers and the brows, using a smaller number of thick hairs. Believability and realism were increased further by adding a third layer of hair, also known as <italic>Peach Fuzz</italic> or <italic>Vellus</italic>, that consists of tiny hairs that are distributed within the face area. The final result is shown in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1F</xref>.</p></sec><sec id="s11"><title>Human avatar</title><p>The human avatar was based on a female face scan provided by the company <italic>EISKO</italic> ( <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2A</xref>). The commercial package also includes all main textures (diffuse map, specular map, base displacement map, etc; <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2B and C</xref>) for the neutral pose, as well as the corresponding textures for face compression and stretching (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2B and C</xref>). We applied just small color adjustments to the diffuse and specular maps, similar to texture creation of the monkey head model. The <italic>EISKO</italic> model package also included a whole face rig with 154 blend shapes, suitable for changing the face shape by blending (interpolation), resulting in naturally looking shape variations (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2A</xref>). The interpolation was driven by control points equivalent to the ones in the monkey model, defined by the motion-captured markers exploiting a ribbon-like structure that was inspired by the human muscle anatomy (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2F</xref>). Using a tension map algorithm, we determined the local deformations of the texture from the mesh deformations relative to the neutral pose. For the generation of high-frequency details, contrasting with the approach for the monkey avatar, we employed a multichannel texture package from <italic>TexturingXYZ.</italic> This package provides diffuse maps and high-frequency details as displacement maps (pores, wrinkles, etc) derived from a scanned real face. Exploiting the programs <italic>R3dS Wrap</italic> (trial version) and <italic>xNormal</italic>, we transferred shape details similar to the ones of the monkey face to the human face model (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2G</xref>). Hair animation used the same tools as for the monkey face. The final result is shown in <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2H</xref>.</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Details of generation of the human head model.</title><p>(<bold>A</bold>) Human face mesh and deformations by a blendshape approach, in which face poses are driven by the 43 control points (top panel). Tension map algorithm computes compression (green) and stretching (red) of mesh during animation (middle panel). Corresponding texture maps were blended (bottom panel). (<bold>B</bold>) Examples of diffuse texture maps (top panel), with additional maps for stretching (middle panel) and compression (bottom panel). (<bold>C</bold>) Subsurface color map modeling the color variations by light scattering and reflection by the skin. (<bold>D</bold>) Specular map modeling the specularity of the skin. (<bold>E</bold>) Wetness map modeling the degree of wetness vs. oilyness of the skin. (<bold>F</bold>) Regularized basic mesh with embedded muscle-like ribbon structures (violet) for animation. Yellow points indicate the control points defined by the motion capture markers. (<bold>G</bold>) Mesh with additional high-frequency details. (<bold>H</bold>) Final human avatar including hair animation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61197-app1-fig2-v1.tif"/></fig></sec><sec id="s12"><title>Motion capture</title><p>Motion capture was realized with a <italic>VICON FX20</italic> motion capture system with six cameras (focal length, 24 mm) using a camera setting that was optimized for face capturing. We used 43 reflecting markers (2 mm) that were placed in the face, using a marker set that we developed ourselves (<xref ref-type="fig" rid="fig1">Figure 1B</xref> and S1B). Recording frequency was 120 Hz. Trajectories were preprocessed using <italic>Nexus</italic> 1.85 software by <italic>VICON</italic>, smoothed and segmented by an expert into individual facial expressions with a duration between 3 and 5 s. The trajectories were resampled with 150-time steps and 30 fps.</p><p>The monkey expressions were recorded from a 9-year-old male animal. The expressions were elicited by showing the animal different objects, including a screw driver, a mirror, and an unknown male human individual. The animal was head-fixed and observed the stimuli at a distance of 200 cm in front of the camera set-up. The recorded trajectories were segmented by a monkey expert, who had extensive experience with macaque monkeys on a daily basis.</p><p>The human marker set was corresponding to the one of the monkey, except that it lacked markers on the ears (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2D</xref>). The human actor was instructed to show two facial expressions ‘anger’ and ‘fear’. Processing was identical to the marker trajectories of the monkey expressions.</p></sec><sec id="s13"><title>Motion morphing algorithm</title><p>In order to create continuous parameterized spaces of facial movements, we exploited a motion morphing method that is based on a hierarchical Gaussian process model. The method is in principle real-time capable, thus allowing for instantaneous changes of motion style modulations based on the on-line user input. This functionality was not critical for the experiments presented in this paper, but it is used in ongoing experiments that build on the presented results.</p><p>Our motion morphing algorithm is based on a hierarchical probabilistic generative model that is learned from facial movement data. The architecture (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3A</xref>) comprises three layers. The lower two layers are formed by Gaussian process latent variable models (GP-LVMs) (<xref ref-type="bibr" rid="bib27">Lawrence and Moore, 2007</xref>), and the highest layer is formed by a Gaussian process dynamical model (GPDM) (<xref ref-type="bibr" rid="bib46">Wang et al., 2008</xref>).</p><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Motion morphing algorithm and additional results.</title><p>(<bold>A</bold>) Graphical model showing the generative model underlying our motion morphing technique. The hierarchical Bayesian model has three layers, reducing subsequently the dimensionality of the motion data <bold>y</bold><italic><sub>n</sub></italic>. The top layer models the trajectory in latent space using a Gaussian process dynamical model (GPDM). The vectors <bold>e</bold> and <bold>s</bold> are additional style vectors that encode the expression type and the species type. They are binomially distributed. Plate notation indicates the replication of model components for the encoding of the temporal sequence, and the different styles. Nonlinear functions are realized as samples from Gaussian processes with appropriately chosen kernels (for details, see text). (<bold>B</bold>) Results from Turing test experiment. Accuracy for the distinction between animations with original motion capture data and trajectories generated by our motion morphing algorithm is close to chance level (dashed line), opposed to the accuracy for the detection of small motion artifacts in control stimuli, which was almost one for both avatar types.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61197-app1-fig3-v1.tif"/></fig><p>The facial motion was given by the <italic>M</italic>-dimensional trajectories of the control points, which were parameterized by an <italic>N</italic> x <italic>D</italic> time series matrix <bold>Y</bold> = [<inline-formula><mml:math id="inf58"><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, …,<inline-formula><mml:math id="inf59"><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>]<sup>T</sup> with <italic>N</italic> = 600 (two expressions) or <italic>N</italic> = 900 (neutral expression included for equilibration) and <italic>D</italic> = 208 dimensions. The two GP-LVM layers reduce the dimensionality of the patterns in the high-dimensional trajectory space in a nonlinear way. For this purpose, the first layer represents the trajectory points as nonlinear functions of a lower-dimensional hidden state variable, specifying the <italic>N</italic> x <italic>M</italic> matrix <bold>H</bold> = [<inline-formula><mml:math id="inf60"><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, …,<inline-formula><mml:math id="inf61"><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>]<sup>T</sup>. In our case, the dimensionality <italic>M</italic> of the hidden variables <bold>h</bold><italic><sub>n</sub></italic> was six. Signifying (<bold>y</bold><italic><sup>d</sup></italic>)<sup>T</sup>, the row vectors of the matrix <bold>Y</bold>, the trajectory components are modeled in the form<disp-formula id="equ7"><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow/><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where the variables <inline-formula><mml:math id="inf62"><mml:msub><mml:mrow><mml:mi mathvariant="bold">ε</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> specify independent Gaussian noise vectors, and with the function <inline-formula><mml:math id="inf63"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi/></mml:mrow></mml:msub></mml:math></inline-formula> being drawn from a Gaussian process <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒢</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="script">℘</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, that is, all vectors of the form <inline-formula><mml:math id="inf65"><mml:msub><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi/></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi/><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi/><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi/></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> are distributed according to the Gaussian distribution <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with the covariance matrix <inline-formula><mml:math id="inf67"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula>, whose elements are specified by a kernel function <italic>k</italic><sub>1</sub> in the form <inline-formula><mml:math id="inf68"><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>'</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>'</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>'</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The kernel function is given by a linear combination of two types of kernels, a radial basis function kernel and a linear kernel<disp-formula id="equ8"><mml:math id="m8"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">'</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mfenced close="|" open="|" separators="|"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">'</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mi mathvariant="bold"/><mml:mi mathvariant="bold"/><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold"/><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">'</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula></p><p>The RBF (Radial Basis Function) part allows to capture nonlinear structures in the data, while the linear kernel supports smooth and linear inter- and extrapolation in the pattern space. In addition, we found that the Kronecker delta part of the Kernel matrix is critical for the smoothness of the learned trajectories in the latent space. The parameter <inline-formula><mml:math id="inf69"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> specifies the inverse width of the Gaussian radial basis functions.</p><p>The second layer of the model is defined exactly as the first layer. Here, the dimensionality of the variables <bold>h</bold><italic><sub>n</sub></italic> is further reduced by generating a nonlinear mapping from the hidden state variables <bold>x</bold><italic><sub>n</sub></italic> with <italic>Q</italic> = 2 dimensions, which defines the matrix <bold>X</bold> = [<inline-formula><mml:math id="inf70"><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, …,<inline-formula><mml:math id="inf71"><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>]<sup>T</sup>. Like in the first layer, the nonlinear mappings between the components of the variables <bold>x</bold> and <bold>h</bold> are defined by functions drawn from a Gaussian process <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒢</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="script">℘</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow/><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow/><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow/><mml:msup><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where the hyper-parameters of the kernel function differ from the ones of the kernel <italic>k</italic><sub>1</sub>. In addition, the kernel of this layer depends on the style vector variables e and s. These variables enter the kernel of the Gaussian process as multiplicative linear kernel terms<disp-formula id="equ9"><mml:math id="m9"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold"/><mml:mi mathvariant="bold">e</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold"/><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">'</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">'</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold"/><mml:mi mathvariant="bold">e</mml:mi><mml:mi>'</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">'</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal"/><mml:msup><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">'</mml:mi></mml:mrow></mml:msup> <mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mfenced close="|" open="|" separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">'</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mi mathvariant="bold"/><mml:mi mathvariant="bold"/><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold"/><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">'</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold"/><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>The random variables e and s encode the motion style using one-out-of-two encoding, and they were estimated from the training data together with the state variables <bold>x</bold><italic><sub>n</sub></italic> using a maximum-likelihood approach. This parametrization turns out to be favorable to separate the different style components and the motion content in the latent space, similar to multi-factor models (<xref ref-type="bibr" rid="bib47">Wang and Fleet, 2007</xref>). We constrained the style vectors for all trials of the training data that represented the same motion style (e.g., ‘expression 1’, ‘human motion’) to be equal. In this way, the training data specify estimates <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> that correspond to averages of the expression types 1 and 2, and estimates <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> that correspond to the average monkey and the human expressions. In order to generate new intermediate motion styles, we ran the learned Gaussian model in a generative mode, fixing the values of these style vectors to blends between these estimates. The style vectors for the motion morphs as functions of the style parameters <italic>e</italic> and <italic>s,</italic> as discussed in the main part of the paper, were given by the relationships <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mrow/><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow/></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow/></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively.</p><p>The highest level of the probabilistic model approximates the dynamics of the trajectories of the hidden state variables <bold>x</bold><italic><sub>n</sub></italic> using a nonlinear extension of an auto-regressive model, which is known as GPDM. For this purpose, the state dynamics is modeled as a function of the two-dimensional hidden state variable <inline-formula><mml:math id="inf79"><mml:mi mathvariant="bold">x</mml:mi></mml:math></inline-formula><italic><sub>n</sub></italic> that obeys the nonlinear dynamics<disp-formula id="equ10"><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf80"><mml:msub><mml:mrow><mml:mi mathvariant="bold">ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is isotropic white Gaussian noise and where the nonlinear function <inline-formula><mml:math id="inf81"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mi/></mml:mrow></mml:msub></mml:math></inline-formula> is again specified by a Gaussian process. The hidden state dynamics can again be learned using a GP-LVM framework (<xref ref-type="bibr" rid="bib46">Wang et al., 2008</xref>), where we used a kernel function of the form<disp-formula id="equ11"><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow/><mml:mrow/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mrow/><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mrow/><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To determine the parameters of the GP-LVMs, we maximized the logarithm of their marginal likelihood and fitted all hyper-parameters using a scaled conjugate gradient algorithm (<xref ref-type="bibr" rid="bib31">Møller, 1993</xref>). Since the evaluation of the marginal requires the inversion of a kernel matrix with a dimensionality that is given by all pairs of latent points, its direct implementation is computationally infeasible for large data sets. To render this inversion feasible, we applied a sparse approximation method that approximates the marginal distribution based on a low number of inducing points in the hidden spaces (<xref ref-type="bibr" rid="bib26">Lawrence, 2007</xref>). The model was trained using six motion-captured example trajectories for each of the two basic human and monkey expressions, sampled with 150-time steps. Training using an AMD Ryzen Threadripper 1950X CPU with 32 cores with a clock frequency of 3.4 GHz took about 1.5 hr. The most important parameters of the algorithm are summarized in <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>.</p></sec><sec id="s14"><title>Turing test experiment</title><p>The described motion morphing algorithm interpolates between the original motion-captured movements in space-time. It was critical to verify that the morphing algorithm does not destroy the naturalness of the facial movements, at least for the prototypical expressions between which we blended. In order to verify this question, we realized a Turing test experiment that included 16 new participants. They had to discriminate between animations with original motion capture data (‘original trajectories’) and ones generated with movements that were generated by the morphing algorithm (‘algorithm-generated trajectories’). The movements generated with the algorithm approximated the prototype movements (the style variables <italic>e</italic> and <italic>s</italic> being 0 or 1). In order to induce some variability, we used three different motion capture trials of each of the original human and monkey expressions, and their approximations based on the morphing model. The compared stimulus pairs were presented sequentially, and motions were presented in a block-randomized order 20 times, in separate blocks for the two avatar types. To verify that participants can pick up artifacts in the animations, we added a further condition in which instead of movements generated by the morphing algorithm we used control movements, which were generated by reversing the temporal order of short four-frame segments in the original motion-captured movements. Animations with these control movements also had to be distinguished from ones with the original motion capture data.</p><p>The results of this control experiments are shown in <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3B</xref>. The accuracy of the detection of original motion capture data, as opposed to the generated one, was 40.6% for the monkey avatar and 47.5% for the human avatar. Compared to the chance probability 0.5, both values were significantly lower (<inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>16</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>18.18</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for the monkey and <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>16</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>11.43</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for the human avatar). This implies that the animations using motion capture data were judged even less frequently as ‘original trajectories’ than the animations generated with our motion synthesis algorithm. The morphing algorithm thus does not degrade the perceived naturalness of the motion. The even higher perceived naturalness of the algorithm-generated motion likely is a consequence of the motion being slightly more smooth, due to the smoothing properties of Gaussian process models. The artificial control movements were detected with very high reliability, as indicated by the high accuracies 96.88%, for the monkey, and 96.56%, for the human avatars, which are highly significantly different from chance level (<inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>16</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>35.85</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> vs. <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>16</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>34.93</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p></sec><sec id="s15"><title>Comparison of different classification models</title><p>Different multinomial regression models were compared in order to find the most compact model that explains our classification data. The models differed in terms of the predictor variables of the linear model for the approximation of the variables <italic>y<sub>j</sub></italic>. The six compared models were defined as</p><list list-type="bullet"><list-item><p>Model 1: <inline-formula><mml:math id="inf86"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p></list-item><list-item><p>Model 2: <inline-formula><mml:math id="inf87"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>e</mml:mi><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal"/></mml:math></inline-formula>,</p></list-item><list-item><p>Model 3: <inline-formula><mml:math id="inf88"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal"/></mml:math></inline-formula>,</p></list-item><list-item><p>Model 4: <inline-formula><mml:math id="inf89"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal"/></mml:math></inline-formula>,</p></list-item><list-item><p>Model 5: <inline-formula><mml:math id="inf90"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal"/><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal"/><mml:mo>∙</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal"/></mml:math></inline-formula>, and </p></list-item><list-item><p>Model 6: <inline-formula><mml:math id="inf91"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal"/><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal"/><mml:mi>O</mml:mi><mml:mi>F</mml:mi><mml:mi mathvariant="normal"/><mml:mi mathvariant="normal"/></mml:math></inline-formula>.</p></list-item></list><p>Apart from the style variables <italic>e</italic> and <italic>s</italic>, the variable <italic>OF</italic> signifies the optic flow computed from the image sequence with an optic flow algorithm. Models were compared based on two criteria. First, we required that the introduction of additional predictors did not result in a significantly higher prediction accuracy. According to this criterion, for almost all stimulus types, model four was the most compact model for the front view stimuli (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>). Only for the rotated views of the avatars, however, we found a slight significant increase of the prediction accuracy (by less than 1.57%). For this reason, we decided to use model four as the basis for our further analyses of all classification data in the main experiment.</p></sec><sec id="s16"><title>Testing of low-level information that predicts expressivity</title><p>Since we found for natural dynamic expressions that a larger part of the tested perceptual space was classified as monkey than as human expressions (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), we suspected this result to be a potential consequence of monkey expressions specifying more salient low-level features, such as local motion or geometrical deformations. In order to control for this variable, we created a second stimulus set for which the amount of low-level information was balanced. Since it was a priori unknown which type of low-level information drives the expressivity of facial expressions, we tested nine possible measures, quantifying the amount of low-level features in a separate psychophysical experiment with nine participants. These measures were two-dimensional optic flow computed from the movies with a Horn-Schunck algorithm (<italic>MATLAB</italic> implementation), the absolute spatial deformation relative to the neutral frame, and the motion flow computed either from the control point trajectories or from the regularized mesh points, either in three dimensions or after projection to the two-dimensional image plane.</p><p>The <italic>spatial deformation</italic> relative to the neutral frame was quantified using the measures<disp-formula id="equ12"><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <bold>X</bold><italic><sub>t</sub></italic> signifies a vector that contains the relevant control point or (two- or three-dimensional) mesh point coordinates and where <italic>N</italic> is the number of stimulus frames. Likewise, the <italic>motion flow</italic> was defined by the quantity<disp-formula id="equ13"><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>For the true optic flow, the motion measure was computed by summing up the absolute values of all estimated local motion vectors across the image. The stimuli for this experiment were motion morphs between each of the four prototype expressions (two human and two monkey expressions) and a neutral expression. The original expression entered the motion morph with a weight of <inline-formula><mml:math id="inf92"><mml:mi>λ</mml:mi></mml:math></inline-formula>, and the neutral expression with a weight of <inline-formula><mml:math id="inf93"><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>λ</mml:mi></mml:math></inline-formula>), where the morphing weight was adjusted to obtain the same amount of low-level information in all adjusted prototypes.</p><p>In order to cut down the number of measures for the amount of low-level information in the first place, we generated a set of face motion stimuli with reduced and exaggerated expressivity, separately for the two face avatars, by choosing six different values for the morphing weight <inline-formula><mml:math id="inf94"><mml:mi>λ</mml:mi></mml:math></inline-formula> (values 0–25–50–75–100–125% for the monkey expressions and the values 0-37.5–75-112.5–150% for the human expressions). For all rendered movies, we computed the nine different measures for the low-level feature content and analyzed their dependence on the morphing weight <inline-formula><mml:math id="inf95"><mml:mi>λ</mml:mi></mml:math></inline-formula> and their similarities. We found that the measures <italic>DF</italic> and <italic>MF,</italic> computed from the two-dimensional and three-dimensional mesh coordinates, and the control points were very highly correlated (r &gt; 86.24, r<sub>average</sub> = 98.74; p &lt; 0.0403). The mesh point-based measures were monotonically increasing functions of the morphing level <inline-formula><mml:math id="inf96"><mml:mi>λ</mml:mi></mml:math></inline-formula>. This was not the case for the quantities computed from the control point trajectories, due to which we discarded the measures derived from the control point trajectories from the balancing of the stimuli. Because of the high correlation between the measures computed from the two- and three-dimensional mesh-point trajectories, and the higher similarity of the two-dimensional trajectories with image motion, we kept only the measures computed from the two-dimensional mesh-point trajectories for the further analysis. In addition, we tested the optic flow computed by the optic flow algorithm from pixel images as a third possible predictor of the low-level information. For each of these three predictors, we constructed a balanced stimulus set by adjusting the morph levels of all prototypes, except for the one with the lowest low-level feature content, in order to match their low-level information contents. As a result, we obtained three balanced sets of stimuli, each with four dynamic expressions, separately for each avatar type.</p><p>All stimuli were shown in a block-wise randomized order to the participants who had to rate their expressivity on a nine-point Likert scale. For the human avatar, the stimuli, the expressivity of which was balanced using the motion flow measure <italic>MF</italic>, showed the smallest variability across participants and the largest expressivity. For the monkey avatar, the expressivity was rated similarly for stimuli balanced using the measures <italic>MF</italic> and <italic>DF</italic>, while it was significantly lower for stimuli balanced using the optic flow (t(275) = 2.8; p = 0.0054 and t(269) = 3.95; p &lt; 0.001). A step-wise regression analysis, in which we predicted the expressivity ratings from the remaining measures (<italic>MF</italic> and <italic>DF</italic> computed from the two-dimensional mesh motion), showed that the motion flow <italic>MF</italic> is sufficient, while the other predictor <italic>DF</italic> did not add significant additional information. Using a model comparison analysis exploiting the Bayesian Information Criterion (BIC), we found no significant difference in the explanatory values of the models including the predictor <italic>MF</italic>, and the predictors <italic>MF</italic> and <italic>DF (</italic> <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>284</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>3.49</mml:mn><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.062</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p></sec><sec id="s17"><title>Asymmetry index</title><p>The deviation of the four discriminant functions <inline-formula><mml:math id="inf98"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> from the completely symmetrical case, where all four discriminant functions have the same basic shape (with their peaks centered on the different prototypes), was quantified by defining the asymmetry index AI. This index is exactly zero if the four discriminant functions are exactly symmetrical with respect to the axes <inline-formula><mml:math id="inf99"><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf100"><mml:mi>s</mml:mi><mml:mi/><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>. This implies the symmetry relationship <inline-formula><mml:math id="inf101"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi/><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mi/><mml:mo>=</mml:mo><mml:mi/><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi/></mml:math></inline-formula><inline-formula><mml:math id="inf102"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>. In order to compute the index, we first computed a symmetrized average of all four discriminant functions according to the formula<disp-formula id="equ14"><mml:math id="m14"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi/><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi/><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi/><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Likewise, we defined a standard deviation relative to this symmetrized average by the expression <inline-formula><mml:math id="inf103"><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msqrt><mml:msub><mml:mrow><mml:mi mathvariant="normal">Q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mi/><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:msqrt></mml:math></inline-formula> with the least square deviation sum<disp-formula id="equ15"><mml:math id="m15"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo></mml:math></disp-formula><disp-formula id="equ16"><mml:math id="m16"><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></disp-formula></p><p>The asymmetry index was defined by the expression<disp-formula id="equ17"><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∬</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">D</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∬</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The AI increases with the deviation from the completely symmetric case, where all four categories are represented equally well.</p><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Model comparison.</title><p>Results of the accuracy and the Bayesian Information Criterion (BIC) for the different logistic multinomial regression models for the stimuli derived from the original motion (no occlusions) for the monkey and the human avatar. The models included the following predictors: Model 1: constant; Model 2: constant, <italic>s</italic>; Model 3: constant, <italic>e</italic>; Model 4: constant, <italic>s</italic>, <italic>e</italic>; Model 5: constant, <italic>s</italic>, <italic>e</italic>, product <italic>s</italic>⋅<italic>e</italic>; Model 5: constant, <italic>s</italic>, <italic>e</italic>, Optic Flow.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model comparison</th><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/></tr></thead><tbody><tr><td valign="bottom">Monkey front view</td><td valign="bottom">Model</td><td valign="bottom">Accuracy [%]</td><td valign="bottom">Accuracy increase [%]</td><td valign="bottom">BIC</td><td valign="bottom">Parameters</td><td valign="bottom">df</td><td valign="bottom">χ<sup>2</sup></td><td valign="bottom">p</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 1</td><td valign="bottom">38.29</td><td valign="bottom"/><td valign="bottom">7487</td><td valign="bottom">33</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Model 2</td><td valign="bottom">57.86</td><td valign="bottom">19,56 (relative to Model 1)</td><td valign="bottom">5076</td><td valign="bottom">36</td><td valign="bottom">3</td><td valign="bottom">2411</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 3</td><td valign="bottom">49.49</td><td valign="bottom">11,2 (relative to Model 1)</td><td valign="bottom">6125</td><td valign="bottom">36</td><td valign="bottom">3</td><td valign="bottom">1362</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 4</td><td valign="bottom">77.53</td><td valign="bottom">19,67 (relative to Model 2)</td><td valign="bottom">3586</td><td valign="bottom">39</td><td valign="bottom">3</td><td valign="bottom">1490</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 5</td><td valign="bottom">77.53</td><td valign="bottom">0 (relative to Model 4)</td><td valign="bottom">3598</td><td valign="bottom">42</td><td valign="bottom">3</td><td valign="bottom">11.997</td><td valign="bottom">&lt;0.0074</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 6</td><td valign="bottom">77.42</td><td valign="bottom">−0,11 (relative to Model 4)</td><td valign="bottom">3580</td><td valign="bottom">42</td><td valign="bottom">3</td><td valign="bottom">5.675</td><td valign="bottom">0.129</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom">Human front view</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Model 1</td><td valign="bottom">36.84</td><td valign="bottom"/><td valign="bottom">7481</td><td valign="bottom">33</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Model 2</td><td valign="bottom">54.22</td><td valign="bottom">17,38 (relative to Model 1)</td><td valign="bottom">5541</td><td valign="bottom">36</td><td valign="bottom">3</td><td valign="bottom">1940</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 3</td><td valign="bottom">53.56</td><td valign="bottom">16,72 (relative to Model 1)</td><td valign="bottom">5847</td><td valign="bottom">36</td><td valign="bottom">3</td><td valign="bottom">1633</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 4</td><td valign="bottom">81.56</td><td valign="bottom">27,35 (relative to Model 2)</td><td valign="bottom">3420</td><td valign="bottom">39</td><td valign="bottom">3</td><td valign="bottom">2120</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 5</td><td valign="bottom">81.35</td><td valign="bottom">−0,22 (relative to Model 4)</td><td valign="bottom">3309</td><td valign="bottom">42</td><td valign="bottom">3</td><td valign="bottom">112</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 6</td><td valign="bottom">81.38</td><td valign="bottom">−0,18 (relative to Model 4)</td><td valign="bottom">3389</td><td valign="bottom">42</td><td valign="bottom">3</td><td valign="bottom">31.66</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom">Monkey 30-degree</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Model 1</td><td valign="bottom">35.32</td><td valign="bottom"/><td valign="bottom">6913</td><td valign="bottom">33</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Model 2</td><td valign="bottom">57.40</td><td valign="bottom">22,08 (relative to Model 1)</td><td valign="bottom">4314</td><td valign="bottom">36</td><td valign="bottom">3</td><td valign="bottom">2622</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 3</td><td valign="bottom">49.36</td><td valign="bottom">14,04 (relative to Model 1)</td><td valign="bottom">5179</td><td valign="bottom">36</td><td valign="bottom">3</td><td valign="bottom">1757</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 4</td><td valign="bottom">84.04</td><td valign="bottom">26,64 (relative to Model 2)</td><td valign="bottom">2359</td><td valign="bottom">39</td><td valign="bottom">3</td><td valign="bottom">1977</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 5</td><td valign="bottom">84.88</td><td valign="bottom">0,84 (relative to Model 4)</td><td valign="bottom">2335</td><td valign="bottom">42</td><td valign="bottom">3</td><td valign="bottom">48</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 6</td><td valign="bottom">84.08</td><td valign="bottom">0,04 (relative to Model 4)</td><td valign="bottom">2331</td><td valign="bottom">42</td><td valign="bottom">3</td><td valign="bottom">28</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom">Human 30-degree</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Model 1</td><td valign="bottom">37.40</td><td valign="bottom"/><td valign="bottom">6819</td><td valign="bottom">33</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Model 2</td><td valign="bottom">55.72</td><td valign="bottom">18,32 (relative to Model 1)</td><td valign="bottom">4843</td><td valign="bottom">36</td><td valign="bottom">3</td><td valign="bottom">1975</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 3</td><td valign="bottom">54.36</td><td valign="bottom">16,96 (relative to Model 1)</td><td valign="bottom">5217</td><td valign="bottom">36</td><td valign="bottom">3</td><td valign="bottom">1602</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 4</td><td valign="bottom">81.32</td><td valign="bottom">25,6 (relative to Model 2)</td><td valign="bottom">2910</td><td valign="bottom">39</td><td valign="bottom">3</td><td valign="bottom">1956</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 5</td><td valign="bottom">82.88</td><td valign="bottom">1,56 (relative to Model 4)</td><td valign="bottom">2809</td><td valign="bottom">42</td><td valign="bottom">3</td><td valign="bottom">101</td><td valign="bottom">&lt;0,0001</td></tr><tr><td valign="bottom"/><td valign="bottom">Model 6</td><td valign="bottom">81.92</td><td valign="bottom">0,6 (relative to Model 4)</td><td valign="bottom">2890</td><td valign="bottom">42</td><td valign="bottom">3</td><td valign="bottom">19</td><td valign="bottom">0.0002</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Parameters of the Bayesian motion morphing algorithm.</title><p>The observation matrix <bold>Y</bold> is formed by <italic>N</italic> samples of dimension <italic>D</italic>, where <italic>N</italic> results from <italic>S</italic> * <italic>E</italic> trails with <italic>T</italic> time steps. The dimensions <italic>M</italic> and <italic>Q</italic> of the latent variables were manually chosen. The integers <italic>S</italic> and <italic>E</italic> specify the number of species and expressions (two in our case).</p></caption><table frame="hsides" rules="groups"><tbody><tr><th colspan="3">Parameters of motion morphing algorithm</th></tr><tr><th valign="bottom">Parameters</th><th valign="bottom">Description</th><th valign="bottom">Value</th></tr><tr><td valign="bottom"> <italic>D</italic></td><td valign="bottom">Data dimension</td><td valign="bottom">208</td></tr><tr><td valign="bottom"> <italic>M</italic></td><td valign="bottom">First layer dimension</td><td valign="bottom">6</td></tr><tr><td valign="bottom"> <italic>Q</italic></td><td valign="bottom">Second layer dimension</td><td valign="bottom">2</td></tr><tr><td valign="bottom"> <italic>T</italic></td><td valign="bottom">Number of samples per trial</td><td valign="bottom">150</td></tr><tr><td valign="bottom"> <italic>S</italic></td><td valign="bottom">Number of species</td><td valign="bottom">2</td></tr><tr><td valign="bottom"> <italic>E</italic></td><td valign="bottom">Number of expressions</td><td valign="bottom">two or 3</td></tr><tr><td valign="bottom"> <italic>N</italic></td><td valign="bottom">Number of all samples</td><td valign="bottom"><italic>T</italic> * <italic>S</italic> * <italic>E</italic></td></tr><tr><th colspan="2" valign="bottom">Hyper parameters (learned)</th><th valign="bottom">Size</th></tr><tr><td><inline-formula><mml:math id="inf104"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Inverse width of kernel <inline-formula><mml:math id="inf105"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">1</td></tr><tr><td><inline-formula><mml:math id="inf106"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Inverse width of kernel <inline-formula><mml:math id="inf107"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">1</td></tr><tr><td><inline-formula><mml:math id="inf108"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Inverse width for non-linear part one of kernel <inline-formula><mml:math id="inf109"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">1</td></tr><tr><td><inline-formula><mml:math id="inf110"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Inverse width for non-linear part two of kernel <inline-formula><mml:math id="inf111"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">1</td></tr><tr><td><inline-formula><mml:math id="inf112"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Precision absorbed from noise term <inline-formula><mml:math id="inf113"><mml:msub><mml:mrow><mml:mi mathvariant="bold">ε</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">1</td></tr><tr><td><inline-formula><mml:math id="inf114"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Variance for non-linear part of <inline-formula><mml:math id="inf115"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">1</td></tr><tr><td><inline-formula><mml:math id="inf116"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Variance for linear part of <inline-formula><mml:math id="inf117"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">1</td></tr><tr><td><inline-formula><mml:math id="inf118"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Variance for non-linear part of <inline-formula><mml:math id="inf119"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">1</td></tr><tr><td><inline-formula><mml:math id="inf120"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Variance for linear part of <inline-formula><mml:math id="inf121"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">1</td></tr><tr><td><inline-formula><mml:math id="inf122"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Variance for non-linear part of <inline-formula><mml:math id="inf123"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">1</td></tr><tr><td><inline-formula><mml:math id="inf124"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Variance for linear part one of <inline-formula><mml:math id="inf125"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">1</td></tr><tr><td><inline-formula><mml:math id="inf126"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Variance for linear part two of <inline-formula><mml:math id="inf127"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">1</td></tr><tr><th valign="bottom">Variables</th><th valign="bottom"/><th valign="bottom">Size</th></tr><tr><td valign="bottom"><bold>Y</bold></td><td valign="bottom">Data</td><td valign="bottom"><italic>N</italic> x <italic>D</italic></td></tr><tr><td valign="bottom"><bold>H</bold></td><td valign="bottom">Latent variable of first layer</td><td valign="bottom"><italic>N</italic> x <italic>M</italic></td></tr><tr><td valign="bottom"><bold>X</bold></td><td valign="bottom">Latent variable of second layer</td><td valign="bottom"><italic>N</italic> x <italic>Q</italic></td></tr><tr><td><inline-formula><mml:math id="inf128"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Style variable vector for monkey species</td><td valign="bottom"><italic>S</italic> x <italic>1</italic></td></tr><tr><td><inline-formula><mml:math id="inf129"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Style variable vector for human species</td><td valign="bottom"><italic>S</italic> x <italic>1</italic></td></tr><tr><td><inline-formula><mml:math id="inf130"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Style variable vector for expression one</td><td valign="bottom"><italic>E</italic> x <italic>1</italic></td></tr><tr><td><inline-formula><mml:math id="inf131"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="bottom">Style variable vector for expression two</td><td valign="bottom"><italic>E</italic> x <italic>1</italic></td></tr></tbody></table></table-wrap><table-wrap id="app1table3" position="float"><label>Appendix 1—table 3.</label><caption><title>Detailed results of the two-way ANOVAs.</title><p>ANOVA for the threshold: two-way mixed model with expression type as within-subject factor and the stimulus type as between-subject factor for both the monkey and the human avatar. Steepness: two-way ANOVA with avatar type and expression factor for each stimulus motion type (original, occluded, and equilibrated). The mean square is defined as Mean Square = Sum of Square/df; df = degree of freedom.</p></caption><table frame="hsides" rules="groups"><tbody><tr><th colspan="7">ANOVAs</th></tr><tr><th valign="bottom">Threshold</th><th valign="bottom"><italic>Monkey avatar</italic></th><th valign="bottom">Sum of square</th><th valign="bottom">df</th><th valign="bottom">Mean square</th><th valign="bottom">F</th><th valign="bottom">p</th></tr><tr><td valign="bottom"/><td valign="bottom">Stimulus type</td><td valign="bottom">0,00</td><td valign="bottom">2</td><td valign="bottom">0,00</td><td valign="bottom">0,00</td><td valign="bottom">0999</td></tr><tr><td valign="bottom"/><td valign="bottom">Expression type</td><td valign="bottom">1,20</td><td valign="bottom">1</td><td valign="bottom">1,20</td><td valign="bottom">188,83</td><td valign="bottom">0000</td></tr><tr><td valign="bottom"/><td valign="bottom">Stimulus * Expression</td><td valign="bottom">0,06</td><td valign="bottom">2</td><td valign="bottom">0,03</td><td valign="bottom">4,51</td><td valign="bottom">0015</td></tr><tr><td valign="bottom"/><td valign="bottom">Error</td><td valign="bottom">0,42</td><td valign="bottom">60</td><td valign="bottom">0,01</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Total</td><td valign="bottom">1,72</td><td valign="bottom">65</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom"><italic>Human avatar</italic></td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Stimulus type</td><td valign="bottom">0,00</td><td valign="bottom">2</td><td valign="bottom">0,00</td><td valign="bottom">0,01</td><td valign="bottom">0993</td></tr><tr><td valign="bottom"/><td valign="bottom">Expression type</td><td valign="bottom">0,40</td><td valign="bottom">1</td><td valign="bottom">0,40</td><td valign="bottom">46,37</td><td valign="bottom">0000</td></tr><tr><td valign="bottom"/><td valign="bottom">Stimulus * Expression</td><td valign="bottom">0,05</td><td valign="bottom">2</td><td valign="bottom">0,03</td><td valign="bottom">3,15</td><td valign="bottom">0049</td></tr><tr><td valign="bottom"/><td valign="bottom">Error</td><td valign="bottom">0,57</td><td valign="bottom">60</td><td valign="bottom">0,01</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Total</td><td valign="bottom">1,02</td><td valign="bottom">65</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><th valign="bottom">Steepness</th><td valign="bottom"><italic>Original motion stimulus</italic></td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Avatar type</td><td valign="bottom">376,68</td><td valign="bottom">1</td><td valign="bottom">376,68</td><td valign="bottom">6,3</td><td valign="bottom">0016</td></tr><tr><td valign="bottom"/><td valign="bottom">Expression type</td><td valign="bottom">0,36</td><td valign="bottom">1</td><td valign="bottom">0,36</td><td valign="bottom">0,01</td><td valign="bottom">0939</td></tr><tr><td valign="bottom"/><td valign="bottom">Avatar * Expression</td><td valign="bottom">0,16</td><td valign="bottom">1</td><td valign="bottom">0,16</td><td valign="bottom">0</td><td valign="bottom">0959</td></tr><tr><td valign="bottom"/><td valign="bottom">Error</td><td valign="bottom">2391,21</td><td valign="bottom">40</td><td valign="bottom">59,78</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Total</td><td valign="bottom">2768,41</td><td valign="bottom">43</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom"><italic>Occluded motion stimulus</italic></td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Avatar type</td><td valign="bottom">286,17</td><td valign="bottom">1</td><td valign="bottom">286,17</td><td valign="bottom">3,33</td><td valign="bottom">0076</td></tr><tr><td valign="bottom"/><td valign="bottom">Expression type</td><td valign="bottom">0,02</td><td valign="bottom">1</td><td valign="bottom">0,02</td><td valign="bottom">0</td><td valign="bottom">0988</td></tr><tr><td valign="bottom"/><td valign="bottom">Avatar * Expression</td><td valign="bottom">0,00</td><td valign="bottom">1</td><td valign="bottom">0,00</td><td valign="bottom">0</td><td valign="bottom">0995</td></tr><tr><td valign="bottom"/><td valign="bottom">Error</td><td valign="bottom">3094,54</td><td valign="bottom">36</td><td valign="bottom">85,96</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Total</td><td valign="bottom">3380,73</td><td valign="bottom">39</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom"><italic>Equilibrated motion stimulus</italic></td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Avatar type</td><td valign="bottom">1,57</td><td valign="bottom">1</td><td valign="bottom">1,57</td><td valign="bottom">0,4</td><td valign="bottom">0533</td></tr><tr><td valign="bottom"/><td valign="bottom">Expression type</td><td valign="bottom">0,25</td><td valign="bottom">1</td><td valign="bottom">0,25</td><td valign="bottom">0,06</td><td valign="bottom">0803</td></tr><tr><td valign="bottom"/><td valign="bottom">Avatar * Expression</td><td valign="bottom">0,02</td><td valign="bottom">1</td><td valign="bottom">0,02</td><td valign="bottom">0</td><td valign="bottom">0945</td></tr><tr><td valign="bottom"/><td valign="bottom">Error</td><td valign="bottom">174,76</td><td valign="bottom">44</td><td valign="bottom">3,97</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td valign="bottom"/><td valign="bottom">Total</td><td valign="bottom">176,60</td><td valign="bottom">47</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr></tbody></table></table-wrap></sec></boxed-text></sec></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.61197.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kok</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This paper employs novel cross species stimuli (human and monkey) and a well-designed psychophysical paradigm to study the processing of dynamic facial expressions. Strikingly, the study shows that facial expression discrimination is largely independent on whether the expression is conveyed by a human or a monkey face. The novel photo-realistic dynamic avatars developed here will allow future studies to address novel questions about facial expression and social communication.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Shape-invariant perceptual encoding of dynamic facial expressions across species&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, when editors judge that a submitted work as a whole belongs in <italic>eLife</italic> but that some conclusions require a modest amount of additional new data, as they do with your paper, we are asking that the manuscript be revised to either limit claims to those supported by data in hand, or to explicitly state that the relevant conclusions require additional supporting data.</p><p>Our expectation is that the authors will eventually carry out the additional experiments and report on how they affect the relevant conclusions either in a preprint on bioRxiv or medRxiv, or if appropriate, as a Research Advance in <italic>eLife</italic>, either of which would be linked to the original paper.</p><p>Summary:</p><p>The paper employs novel cross species stimuli and a well-designed psychophysical paradigm to study the visual processing of facial expression. The authors show that facial expression discrimination is largely invariant to face shape (human vs. monkey). Furthermore, they reveal sharper tuning for recognising human expressions compared to monkey expressions, independent of whether these expressions were conveyed by a human or a monkey face. Technically, the paper is of a very high quality, both in terms of stimulus creation, but also in terms of analysis.</p><p>Revisions for this paper:</p><p>1. A central claim of the paper and the first words in the title are that the behavior studied (categorization of facial expression dynamics) is &quot;shape-invariant&quot;. However, the lack of variation in facial shapes (n = 2) used here limits the strength of the conclusions that can be drawn, and it certainly remains an open question whether representations of facial expression dynamics are truly &quot;shape-invariant&quot;. A simple control would have been to vary the viewing angle of the avatars, in order to dissociate 3D object shapes from their 2D projections (images). The authors also claim that &quot;face shapes differ considerably&quot; (line 49) amongst primate species, which is clearly true in absolute terms. However, the structural similarity of simian primate facial morphology (i.e. humans and macaques used here) is striking when compared to various non-primate species, which naturally raises questions about just how shape-invariant facial expression recognition is. The lack of data to more thoroughly support the central claim is problematic. In the absence of additional data, the authors should tone down this claim.</p><p>2. As the authors note, macaque and human facial expressions of 'fear' and 'threat' differ considerably in visual salience and motion content – both in 3D and their 2D projections (i.e. optic flow). Indeed, the decision to 'match' expressions across species based on semantic meaning rather than physical muscle activations is a central problem here. Figure 1A illustrates clearly the relative subtlety of the human expression compared to the macaque avatar's extreme open-mouthed pose, while Figure 1D (right panels) shows that this is also true of macaque expressions mapped onto the human avatar. The authors purportedly controlled for this in an 'optic-flow equilibrated' experiment that produced similar results. However, this crucial control is currently difficult to assess since the control stimuli are not illustrated and the description of their creation (in the supplementary materials) is rather convoluted and obfuscates what the actual control stimuli were.</p><p>The results of this control experiment that are presented (hidden away in supplementary Appendix 3—figure 1C) show that subjects rated the equilibrated stimuli at similar levels of expressiveness for the human vs. macaque avatars. However, what the reader really needs to know is whether subjects rated the human vs. macaque expression dynamics to be similarly expressive (irrespective of avatar)? My understanding is that species expression (and not species face shape) is the variable that the authors were attempting to equilibrate for.</p><p>In short, the authors have not presented data to convince a reader that their equilibrated stimuli resolve the obvious confound in their original stimuli (namely the correlation between low level visual salience – especially around the mouth region – and the species of the expression dynamics). These data should either be presented, or the authors' claims should be toned down.</p><p>3. This paper appears to be the human psychophysics component of work that the authors have recently published using the macaque avatar. The separate paper (Siebert et al., 2020 – eNeuro) reported basic macaque behavioral responses to similar animations, while the task here takes advantage of the more advanced behavioral methods that are possible in human subjects. Nevertheless, the emphasis of the current paper on cross-species perception raises the question – how do macaques perceive these stimuli? Do the authors have any macaque behavioral data for these stimuli (even if not for the 4AFC task) that could be included to round this out? If not, we recommend rewording the title since its current grammatical structure implies that the encoding is &quot;across species&quot;, whereas encoding <italic>of species</italic> (shape and expression) was only tested in one species (humans).</p><p>4. The authors may want to consider restructuring the Results section. The main take-home messages do not come through clearly in the way the Results section is currently structured. It contains a lot of technical detail – despite considerable use of Supplementary Information (SI) – which made extracting the empirical findings quite hard work. The details of the multinominal regression, the model comparisons (Table 1) and even the Discriminant Functions (Figure 2), for example, could all be briefly mentioned in the main text, with details provided in Methods or SI. These are all interesting, but the focus of the Results section should be on the behavioural findings, not the methods. The authors could use their Discussion – which clearly states the key points – as a guide, making sure the focus is more on Figure 3 and then working through the points more concisely.</p><p>Revisions expected in follow-up work:</p><p>1. Future work should expand the set of face shapes in order to properly test for shape invariance.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Shape-invariant encoding of dynamic primate facial expressions in human perception&quot; for further consideration by <italic>eLife</italic>. Your revised article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor.</p><p>The manuscript has been very much improved. We were particularly impressed with the additional experiments conducted. There are some remaining issues that need to be addressed, as outlined below:</p><p>– The additional explanation of the analysis now provided by Figure 2 is welcome, but the layout and labeling of the main result in Figure 3 is still rather taxing on the reader and suboptimal for conveying the main result ('shape invariance'). In fact, Figure 3 is now less focussed as a result of including more data. Readers currently have to memorize which species and expression each of the 4 'style parameters' (&quot;P1(e,s) – P4(e,s)&quot;) correspond to, and then parse a grid of 40 such plots in order to discern which they should be visually comparing. The previous iteration of this figure (previously Figure 2) with one column for macaque avatar and another for human avatar made this a bit easier. Minimally, the authors could improve the labeling of plots in Figure 3 to help readers parse the large array of heatmaps.</p><p>– The authors continue to insist that their macaque avatar achieves &quot;the best known degree of realism&quot; to date. There does not seem to be empirical evidence to support such a claim, since the authors' previous work (Siebert et al., 2020) did not compare their own avatar to other groups' avatars. At least one other group has found similar looking preference results for their own macaque avatar (Wilson et al., 2020).</p><p>– The links provided to online data and stimuli (https://hih-git.neurologie.uni-tuebingen.de/ntaubert/FacialExpressions) appear not to be publicly accessible at present. Will this be changed before publication or does access need to be requested? If the latter, instructions for obtaining access should be provided.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.61197.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Revisions for this paper:</p><p>1. A central claim of the paper and the first words in the title are that the behavior studied (categorization of facial expression dynamics) is &quot;shape-invariant&quot;. However, the lack of variation in facial shapes (n = 2) used here limits the strength of the conclusions that can be drawn, and it certainly remains an open question whether representations of facial expression dynamics are truly &quot;shape-invariant&quot;. A simple control would have been to vary the viewing angle of the avatars, in order to dissociate 3D object shapes from their 2D projections (images). The authors also claim that &quot;face shapes differ considerably&quot; (line 49) amongst primate species, which is clearly true in absolute terms. However, the structural similarity of simian primate facial morphology (i.e. humans and macaques used here) is striking when compared to various non-primate species, which naturally raises questions about just how shape-invariant facial expression recognition is. The lack of data to more thoroughly support the central claim is problematic. In the absence of additional data, the authors should tone down this claim.</p></disp-quote><p>In fact, the possibility to test different head shapes in our study had been strongly limited by the enormous technical effort that is necessary to create such highly believable dynamic avatar models, even for one head shape. Unfortunately, this methodological limitation cannot easily be overcome. We decided thus to follow the reviewers’ suggestion to increase the number of tested stimuli by including view variations. This increases the number of tested 2D facial patterns that are associated with the same dynamic expression. To accommodate this suggestion, we have repeated a substantial part of our experiments with patterns with a different view. This view was taken as dissimilar to the front view as possible, without producing salient occlusions or artifacts. As result, we found that our main hypothesis is largely confirmed, even for this more variable stimulus set.</p><p>In order to accommodate this suggested extension, we had to change multiple things in the manuscript:</p><p>– We have added the new data from the repetition of two experiments with different stimulus views to the main paper and have adjusted the statistical analysis. As result, the amount of presented data now is substantially larger. This made it necessary to reorganize the figures in the manuscript in order to keep the presentation of the data understandable. Specifically, we have increased the number of figures from 3 to 5. Several figure panels had to be assigned to new figures for this purpose.</p><p>– We have toned down and refined the claims about shape invariance in an absolute sense. Specifically, we have introduced a distinction of 3D and 2D shape, and we have restricted some claims to the class of faces of primates. Also we have added a short discussion of the mentioned limitations to the Discussion section.</p><disp-quote content-type="editor-comment"><p>2. As the authors note, macaque and human facial expressions of 'fear' and 'threat' differ considerably in visual salience and motion content – both in 3D and their 2D projections (i.e. optic flow). Indeed, the decision to 'match' expressions across species based on semantic meaning rather than physical muscle activations is a central problem here. Figure 1A illustrates clearly the relative subtlety of the human expression compared to the macaque avatar's extreme open-mouthed pose, while Figure 1D (right panels) shows that this is also true of macaque expressions mapped onto the human avatar. The authors purportedly controlled for this in an 'optic-flow equilibrated' experiment that produced similar results. However, this crucial control is currently difficult to assess since the control stimuli are not illustrated and the description of their creation (in the supplementary materials) is rather convoluted and obfuscates what the actual control stimuli were.</p><p>The results of this control experiment that are presented (hidden away in supplementary Figure S3C) show that subjects rated the equilibrated stimuli at similar levels of expressiveness for the human vs macaque avatars. However, what the reader really needs to know is whether subjects rated the human vs macaque expression dynamics to be similarly expressive (irrespective of avatar)? My understanding is that species expression (and not species face shape) is the variable that the authors were attempting to equilibrate for.</p><p>In short, the authors have not presented data to convince a reader that their equilibrated stimuli resolve the obvious confound in their original stimuli (namely the correlation between low level visual salience – especially around the mouth region – and the species of the expression dynamics). These data should either be presented, or the authors' claims should be toned down.</p></disp-quote><p>In fact, our control stimuli were designed to control for the perceived expressiveness of the dynamic expression within the individual avatars, but across different species-specific motion types. However, the finally chosen equilibration method also has the consequence that the expressiveness between the two avatar types was balanced. (Relevant data has been added and is discussed more clearly now.) The reviewers are right that the motivation and the details about the equilibration were difficult to access in the old version of the manuscript. In addition, the description was not always very clear. We have tried to fix these issues and have substantially reorganized the material to adopt this criticism. More specifically, we have made the following changes:</p><p>– The description and motivation of the equilibration procedure has been massively rewritten in the main paper as well as in the Supplemental Information. The data supporting the efficiency of the equilibration method has been integrated in the main paper.</p><p>– We have tried to improve substantially the clarity of the explanation of the equilibration procedure and of the underlying assumptions. The Supplemental Information has been adopted to this reorganization of the material.</p><p>– We have added a new figure (Figure 5) showing examples of the original as well as of the equilibrated expressions for both avatar types. In addition, this figure shows the efficiency of the chosen procedure to balance the motion flow, as well as the expressiveness rating data that justifies the chosen method.</p><p>– We have quantified the efficiency of the procedure in terms of equilibrating the motion flow for the whole face, and separately also for the mouth region, finding a quite similar reductions of the variability of low-level information across the different conditions in style space.</p><disp-quote content-type="editor-comment"><p>3. This paper appears to be the human psychophysics component of work that the authors have recently published using the macaque avatar. The separate paper (Siebert et al., 2020 – eNeuro) reported basic macaque behavioral responses to similar animations, while the task here takes advantage of the more advanced behavioral methods that are possible in human subjects. Nevertheless, the emphasis of the current paper on cross-species perception raises the question – how do macaques perceive these stimuli? Do the authors have any macaque behavioral data for these stimuli (even if not for the 4AFC task) that could be included to round this out? If not, we recommend rewording the title since its current grammatical structure implies that the encoding is &quot;across species&quot;, whereas encoding of species (shape and expression) was only tested in one species (humans).</p></disp-quote><p>We agree with the reviewers that it would be very attractive to have also macaque psychophysical data on the same stimulus set. Unfortunately, this would imply a major experimental effort because the animals would have to be trained on an active task, requiring a discrimination the expression type independent of the avatar type, without the possibility to give direct instructions about this to the animal. We thus followed the reviewers’ suggestion to reword the title, in order to reflect more appropriately that our study is based on human data only. As new title we chose: ‘Shape-invariant encoding of dynamic primate facial expressions in human perception’. We also have carefully corrected the argumentation in the rest of the manuscript to avoid misunderstandings concerning the mentioned point.</p><disp-quote content-type="editor-comment"><p>4. The authors may want to consider restructuring the Results section. The main take-home messages do not come through clearly in the way the Results section is currently structured. It contains a lot of technical detail – despite considerable use of Supplementary Information (SI) – which made extracting the empirical findings quite hard work. The details of the multinominal regression, the model comparisons (Table 1) and even the Discriminant Functions (Figure 2), for example, could all be briefly mentioned in the main text, with details provided in Methods or SI. These are all interesting, but the focus of the Results section should be on the behavioural findings, not the methods. The authors could use their Discussion – which clearly states the key points – as a guide, making sure the focus is more on Figure 3 and then working through the points more concisely.</p></disp-quote><p>We thank the reviewers and the editors very much for this suggestion. We have massively restructured the paper to separate more clearly the Results and the Methods part. Several methodological details have been shifted to the Methods section, and specifically all equations. We tried, however, to keep the Results understandable by providing the necessary minimal information about the relevant methods. The Methods section has been restructured in order to accommodate the relevant additional material. Very detailed or technical parts of the methods have been reserved for the Supplementary Information.</p><disp-quote content-type="editor-comment"><p>Revisions expected in follow-up work:</p><p>1. Future work should expand the set of face shapes in order to properly test for shape invariance.</p></disp-quote><p>We have added a substantial amount of new data with different 2D shapes by testing stimuli with a different view angle. We have also added a discussion of this limitation to the final Discussion section, where we point to this limitation and the necessity to test a broader set of head shapes, including also non-primate faces, defining an interesting direction for future work.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been very much improved. We were particularly impressed with the additional experiments conducted. There are some remaining issues that need to be addressed, as outlined below:</p><p>– The additional explanation of the analysis now provided by Figure 2 is welcome, but the layout and labelling of the main result in Figure 3 is still rather taxing on the reader and suboptimal for conveying the main result ('shape invariance'). In fact, Figure 3 is now less focussed as a result of including more data. Readers currently have to memorize which species and expression each of the 4 'style parameters' (&quot;P1(e,s) – P4(e,s)&quot;) correspond to, and then parse a grid of 40 such plots in order to discern which they should be visually comparing. The previous iteration of this figure (previously Figure 2) with one column for macaque avatar and another for human avatar made this a bit easier. Minimally, the authors could improve the labelling of plots in Figure 3 to help readers parse the large array of heatmaps.</p></disp-quote><p>The reviewers’ criticism seems justified and is obviously a consequence of trying to squeeze so much more data in a single figure. We have now tried to split the figure into multiple ones, and we have tried to improve the labelling of the individual panels to make the figure more easily understandable. Specifically:</p><p>– We have split the conditions without and with equilibration and the data from the occlusion condition in three separate figures. Each of them has, in addition, the corresponding color plot of the significance levels belonging to the tests for differences between the multinomial classification distributions between the human and the monkey avatar.</p><p>– We have added an additional list that specifies clearly the association of the different discriminant functions π with the class labels (‘Human Angry/Threat’, ‘Human Fear’, ‘Monkey Angry/Threat’, ‘Monkey Fear’).</p><p>– We have adjusted the numbering of the figures in the text and updated the captions.</p><disp-quote content-type="editor-comment"><p>– The authors continue to insist that their macaque avatar achieves &quot;the best known degree of realism&quot; to date. There does not seem to be empirical evidence to support such a claim, since the authors' previous work (Siebert et al., 2020) did not compare their own avatar to other groups' avatars. At least one other group has found similar looking preference results for their own macaque avatar (Wilson et al., 2020).</p></disp-quote><p>Following the reviewers’ advice, we have carefully reworded the relevant section in the text to avoid the mentioned overclaims. The avatar by Wilson et al. is not dynamic and does not show an uncanny valley effect, but it results in similar looking times as original videos. We have included this reference and refined our wording. The modified text reads now:</p><p>‘The used dynamic head models achieve state-of-the-art degree of realism for the human head, and to our knowledge we present the only highly realistic monkey avatar used in physiology so far that is animated with motion capture data from real animals. […] In another study monkey facial avatar motion was controlled by parameters derived from video frames<sup>35</sup>. A further study showed similar looking times for static pictures of monkeys and avatar faces (without facial motion)<sup>40</sup>.’</p><p>In our view, this formulation is not an overclaim and should be justified by the data that we have provided. If reviewers should keep having problems with this paragraph, we will probably take it out completely, since it is not central for the paper.</p><disp-quote content-type="editor-comment"><p>– The links provided to online data and stimuli (https://hih-git.neurologie.uni-tuebingen.de/ntaubert/FacialExpressions) appear not to be publicly accessible at present. Will this be changed before publication or does access need to be requested? If the latter, instructions for obtaining access should be provided.</p></disp-quote><p>We have updated the links and hope that the material now is accessible. Please let us know if there remain any problems.</p></body></sub-article></article>