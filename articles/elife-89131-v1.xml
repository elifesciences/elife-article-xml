<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">89131</article-id><article-id pub-id-type="doi">10.7554/eLife.89131</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89131.4</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>When and why does motor preparation arise in recurrent neural network models of motor control?</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Schimel</surname><given-names>Marine</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6937-011X</contrib-id><email>mmcs3@cam.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Kao</surname><given-names>Ta-Chu</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Hennequin</surname><given-names>Guillaume</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>Computational and Biological Learning Lab, Department of Engineering, University of Cambridge</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution>Meta Reality Labs</institution><addr-line><named-content content-type="city">Burlingame</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Diedrichsen</surname><given-names>Jörn</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Western University</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Senior Editor</role><aff><institution>Donders Institute for Brain, Cognition and Behaviour</institution><country>Netherlands</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>24</day><month>09</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP89131</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-05-23"><day>23</day><month>05</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-06-19"><day>19</day><month>06</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.04.03.535429"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-08-10"><day>10</day><month>08</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89131.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-05-20"><day>20</day><month>05</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89131.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-08-13"><day>13</day><month>08</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89131.3"/></event></pub-history><permissions><copyright-statement>© 2023, Schimel et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Schimel et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-89131-v1.pdf"/><related-article related-article-type="commentary" ext-link-type="doi" xlink:href="10.7554/eLife.102187" id="ra1"/><abstract><p>During delayed ballistic reaches, motor areas consistently display movement-specific activity patterns prior to movement onset. It is unclear why these patterns arise: while they have been proposed to seed an initial neural state from which the movement unfolds, recent experiments have uncovered the presence and necessity of ongoing inputs during movement, which may lessen the need for careful initialization. Here, we modeled the motor cortex as an input-driven dynamical system, and we asked what the optimal way to control this system to perform fast delayed reaches is. We find that delay-period inputs consistently arise in an optimally controlled model of M1. By studying a variety of network architectures, we could dissect and predict the situations in which it is beneficial for a network to prepare. Finally, we show that optimal input-driven control of neural dynamics gives rise to multiple phases of preparation during reach sequences, providing a novel explanation for experimentally observed features of monkey M1 activity in double reaching.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>recurrent neural networks</kwd><kwd>motor control</kwd><kwd>motor preparation</kwd><kwd>optimal control</kwd><kwd>delayed reaching</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>RG94782</award-id><principal-award-recipient><name><surname>Schimel</surname><given-names>Marine</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A computational model shows that preparation arises as an optimal control strategy in input-driven recurrent neural networks performing a delayed-reaching task.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>During the production of ballistic movements, the motor cortex is thought to operate as a dynamical system whose state trajectories trace out the appropriate motor commands for downstream effectors (<xref ref-type="bibr" rid="bib45">Shenoy et al., 2013</xref>; <xref ref-type="bibr" rid="bib33">Miri et al., 2017</xref>; <xref ref-type="bibr" rid="bib39">Russo et al., 2018</xref>). The extent to which these cortical dynamics are controlled by exogenous inputs before and/or during movement is the subject of ongoing study.</p><p>On the one hand, several experimental and modeling studies point to a potential role for exogenous inputs in motor preparation. First, cortical state trajectories are empirically well described by a low-dimensional dynamical system evolving near-autonomously during movement (<xref ref-type="bibr" rid="bib6">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib35">Pandarinath et al., 2018</xref>; <xref ref-type="bibr" rid="bib42">Schimel et al., 2022</xref>), such that there is a priori no reason to suspect that inputs are required for motor production. Rather, inputs would be required during preparation to bring the state of the cortical network into a suitable initial condition. This input-driven seeding process is corroborated by observations of movement-specific primary motor cortex (M1) activity arising well before movement initiation (<xref ref-type="bibr" rid="bib26">Lara et al., 2018</xref>; <xref ref-type="bibr" rid="bib24">Kaufman et al., 2014</xref>; <xref ref-type="bibr" rid="bib6">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib31">Meirhaeghe et al., 2023</xref>; <xref ref-type="fig" rid="fig1">Figure 1A</xref>), and associated models demonstrate the critical role of preparatory inputs therein (<xref ref-type="bibr" rid="bib52">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib19">Hennequin et al., 2014</xref>; <xref ref-type="bibr" rid="bib23">Kao et al., 2021</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Control is possible under different strategies.</title><p>(<bold>A</bold>) Trial-averaged firing rate of two representative monkey primary motor cortex (M1) neurons, across eight different movements, separately aligned to target onset (left) and movement onset (right). Neural activity starts separating across movements well before the animal starts moving. (<bold>B</bold>) Top: a recurrent neural network (RNN) model of M1 dynamics receives external inputs <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> from a higher-level controller, and outputs control signals for a biophysical two-jointed arm model. Inputs are optimized for the correct production of eight center-out reaches to targets regularly positioned around a circle. Bottom: firing rate of a representative neuron in the RNN model for each reach, under two extreme control strategies. In the first strategy (left, solid lines), the external inputs <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are optimized while being temporally confined to the preparatory period. In the second strategy (right, dashed lines), they are optimized while confined to the movement period. Although slight differences in hand kinematics can be seen (compare corresponding solid and dashed hand trajectories), both control policies lead to successful reaches. These introductory simulations are shown for illustration purposes; the particular choice of network connectivity and the way the control inputs were found are described in the Results section.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-fig1-v1.tif"/></fig><p>On the other hand, recent studies in mice have shown that the motor cortex receives critical pattern-generating input from the thalamus during movement production (<xref ref-type="bibr" rid="bib40">Sauerbrei et al., 2020</xref>), and recurrent neural network (RNN)-based modeling of the motor feedback loop involved in reaching movements suggests that sensory feedback may also contribute significantly to the observed dynamics of M1 (<xref ref-type="bibr" rid="bib20">Kalidindi et al., 2021</xref>). Moreover, most published network models of delayed reaches are able to perform the task just as well without preparatory inputs, i.e., with external inputs forcefully confined to the movement epoch – an illustratory example is shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. Thus, the relative contributions of preparatory vs. movement-epoch inputs to the dynamics implemented by M1 (potentially as part of a broader set of areas) remain unclear.</p><p>In addition to the specific form that inputs to cortical dynamics might take, one may ask more broadly about the computational role of motor preparation. Motor preparation is known to benefit behavior (e.g. by shortening reaction times and enabling more accurate execution <xref ref-type="bibr" rid="bib38">Riehle and Requin, 1989</xref>; <xref ref-type="bibr" rid="bib4">Churchland and Shenoy, 2007</xref>; <xref ref-type="bibr" rid="bib32">Michaels et al., 2015</xref>) and may facilitate motor learning (<xref ref-type="bibr" rid="bib44">Sheahan et al., 2016</xref>; <xref ref-type="bibr" rid="bib51">Sun et al., 2022</xref>). However, from the perspective of cortical dynamics, preparation also introduces additional constraints.</p><p>Specifically, the high density of M1 neurons projecting directly to the spinal cord (<xref ref-type="bibr" rid="bib11">Dum and Strick, 1991</xref>) suggests that motor cortical outputs control lower-level effectors with little intermediate processing. For preparatory processes to avoid triggering premature movement, any pre-movement activity in the motor and dorsal premotor (PMd) cortices must therefore engage the pyramidal tract neurons in a way that ensures their activity patterns will not lead to any movement.</p><p>While this can be achieved by constraining neural activity to evolve in a nullspace of the motor output (<xref ref-type="bibr" rid="bib24">Kaufman et al., 2014</xref>), the question nevertheless arises: what advantage is there to having neural dynamics begin earlier in a constrained manner, rather than unfold freely just in time for movement production?</p><p>Here, we sought a normative explanation for motor preparation at the level of motor cortex dynamics: we asked whether preparation arises in RNNs performing delayed-reaching tasks, and what factors lead to more or less preparation.</p><p>Such an explanation could not be obtained from previous network models of delayed reaches, as they typically assume from the get-go that the cortical network receives preparatory inputs during a fixed time window preceding the go cue (<xref ref-type="bibr" rid="bib52">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib23">Kao et al., 2021</xref>). In this case, pre-movement activity is by designing a critical determinant of the subsequent behavior (<xref ref-type="bibr" rid="bib52">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib23">Kao et al., 2021</xref>; <xref ref-type="bibr" rid="bib57">Zimnik and Churchland, 2021</xref>). In this work, we removed this modeling assumption and studied models in which the correct behavior could in principle be obtained without explicit motor preparation.</p><p>To study the role of motor preparation, and that of exogenous inputs in this process, we followed an optimal control approach (<xref ref-type="bibr" rid="bib16">Harris and Wolpert, 1998</xref>; <xref ref-type="bibr" rid="bib54">Todorov and Jordan, 2002</xref>; <xref ref-type="bibr" rid="bib56">Yeo et al., 2016</xref>). We considered the dynamics of an RNN model of M1 coupled to a model arm (<xref ref-type="bibr" rid="bib55">Todorov and Li, 2003</xref>), and used a standard control cost functional to quantify and optimize performance in a delayed-reaching task. We used the iterative linear quadratic regulator algorithm (iLQR) algorithm (<xref ref-type="bibr" rid="bib27">Li and Todorov, 2004</xref>) to find the spatiotemporal patterns of network inputs that minimize this cost functional, for any given network connectivity. Critically, these inputs could arise both before and during movement; thus, our framework allowed for principled selection among a continuum of motor strategies, going from purely autonomous motor generation following preparation, to purely input-driven unprepared dynamics.</p><p>We considered an inhibition-stabilized network – which was shown previously to capture prominent aspects of monkey M1 activity (<xref ref-type="bibr" rid="bib19">Hennequin et al., 2014</xref>; <xref ref-type="bibr" rid="bib23">Kao et al., 2021</xref>) – and found that optimal control of the model requires preparation, with optimal inputs arising well before movement begins. To understand what features of network connectivity lead to optimal preparatory control strategies, we first turned to low-dimensional models, which could be more easily dissected. We then generalized insights from those systems back to high-dimensional networks using tools from control theory, and found that preparation can be largely explained by two quantities summarizing the dynamical response properties of the network.</p><p>Finally, we studied the optimal control of movement <italic>sequences</italic>. Consistent with recent experimental findings (<xref ref-type="bibr" rid="bib57">Zimnik and Churchland, 2021</xref>), we observed that optimal control of compound reaches leads to input-driven preparatory activity in a dedicated activity subspace prior to each movement.</p><p>Overall, our results show that preparatory neural activity patterns arise from optimal control of reaching movements at the level of motor cortical circuits, thus providing a possible explanation for a number of observed experimental findings.</p><sec id="s1-1"><title>Model</title><sec id="s1-1-1"><title>A model of cortical dynamics for reaching movements</title><p>We considered a simple reaching task, in which the hand must move from a resting location to one of eight radially located targets in a 2D plane as fast as possible (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The target had to be reached within 600 ms of a go cue that follows a delay period of varying (but known) duration. We modeled the trajectory of the hand via a two-jointed model arm (<xref ref-type="bibr" rid="bib27">Li and Todorov, 2004</xref>; <xref ref-type="bibr" rid="bib23">Kao et al., 2021</xref>), driven into motion by a pair of torques <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (Methods). We further assumed that these torques arise as a linear readout of the momentary firing rates <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of a population of M1 neurons,<disp-formula id="equ1">,<label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> was a randomly generated readout matrix, projecting the neural activity into the output space. We modeled the dynamics of <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mstyle></mml:math></inline-formula> M1 neurons using a standard rate equation,<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the momentary population firing rate vector <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> was obtained by passing a vector of internal neuronal activations <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> through a rectified linear function <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mo>⋅</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, element-wise. In <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a constant input that establishes a baseline firing rate of 5 Hz on average, with a standard deviation of 5 Hz across neurons, <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is a task-dependent control input (see below), and <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the matrix of recurrent connection weights. Throughout most of this work, we considered inhibition-stabilized M1 dynamics (<xref ref-type="bibr" rid="bib19">Hennequin et al., 2014</xref>; Methods), which have previously been shown to produce activity resembling that of M1 during reaching (<xref ref-type="bibr" rid="bib23">Kao et al., 2021</xref>).</p><p>Thus, our model can be viewed as a two-level controller, with the arm being controlled by M1, and M1 being controlled by external inputs. Note that each instantiation of our model corresponds to a set of <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, none of which are specifically optimized for the task.</p></sec><sec id="s1-1-2"><title>To prepare or not to prepare?</title><p>Previous experimental (<xref ref-type="bibr" rid="bib6">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib45">Shenoy et al., 2013</xref>) and modeling (<xref ref-type="bibr" rid="bib19">Hennequin et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Pandarinath et al., 2018</xref>) work suggests that fast ballistic movements rely on strong dynamics, which are observed in M1 and well modeled as near-autonomous (although the underlying dynamical system may not be anatomically confined to M1, as we discuss later). Network-level models of ballistic control thus rely critically on a preparation phase during which they are driven into a movement-specific state that seeds their subsequent autonomous dynamics (<xref ref-type="bibr" rid="bib23">Kao et al., 2021</xref>; <xref ref-type="bibr" rid="bib52">Sussillo et al., 2015</xref>). However, somewhat paradoxically, the same recurrent neural network models can also solve the task in a completely different regime, in which task-related inputs arise during movement only, with no preparatory inputs whatsoever. We illustrate this dichotomy in <xref ref-type="fig" rid="fig1">Figure 1</xref>. The same center-out reach can be produced with control inputs to M1 that arise either prior to movement only (full lines), or during movement only (dashed lines). In the latter case, no reach-specific preparatory activity is observed, making the model inconsistent with experimental findings. But what rationale is there in preparing for upcoming movements, then?</p><p>To address this question, we formulated delayed reaching as an optimal control problem, and asked what external inputs are required, and at what time, to drive the hand into the desired position with minimum control effort. Specifically, we sought inputs that were as weak as possible yet accurately drove the hand to the target within an allotted time window. We also penalized inputs that caused premature movement before the go cue.</p><p>Thus, we solved for spatiotemporal input trajectories that minimized a cost functional capturing the various task requirements. Our cost was composed of three terms: <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>target</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> penalizes deviations away from the target, with an ‘urgency’ weight that increases quadratically with time, thus capturing the implicit incentive for animals to perform fast reaches in such experiments (which are normally conducted in sessions of fixed duration).</p><p><inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> penalizes premature movement during preparation, as measured by any deviation in position, speed, and acceleration of the hand. Finally, <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>effort</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> penalizes control effort in the form of input magnitude throughout the whole trial, thus promoting energy-efficient control solutions among a typically infinite set of possibilities (<xref ref-type="bibr" rid="bib23">Kao et al., 2021</xref>; <xref ref-type="bibr" rid="bib49">Sterling and Laughlin, 2015</xref>). Note that <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>effort</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> can be viewed as a standard regularization term, and must be included to ensure the control problem is well defined. The total objective thus had the following form:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⋆</mml:mo></mml:msup><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:mfrac></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mtext>target</mml:mtext></mml:msub></mml:mrow></mml:munder></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mtext>null</mml:mtext></mml:msub><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo mathvariant="bold">˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:mfrac></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mtext>null</mml:mtext></mml:msub></mml:mrow></mml:munder></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mtext>effort</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mtext>prep</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mtext>effort</mml:mtext></mml:msub></mml:mrow></mml:munder><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> denote the position and velocity of the hand in angular space, <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> was the duration of the delay period, and <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula> that of the movement period. As <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>target</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> depend on <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> implicitly through <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref>, <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a function of <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> only.</p><p>Importantly, we allowed for inputs within a time window beginning <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> ms before, and ending <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula> ms after the go cue (set at <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>). Therefore, both preparation-only and movement-only input strategies (<xref ref-type="fig" rid="fig1">Figure 1</xref>) could potentially arise, as well as anything in-between.</p><p>Note that this control objective assumes that the delay duration (<inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>) is known ahead of time, an assumption that does not hold for many delayed-reaching tasks in monkeys where the delay is uncertain. We make this assumption for computational tractability and later discuss extensions to the uncertain case (Discussion).</p><p>Here, we solved for the optimal control inputs using the iLQR (<xref ref-type="bibr" rid="bib27">Li and Todorov, 2004</xref>), an efficient trajectory optimization algorithm that is well suited for handling the nonlinear nature of both the arm’s and the network’s dynamics. As our primary goal was to assess the role of preparation in a normative way, we did not study the putative circuit dynamics upstream of M1 that might lead to the computation of these optimal inputs.</p><p>We balanced the various components of our cost functional by choosing <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>effort</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> to qualitatively match the behavioral requirements of a typical reach-and-hold task. Specifically, we tuned them jointly so as to ensure (i) stillness during preparation and (ii) reach duration of approximately ∼400 ms, with the hand staying within 0.5 cm of the target for ∼200 ms after the end of the reach. We ensured that the main qualitative features of the solution, i.e., the results presented below, were robust to the choice of hyperparameter values within the fairly large range in which the above soft-constraints are satisfied (Appendix 1).</p></sec></sec></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Preparation arises as an optimal control strategy</title><p>Using the above control framework, we assessed whether the optimal way of performing a delayed reach involves preparation.</p><p>More concretely, does the optimal control strategy of the model described in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> involve any preparatory inputs during the delay period? For any single optimally performed reach, we found that network activity began changing well before the go cue (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, bottom), and that this was driven by inputs that arose early (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, middle). Thus, although preparatory network activity cancels in the readout (such that the hand remains still; <xref ref-type="fig" rid="fig2">Figure 2B</xref>, top) and therefore does not contribute directly to movement, it still forms an integral part of the optimal reach strategy.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Optimal control of the inhibition-stabilized network (ISN).</title><p>(<bold>A</bold>) Illustration of the different terms in the control cost function, designed to capture the different requirements of the task. ‘Tgt’ marks the time of target onset, ‘Go’ that of the go cue (known in advance), and ‘End’ the end of the trial. (<bold>B</bold>) Time course of the hand velocity (top), optimal control inputs (middle; 10 example neurons), and firing rates (bottom, same neurons) during a delayed reach to one of the eight targets shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. Here, the delay period was set to <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> ms. Note that inputs arise well before the go cue, even though they have no direct effect on behavior at that stage. (<bold>C</bold>) Dependence of the different terms of the cost function on preparation time. All costs are normalized by the total cost at <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> ms. The inset shows the time course of the hand’s average distance to the relevant target when no preparation is allowed (blue) and when preparation is allowed (red). Although the target is eventually reached for all values of <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, the hand gets there faster with longer preparation times, causing a decrease in <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>tgt</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> – and therefore also in <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>tot</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. Another part of the decrease in <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>tot</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is due to a progressively lower input energy cost <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>effort</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. On the other hand, the cost of staying still before the go cue increases slightly with <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>D</bold>) We define the preparation index as the ratio of the norms of the external inputs during preparation and during movement (see text). The preparation index measures how much the optimal strategy relies on the preparatory period. As more preparation time is allowed, this is used by the optimal controller and more inputs are given during preparation. For longer preparation times, this ratio increases sub-linearly, and eventually settles.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-fig2-v1.tif"/></fig><p>To quantify how much the optimal control strategy relied on inputs prior to movement, we defined the <italic>preparation index</italic> as the ratio of input magnitude during the delay period to that during the remainder of the trial:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mtext>prep. index</mml:mtext><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msqrt><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:msqrt><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We found that the preparation index rose sharply as we increased the delay period, and eventually plateaued at ∼1.3 for delay periods longer than 300 ms (<xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><p>Similarly, the total cost of the task was highest in the absence of preparation, and decreased until it also reached a plateau at <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mn>300</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> ms (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, black). This appears somewhat counterintuitive, as having a larger <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> means that both <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>effort</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are accumulated over a longer period. To resolve this paradox, we examined each component of the cost function. We found that the overall decrease in cost with increasing preparation time was driven by a concurrent decrease in both <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>tgt</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>effort</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. The former effect was due to the model producing faster reaches (<xref ref-type="fig" rid="fig2">Figure 2C</xref> inset; hand position for a reach with [red] and without [blue] preparation) while the latter arose from smaller control inputs being necessary when preparation was allowed. Together, these results suggest that the presence of a delay period changes the optimal control strategy for reaching, and increases performance in the task.</p><p>The results above show that delaying the reach beyond ∼300 ms brings little benefit; in particular, all components of the cost stabilize past that point (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). We thus wondered what features the optimally controlled dynamics would display as <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> increased beyond 300 ms. Would the network defer preparation to a last minute surge, or prepare more gently over the entire preparatory window?</p><p>Would the network produce the same neural activity patterns? We found that the optimal controller made very little use of any preparation time available up to 300 ms before the go cue: with longer preparation times, external input continued to arise just a couple of hundred milliseconds before movement initiation, and single neuron firing rates remained remarkably similar (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). This was also seen in PCA projections of the firing rates, which traced out similar trajectories irrespective of the delay period (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). We hypothesized that this behavior is due to the network dynamics having a certain maximum characteristic timescale, such that inputs that arrive too early end up being ‘forgotten’ – they increase <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>effort</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and possibly <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> without having a chance to influence <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>tgt</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. We confirmed this by varying the characteristic time constant (<inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). For a fixed <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, we found that for larger (resp. lower) values of <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi></mml:mstyle></mml:math></inline-formula>, the optimal control inputs started rising earlier (resp. later) and thus occupied more (resp. less) of the alloted preparatory period (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Conservation of the optimal control strategy across delays.</title><p>(<bold>A</bold>) Optimal control inputs to 10 randomly chosen neurons in the model recurrent neural network (RNN) (left) and their corresponding firing rates (right) for different preparation times <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> (ranging from 0 to 800 ms; c.f. labels). (<bold>B</bold>) Projection of the movement-epoch population activity for each of the eight reaches (panels) and each value of <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> shown in A (darker to lighter colors). These population trajectories are broadly conserved across delay times, and become more similar for larger delays.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-fig3-v1.tif"/></fig></sec><sec id="s2-2"><title>Understanding optimal control in simplified models</title><p>Having established that the inhibition-stabilized network (ISN) model of M1 relies on preparatory inputs to solve the delayed-reaching task, we next tried to understand <italic>why</italic> it does so.</p><p>To further unravel the interplay between the structure of the network and the optimal control strategy, i.e., what aspects of the dynamics of the network warrant preparation, we turned to simpler, two-dimensional (2D) models of cortical dynamics. These 2D models are small enough to enable detailed analysis (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>), yet rich enough to capture the two dominant dynamical phenomena that arise in ISN dynamics: nonnormal amplification (<xref ref-type="bibr" rid="bib34">Murphy and Miller, 2009</xref>; <xref ref-type="bibr" rid="bib13">Goldman, 2009</xref>; <xref ref-type="bibr" rid="bib18">Hennequin, 2012</xref>) and oscillations (<xref ref-type="bibr" rid="bib3">Brunel, 2000</xref>; <xref ref-type="bibr" rid="bib8">Dayan and Abbott, 2001</xref>). Specifically, networks of E and I neurons have been shown to embed two main motifs of effective connectivity which are revealed by appropriate orthogonal changes of basis: (i) feedforward (‘nonnormal’) connectivity whereby a ‘source mode’ of E-I imbalance feeds into a ‘sink mode’ in which balance is restored, and (ii) anti-symmetric connectivity that causes the two populations to oscillate.</p><p>To study the impact of each of these prototypical connectivity motifs on movement preparation, we implemented them separately, i.e., as two small networks of two units each, with an overall connectivity scale parameter <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>w</mml:mi></mml:mstyle></mml:math></inline-formula> which we varied (<xref ref-type="fig" rid="fig4">Figure 4A and D</xref>; Methods). As both nonnormal and oscillatory dynamics arise from linear algebraic properties of the connectivity matrix, we considered linear network dynamics for this analysis (<inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>). Moreover, to preserve the existence of an output nullspace in which preparation could in principle occur without causing premature movement, we reduced the dimensionality of the motor readout from 2D (where there would be no room left for a nullspace) to 1D (leaving a 1D nullspace), and adapted the motor task so that the network now had to move the hand position along a single dimension (<xref ref-type="fig" rid="fig4">Figure 4B and E</xref>, top). Analogous to the previous arm model, we assumed that the hand’s acceleration along this axis was directly given by the 1D network readout.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Analysis of the interplay between the optimal control strategy and two canonical motifs of E-I network dynamics: nonnormal transients driven by feedforward connectivity (<bold>A–C</bold>), and oscillations driven by anti-symmetric connectivity (<bold>D–F</bold>).</title><p>(<bold>A</bold>) Activity flow field (10 example trajectories) of the nonnormal network, in which a ‘source’ unit (orange) drives a ‘sink’ unit (brown). We consider two opposite readout configurations, where it is either the sink (left) or the source (right) that drives the acceleration of the hand. (<bold>B</bold>) Temporal evolution of the hand position (top; the dashed horizontal line indicates the reach target), hand acceleration (middle), and optimal control inputs to the two units (bottom; colors matching panel A), under optimal control given each of the two readout configurations shown in A (left vs. right). The dashed vertical line marks the go cue, and the gray bar indicates the delay period. While the task can be solved successfully in both cases, preparatory inputs are only useful when the sink is read out. (<bold>C</bold>) Network activity trajectories under optimal control. Each trajectory begins at the origin, and the end of the delay period is shown with a black cross. (<bold>D–F</bold>) Same as (<bold>A–C</bold>), for the oscillatory network. (<bold>G–H</bold>) Preparation index (top) and total amount of preparatory inputs (bottom) as a function of the scale parameter <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>w</mml:mi></mml:mstyle></mml:math></inline-formula> of the network connectivity, for various readout configurations (color-coded as shown in the top inset). The nonnormal network (top) prepares more when the readout is aligned to the most controllable mode, while the amount of preparation in the oscillatory network (bottom) is independent of the readout direction. The optimal strategy must balance the benefits from preparatory inputs which allow to exploit the intrinsic network dynamics, with the constraint to remain still. This is more difficult when the network dynamics are strong and pushing activity out of the readout-null subspace, explaining the decrease in preparation index for large values of <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>w</mml:mi></mml:mstyle></mml:math></inline-formula> in the oscillatory network.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-fig4-v1.tif"/></fig><p>We found that optimal control of both dynamical motifs generally led to preparatory dynamics, with inputs arising before the go cue (<xref ref-type="fig" rid="fig4">Figure 4B and E</xref>, bottom). In the feedforward motif, the amount of preparatory inputs appeared to depend critically on the orientation of the readout. When the readout was aligned with the sink (brown) mode (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, left), the controller prepared the network by moving its activity along the source (orange) mode (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, left). This placed the network in a position from which it had a natural propensity to generate large activity transients along the readout dimension (c.f. flow field in <xref ref-type="fig" rid="fig4">Figure 4A</xref>); here, these transients were exploited to drive the fast upstroke in hand acceleration and throw the hand toward the target location. Note that this strategy reduces the amount of input the controller needs to deliver during the movement, because the network itself does most of the work.</p><p>Nevertheless, in this case the network’s own impulse response was not rich enough to accommodate the phase reversal required to subsequently slow the hand down and terminate the movement. Optimal control therefore also involved inputs during the movement epoch, leading to a preparatory index of ∼0.54 (<xref ref-type="fig" rid="fig4">Figure 4G</xref>, dark blue).</p><p>When it was instead the source mode that was read out (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, right), the only dimension along which the system could prepare without moving was the sink mode. Preparing this way is of no benefit, because the flow field along the sink mode has no component along the source (readout) mode.</p><p>Thus, here the optimal strategy was to defer control to the movement epoch, during which the transient growth of network activity along the readout rested entirely on adequate control inputs. This led to a preparation index of ∼0 (<xref ref-type="fig" rid="fig4">Figure 4G</xref>, pale green). Although the network did react with large activity excursions along the sink mode (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, right), these were inconsequential for the movement. Importantly, of the two extreme readout configurations discussed above, the first one yielded a smaller overall optimal control cost (by a factor of ∼1.5). Thus, at a meta-control level, ideal downstream effectors would read out the sink mode, not the source mode. Note that while increasing the connectivity strength initially led to more preparation (<xref ref-type="fig" rid="fig4">Figure 4H</xref>), a plateau was eventually reached for <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>w</mml:mi><mml:mo>≥</mml:mo><mml:mn>4</mml:mn></mml:mstyle></mml:math></inline-formula>. Indeed, while stronger dynamics initially make preparation more beneficial, they also make it more difficult for preparatory activity to remain in the readout nullspace.</p><p>We obtained similar insights for oscillatory network dynamics (<xref ref-type="fig" rid="fig4">Figure 4D–F</xref>). A key difference however was that the flow field was rotationally symmetric such that no distinction could be made between ‘source’ and ‘sink’ units – indeed the optimal control strategy yielded the same results (up to a rotation of the state space) irrespective of which of the two units was driving the hand’s acceleration (compare left and right panels in <xref ref-type="fig" rid="fig4">Figure 4D–F</xref>). Nevertheless, the optimal controller consistently moved the network’s activity along the output-null axis during preparation, in such a way as to engage the network’s own rotational flow immediately after the go cue (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). This rotational flow drove a fast rise and decay of activity in the readout unit, thus providing the initial segment of the required hand acceleration. The hand was subsequently slowed down by modest movement-epoch control inputs which eventually receded, leading to a preparation index of ∼0.58. Interestingly, the preparation index showed a decrease for very large <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>w</mml:mi></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4G</xref>), which did not reflect smaller preparatory inputs (<xref ref-type="fig" rid="fig4">Figure 4H</xref>) but rather reflected the larger inputs that were required during movement to cancel the fast oscillations naturally generated by the network.</p><p>The above results highlight how the optimal control strategy is shaped by the dynamical motifs present in the network. Crucially, we found that the optimal way to control the movement depends not only on the strength and flow of the internal network dynamics, but also on their interactions with the readout.</p></sec><sec id="s2-3"><title>Control-theoretic properties predict the amount of preparation</title><p>Our investigation of preparation in a low-dimensional system allowed us to isolate the impact of core dynamical motifs, and highlighted how preparation depends on the geometry of the flow field, and its alignment to the readout. However, these intuitions remain somewhat qualitative, making them difficult to generalize to our high-dimensional ISN model.</p><p>To quantify the key criteria that appear important for preparation, we turned to tools from control theory. We reasoned that, for a network to be able to benefit from preparation and thus exhibit a large preparation index, there must be some advantage to using early inputs that do not immediately cause movement, relative to using later inputs that do. We hypothesized that this advantage could be broken down into two criteria. First, there must exist activity patterns that are momentarily output-null (i.e. do not immediately cause movement) yet seed output-potent dynamics that subsequently move the arm. The necessity of this criterion was obvious in the 2D nonnormal network, which did not display any preparation when its nullspace was aligned with its ‘sink’ mode. In the language of control theory, this criterion implies that the nullspace of the readout must be sufficiently ‘observable’ – we captured this in a scalar quantity <italic>α</italic> (Methods; <xref ref-type="bibr" rid="bib22">Kao and Hennequin, 2019</xref>; <xref ref-type="bibr" rid="bib46">Skogestad, 2007</xref>). Second, there must be a sizeable cost to performing the movement in an entirely input-driven manner without relying on preparation. In other words, the network should be hard to steer along the readout direction, i.e., the readout must be of limited ‘controllability’ – we captured this in another scalar quantity <italic>β</italic> (Methods).</p><p>We illustrate the meaning of these two metrics in <xref ref-type="fig" rid="fig5">Figure 5A and B</xref> for a 2D example network that combines nonnormality and oscillations. We show two extreme choices of readout direction (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, dashed black): the one that maximizes <italic>α</italic> (top) and the one that minimizes it (bottom). In the first case, the readout nullspace (dashed orange) is very observable, i.e., trajectories that begin in the nullspace evolve to produce large transients along the readout (solid orange and inset). In the second case, the opposite is true. For each case, we also assessed the controllability of the readout (<italic>β</italic>). The controllability of a direction corresponds to how much variance activity trajectories exhibit along that direction, when they are randomly and isotropically initialized (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). In other words, a very controllable direction is one along which network trajectories have a natural tendency to evolve.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Predicting the preparation index from the observability of the output nullspace (<italic>α</italic>) and the controllability of the readout (<italic>β</italic>, see details in text).</title><p>(<bold>A</bold>) Illustration of the observability of the output nullspace in a synthetic two-dimensional system. The observability of a direction is characterized by how much activity (integrated squared norm) is generated along the readout by a unit-norm initial condition aligned with that direction. The top and bottom panels show the choices of readout directions (dotted black) for which the corresponding nullspace (dotted orange) is most (maximum <italic>α</italic>) and least (minimum <italic>α</italic>) observable, respectively. Trajectories initialized along the null direction are shown in solid orange, and their projections onto the readout are shown in the inset. (<bold>B</bold>) Illustration of the controllability of the readout in the same 2D system as in (<bold>A</bold>). To compute controllability, the distribution of activity patterns collected along randomly initialized trajectories is estimated (heatmap); the controllability of a given direction then corresponds to how much variance it captures in this distribution. Here, the network has a natural propensity to generate activity patterns aligned with the dashed white line (‘most controllable’ direction). The readout directions are repeated from panel A (dotted black). The largest (resp. smallest) value of <italic>β</italic> would by definition be obtained when the readout is most (resp. least) controllable. Note the tradeoff in this example: the choice of readout that maximizes <italic>α</italic> (top) does not lead to the smallest <italic>β</italic>. (<bold>C</bold>) The values of <italic>α</italic> and <italic>β</italic> accurately predict the preparation index (<inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.93</mml:mn></mml:mstyle></mml:math></inline-formula>) for a range of high-dimensional inhibition-stabilized networks (ISNs) (maroon dots) with different connectivity strengths and characteristic timescales (Methods). The best fit (after z-scoring) is given by <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>16.94</mml:mn><mml:mo>±</mml:mo><mml:mn>0.02</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>α</mml:mi><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>15.97</mml:mn><mml:mo>±</mml:mo><mml:mn>0.02</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>β</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> (mean ± s.e.m. were evaluated by boostrapping). This confirms our hypothesis that optimal control relies more on preparation when <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> is large and <italic>β</italic> is small. Note that <italic>α</italic> and <italic>β</italic> alone only account for 34.8% and 30.4% of the variance in the preparation index, respectively (inset). Thus, <italic>α</italic> and <italic>β</italic> provide largely complementary information about the networks’ ability to use inputs, and can be combined into a very good predictor of the preparation index. Importantly, even though this fit was obtained <italic>using ISNs only</italic>, it still captures 69% of preparation index variance across networks from other families (blue dots; Methods).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-fig5-v1.tif"/></fig><p>We then assessed how well <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> could predict the preparation index of individual networks. In 2D networks, we found that a simple function that grows with <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> and decreases with <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> could accurately predict preparation across thousands of networks (Appendix 1 - Section 3 ‘Additional results in the 2D system’). To assess whether these insights carried over to high-dimensional networks, we then generated a range of large ISNs with parameterically varied connectivity strengths and decay timescales (Methods). We then regressed the preparation index against the values of <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> computed for each of these networks (as controllability and observability are only defined for linear networks, we set <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> for this investigation). We found that a simple linear mapping, <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>prep.\ index</mml:mtext></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>β</mml:mi></mml:mrow></mml:msub><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>, with parameters fitted to one half of the ISNs, accurately predicted the preparation indices of the other half (<xref ref-type="fig" rid="fig5">Figure 5C</xref>; <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.93</mml:mn></mml:mstyle></mml:math></inline-formula>, fivefold cross-validated). Interestingly, we observed that although <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> (which are both functions of the network connectivity) were highly correlated across different networks, discarding either variable in our linear regression led to a significant drop in <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, inset). Importantly, it was their difference that best predicted the preparation index (<inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>β</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>), consistent with our hypothesis that the preparation index is a relative quantity which increases as the nullspace becomes more observable, but decreases as readout dimensions become more controllable.</p><p>We were able to confirm the generality of this predictive model by generating networks with other types of connectivity (oscillatory networks, and networks with unstructured random weights), which displayed dynamics very different from the ISNs (see <xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6</xref>). Interestingly, despite the different distribution of <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> parameters in those networks, we could still capture a large fraction of the variance in their preparation index (<inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.69</mml:mn></mml:mstyle></mml:math></inline-formula>) using the linear fit obtained from the ISNs alone.</p><p>This confirms that <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> can capture information about the networks’ dynamics in a universal manner.</p><p>Note that we do not make any claims about the specific functional form of the relationship between <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>, and the preparation index. Rather, we claim that there should be a broad trend for the preparation index to increase with <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> and decrease with <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>, and we acknowledge that this relationship could in general be nonlinear. Indeed, in 2D networks, we found that the preparation index was in fact better predicted by the ratio of <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> over <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> than by their difference (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>).</p><p>Finally, as the above results highlight that the amount of preparation depends on the alignment between internal dynamics and readout, one may wonder how much our conclusions depend on our use of a random unstructured readout matrix. First, we note that the effect of the alignment on preparation index is greatly amplified in the low-dimensional networks (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). In high-dimensional networks, the null space of a random readout matrix <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> will have some overlap with the most observable directions of the dynamics, thus encouraging preparation. Second, we performed additional simulations where we meta-optimized the readout so as to minimize the average optimal cost per movement. The resulting system is more observable overall (as it allows the network to solve the task at a lower cost) but relies just as much on preparation (<xref ref-type="fig" rid="app1fig7">Appendix 1—figure 7</xref>).</p></sec><sec id="s2-4"><title>Modeling movement sequences</title><p>Having gained a better understanding of what features lead a network to prepare, we next set out to assess whether optimal control could also explain the neural preparatory processes underlying the generation of movement <italic>sequences</italic>. We revisited the experimental studies of <xref ref-type="bibr" rid="bib57">Zimnik and Churchland, 2021</xref>, where monkeys were trained to perform two consecutive reaches. Each trial started with the display of both targets, followed by an explicitly enforced delay period before the onset of the first reach. A distinction was made between ‘double’ reaches in which a pause was enforced between reaches, and ‘compound’ reaches in which no pause was required. This study concluded that, rather than the whole movement sequence unrolling from a single preparatory period, each reach was instead successively seeded by its own preparatory activity.</p><p>Here, we asked whether such an independent, successive preparation strategy would arise as an optimal control solution, in the same way that single-reach preparation did. Importantly, we could not answer this question by directly examining network inputs as we did for single reaches. Indeed, any network input observed before the second reach could be contributing either to the end of the first movement, or to the preparation of the next. In fact, the issue of teasing apart preparatory vs. movement-related activity patterns also arose in the analysis of the monkey data. To address this, <xref ref-type="bibr" rid="bib57">Zimnik and Churchland, 2021</xref>, exploited the fact that monkey M1 activity just before and during single reaches is segregated into two distinct subspaces. Thus, momentary activity patterns (during either single or double reaches) can be unambiguously labeled as preparatory or movement-related depending on which of the two subspaces they occupied. We performed a similar analysis (Methods) and verified that preparatory and movement activity patterns in the model were also well segregated in their respective subspaces in the single-reach task (<xref ref-type="fig" rid="fig6">Figure 6A and B</xref>). We then assessed the occupancy of the preparatory subspace during double reaching in the model, and took this measure as a signature of preparation.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>The model executes a sequence of two reaches using an independent strategy.</title><p>(<bold>A</bold>) Hand velocity during one of the reaches, with the corresponding hand trajectory shown in the inset. (<bold>B–C</bold>) We identified two six-dimensional orthogonal subspaces, capturing 79% and 85% of total activity variance during single-reach preparation and movement respectively. (<bold>B</bold>) First principal component of the model activity for the eight different reaches projected into the subspaces identified using preparatory (top) and movement-epoch (bottom) activity. (<bold>C</bold>) Occupancy (total variance captured across movements) of the orthogonalized preparatory and movement subspaces, in the model (top) and in monkey motor cortical activity (bottom; reproduced from <xref ref-type="bibr" rid="bib26">Lara et al., 2018</xref>, for monkey Ax). We report mean ± s.e.m., where the error is computed by bootstrapping from the neural population as in <xref ref-type="bibr" rid="bib26">Lara et al., 2018</xref>. We normalize each curve separately to have a maximum mean value of 1. To align the model and monkey temporally, we re-defined the model’s ‘movement onset’ time to be 120 ms after the model’s hand velocity crossed a threshold – this accounts for cortico-spinal delays and muscle inertia in the monkey. Consistent with <xref ref-type="bibr" rid="bib26">Lara et al., 2018</xref>’s monkey primary motor cortex (M1) recordings, preparatory subspace occupancy in the model peaks shortly before movement onset, rapidly dropping thereafter to give way to pronounced occupancy of the movement subspace. Conversely, there is little movement subspace occupancy during preparation. (<bold>D</bold>) Behavioral (top) and neural (middle) correlates of the delayed reach for one example of a double reach with an enforced pause of 0.6 s. The optimal strategy relies on preparatory inputs preceding each movement. (<bold>E</bold>) Same as (<bold>C</bold>), for double reaches. The onsets of the monkey’s two reaches are separately aligned to the model’s using the same convention as in (<bold>C</bold>). The preparatory subspace displays two clear peaks of occupancy. This double occupancy peak is also observed in monkey neural activity (bottom; reproduced from <xref ref-type="bibr" rid="bib57">Zimnik and Churchland, 2021</xref>, with the first occupancy peak aligned to that of the model). (<bold>F</bold>) Same as (<bold>D</bold>), for compound reaches with no enforced pause in-between. Even though the sequence could be viewed as a single long movement, the control strategy relies on two periods of preparation. Here, inputs before the second reach are used to reinject energy into the system after slowing down at the end of the first reach. (<bold>G</bold>) Even though no explicit delay period is enforced in-between reaches during the compound movement, the preparatory occupancy rises twice, before the first reach and once again before the second reach. This is similar to observations in neural data (bottom; reproduced from <xref ref-type="bibr" rid="bib57">Zimnik and Churchland, 2021</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-fig6-v1.tif"/></fig><p>To model optimal control of a double reach, we modified our cost functional to account for the presence of two consecutive targets (see Methods). We considered the same set of eight targets as in our single-reach task, and modeled all possible combinations of two targets (one example shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>). We set the hyper-parameters of the cost function such that both targets could be reached by the resulting optimal controller, in a way that matched important qualitative aspects of the monkeys’ behavior (in particular, such that both reaches were performed at similar velocities, with the second reach lasting slightly longer on average; <xref ref-type="fig" rid="fig6">Figure 6B and C</xref>, top).</p><p>We projected the network activity onto preparatory and movement subspaces identified using single and double reaches activity (Methods). For double reaches with a long (600 ms) pause, the preparatory subspace was transiently occupied twice, with the two peaks occurring just before the onset of each movement in the sequence (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, bottom).</p><p>Notably, the occupancy during the ‘compound’ reach (without pause; <xref ref-type="fig" rid="fig6">Figure 6C</xref>) also started rising prior to the first movement before decaying very slightly and peaking again before the second reach, indicating two independent preparatory events. This is somewhat surprising, given that a movement sequence can also be viewed as a single ‘compound’ movement, for which we have shown previously a unique preparatory phase is sufficient (<xref ref-type="fig" rid="fig2">Figure 2</xref>). In our model, this behavior can be understood to arise from the requirement that the hand stop briefly at the first target. To produce the second reach, the hand needs to accelerate again, which requires transient re-growth of activity in the network. Given that the network’s dynamical repertoire exhibits limited timescales, this is most easily achieved by reinjecting inputs into the system.</p><p>In summary, our results suggest that the ‘independent’ preparation strategy observed in monkeys is consistent with the optimal control of a two-reach sequence. While <xref ref-type="bibr" rid="bib57">Zimnik and Churchland, 2021</xref>, showed that RNNs trained on this task used this ‘independent’ strategy, this was by design as the network was only cued for the second reach after the first one had started. In addition to replicating this proof of concept that it is possible to prepare while moving, our model also shows how and why independent preparation might arise as an optimal control solution.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this work, we proposed a model for the dynamics of motor cortex during a delayed-reaching task in non-human primates. Unlike previous work, we treated M1 as an input-driven nonlinear dynamical system, with generic connectivity not specifically optimized for the task, but with external inputs assumed to be optimal for each reach.</p><p>Motivated by a large body of evidence suggesting that preparation is useful before delayed reaches (<xref ref-type="bibr" rid="bib5">Churchland et al., 2010</xref>; <xref ref-type="bibr" rid="bib26">Lara et al., 2018</xref>; <xref ref-type="bibr" rid="bib1">Afshar et al., 2011</xref>; <xref ref-type="bibr" rid="bib45">Shenoy et al., 2013</xref>), but also evidence for thalamic inputs being necessary for accurate movement execution (<xref ref-type="bibr" rid="bib40">Sauerbrei et al., 2020</xref>), we used this model to investigate whether and why neural circuits might rely on motor preparation during delayed-reaching tasks. Interestingly, preparation arose as an optimal control strategy in our model, with the optimal solution to the task relying strongly on inputs prior to movement onset. Moreover, the benefits of preparation were dependent on the network connectivity, with preparation being more prevalent in networks whose rich internal dynamics can be advantageously seeded by early external inputs. We were able to quantify this intuition with a predictive model relating the dynamical response properties of a network to the amount of preparation it exhibits when controlled optimally.</p><p>Finally, we found that prominent features of the monkeys’ neural activity during sequential reaches arose naturally from optimal control assumptions. Specifically, optimally controlled networks relied on two phases of preparation when executing sequences of two reaches, corroborating recent experimental observations in monkey M1 (<xref ref-type="bibr" rid="bib57">Zimnik and Churchland, 2021</xref>). Together, our results provide a normative explanation for the emergence of preparatory activity in both single and sequential reaching movements.</p><p>In recent years, task-optimized RNNs have become a very popular tool to model neural circuit dynamics. Classically, those models incorporate only those inputs that directly reflect task-related stimuli (e.g. motor target, go cue, etc.). This requires assumptions about the form of the inputs, such as modeling them as simple step functions active during specific task epochs. However, as local neural circuits are part of a wider network of brain areas, a large portion of their inputs come from other brain areas at intermediate stages of the computation and may therefore not be directly tied to task stimuli. Thus, it is not always obvious what assumptions can reasonably be made about the inputs that drive the circuit’s dynamics.</p><p>Our optimization framework, which does not require us to make any specific assumptions about when and how inputs enter the network (although it does allow to incorporate prior information in the form of constraints), allows to bypass this problem and to implicitly model unobserved inputs from other areas. Importantly, our framework allows to ask questions – such as ‘why prepare’ – that are difficult to phrase in standard input-driven RNN models. We note, however, that in the investigation we have presented here, the lack of imposed structure for the inputs also implied that the model could not make use of mechanisms known to contribute certain aspects of preparatory neural activity. For example, our model did not exhibit the usual visually driven response to the target input, nor did it have to use the delay epoch to keep such a transient sensory input in memory (<xref ref-type="bibr" rid="bib14">Guo et al., 2014</xref>; <xref ref-type="bibr" rid="bib28">Li et al., 2015</xref>).</p><p>The main premise of our approach is that one can somehow delineate the dynamical system which M1 implements, and attribute any activity patterns that it cannot autonomously generate to external inputs. Just where the anatomical boundary of ‘M1 dynamics’ lie – and therefore where ‘external inputs’ originate – is unclear, and our results must be interpreted with this limitation in mind. Operationally, previous works in reaching monkeys have shown that M1 data can be mathematically well described by a dynamical system that appears largely autonomous during movement. These works have emphasized that those abstract dynamics, while inferred from M1 data alone, may not be anatomically confined to M1 itself. Instead, they may involve interactions between multiple brain areas, and even possibly parts of the body through delayed sensory feedback. Here, we too tend to think of our M1 models in this way, and therefore attribute external input to brain areas that are one step removed from this potentially broad motor-generating network. Nevertheless, a more detailed multi-area model of the motor-generating circuitry including, e.g., spinal networks (<xref ref-type="bibr" rid="bib36">Prut and Fetz, 1999</xref>) could enable more detailed comparisons to multi-region neural data. In a similar vein, our model makes no distinction between external inputs that drive movement-specific planning computations, and other types of movement-unspecific inputs that might drive the transition from planning to execution (e.g. ‘trigger’ inputs, <xref ref-type="bibr" rid="bib25">Kaufman et al., 2016</xref>). Incorporating such distinctions (e.g. by temporally modulating the cost in individual input channels depending on specific task events, or by having separate channels for movement-unspecific inputs) might allow to ask more targeted questions about the role and provenance of external inputs.</p><p>A major limitation of our study is the specific choice of a quadratic penalty on the external input in our control objective. While there are possible justifications for such a cost (e.g. regularization of the dynamics to promote robustness of the control solution), its use here is mainly motivated by mathematical tractability. Other costs might be conceivably more relevant and might affect our results. For example, studies of motor cortex have long thought of its dynamics as converting relatively simple inputs reflecting high-level, temporally stable plans, into detailed, temporally varying motor commands. Thus, a potentially relevant form of a penalty for external inputs would be their temporal complexity. Such a penalty would have the advantage of encouraging a clearer separation between the inputs and the RNN activations; indeed, in our current model, we find that the optimal controls themselves have a temporal structure, part of which could be generated by a dynamical system and thus potentially absorbed into our ‘M1 dynamics’. To address this, we note that our optimization framework can be adjusted to penalize the magnitude of the temporal <italic>derivative</italic> of the external input <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, instead of <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. We experimented with this extension and found qualitatively different optimal inputs and M1 firing rates, which evolved more slowly and plateaued for sufficiently long preparation (<xref ref-type="fig" rid="app1fig8">Appendix 1—figure 8A-D</xref>) – this is in fact more consistent with monkey M1 data (e.g. <xref ref-type="bibr" rid="bib12">Elsayed et al., 2016</xref>). Despite these qualitative difference in the specific form of preparation, our main conclusion stands that input-driven preparation continues to arise as an optimal solution (<xref ref-type="fig" rid="app1fig8">Appendix 1—figure 8E-F</xref>).</p><p>Another important assumption we have made is that the optimal controller is aware of the duration of the delay period. While this made solving for the optimal control inputs easier, it made our task more akin to a self-initiated reach (<xref ref-type="bibr" rid="bib26">Lara et al., 2018</xref>) than to a typical delayed reach with unpredictable, stochastic delay durations. Future work could revisit this assumption. As a first step toward this, we now briefly outline pilot experiments in this direction. We used an exponential distribution of delays (with mean 300 ms) and devised two modified versions of our model that dealt with the resulting uncertainty in two different ways. In the first strategy, at any time during preparation, the model would estimate the most probable time-to-go-cue given that it hadn’t arrived yet (in this case, this is always 300 ms in the future) and would plan an optimal sequence of inputs accordingly. In the second strategy, the network would prudently assume the earliest possible go cue (i.e. the next time step) and plan accordingly. In both cases, only the first input in the optimal input sequence would be used at each step, and complete replanning would follow in the next step, as the model re-assesses the situation given new information (i.e. whether the actual go cue arrived or not; this is a form of ‘model predictive control’, <xref ref-type="bibr" rid="bib37">Rawlings et al., 2017</xref>). Preparatory inputs arose in both settings, but we found that only the latter strategy led to activity patterns that plateaued early during preparation (see <xref ref-type="fig" rid="app1fig9">Appendix 1—figure 9</xref>).</p><p>Throughout the main text, we have referred to <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as the task-enforced delay period. However, a more accurate description may be that it corresponds to a delay period determined by an internally set go signal, which can lag behind the external go cue. While we would not expect a large difference between those two signals, the way in which we define <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> becomes important as it approaches 0 ms (limit of a quasi-automatic reach; <xref ref-type="bibr" rid="bib26">Lara et al., 2018</xref>). Indeed, in this limit, our model exhibits almost no activity in the preparatory subspace (as defined in <xref ref-type="fig" rid="fig6">Figure 6</xref> – see further analyses in <xref ref-type="fig" rid="app1fig10">Appendix 1—figure 10</xref>). In contrast, monkey M1 activity was found to transiently occupy the preparatory subspace even in this case (<xref ref-type="bibr" rid="bib26">Lara et al., 2018</xref>). Evidence for a delay between the earliest possible response to sensory cues and the trigger of movement was also observed in <xref ref-type="bibr" rid="bib25">Kaufman et al., 2016</xref>, as well as in human behavioral studies (<xref ref-type="bibr" rid="bib15">Haith et al., 2016</xref>). Thus, one may wish to explicitly incorporate this additional delay in the model in order to make it more realistic. Note however that <xref ref-type="bibr" rid="bib15">Haith et al., 2016</xref>, showed that this internal delay could be shortened without affecting movement accuracy, suggesting that part of the processing that empirically occurs in-between the internal and external go cues may not be necessary, but rather reflect a decoupling between the end of preparation and the trigger of movement. This may be important to consider when attempting to compare the model to, e.g., reaction times from behavioral experiments.</p><p>Dynamical systems have a longstanding history as models of neural populations (<xref ref-type="bibr" rid="bib8">Dayan and Abbott, 2001</xref>). However, understanding how neural circuits can perform various computations remains a challenging question.</p><p>Recently, there has been increased interest in trying to understand the role of inputs in shaping cortical dynamics. This question has been approached both from a data-driven perspective (<xref ref-type="bibr" rid="bib30">Malonis et al., 2021</xref>; <xref ref-type="bibr" rid="bib48">Soldado-Magraner et al., 2023</xref>) and in modeling work with, e.g., <xref ref-type="bibr" rid="bib9">Driscoll et al., 2022</xref>, showing how a single network can perform different tasks by reorganizing its dynamics under the effect of external inputs and <xref ref-type="bibr" rid="bib10">Dubreuil et al., 2021</xref>, relating network structure to the ability to process contextual inputs. To better understand how our motor system can generate flexible behaviors (<xref ref-type="bibr" rid="bib29">Logiaco et al., 2021</xref>; <xref ref-type="bibr" rid="bib50">Stroud et al., 2018</xref>), and to characterize learning on short timescales (<xref ref-type="bibr" rid="bib47">Sohn et al., 2021</xref>; <xref ref-type="bibr" rid="bib17">Heald et al., 2023</xref>), it is important to study how network dynamics can be modulated by external signals that allow rapid adaptation to new contexts without requiring extensive modifications of the network’s connectivity. The optimal control approach we proposed here offers a way to systematically perform such evaluations, in a variety of tasks and under different assumptions regarding how inputs are allowed to impact the dynamics of the local circuit of interest. While our model’s predictions will depend on, e.g., the choice of connectivity or the design of the cost function, an exciting direction for future work will be to obtain those parameters in a data-driven manner, for instance using recently developed methods to infer dynamics from data (<xref ref-type="bibr" rid="bib35">Pandarinath et al., 2018</xref>; <xref ref-type="bibr" rid="bib42">Schimel et al., 2022</xref>), and advances in inverse reinforcement learning and differentiable control (<xref ref-type="bibr" rid="bib2">Amos et al., 2018</xref>) to infer the cost function that behavior optimizes. These could additionally be combined with more biomechanically realistic effectors, such as the differentiable arm models from <xref ref-type="bibr" rid="bib7">Codol et al., 2023</xref>.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Experimental model and subject details</title><p>In <xref ref-type="fig" rid="fig1">Figure 1</xref>, we showed data from two primate datasets that were made available to us by Mark Churchland, Matthew Kaufman, and Krishna Shenoy. Details of animal care, surgery, electrophysiological recordings, and behavioral task have been reported previously in <xref ref-type="bibr" rid="bib6">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib24">Kaufman et al., 2014</xref> (see in particular the details associated with the J and N ‘array’ datasets). The subjects of this study, J and N, were two adult male macaque monkeys (<italic>Macaca mulatta</italic>). The animal protocols were approved by the Stanford University Institutional Animal Care and Use Committee. Both monkeys were trained to perform a delayed-reaching task on a fronto-parallel screen. At the beginning of each trial, they fixated on the center of the screen for some time, after which a target appeared on the screen. After a variable delay period (0–1000 ms), a go cue appeared instructing the monkeys to reach toward the target. Recordings were made in the PMd cortex and in the M1 using a pair of implanted 96-electrode arrays. In <xref ref-type="fig" rid="fig6">Figure 6</xref>, we also reproduced data from <xref ref-type="bibr" rid="bib26">Lara et al., 2018</xref>, and <xref ref-type="bibr" rid="bib57">Zimnik and Churchland, 2021</xref>. Details of animal care, surgery, electrophysiological recordings, and behavioral task for those data can be found in the Methods section of the respective papers.</p></sec><sec id="s4-2"><title>Arm model</title><p>To simulate reaching movements, we used the planar two-link arm model described in <xref ref-type="bibr" rid="bib27">Li and Todorov, 2004</xref>. The two links have lengths <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, masses <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, and moments of inertia <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, respectively. The lower arm’s center of mass is located a distance <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> from the elbow. By considering the geometry of the upper and lower limb, the position of the hand and elbow can be written as vectors <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mtext>h</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mtext>e</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> given by<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mtext>h</mml:mtext></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnalign="center center" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mtext> and </mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mtext>e</mml:mtext></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnalign="center center" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The joint angles <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> evolve dynamically according to the differential equation<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">M</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>¨</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">X</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">B</mml:mi></mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the momentary torque vector, <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the matrix of inertia, <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> accounts for the centripetal and Coriolis forces, and <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a damping matrix representing joint friction. These parameters are given by<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi class="mathcal" mathvariant="script">M</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnalign="center center" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mspace width="1em"/></mml:mtd><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mspace width="1em"/></mml:mtd><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi class="mathcal" mathvariant="script">X</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnalign="center center" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mover><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi class="mathcal" mathvariant="script">B</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnalign="center center" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>0.05</mml:mn><mml:mspace width="1em"/></mml:mtd><mml:mtd><mml:mn>0.025</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0.025</mml:mn><mml:mspace width="1em"/></mml:mtd><mml:mtd><mml:mn>0.05</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-3"><title>iLQR algorithm</title><p>Throughout this work, we used the iLQR algorithm (<xref ref-type="bibr" rid="bib27">Li and Todorov, 2004</xref>) to find the locally optimal inputs that minimize our cost function. iLQR is a trajectory optimization algorithm that can handle nonlinear dynamics and non-quadratic costs. iLQR works in an iterative manner, by linearizing the dynamics and performing a quadratic approximation of the cost at each iteration, thus turning the control problem into a local linear quadratic problem whose unique solution is found using LQR (<xref ref-type="bibr" rid="bib21">Kalman, 1960</xref>). The LQR solver uses a highly efficient dynamic programming approach that exploits the sequential structure of the problem. Our <ext-link ext-link-type="uri" xlink:href="https://github.com/tachukao/dilqr">implementation of iLQR</ext-link> (<xref ref-type="bibr" rid="bib41">Schimel et al., 2021</xref>) followed from <xref ref-type="bibr" rid="bib27">Li and Todorov, 2004</xref>, with the difference that we performed regularization of the local curvature matrix as recommended by <xref ref-type="bibr" rid="bib53">Tassa, 2011</xref>.</p></sec><sec id="s4-4"><title>Generation of the high-dimensional readouts and networks</title><sec id="s4-4-1"><title>Generation of inhibitory-stabilized networks</title><p>Simulations in <xref ref-type="fig" rid="fig1">Figures 1</xref>, <xref ref-type="fig" rid="fig3">3</xref>, <xref ref-type="fig" rid="fig5">5</xref>, and <xref ref-type="fig" rid="fig6">6</xref> were conducted using ISNs. Those were generated according to the procedure described in <xref ref-type="bibr" rid="bib19">Hennequin et al., 2014</xref>, with minor adjustments. In brief, we initialized strongly connected chaotic networks with sparse and log-normally distributed excitatory weights, and stabilized them through progressive <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">H</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>-optimal adjustments of the inhibitory weights until the spectral abscissa of the connectivity matrix fell below 0.8. This yielded strongly connected but stable networks with a strong degree of nonnormality. When considering a larger range of ISNs (<xref ref-type="fig" rid="fig5">Figure 5</xref>), we independently varied both the variance of the distribution of initial excitatory weights and the spectral abscissa below which we stopped optimizing the inhibitory weights.</p></sec><sec id="s4-4-2"><title>Generation of additional networks in <xref ref-type="fig" rid="fig5">Figure 5</xref></title><p>To assess the generality of our findings in <xref ref-type="fig" rid="fig5">Figure 5</xref>, we additionally generated randomly connected networks by sampling each weight from a Gaussian distribution with <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mstyle></mml:math></inline-formula>, where the spectral radius <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi></mml:mstyle></mml:math></inline-formula> was varied between 0 and 0.99. We also sampled skew-symmetric networks by drawing a random network <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> as above, and setting <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. We varied the radius <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi></mml:mstyle></mml:math></inline-formula> of the <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> matrices between 0 and 5. Moreover, we considered diagonally shifted skew-symmetric networks <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi></mml:mstyle></mml:math></inline-formula> denotes the real part of all the eigenvalues and was varied between 0 and 0.8.</p><p>The elements of the readout matrix <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> mapping neural activity onto torques were drawn from a normal distribution with zero mean and standard deviation <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mstyle></mml:math></inline-formula>. This was chosen to ensure that firing rates of standard deviation on the order of 30 Hz would be decoded into torques of standard deviation ∼2 N/m, which is the natural variation required for the production of the reaches we considered.</p></sec></sec><sec id="s4-5"><title>Details of <xref ref-type="fig" rid="fig4">Figure 4</xref></title><p>To more easily dissect the phenomena leading to the presence or absence of preparation, we turned to 2D linear networks in <xref ref-type="fig" rid="fig4">Figure 4</xref>. We modeled nonnormal networks with a connectivity <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>w</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and oscillatory networks with connectivity <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mi>w</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>w</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The activity of the two units evolved as<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo mathvariant="bold">˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and directly influenced the acceleration of a 1D output <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> according to<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>¨</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> was a row matrix reading the activity of the network along an angle <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> from the horizontal (first unit). Our setup aimed to mirror the reaching task studied in this work. We thus optimized inputs to minimize the following cost function:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mtable columnalign="left left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=:</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>⋆</mml:mo></mml:msup><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:mfrac></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mtext>target</mml:mtext></mml:msub></mml:mrow></mml:munder></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mtext>null</mml:mtext></mml:msub><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>¨</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:mfrac></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mtext>null</mml:mtext></mml:msub></mml:mrow></mml:munder></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mtext>effort</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mtext>prep</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mtext>effort</mml:mtext></mml:msub></mml:mrow></mml:munder><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mstyle></mml:math></inline-formula> was the target position.</p></sec><sec id="s4-6"><title>Computing networks’ controllability and observability to predict preparation in <xref ref-type="fig" rid="fig5">Figure 5</xref></title><p>As part of our attempt to predict how much a network will prepare given its intrinsic properties only, we computed the prospective potency of the nullspace α, and the controllability of the readout β. For a stable linear dynamical system described by<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>the system’s observability Gramian <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> can be computed as the unique positive-definite solution of the Lyapunov equation<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The prospective potency of the nullspace <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊥</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is then defined as<disp-formula id="equ17">.<label>(17)</label><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mo>≜</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi mathvariant="normal">⊥</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:msup><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊥</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Note that this measure <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> is invariant to the specific choice of basis for the nullspace <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊥</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Similarly, to assess the controllability of the readout, we first computed the controllability Gramian of the system <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is the solution of<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in our system. We then defined the controllability of the readout as<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi><mml:mo>≜</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-7"><title>Details of <xref ref-type="fig" rid="fig6">Figure 6</xref></title><sec id="s4-7-1"><title>Cost function</title><p>We modeled sequences of reaches by modifying our cost functional to account for the presence of two targets, as<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>move</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mn>1</mml:mn><mml:mo>⋆</mml:mo></mml:msubsup><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mtext>target</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:munder></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mtext>pause</mml:mtext></mml:mrow></mml:msub><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>move</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>move</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mtext>pause</mml:mtext></mml:msub></mml:mrow></mml:munder></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>move</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mn>2</mml:mn><mml:mo>⋆</mml:mo></mml:msubsup><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>move</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mtext>target</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:munder></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo mathvariant="bold">˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mtext>effort</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mtext>prep</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mtext>effort</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi></mml:mstyle></mml:math></inline-formula> describes how long the monkey’s hands had to stay on the intermediate target before performing its second reach. We used <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>600</mml:mn><mml:mspace width="thinmathspace"/><mml:mtext>ms</mml:mtext></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>pause</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mstyle></mml:math></inline-formula> for the double reaches in which a pause was explicitly enforced during the experiment. For compound reaches, the experiment did not require monkeys to stop for any specific duration. However, to ensure that the hand stopped on the target in the model (as it does in experiments when monkeys touch the screen) rather than fly through it, we set <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn><mml:mspace width="thinmathspace"/><mml:mtext>ms</mml:mtext></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>pause</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mstyle></mml:math></inline-formula> when modeling compound reaches.</p></sec><sec id="s4-7-2"><title>Preparatory subspace analysis</title><p><xref ref-type="bibr" rid="bib26">Lara et al., 2018</xref>, proposed an analysis to identify preparatory and movement-related subspaces. This analysis allows to assess when the neural activity enters those subspaces, independently of whether it is delay-period or post-go-cue activity.</p><p>The method identifies a set of preparatory dimensions and a set of movement dimensions, constrained to be orthogonal to one another, as in <xref ref-type="bibr" rid="bib12">Elsayed et al., 2016</xref>. These are found in the following manner: the trial-averaged neural activity is split between preparatory and movement-related epochs, yielding two matrices of size <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi></mml:mstyle></mml:math></inline-formula> is the number of neurons, <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula> is the number of time bins, and <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> is the number of reaches. One then optimizes the <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>prep</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>prep</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>mov</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>mov</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (where <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>prep</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>mov</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are the predefined dimensions of the two subspaces) such that the subspaces respectively capture most variance in the preparatory and movement activities, while being orthogonal to one another. This is achieved by maximizing the following objective:<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi class="mathcal" mathvariant="script">C</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext>mov</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mtext>prep</mml:mtext></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mtext>prep</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>mov</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mtext>mov</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext>mov</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mtext>mov</mml:mtext></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mtext>mov</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>C</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>prep</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>mov</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are the covariance matrices of the neural activity during the preparatory and movement epochs, respectively. The normalizing constant <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> denotes the maximum amount of variance in preparatory activity that can be captured by any subspace of dimension <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> (this is found via SVD), and similarly for <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>mov</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>mov</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. The objective is maximized under the constraints <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>prep</mml:mtext></mml:mrow></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>mov</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>prep</mml:mtext></mml:mrow></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>prep</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>I</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>mov</mml:mtext></mml:mrow></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>mov</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>I</mml:mi></mml:mstyle></mml:math></inline-formula>. We set subspace dimensions <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>prep</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>mov</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mstyle></mml:math></inline-formula>, although our results were robust to this choice.</p><p>The occupancy of the preparatory subspace was defined as<disp-formula id="equ23"><mml:math id="m23"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mtext>occupancy</mml:mtext><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mtext>var</mml:mtext><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and that of the movement subspace was defined as<disp-formula id="equ24"><mml:math id="m24"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mtext>occupancy</mml:mtext><mml:mrow><mml:mrow><mml:mtext>mov</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mtext>mov</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mtext>var</mml:mtext><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>mov</mml:mtext></mml:mrow></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For single reaches, we defined preparatory epoch responses as the activity in a 300 ms window before the end of the delay period, and movement-epoch responses as the activity in a 300 ms window starting 50 ms after the go cue. We normalized all neural activity traces using the same procedure as <xref ref-type="bibr" rid="bib6">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib12">Elsayed et al., 2016</xref>. For double reaches, we followed <xref ref-type="bibr" rid="bib57">Zimnik and Churchland, 2021</xref>, and used neural activity traces from both single reaches and the first reach of double-reach sequences. Note that we did not include any activity from the second reaches in the sequence, or from compound reaches, when defining the subspaces.</p></sec></sec><sec id="s4-8"><title>Parameter table</title><p>Parameters used for the various simulations.</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Symbol</th><th align="left" valign="bottom"><xref ref-type="fig" rid="fig1">Figure 1</xref></th><th align="left" valign="bottom"><xref ref-type="fig" rid="fig2">Figure 2</xref></th><th align="left" valign="bottom"><xref ref-type="fig" rid="fig3">Figure 3</xref></th><th align="left" valign="bottom"><xref ref-type="fig" rid="fig5">Figure 5</xref></th><th align="left" valign="bottom"><xref ref-type="fig" rid="fig4">Figure 4</xref></th><th align="left" valign="bottom"><xref ref-type="fig" rid="fig6">Figure 6</xref></th><th align="left" valign="bottom">Unit</th><th align="left" valign="bottom">Description</th></tr></thead><tbody><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">30</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">30</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">Length of the <break/>upper arm <break/>in model</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">30</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">30</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">Length of the <break/>forearm in model</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">0.025</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">0.025</td><td align="left" valign="bottom">kg/m<sup>–2</sup></td><td align="left" valign="bottom">Inertia of <break/>upper arm</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">0.045</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">0.045</td><td align="left" valign="bottom">kg/m<sup>–2</sup></td><td align="left" valign="bottom">Inertia of forearm</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">1.4</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">1.4</td><td align="left" valign="bottom">kg</td><td align="left" valign="bottom">Mass of <break/>upper arm</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">1.0</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom">kg</td><td align="left" valign="bottom">Mass of forearm</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">16</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">16</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">Elbow to lower <break/>arm center of <break/>mass distance</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">12</td><td align="left" valign="bottom">20</td><td align="left" valign="bottom">12</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">Radius of the <break/>target reach</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">20</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">mV</td><td align="left" valign="bottom">Mean baseline <break/>firing rate</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">5</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">mV</td><td align="left" valign="bottom">s.t.d of the <break/>baseline firing <break/>rate</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>effort</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">5E-7</td><td align="left" valign="bottom">1E-5</td><td align="left" valign="bottom">5E-7</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">Coeff. of <break/>input cost</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>null</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">1</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">Coeff. of cost <break/>of moving <break/>during the delay</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>pause</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="5">-</td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">Coeff. of cost<break/>of moving <break/>between <break/>reaches</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="6">150</td><td align="left" valign="bottom">ms</td><td align="left" valign="bottom">Single-neuron <break/>time constant</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>move</mml:mtext></mml:mrow></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="5">–</td><td align="left" valign="bottom">300</td><td align="left" valign="bottom">ms</td><td align="left" valign="bottom">Duration of the <break/>first reach</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>prep</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">500</td><td align="char" char="." valign="bottom">300</td><td align="left" valign="bottom">-</td><td align="char" char="." valign="bottom" colspan="2">300</td><td align="left" valign="bottom">500</td><td align="left" valign="bottom">ms</td><td align="left" valign="bottom">Delay period time</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">1100</td><td align="char" char="." valign="bottom">900</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom" colspan="2">900</td><td align="left" valign="bottom">2000—1406</td><td align="left" valign="bottom">ms</td><td align="left" valign="bottom">Total movement <break/>duration</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">200</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">200</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">Number of <break/>neurons</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>con</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">0.2</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">Connection <break/>probability <break/>(E neurons)</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>E</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">80</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">80</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">Percentage of <break/>E neurons</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>I</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="4">20</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">20</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">Percentage of<break/>I neurons</td></tr></tbody></table></table-wrap></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Visualization, Writing - original draft, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-89131-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. Modelling code and the code used to generate figures and analyses is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/marineschimel/why-prep-2">https://github.com/marineschimel/why-prep-2</ext-link> (copy archived at <xref ref-type="bibr" rid="bib43">Schimel, 2024</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>We are grateful to Matthew T Kaufman and Mark M Churchland for sharing data for the monkey experiments. We thank Kristopher Jensen, David Liu, Javier Antorán, and Rory Byrne for helpful comments on the manuscript. MS was funded by an EPSRC DTP studentship, and part of this work was performed using resources provided by the Cambridge Tier-2 system operated by the University of Cambridge Research Computing Service (<ext-link ext-link-type="uri" xlink:href="http://www.hpc.cam.ac.uk">http://www.hpc.cam.ac.uk</ext-link>) funded by EPSRC Tier-2 capital grant EP/P020259/1. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising from this submission.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Afshar</surname><given-names>A</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Single-trial neural correlates of arm movement preparation</article-title><source>Neuron</source><volume>71</volume><fpage>555</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.05.047</pub-id><pub-id pub-id-type="pmid">21835350</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Amos</surname><given-names>B</given-names></name><name><surname>Jimenez</surname><given-names>I</given-names></name><name><surname>Sacks</surname><given-names>J</given-names></name><name><surname>Boots</surname><given-names>B</given-names></name><name><surname>Kolter</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Differentiable Mpc for End-to-End Planning and Control</article-title><conf-name>Advances in Neural Information Processing Systems 31 (NeurIPS 2018)</conf-name><fpage>8289</fpage><lpage>8300</lpage></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title><source>Journal of Computational Neuroscience</source><volume>8</volume><fpage>183</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1023/a:1008925309027</pub-id><pub-id pub-id-type="pmid">10809012</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Delay of movement caused by disruption of cortical preparatory activity</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>348</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1152/jn.00808.2006</pub-id><pub-id pub-id-type="pmid">17005608</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cortical preparatory activity: representation of movement or first cog in a dynamical machine?</article-title><source>Neuron</source><volume>68</volume><fpage>387</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.09.015</pub-id><pub-id pub-id-type="pmid">21040842</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Foster</surname><given-names>JD</given-names></name><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural population dynamics during reaching</article-title><source>Nature</source><volume>487</volume><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/nature11129</pub-id><pub-id pub-id-type="pmid">22722855</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Codol</surname><given-names>O</given-names></name><name><surname>Michaels</surname><given-names>JA</given-names></name><name><surname>Kashefi</surname><given-names>M</given-names></name><name><surname>Pruszynski</surname><given-names>JA</given-names></name><name><surname>Gribble</surname><given-names>PL</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>MotorNet: A python toolbox for controlling differentiable biomechanical effectors with artificial neural networks</article-title><source>eLife</source><volume>30</volume><elocation-id>RP88591</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.88591.1</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Theoretical Neuroscience</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Driscoll</surname><given-names>L</given-names></name><name><surname>Shenoy</surname><given-names>K</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Flexible multitask computation in recurrent networks utilizes shared dynamical motifs</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.08.15.503870</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dubreuil</surname><given-names>A</given-names></name><name><surname>Valente</surname><given-names>A</given-names></name><name><surname>Beiran</surname><given-names>M</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The role of population structure in computations through neural dynamics</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.07.03.185942</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dum</surname><given-names>RP</given-names></name><name><surname>Strick</surname><given-names>PL</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>The origin of corticospinal projections from the premotor areas in the frontal lobe</article-title><source>The Journal of Neuroscience</source><volume>11</volume><fpage>667</fpage><lpage>689</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.11-03-00667.1991</pub-id><pub-id pub-id-type="pmid">1705965</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elsayed</surname><given-names>GF</given-names></name><name><surname>Lara</surname><given-names>AH</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reorganization between preparatory and movement population responses in motor cortex</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13239</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13239</pub-id><pub-id pub-id-type="pmid">27807345</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Memory without feedback in a neural network</article-title><source>Neuron</source><volume>61</volume><fpage>621</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.12.012</pub-id><pub-id pub-id-type="pmid">19249281</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>ZV</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Ophir</surname><given-names>E</given-names></name><name><surname>Gutnisky</surname><given-names>D</given-names></name><name><surname>Ting</surname><given-names>JT</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Flow of cortical activity underlying a tactile decision in mice</article-title><source>Neuron</source><volume>81</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.020</pub-id><pub-id pub-id-type="pmid">24361077</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haith</surname><given-names>AM</given-names></name><name><surname>Pakpoor</surname><given-names>J</given-names></name><name><surname>Krakauer</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Independence of movement preparation and movement initiation</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>3007</fpage><lpage>3015</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3245-15.2016</pub-id><pub-id pub-id-type="pmid">26961954</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CM</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Signal-dependent noise determines motor planning</article-title><source>Nature</source><volume>394</volume><fpage>780</fpage><lpage>784</lpage><pub-id pub-id-type="doi">10.1038/29528</pub-id><pub-id pub-id-type="pmid">9723616</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heald</surname><given-names>JB</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The computational and neural bases of context-dependent learning</article-title><source>Annual Review of Neuroscience</source><volume>46</volume><fpage>233</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-092322-100402</pub-id><pub-id pub-id-type="pmid">36972611</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Non-normal amplification in random balanced neuronal networks</article-title><source>Physical Review E</source><volume>86</volume><elocation-id>011909</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.86.011909</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimal control of transient dynamics in balanced networks supports generation of complex movements</article-title><source>Neuron</source><volume>82</volume><fpage>1394</fpage><lpage>1406</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.045</pub-id><pub-id pub-id-type="pmid">24945778</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalidindi</surname><given-names>HT</given-names></name><name><surname>Cross</surname><given-names>KP</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Omrani</surname><given-names>M</given-names></name><name><surname>Falotico</surname><given-names>E</given-names></name><name><surname>Sabes</surname><given-names>PN</given-names></name><name><surname>Scott</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Rotational dynamics in motor cortex are consistent with a feedback controller</article-title><source>eLife</source><volume>10</volume><elocation-id>e67256</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67256</pub-id><pub-id pub-id-type="pmid">34730516</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalman</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>Contributions to the theory of optimal control</article-title><source>Boletín de La Sociedad Matemática Mexicana</source><volume>5</volume><fpage>102</fpage><lpage>119</lpage></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kao</surname><given-names>TC</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neuroscience out of control: control-theoretic perspectives on neural circuit dynamics</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>122</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.09.001</pub-id><pub-id pub-id-type="pmid">31563084</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kao</surname><given-names>TC</given-names></name><name><surname>Sadabadi</surname><given-names>MS</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Optimal anticipatory control as A theory of motor preparation: A thalamo-cortical circuit model</article-title><source>Neuron</source><volume>109</volume><fpage>1567</fpage><lpage>1581</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.03.009</pub-id><pub-id pub-id-type="pmid">33789082</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cortical activity in the null space: permitting preparation without movement</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>440</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1038/nn.3643</pub-id><pub-id pub-id-type="pmid">24487233</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Seely</surname><given-names>JS</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The largest response component in the motor cortex reflects movement timing but not movement type</article-title><source>eNeuro</source><volume>3</volume><elocation-id>ENEURO.0085-16.2016</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0085-16.2016</pub-id><pub-id pub-id-type="pmid">27761519</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lara</surname><given-names>AH</given-names></name><name><surname>Elsayed</surname><given-names>GF</given-names></name><name><surname>Zimnik</surname><given-names>AJ</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Conservation of preparatory neural events in monkey motor cortex regardless of how movement is initiated</article-title><source>eLife</source><volume>7</volume><elocation-id>e31826</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31826</pub-id><pub-id pub-id-type="pmid">30132759</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Todorov</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Iterative linear quadratic regulator design for nonlinear biological movement systems</article-title><conf-name>ICINCO 2004, Proceedings of the First International Conference on Informatics in Control, Automation and Robotics</conf-name><conf-loc>Setúbal, Portugal</conf-loc><fpage>222</fpage><lpage>229</lpage></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Chen</surname><given-names>T-W</given-names></name><name><surname>Guo</surname><given-names>ZV</given-names></name><name><surname>Gerfen</surname><given-names>CR</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A motor cortex circuit for motor planning and movement</article-title><source>Nature</source><volume>519</volume><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/nature14178</pub-id><pub-id pub-id-type="pmid">25731172</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logiaco</surname><given-names>L</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Escola</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Thalamic control of cortical dynamics in a model of flexible motor sequencing</article-title><source>Cell Reports</source><volume>35</volume><elocation-id>109090</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.109090</pub-id><pub-id pub-id-type="pmid">34077721</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Malonis</surname><given-names>PJ</given-names></name><name><surname>Hatsopoulos</surname><given-names>NG</given-names></name><name><surname>MacLean</surname><given-names>JN</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>M1 Dynamics Share Similar Inputs for Initiating and Correcting Movement</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.10.18.464704</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meirhaeghe</surname><given-names>N</given-names></name><name><surname>Riehle</surname><given-names>A</given-names></name><name><surname>Brochier</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Parallel movement planning is achieved via an optimal preparatory state in motor cortex</article-title><source>Cell Reports</source><volume>42</volume><elocation-id>112136</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2023.112136</pub-id><pub-id pub-id-type="pmid">36807145</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michaels</surname><given-names>JA</given-names></name><name><surname>Dann</surname><given-names>B</given-names></name><name><surname>Intveld</surname><given-names>RW</given-names></name><name><surname>Scherberger</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Predicting reaction time from the neural state space of the premotor and parietal grasping network</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>11415</fpage><lpage>11432</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1714-15.2015</pub-id><pub-id pub-id-type="pmid">26269647</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miri</surname><given-names>A</given-names></name><name><surname>Warriner</surname><given-names>CL</given-names></name><name><surname>Seely</surname><given-names>JS</given-names></name><name><surname>Elsayed</surname><given-names>GF</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Jessell</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Behaviorally selective engagement of short-latency effector pathways by motor cortex</article-title><source>Neuron</source><volume>95</volume><fpage>683</fpage><lpage>696</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.042</pub-id><pub-id pub-id-type="pmid">28735748</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>BK</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Balanced amplification: a new mechanism of selective amplification of neural activity patterns</article-title><source>Neuron</source><volume>61</volume><fpage>635</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.005</pub-id><pub-id pub-id-type="pmid">19249282</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandarinath</surname><given-names>C</given-names></name><name><surname>O’Shea</surname><given-names>DJ</given-names></name><name><surname>Collins</surname><given-names>J</given-names></name><name><surname>Jozefowicz</surname><given-names>R</given-names></name><name><surname>Stavisky</surname><given-names>SD</given-names></name><name><surname>Kao</surname><given-names>JC</given-names></name><name><surname>Trautmann</surname><given-names>EM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title><source>Nature Methods</source><volume>15</volume><fpage>805</fpage><lpage>815</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0109-9</pub-id><pub-id pub-id-type="pmid">30224673</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prut</surname><given-names>Y</given-names></name><name><surname>Fetz</surname><given-names>EE</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Primate spinal interneurons show pre-movement instructed delay activity</article-title><source>Nature</source><volume>401</volume><fpage>590</fpage><lpage>594</lpage><pub-id pub-id-type="doi">10.1038/44145</pub-id><pub-id pub-id-type="pmid">10524626</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rawlings</surname><given-names>JB</given-names></name><name><surname>Mayne</surname><given-names>DQ</given-names></name><name><surname>Diehl</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Model Predictive Control: Theory, Computation, and Design</source><publisher-loc>Madison, WI</publisher-loc><publisher-name>Nob Hill Publishing</publisher-name></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riehle</surname><given-names>A</given-names></name><name><surname>Requin</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Monkey primary motor and premotor cortex: single-cell activity related to prior information about direction and extent of an intended movement</article-title><source>Journal of Neurophysiology</source><volume>61</volume><fpage>534</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.1152/jn.1989.61.3.534</pub-id><pub-id pub-id-type="pmid">2709098</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russo</surname><given-names>AA</given-names></name><name><surname>Bittner</surname><given-names>SR</given-names></name><name><surname>Perkins</surname><given-names>SM</given-names></name><name><surname>Seely</surname><given-names>JS</given-names></name><name><surname>London</surname><given-names>BM</given-names></name><name><surname>Lara</surname><given-names>AH</given-names></name><name><surname>Miri</surname><given-names>A</given-names></name><name><surname>Marshall</surname><given-names>NJ</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Jessell</surname><given-names>TM</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Motor cortex embeds muscle-like commands in an untangled population response</article-title><source>Neuron</source><volume>97</volume><fpage>953</fpage><lpage>966</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.004</pub-id><pub-id pub-id-type="pmid">29398358</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sauerbrei</surname><given-names>BA</given-names></name><name><surname>Guo</surname><given-names>J-Z</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Mischiati</surname><given-names>M</given-names></name><name><surname>Guo</surname><given-names>W</given-names></name><name><surname>Kabra</surname><given-names>M</given-names></name><name><surname>Verma</surname><given-names>N</given-names></name><name><surname>Mensh</surname><given-names>B</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name><name><surname>Hantman</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical pattern generation during dexterous movement is input-driven</article-title><source>Nature</source><volume>577</volume><fpage>386</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1869-9</pub-id><pub-id pub-id-type="pmid">31875851</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Schimel</surname><given-names>M</given-names></name><name><surname>Kao</surname><given-names>TC</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Differentiable ilqr</data-title><version designator="f42d178">f42d178</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/tachukao/dilqr">https://github.com/tachukao/dilqr</ext-link></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schimel</surname><given-names>M</given-names></name><name><surname>Kao</surname><given-names>TC</given-names></name><name><surname>Jensen</surname><given-names>KT</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>iLQR-VAE: control-based learning of input-driven dynamics with applications to neural data</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.10.07.463540</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Schimel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Why-prep-2</data-title><version designator="swh:1:rev:09d1949a43c0b5066a888b0ceb2a951e70539992">swh:1:rev:09d1949a43c0b5066a888b0ceb2a951e70539992</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e8187bafb874aba01c5955902765fabf85ba477d;origin=https://github.com/marineschimel/why-prep-2;visit=swh:1:snp:4c3a0cc3bdee5ea0f46045056329da0e1bc69410;anchor=swh:1:rev:09d1949a43c0b5066a888b0ceb2a951e70539992">https://archive.softwareheritage.org/swh:1:dir:e8187bafb874aba01c5955902765fabf85ba477d;origin=https://github.com/marineschimel/why-prep-2;visit=swh:1:snp:4c3a0cc3bdee5ea0f46045056329da0e1bc69410;anchor=swh:1:rev:09d1949a43c0b5066a888b0ceb2a951e70539992</ext-link></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheahan</surname><given-names>HR</given-names></name><name><surname>Franklin</surname><given-names>DW</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Motor planning, not execution, separates motor memories</article-title><source>Neuron</source><volume>92</volume><fpage>773</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.017</pub-id><pub-id pub-id-type="pmid">27817979</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical control of arm movements: a dynamical systems perspective</article-title><source>Annual Review of Neuroscience</source><volume>36</volume><fpage>337</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150509</pub-id><pub-id pub-id-type="pmid">23725001</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Skogestad</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Multivariable Feedback Control: Analysis and Design</source><publisher-loc>New York</publisher-loc><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohn</surname><given-names>H</given-names></name><name><surname>Meirhaeghe</surname><given-names>N</given-names></name><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A network perspective on sensorimotor learning</article-title><source>Trends in Neurosciences</source><volume>44</volume><fpage>170</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2020.11.007</pub-id><pub-id pub-id-type="pmid">33349476</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Soldado-Magraner</surname><given-names>J</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Inferring context-dependent computations through linear approximations of prefrontal cortex dynamics</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.02.06.527389</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sterling</surname><given-names>P</given-names></name><name><surname>Laughlin</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Principles of neural design</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stroud</surname><given-names>JP</given-names></name><name><surname>Porter</surname><given-names>MA</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Motor primitives in space and time via targeted gain modulation in cortical networks</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1774</fpage><lpage>1783</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0276-0</pub-id><pub-id pub-id-type="pmid">30482949</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>X</given-names></name><name><surname>O’Shea</surname><given-names>DJ</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Trautmann</surname><given-names>EM</given-names></name><name><surname>Vyas</surname><given-names>S</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Cortical preparatory activity indexes learned motor memories</article-title><source>Nature</source><volume>602</volume><fpage>274</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-04329-x</pub-id><pub-id pub-id-type="pmid">35082444</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neural network that finds A naturalistic solution for the production of muscle activity</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1025</fpage><lpage>1033</lpage><pub-id pub-id-type="doi">10.1038/nn.4042</pub-id><pub-id pub-id-type="pmid">26075643</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tassa</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>Theory and Implementation of Biomimetic Motor Controllers</source><publisher-name>Hebrew University of Jerusalem</publisher-name></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorov</surname><given-names>E</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Optimal feedback control as a theory of motor coordination</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>1226</fpage><lpage>1235</lpage><pub-id pub-id-type="doi">10.1038/nn963</pub-id><pub-id pub-id-type="pmid">12404008</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Todorov</surname><given-names>E</given-names></name><name><surname>Li</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Optimal control methods suitable for biomechanical systems</article-title><conf-name>25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</conf-name><conf-loc>Cancun, Mexico</conf-loc><fpage>1758</fpage><lpage>1761</lpage><pub-id pub-id-type="doi">10.1109/IEMBS.2003.1279748</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeo</surname><given-names>S-H</given-names></name><name><surname>Franklin</surname><given-names>DW</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>When optimal feedback control is not enough: feedforward strategies are required for optimal control with active sensing</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005190</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005190</pub-id><pub-id pub-id-type="pmid">27973566</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zimnik</surname><given-names>AJ</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Independent generation of sequence elements by motor cortex</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>412</fpage><lpage>424</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00798-5</pub-id><pub-id pub-id-type="pmid">33619403</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>A1.1 Choice of the hyperparameters of the model</title><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Correlates of the behavior and control strategy across a wide range of hyperparameters.</title><p>The ‘reach success’” and ‘holding success’” are set to 1 if the success criterion (see text) is satisfied and 0 otherwise. The task is executed successfully over a wide range of hyperparameters. The red star denotes the set of hyperparameters used in the main text simulations. This configuration was chosen to lie in a region in which the task can be successfully solved, with the performance being robust to small changes in the hyperparameters.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-app1-fig1-v1.tif"/></fig><p>Our cost function for the delayed single-reaching task was composed of three components. The relative weighings of the different terms in our cost, which are hyperparameters of the model, affect the way in which the task is solved. To ensure robustness of our results to hyperparameter changes, we scanned the space of <inline-formula><mml:math id="inf179"><mml:msub><mml:mi>α</mml:mi><mml:mtext>null</mml:mtext></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf180"><mml:msub><mml:mi>α</mml:mi><mml:mtext>effort</mml:mtext></mml:msub></mml:math></inline-formula> (as the solution is invariant to scaling of the cost, only those relative weighings matter), and evaluated the solutions found across this hyperparameter space for a delayed reach of 300 ms.</p><p>Our evaluation was based on multiple criteria. We considered the target to have been successfully reached if the mean distance to the target in the last 200 ms of the movement was lower than 5 mm (for a reach radius of 12 cm). We considered that the requirement to stay still during the delay period was satisfied if the mean torques during preparation were smaller than 0.02 N/m. We computed the preparation index and total cost as described in <xref ref-type="disp-formula" rid="equ4 equ5">Equations 4 and 5</xref>. We moreover computed the total input energy per neuron as <inline-formula><mml:math id="inf181"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mo movablelimits="false">∫</mml:mo><mml:mrow><mml:mo lspace="0em" rspace="0em">−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mi>‖</mml:mi><mml:mi>𝒖</mml:mi><mml:msup><mml:mi>‖</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, and the maximum velocity as <inline-formula><mml:math id="inf182"><mml:mrow><mml:msub><mml:mtext>max</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:msqrt><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">˙</mml:mo></mml:mover><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">˙</mml:mo></mml:mover><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>. These various quantities are shown for a range of hyperparameters in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>, with the choice of hyperparameters used throughout our simulations marked with a red star. This shows that the behavior of the model is consistent across a range of hyperparameter settings around the one we used.</p><p>In <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>, we illustrate the output of the model for several hyperparameter settings. One can notice that for very small values of <inline-formula><mml:math id="inf183"><mml:msub><mml:mi>α</mml:mi><mml:mtext>effort</mml:mtext></mml:msub></mml:math></inline-formula> the reach is successful, but executed with larger torques and velocity than is necessary – e.g., the red and yellow reaches are equally successful but the red one is much faster – which comes at the cost of larger inputs. We chose the set of hyperparameters for our simulations such as to lie in an intermediate regime in which the task is solved successfully, but without requiring more inputs than necessary.</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Illustration of the behavior for several hyperparameter settings.</title><p>(Left) Hand position along the horizontal axis, with the dotted line denoting the position of the target. (Middle) Temporal profile of the hand velocity. (Right) Temporal profile of the torques driving the hand.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-app1-fig2-v1.tif"/></fig></sec><sec sec-type="appendix" id="s9"><title>A1.2 Investigation of the effect of the network decay timescale</title><p><xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref> highlighted that preparatory inputs tend to consistently arise late during the delay period. We hypothesized that this may be a reflection of the intrinsic tendency of the network dynamics to decay, such that inputs given too early may be ‘lost’. To test this, we changed the characteristic timescale of the dynamics <italic>during preparation only</italic>, leading to the following dynamics:<disp-formula id="equ25"><label>(A1.1)</label><mml:math id="m25"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>mov</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mtext>where </mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>prep</mml:mtext></mml:mrow></mml:msub><mml:mtext>if</mml:mtext><mml:mspace width="thinmathspace"/><mml:mi mathvariant="italic">t</mml:mi><mml:mo>≤</mml:mo><mml:mn mathvariant="italic">0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>mov</mml:mtext></mml:mrow></mml:msub><mml:mtext>if</mml:mtext><mml:mspace width="thinmathspace"/><mml:mi mathvariant="italic">t</mml:mi><mml:mo>≥</mml:mo><mml:mn mathvariant="italic">0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>mov</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>150</mml:mn><mml:mspace width="thinmathspace"/><mml:mtext>ms</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula>. This allowed us to evaluate whether having dynamics decaying more slowly during preparation led to inputs starting earlier. Note that we also rescaled the inputs during preparation by <inline-formula><mml:math id="inf185"><mml:mfrac><mml:msub><mml:mi>τ</mml:mi><mml:mtext>prep</mml:mtext></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mtext>mov</mml:mtext></mml:msub></mml:mfrac></mml:math></inline-formula>, to ensure that the effective cost of the inputs was not affected by the timescale change.</p><p>As shown in <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>, inputs started rising earlier when the network’s decay timescale was longer. This was consistent with the hypothesis that the length of the window of preparation that the optimal controller uses depends on the network’s intrinsic timescale.</p><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Illustration of the effect of the characteristic neuronal timescale on the temporal distribution of the inputs.</title><p>We modified the characteristic neuronal timescale of the inhibition-stabilized network (ISN) during preparation only and assessed how that changed the temporal distribution of inputs for three different timescales (<italic>τ</italic><sub>prep</sub> = 50 ms, <italic>τ</italic><sub>prep</sub> = 150 ms, <italic>τ</italic><sub>prep</sub> = 300 ms, top to bottom). As hypothesized, inputs start earlier during the preparation window when the decay timescale of the network was longer.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-app1-fig3-v1.tif"/></fig></sec><sec sec-type="appendix" id="s10"><title>A1.3 Additional results in the 2D system</title><p>Our visualization of the behavior of 2D networks in <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref> allowed us to identify features of the dynamics that were well suited to predicting preparation. Below, we compute <inline-formula><mml:math id="inf186"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf187"><mml:mi>β</mml:mi></mml:math></inline-formula> numerically and analytically in 2D oscillatory and nonnormal networks, to gain insights into how these quantities vary with the networks’ dynamics. We then show how preparation can be predicted highly accurately across a large number of 2D systems, using only those quantities to summarize the network dynamics.</p><sec sec-type="appendix" id="s10-1"><title>A1.3.1 Controllability and observability computations</title><p>In <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>, we computed <inline-formula><mml:math id="inf188"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf189"><mml:mi>β</mml:mi></mml:math></inline-formula> numerically, as a function of the connectivity strength and the choice of readout, for the nonnormal and the oscillatory motifs shown in <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>.</p><p>This highlights the very different behaviors of the two networks, which are to some extent also reflected in higher-dimensional models. In particular, we find a strong effect of the alignment between the readout and the network dynamics in nonnormal networks, while <inline-formula><mml:math id="inf190"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf191"><mml:mi>β</mml:mi></mml:math></inline-formula> are independent of <inline-formula><mml:math id="inf192"><mml:msub><mml:mi>θ</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula> in oscillatoryq networks. Interestingly, we see that <inline-formula><mml:math id="inf193"><mml:mi>β</mml:mi></mml:math></inline-formula> is constant across all oscillatory networks, while <inline-formula><mml:math id="inf194"><mml:mi>α</mml:mi></mml:math></inline-formula> increases with <inline-formula><mml:math id="inf195"><mml:mi>w</mml:mi></mml:math></inline-formula>.</p><p>As the reduced 2D model is more amenable to mathematical analysis than its high-dimensional counterpart, we can gain further insights into the origin of these differences by computing <inline-formula><mml:math id="inf196"><mml:mrow><mml:mi>α</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf197"><mml:mrow><mml:mi>β</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> analytically.</p><p>Recall that the observability Gramian <italic>Q</italic> of a linear input-driven dynamical system satisfies<disp-formula id="equ26"><label>(A1.2)</label><mml:math id="m26"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p><p>and the controllability Gramian satisfies<disp-formula id="equ27"><label>(A1.3)</label><mml:math id="m27"><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and that we defined <inline-formula><mml:math id="inf198"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mtext>Tr</mml:mtext><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msup><mml:mi>𝑪</mml:mi><mml:mi>⊥</mml:mi></mml:msup><mml:mi>𝑸</mml:mi><mml:msup><mml:mi>𝑪</mml:mi><mml:msup><mml:mi>⊥</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf199"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mtext>Tr</mml:mtext><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝑪</mml:mi><mml:mi>𝑷</mml:mi><mml:msup><mml:mi>𝑪</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf200"><mml:msup><mml:mi>𝑪</mml:mi><mml:mi>⊥</mml:mi></mml:msup></mml:math></inline-formula> denotes the nullspace of the readout matrix. Below, we compute these quantities for the 2D oscillatory and nonnormal networks, with <inline-formula><mml:math id="inf201"><mml:mrow><mml:mi>𝑩</mml:mi><mml:mo>=</mml:mo><mml:mi>𝑰</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf202"><mml:mi>𝑪</mml:mi></mml:math></inline-formula> a unit-norm vector whose direction we parametrize via a quantity <inline-formula><mml:math id="inf203"><mml:msub><mml:mi>θ</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula>. Note that we ignore the effect of <italic>dt</italic> and <inline-formula><mml:math id="inf204"><mml:mi>τ</mml:mi></mml:math></inline-formula> in the mathematical analysis, as those quantities can straightforwardly be included in the final result via a rescaling of <inline-formula><mml:math id="inf205"><mml:mi>w</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf206"><mml:mi>𝑩</mml:mi></mml:math></inline-formula>.</p><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Illustration of <italic>α</italic> and <italic>β</italic> as a function of <italic>θ<sub>C</sub></italic> and <italic>w</italic> in the 2D networks.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-app1-fig4-v1.tif"/></fig><sec sec-type="appendix" id="s10-1-1"><title>Oscillatory network</title><p>In the case of <inline-formula><mml:math id="inf207"><mml:mrow><mml:mi>𝑨</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mi>𝑰</mml:mi><mml:mo>+</mml:mo><mml:mi>𝑺</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf208"><mml:mi>𝑺</mml:mi></mml:math></inline-formula> is a skew-symmetric network (i.e. <inline-formula><mml:math id="inf209"><mml:mrow><mml:msup><mml:mi>𝑺</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mi>𝑺</mml:mi></mml:mrow></mml:math></inline-formula>), <xref ref-type="disp-formula" rid="equ27">Equation A1.3</xref> is solved by <inline-formula><mml:math id="inf210"><mml:mrow><mml:mi>𝑷</mml:mi><mml:mo>=</mml:mo><mml:mi>𝑰</mml:mi><mml:mi>/</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> independently of the value of <inline-formula><mml:math id="inf211"><mml:mi>𝑺</mml:mi></mml:math></inline-formula>. This explains why <inline-formula><mml:math id="inf212"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mtext>Tr</mml:mtext><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝑪</mml:mi><mml:mi>𝑷</mml:mi><mml:msup><mml:mi>𝑪</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>‖</mml:mi><mml:mi>𝑪</mml:mi><mml:msup><mml:mi>‖</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is independent of both the connectivity strength <inline-formula><mml:math id="inf213"><mml:mi>w</mml:mi></mml:math></inline-formula> and the orientation of the readout <inline-formula><mml:math id="inf214"><mml:msub><mml:mi>θ</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula> for skew-symmetric networks (see <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>, bottom right). Practically, this means that skew-symmetric networks are equally controllable in all directions: when driven by random inputs, these networks display isotropic activity of equal variance along all directions. Moreover, as <inline-formula><mml:math id="inf215"><mml:mi>w</mml:mi></mml:math></inline-formula> controls the oscillation frequency of the network, but does not change the decay timescale of the eigenmodes, the amount of variance generated by a random stimulation is independent of <inline-formula><mml:math id="inf216"><mml:mi>w</mml:mi></mml:math></inline-formula>. Interestingly, we can see in <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref> (top right) that <inline-formula><mml:math id="inf217"><mml:mi>α</mml:mi></mml:math></inline-formula> displays a different behavior, and increases with <inline-formula><mml:math id="inf218"><mml:mi>w</mml:mi></mml:math></inline-formula>. As highlighted above, skew-symmetric systems are rotationally symmetric. Without loss of generality, we can thus define our 1D vector to read out the first unit, i.e., <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The observability Gramian must satisfy<disp-formula id="equ28"><label>(A1.4)</label><mml:math id="m28"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mspace width="thickmathspace"/><mml:mo stretchy="false">⟹</mml:mo><mml:mspace width="thickmathspace"/><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi>w</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mi>w</mml:mi></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mi>w</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>w</mml:mi></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This can be found in closed form by solving the 2D system of equations, yielding<disp-formula id="equ29"><label>(A1.5)</label><mml:math id="m29"><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>4</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:mfrac><mml:mi>w</mml:mi><mml:mrow><mml:mn>4</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:mfrac><mml:mi>w</mml:mi><mml:mrow><mml:mn>4</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>4</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>From there, we obtain <inline-formula><mml:math id="inf220"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mtext>Tr</mml:mtext><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msup><mml:mi>𝑪</mml:mi><mml:mi>⊥</mml:mi></mml:msup><mml:mi>𝑸</mml:mi><mml:msup><mml:mi>𝑪</mml:mi><mml:msup><mml:mi>⊥</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>4</mml:mn><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. As found empirically, this quantity will initially increase before plateauing toward <inline-formula><mml:math id="inf221"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> as <inline-formula><mml:math id="inf222"><mml:mi>w</mml:mi></mml:math></inline-formula> becomes large.</p><p>One might wonder why observability displays such a dependence on the oscillatory frequency of the network, even though the network is rotationally symmetric, and <inline-formula><mml:math id="inf223"><mml:mi>w</mml:mi></mml:math></inline-formula> does not affect the decay timescale. As highlighted in <xref ref-type="disp-formula" rid="equ26">Equation A1.2</xref>, controllability and observability Gramian would be identical for a skew-symmetric system if <inline-formula><mml:math id="inf224"><mml:mrow><mml:mi>𝑪</mml:mi><mml:mo>=</mml:mo><mml:mi>𝑰</mml:mi></mml:mrow></mml:math></inline-formula>. However, a feature of the systems we consider is the existence of a nullspace, i.e., the fact that the readout <inline-formula><mml:math id="inf225"><mml:mi>𝑪</mml:mi></mml:math></inline-formula> only targets a subset of dimensions across the whole space (implying that <inline-formula><mml:math id="inf226"><mml:mrow><mml:msup><mml:mi>𝑪</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>𝑪</mml:mi></mml:mrow></mml:math></inline-formula> is a low-rank matrix). Intuitively, the reason why <inline-formula><mml:math id="inf227"><mml:mi>α</mml:mi></mml:math></inline-formula> increases with <inline-formula><mml:math id="inf228"><mml:mi>w</mml:mi></mml:math></inline-formula> while <inline-formula><mml:math id="inf229"><mml:mi>β</mml:mi></mml:math></inline-formula> is constant in skew-symmetric networks can be understood as follows: <inline-formula><mml:math id="inf230"><mml:mi>α</mml:mi></mml:math></inline-formula> is computing how much <italic>readout activity</italic> a vector initialized in the nullspace of <inline-formula><mml:math id="inf231"><mml:mi>𝑪</mml:mi></mml:math></inline-formula> will generate, while <inline-formula><mml:math id="inf232"><mml:mi>β</mml:mi></mml:math></inline-formula> is computing the amount of energy that will be generated <italic>across all directions</italic> by a vector initialized in the readout space. Thus, assuming once again <inline-formula><mml:math id="inf233"><mml:mrow><mml:mi>𝑪</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">[</mml:mo><mml:mtable><mml:mtr><mml:mtd style="padding-left:0em;"><mml:mn>1</mml:mn></mml:mtd><mml:mtd style="padding-right:0em;"><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf234"><mml:mrow><mml:msup><mml:mi>𝑪</mml:mi><mml:mi>⊥</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">[</mml:mo><mml:mtable><mml:mtr><mml:mtd style="padding-left:0em;"><mml:mn>0</mml:mn></mml:mtd><mml:mtd style="padding-right:0em;"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the activity of vectors initialized along <inline-formula><mml:math id="inf235"><mml:mi>𝑪</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf236"><mml:msup><mml:mi>𝑪</mml:mi><mml:mi>⊥</mml:mi></mml:msup></mml:math></inline-formula> respectively and evolving autonomously from there is given by <inline-formula><mml:math id="inf237"><mml:mrow><mml:msub><mml:mi>𝒗</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">[</mml:mo><mml:mtable><mml:mtr><mml:mtd style="padding-left:0em;"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd style="padding-right:0em;"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf238"><mml:mrow><mml:msub><mml:mi>𝒗</mml:mi><mml:msup><mml:mi>C</mml:mi><mml:mi>⊥</mml:mi></mml:msup></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">[</mml:mo><mml:mtable><mml:mtr><mml:mtd style="padding-left:0em;"><mml:mrow><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd style="padding-right:0em;"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mi>.</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p><p>From there, we can compute <inline-formula><mml:math id="inf239"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo movablelimits="false">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>∞</mml:mi></mml:msubsup><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝒗</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:msup><mml:mi>‖</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo movablelimits="false">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>∞</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">−</mml:mo><mml:mn>2</mml:mn><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>. Thus, as found above, only the decay timescale of the envelope (fixed to 1 here) affects the value of <inline-formula><mml:math id="inf240"><mml:mi>β</mml:mi></mml:math></inline-formula>.</p><p>Importantly, <inline-formula><mml:math id="inf241"><mml:mi>α</mml:mi></mml:math></inline-formula> will instead have a dependence on <inline-formula><mml:math id="inf242"><mml:mi>w</mml:mi></mml:math></inline-formula> arising from the fact that it depends on the size of the activity <italic>projected into the readout</italic>, as<disp-formula id="equ30"><label>(A1.6)</label><mml:math id="m30"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi mathvariant="normal">⊥</mml:mi></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>sin</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ31"><label>(A1.7)</label><mml:math id="m31"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ32"><label>(A1.8)</label><mml:math id="m32"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi mathvariant="normal">ℜ</mml:mi><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ33"><label>(A1.9)</label><mml:math id="m33"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>4</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ34"><label>(A1.10)</label><mml:math id="m34"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>4</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The dependence of this quantity on <inline-formula><mml:math id="inf243"><mml:mi>w</mml:mi></mml:math></inline-formula> can be understood by the fact that activity patterns initialized in the readout nullspace benefit from the existence of rotational dynamics, which allows them to be readout before the activity decays completely.</p></sec><sec sec-type="appendix" id="s10-1-2"><title>Nonnormal network</title><p>In the nonnormal network, we have <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>w</mml:mi></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The nonnormal 2D system, unlike its oscillatory counterpart, does not have rotational symmetry. Thus, to remain general, we will consider <inline-formula><mml:math id="inf245"><mml:mrow><mml:mi>𝑪</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">[</mml:mo><mml:mtable><mml:mtr><mml:mtd style="padding-left:0em;"><mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mspace width="0.1667em"/></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd style="padding-right:0em;"><mml:mrow><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mspace width="0.1667em"/></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf246"><mml:mrow><mml:msup><mml:mi>𝑪</mml:mi><mml:mi>⊥</mml:mi></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">[</mml:mo><mml:mtable><mml:mtr><mml:mtd style="padding-left:0em;"><mml:mrow><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mspace width="0.1667em"/></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd style="padding-right:0em;"><mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mspace width="0.1667em"/></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Solving <xref ref-type="disp-formula" rid="equ27">Equation A1.3</xref> for <inline-formula><mml:math id="inf247"><mml:mrow><mml:mi>𝑩</mml:mi><mml:mo>=</mml:mo><mml:mi>𝑰</mml:mi></mml:mrow></mml:math></inline-formula> leads to an expression for the controllability Gramian of the nonnormal system as<disp-formula id="equ35"><label>(A1.11)</label><mml:math id="m35"><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>w</mml:mi><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>w</mml:mi><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Similarly, computation of the observability Gramian leads to<disp-formula id="equ36"><label>(A1.12)</label><mml:math id="m36"><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>sin</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mi>w</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>cos</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:mi>w</mml:mi><mml:msup><mml:mi>sin</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>w</mml:mi><mml:msup><mml:mi>sin</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:msup><mml:mi>sin</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We can then compute<disp-formula id="equ37"><label>(A1.13)</label><mml:math id="m37"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊥</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊥</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>4</mml:mn></mml:mfrac><mml:msup><mml:mi>sin</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ38"><label>(A1.14)</label><mml:math id="m38"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>cos</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This highlights the dependence of <inline-formula><mml:math id="inf248"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf249"><mml:mi>β</mml:mi></mml:math></inline-formula> on <inline-formula><mml:math id="inf250"><mml:msub><mml:mi>θ</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula>, which can also be seen in <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref> (left). Interestingly, these expressions also make evident the supralinear scaling of <inline-formula><mml:math id="inf251"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf252"><mml:mi>β</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="inf253"><mml:mi>w</mml:mi></mml:math></inline-formula> in nonnormal networks. Note however that we never investigate preparation in the very large <inline-formula><mml:math id="inf254"><mml:mi>w</mml:mi></mml:math></inline-formula> regime, as the simulation of such networks with discretized dynamics is prone to numerical issues.</p></sec></sec><sec sec-type="appendix" id="s10-2"><title>A.13.2 Predicting preparation in 2D networks</title><p>To assess how well preparation could be predicted from the control-theoretic properties <inline-formula><mml:math id="inf255"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf256"><mml:mi>β</mml:mi></mml:math></inline-formula> (c.f. main text) of 2D networks, we generated 20,000 networks with weight matrix<disp-formula id="equ39"><label>(A1.15)</label><mml:math id="m39"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>ff</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>a</mml:mi></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>ff</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msqrt><mml:msubsup><mml:mi>w</mml:mi><mml:mtext>ff</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mn>4</mml:mn><mml:msup><mml:mi>ω</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>ff</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msqrt><mml:msubsup><mml:mi>w</mml:mi><mml:mtext>ff</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mn>4</mml:mn><mml:msup><mml:mi>ω</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mi>a</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf257"><mml:mrow><mml:mi>a</mml:mi><mml:mo>∼</mml:mo><mml:mi class="mathcal">𝒰</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0,0.8</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf258"><mml:mrow><mml:mi>ω</mml:mi><mml:mo>∼</mml:mo><mml:mi class="mathcal">𝒰</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0,4</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf259"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mtext>ff</mml:mtext></mml:msub><mml:mo>∼</mml:mo><mml:mi class="mathcal">𝒰</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>4</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. <xref ref-type="disp-formula" rid="equ29">Equation A1.5</xref> implies that <inline-formula><mml:math id="inf260"><mml:mi>𝑾</mml:mi></mml:math></inline-formula> has a pair of complex-conjugate eigenvalues <inline-formula><mml:math id="inf261"><mml:mrow><mml:mi>a</mml:mi><mml:mo>±</mml:mo><mml:mi>i</mml:mi><mml:mi>ω</mml:mi></mml:mrow></mml:math></inline-formula>, and also embeds a feedforward coupling of strength <inline-formula><mml:math id="inf262"><mml:msub><mml:mi>w</mml:mi><mml:mtext>ff</mml:mtext></mml:msub></mml:math></inline-formula> from the second to the first dimension. For each network configuration, we computed the corresponding values of <inline-formula><mml:math id="inf263"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf264"><mml:mi>β</mml:mi></mml:math></inline-formula>. To confirm our intuition that the preparation index should increase with <inline-formula><mml:math id="inf265"><mml:mi>α</mml:mi></mml:math></inline-formula> and decrease with <inline-formula><mml:math id="inf266"><mml:mi>β</mml:mi></mml:math></inline-formula>, we first attempted to fit <inline-formula><mml:math id="inf267"><mml:mrow><mml:mtext>prep. index</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfrac><mml:mi>α</mml:mi><mml:mi>β</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>. Interestingly, we found that while this quantity was positively correlated with the preparation index across networks, a substantial fraction of variance remained unexplained (test <inline-formula><mml:math id="inf268"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.16</mml:mn></mml:mrow></mml:math></inline-formula>). Labeling the preparation index by the rotational frequency of the network highlighted that a substantial fraction of the variance across networks came from this timescale of oscillations (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>, left). Indeed, a regression model of the <inline-formula><mml:math id="inf269"><mml:mrow><mml:mtext>prep. index</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>ω</mml:mi><mml:mfrac><mml:mi>α</mml:mi><mml:mi>β</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> captured 80% of the variance in preparation index, yielding an accurate fit across networks with only two free parameters (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>, right).</p><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>Predicting the preparation index from characteristic network quantities.</title><p>We evaluated how well the preparation index could be predicted as a linear function of <inline-formula><mml:math id="inf270"><mml:mfrac><mml:mi>α</mml:mi><mml:mi>β</mml:mi></mml:mfrac></mml:math></inline-formula> (left). A substantial amount of residual variance appeared to arise from variability in the oscillation frequency <inline-formula><mml:math id="inf271"><mml:mi>ω</mml:mi></mml:math></inline-formula> (color). Accounting for this frequency by regressing the preparation index against <inline-formula><mml:math id="inf272"><mml:mrow><mml:mi>ω</mml:mi><mml:mfrac><mml:mi>α</mml:mi><mml:mi>β</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> gave a better fit (right).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-app1-fig5-v1.tif"/></fig><p>We stress that the predictive power of these simple fits is remarkable given that the preparation index comes out of a complex process of optimization over control inputs. Thus, the control-theoretic quantities <inline-formula><mml:math id="inf273"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf274"><mml:mi>β</mml:mi></mml:math></inline-formula> appear to appropriately summarize the benefits of preparation for individual networks.</p><p>The fact that the preparation index also grows with <inline-formula><mml:math id="inf275"><mml:mi>ω</mml:mi></mml:math></inline-formula> can be understood by considering the alignment between the activity trajectories which the network can autonomously generate and those that are required for solving the motor task. Indeed, a network that is intrinsically unable to generate outputs with the right oscillatory timescale would have to rely on movement-related inputs, i.e., would have a low preparation index. As observed here, the network’s characteristic frequency has a big impact in 2D networks, consistent with <inline-formula><mml:math id="inf276"><mml:mi>ω</mml:mi></mml:math></inline-formula> determining the <italic>only</italic> oscillatory pattern that the network can generate on its own. For high-dimensional networks, however, we did not have to incorporate such a measure of compatibility between task requirements and network dynamics (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>). We speculate that this is due to averaging effects. Indeed, larger networks possess a wide range of intrinsic oscillatory timescales, and the readout matrix – which here was not aligned to the network’s dynamics in any specific way – is expected to read out a little bit of all frequencies, including task-appropriate ones.</p></sec></sec><sec sec-type="appendix" id="s11"><title>A1.4 Comparison across networks</title><fig id="app1fig6" position="float"><label>Appendix 1—figure 6.</label><caption><title>Preparation arises across a range of network architectures: neural correlates of the reach are shown for five different networks (<bold>A</bold>), alongside the loss and prep.</title><p>index as a function of <inline-formula><mml:math id="inf277"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>prep</mml:mtext></mml:msub></mml:math></inline-formula>. (<bold>A</bold>) Eigenvalue spectrum (top), internal network activations (middle), and inputs (bottom) for different network types. The unconnected network does not rely on preparatory inputs at all. The random network with weights draw from <inline-formula><mml:math id="inf278"><mml:mrow><mml:mi class="mathcal">𝒩</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>0.95</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> uses very little delay-period inputs while the skew-symmetric network with <inline-formula><mml:math id="inf279"><mml:mrow><mml:mi class="mathcal">𝒩</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>4</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> shows a substantial amount of inputs during the delay period. The inhibition-stabilized network can be seen to rely most on preparation, more so than the similarity transformed inhibition-stabilized network (ISN). (<bold>B</bold>) Loss (top) and preparation index (bottom) as a function of delay-period length for the different networks. The unconnected and random networks can be seen to benefit very little from longer preparation times. Indeed, even as <inline-formula><mml:math id="inf280"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>prep</mml:mtext></mml:msub></mml:math></inline-formula> increases, their amount of preparatory inputs remains very close to 0. On the other hand, the skew-symmetric network and the ISN use preparatory inputs (bottom), which allow them to have a lower loss for larger values of <inline-formula><mml:math id="inf281"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>prep</mml:mtext></mml:msub></mml:math></inline-formula>. Interestingly, the surrogate ISN prepares considerably less than the full ISN.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-app1-fig6-v1.tif"/></fig><p>Our main investigation was largely focused on behavior of inhibition-stabilized networks, which are believed to constitute good models of M1. We however found that the expression we derived to obtain a network’s preparation index from its control-theoretic properties generalized across to other types of networks. Below, we detail the other network families we considered, and show how their dynamics qualitatively differ from the ISN, although their preparation can be predicted using the same quantities.</p><p>We modeled three additional classes of networks: randomly connected networks with either (i) unstructured or (ii) skew-symmetric connectivities, (iii) a surrogate network obtained by applying a similarity transformation to the ISN that preserved its eigenvalue spectrum but eliminated any ‘nonnormality’ (i.e., we found <inline-formula><mml:math id="inf282"><mml:mi class="mathcal">𝒯</mml:mi></mml:math></inline-formula> such that <inline-formula><mml:math id="inf283"><mml:mrow><mml:mover><mml:mi>𝑨</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mi class="mathcal">𝒯</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>𝑨</mml:mi><mml:mi class="mathcal">𝒯</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf284"><mml:mover><mml:mi>𝑨</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">~</mml:mo></mml:mover></mml:math></inline-formula> was a diagonal matrix with the same eigenvalues as <inline-formula><mml:math id="inf285"><mml:mi>𝑨</mml:mi></mml:math></inline-formula>). Note that we did not apply the transformation to the readout or input matrices, such that the transfer function of the system was changed by our transformation. This was voluntary, as we were interested in the effect that transforming the dynamics would have on the input-output response. These networks were chosen for the diversity of dynamical motifs they exhibit: combinations of rapidly and slowly decaying modes, oscillations, and transient dynamics. Moreover, each of these network families could be sampled from in a straightforward manner, allowing to compute results across many instantiations of each network type. We again used random readout matrices not specifically adjusted to the dynamics of the network nor to the motor task.</p><p>To get an intuition for how different networks solve the task, we generated one network from each family and qualitatively compared their inputs and internal activations when performing the same delayed reach (<xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6A</xref>). We first considered an unconnected network, i.e., a network whose recurrent weights were all 0. Unsurprisingly, this network had no use for a preparation phase. Indeed, there is no benefit to giving early inputs as the network is unable to amplify them. More surprisingly, a random network with a much stronger connectivity – as can be seen in its eigenvalue spectrum forming a small ball of radius close to 1 (<xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6A</xref>, top) – also displayed very little preparation. The strong, visually apparent similarity between the inputs to the random and unconnected networks suggests that the optimal way of controlling the random network relies largely on ignoring its internal dynamics and solving the task almost entirely in an input-driven regime. The example skew-symmetric network, which had imaginary eigenvalues only (ranging between –5.5 and 5.5), displayed considerably more preparation, but still relied on strong inputs during the movement phase that resembled those of the unconnected and random networks. Finally, the ISN relied much more on preparation; the small inputs it receives are strongly amplified into large activity patterns owing to its strong, nonnormal recurrent connectivity. Interestingly however, the similarity transformed ISN lost much of that ability to amplify inputs, instead displaying dynamics resembling that of the skew-symmetric network. This highlights the effect of the ISN’s nonnormal dynamics in shaping the network’s activity and optimal inputs.</p><p>Next, we assessed more directly how beneficial preparation was for the different networks. We evaluated how the total loss and preparation index evolved as a function of the delay-period length (<xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6B</xref>). As expected, the control of networks that relied on preparation (skew-symmetric and ISN) benefited more from longer delays. The ISN has markedly lower control cost and higher preparation index than other networks, reflecting the fact that even weak (thus energetically cheap) inputs were sufficient to produce internal activity and thus output torques of the desired magnitude (<xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6A</xref>, right).</p><p>The above results give a sense of the range of possible dynamics that different types of networks display. Interestingly, despite these differences, we showed in <xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref> that the preparation index could be predicted with a simple formula across all networks.</p><fig id="app1fig7" position="float"><label>Appendix 1—figure 7.</label><caption><title>Illustration of the effect of optimizing the readout matrix such as to minimize the cost of the reaches, across all movements.</title><p>To evaluate the effect that our choice of random readout directions has on our conclusions, we additionally compare to a model with the same dynamics, but where the readout was optimized such as to minimize the cost across movements (i.e. <inline-formula><mml:math id="inf286"><mml:mrow><mml:mi class="mathcal">ℒ</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝑪</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mtext>targets</mml:mtext></mml:mrow></mml:munder></mml:mrow><mml:msup><mml:mi class="mathcal">𝒥</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>i</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝑪</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>), under the constraint that its norm was fixed. In (<bold>A</bold>), we see that this leads to an increase in the observability of the system (compare the observability of the modes of the optimized system in black with those of the random readout in red). In (<bold>B</bold>) and (<bold>C</bold>), we see that this leads to an output of similar amplitude (<bold>B</bold>), but that is generated using smaller inputs (<bold>C</bold>). Importantly, we see that the system still relies on preparatory inputs. Thus, the exact choice of readout does not alter the network strategy, but can help the system perform the same movements in a more efficient manner.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-app1-fig7-v1.tif"/></fig><fig id="app1fig8" position="float"><label>Appendix 1—figure 8.</label><caption><title>Comparison of the effect of penalizing temporal input smoothness vs. input norm.</title><p>We compare the effect of using a cost over inputs that penalizes input norm, vs. using a cost that penalizes the ‘temporal complexity’ of the inputs – defined here as the temporal derivative of the inputs (i.e. <inline-formula><mml:math id="inf287"><mml:mrow><mml:mi>‖</mml:mi><mml:mi>𝒖</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>𝒖</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:msup><mml:mi>‖</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> in discrete time). This is achieved by augmenting the dynamical system to include an input integration stage, which then feeds into the original dynamical system; this way, the input to the augmented system – of which we continue to penalize the squared norm to enable the iterative linear quadratic regulator algorithm (iLQR) framework – is the derivative of the input to the original system. We perform this comparison in linear recurrent neural networks (RNNs), across a range of different preparation times. We show the activation of an example neuron in (<bold>A</bold>) and (<bold>B</bold>), and activity in an example input channel in (<bold>C</bold>) and (<bold>D</bold>). Each color denotes a different reach. We see that the rates vary more slowly when penalizing the temporal complexity (<bold>A</bold>) vs. the input norm (<bold>B</bold>), exhibiting a plateau for longer preparation times that is more similar to neural recordings. This is a reflection of the fact that the inputs themselves vary more slowly when the temporal complexity is penalized (compare <bold>C</bold> and <bold>D</bold>). As we do not penalize the input norm within our definition of temporal complexity, the optimal strategy is for the network to rely on steady inputs, which is different from the strategy used when the norm is penalized (compare <bold>C</bold> and <bold>D</bold>). We note that, under this different choice of input penalty, preparation nevertheless remains optimal, with the normalized loss (shown in <bold>E</bold>) decreasing as <inline-formula><mml:math id="inf288"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>prep</mml:mtext></mml:msub></mml:math></inline-formula> increases, and the preparation index (shown in <bold>F</bold>) increasing as <inline-formula><mml:math id="inf289"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>prep</mml:mtext></mml:msub></mml:math></inline-formula> increases.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-app1-fig8-v1.tif"/></fig><fig id="app1fig9" position="float"><label>Appendix 1—figure 9.</label><caption><title>Control strategy adopted by the model when planning with uncertain delays.</title><p>We investigate an extension of the model, that includes uncertainty about the arrival of the go cue, and involves replanning at regular intervals to update the model with new information (e.g. whether the go cue has arrived). In (<bold>A</bold>), we model this by assuming that the network is adopting a strategy whereby it plans to be ready to move <italic>as early as possible</italic> following the target onset. We optimize the inputs using this assumption, and replan every 20 ms to update the model with the available information (which here corresponds to the actual go cue only arriving 500 ms after target onset). We plot the activity of two example neurons (left and right panels, respectively), for each of the reaches (each color denotes a different reach). We can see that the neuronal activations start ramping up at the beginning of the task, and plateau before the actual target onset. In (<bold>B</bold>), we use a similar optimization strategy, but use a different ‘mental model’ for the network, whereby we assume that, until it sees the actual go cue, the model is always assuming that the delay period will be equal to the most likely a posteriori preparation time. Under the assumption of exponentially distributed delays with a mean of 150 ms, this corresponds to always replanning assuming a delay of 150 ms. We see that the network then adopts a different strategy, which does not include ramping/plateauing of the neural activity.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-app1-fig9-v1.tif"/></fig><fig id="app1fig10" position="float"><label>Appendix 1—figure 10.</label><caption><title>Comparison of the occupancy of the preparatory and movement subspaces across different delay periods.</title><p>Occupancy (normalized by the maximum value across preparatory and movement occupancies) of the preparatory and movement subspaces identified using a delay period of 500 ms, for the activity generated using <inline-formula><mml:math id="inf290"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>prep</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> ms (left), <inline-formula><mml:math id="inf291"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>prep</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> ms (center), and <inline-formula><mml:math id="inf292"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>prep</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> ms (right). We see that the network does not rely on preparatory activity when <inline-formula><mml:math id="inf293"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>prep</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> ms.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89131-app1-fig10-v1.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89131.4.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Diedrichsen</surname><given-names>Jörn</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Western University</institution><country>Canada</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study provides a new perspective on why preparatory activity occurs before the onset of movement. The authors report that when there is a cost on the inputs, the optimal inputs should start before the desired network output for a wide variety of recurrent networks. The authors present <bold>compelling</bold> evidence by combining mathematically tractable analyses in linear networks and numerical simulation in nonlinear networks.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89131.4.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>In this work, the authors investigate an important question - under what circumstances should a recurrent neural network optimised to produce motor control signals receive preparatory input before the initiation of a movement, even though it is possible to use inputs to drive activity just-in-time for movement?</p><p>This question is important because many studies across animal models have shown that preparatory activity is widespread in neural populations close to motor output (e.g. motor cortex / M1), but it isn't clear under what circumstances this preparation is advantageous for performance, especially since preparation could cause unwanted motor output during a delay.</p><p>They show that networks optimised under reasonable constraints (speed, accuracy, lack of pre-movement) will use the input to seed the state of the network before movement and that these inputs reduce the need for ongoing input during the movement. By examining many different parameters in simplified models they identify a strong connection between the structure of the network and the amount of preparation that is optimal for control - namely, that preparation has the most value when nullspaces are highly observable relative to the readout dimension and when the controllability of readout dimensions is low. They conclude by showing that their model predictions are consistent with the observation in monkey motor cortex that even when a sequence of two movements is known in advance, preparatory activity only arises shortly before movement initiation.</p><p>Overall, this study provides valuable theoretical insight into the role of preparation in neural populations that generate motor output, and by treating input to motor cortex as a signal that is optimised directly this work is able to sidestep many of the problematic questions relating to estimating the potential inputs to motor cortex.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89131.4.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This work clarifies neural mechanisms that can lead to a phenomenology consistent with motor preparation in its broader sense. In this context, motor preparation refers to activity that occurs before the corresponding movement. Another property often associated with preparatory activity is a correlation with global movement characteristics such as reach speed (Churchland et al., Neuron 2006), reach angle (Sun et al., Nature 2022), or grasp type (Meirhaeghe et al., Cell Reports 2023). Such activity has notably been observed in premotor and primary motor cortices, and it has been hypothesized to serve as an input to a motor execution circuit. The timing and mechanisms by which such 'preparatory' inputs are made available to motor execution circuits remain however unclear in general, especially in light of the presence of a 'trigger-like' signal that appears to relate to the transition from preparatory dynamics to execution activity (Kaufman et al. eNeuron 2016, Iganaki et al., Cell 2022, Zimnik and Churchland, Nature Neuroscience 2021).</p><p>The preparatory inputs have been hypothesized to fulfill one or several (non-mutually-exclusive) possible objectives. Two notable hypotheses are that these inputs could be shaped to maximize output accuracy under regularization of the input magnitude; or that they may help the flexible re-use of the neural machinery involved in the control of movements in different contexts.</p><p>Here, the authors investigate in detail how the former hypothesis may be compatible with the presence of early inputs in recurrent network models driving arm movements, and compare models to data.</p><p>Strengths:</p><p>The authors are able to deploy an in-depth evaluation of inputs that are optimized for producing an accurate output at a pre-defined time while using a regularization term on the input magnitude, in the case of movements that are thought to be controlled in a quasi-open loop fashion such as reaches.</p><p>First, the authors have identified that optimal control theory is a great framework to study this question as it provides methods to find and analyze exact solutions to this cost function in the case of models with linear dynamics. The authors not only use this framework to get an exact assessment of how much pre-movement input arises in large recurrent networks, but also give insight into the mechanisms by which it happens by dissecting in detail low-dimensional networks. The authors find that two key network properties - observability of the readout's nullspace and limited controllability - give rise to optimal inputs that are large before the start of the movement (while the corresponding network activity lies in the nullspace of the readout). Further, the authors numerically investigate the timing of optimized inputs in models with nonlinear dynamics, and find that pre-movement inputs can also arise in these more general networks. The authors also explore how some variations on their model's constraints - such as penalizing the input roughness or changing task contingencies about the go cue timing - affect their results. Finally, the authors point out some coarse-grained similarities between the pre-movement activity driven by the optimized inputs in some of the models they studied, and the phenomenology of preparation observed in the brain during single reaches and reach sequences. Overall, the authors deploy an impressive arsenal of tools and a very in-depth analysis of their models.</p><p>Oustanding questions that could lead to interesting follow-up work:</p><p>Like all great pieces of research, this article makes it clear where current limitations lie and therefore opens up opportunities for future work.</p><p>(1) Though the optimal control theory framework is ideal for determining inputs that minimize output error while regularizing the input norm or other simple input features, it cannot easily account for some other varied types of objectives - especially those that may lead to a complex optimization landscape. For instance, the reusability of parts of the circuit, sparse use of additional neurons when learning many movements, and ease of planning (especially under uncertainty about when to start the movement), may be alternative or additional reasons that could help explain the preparatory activity observed in the brain. It is interesting to note that inputs that optimize the objective chosen by the authors arguably lead to a trade-off in terms of other desirable objectives. Specifically, the inputs the authors derive are time-dependent, so a recurrent network would be needed to produce them and it may not be easy to interpolate between them to drive new movement variants. In addition, these inputs depend on the desired time of output and therefore make it difficult to plan, e.g. in circumstances when timing should be decided depending on sensory signals. Finally, these inputs are specific to the full movement chain that will unfold, so they do not permit reuse of the inputs e.g. in movement sequences of different orders. Of note, the authors have pointed out in the discussion how their framework may be extended in future work to account for some additional objectives, such as inputs' temporal smoothness or some strategies for dealing with go cue timing uncertainty.</p><p>(2) Relatedly, if the motor circuits were to balance different types of objectives, the activity and inputs occurring before each movement may be broken down into different categories that may each specialize into their own objective. For instance, previous work (Kaufman et al. eNeuron 2016, Iganaki et al., Cell 2022, Zimnik and Churchland, Nature Neuroscience 2021) has suggested that inputs occurring before the movement could be broken down into preparatory inputs 'stricto sensu' - relating to the planned characteristics of the movement - and a trigger signal, relating to the transition from planning to execution - irrespective of whether the movement is internally timed or triggered by an external event. The current work does not address which type(s) of early input may be labeled as 'preparatory' or may be thought of as a part of 'planning' computations, or whether these inputs may come from several different source circuits. Future research could investigate these questions using a different approach, for instance, by including structural constraints from brain architecture into a neural network model.</p><p>(3) While the authors rightly point out some similarities between the inputs that they derive and observed preparatory activity in the brain, notably during motor sequences, there are also some differences. For instance, while both the derived inputs and the data show two peaks during sequences, the data reproduced from Zimnik and Churchland show preparatory inputs that have a very asymmetric shape that really plummets before the start of the next movement, whereas the derived inputs have larger amplitude during the movement period - especially for the second movement of the sequence. In addition, the data show trigger-like signals before each of the two reaches. Finally, while the data show a very high correlation between the pattern of preparatory activity of the second reach in the double reach and compound reach conditions, the derived inputs appear to be more different between the two conditions. Note that the data would be consistent with separate planning of the two reaches even in the compound reach condition, as well as the re-use of the preparatory input between the compound and double reach conditions. Therefore, different motor sequence datasets - notably, those that would show even more coarticulation between submovements - may be more promising for finding a tight match between the data and the author's inputs. In the future, further analyses in these datasets could help determine whether the coarticulation could be due to simple filtering by the circuits and muscles downstream of M1, planning of movements with adjusted curvature to mitigate the work performed by the muscles while permitting some amount of re-use across different sequences, or - as suggested by the authors - inputs fully tailored to one specific movement sequence that maximize accuracy and minimize the M1 input magnitude.</p><p>(4) Though iLQR is a powerful optimization method to find inputs optimizing the author's cost function, it also has some limitations. First, given that it relies on a linearization of the dynamics at each timestep, it has a limited ability to leverage potential advantages of nonlinearities in the dynamics. Second, the iLQR algorithm is not a biologically plausible learning rule and does not account for biological constraints affecting the circuits that produce and process these inputs. Therefore, it might be difficult for the brain to learn to produce the inputs that it finds. Consequently, when observing differences between model and data, this can confound the question of whether it comes from a difference of assumed objective or a difference of optimization procedure or circuit implementation. It remains unclear whether using alternative algorithms with different limitations - for instance, using variants of BPTT to train a separate RNN to produce the inputs in question - could impact some of the results.</p><p>(5) Under the objective considered by the authors, the amount of input occurring before the movement might be impacted by the presence of online sensory signals for closed-loop control. Even if the inputs include some sensory activity and/or the RNN activity could represent all general variables (e.g. sensory) whose states can be decoded from M1, the model does not currently include mechanisms that process imperfect (delayed, noisy) sensory feedback to adapt the output in a trial-specific manner. The information related to such sensory feedback cannot be anticipated, and therefore the related input would have to reach the motor cortex after preparation. Thus, it is an open question whether the objective and network characteristics suggested by the authors could also explain the presence of large preparatory activity before e.g. grasping movements that are thought to be more sensory-feedback-driven (Meirhaeghe et al., Cell Reports 2023).</p><p>(6) More broadly, with the type of objectives that the authors assume the inputs fulfill, some M1 properties that lead to strong preparation - notably, limited readout controllability - may not be favorable for control in general, so it would be interesting if other objectives and assumptions could robustly lead to strong preparation under more general M1 properties.'</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89131.4.sa3</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>I remain enthusiastic about this study. The manuscript is well-written, logical, and conceptually clear. To my knowledge, no prior modeling study has tackled the question of 'why prepare before executing, why not just execute?' Prior studies have simply assumed, to emulate empirical findings, that preparatory inputs precede execution. They never asked why. The authors show that, when there are constraints on inputs, preparation becomes a natural strategy. In contrast, with no constraint on inputs, there is no need for preparation as one could get anything one liked just via the inputs during movement. For the sake of tractability, the authors use a simple magnitude constraint: the cost function punishes the integral of the squared inputs. Thus, if small inputs before movement can reduce the size of the inputs needed during movement, preparation is a good strategy. This occurs if (and only if) the network has strong dynamics (otherwise feeding it preparatory activity would not produce anything interesting). All of this is sensible and clarifying.</p><p>As discussed in the prior round of reviews, the central constraint that the authors use is a mathematically tractable stand-in for a range of plausible (but often trickier to define and evaluate) constraints, such as simplicity of inputs (or inputs being things that other areas could provide). The manuscript now embraces this fact more explicitly and also gives some results showing that other constraints (such as on the derivative of activity, which is one component of complexity) can have the same effect. The manuscript also now discusses and addresses a modest weakness of the previous manuscript: the preparatory activity in their simulations is often overly complex temporally, lacking the (rough) plateau typically seen for data. Depending on your point of view, this is simply 'window dressing', but from my perspective it was important to know that their approach could yield more realistic-looking preparatory activity.</p><p>The most recent version of the manuscript also has a useful section in the Discussion on the topic of preparation when there is no external delay, which I found helpful given prior behavioral and physiological studies arguing that preparation can (1) be very brief, but (2) is always present. These findings mesh nicely with the authors' central result that preparation is a good network strategy, and that it would thus be normative for there to be at least a brief interval of preparation even when not imposed externally.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89131.4.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Schimel</surname><given-names>Marine</given-names></name><role specific-use="author">Author</role><aff><institution>University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Kao</surname><given-names>Ta-Chu</given-names></name><role specific-use="author">Author</role><aff><institution>Meta Reality Labs</institution><addr-line><named-content content-type="city">Burlingame</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Hennequin</surname><given-names>Guillaume</given-names></name><role specific-use="author">Author</role><aff><institution>University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 1:</bold></p></disp-quote><p>Thank you for your review and pointing out multiple things to be discussed and clarified! Below, we go through the various limitations you pointed out and refer to the places where we have tried to address them.</p><disp-quote content-type="editor-comment"><p>(1) It's important to keep in mind that this work involves simplified models of the motor system, and often the terminology for 'motor cortex' and 'models of motor cortex' are used interchangeably, which may mislead some readers. Similarly, the introduction fails in many cases to state what model system is being discussed (e.g. line 14, line 29, line 31), even though these span humans, monkeys, mice, and simulations, which all differ in crucial ways that cannot always be lumped together.</p></disp-quote><p>That is a good point. We have clarified this in the text (Introduction and Discussion), to highlight the fact that our model isn’t necessarily meant to just capture M1. We have also updated the introduction to make it more clear which species the experiments which motivate our investigation were performed in.</p><disp-quote content-type="editor-comment"><p>(2) At multiple points in the manuscript thalamic inputs during movement (in mice) is used as a motivation for examining the role of preparation. However, there are other more salient motivations, such as delayed sensory feedback from the limb and vision arriving in the motor cortex, as well as ongoing control signals from other areas such as the premotor cortex.</p></disp-quote><p>Yes – the motivation for thalamic inputs came from the fact that those have specifically been shown to be necessary for accurate movement generation in mice. However, it is true that the inputs in our model are meant to capture any signals external to the dynamical system modeled, and as such are likely to represent a mixture of sensory signals, and feedback from other areas. We have clarified this in the Discussion, and have added this additional motivation in the Introduction.</p><disp-quote content-type="editor-comment"><p>(3) Describing the main task in this work as a delayed reaching task is not justified without caveats (by the authors' own admission: line 687), since each network is optimized with a fixed delay period length. Although this is mentioned to the reader, it's not clear enough that the dynamics observed during the delay period will not resemble those in the motor cortex for typical delayed reaching tasks.</p></disp-quote><p>Yes, we completely agree that the terminology might be confusing. While the task we are modeling is a delayed reaching task, it does differ from the usual setting since the network has knowledge of the delay period, and that is indeed a caveat of the model. We have added a brief paragraph just after the description of the optimal control objective to highlight this limitation.</p><p>We have also performed additional simulations using two different variants of a model-predictive control approach that allow us to relax the assumption that the go-cue time is known in advance. We show that these modifications of the optimal controller yield results that remain consistent with our main conclusions, and can in fact in some settings lead to preparatory activity plateaus during the preparation epoch as often found in monkey M1 (e.g in Elsayed et al. 2016). We have modified the Discussion to explain these results and their limitations, which are summarized in a new Supplementary Figure (S9).</p><disp-quote content-type="editor-comment"><p>(4) A number of simplifications in the model may have crucial consequences for interpretation.</p><p>a) Even following the toy examples in Figure 4, all the models in Figure 5 are linear, which may limit the generalisability of the findings.</p></disp-quote><p>While we agree that linear models may be too simplistic, much prior analyses of M1 data suggest that it is often good enough to capture key aspects of M1 dynamics; for example, the generative model underlying jPCA is linear, and Sussillo et al. (2015) showed that the internal activity of nonlinear RNN models trained to reproduce EMG data aligned best with M1 activity when heavily regularized; in this regime, the RNN dynamics were close to linear. Nevertheless, this linearity assumption is indeed convenient from a modeling viewpoint: the optimal control problem is more easily solved for linear network dynamics and the optimal trajectories are more consistent across networks. Indeed, we had originally attempted to perform the analyses of Figure 5 in the nonlinear setting, but found that while the results were overall similar to what we report in the linear regime, iLQR was occasionally trapped into local minimal, resulting in more variable results especially for inhibition-stabilized network in the strongly connected end of the spectrum. Finally, Figure 5 is primarily meant to explore to what extent motor preparation can be predicted from basic linear control-theoretic properties of the Jacobian of the dynamics; in this regard, it made sense to work with linear RNNs (for which the Jacobian is constant).</p><disp-quote content-type="editor-comment"><p>b) Crucially, there is no delayed sensory feedback in the model from the plant. Although this simplification is in some ways a strength, this decision allows networks to avoid having to deal with delayed feedback, which is a known component of closed-loop motor control and of motor cortex inputs and will have a large impact on the control policy.</p></disp-quote><p>This comment resonates well with Reviewer 3's remark regarding the autonomous nature (or not) of M1 during movement. Rather than thinking of our RNN models as anatomically confined models of M1 alone, we think of them as models of the dynamics which M1 implements possibly as part of a broader network involving “inter-area loops and (at some latency) sensory feedback”, and whose state appears to be near-fully decodable from M1 activity alone. We have added a paragraph of Discussion on this important point.</p><disp-quote content-type="editor-comment"><p>(5) A key feature determining the usefulness of preparation is the direction of the readout dimension. However, all readouts had a similar structure (random Gaussian initialization). Therefore, it would be useful to have more discussion regarding how the structure of the output connectivity would affect preparation, since the motor cortex certainly does not follow this output scheme.</p></disp-quote><p>We agree with this limitation of our model — indeed one key message of Figure 4 is that the degree of reliance on preparatory inputs depends strongly on how the dynamics align with the readout. However, this strong dependence is somewhat specific to low-dimensional models; in higher-dimensional models (most of our paper), one expects that any random readout matrix C will pick out activity dimensions in the RNN that are sufficiently aligned with the most controllable directions of the dynamics to encourage preparation.</p><p>We did consider optimizing C away (which required differentiating through the iLQR optimizer, which is possible but very costly), but the question inevitably arises what exactly should C be optimized for, and under what constraints (e.g fixed norm or not). One possibility is to optimize C with respect to the same control objective that the control inputs are optimized for, and constrain its norm (otherwise, inputs to the M1 model, and its internal activity, could become arbitrarily small as C can grow to compensate). We performed this experiment (new Supplementary Figure S7) and obtained a similar preparation index; there was one notable difference, namely that the optimized readout modes led to greater observability compared to a random readout; thus, the same amount of “muscle energy” required for a given movement could now be produced by a smaller initial condition. In turn, this led to smaller control inputs, consistent with a lower control cost overall.</p><p>Whilst we could have systematically optimized C away, we reasoned that (i) it is computationally expensive, and (ii) the way M1 affects downstream effectors is presumably “optimized” for much richer motor tasks than simple 2D reaching, such that optimizing C for a fixed set of simple reaches could lead to misleading conclusions. We therefore decided to stick with random readouts.</p><disp-quote content-type="editor-comment"><p>Additional comments:</p><p>(1) The choice of cost function seems very important. Is it? For example, penalising the square of u(t) may produce very different results than penalising the absolute value.</p></disp-quote><p>Yes, the choice of cost function does affect the results, at least qualitatively. The absolute value of the inputs is a challenging cost to use, as iLQR relies on a local quadratic approximation of the cost function. However, we have included additional experiments in which we penalized the squared derivative of the inputs (Supplementary Figure S8; see also our response to Reviewer 3's suggestion on this topic), and we do see differences in the qualitative behavior of the model (though the main takeaway, i.e. the reliance on preparation, continues to hold). This is now referred to and discussed in the Discussion section.</p><disp-quote content-type="editor-comment"><p>(2) In future work it would be useful to consider the role of spinal networks, which are known to contribute to preparation in some cases (e.g. Prut and Fetz, 1999).</p><p>(3) The control signal magnitude is penalised, but not the output torque magnitude, which highlights the fact that control in the model is quite different from muscle control, where co-contraction would be a possibility and therefore a penalty of muscle activation would be necessary. Future work should consider the role of these differences in control policy.</p></disp-quote><p>Thank you for pointing us to this reference! Regarding both of these concerns, we agree that the model could be greatly improved and made more realistic in future work (another avenue for this would be to consider a more realistic biophysical model, e.g. using the MotorNet library). We hope that the current Discussion, which highlights the various limitations of our modeling choices, makes it clear that a lot of these choices could easily be modified depending on the specific assumptions/investigation being performed.</p><p><bold>Reviewer 2:</bold></p><p>Thank you for your positive review! We very much agree with the limitations you pointed out, some of which overlapped with the comments of the other reviewers. We have done our best to address them through additional discussion and new supplementary figures. We briefly highlight below where those changes can be found.</p><disp-quote content-type="editor-comment"><p>(1) Though the optimal control theory framework is ideal to determine inputs that minimize output error while regularizing the input norm, it however cannot easily account for some other varied types of objectives especially those that may lead to a complex optimization landscape. For instance, the reusability of parts of the circuit, sparse use of additional neurons when learning many movements, and ease of planning (especially under uncertainty about when to start the movement), may be alternative or additional reasons that could help explain the preparatory activity observed in the brain. It is interesting to note that inputs that optimize the objective chosen by the authors arguably lead to a trade-off in terms of other desirable objectives. Specifically, the inputs the authors derive are time-dependent, so a recurrent network would be needed to produce them and it may not be easy to interpolate between them to drive new movement variants. In addition, these inputs depend on the desired time of output and therefore make it difficult to plan, e.g. in circumstances when timing should be decided depending on sensory signals. Finally, these inputs are specific to the full movement chain that will unfold, so they do not permit reuse of the inputs e.g. in movement sequences of different orders.</p></disp-quote><p>Yes, that is a good point! We have incorporated further Discussion related to this point. We have additionally included a new example in which we regularize the temporal complexity of the inputs (see also our response to Reviewer 3's suggestion on this topic), which leads to more slowly varying inputs, and may indeed represent a more realistic constraint and lead to simpler inputs that can more easily be interpolated between. We also agree that uncertainty about the upcoming go cue may play an important role in the strategy adopted by the animals. While we have not performed an extensive investigation of the topic, we have included a Supplementary Figure (S9) in which we used Model Predictive Control to investigate the effect of planning under uncertainty about the go cue arrival time. We hope that this will give the reader a better sense of what sort of model extensions are possible within our framework.</p><disp-quote content-type="editor-comment"><p>(2) Relatedly, if the motor circuits were to balance different types of objectives, the activity and inputs occurring before each movement may be broken down into different categories that may each specialize into one objective. For instance, previous work (Kaufman et al. eNeuron 2016, Iganaki et al., Cell 2022, Zimnik and Churchland, Nature Neuroscience 2021) has suggested that inputs occurring before the movement could be broken down into preparatory inputs 'stricto sensu' - relating to the planned characteristics of the movement - and a trigger signal, relating to the transition from planning to execution - irrespective of whether the movement is internally timed or triggered by an external event. The current work does not address which type(s) of early input may be labeled as 'preparatory' or may be thought of as a part of 'planning' computations.</p></disp-quote><p>Yes, our model does indeed treat inputs in a very general way, and does not distinguish between the different types of processes they may be composed of. This is partly because we do not explicitly model where the inputs come from, such that our inputs likely englobe multiple processes. We have added discussion related to this point.</p><disp-quote content-type="editor-comment"><p>(3) While the authors rightly point out some similarities between the inputs that they derive and observed preparatory activity in the brain, notably during motor sequences, there are also some differences. For instance, while both the derived inputs and the data show two peaks during sequences, the data reproduced from Zimnik and Churchland show preparatory inputs that have a very asymmetric shape that really plummets before the start of the next movement, whereas the derived inputs have larger amplitude during the movement period - especially for the second movement of the sequence. In addition, the data show trigger-like signals before each of the two reaches. Finally, while the data show a very high correlation between the pattern of preparatory activity of the second reach in the double reach and compound reach conditions, the derived inputs appear to be more different between the two conditions. Note that the data would be consistent with separate planning of the two reaches even in the compound reach condition, as well as the re-use of the preparatory input between the compound and double reach conditions. Therefore, different motor sequence datasets - notably, those that would show even more coarticulation between submovements - may be more promising to find a tight match between the data and the author's inputs. Further analyses in these datasets could help determine whether the coarticulation could be due to simple filtering by the circuits and muscles downstream of M1, planning of movements with adjusted curvature to mitigate the work performed by the muscles while permitting some amount of re-use across different sequences, or - as suggested by the authors - inputs fully tailored to one specific movement sequence that maximize accuracy and minimize the M1 input magnitude.</p></disp-quote><p>Regarding the exact shape of the occupancy plots, it is important to note that some of the more qualitative aspects (e.g the relative height of the two peaks) will change if we change the parameters of the cost function. Right now, we have chosen the parameters to ensure that both reaches would be performed at roughly the same speed (as a way to very loosely constrain the parameters based on the observed behavior). However, small changes to the hyperparameters can lead to changes in the model output (e.g one of the two consecutive reaches being performed using greater acceleration than the other), and since our biophysical model is fairly simple, changes in the behavior are directly reflected in the network activity. Essentially, what this means is that while the double occupancy is a consistent feature of the model, the exact shape of the peaks is more sensitive to hyperparameters, and we do not wish to draw any strong conclusions from them, given the simplicity of the biophysical model. However, we do agree that our model exhibits some differences with the data. As discussed above, we have included additional discussion regarding the potential existence of separate inputs for planning vs triggering the movement in the context of single reaches.</p><p>Overall, we are excited about the suggestions made by the Reviewer here about using our approach to analyze other motor sequence datasets, but we think that in order to do this properly, one would need to adopt a more realistic musculo-skeletal model (such as one provided by MotorNet).</p><disp-quote content-type="editor-comment"><p>(4) Though iLQR is a powerful optimization method to find inputs optimizing the author's cost function, it also has some limitations. First, given that it relies on a linearization of the dynamics at each timestep, it has a limited ability to leverage potential advantages of nonlinearities in the dynamics. Second, the iLQR algorithm is not a biologically plausible learning rule and therefore it might be difficult for the brain to learn to produce the inputs that it finds. It remains unclear whether using alternative algorithms with different limitations - for instance, using variants of BPTT to train a separate RNN to produce the inputs in question - could impact some of the results.</p></disp-quote><p>We agree that our choice of iLQR has limitations: while it offers the advantage of convergence guarantees, it does indeed restrict the choice of cost function and dynamics that we can use. We have now included extensive discussion of how the modeling choices affect our results.</p><p>We do not view the lack of biological plausibility of iLQR as an issue, as the results are agnostic to the algorithm used for optimization. However, we agree that any structure imposed on the inputs (e.g by enforcing them to be the output of a self-contained dynamical system) would likely alter the results. A potentially interesting extension of our model would be to do just what the reviewer suggested, and try to learn a network that can generate the optimal inputs. However, this is outside the scope of our investigation, as it would then lead to new questions (e.g what brain region would that other RNN represent?).</p><disp-quote content-type="editor-comment"><p>(5) Under the objective considered by the authors, the amount of input occurring before the movement might be impacted by the presence of online sensory signals for closed-loop control. It is therefore an open question whether the objective and network characteristics suggested by the authors could also explain the presence of preparatory activity before e.g. grasping movements that are thought to be more sensory-driven (Meirhaeghe et al., Cell Reports 2023).</p></disp-quote><p>It is true that we aren’t currently modeling sensory signals explicitly. However, some of the optimal inputs we infer may be capturing upstream information which could englobe some sensory information. This is currently unclear, and would likely depend on how exactly the model is specified. We have added new discussion to emphasize that our dynamics should not be understood as just representing M1, but more general circuits whose state can be decoded from M1.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p></disp-quote><p>Additionally, thank you for pointing out various typos in the manuscript, we have fixed those!</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 3:</bold></p></disp-quote><p>Thank you very much for your review, which makes a lot of very insightful points, and raises several interesting questions. In summary, we very much agree with the limitations you pointed out. In particular, the choice of input cost is something we had previously discussed, but we had found it challenging to decide on what a reasonable cost for “complexity” could be. Following your comment, we have however added a first attempt at penalizing “temporal complexity”, which shows promising behavior. We have only included those additional analyses as supplementary figures, and we have included new discussion, which hopefully highlights what we meant by the different model components, and how the model behavior may change as we vary some of our choices. We hope this can be informative for future models that may use a similar approach. Below, we highlight the changes that we have made to address your comments.</p><disp-quote content-type="editor-comment"><p>The main limitation of the study is that it focuses exclusively on one specific constraint - magnitude - that could limit motor-cortex inputs. This isn't unreasonable, but other constraints are at least as likely, if less mathematically tractable. The basic results of this study will probably be robust with regard such issues - generally speaking, any constraint on what can be delivered during execution will favor the strategy of preparing - but this robustness cuts both ways. It isn't clear that the constraint used in the present study - minimizing upstream energy costs - is the one that really matters. Upstream areas are likely to be limited in a variety of ways, including the complexity of inputs they can deliver. Indeed, one generally assumes that there are things that motor cortex can do that upstream areas can't do, which is where the real limitations should come from. Yet in the interest of a tractable cost function, the authors have built a system where motor cortex actually doesn't do anything that couldn't be done equally well by its inputs. The system might actually be better off if motor cortex were removed. About the only thing that motor cortex appears to contribute is some amplification, which is 'good' from the standpoint of the cost function (inputs can be smaller) but hardly satisfying from a scientific standpoint.</p><p>The use of a term that punishes the squared magnitude of control signals has a long history, both because it creates mathematical tractability and because it (somewhat) maps onto the idea that one should minimize the energy expended by muscles and the possibility of damaging them with large inputs. One could make a case that those things apply to neural activity as well, and while that isn't unreasonable, it is far from clear whether this is actually true (and if it were, why punish the square if you are concerned about ATP expenditure?). Even if neural activity magnitude an important cost, any costs should pertain not just to inputs but to motor cortex activity itself. I don't think the authors really wish to propose that squared input magnitude is the key thing to be regularized. Instead, this is simply an easily imposed constraint that is tractable and acts as a stand-in for other forms of regularization / other types of constraints. Put differently, if one could write down the 'true' cost function, it might contain a term related to squared magnitude, but other regularizing terms would by very likely to dominate. Using only squared magnitude is a reasonable way to get started, but there are also ways in which it appears to be limiting the results (see below).</p><p>I would suggest that the study explore this topic a bit. Is it possible to use other forms of regularization? One appealing option is to constrain the complexity of inputs; a long-standing idea is that the role of motor cortex is to take relatively simple inputs and convert them to complex time-evolving inputs suitable for driving outputs. I realize that exploring this idea is not necessarily trivial. The right cost-function term is not clear (should it relate to low-dimensionality across conditions, or to smoothness across time?) and even if it were, it might not produce a convex cost function. Yet while exploring this possibility might be difficult, I think it is important for two reasons.</p><p>First, this study is an elegant exploration of how preparation emerges due to constraints on inputs, but at present that exploration focuses exclusively on one constraint. Second, at present there are a variety of aspects of the model responses that appear somewhat unrealistic. I suspect most of these flow from the fact that while the magnitude of inputs is constrained, their complexity is not (they can control every motor cortex neuron at both low and high frequencies). Because inputs are not complexity-constrained, preparatory activity appears overly complex and never 'settles' into the plateaus that one often sees in data. To be fair, even in data these plateaus are often imperfect, but they are still a very noticeable feature in the response of many neurons. Furthermore, the top PCs usually contain a nice plateau. Yet we never get to see this in the present study. In part this is because the authors never simulate the situation of an unpredictable delay (more on this below) but it also seems to be because preparatory inputs are themselves strongly time-varying. More realistic forms of regularization would likely remedy this.</p></disp-quote><p>That is a very good point, and it mirrors several concerns that we had in the past. While we did focus on the input norm for the sake of simplicity, and because it represents a very natural way to regularize our control solutions, we agree that a “complexity cost” may be better suited to models of brain circuits. We have addressed this in a supplementary investigation. We chose to focus on a cost that penalizes the temporal complexity of the inputs, as ||u(t+1) - u(t)||^2. Note that this required augmenting the state of the model, making the computations quite a bit slower; while it is doable if we only penalize the first temporal derivative, it would not scale well to higher orders.</p><p>Interestingly, we did find that the activity in that setting was somewhat more realistic (see new Supplementary Figure S8), with more sustained inputs and plateauing activity. While we have kept the original model for most of the investigations, the somewhat more realistic nature of the results under that setting suggests that further exploration of penalties of that sort could represent a promising avenue to improve the model.</p><p>We also found the idea of a cost that would ensure low-dimensionality of the inputs across conditions very interesting. However, it is challenging to investigate with iLQR as we perform the optimization separately for each condition; nevertheless, it could be investigated using a different optimizer.</p><disp-quote content-type="editor-comment"><p>At present, it is also not clear whether preparation always occurs even with no delay. Given only magnitude-based regularization, it wouldn't necessarily have to be. The authors should perform a subspace-based analysis like that in Figure 6, but for different delay durations. I think it is critical to explore whether the model, like monkeys, uses preparation even for zero-delay trials. At present it might or might not. If not, it may be because of the lack of more realistic constraints on inputs. One might then either need to include more realistic constraints to induce zero-delay preparation, or propose that the brain basically never uses a zero delay (it always delays the internal go cue after the preparatory inputs) and that this is a mechanism separate from that being modeled.</p><p>I agree with the authors that the present version of the model, where optimization knows the exact time of movement onset, produces a reasonably realistic timecourse of preparation when compared to data from self-paced movements. At the same time, most readers will want to see that the model can produce realistic looking preparatory activity when presented with an unpredictable delay. I realize this may be an optimization nightmare, but there are probably ways to trick the model into optimizing to move soon, but then forcing it to wait (which is actually what monkeys are probably doing). Doing so would allow the model to produce preparation under the circumstances where most studies have examined it. In some ways this is just window-dressing (showing people something in a format they are used to and can digest) but it is actually more than that, because it would show that the model can produce a reasonable plateau of sustained preparation. At present it isn't clear it can do this, for the reasons noted above. If it can't, regularizing complexity might help (and even if this can't be shown, it could be discussed).</p><p>In summary, I found this to be a very strong study overall, with a conceptually timely message that was well-explained and nicely documented by thorough simulations. I think it is critical to perform the test, noted above, of examining preparatory subspace activity across a range of delay durations (including zero) to see whether preparation endures as it does empirically. I think the issue of a more realistic cost function is also important, both in terms of the conceptual message and in terms of inducing the model to produce more realistic activity. Conceptually it matters because I don't think the central message should be 'preparation reduces upstream ATP usage by allowing motor cortex to be an amplifier'. I think the central message the authors wish to convey is that constraints on inputs make preparation a good strategy. Many of those constraints likely relate to the fact that upstream areas can't do things that motor cortex can do (else you wouldn't need a motor cortex) and it would be good if regularization reflected that assumption. Furthermore, additional forms of regularization would likely improve the realism of model responses, in ways that matter both aesthetically and conceptually. Yet while I think this is an important issue, it is also a deep and tricky one, and I think the authors need considerable leeway in how they address it. Many of the cost-function terms one might want to use may be intractable. The authors may have to do what makes sense given technical limitations. If some things can't be done technically, they may need to be addressed in words or via some other sort of non-optimization-based simulation.</p><p>Specific comments</p><p>As noted above, it would be good to show that preparatory subspace activity occurs similarly across delay durations. It actually might not, at present. For a zero ms delay, the simple magnitude-based regularization may be insufficient to induce preparation. If so, then the authors would either have to argue that a zero delay is actually never used internally (which is a reasonable argument) or show that other forms of regularization can induce zero-delay preparation.</p></disp-quote><p>Yes, that is a very interesting analysis to perform, which we had not considered before! When investigating this, we found that the zero-delay strategy does not rely on preparation in the same way as is seen in the monkeys. This seems to be a reflection of the fact that our “Go cue” corresponds to an “internal” go cue which would likely come after the true, “external go cue” – such that we would indeed never actually be in the zero delay setting. This is not something we had addressed (or really considered) before, although we had tried to ensure we referred to “delta prep” as the duration of the preparatory period but not necessarily the delay period. We have now included more discussion on this topic, as well as a new Supplementary Figure S10.</p><disp-quote content-type="editor-comment"><p>I agree with the authors that prior modeling work was limited by assuming the inputs to M1, which meant that prior work couldn't address the deep issue (tackled here) of why there should be any preparatory inputs at all. At the same time, the ability to hand-select inputs did provide some advantages. A strong assumption of prior work is that the inputs are 'simple', such that motor cortex must perform meaningful computations to convert them to outputs. This matters because if inputs can be anything, then they can just be the final outputs themselves, and motor cortex would have no job to do. Thus, prior work tried to assume the simplest inputs possible to motor cortex that could still explain the data. Most likely this went too far in the 'simple' direction, yet aspects of the simplicity were important for endowing responses with realistic properties. One such property is a large condition-invariant response just before movement onset. This is a very robust aspect of the data, and is explained by the assumption of a simple trigger signal that conveys information about when to move but is otherwise invariant to condition. Note that this is an implicit form of regularization, and one very different from that used in the present study: the input is allowed to be large, but constrained to be simple. Preparatory inputs are similarly constrained to be simple in the sense that they carry only information about which condition should be executed, but otherwise have little temporal structure. Arguably this produces slightly too simple preparatory-period responses, but the present study appears to go too far in the opposite direction. I would suggest that the authors do what they can to address these issue via simulations and/or discussion. I think it is fine if the conclusion is that there exist many constraints that tend to favor preparation, and that regularizing magnitude is just one easy way of demonstrating that. Ideally, other constraints would be explored. But even if they can't be, there should be some discussion of what is missing - preparatory plateaus, a realistic condition-invariant signal tied to movement onset - under the present modeling assumptions.</p></disp-quote><p>As described above, we have now included two additional figures. In the first one (S8, already discussed above), we used a temporal smoothness prior, and we indeed get slightly more realistic activity plateaus. In a second supplementary figure (S9), we have also considered using model predictive control (MPC) to optimize the inputs under an uncertain go cue arrival time. There, we found that removing the assumption that the delay period is known came with new challenges: in particular, it requires the specification of a “mental model” of when the Go cue will arrive. While it is reasonable to expect that monkeys will have a prior over the go time arrival cue that will be shaped by the design of the experiment, some assumptions must be made about the utility functions that should be used to weigh this prior. For instance, if we imagine that monkeys carry a model of the possible arrival time of the go cue that is updated online, they could nonetheless act differently based on this information, for instance by either preparing so as to be ready for the earliest go cue possible or alternatively to be ready for the average go cue. This will likely depend on the exact task design and reward/penalty structure. Here, we added simulations with those two cases (making simplifying assumptions to make the problem tractable/solvable using model predictive control), and found that the “earliest preparation” strategy gives rise to more realistic plateauing activity, while the model where planning is done for the “most likely go time” does not. We suspect that more realistic activity patterns could be obtained by e.g combining this framework with the temporal smoothness cost. However, the main point we wished to make with this new supplementary figure is that it is possible to model the task in a slightly more realistic way (although here it comes at the cost of additional model assumptions). We have now added more discussion related to those points. Note that we have kept our analyses on these new models to a minimum, as the main takeaway we wish to convey from them is that most components of the model could be modified/made more realistic. This would impact the qualitative behavior of the system and match to data but – in the examples we have so far considered – does not appear to modify the general strategy of networks relying on preparation.</p><disp-quote content-type="editor-comment"><p>On line 161, and in a few other places, the authors cite prior work as arguing for &quot;autonomous internal dynamics in M1&quot;. I think it is worth being careful here because most of that work specifically stated that the dynamics are likely not internal to M1, and presumably involve inter-area loops and (at some latency) sensory feedback. The real claim of such work is that one can observe most of the key state variables in M1, such that there are periods of time where the dynamics are reasonably approximated as autonomous from a mathematical standpoint. This means that you can estimate the state from M1, and then there is some function that predicts the future state. This formal definition of autonomous shouldn't be conflated with an anatomical definition.</p></disp-quote><p>Yes, that is a good point, thank you for making it so clearly! Indeed, as previous work, we do not think of our “M1 dynamics” as being internal to M1, but they may instead include sensory feedback / inter-area loops, which we summarize into the connectivity, that we chose to have dynamics that qualitatively resemble data. We have now incorporated more discussion regarding what exactly the dynamics in our model represent.</p><p>Round 2 of reviews</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 3:</bold></p><p>My remaining comments largely pertain to some subtle (but to me important) nuances at a few locations in the text. These should be easy for the authors to address, in whatever way they see fit.</p><p>Specific comments:</p><p>(1) The authors state the following on line 56: &quot;For preparatory processes to avoid triggering premature movement, any pre-movement activity in the motor and dorsal pre-motor (PMd) cortices must carefully exclude those pyramidal tract neurons.&quot;</p><p>This constraint is overly restrictive. PT neurons absolutely can change their activity during preparation in principle (and appear to do so in practice). The key constraint is looser: those changes should have no net effect on the muscles. E.g., if d is the vector of changes in PT neuron firing rates, and b is the vector of weights, then the constraint is that b'd = 0. d = 0 is one good way of doing this, but only one. Half the d's could go up and half could go down. Or they all go up, but half the b's are negative. Put differently, there is no reason the null space has to be upstream of the PT neurons. It could be partly, or entirely, downstream. In the end, this doesn't change the point the authors are making. It is still the case that d has to be structured to avoid causing muscle activity, which raises exactly the point the authors care about: why risk this unless preparation brings benefits? However, this point can be made with a more accurate motivation. This matters, because people often think that a null-space is a tricky thing to engineer, when really it is quite natural. With enough neurons, preparing in the null space is quite simple.</p></disp-quote><p>That is a good point – we have now reformulated this sentence to instead say “to avoid triggering premature movement, any pre-movement activity in the motor and dorsal premotor (PMd) cortices must engage the pyramidal tract neurons in a way that ensures their activity patterns will not lead to any movement”.</p><disp-quote content-type="editor-comment"><p>(2) Line 167: 'near-autonomous internal dynamics in M1'.</p><p>It would be good if such statements, early in the paper, could be modified to reflect the fact that the dynamics observed in M1 may depend on recurrence that is NOT purely internal to M1. A better phrase might be 'near-autonomous dynamics that can be observed in M1'. A similar point applies on line 13. This issue is handled very thoughtfully in the Discussion, starting on line 713. Obviously it is not sensible to also add multiple sentences making the same point early on. However, it is still worth phrasing things carefully, otherwise the reader may have the wrong impression up until the Discussion (i.e. they may think that both the authors, and prior studies, believe that all the relevant dynamics are internal to M1). If possible, it might also be worth adding one sentence, somewhere early, to keep readers from falling into this hole (and then being stuck there till the Discussion digs them out).</p></disp-quote><p>That is a good point: we have now edited the text after line 170 to make it clear that the underlying dynamics may not be confined to M1, and have referenced the later discussion there.</p><disp-quote content-type="editor-comment"><p>(3) The authors make the point, starting on line 815, that transient (but strong) preparatory activity empirically occurs without a delay. They note that their model will do this but only if 'no delay' means 'no external delay'. For their model to prepare, there still needs to be an internal delay between when the first inputs arrive and when movement generating inputs arrive.</p><p>This is not only a reasonable assumption, but is something that does indeed occur empirically. This can be seen in Figure 8c of Lara et al. Similarly, Kaufman et al. 2016 noted that &quot;the sudden change in the CIS [the movement triggering event] occurred well after (~150 ms) the visual go cue... (~60 ms latency)&quot; Behavioral experiments have also argued that internal movement-triggering events tend to be quite sluggish relative to the earliest they could be, causing RTs to be longer than they should be (Haith et al. Independence of Movement Preparation and Movement Initiation). Given this empirical support, the authors might wish to add a sentence indicating that the data tend to justify their assumption that the internal delay (separating the earliest response to sensory events from the events that actually cause movement to begin) never shrinks to zero.</p><p>While on this topic, the Haith and Krakauer paper mentioned above good to cite because it does ponder the question of whether preparation is really necessary. By showing that they could get RTs to shrink considerably before behavior became inaccurate, they showed that people normally (when not pressured) use more preparation time than they really need. Given Lara et al, we know that preparation does always occur, but Haith and Krakauer were quite right that it can be very brief. This helped -- along with neural results -- change our view of preparation from something more cognitive that had to occur, so something more mechanical that was simply a good network strategy, which is indeed the authors current point. Working a discussion of this into the current paper may or may not make sense, but if there is a place where it is easy to cite, it would be appropriate.</p></disp-quote><p>This is a nice suggestion, and we thank the reviewer for pointing us to the Haith and Krakauer paper. We have now added this reference and extended the paragraph following line 815 to briefly discuss the possible decoupling between preparation and movement initiation that is shown in the Haith paper, emphasizing how this may affect the interpretation of the internal delay and comparisons with behavioral experiments.</p></body></sub-article></article>