<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">91243</article-id><article-id pub-id-type="doi">10.7554/eLife.91243</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.91243.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Analysis of foothold selection during locomotion using terrain reconstruction</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Muller</surname><given-names>Karl S</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Bonnen</surname><given-names>Kathryn</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9210-8275</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Shields</surname><given-names>Stephanie M</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Panfili</surname><given-names>Daniel P</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Matthis</surname><given-names>Jonathan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3683-646X</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Hayhoe</surname><given-names>Mary M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6671-5207</contrib-id><email>hayhoe@utexas.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>Center for Perceptual Systems, The University of Texas at Austin</institution></institution-wrap><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kg8sb98</institution-id><institution>School of Optometry, Indiana University</institution></institution-wrap><addr-line><named-content content-type="city">Bloomington</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04t5xt781</institution-id><institution>Department of Biology, Northeastern University</institution></institution-wrap><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Spering</surname><given-names>Miriam</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03rmrcq20</institution-id><institution>The University of British Columbia</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Stanford University, Howard Hughes Medical Institute</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>09</day><month>12</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP91243</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-08-06"><day>06</day><month>08</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-08-18"><day>18</day><month>08</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.08.18.553818"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-10-23"><day>23</day><month>10</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91243.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-09-24"><day>24</day><month>09</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91243.2"/></event></pub-history><permissions><copyright-statement>© 2023, Muller, Bonnen et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Muller, Bonnen et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-91243-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-91243-figures-v1.pdf"/><related-article related-article-type="commentary" ext-link-type="doi" xlink:href="10.7554/eLife.104965" id="ra1"/><abstract><p>Relatively little is known about the way vision is used to guide locomotion in the natural world. What visual features are used to choose paths in natural complex terrain? To answer this question, we measured eye and body movements while participants walked in natural outdoor environments. We incorporated measurements of the three-dimensional (3D) terrain structure into our analyses and reconstructed the terrain along the walker’s path, applying photogrammetry techniques to the eye tracker’s scene camera videos. Combining these reconstructions with the walker’s body movements, we demonstrate that walkers take terrain structure into account when selecting paths through an environment. We find that they change direction to avoid taking steeper steps that involve large height changes, instead of choosing more circuitous, relatively flat paths. Our data suggest walkers plan the location of individual footholds and plan ahead to select flatter paths. These results provide evidence that locomotor behavior in natural environments is controlled by decision mechanisms that account for multiple factors, including sensory and motor information, costs, and path planning.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>natural locomotion</kwd><kwd>visual control of locomotion</kwd><kwd>photogrammetry</kwd><kwd>visual decisions</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 EY05729</award-id><principal-award-recipient><name><surname>Hayhoe</surname><given-names>Mary M</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>T32 LM012414</award-id><principal-award-recipient><name><surname>Muller</surname><given-names>Karl S</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>K99 EY028229</award-id><principal-award-recipient><name><surname>Matthis</surname><given-names>Jonathan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The visual information walkers use for path selection during locomotion was revealed by analysis of a three-dimensional numerical representation of the natural terrain.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Sensory input guides actions, and in turn, those actions shape the sensory input. Consequently, to develop a sophisticated scientific understanding of even simple actions in the natural world, we must monitor both the sensory input and the actions. While technology for monitoring gaze and body position during natural behavior is both readily available and widely used in vision science and in movement science, the use of technology for the measurement of the visual input in natural environments has been limited. In this paper, we aim to bridge that gap by using photogrammetry techniques from computer vision to reconstruct the environment and subsequently approximate the visual input. The combination of the reconstructions with body pose information and gaze data allows a full specification of how walkers interact with complex real-world environments. These data can help expand our understanding of how visual information about the structure of the environment drives locomotion via sensorimotor decision-making.</p><p>Natural visually guided behaviors, like visually guided walking, can be characterized as a sequence of complex sensorimotor decisions (<xref ref-type="bibr" rid="bib10">Hayhoe, 2017</xref>; <xref ref-type="bibr" rid="bib8">Gallivan et al., 2018</xref>; <xref ref-type="bibr" rid="bib5">Domínguez-Zamora and Marigold, 2021</xref>). However, much of our current understanding of locomotion comes from work characterizing steady-state walking in laboratory settings – most commonly with participants walking on treadmills. That work has shown that humans converge toward energetic optima. For example, walkers adopt a preferred gait that constitutes an energetic minimum given their own biomechanics (<xref ref-type="bibr" rid="bib31">Warren, 1984</xref>; <xref ref-type="bibr" rid="bib32">Warren et al., 1986</xref>; <xref ref-type="bibr" rid="bib13">Kuo et al., 2005</xref>; <xref ref-type="bibr" rid="bib27">Selinger et al., 2015</xref>; <xref ref-type="bibr" rid="bib6">Finley et al., 2013</xref>; <xref ref-type="bibr" rid="bib14">Lee and Harris, 2018</xref>; <xref ref-type="bibr" rid="bib26">Rock et al., 2018</xref>; <xref ref-type="bibr" rid="bib34">Yokoyama et al., 2018</xref>; <xref ref-type="bibr" rid="bib21">O’Connor et al., 2012</xref>).</p><p>There are a number of problems in generalizing these findings to walking in natural environments. In particular, locomotion over rough terrain depends on both the biomechanics of the walker and visual information about the structure of the environment. When the terrain is more complex, walkers use visual information to find stable footholds (<xref ref-type="bibr" rid="bib18">Matthis et al., 2018</xref>). There are also other factors to consider, such as the need to reach a goal or attend to the navigational context (<xref ref-type="bibr" rid="bib33">Warren et al., 2001</xref>; <xref ref-type="bibr" rid="bib25">Rio et al., 2014</xref>; <xref ref-type="bibr" rid="bib16">Logan et al., 2010</xref>; <xref ref-type="bibr" rid="bib22">Patla and Vickers, 1997</xref>). Thus, the sensorimotor decisions in natural locomotion will be shaped by more complex cost functions than in treadmill walking. Furthermore, in the face of this complexity, individuals may be adopting heuristics rather than converging upon optimal solutions.</p><p>Previous studies tracking the eyes during outdoor walking have found that gaze patterns change with the demands of the terrain (<xref ref-type="bibr" rid="bib23">Pelz and Rothkopf, 2007</xref>; <xref ref-type="bibr" rid="bib7">Foulsham et al., 2011</xref>; ’t <xref ref-type="bibr" rid="bib29">’t Hart and Einhäuser, 2012</xref>). However, in those studies, foot placement was not measured, making it impossible to analyze the relationship between gaze and foot placement. Recent work by <xref ref-type="bibr" rid="bib18">Matthis et al., 2018</xref>, and <xref ref-type="bibr" rid="bib3">Bonnen et al., 2021</xref>, integrated gaze and body measurements of walking in outdoor environments. Those papers demonstrated that walkers modulate gait speed in order to gather visual information necessary for the selection of stable footholds as the terrain became more irregular. Walkers spent more time looking at the ground close to their body (2–3 steps ahead) with increasing terrain complexity. While gaze and gait were tightly linked, the absence of terrain measurements made it impossible to ask what visual terrain features walkers use to choose footholds and navigate toward the goal.</p><p>In this paper, we ask how vision is used identify viable footholds and choose paths in natural environments. In particular, what are the visual features of the terrain that underlie path choice? How do walkers use visual information to alter the preferred gait cycle appropriately for the upcoming path? We accomplish this by reconstructing the terrain and aligning the gaze and gait data to that reconstruction. Then, we perform a series of analyses of walkers’ body movements and the terrain, demonstrating that: (a) depth information available to walkers is predictive of upcoming footholds, (b) walkers prefer flatter paths, and (c) walkers choose indirect routes to avoid height changes. These findings shed light on how walkers use visual information to find stable footholds and choose paths, a crucial everyday function of the visual system.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We analyzed data recorded while participants walked over rough terrain (<italic>n</italic>=9). The data were collected by the authors for two separate, previously published studies of visually guided walking (<xref ref-type="bibr" rid="bib3">Bonnen et al., 2021</xref>, <italic>n</italic>=7; <xref ref-type="bibr" rid="bib19">Matthis et al., 2022</xref>, <italic>n</italic>=2). Walkers’ eye and body movements were recorded using a Pupil Labs Core mobile binocular eye tracker and a Motion Shadow full-body motion capture suit. Additionally, the walker’s view of the scene was recorded by the eye tracker’s outward-facing scene camera. As the scene camera moves with the head, the camera’s view of the terrain changes along with the walker’s. Due to those changes in viewpoint, the scene videos contain information about the terrain’s depth structure, which we aimed to recover via photogrammetry.</p><sec id="s2-1"><title>Terrain reconstruction</title><p>To reconstruct the three-dimensional (3D) environment from the scene videos recorded by the eye tracker’s scene camera, we used the photogrammetry software package Meshroom (<xref ref-type="bibr" rid="bib9">Griwodz et al., 2021</xref>), which combines multiple image processing and computer vision algorithms. The terrain reconstruction procedure uses the viewpoint changes across each scene video’s frames to recover the environment’s depth structure. The outputs, generated per scene video, are: (a) a 3D textured mesh of the terrain and (b) an estimate of the 6D camera pose (both 3D location and 3D orientation) within the terrain’s coordinate system. To give a sense of the quality of these reconstructions, <xref ref-type="fig" rid="fig1">Figure 1</xref> shows an example comparison of (A) the original scene camera image and (B) a corresponding view of the reconstructed terrain.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Example comparison of original and rendered video frames.</title><p>We used the scene videos recorded by the eye tracker’s outward-facing camera to estimate the structure of the environment and the scene camera’s pose in each frame of the video. By moving a virtual camera to those poses and rendering the camera’s view of the textured mesh, we can generate comparison images to help assess the reconstruction’s accuracy. (<bold>A</bold>) Frame from original scene video. (<bold>B</bold>) Corresponding rendered image.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig1-v1.tif"/></fig></sec><sec id="s2-2"><title>Aligned gaze, body, and terrain data</title><p>We aimed to analyze the body and gaze data in the context of the reconstructed environment, which meant that we needed to align our data on the walker’s movements to the terrain’s coordinate system. We determined how to position the eye and body movement data relative to the terrain by aligning the head pose measured by the motion capture system to the estimated scene camera pose (<xref ref-type="fig" rid="fig2">Figure 2</xref>). To visualize the fully aligned data, we created videos showing the walker’s skeleton moving through the associated textured terrain mesh (for an example, see <xref ref-type="video" rid="video1">Video 1</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Alignment of motion capture data and terrain reconstruction.</title><p>We combine the motion capture data with the reconstructed environment (photogrammetry data) by aligning the head’s pose (RGB axes) to the scene camera’s pose (CMY axes). (<bold>A</bold>) Motion capture data provides body pose (i.e. position and orientation) information, including the head’s pose (RGB axes). (<bold>B</bold>) The process of reconstructing the environment via photogrammetry produces a three-dimensional (3D) terrain mesh (image) and scene camera’s poses (CMY axes). (<bold>C</bold>) Aligning the head’s pose (RGB axes) to the scene camera’s pose (CMY axes) places the body within the terrain reconstruction.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig2-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-91243-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Visualization of the aligned motion capture, eye tracking, and terrain data for one traversal of the Austin trail: <ext-link ext-link-type="uri" xlink:href="https://youtu.be/TzrA_iEtj1s">https://youtu.be/TzrA_iEtj1s</ext-link>.</title><p>The video shows the three-dimensional (3D) motion capture skeleton walking over the textured mesh. Gaze vectors are illustrated as blue lines. On the terrain surface, the heatmap shows gaze density, and the magenta dots represent foothold locations.</p></caption></media><p>We also visualized the different paths that walkers took through the terrain. <xref ref-type="fig" rid="fig3">Figure 3</xref> shows an overhead view of the reconstructed terrains from the Austin dataset, with the paths chosen by the two Austin subjects overlaid onto the terrain. (For examples from the Berkeley dataset, see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). The recorded paths were certainly not identical, indicating that foothold locations were not highly constrained. However, the two subjects’ paths show considerable regularities. Visual inspection of the paths, particularly in 3D, gives the impression that the terrain’s structure impacts the regularity of paths. In other words, features of the 3D environment might impact the degree of variability between paths, suggesting that there may be some identifiable visual features that underlie path choices.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Repeated walks across rough terrain in the Austin data.</title><p>Each of the two Austin subjects walked out and back over a rocky trail three times. These overhead views show the textured three-dimensional (3D) terrain mesh along with the paths the subjects took through that terrain. Each color (red and cyan) corresponds to a different subject. Note that in some sections of the terrain, paths were highly similar across repetitions and across subjects, while in other sections, paths differed notably. (<bold>A</bold>) Both subjects’ three walks from the start of the path to the end. (<bold>B</bold>) Both subjects’ three walks returning to the start location.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Berkeley path consistency, convergence and divergence.</title><p>(<bold>A</bold>) Overhead view of paths taken (colored lines correspond to individual subjects) through a portion of the Berkeley terrain. (<bold>B–D</bold>) These panels show examples of path convergence and divergence. The colored lines indicate the paths that subjects traveled in this section of the terrain, with each color representing a different subject. For each path, the walk in this section of terrain begins at the tick mark near the bottom of the image and ends at the colored <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>▴</mml:mi></mml:mstyle></mml:math></inline-formula> In (<bold>B</bold>), subjects diverge by choosing two different routes around a root, but then converge again. In (<bold>C</bold>) subject paths converge to avoid the large outcrop. In (<bold>D</bold>) subject paths converge around a mossy section of a large rock.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig3-figsupp1-v1.tif"/></fig></fig-group><p>To further illustrate the information present in our dataset, <xref ref-type="fig" rid="fig4">Figure 4</xref> shows an excerpt of the terrain from <xref ref-type="fig" rid="fig3">Figure 3</xref> with the following aligned data: gaze locations (green and blue dots), foothold locations (pink dots), and head locations (orange dots).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Gaze and body data embedded in the corresponding reconstructed terrain.</title><p>This overhead view shows a representative excerpt of 20 steps from one of the Austin traversals. The walker was, in this overhead view, moving left to right. Dots mark the footholds locations (pink), gaze locations near the path (green), gaze locations off the path (blue), and head locations (orange). To illustrate the relationship between ‘off-the-path‘ gaze and head location, blue lines connect each blue gaze point to the simultaneous location of the head.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig4-v1.tif"/></fig><p>The close relationship between the gaze locations near the path (green dots) and the foothold locations was the concern of the investigations that generated our dataset: <xref ref-type="bibr" rid="bib18">Matthis et al., 2018</xref>, and <xref ref-type="bibr" rid="bib3">Bonnen et al., 2021</xref>. Those studies showed that gaze was clustered most densely in the region 2–3 steps ahead of the walker’s current foothold and ranged between 1 and 5 steps ahead. In other words, we previously found that walkers look close to the locations where the feet will be placed, up to five steps ahead of their current location.</p><p>Relevant to the work in this article are the gaze locations ‘off-the-path’ (blue dots) and the concurrent head locations (connected by blue lines). Those gaze points are off of the walker’s chosen path but are still on the ground. Further, they seem to precede turns – a pattern which we observed throughout the data. In later sections, we provide evidence that walkers make a trade-off between maintaining a straight path vs. maintaining a flat path; this gaze pattern points to how the visual system might collect the information used to make that trade-off.</p></sec><sec id="s2-3"><title>Benefits of reconstructing terrain</title><p>Incorporating the reconstructed terrain into our analyses has several advantages. Perhaps the most obvious is that having information about the terrain’s depth structure allows us to analyze the relationship between that depth structure and the walkers’ chosen foothold locations. Another major advantage of incorporating the terrain reconstruction is that it enables more accurate gaze and body localization. In previous work, we assumed a flat ground plane, which led to parallax error in gaze location estimates (<xref ref-type="bibr" rid="bib18">Matthis et al., 2018</xref>). Here, we used the reconstructed 3D terrain surface, eliminating the need to assume a flat ground plane and thus eliminating that source of error. Additionally, body position estimates were previously negatively impacted by inertial measurement unit (IMU) drift, which results from the accumulation of small errors in the accelerometer and gyroscope measurements over time. This drift causes global, i.e., not local error – error in the overall localization of the motion capture skeleton, not in the localization of different body parts relative to one another. We were able to address this source of error by fixing the body’s reference frame to that of the environment (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Thus, by eliminating both of these sources of error, utilizing photogrammetry allowed us to more precisely estimate gaze and foothold locations.</p></sec><sec id="s2-4"><title>Evaluating reliability of terrain reconstruction</title><p>To evaluate the reliability of the 3D reconstruction procedure, we compared the terrain meshes calculated from multiple traversals of the same terrain. We used the Austin dataset for this reliability analysis because the terrain is contiguous, the walking paths have clearer start/end points, and there are 12 traversals of the terrain (2 subjects, 6 traversals each; see <xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><p>Since we performed the reconstruction procedure on each traversal separately, we generated 12 Austin meshes that represented the same physical terrain. Each mesh contained a cloud of points, which we aligned and compared using CloudCompare (<ext-link ext-link-type="uri" xlink:href="https://www.danielgm.net/cc/">https://www.danielgm.net/cc/</ext-link>). For each pair of meshes, one mesh served as the baseline mesh, and one mesh served as the comparison mesh. For each point in the baseline mesh that was within 2 m of the walking path, we found its nearest neighbor in the comparison mesh and calculated the distance, resulting in a distribution of distances (errors) between the two meshes.</p><p>The aggregate distribution across all pairwise mesh comparisons is shown in <xref ref-type="fig" rid="fig5">Figure 5A</xref>. The median error was 4.5 cm, and the 95% quantile was 20.0 cm. To evaluate reliability specifically at footholds, we also isolated the points in each baseline mesh associated with foothold locations (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). For the foothold locations, the median error was 4.0 cm, and the 95% quantile was 16.8 cm. To put these numbers into context, the average foot length of a person from North America is 25.8 cm (<xref ref-type="bibr" rid="bib11">Jurca et al., 2019</xref>), so in both cases, the majority of mesh errors fall below 20% of the average foot length. Thus, our terrain reconstruction procedure produces reasonably reliable reconstructions of a walker’s 3D environment.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Accuracy of terrain reconstructions.</title><p>(<bold>A</bold>) Nearest neighbor error distribution for the whole terrain (median = 4.5 cm, 95% quantile = 20.0 cm). (<bold>B</bold>) Nearest neighbor error distribution for individual footholds (median = 4.0 cm, 95% quantile = 16.8 cm).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig5-v1.tif"/></fig></sec><sec id="s2-5"><title>Retinocentric depth information affects foothold selection</title><p>The results of <xref ref-type="bibr" rid="bib3">Bonnen et al., 2021</xref>, suggest depth judgments are important in foothold finding, as the removal of depth information shifts gaze to foothold locations that are closer to the walker. Taking advantage of the output of the terrain reconstruction procedure, we sought to confirm that depth information from the walker’s point of view could be used to predict the upcoming foothold locations.</p><p>We first used the reconstructed terrain – along with the aligned foothold and gaze information – to generate retinocentric depth images that approximate the visual information subjects have access to during walking (e.g. see Figure 11). Note that for each frame in the training dataset, the camera field of view includes multiple future footholds (up to 5; depicted as green circles in <xref ref-type="fig" rid="fig6">Figure 6A and B</xref>). We used the location of the footholds in these retinocentric depth images to create a training dataset. If a convolutional neural network (CNN) can predict foothold locations above chance based on these retinocentric depth images, that would suggest that terrain depth structure plays a role in foothold selection.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Predicting foothold locations from depth information.</title><p>A convolutional neural network (CNN) was trained to predict foothold locations in retinocentric depth images. (<bold>A</bold>) Example retinocentric depth image associated with relatively good CNN performance (i.e. a high area under the ROC curve [AUC] value). The image is overlaid with the foothold locations (green) and a heatmap showing the CNN’s likelihood output, which indicates the likelihood of finding a foothold in a particular location. (<bold>B</bold>) Example retinocentric depth image associated with relatively poor CNN performance (i.e. a low AUC value). (<bold>C</bold>) Median AUC values for the data of each of the nine participants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig6-v1.tif"/></fig><p>Per subject, we trained the network on half of the terrain and tested on the remaining half (ensuring that the network was tested on terrain that it had not previously seen). For each depth image, we calculated the area under the ROC curve (AUC), which quantifies the discriminability between image locations that show footholds and image locations that do not show footholds. <xref ref-type="fig" rid="fig6">Figure 6C</xref> shows that, per subject, the median AUC value for depth images from the test set was above chance. We can thus conclude that the network was able to find the potential footholds in the depth images, suggesting that retinocentric depth information contributes to foothold finding.</p></sec><sec id="s2-6"><title>Walkers prefer flatter paths</title><p>Our CNN analysis suggested that depth features in the upcoming terrain have some predictive value in the selection of footholds. We next decided to narrow our focus to examine whether terrain height, specifically, might play a role in foothold selection. Stepping up and down is energetically costly, and previous work in simpler environments has demonstrated that humans attempt to minimize energy expenditure during locomotion (<xref ref-type="bibr" rid="bib27">Selinger et al., 2015</xref>; <xref ref-type="bibr" rid="bib6">Finley et al., 2013</xref>; <xref ref-type="bibr" rid="bib14">Lee and Harris, 2018</xref>). Furthermore, a walker avoiding large steps up and down would be choosing to take a flatter path, and walking on a flatter path would result in less deviation from their preferred gait cycle and thus more stable locomotion. To test the hypothesis that walkers seek out a flatter path to avoid large changes in terrain height, we measured the slope of steps chosen by our walkers and compared them to the slope of steps in paths we simulated along the same terrain.</p><p>We simulated plausible paths for each walker that were comparable to their chosen paths. To ensure plausibility, we constrained potential foothold locations and potential steps based on human walking behavior. We identified potential foothold locations based on the maximum walk-on-able surface slant reported in <xref ref-type="bibr" rid="bib12">Kinsella-Shaw et al., 1992</xref>, excluding areas of the terrain with an average local surface slant greater than 33°. We identified potential steps between foothold locations based on each walker’s data, excluding steps with a step slope (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>), step ground distance (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>), and/or step deviation from goal (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>) greater than the corresponding maximum absolute value for that walker’s chosen steps (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Only foothold locations and steps that met these conditions were considered viable.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Step parameter distributions help define feasible alternative paths.</title><p>The histograms show the distributions of (<bold>A</bold>) step slopes, (<bold>B</bold>) step lengths, and (<bold>C</bold>) step direction relative to goal direction. These distributions define the set of feasible next steps for a given foothold, allowing the calculation of feasible alternative paths to the one actually chosen by the subject. This figure shows histograms of these quantities pooled over subjects, but note that calculations of viable paths were done based on the step parameters of each individual subject.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Per-subject histograms of the step parameters shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>.</title><p>Each row shows one subject’s data. Each column shows data for one of the step parameters used to constrain simulated steps: (1) step slope, (2) step length normalized by the average step length, and (3) step direction relative to goal direction.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig7-figsupp1-v1.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Aggregate histograms of the step parameters defined in the Methods section titled ‘Step analysis‘.</title><p>Top row, from left to right: (1) step distance (m), (2) step ground distance (m), (3) step height (m), (4) step ground distance (step length; see also <xref ref-type="fig" rid="fig7">Figure 7B</xref>). Bottom row: (1) step slope (deg; see also <xref ref-type="fig" rid="fig7">Figure 7A</xref>), (2) step direction (deg), (3) goal direction (deg), (4) step direction relative to goal direction (deg; see also <xref ref-type="fig" rid="fig7">Figure 7C</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig7-figsupp2-v1.tif"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 3.</label><caption><title>Scatterplots between all parameters defined in the Methods section titled ‘Step analysis‘.</title><p>The order of the step parameters along the <italic>x</italic>- and <italic>y</italic>-subplot axes are: (1) step distance, (2) step ground distance, (3) step height, (4) step slope, (5) step direction, (6) goal direction, (7) step direction relative to goal direction, and (8) step ground distance in step lengths. The title of each subplot contains the correlation value <italic>r</italic> and its associated p-value.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig7-figsupp3-v1.tif"/></fig></fig-group><p>To ensure comparability, we split each walker’s path into 5-step (i.e. 6-foothold) segments, and we simulated corresponding 5-step sequences by starting at the walker’s chosen foothold and randomly choosing each subsequent step from the available viable options (<xref ref-type="fig" rid="fig8">Figure 8A</xref>). (Note that, for this simulation, we did not predefine the end points of path segments).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Paths chosen by walkers have a lower step slope.</title><p>We simulated path segments composed of viable steps and compared them to subjects’ chosen step sequences. (<bold>A</bold>) Overhead view of an example chosen step sequence (magenta), along with a subset of the corresponding simulated viable step sequences (yellow). The cyan and red lines show the walker’s skeleton. (<bold>B</bold>) Histograms of mean step slope for chosen and simulated step sequences for one participant.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig8-v1.tif"/></fig><p>After simulating walkable path segments that we could directly compare with walkers’ chosen path segments, we calculated the overall slope of each path segment by averaging the absolute slope of the steps in the sequence. The resulting path segment slope values quantify the net flatness of each path. The per-subject median chosen path segment slope ranged from 7.7° to 11.8°, with a mean of 9.5° and a standard deviation of 1.7°. This corresponds to quite a small change in height; for a step of average length, a 9.5° slope corresponds to a height change of just a few inches. The per-subject median simulated path segment slope ranged from 9.1° to 19.2°, with a mean of 14.9° and a standard deviation of 3.3°.</p><p>As is evident from the per-subject medians, the slopes of chosen path segments tended to be lower than the slopes of simulated path segments, consistent with the idea that walkers seek to minimize energetic costs by taking flatter routes. The bias in the chosen path segment slope distribution toward lower slopes (vs. the simulated distribution) can be seen in <xref ref-type="fig" rid="fig8">Figure 8</xref>, where both path segment slope distributions for one example subject are plotted. For every subject, their median chosen path segment slope was lower than their median simulated path slope, with the simulated slopes being 5.56° larger on average (SD = 2.18°). A paired sample <italic>t</italic>-test confirmed that the differences between the two medians were statistically significant, <italic>t</italic>(8) = 7.64, p&lt;&lt;0.001.</p><p>Our results suggest that walkers prefer taking flatter paths. This does not mean that they categorically avoid large steps up and down. Clearly they do sometimes choose paths with greater slopes, as is indicated by the tail of the chosen distribution. However, on average they tend toward flatter paths than would be predicted by the terrain alone.</p></sec><sec id="s2-7"><title>Walkers choose indirect routes to avoid height changes</title><p>Another factor that influences the energetic cost of taking a particular path is how straight the path is. Changing direction requires more energy than walking on an equivalent straight path (<xref ref-type="bibr" rid="bib20">McNarry et al., 2017</xref>), as one might expect since curvier (i.e. more tortuous) paths are longer and require walkers to deviate from their preferred gait cycle. However, walkers frequently alter their direction of travel in rocky terrain (see, e.g., <xref ref-type="fig" rid="fig4">Figure 4</xref>). If there is a large height change along the straight path, turning might require less energy than stepping up or down while following the straight path. Therefore, building on our finding that walkers prefer flatter paths, we hypothesized that walkers choose to turn when turning allows them to avoid notable changes in terrain height. We evaluated that possibility by examining the relationship between the tortuosity of their chosen path segments and the slope of corresponding straight alternative path segments.</p><p>As in the prior section, we simulated path segments to compare walkers’ chosen steps to the viable steps along that specific terrain. We followed a similar procedure with one notable difference: Here, for each chosen path segment, we simulated steps between the first and sixth footholds in the chosen path segment. In other words, we predefined both the starting and ending locations of simulated path segments (<xref ref-type="fig" rid="fig9">Figure 9A</xref>), whereas above, we predefined only the starting locations (<xref ref-type="fig" rid="fig8">Figure 8A</xref>).</p><fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Average tortuosity of chosen path increases with increased straight path slope.</title><p>(<bold>A</bold>) An example chosen path segment (cyan; 5-step sequence), along with one straighter alternative path segment (green). (<bold>B</bold>) An illustration of the relationship between chosen path segment tortuosity and the slope of ‘straight’ path segments that were simulated across the same terrain. Each subpanel depicts one subject’s data. To summarize the large amount of data per subject (317,497 path segments), we binned the data into 20 quantiles of straight path slope and averaged tortuosity per bin, generating one summary tortuosity value per slope level. These scatterplots show the average tortuosity as a function of the straight path slope quantile (cyan crosses), along with best fit lines (black). (For scatterplots showing data per chosen path segment, see <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1A</xref>). Associated correlation values (Pearson’s <italic>r</italic>) are shown at the top of each panel.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig9-v1.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 1.</label><caption><title>Relationship between average straight path segment slope and the tortuosity of each chosen path segment.</title><p>(<bold>A</bold>) Tortuosity of the chosen path segments (5-step sequences) vs. slope <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> for the simulated ‘straight‘ path segments for each of the nine subjects. Correlation values and corresponding p-values are indicated at the top of each panel. (<bold>B</bold>) Correlation between subject leg length and the strength of the straight path slope vs. tortuosity relationship in their foothold selection data. Leg lengths (in cm) are plotted on the horizontal axis, and the correlation coefficients for each of the plots in panel A are plotted on the vertical axis. We found a statistically significant negative correlation between those two values (<italic>r</italic> = –0.70, p=0.04).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig9-figsupp1-v1.tif"/></fig><fig id="fig9s2" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 2.</label><caption><title>Distributions of straight path slope and chosen path segment tortuosity.</title><p>Each row presents one subject’s data. The left column contains histograms of the average slope <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> for simulated ‘straight‘ path segments, and the right column contains histograms of the tortuosity of that subject’s chosen path segments (5-step sequences).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig9-figsupp2-v1.tif"/></fig></fig-group><p>We used the simulated path segments to quantify, for each chosen path segment, the average step slope a subject would encounter if they tried to take a straighter path between the segment’s end points. To accomplish that, we quantified the straightness of all path segments via a tortuosity metric (<xref ref-type="bibr" rid="bib1">Batschelet, 1981</xref>; <xref ref-type="bibr" rid="bib2">Benhamou, 2004</xref>), and per terrain mesh, we used the tortuosity of chosen path segments to compute a conservative ‘straightness’ threshold. We then selected the simulated path segments with a tortuosity below the computed threshold, computed their slope, and averaged across those path segment slopes. Those calculations resulted in, for each path segment, the mean slope of relatively straight alternative paths, which we refer to here as ‘straight path slope’. <xref ref-type="fig" rid="fig9">Figure 9A</xref> shows one example straight path segment, together with the path that the subject actually chose.</p><p>To determine whether subjects chose longer paths as the slope of relatively straight options increased, we compared path segment tortuosity to the corresponding straight path slope. The tortuosity of chosen path segments ranged from 1 (which denotes a straight path) to 14.73 (which denotes a quite curvy path), with a mean of 1.09 and a standard deviation of 0.29. For all walkers in our dataset, the distribution of tortuosity values was concentrated near 1, with median values per subject of 1.041.07 (M=1.06, SD = 0.01; <xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>, left). The simulated straight path slopes ranged from 1.89° to 21.72°, with a mean of 10.31° and a standard deviation of 3.44°. There was some variability in the per-subject distributions, with median values of 7.83°–11.94° (M=10.12°, SD=1.20°; <xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>, right), but the primary difference was between Austin and Berkeley participants, suggesting that variability was at least somewhat a function of the terrain.</p><p>We expected that walkers would choose curvier (i.e. more tortuous) paths when the relatively straight alternatives were also relatively steep (i.e. have a relatively high slope). Thus, we expected that, when directly comparing the tortuosity and straight path slope values for each chosen path segment, (a) relatively low straight path slope values would be associated with tortuosity values fairly close to 1 and (b) tortuosity values would tend to increase as straight path slope increased. For each subject, we calculated the average tortuosity of the chosen paths for different levels of ‘straight path slope’ (<xref ref-type="fig" rid="fig9">Figure 9B</xref>), and we found that, indeed, (a) average tortuosity values were near 1 at the lowest slope level and average tortuosity increased across increasing quantiles of straight path slope. This relationship suggests that walkers made a trade-off, choosing paths that were flatter but more tortuous over paths that were steeper but more direct. Such choices may reflect decisions that the cost of taking the longer, flatter path is ultimately less than cost of taking the straighter, steeper path. All subjects show this relationship, though its strength does vary between subjects.</p><p>The energetic cost of taking steeper steps likely varies with factors affecting a walker’s biomechanics. The question of whether a flatter, more torturous path is a more energetically efficient path than a steeper, straighter path likely has a walker-specific answer. Therefore, one might expect that the some of the between-subject variability in the strength of the slope-tortuosity correlation is due to biomechanically relevant factors, such as the walker’s leg length. We thus asked whether the strength of the trade-off between tortuosity and straight path slope (i.e. the correlation coefficient) varied with walkers’ leg lengths. Subject leg lengths ranged from 81 cm to 103.5 cm, and slope-tortuosity correlation values ranged from 81 cm to 103.5 cm (<xref ref-type="fig" rid="fig9">Figure 9B</xref>). Longer leg lengths were associated with lower correlation coefficients (<italic>r</italic>=–0.83, p=0.005; <xref ref-type="fig" rid="fig10">Figure 10</xref>), suggesting that subjects with the shortest legs are more likely to choose longer paths when the straight path becomes less flat (i.e. with increasing values of straight path slope).</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Relationship between leg length and the correlation between straight path step slope and path tortuosity.</title><p>Subjects’ leg lengths (in cm) are plotted on the horizontal axis. The correlation coefficients drawn from the analyses depicted in <xref ref-type="fig" rid="fig9">Figure 9B</xref> are plotted on the vertical axis. The scatterplot shows one point per subject (black crosses). The linear trendline is also shown (red line). We found that there was a statistically significant negative correlation between subjects’ leg lengths and the straight path slope vs. average path tortuosity correlations in their data (<italic>r</italic>=–0.83, p=0.005). (For a comparable plot showing the correlations derived from data per chosen path segment, see <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1B</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig10-v1.tif"/></fig><p>Note that <xref ref-type="fig" rid="fig9">Figure 9</xref> and <xref ref-type="fig" rid="fig10">Figure 10</xref> both use the aforementioned binned data, with the average tortuosity of chosen paths calculated for each of 20 ‘straight path slope’ quantiles to summarize the per-path-segment data. Parallel plots made using the per-path-segment data are shown in <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>. Importantly, the analysis of per-path-segment data reveals similar relationships as those described above. For eight of the nine subjects, the correlations are significant. The correlation coefficients between straight path slope and chosen path tortuosity are substantially smaller for the per-path-segment data. The lower correlation values result from the amount of variability across path segments. The variability can be seen in the spread of the points shown in <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1A</xref> (vs. <xref ref-type="fig" rid="fig9">Figure 9</xref>), but note that even those subplots do not show the full extent of the variability. Those scatterplots include the full range of straight path slope values (1.89–21.72°) but, in effect, omit the tail of the tortuosity distribution (max = 14.73) to ensure that the majority of the data is readily visible. This cross-path variability suggests that subjects are not using strict criteria to make decisions about the trade-off between path slope and path curvature. Rather, the trends we observe likely reflect learned heuristics about which paths are more or less preferable, which walkers can use to flexibly select paths.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this work, we present novel analyses of natural terrain navigation that take advantage of the 3D terrain reconstructions we generated using photogrammetry. The terrain reconstructions allowed for greater precision than was possible in previous studies of walking in natural outdoor environments (<xref ref-type="bibr" rid="bib18">Matthis et al., 2018</xref>; <xref ref-type="bibr" rid="bib3">Bonnen et al., 2021</xref>). We were able to more accurately calculate both gaze and foothold locations. Most importantly, the quantification of terrain geometry allowed us to examine how the structure of the visual environment influences foothold selection. An analysis of this relationship – between the structure of the visual environment and selected footholds/paths – has been missing in much previous work on visually guided action in the natural world, where the depth structure is typically not measured.</p><p>After developing the reconstruction and data alignment procedure, our next challenge was to develop a strategy for identifying visual features that influenced subjects’ foothold and path selection. We noted regularities in the paths chosen by walkers, both across individuals and across repeats of the same walk, suggesting that there are some terrain features that serve as a basis for path choice. Previous work suggested a role for depth features in visually guided walking (<xref ref-type="bibr" rid="bib3">Bonnen et al., 2021</xref>). Using a CNN to predict foothold locations on the basis of retinocentric depth images, we confirmed a role for depth information in foothold selection. This result justified the further exploration of depth variation in the terrain (e.g. changes in terrain height) as a potential feature used by walkers in foothold selection.</p><p>To ask whether changes in terrain height (i.e. depth structure) influenced path selection, we simulated viable paths that could be compared with the chosen paths. Comparing the sets of chosen and viable paths, we found that walkers prefer flatter paths and avoid regions with large height irregularities. While in some ways this might not be a surprising result, the data reveal that this is a strong constraint on path choice. The median slope of 5-step paths was less than 10°, which corresponds to a quite small height change of about 14–17 cm.</p><p>This work did not investigate which depth cues walkers used to make these path choices, but we highlight that gap in knowledge here as an avenue ripe for future study. A variety of depth cues might be relevant to such sensorimotor decisions, including motion parallax generated by the movement of the head, binocular disparity, local surface slant, and the size of the step-able area. Determining how depth cues are used to make these sensorimotor decisions will require more controlled experiments.</p><p>Finally, we observed that walkers chose longer paths when the straightest viable paths involved greater height changes (<xref ref-type="fig" rid="fig9">Figure 9</xref>), and further, our data suggest that walkers were more likely to choose longer paths if their legs were shorter (<xref ref-type="fig" rid="fig10">Figure 10</xref>). This suggests that the sensorimotor decision-making that supports walking complex terrain is highly body-specific, taking into account the details of a walker’s body, like leg length. This suggests that any cost function or model describing the sensorimotor decision-making processes that support walking in complex natural terrains will also need to be body-specific.</p><sec id="s3-1"><title>Cost functions in visually guided walking</title><p>While we do not know what contributes to the internal cost functions that determine walkers’ choices, the preference for flatter paths is likely driven in part by the energetic cost of stepping up or down. On flat ground, humans converge to an energetic optimum consistent with their passive dynamics (<xref ref-type="bibr" rid="bib13">Kuo et al., 2005</xref>; <xref ref-type="bibr" rid="bib27">Selinger et al., 2015</xref>; <xref ref-type="bibr" rid="bib6">Finley et al., 2013</xref>; <xref ref-type="bibr" rid="bib14">Lee and Harris, 2018</xref>). Deviations from this gait pattern, including turns and changes in speed, are energetically costly (<xref ref-type="bibr" rid="bib30">Voloshina et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Soule and Goldman, 1972</xref>). Recent work by <xref ref-type="bibr" rid="bib4">Darici and Kuo, 2023</xref>, also showed that subjects are able to minimize energetic cost on uneven ground planes by modulating speed. Our findings suggest that walkers may be adjusting their behavior to minimize energetic costs in natural outdoor terrains as well. Future work should examine more directly how particular walking decisions impact energetic costs in natural outdoor terrains.</p></sec><sec id="s3-2"><title>Path planning</title><p>Our analyses show that vision is used to locate flatter paths in upcoming steps. We found that the average step slope of the chosen path was significantly lower than simulated paths, suggesting that walkers were intentionally maintaining a flatter path. Furthermore, our findings suggest that walkers turn to avoid paths with large changes in terrain height. To accomplish this, walkers must plan ahead, demonstrating that planning is an important component of path selection in rugged terrain. Though this study has not explicitly examined the role of gaze in walking, future studies of gaze during walking will be critical to understanding how individuals perform path planning.</p><p>Laboratory studies suggest that walkers need to look 2 steps ahead to preserve inverted pendulum dynamics (<xref ref-type="bibr" rid="bib17">Matthis et al., 2017</xref>). Biomechanical models indicate that walkers can adjust their gait to accommodate upcoming obstacles and may plan up to 8 or 9 steps ahead (<xref ref-type="bibr" rid="bib4">Darici and Kuo, 2023</xref>). Our previous work studying gaze suggests that, in rocky terrain, walkers distribute most of their gaze on the ground to footholds up to 5 steps ahead (<xref ref-type="bibr" rid="bib18">Matthis et al., 2018</xref>; <xref ref-type="bibr" rid="bib3">Bonnen et al., 2021</xref>). Because of the differences between these studies, it is difficult to say exactly what causes the discrepancy (5 steps vs. 9 steps) in the planning horizons reported in these two studies. However, there are notable differences between the laboratory obstacle paths they used and our natural environments. Their walking paths involved height changes of no more than 7.5 cm, the surfaces themselves were flat, and the path required no changes in direction. Our terrains involved greater height changes, irregular and sloping surfaces, large boulders, and frequent direction changes based on visual information. More complex terrains may also impose a greater load on visual working memory (<xref ref-type="bibr" rid="bib15">Lin and Lin, 2016</xref>). Thus, a shorter planning horizon in our data might be expected as individuals adjust their planning horizon depending on the nature of the terrain. On the other hand, because there is no eye tracking in <xref ref-type="bibr" rid="bib4">Darici and Kuo, 2023</xref>, we cannot rule out the possibility that these two planning horizons are in fact the same – individuals may be able to get information about 8–9 steps ahead from their peripheral vision. More study is needed on the details of planning horizons in walking and how individuals adjust them depending on the task and terrain.</p></sec><sec id="s3-3"><title>Conclusion</title><p>In conclusion, we have integrated eye tracking, motion capture, and photogrammetry to create a visuomotor dataset that includes gaze information, body position data, and accurate 3D terrain representations. The reconstructed 3D terrains were a valuable addition to our methodology because they allowed a much more direct, more precise investigation of the visual terrain features that are used to guide path choice. Previous investigations of walking in natural outdoor environments have been limited to video recordings. The reconstruction and integration procedures outlined in this paper should be generally useful for the analysis of visually guided behavior in natural environments. In our analyses, we observed that visual information about depth appeared to play a role in path choice. Despite the complexity of the sensory-motor decisions in natural, complex terrain, we observed that there were consistencies in the paths walkers chose. In particular, walkers chose to take more indirect routes to achieve flatter paths, which required them to plan ahead. Taken together, these findings suggest that walkers’ locomotor behavior in complex terrain reflects sensorimotor decision mechanisms that involve different costs, sensory and motor information, and path planning.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Experimental data</title><p>The data used here was collected by the authors in two separate studies, conducted in two separate locations: Austin, Texas (<xref ref-type="bibr" rid="bib19">Matthis et al., 2022</xref>) and Berkeley, California (<xref ref-type="bibr" rid="bib3">Bonnen et al., 2021</xref>). The studies were approved by the Institutional Review Boards at the University of Texas at Austin and the University of California, Berkeley, respectively. All participants gave informed consent and were treated according to the principles set forth in the Declaration of Helsinki of the World Medical Association. Both studies used the same eye and body tracking equipment as well as the same data collection methods. Additionally, both included multiple terrain conditions. One terrain condition common to both was rough terrain, which consisted of large rock obstacles with significant height deviations.</p><sec id="s4-1-1"><title>Data selection</title><p>In our combined dataset, we included data from only (a) rough terrain, (b) participants walking with normal or corrected-to-normal vision, and (c) participants with scene videos of sufficiently high quality for terrain reconstruction. We therefore did not include any of the Berkeley data used to study the impact of binocular visual impairments. Further, we excluded one Austin participant and one Berkeley participant because the quality of their scene videos caused issues with the terrain reconstruction process, which was essential for the analyses we describe here. (More specifically, one Austin participant was excluded because the scene camera was angled too far upward, limiting the view of the ground, and one Berkeley participant was excluded because their scene videos were too low contrast due to the dim outdoor lighting conditions at the time of the recording).</p></sec><sec id="s4-1-2"><title>Participants</title><p>We used data from 9 participants: 2 from the Austin study and 7 from the Berkeley study (<xref ref-type="table" rid="table1">Table 1</xref>). All had normal or corrected-to-normal vision. There were 5 male and 4 female subjects. They were 2354 years of age at the time of data collection, with an average age of 31 years (median: 27). Their legs were 81–103.5 cm long, with an average of 93.9 cm (median: 96.5 cm).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Information about participants included in dataset.</title><p>Table includes the location of data collection, key demographics, and the amount of data recorded per participant (quantified as the number of steps in rough terrain in participants’ processed data).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Location</th><th align="left" valign="bottom">TX</th><th align="left" valign="bottom">TX</th><th align="left" valign="bottom">CA</th><th align="left" valign="bottom">CA</th><th align="left" valign="bottom">CA</th><th align="left" valign="bottom">CA</th><th align="left" valign="bottom">CA</th><th align="left" valign="bottom">CA</th><th align="left" valign="bottom">CA</th></tr></thead><tbody><tr><td align="left" valign="bottom">Age</td><td align="left" valign="bottom">23</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">27</td><td align="left" valign="bottom">39</td><td align="left" valign="bottom">34</td><td align="left" valign="bottom">29</td><td align="left" valign="bottom">24</td><td align="left" valign="bottom">24</td><td align="left" valign="bottom">54</td></tr><tr><td align="left" valign="bottom">Gender</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">M</td><td align="left" valign="bottom">M</td><td align="left" valign="bottom">M</td><td align="left" valign="bottom">M</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">M</td></tr><tr><td align="left" valign="bottom">Leg length (cm)</td><td align="left" valign="bottom">89</td><td align="left" valign="bottom">102.5</td><td align="left" valign="bottom">103.5</td><td align="left" valign="bottom">101</td><td align="left" valign="bottom">96.5</td><td align="left" valign="bottom">81</td><td align="left" valign="bottom">85</td><td align="left" valign="bottom">90</td><td align="left" valign="bottom">96.5</td></tr><tr><td align="left" valign="bottom">Step count</td><td align="left" valign="bottom">468</td><td align="left" valign="bottom">347</td><td align="left" valign="bottom">462</td><td align="left" valign="bottom">489</td><td align="left" valign="bottom">385</td><td align="left" valign="bottom">537</td><td align="left" valign="bottom">486</td><td align="left" valign="bottom">603</td><td align="left" valign="bottom">453</td></tr></tbody></table></table-wrap><p>The amount of data recorded per participant varied since they were tasked with walking along loosely defined paths, rather than walking for a fixed duration, number of steps, etc. In <xref ref-type="table" rid="table1">Table 1</xref>, we represent the amount of data recorded via the number of steps in rough terrain in each participant’s processed data. In total, the dataset included 4230 steps. Per participant, there were 347–603 steps, with an average of 470 steps (median: 468). Overall, participants with longer legs took fewer steps (<italic>r</italic>=–0.57), and the Berkeley participants took approximately 125 steps more than Austin participants with similar leg lengths.</p></sec><sec id="s4-1-3"><title>Equipment</title><p>Eye movements were recorded using a Pupil Labs Core mobile eye tracker and the Pupil Capture app (Pupil Labs, Berlin, Germany). The eye tracker had two infrared, eye-facing cameras, which recorded videos of the eyes at 120 Hz with 640×480 pixel resolution. Additionally, there was an outward-facing camera mounted 3 cm above the right eye, which recorded the scene in front of the subject at 30 Hz with 1920×1080 pixel resolution and a 100° diagonal field of view. A pair of dilation glasses was fitted over the eyes and eye-facing cameras to protect the infrared eye cameras from interference due to the sun. For participants, this felt like wearing a pair of sunglasses.</p><p>Body movements were recorded using Motion Shadow’s full-body motion capture suit and the Shadow app (Motion Shadow, Seattle, WA, USA). The suit included 17 IMUs, which each contained three three-axis sensors: an accelerometer, a gyroscope, and a magnetometer. The Shadow app recorded data from the suit at 100 Hz and simultaneously estimated the joint poses (i.e. positions and orientations) for the full 30-node 3D skeleton. IMUs were placed on the head, chest, and hips as well as on both the left and right shoulders, upper arms, forearms, hands, thighs, calves, and feet. The 3D skeleton then included nodes for the head, head top, neck, chest, body, hips, mid spine, and low spine as well as the left and right shoulders, arms, forearms, hands, fingers, thighs, legs, feet, toes, heels, and foot pads.</p><p>In addition to the eye tracker and motion capture suit, subjects wore a backpack-mounted laptop, which was used to record all raw data. Importantly, using the same computer to record both data streams meant both were recording timestamps queried from the same internal clock. Their timestamps were therefore already synchronized.</p></sec><sec id="s4-1-4"><title>Task</title><p>At the beginning of each recording, participants performed a 9-point vestibulo-ocular reflex (VOR) calibration task. They were instructed to stand on a calibration mat 1.5 m from a calibration point marked on the mat in front of them. This distance was chosen based on the most frequent gaze distance in front of the body during natural walking in these terrains (<xref ref-type="bibr" rid="bib18">Matthis et al., 2018</xref>). They were instructed to fixate on the calibration point while rotating their head along each of the eight principal winds, i.e., the four cardinal and four ordinal directions. Their resulting VOR eye movements were later used to calibrate the eye tracking data and to spatially align the eye and body data.</p><p>Participants’ primary task was to walk along a trail. The Austin participants walked along a rocky, dried out creek bed in-between two specific points that the experimenters had marked (<xref ref-type="fig" rid="fig3">Figure 3</xref>). They traversed the trail three times in each direction, for a total of 6 traversals per subject. The Berkeley participants walked along a hiking trail in-between two distinctive landmarks. They traversed the trail once in each direction, for a total of 2 traversals per subject. The trail included pavement, flat terrain, medium terrain, and rough terrain, so as with the walk’s start and end points, the experimenters used existing landmarks in the environment to mark the transitions between terrain types. We found the sections of recordings marked as rough terrain and included only that subset of the data in this study.</p></sec><sec id="s4-1-5"><title>Data processing</title><p>Following data collection, we performed a post hoc eye tracking calibration using the 9-point VOR calibration task data. Per subject, we placed a reference marker on the calibration fixation target at 10 timepoints in the recording (corresponding to the 9 points of the VOR calibration task, plus an additional repeat of the center marker at the end). With the Pupil Player app in natural features mode (Pupil Labs, Berlin, Germany), we used those markers to perform gaze mapping, generating 3D gaze vectors for both eyes.</p><p>We then had three sets of tracking data recorded at two different timescales and expressed in three different coordinate systems: the left eye’s gaze data, the right eye’s gaze data, and the 3D skeleton’s pose data. Thus, our next step was to temporally and then spatially align the recordings via a procedure detailed in <xref ref-type="bibr" rid="bib19">Matthis et al., 2022</xref>.</p><p>The timestamps from both systems were already synchronized to the same clock since they were recorded by the same computer, but the sampling rates (motion capture, 100 Hz; eye tracking 120 Hz) and specific timestamps were different. Using MATLAB’s ‘resample’ function (Signal Processing Toolbox; MathWorks, Natick, MA, USA), we performed interpolation so that the motion capture and eye tracking data streams had the same sampling rate and timestamps (120 Hz). The result was that the left eye, right eye, and kinematic data streams were temporally aligned.</p><p>Once the three sets of data were temporally aligned, we used the VOR calibration data to spatially align them. During the VOR task, participants were fixating on a single point while moving their head, so we aligned each eye’s coordinate system to that of the 3D skeleton by shifting and rotating them, such that the eyes were in an appropriate location relative to the head and the gaze vectors remained directed at the calibration fixation target as the head and eyes moved. To determine the shift per eye coordinate system, we estimated the position of each eye’s center in 3D skeleton coordinates. We based our estimate on (a) the position and orientation of the skeleton’s head node and (b) average measurements of the where the eyes are located within the human head. To determine the rotation per eye coordinate system, we found the single optimal rotation that minimized the distance between the calibration fixation target and the gaze vector’s intersection with the mat. Note that because the head’s position and orientation changed throughout the recording, we applied transformations relative to the position and orientation of the skeleton’s head node in each frame.</p><p>Once the eye and body tracking data were fully aligned, we used the data from the body pose foot nodes to find the time and location of each step (both heel strike and toe off), following the velocity-based step-finding procedure outlined in <xref ref-type="bibr" rid="bib35">Zeni et al., 2008</xref>.</p><p>Additionally, we identified periods of time when subjects were potentially collecting visual information by differentiating between when they were fixating and when they were making saccadic eye movements. We only used periods of fixation when considering the visual information available to participants, as mid-saccade visual input is unlikely to be used for locomotor guidance due to saccadic suppression and image blur.</p><p>We identified fixations by applying an eye-in-orbit velocity threshold of 65 °/s and an acceleration threshold of 5 °/s<sup>2</sup>. (Note that the velocity threshold is quite high to avoid including the smooth counter-rotations that occur during eye stabilization). If both values were below threshold for a given frame, we classified that frame as containing a fixation; if not, the frame was classified as containing a saccade.</p></sec></sec><sec id="s4-2"><title>Terrain reconstruction</title><p>To factor terrain height into our analyses, we needed information about the 3D structure of the terrain that participants walked over. Our dataset did not include data on terrain depth, but the scene videos recorded by the eye tracker’s outward-facing camera did provide us with two-dimensional (2D) images of the terrain. In principle, photogrammetry should allow us to extract accurate 3D information about the terrain’s structure from those 2D video frames, so our first step beyond the original analyses of these data (<xref ref-type="bibr" rid="bib3">Bonnen et al., 2021</xref>; <xref ref-type="bibr" rid="bib19">Matthis et al., 2022</xref>) was to use photogrammetry to generate 3D terrain reconstructions.</p><sec id="s4-2-1"><title>Photogrammetry pipeline</title><p>To reconstruct the 3D environment from our 2D videos, we used an open-source software package called Meshroom, which is based on the AliceVision Photogrammetric Computer Vision framework (<xref ref-type="bibr" rid="bib9">Griwodz et al., 2021</xref>). Meshroom combines multiple image processing and computer vision algorithms, ultimately allowing the user to estimate both environmental structure and relative camera position from a series of images.</p><p>The steps in the AliceVision photogrammetry pipeline are (1) natural feature extraction, (2) image matching, (3) features matching, (4) structure from motion, (5) depth maps estimation, (6) meshing, and (7) texturing (<xref ref-type="bibr" rid="bib9">Griwodz et al., 2021</xref>).</p><p>To summarize in greater detail: (1) Features that are minimally variant with respect to viewpoint are extracted from each image. (2) To find images that show the same areas of the scene, images are grouped and matched on the basis of those features. (3) The features themselves are then matched between the two images in each candidate pair. (4) Those feature matches are then used to infer rigid scene structure (3D points) and image pose (position and orientation) via an incremental pipeline that operates on each image pair, uses the best pair to compute an initial two-view reconstruction, and then iteratively extends that reconstruction by adding new views. (5) The inferred 3D points are used to compute a depth value for each pixel in the original images. (6) The depth maps are then merged into a global octree, which is refined through a series of operations that ultimately produce a dense geometric surface representation of the scene. (7) Texture is added to each triangle in the resulting mesh via an approach that factors in each vertex’s visibility and blends candidate pixel values with a bias toward low-frequency texture.</p><p>The most relevant outputs of this pipeline for our analyses are the 3D triangle mesh and the 6D camera trajectory (i.e. the estimated position and orientation of the camera, per input frame). We also used the textured triangle mesh but solely for visualization (e.g. <xref ref-type="fig" rid="fig3">Figure 3</xref>).</p></sec><sec id="s4-2-2"><title>Reconstruction procedure</title><p>Prior to terrain reconstruction, we processed the raw scene videos recorded by the eye tracker’s outward-facing camera. We first used the software package ‘ffmpeg’ to extract the individual frames from the videos. We then undistorted each frame using a camera intrinsic matrix, which we estimated via checkerboard calibration (<xref ref-type="bibr" rid="bib24">Qilong and Pless, 2004</xref>).</p><p>Then, we used Meshroom to process the scene video frames, one traversal at a time, specifying the camera intrinsics (focal length in pixels and viewing angle in degrees) and using Meshroom’s default parameters. Meshroom processed the scene video frames according to the pipeline described above, producing both a terrain mesh and a 6D camera trajectory (3D position and 3D orientation), with one 6D vector for each frame of the original video. To give a sense of the mesh output, we have provided a rendered image of a small section of the textured Meshroom output in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p></sec><sec id="s4-2-3"><title>Data alignment</title><p><xref ref-type="fig" rid="fig2">Figure 2</xref> illustrates the data alignment process which positions the body and eye movement data within the reconstructed terrain. Data alignment was performed on a per-traversal basis. First the body/eye tracking data was translated, pinning the location of the head node to Meshroom’s estimated camera position. Next, the orientation of the head node was matched to Meshroom’s estimated camera orientation by finding a single three-element Euler angle rotation that minimized the L2 error (i.e. the sum of squared errors) across frames using MATLAB’s ‘fminsearch’ function. After applying that rotation, the body/eye tracking data was scaled so that, across all heel strikes in a given recording, the distance between the skeleton’s heel and the closest point on the mesh at the time of that heel strike was minimized.</p><p>A visualization of the aligned motion capture, eye tracking, and terrain data for one traversal of the Austin trail can be seen in <xref ref-type="video" rid="video1">Video 1</xref>. We can also project locations into the scene camera video. <xref ref-type="video" rid="video2">Video 2</xref> shows an example of this, visualizing foothold locations in the scene camera’s view for one traversal of the Austin trial.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-91243-video2.mp4" id="video2"><label>Video 2.</label><caption><title>Visualization of foothold locations in the scene camera’s view for one traversal of the Austin trail: <ext-link ext-link-type="uri" xlink:href="https://youtu.be/llulrzhIAVg">https://youtu.be/llulrzhIAVg</ext-link>.</title><p>Computed foothold locations are marked with cyan dots.</p></caption></media></sec><sec id="s4-2-4"><title>Evaluating terrain reconstruction</title><p>To evaluate the accuracy of the 3D reconstruction, we used the terrain meshes estimated from different traversals of the same terrain, both by an individual subject and also by the different subjects. We used only the Austin data here, as that dataset included 6 traversals per subject (vs. 2) and was collected over a much shorter time span (5 days) and thus was less likely to physically change.</p><p>The meshes were aligned using the open-source software package CloudCompare. To align two meshes in CloudCompare, one mesh needs to be designated as the fixed ‘reference’ mesh and the other as the moving ‘aligned’ mesh (i.e. the mesh that will be moved to align with the reference). We first coarsely aligned the meshes via a similarity transform. That step requires an initial set of keypoints, so we chose five easily discernible features in the environment (e.g. permanent marks on rocks) that were visible in each terrain mesh and manually marked their locations. We then completed the point cloud registration on a finer scale using the iterative closest point method. That procedure involves locating, for each point in the moving mesh, the closest point in the fixed point cloud (i.e. the moving point’s nearest neighbor). The distance between nearest neighbors is then iteratively minimized via best-fitting similarity transforms.</p><p>We can then evaluate the reliability of terrain reconstruction by measuring the distance between nearest neighbors on the two meshes. We make two key comparisons: (1) measuring nearest neighbor distances for all points on the terrain mesh within 2 m of the path (see <xref ref-type="fig" rid="fig5">Figure 5A</xref>) and (2) measuring nearest neighbor distances for all footholds locations by taking the nearest neighbor distance for the mesh point closest to that foothold (see <xref ref-type="fig" rid="fig5">Figure 5B</xref>). For the path comparison, the median error was 4.5 cm, and the 95% quantile was 20.0 cm (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). For the foothold comparison, the median error was 4.0 cm, and the 95% quantile was 16.8 cm (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). To put this into context, the average foot of a person from North America is 25.8 cm (<xref ref-type="bibr" rid="bib11">Jurca et al., 2019</xref>). In both cases, the majority of mesh errors fall below 20% of the average foot size.</p></sec></sec><sec id="s4-3"><title>Retinocentric analysis</title><p>To assess whether depth features can be used to explain some variation in foothold selection, we trained a CNN to predict future foothold locations given the walker’s view of the terrain’s depth. This analysis involved computing both the retinocentric depth images that served as the input data (<xref ref-type="fig" rid="fig11">Figure 11A</xref>) and the foothold likelihood maps that served as target output for training (<xref ref-type="fig" rid="fig11">Figure 11B</xref>). The retinocentric depth images approximate the visual information subjects have when deciding on future foothold locations, and the foothold likelihood maps represent the subjects’ subsequent decisions. After training and testing the CNN, we quantified its prediction accuracy by calculating the AUC. With that metric, scores above chance (50%) would indicate that the depth information plays some role in determining where individuals will put their feet.</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Convolutional neural network (CNN) inputs and outputs.</title><p>Schematic shows the inputs and outputs for one example frame. (<bold>A</bold>) Input: retinocentric depth image. (<bold>B</bold>) Target output: foothold likelihood map. (<bold>C</bold>) Output: predicted foothold locations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig11-v1.tif"/></fig><sec id="s4-3-1"><title>Retinocentric depth images</title><p>We computed subject-perspective depth images (e.g. <xref ref-type="fig" rid="fig11">Figure 11A</xref>) using the aligned eye tracking, motion capture, and photogrammetry data in the open-source 3D computer graphics software Blender. Per subject, we moved a virtual camera through the reconstructed terrain, updating its position and orientation in each frame based on Meshroom’s estimate of the scene camera’s location in the corresponding frame of the scene video. Blender’s ‘Z-buffer’ method then captured an image showing the depth of the 3D triangle mesh representation relative to the camera.</p><p>The resulting egocentric depth images were then transformed to polar coordinates (polar angle <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>θ</mml:mi></mml:mstyle></mml:math></inline-formula> and eccentricity <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula>) to approximate a retinocentric perspective. We defined the diameter of images as 45° of visual angle, and we used 2D interpolation to compute the pixel values at each polar coordinate from the pixel values in Cartesian coordinates.</p><p>The depth values in the images were then converted into relative depth values. To make that shift, we subtracted the gaze point’s depth (i.e. the value at the center pixel) from the entire depth image. The value at center thus became 0, and the rest of the depth values were relative to the depth of the gaze point.</p></sec><sec id="s4-3-2"><title>Foothold likelihood maps</title><p>To calculate the ground truth of the future foothold locations in each depth image, we found the point at which the line between the current camera position and the foothold intersected the camera’s image plane.</p><p>The ground truth foothold locations in the world video frame were converted to likelihood maps (e.g. <xref ref-type="fig" rid="fig11">Figure 11B</xref>) by smoothing foothold locations with a Gaussian kernel: <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula> = 5 pixels. (In degrees of visual angle, the kernel size was roughly 1°. That value is not exact because the conversion between pixels and degrees is not constant throughout the visual field). This smoothing mitigated the impact of any noise in our estimation of foothold location to allow more robustness in the CNN learned features.</p></sec><sec id="s4-3-3"><title>Convolutional neural network</title><p>The retinocentric depth images and foothold likelihood maps were then used to train a custom CNN to predict the probability that each location in the retinocentric depth images was a foothold location. The network input was a depth image (<xref ref-type="fig" rid="fig11">Figure 11A</xref>), and the target output was a foothold likelihood map (<xref ref-type="fig" rid="fig11">Figure 11B</xref>).</p><p>The CNN had a convolutional-deconvolutional architecture with three convolutional layers followed by three transposed convolutional layers (<xref ref-type="table" rid="table2">Table 2</xref>). Training was performed using KL divergence between the CNN output (<xref ref-type="fig" rid="fig11">Figure 11C</xref>) and the foothold likelihood maps (<xref ref-type="fig" rid="fig11">Figure 11B</xref>). Data was split so that half of the pairs of depth images and likelihood maps were used to train the network and the other half was reserved for testing. This split ensured that the network was tested on terrain that it had not previously ‘seen’.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Layers of custom convolutional neural network (CNN).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Layer</th><th align="left" valign="bottom">Output shape</th><th align="left" valign="bottom"># Params</th></tr></thead><tbody><tr><td align="left" valign="bottom">Conv2D</td><td align="left" valign="bottom">(100, 100, 4)</td><td align="left" valign="bottom">404</td></tr><tr><td align="left" valign="bottom">BatchNormalization</td><td align="left" valign="bottom">(100, 100, 4)</td><td align="left" valign="bottom">16</td></tr><tr><td align="left" valign="bottom">MaxPooling2D</td><td align="left" valign="bottom">(50, 50, 4)</td><td align="left" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Conv2D</td><td align="left" valign="bottom">(50, 50, 8)</td><td align="left" valign="bottom">3208</td></tr><tr><td align="left" valign="bottom">BatchNormalization</td><td align="left" valign="bottom">(50, 50, 8)</td><td align="left" valign="bottom">32</td></tr><tr><td align="left" valign="bottom">MaxPooling2D</td><td align="left" valign="bottom">(25, 25, 8)</td><td align="left" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Conv2D</td><td align="left" valign="bottom">(25, 25, 16)</td><td align="left" valign="bottom">12,816</td></tr><tr><td align="left" valign="bottom">BatchNormalization</td><td align="left" valign="bottom">(25, 25, 16)</td><td align="left" valign="bottom">64</td></tr><tr><td align="left" valign="bottom">Conv2DTranspose</td><td align="left" valign="bottom">(25, 25, 16)</td><td align="left" valign="bottom">25,616</td></tr><tr><td align="left" valign="bottom">BatchNormalization</td><td align="left" valign="bottom">(25, 25, 16)</td><td align="left" valign="bottom">64</td></tr><tr><td align="left" valign="bottom">UpSampling2D</td><td align="left" valign="bottom">(50, 50, 16)</td><td align="left" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Conv2DTranspose</td><td align="left" valign="bottom">(50, 50, 8)</td><td align="left" valign="bottom">12,808</td></tr><tr><td align="left" valign="bottom">BatchNormalization</td><td align="left" valign="bottom">(50, 50, 8)</td><td align="left" valign="bottom">32</td></tr><tr><td align="left" valign="bottom">UpSampling2D</td><td align="left" valign="bottom">(100, 100, 8)</td><td align="left" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Conv2DTranspose</td><td align="left" valign="bottom">(100, 100, 4)</td><td align="left" valign="bottom">3204</td></tr><tr><td align="left" valign="bottom">Conv2DTranspose</td><td align="left" valign="bottom">(100, 100, 1)</td><td align="left" valign="bottom">401</td></tr><tr><td align="left" valign="bottom">BatchNormalization</td><td align="left" valign="bottom">(100, 100, 1)</td><td align="left" valign="bottom">4</td></tr><tr><td align="left" valign="bottom">Flatten</td><td align="left" valign="bottom">(10,000)</td><td align="left" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Softmax</td><td align="left" valign="bottom">(10,000)</td><td align="left" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Reshape</td><td align="left" valign="bottom">(100, 100)</td><td align="left" valign="bottom">0</td></tr></tbody></table></table-wrap><p>To evaluate performance, we calculated the AUC per depth image. The true foothold locations per image were known, and the CNN generated a probability per pixel per image. To generate the ROC curve, we treated the CNN task as a binary classification of pixels, and we calculated the rate of false positives and true positives at different probability criterion values, increasing from 0 to 1. Calculating AUC was then just a matter of computing the area under the resulting ROC curve.</p></sec></sec><sec id="s4-4"><title>Step analysis</title><p>We sought to better understand how subjects chose their footholds by analyzing the properties of their chosen steps and step sequences. Throughout this work, a foothold location is defined as the 3D position of the left or right foot marker at the time of heel strike, and a step is defined as the transition between two footholds. To analyze sets of steps, we segmented participants’ paths into 5-step sequences, consisting of 6 consecutive footholds.</p><sec id="s4-4-1"><title>Step properties</title><p>For each step in the dataset, we computed seven properties: distance, ground distance, direction, goal direction, deviation from goal, height, and slope.</p><p>To illustrate how we compute a step’s properties, consider a step vector <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> that starts at a foothold with coordinates <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and ends at a foothold with coordinates <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where the <italic>y</italic>-axis corresponds to gravity.</p><sec id="s4-4-1-1"><title>In 3D</title><p>We define step distance <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi></mml:mstyle></mml:math></inline-formula> as the magnitude of step vector <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>, i.e., the 3D Euclidean distance between the start and end footholds:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow><mml:mo>|</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow><mml:mo>(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow><mml:mo>(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow><mml:mo>(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:msqrt></mml:math></disp-formula></p></sec><sec id="s4-4-1-2"><title>In 2D, from overhead</title><p>To focus on the progression of the step along the subject’s route and ignore the step’s vertical component, we project step vector <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> onto the ground plane (<inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mi>z</mml:mi></mml:mstyle></mml:math></inline-formula>-space), producing ground vector <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>. We define step ground distance <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>G</mml:mi></mml:mstyle></mml:math></inline-formula> as the magnitude of ground vector <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>, i.e., the 2D Euclidean distance in <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mi>z</mml:mi></mml:mstyle></mml:math></inline-formula>-space between the start and end footholds:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow><mml:mo>|</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow><mml:mo>(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>x</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow><mml:mo>(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>z</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:msqrt></mml:math></disp-formula></p><p>We define step direction <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> as the direction of ground vector <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mi>arctan</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>We then consider the end point of the current terrain traversal, foothold <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi></mml:mstyle></mml:math></inline-formula>. Along the ground plane, the vector <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>e</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> connects the step’s starting foothold <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> to that traversal’s end point <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. That vector represents the most direct path the participant could take to reach their current goal. We refer to the direction of vector <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>e</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> as the goal direction <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ω</mml:mi></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mi>ω</mml:mi><mml:mo>=</mml:mo><mml:mi>arctan</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>We use that angle to calculate step deviation from goal <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi></mml:mstyle></mml:math></inline-formula>, which we define as the angle between the step direction <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> and goal direction <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ω</mml:mi></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:mo>−</mml:mo><mml:mi>ω</mml:mi></mml:math></disp-formula></p></sec><sec id="s4-4-1-3"><title>In 2D, from the side</title><p>To analyze the step’s vertical component, we calculate step height <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>h</mml:mi></mml:mstyle></mml:math></inline-formula> by finding the change in vertical position between footholds <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>y</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula></p><p>We then compute step slope by dividing step height <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>h</mml:mi></mml:mstyle></mml:math></inline-formula> by step distance <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mi>arcsin</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:mfrac></mml:mrow></mml:math></disp-formula></p></sec></sec></sec><sec id="s4-5"><title>Properties of step sequences</title><p>Step sequences (sometimes also called paths or path segments) are composed of a series of steps. In this paper, we primarily focused on 5-step sequences. We calculated two key properties: mean slope and tortuosity.</p><sec id="s4-5-1"><title>Mean slope</title><p>Each step sequence has a mean slope, <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is defined as the average step slope across steps within the sequence (<xref ref-type="fig" rid="fig12">Figure 12</xref>):<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:math></disp-formula></p><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Schematic depicting the calculation of the mean slope of a step sequence.</title><p>To calculate the slope of a step sequence – chosen or simulated – we first calculated the step slope for each step in the path, and we then averaged the absolute values of those slopes.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91243-fig12-v1.tif"/></fig></sec><sec id="s4-5-2"><title>Tortuosity</title><p>We quantified the curvature of each step sequence by calculating tortuosity, <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> is the number of steps in the sequence, <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the magnitude of a given step vector, and <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the magnitude of the vector connecting the start and end foothold locations.</p><p>Thus, we quantify tortuosity as the ratio of the cumulative step distance to the distance between the first and final footholds. This metric is the inverse of the straightness index formula proposed in <xref ref-type="bibr" rid="bib1">Batschelet, 1981</xref>, which has been shown to be a reliable estimate of the tortuosity of oriented paths (<xref ref-type="bibr" rid="bib2">Benhamou, 2004</xref>). A tortuosity of 1 indicates a straight path, while a tortuosity greater than 1 indicates a curved path. A perfect semi-circle would have a tortuosity of <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>π</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula> (approximately 1.57), and a circle would be infinitely tortuous.</p></sec></sec><sec id="s4-6"><title>Path simulation</title><p>We sought to evaluate differences between the paths subjects chose and the alternative paths they could have chosen. To do so, we simulated 5-step sequences and compared the properties of chosen paths to those of simulated paths.</p><sec id="s4-6-1"><title>Identifying viable footholds</title><p>Previous work has found that subjects are able to walk on surfaces slanted up to approximately 33° (<xref ref-type="bibr" rid="bib12">Kinsella-Shaw et al., 1992</xref>). We thus constrained possible foothold locations to those with a local surface slant below that value.</p><p>To calculate the slant of possible footholds, we computed the surface normal vector for each triangle in the 3D terrain mesh. We then calculated the mean local surface slant for each point in the mesh’s point cloud representation by averaging the surface slants of all triangles within a radius of 1 foot length (25.8 cm).</p></sec><sec id="s4-6-2"><title>Identifying viable steps</title><p>After identifying viable foothold locations, viable steps between viable foothold locations were determined based on three constraints (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Per subject, we found the distributions of (a) step slope (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>), (b) step ground distance (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) relative to that participant’s leg length, and (c) step deviation from goal (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>). We computed the maximum observed absolute values, and we then deemed a step viable only if its properties fell within those maxima.</p></sec><sec id="s4-6-3"><title>Simulating possible paths</title><sec id="s4-6-3-1"><title>Paths with a fixed start point and a random end point</title><p>For each foothold that a subject chose, we simulated possible alternative paths consisting of 5 steps (i.e. 6 footholds). These simulated path segments started at the chosen foothold, and subsequent footholds were iteratively selected, in accordance with the foothold constraint and step constraints defined above. The resulting set of path segments could then be directly compared to the path segment that the walker actually chose.</p></sec><sec id="s4-6-3-2"><title>Paths with fixed start and end points</title><p>We also simulated paths that started and ended at chosen foothold locations. To accomplish that, we treated the set of possible footholds and viable steps between them as a directed graph. We then used MATLAB’s ‘maxflow’ function to find the subset of footholds that have non-zero flow values in a directed graph between the two selected footholds (starting point and ending point). The ‘maxflow’ function then returns a set of footholds that can be visited from the starting foothold and still have available paths to the final foothold (i.e. 6th foothold in path).</p><p>Possible paths connecting the two end points of the actual path are then generated from this subset of possible foothold locations following the procedure in the previous section, iteratively selecting footholds in accordance with the step constraints defined above.</p></sec></sec></sec><sec id="s4-7"><title>Estimating straight path slope</title><p>When walking from one point to another in flat terrain, straight paths are almost certainly the preferable option. In rough terrain, however, there may be obstacles that make walking straight impossible – or at least less preferable – than taking a slightly longer curved path. To analyze this potential trade-off, for each step sequence, we estimated the slope a walker would encounter if they tried to take a relatively straight path.</p><p>To compute those values, we first found the median tortuosity of all chosen 5-step sequences in a particular terrain traversal (i.e. across a particular mesh). That gave us a conservative, terrain-specific tortuosity threshold that we could use to determine which of the possible paths were relatively straight. For each step sequence in that traversal, we then identified simulated paths with a tortuosity below that threshold and calculated the average of their mean step slopes (<inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>). The resulting values are treated as the average step slope the subject would encounter if they tried to take a straighter path for that segment of terrain.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Validation, Investigation, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Validation, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Methodology</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Software, Methodology</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Supervision, Funding acquisition, Investigation, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Human subjects: Informed consent and consent to publish was obtained and protocols were approved by the institutional IRBs at University of Texas Austin approval number 2006- 06- 0085 and UC Berkeley approval number 2011- 07- 3429.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-91243-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data and code for generating <xref ref-type="fig" rid="fig5">Figures 5</xref>, <xref ref-type="fig" rid="fig7">7</xref>—<xref ref-type="fig" rid="fig10">10</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplements 1</xref>–<xref ref-type="fig" rid="fig7s3">3</xref>, and <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplements 1</xref> and <xref ref-type="fig" rid="fig9s2">2</xref> has been made available via <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.r7sqv9sn2">Dryad</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.13932346">Zenodo</ext-link>.</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Muller</surname><given-names>K</given-names></name><name><surname>Panfili</surname><given-names>D</given-names></name><name><surname>Shields</surname><given-names>S</given-names></name><name><surname>Matthis</surname><given-names>J</given-names></name><name><surname>Bonnen</surname><given-names>K</given-names></name><name><surname>Hayhoe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Foothold selection during locomotion in uneven terrain: Results from the integration of eye tracking, motion capture, and photogrammetry</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.r7sqv9sn2</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Muller</surname><given-names>K</given-names></name><name><surname>Panfili</surname><given-names>D</given-names></name><name><surname>Shields</surname><given-names>S</given-names></name><name><surname>Matthis</surname><given-names>J</given-names></name><name><surname>Bonnen</surname><given-names>K</given-names></name><name><surname>Hayhoe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Foothold selection during locomotion in uneven terrain: Results from the integration of eye tracking, motion capture, and photogrammetry</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.13932346</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by NIH Grants EY05729 and K99 EY 028229.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Batschelet</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1981">1981</year><source>Circular Statistics in Biology. Mathematics in Biology</source><publisher-loc>London</publisher-loc><publisher-name>Academic Press</publisher-name><pub-id pub-id-type="doi">10.1604/9780120810505</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benhamou</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>How to reliably estimate the tortuosity of an animal’S path: straightness, sinuosity, or fractal dimension?</article-title><source>Journal of Theoretical Biology</source><volume>229</volume><fpage>209</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1016/j.jtbi.2004.03.016</pub-id><pub-id pub-id-type="pmid">15207476</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonnen</surname><given-names>K</given-names></name><name><surname>Matthis</surname><given-names>JS</given-names></name><name><surname>Gibaldi</surname><given-names>A</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name><name><surname>Levi</surname><given-names>DM</given-names></name><name><surname>Hayhoe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Binocular vision and the control of foot placement during walking in natural terrain</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>20881</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-99846-0</pub-id><pub-id pub-id-type="pmid">34686759</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Darici</surname><given-names>O</given-names></name><name><surname>Kuo</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Humans plan for the near future to walk economically on uneven terrain</article-title><source>PNAS</source><volume>120</volume><elocation-id>e2211405120</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2211405120</pub-id><pub-id pub-id-type="pmid">37126717</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Domínguez-Zamora</surname><given-names>FJ</given-names></name><name><surname>Marigold</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Motives driving gaze and walking decisions</article-title><source>Current Biology</source><volume>31</volume><fpage>1632</fpage><lpage>1642</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.01.069</pub-id><pub-id pub-id-type="pmid">33600769</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finley</surname><given-names>JM</given-names></name><name><surname>Bastian</surname><given-names>AJ</given-names></name><name><surname>Gottschall</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Learning to be economical: the energy cost of walking tracks motor adaptation</article-title><source>The Journal of Physiology</source><volume>591</volume><fpage>1081</fpage><lpage>1095</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2012.245506</pub-id><pub-id pub-id-type="pmid">23247109</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foulsham</surname><given-names>T</given-names></name><name><surname>Walker</surname><given-names>E</given-names></name><name><surname>Kingstone</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The where, what and when of gaze allocation in the lab and the natural environment</article-title><source>Vision Research</source><volume>51</volume><fpage>1920</fpage><lpage>1931</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2011.07.002</pub-id><pub-id pub-id-type="pmid">21784095</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallivan</surname><given-names>JP</given-names></name><name><surname>Chapman</surname><given-names>CS</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Decision-making in sensorimotor control</article-title><source>Nature Reviews. Neuroscience</source><volume>19</volume><fpage>519</fpage><lpage>534</lpage><pub-id pub-id-type="doi">10.1038/s41583-018-0045-9</pub-id><pub-id pub-id-type="pmid">30089888</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Griwodz</surname><given-names>C</given-names></name><name><surname>Gasparini</surname><given-names>S</given-names></name><name><surname>Calvet</surname><given-names>L</given-names></name><name><surname>Gurdjos</surname><given-names>P</given-names></name><name><surname>Castan</surname><given-names>F</given-names></name><name><surname>Maujean</surname><given-names>B</given-names></name><name><surname>De Lillo</surname><given-names>G</given-names></name><name><surname>Lanthony</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>AliceVision Meshroom</article-title><conf-name>MMSys ’21</conf-name><pub-id pub-id-type="doi">10.1145/3458305.3478443</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayhoe</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Vision and Action</article-title><source>Annual Review of Vision Science</source><volume>3</volume><fpage>389</fpage><lpage>413</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-102016-061437</pub-id><pub-id pub-id-type="pmid">28715958</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jurca</surname><given-names>A</given-names></name><name><surname>Žabkar</surname><given-names>J</given-names></name><name><surname>Džeroski</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Analysis of 1.2 million foot scans from North America, Europe and Asia</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>19155</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-55432-z</pub-id><pub-id pub-id-type="pmid">31844106</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kinsella-Shaw</surname><given-names>JM</given-names></name><name><surname>Shaw</surname><given-names>B</given-names></name><name><surname>Turvey</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Perceiving “Walk-on-able” Slopes</article-title><source>Ecological Psychology</source><volume>4</volume><fpage>223</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1207/s15326969eco0404_2</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>AD</given-names></name><name><surname>Donelan</surname><given-names>JM</given-names></name><name><surname>Ruina</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Energetic consequences of walking like an inverted pendulum: Step-to-Step Transitions</article-title><source>Exercise and Sport Sciences Reviews</source><volume>33</volume><fpage>88</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1097/00003677-200504000-00006</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>DV</given-names></name><name><surname>Harris</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Linking gait dynamics to mechanical cost of legged locomotion</article-title><source>Frontiers in Robotics and AI</source><volume>5</volume><elocation-id>111</elocation-id><pub-id pub-id-type="doi">10.3389/frobt.2018.00111</pub-id><pub-id pub-id-type="pmid">33500990</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>M-IB</given-names></name><name><surname>Lin</surname><given-names>K-H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Walking while performing working memory tasks changes the prefrontal cortex hemodynamic activations and gait kinematics</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>10</volume><elocation-id>92</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2016.00092</pub-id><pub-id pub-id-type="pmid">27242461</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logan</surname><given-names>D</given-names></name><name><surname>Kiemel</surname><given-names>T</given-names></name><name><surname>Dominici</surname><given-names>N</given-names></name><name><surname>Cappellini</surname><given-names>G</given-names></name><name><surname>Ivanenko</surname><given-names>Y</given-names></name><name><surname>Lacquaniti</surname><given-names>F</given-names></name><name><surname>Jeka</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The many roles of vision during walking</article-title><source>Experimental Brain Research</source><volume>206</volume><fpage>337</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1007/s00221-010-2414-0</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matthis</surname><given-names>JS</given-names></name><name><surname>Barton</surname><given-names>SL</given-names></name><name><surname>Fajen</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The critical phase for visual control of human walking over complex terrain</article-title><source>PNAS</source><volume>114</volume><fpage>E6720</fpage><lpage>E6729</lpage><pub-id pub-id-type="doi">10.1073/pnas.1611699114</pub-id><pub-id pub-id-type="pmid">28739912</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matthis</surname><given-names>JS</given-names></name><name><surname>Yates</surname><given-names>JL</given-names></name><name><surname>Hayhoe</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Gaze and the control of foot placement when walking in natural terrain</article-title><source>Current Biology</source><volume>28</volume><fpage>1224</fpage><lpage>1233</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.03.008</pub-id><pub-id pub-id-type="pmid">29657116</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matthis</surname><given-names>JS</given-names></name><name><surname>Muller</surname><given-names>KS</given-names></name><name><surname>Bonnen</surname><given-names>KL</given-names></name><name><surname>Hayhoe</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Retinal optic flow during natural locomotion</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1009575</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009575</pub-id><pub-id pub-id-type="pmid">35192614</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNarry</surname><given-names>MA</given-names></name><name><surname>Wilson</surname><given-names>RP</given-names></name><name><surname>Holton</surname><given-names>MD</given-names></name><name><surname>Griffiths</surname><given-names>IW</given-names></name><name><surname>Mackintosh</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Investigating the relationship between energy expenditure, walking speed and angle of turning in humans</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0182333</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0182333</pub-id><pub-id pub-id-type="pmid">28796796</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Connor</surname><given-names>SM</given-names></name><name><surname>Xu</surname><given-names>HZ</given-names></name><name><surname>Kuo</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Energetic cost of walking with increased step variability</article-title><source>Gait &amp; Posture</source><volume>36</volume><fpage>102</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/j.gaitpost.2012.01.014</pub-id><pub-id pub-id-type="pmid">22459093</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patla</surname><given-names>AE</given-names></name><name><surname>Vickers</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Where and when do we look as we approach and step over an obstacle in the travel path?</article-title><source>NeuroReport</source><volume>8</volume><fpage>3661</fpage><lpage>3665</lpage><pub-id pub-id-type="doi">10.1097/00001756-199712010-00002</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pelz</surname><given-names>JB</given-names></name><name><surname>Rothkopf</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><chapter-title>Oculomotor behavior in natural and man-made environments</chapter-title><person-group person-group-type="editor"><name><surname>Pelz</surname><given-names>JB</given-names></name></person-group><source>Eye Movements</source><publisher-name>Elsevier</publisher-name><fpage>661</fpage><lpage>676</lpage><pub-id pub-id-type="doi">10.1016/B978-008044980-7/50033-1</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Qilong</surname><given-names>Z</given-names></name><name><surname>Pless</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Extrinsic calibration of a camera and laser range finder (improves camera calibration)</article-title><conf-name>2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566</conf-name><pub-id pub-id-type="doi">10.1109/IROS.2004.1389752</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rio</surname><given-names>KW</given-names></name><name><surname>Rhea</surname><given-names>CK</given-names></name><name><surname>Warren</surname><given-names>WH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Follow the leader: visual control of speed in pedestrian following</article-title><source>Journal of Vision</source><volume>14</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.1167/14.2.4</pub-id><pub-id pub-id-type="pmid">24511143</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rock</surname><given-names>CG</given-names></name><name><surname>Marmelat</surname><given-names>V</given-names></name><name><surname>Yentes</surname><given-names>JM</given-names></name><name><surname>Siu</surname><given-names>KC</given-names></name><name><surname>Takahashi</surname><given-names>KZ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Interaction between step-to-step variability and metabolic cost of transport during human walking</article-title><source>The Journal of Experimental Biology</source><volume>221</volume><elocation-id>jeb181834</elocation-id><pub-id pub-id-type="doi">10.1242/jeb.181834</pub-id><pub-id pub-id-type="pmid">30237239</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Selinger</surname><given-names>JC</given-names></name><name><surname>O’Connor</surname><given-names>SM</given-names></name><name><surname>Wong</surname><given-names>JD</given-names></name><name><surname>Donelan</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Humans can continuously optimize energetic cost during walking</article-title><source>Current Biology</source><volume>25</volume><fpage>2452</fpage><lpage>2456</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.016</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soule</surname><given-names>RG</given-names></name><name><surname>Goldman</surname><given-names>RF</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>Terrain coefficients for energy cost prediction</article-title><source>Journal of Applied Physiology</source><volume>32</volume><fpage>706</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1152/jappl.1972.32.5.706</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>’t Hart</surname><given-names>BM</given-names></name><name><surname>Einhäuser</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mind the step: complementary effects of an implicit task on eye and head movements in real-life gaze allocation</article-title><source>Experimental Brain Research</source><volume>223</volume><fpage>233</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1007/s00221-012-3254-x</pub-id><pub-id pub-id-type="pmid">23001370</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voloshina</surname><given-names>AS</given-names></name><name><surname>Kuo</surname><given-names>AD</given-names></name><name><surname>Daley</surname><given-names>MA</given-names></name><name><surname>Ferris</surname><given-names>DP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Biomechanics and energetics of walking on uneven terrain</article-title><source>The Journal of Experimental Biology</source><volume>216</volume><fpage>3963</fpage><lpage>3970</lpage><pub-id pub-id-type="doi">10.1242/jeb.081711</pub-id><pub-id pub-id-type="pmid">23913951</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>WH</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Perceiving affordances: Visual guidance of stair climbing</article-title><source>Journal of Experimental Psychology</source><volume>10</volume><fpage>683</fpage><lpage>703</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.10.5.683</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>WH</given-names></name><name><surname>Young</surname><given-names>DS</given-names></name><name><surname>Lee</surname><given-names>DN</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Visual control of step length during running over irregular terrain</article-title><source>Journal of Experimental Psychology</source><volume>12</volume><fpage>259</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.12.3.259</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>WH</given-names></name><name><surname>Kay</surname><given-names>BA</given-names></name><name><surname>Zosh</surname><given-names>WD</given-names></name><name><surname>Duchon</surname><given-names>AP</given-names></name><name><surname>Sahuc</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Optic flow is used to control human walking</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>213</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1038/84054</pub-id><pub-id pub-id-type="pmid">11175884</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yokoyama</surname><given-names>H</given-names></name><name><surname>Sato</surname><given-names>K</given-names></name><name><surname>Ogawa</surname><given-names>T</given-names></name><name><surname>Yamamoto</surname><given-names>S-I</given-names></name><name><surname>Nakazawa</surname><given-names>K</given-names></name><name><surname>Kawashima</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Characteristics of the gait adaptation process due to split-belt treadmill walking under a wide range of right-left speed ratios in humans</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0194875</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0194875</pub-id><pub-id pub-id-type="pmid">29694404</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeni</surname><given-names>JA</given-names></name><name><surname>Richards</surname><given-names>JG</given-names></name><name><surname>Higginson</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Two simple methods for determining gait events during treadmill and overground walking using kinematic data</article-title><source>Gait &amp; Posture</source><volume>27</volume><fpage>710</fpage><lpage>714</lpage><pub-id pub-id-type="doi">10.1016/j.gaitpost.2007.07.007</pub-id><pub-id pub-id-type="pmid">17723303</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91243.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Spering</surname><given-names>Miriam</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>The University of British Columbia</institution><country>Canada</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Fundamental</kwd></kwd-group></front-stub><body><p>This <bold>fundamental</bold> study has the potential to substantially advance our understanding of human locomotion in complex real-world settings and opens up new approaches to studying (visually guided) behavior in natural settings outside the lab. The evidence supporting the conclusions is overall <bold>compelling</bold>. Whereas detailed analyses represent multiple ways to visualize and quantify the rich and complex natural behavior, some of the specific conclusions remain more suggestive at this point. The work will be of interest to neuroscientists, kinesiologists, computer scientists, and engineers working on human locomotion.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91243.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The work of Muller and colleagues concerns the question where we place our feet when passing uneven terrain, in particular how we trade-off path length against the steepness of each single step. The authors find that paths are chosen that are consistently less steep and deviate from the straight line more than an average random path, suggesting that participants indeed trade off steepness for path length. They show that this might be related to biomechanical properties, specifically the leg length of the walkers. In addition, they show using a neural network model that participants could choose the footholds based on their sensory (visual) information about depth.</p><p>Strengths:</p><p>The work is a natural continuation of some of the researchers' earlier work that related the immediately following steps to gaze. Methodologically, the work is very impressive and presents a further step forward towards understanding real-world locomotion and its interaction with sampling visual information. While some of the results may seem somewhat trivial in hindsight (as always in this kind of studies), I still think this is a very important approach to understand locomotion in the wild better.</p><p>Weaknesses:</p><p>The concerns I had regarding the initial version of the manuscript have all been fixed in the current one.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91243.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This manuscript examines how humans walk over uneven terrain and use vision to decide where to step. There is a huge lack of evidence about this because the vast majority of locomotion studies have focused on steady, well-controlled conditions, and not on decisions made in the real world. The author team has already made great advances in this topic by pioneering gaze recordings during locomotion, but there has been no practical way to map the gaze targets, specifically the 3D terrain features in naturalistic environments. The team has now developed a way to integrate such measurements along with gaze and step tracking. This allows quantitative evaluation of the proposed trade-offs between stepping vertically onto vs. stepping around obstacles, along with how far people look to decide where to step. The team also introduces several new analysis techniques to accompany these measurements. They use machine learning techniques to examine whether retinocentric depth helps predict footholds and develop simulations to assess possible alternative footholds and walking paths. The technical achievement is impressive.</p><p>This study addresses several real-world questions not normally examined in the laboratory. First, do humans elect to walk around steeper footholds rather than over them? Second, is there a quantifiable benefit to walking around, such as allowing for a flatter path? Third, does visual depth of terrain contribute to selection of footholds? Fourth, are there scale effects, where for example a tall adult can easily walk over an obstacle that a toddler must walk around. One might superficially answer yes to all of these questions, but it is highly nontrival to answer them quantitatively. As for the conclusions, my feelings are mixed. I find strengths in answers to two of the questions, and weaknesses in the other two.</p><p>Strengths:</p><p>I consider the evidence strongest for the first of the main questions. The results show subjects walking with more laterally deviating paths, measured by a quantity called &quot;tortuosity,&quot; when the direct straight-ahead paths appear to have steeper ups and downs (Fig. 9). The measure of straight-ahead steepness is fairly complicated (discussed below), but is shown to be well correlated with tortuosity, effectively predicting when subjects will not walk straight ahead.</p><p>There is also good evidence for the third question, showing that retinocentric depth is predictive of chosen footholds. Retinocentric depth was computed by a series of steps, starting with scene capture to determine a 3D terrain mesh, projecting that mesh into the eye's perspective, and then discarding all but the depth information. This highly involved process is only the beginning, because the depth was then used to train a neural network classifier with chosen footholds. That network was found to predict footholds better than chance, using a test set independent from the training set, each using half the recorded data. The results are strong and are best interpreted along with a previous study (Bonnen et al. 2021) showing that subjects gaze nearer ahead on rougher terrain, and slightly more so when binocular vision was disrupted. Depth information seems important for foothold selection.</p><p>As an aside, humans presumably also select footholds and estimate depth from a number of monocular visual cues, such as shading, shadows, color, and self-motion information. Interestingly, the terrain mesh and depth data here were computed from monocular images, suggesting that monocular vision can in principle be predictive of both depth and footholds. Binocular human vision presumably improves on monocular depth estimation, and so it would be interesting to see whether binocular scene cameras would predict footholds better. In an earlier review, I had suggested other avenues for exploration, but these are not weaknesses so much as opportunities not yet taken. I believe much could be learned from deeper analysis of the neural network, and future experiments using variations of this technique.</p><p>There is much to be appreciated about this study. I was impressed by the overarching outlook and ambitiousness of the team. They seek to understand human decision-making in real-world locomotion tasks, a topic of obvious relevance to the human condition but not often examined in research. The field has been biased toward well-controlled, laboratory studies, which have undeniable scientific advantages but are also quite different from the real world. The present study discards all of the usual advantages of the laboratory, yet still finds a way to explore real-world behaviors in a quantitative manner. It is an exciting and forward-thinking approach, used to tackle an ecologically relevant question.</p><p>I also appreciate the numerous technical challenges of this study. The state of the art in real-world locomotion studies has largely been limited to kinematic motion capture. This team managed to collect and analyze an unprecedented, one-of-a-kind dataset. They applied a number of non-trivial methods to assess retinocentric depth, simulate would-be walking paths and steepness, and predict footholds from neural network. Any of these could and probably will merit individual papers, and to assemble them all at once is quite beyond other studies I am aware of. I hope this study will spur more inquiries of this type, leveraging mobile electronics and modern machine learning techniques to answer questions that were previously only addressable qualitatively.</p><p>Weaknesses:</p><p>Although I am highly enthusiastic about this study, I was not entirely convinced by the evidence for the second and fourth questions. Some of this is because I was confused by aspects of the analysis, limiting my understanding of the evidence. But I also question some of the basic conclusions, whether the authors indeed proved that (from Abstract, emphasis mine) &quot;[walkers] change direction TO AVOID taking steeper steps that involve large height changes, instead of [sic] choosing more circuitous, RELATIVELY FLAT paths.&quot; (I interpret the &quot;of&quot; as a typo that should have been omitted). I think it is more objective to say, &quot;walkers changed direction more when straight-ahead paths seemed to have steeper height changes.&quot;</p><p>I say &quot;seemed&quot; because it is unknown whether humans would have experienced greater height changes if they walked straight ahead (the second main question). The comparison shown is between human tortuous paths taken and simulated straight-ahead paths never experienced by human. Ignoring questions about the simulations for now (discussed below), it is not an apples-to-apples comparison, say between the tortuous paths humans preferred and straight-ahead paths they didn't. The authors determined a measure of steepness, &quot;straight path slope&quot; (Fig. 9), that predicts when humans circuitously, but that is the same as the steepness that humans would actually experience if they had walked straight ahead. That could have been measured with an appropriate control condition, for example asking subjects to walk as straight ahead as they can manage. That also would have eliminated the need for simulations, because the slope of each step actually taken could simply have been measured and compared between conditions. Instead, two different kinds of simulations are compared, where steeper paths are fully simulated, and the circuitous paths are partially simulated but partially based on data. It seems that every fifth circuitous step coincides with a human foothold, but the intervening ones are somewhat random. I don't find this especially strong evidence that the chosen paths were indeed relatively flatter. I would prefer to be convinced by hard data than by unequal simulations.</p><p>I also have trouble accepting &quot;TO AVOID&quot; because it implies a degree of intent not evident in the data. I suppose conscious intent could be assessed subjectively by questionnaire, but I don't know how unconscious intent could be tested objectively. I believe my suggested interpretation above is better supported by evidence.</p><p>My limited acceptance is due in part to confusion about the simulations. I was especially confused about the connection between feasible steps drawn from the distribution in Figure 7, and the histograms of Figure 8. The feasible steps have clear peaks near zero slope, unity step length, and zero step direction (let's call them Flat). If 5-step simulations of Figure 8 draw from that distribution, why is there zero probability for the 0-3 deg bin (which is within {plus minus}3 deg due to absolute values)? It seems to me that Flat steps were eminently available, so why were they completely avoided? It seems that the simulations were probabilistic (and not just figurative) random walks, which implies they should have had about the same mean as Figure 7 but a wider variance, and then passed through absolute value. They look like something else that I cannot understand. This is important because the RELATIVELY FLAT conclusion is based on the chosen walks apparently being skewed flatter than random simulated walks. I have trouble accepting those distributions because Flat steps were unaccountably never taken by either simulation or human. (This issue is less concerning for Figure 9, because one can accept that some simulation measure is predictive of tortuosity even if the measure is hard to understand).</p><p>I was also confused why Figure 7 distances and directions are nearly normally distributed and not more uniform. The methods only mention constraints to eliminate steps, which to me suggests a truncated uniform distribution. It is not clear to me why the terrain should have a high peak at unity step length, which implies that the only feasible footholds were almost exclusively straight ahead and one step length away. It is possible that the &quot;feasible&quot; footholds are themselves drawn from a &quot;likely&quot; normal distribution, perhaps based on level walking data. It could be argued that simulated steps should be performed by drawing from typical step distributions for level ground, eliminating non-viable footholds, and then repeating that across multiple steps. That would explain the normality, but it is not stated in the Methods, and even if they were &quot;feasible and likely&quot; it would not explain the distributions of Figure 8.</p><p>I had some misgivings about the fourth question, where Figure 10 suggests that shorter subjects had greater correlation between straight-path slope and tortuosity than taller ones, who tended to walk straighter ahead. I agree with the authors' rebuttal to my previous review that &quot;the data are the data&quot; but I still have doubts. Now supplied as suggested by another reviewer, Figure 18 provides more detail of the underlying data, with considerably lower correlations. I now suspect that Figure 10 benefits from some statistical artifacts due to binning and other operations, and the weaker correlations of Fig. 18A are closer to reality. I am rather suspicious of correlations of correlations (Figure 18B), which lose some statistical grounding because the second correlation treats all data on equal footing, effectively whitewashing the first correlations of their varying significance (p-values 0.008 to 1e-9).</p><p>Furthermore, I am also unsure about Figure 10's comparison of tortuosity vs. straight path slope against leg length. Both tortuosity and straight path slope are already effectively dimensionless and therefore already seem to eliminate scale. It is my understanding that the simulated paths were recomputed for each subject's parameters, and the horizontal axis, slope, is already an angular measure that should affect short and tall people similarly. Shouldn't all subjects equally avoid steep angles, regardless of their dimensional height? If there is indeed a scale effect, then I would expect it to be demonstrated with a dimensional measure (vertical axis) that depends on leg length.</p><p>I certainly agree with the hypothetical prior that tall adults walk straight over obstacles that shorter adults (or children) walk around. But I feel that simpler tests would better evidence, perhaps in future work. Did shorter subjects walk with greater tortuosity than taller ones on the same terrain? Did shorter subjects take relatively more steps even after normalizing for leg length? A possible comparison would be (number of steps)*(leg length)/(start to end distance). I feel that the evidence from this study is not that strong.</p><p>Although it is a strength of this study that so much can be learned from pure observation, that does not mean controlled conditions are not scientifically helpful. As mentioned earlier, a helpful control could have been to ask subjects to walk straighter but less preferred paths on the same terrain, treating human paths as an independent variable. Another would be to treat terrain as an independent variable, by using level ground and intermediate terrain conditions. This would make it easier to test whether taller subjects walk straighter ahead on more uneven terrain than shorter subjects. Indeed, the data set already includes some patches of flatter terrain, not included here. Additional and simpler tests might be possible based on existing data.</p><p>Conclusion</p><p>This is an ambitious undertaking, presenting a wealth of unprecedented data to quantitatively test basic ecological questions that have long been unanswered. There are a number of considerable strengths that merit appreciation, especially the ability to quantitatively predict when humans will walk more circuitously. The weaknesses are about limitations in the conclusions that can be drawn thus far rather than the correctness of the study. I consider this to be a first step that will hopefully enable and inspire a long line of future work that will address these questions more in depth.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91243.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The systematic way in which path selection is parametrically investigated is the main contribution.</p><p>Strengths:</p><p>The authors have developed an impressive workflow to study gait and gaze in natural terrain. They are able to determine footholds and gaze points in the 3D world, and explore different path selections in the terrain.</p><p>Weaknesses:</p><p>The finding that walkers prefer less tortuous, demanding paths is hardly surprising, and from the data it is still not clear what actual visual features are used to choose among alternative routes or what the nature of the decision process is. The authors discuss energetic cost and other &quot;factors&quot; that might influence path selection, but as yet there is no way to express these ideas rigorously in such complex natural settings.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91243.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Muller</surname><given-names>Karl S</given-names></name><role specific-use="author">Author</role><aff><institution>The University of Texas at Austin</institution><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Bonnen</surname><given-names>Kathryn</given-names></name><role specific-use="author">Author</role><aff><institution>Indiana University</institution><addr-line><named-content content-type="city">Bloomington</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Shields</surname><given-names>Stephanie M</given-names></name><role specific-use="author">Author</role><aff><institution>Indiana University</institution><addr-line><named-content content-type="city">Bloomington</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Panfili</surname><given-names>Daniel P</given-names></name><role specific-use="author">Author</role><aff><institution>The University of Texas at Austin</institution><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Matthis</surname><given-names>Jonathan</given-names></name><role specific-use="author">Author</role><aff><institution>Northeastern University</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Hayhoe</surname><given-names>Mary</given-names></name><role specific-use="author">Author</role><aff><institution>The University of Texas at Austin</institution><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><p>We thank the reviewers for their constructive reviews. Taken together, the comments and suggestions from reviewers made it clear that we needed to focus on improving the clarity of the methods and results. We have revised the manuscript with that in mind. In particular, we have restructured the results to make the logic of the manuscript clearer and we have added details to the methods section.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>The work of Muller and colleagues concerns the question of where we place our feet when passing uneven terrain, in particular how we trade-off path length against the steepness of each single step. The authors find that paths are chosen that are consistently less steep and deviate from the straight line more than an average random path, suggesting that participants indeed trade-off steepness for path length. They show that this might be related to biomechanical properties, specifically the leg length of the walkers. In addition, they show using a neural network model that participants could choose the footholds based on their sensory (visual) information about depth.</p><p>Strengths:</p><p>The work is a natural continuation of some of the researchers' earlier work that related the immediately following steps to gaze [17]. Methodologically, the work is very impressive and presents a further step forward towards understanding real-world locomotion and its interaction with sampling visual information. While some of the results may seem somewhat trivial in hindsight (as always in this kind of study), I still think this is a very important approach to understanding locomotion in the wild better.</p><p>Weaknesses:</p><p>The manuscript as it stands has several issues with the reporting of the results and the statistics. In particular, it is hard to assess the inter-individual variability, as some of the data are aggregated across individuals, while in other cases only central tendencies (means or medians) are reported without providing measures of variability; this is critical, in particular as N=9 is a rather small sample size. It would also be helpful to see the actual data for some of the information merely described in the text (e.g., the dependence of \Delta H on path length). When reporting statistical analyses, test statistics and degrees of freedom should be given (or other variants that unambiguously describe the analysis).</p></disp-quote><p>There is only one figure (Figure 6) that shows data pooled over subjects and this is simply to illustrate how the random paths were calculated. The actual paths generated used individual subject data. We don’t draw our conclusions from these histograms – they are instead used to generate bounds for the simulated paths. We have made clear both in the text and in the figure legends when we have plotted an example subject. Other plots show the individual subject data. We have given the range of subject medians as well as the standard deviation for data illustrated in Figure (random vs chosen), we have also given the details of the statistical test comparing the flatness of the chosen paths versus the randomly generated paths. We have added two supplemental figures to show individual walker data more directly: (Fig. 14) the per subject histograms of step parameters, (Fig. 18) the individual subject distributions for straight path slopes and tortuosity.</p><disp-quote content-type="editor-comment"><p>The CNN analysis chosen to link the step data to visual sampling (gaze and depth features) should be motivated more clearly, and it should describe how training and test sets were generated and separated for this analysis.</p></disp-quote><p>We have motivated the CNN analysis and moved it earlier in the manuscript to help clarify the logic the manuscript. Details of the training and test are now provided, and the data have been replotted. The values are a little different from the original plot after making a correction in the code, but the conclusions drawn from this analysis are unchanged. This analysis simply shows that there is information in the depth images from the subject’s perspective that a network can use to learn likely footholds. This motivates the subsequent analysis of path flatness.</p><disp-quote content-type="editor-comment"><p>There are also some parts of figures, where it is unclear what is shown or where units are missing. The details are listed in the private review section, as I believe that all of these issues can be fixed in principle without additional experiments.</p></disp-quote><p>Several of the Figures have been replotted to fix these issues.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>This manuscript examines how humans walk over uneven terrain using vision to decide where to step. There is a huge lack of evidence about this because the vast majority of locomotion studies have focused on steady, well-controlled conditions, and not on decisions made in the real world. The author team has already made great advances in this topic, but there has been no practical way to map 3D terrain features in naturalistic environments. They have now developed a way to integrate such measurements along with gaze and step tracking, which allows quantitative evaluation of the proposed trade-offs between stepping vertically onto vs. stepping around obstacles, along with how far people look to decide where to step.</p><p>Strengths:</p><p>(1) I am impressed by the overarching outlook of the researchers. They seek to understand human decision-making in real-world locomotion tasks, a topic of obvious relevance to the human condition but not often examined in research. The field has been biased toward well-controlled studies, which have scientific advantages but also serious limitations. A well-controlled study may eliminate human decisions and favor steady or periodic motions in laboratory conditions that facilitate reliable and repeatable data collection. The present study discards all of these usually-favorable factors for rather uncontrolled conditions, yet still finds a way to explore real-world behaviors in a quantitative manner. It is an ambitious and forward-thinking approach, used to tackle an ecologically relevant question.</p><p>(2) There are serious technical challenges to a study of this kind. It is true that there are existing solutions for motion tracking, eye tracking, and most recently, 3D terrain mapping. However most of the solutions do not have turn-key simplicity and require significant technical expertise. To integrate multiple such solutions together is even more challenging. The authors are to be commended on the technical integration here.</p><p>(3) In the absence of prior studies on this issue, it was necessary to invent new analysis methods to go with the new experimental measures. This is non-trivial and places an added burden on the authors to communicate the new methods. It's harder to be at the forefront in the choice of topic, technical experimental techniques, and analysis methods all at once.</p><p>Weaknesses:</p><p>(1) I am predisposed to agree with all of the major conclusions, which seem reasonable and likely to be correct. Ignoring that bias, I was confused by much of the analysis. There is an argument that the chosen paths were not random, based on a comparison of probability distributions that I could not understand. There are plots described as &quot;turn probability vs. X&quot; where the axes are unlabeled and the data range above 1. I hope the authors can provide a clearer description to support the findings. This manuscript stands to be cited well as THE evidence for looking ahead to plan steps, but that is only meaningful if others can understand (and ultimately replicate) the evidence.</p></disp-quote><p>We have rewritten the manuscript with the goal of clarifying the analyses, and we have re-labelled the offending figure.</p><disp-quote content-type="editor-comment"><p>(2) I wish a bit more and simpler data could be provided. It is great that step parameter distributions are shown, but I am left wondering how this compares to level walking. The distributions also seem to use absolute values for slope and direction, for understandable reasons, but that also probably skews the actual distribution. Presumably, there should be (and is) a peak at zero slope and zero direction, but absolute values mean that non-zero steps may appear approximately doubled in frequency, compared to separate positive and negative. I would hope to see actual distributions, which moreover are likely not independent and probably have a covariance structure. The covariance might help with the argument that steps are not random, and might even be an easy way to suggest the trade-off between turning and stepping vertically. This is not to disregard the present use of absolute values but to suggest some basic summary of the data before taking that step.</p></disp-quote><p>We have replotted the step parameter distributions without absolute values. Unfortunately, the covariation of step parameters (step direction and step slope) is unlikely to help establish this tradeoff. Note that the primary conclusion of the manuscript is that works make turns to keep step slope low (when possible). Thus, any correlation that might exist between goal direction and step slope would be difficult to interpret without a direct comparison to possible alternative paths (as we have done in this paper). As such we do not draw our conclusions from them. We use them primarily to generate plausible random paths for comparison with the chosen paths. We have added two supplementary figures including distributions (Fig 15) and covariation of all the step parameters discussed in the methods (Fig 16).</p><disp-quote content-type="editor-comment"><p>(3) Along these same lines, the manuscript could do more to enable others to digest and go further with the approach, and to facilitate interpretability of results. I like the use of a neural network to demonstrate the predictiveness of stepping, but aside from above-chance probability, what else can inform us about what visual data drives that?</p></disp-quote><p>The CNN analysis simply shows that the information is there in the image from the subject’s viewpoint and is used to motivate the subsequent analysis. As noted above, we have generally tried to improve the clarity of the methods.</p><disp-quote content-type="editor-comment"><p>Similarly, the step distributions and height-turn trade-off curves are somewhat opaque and do not make it easy to envision further efforts by others, for example, people who want to model locomotion. For that, clearer (and perhaps) simpler measures would be helpful.</p></disp-quote><p>We have clarified the description of these plots in the main text and in the methods. We have also tried to clarify why we made the choices that we did in measuring the height-turn trade-off and why it is necessary in order to make a fair comparison.</p><disp-quote content-type="editor-comment"><p>I am absolutely in support of this manuscript and expect it to have a high impact. I do feel that it could benefit from clarification of the analysis and how it supports the conclusions.</p><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary:</p><p>The systematic way in which path selection is parametrically investigated is the main contribution.</p><p>Strengths:</p><p>The authors have developed an impressive workflow to study gait and gaze in natural terrain.</p><p>Weaknesses:</p><p>(1) The training and validation data of the CNN are not explained fully making it unclear if the data tells us anything about the visual features used to guide steering. It is not clear how or on what data the network was trained (training vs. validation vs. un-peeked test data), and justification of the choices made. There is no discussion of possible overfitting. The network could be learning just e.g. specific rock arrangements. If the network is overfitting the &quot;features&quot; it uses could be very artefactual, pixel-level patterns and not the kinds of &quot;features&quot; the human reader immediately has in mind.</p></disp-quote><p>The CNN analysis has now been moved earlier in the manuscript to help clarify its significance and we have expanded the description of the methods. Briefly, it simply indicates that there is information in the depth structure of the terrain that can be learned by a network. This helps justify the subsequent analyses. Importantly, the network training and testing sets were separated by terrain to ensure that the model was being tested on “unseen” terrain and avoid the model learning specific arrangements. This is now clarified in the text.</p><disp-quote content-type="editor-comment"><p>(2) The use of descriptive terminology should be made systematic.</p><p>Specifically, the following terms are used without giving a single, clear definition for them: path, step, step location, foot plant, foothold, future foothold, foot location, future foot location, foot position. I think some terms are being used interchangeably. I would really highly recommend a diagrammatic cartoon sketch, showing the definitions of all these terms in a single figure, and then sticking to them in the main text.</p></disp-quote><p>We have made the language more systematic and clarified the definition of each term (see Methods). Path refers to the sequence of 5 steps. Foothold is where the foot was placed in the environment. A step is the transition from one foothold to the next.</p><disp-quote content-type="editor-comment"><p>(3) More coverage of different interpretations / less interpretation in the abstract/introduction would be prudent. The authors discuss the path selection very much on the basis of energetic costs and gait stability. At least mention should be given to other plausible parameters the participants might be optimizing (or that indeed they may be just satisficing). That is, it is taken as &quot;given&quot; that energetic cost is the major driver of path selection in your task, and that the relevant perception relies on internal models. Neither of these is a priori obvious nor is it as far as I can tell shown by the data (optimizing other variables, satisficing behavior, or online &quot;direct perception&quot; cannot be ruled out).</p></disp-quote><p>The abstract has been substantially rewritten. We have adjusted our language in the introduction/discussion to try to address this concern.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewing Editor comments</bold></p><p>You will find a full summary of all 3 reviews below. In addition to these reviews, I'd like to highlight a few points from the discussion among reviewers.</p><p>All reviewers are in agreement that this study has the potential to be a fundamental study with far-reaching empirical and practical implications. The reviewers also appreciate the technical achievements of this study.</p><p>At the same time, all reviewers are concerned with the overall lack of clarity in how the results are presented. There are a considerable number of figures that need better labeling, text parts that require clearer definitions, and the description of data collection and analysis (esp. with regard to the CNN) requires more care. Please pay close attention to all comments related to this, as this was the main concern that all reviewers shared.</p><p>At a more specific level, the reviewers discussed the finding around leg length, and admittedly, found it hard to believe, in short: &quot;extraordinary claims need strong evidence&quot;. It would be important to strengthen this analysis by considering possible confounds, and by including a discussion of the degree of conviction.</p></disp-quote><p>We have weakened the discussion of this finding and provided some an additional analyses in a supplemental figure (Figure 17) to help clarify the finding.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>First, let me apologize for the long delay with this review. Despite my generally positive evaluation (see public review), I have some concerns about the way the data are presented and questions about methodological details.</p><p>(1) Representation of results: I find it hard to decipher how much variability arises within an individual and how much across individuals. For example, Figure 7b seems to aggregate across all individuals, while the analysis is (correctly) based on the subject medians.</p></disp-quote><p>Figure 7b That figure was just one subject. This is now clarified.</p><disp-quote content-type="editor-comment"><p>It would be good to see the distribution of all individuals (maybe use violin plots for each observer with the true data on one side and the baseline data on the other, or simple histograms for each). To get a feeling for inter-individual and intra-individual variability is crucial, as obviously (see the leg-length analysis) there are larger inter-individual differences and representations like these would be important to appreciate whether there is just a scaling of more or less the same effect or whether there are qualitative differences (especially in the light of N=9 being not a terribly huge sample size).</p></disp-quote><p>The medians for the individual subjects are now provided with the standard deviations between subjects to indicate the extent of individual differences. Note that the random paths were chosen from the distribution of actual step slopes for that subject as one of the constraints. This makes the random paths statistically similar to the chosen paths with the differences only being generated by the particular visual context. Thus the test for a difference between chosen and random is quite conservative</p><disp-quote content-type="editor-comment"><p>Similarly, seeing \DeltaH plotted as a function of steps in the path as a figure rather than just having the verbal description would also help.</p></disp-quote><p>To simplify the discussion of our methods/results we have removed the analyses that examine mean slope as a function of steps. Because of the central limit theorem the slopes of the chosen paths remain largely unchanged regardless of the choice path length. The slopes of the simulated paths are always larger irrespective of the choice of path length.</p><disp-quote content-type="editor-comment"><p>(2) Reporting the statistical analyses: This is related to my previous issue: I would appreciate it if the test statistics and degrees-of-freedom of the statistical tests were given along with the p-values, instead of only the p-values. This at some points would also clarify how the statistics were computed exactly (e.g., &quot;All subjects showed comparable difference and the difference in medians evaluated across subjects was highly significant (p&lt;&lt;0.0001).&quot;, p.10, is ambiguous to me).</p></disp-quote><p>Details have been added as requested.</p><disp-quote content-type="editor-comment"><p>(3) Why is the lower half (&quot;tortuosity less than the median tortuosity&quot; of paths used as &quot;straight&quot; rather than simply the minimum of all viable paths)?</p></disp-quote><p>The benchmark for a straight path is somewhat arbitrary. Using the lower half rather than the minimum length path is more conservative.</p><disp-quote content-type="editor-comment"><p>(4) For the CNN analysis, I failed to understand what was training and what was test set. I understand that the goal is to predict for all pixels whether they are a potential foothold or not, and the AUC is a measure of how well they can be discriminated based on depth information and then this is done for each image and the median over all images taken. But on which data is the CNN trained, and on which is it tested? Is this leave-n-out within the same participant? If so, how do you deal with dependencies between subsequent images? Or is it leave-1-out across participants? If so, this would be more convincing, but again, the same image might appear in training and test. If the authors just want to ask how well depth features can discriminate footholds from non-footholds, I do not see the benefit of a supervised method, which leaves the details of the feature combinations inside a black box. Rather than defining the &quot;negative set&quot; (i.e., the non-foothold pixels) randomly, the simulated paths could also be used, instead. If performance (AUC) gets lower than for random pixels, this would confirm that the choice of parameters to define a &quot;viable path&quot; is well-chosen.</p></disp-quote><p>This has been clarified as described above.</p><disp-quote content-type="editor-comment"><p>Minor issues:</p><p>(5) A higher tortuosity would also lead a participant to require more steps in total than a lower tortuosity. Could this partly explain the correlation between the leg length and the slope/tortuosity correlation? (Longer legs need fewer steps in total, thus there might be less tradeoff between \Delta H and keeping the path straight (i.e., saving steps)). To assess this, you could give the total number of steps per (straight) distance covered for leg length and compare this to a flat surface.</p></disp-quote><p>The calculations are done on an individual subject basis and the first and last step locations are chosen from the actual foot placements, then the random paths are generated between those endpoints. The consequence of this is that the number of steps is held constant for the analysis. We have clarified the methods for this analysis to try to make this more clear.</p><disp-quote content-type="editor-comment"><p>(6) As far as I understand, steps happen alternatingly with the two feet. That is, even on a flat surface, one would not reach 0 tortuosity. In other words, does the lateral displacement of the feet play a role (in particular, if paths with even and paths with odd number of steps were to be compared), and if so, is it negligible for the leg-length correlation?</p></disp-quote><p>All the comparisons here are done for 5 step sequences so this potential issue should not affect the slope of the regression lines or the leg length correlation.</p><disp-quote content-type="editor-comment"><p>(7) Is there any way to quantify the quality of the depth estimates? Maybe by taking an actual depth image (e.g., by LIDAR or similar) for a small portion of the terrain and comparing the results to the estimate? If this has been done for similar terrain, can a quantification be given? If errors would be similar to human errors, this would also be interesting for the interpretation of the visual sampling data.</p></disp-quote><p>Unfortunately, we do not have the ground truth depth image from LIDAR. When these data were originally collected, we had not imagined being able to reconstruct the terrain. However, we agree with the reviewers that this would be a good analysis to do. We plan to collect LIDAR in future experiments.</p><p>To provide an assessment of quality for these data in the absence of a ground truth depth image, we have performed an evaluation of the reliability of the terrain reconstruction across repeats of the same terrain both between and within participants. We have expanded the discussion of these reliability analyses in the results section entitled “Evaluating Terrain Reconstruction”, as well as in the corresponding methods section (see Figure 10).</p><disp-quote content-type="editor-comment"><p>(8) The figures are sometimes confusing and a bit sloppy. For example, in Figure 7a, the red, cyan, and green paths are not mentioned in the caption, in Figure 8 units on the axes would be helpful, in Figure 9 it should probably be &quot;tortuosity&quot; where it now states &quot;curviness&quot;.</p></disp-quote><p>These details have been fixed.</p><disp-quote content-type="editor-comment"><p>(9) I think the statement &quot;The maximum median AUC of 0.79 indicates that the 0.79 is the median proportion of pixels in the circular...&quot; is not an appropriate characterization of the AUC, as the number of correctly classified pixels will not only depend on the ROC (and thus the AUC), but also on the operating point chosen on the ROC (which is not specified by the AUC alone). I would avoid any complications at this point and just characterize the AUC as a measure of discriminability between footholds and non-footholds based on depth features.</p></disp-quote><p>This has been fixed.</p><disp-quote content-type="editor-comment"><p>(10) Ref. [16]is probably the wrong Hart paper (I assume their 2012 Exp. Brain Res. [<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00221-012-3254-x">https://doi.org/10.1007/s00221-012-3254-x</ext-link>] paper is meant at this point)</p></disp-quote><p>Fixed</p><disp-quote content-type="editor-comment"><p>Typos (not checked systematically, just incidental discoveries):</p><p>(11) &quot;While there substantial overlap&quot; (p.10)</p><p>(12) &quot;field..&quot; (p.25)</p><p>(13) &quot;Introduction&quot;, &quot;General Discussion&quot; and &quot;Methods&quot; as well as some subheadings are numbered, while the other headings (e.g., Results) are not.</p></disp-quote><p>Fixed</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>The major suggestions have been made in the Public Review. The following are either minor comments or go into more detail about the major suggestions. All of these comments are meant to be constructive, not obstructive.</p><p>Abstract. This is well written, but the main conclusions &quot;Walkers avoid...This trade off is related...5 steps ahead&quot; sound quite qualitative. They could be strengthened by more specificity (NOT p-values), e.g. &quot;positive correlation between the unevenness of the path straight ahead and the probability that people turned off that path.&quot;</p></disp-quote><p>The abstract has been substantially rewritten.</p><disp-quote content-type="editor-comment"><p>P. 5 &quot;pinning the head position estimated from the IMU to the Meshroom estimates&quot; sounds like there are two estimates. But it does not sound like both were used. Clarify, e.g. the Meshroom estimate of head position was used in place of IMU?</p></disp-quote><p>Yes that’s correct. We have clarified this in the text.</p><disp-quote content-type="editor-comment"><p>Figure 5. I was confused by this. First, is a person walking left to right? When the gaze position is shown, where was the eye at the time of that gaze? There are straight lines attached to the blue dots, what do they represent? The caption says gaze is directed further along the path, which made me guess the person is walking right to left, and the line originates at the eye. Except the origins do not lie on or close to the head locations. There's also no scale shown, so maybe I am completely misinterpreting. If the eye locations were connected to gaze locations, it would help to support the finding that people look five steps ahead of where they step.</p></disp-quote><p>We have updated the figure and clarified the caption to remove these confusions. There was a mistake in the original figure (where the yellow indicated head locations, we had plotted the center of mass and the choice of projection gave the incorrect impression that the fixations off the path, in blue, were separated from the head).</p><p>The view of the data is now presented so the person is walking left to right and with a projection of the head location (orange), gaze locations (blue or green) and feet (pink).</p><disp-quote content-type="editor-comment"><p>Figure 6. As stated in the major comments, the step distributions would be expected to have a covariance structure (in terms of raw data before taking absolute values). It would be helpful to report the covariances (6 numbers). As an example of a simple statistical analysis, a PCA (also based on a data covariance) would show how certain combinations of slope/distance/direction are favored over others. Such information would be a simple way to argue that the data are not completely random, and may even show a height-turn trade-off immediately. (By the way, I am assuming absolute values are used because the slopes and directions are only positive, but it wasn't clear if this was the definition). A reason why covariances and PCA are helpful is that such data would be helpful to compute a better random walk, generated from dynamics. I believe the argument that steps are not random is not served by showing the different histograms in Figure 7, because I feel the random paths are not fairly produced. A better argument might draw randomly from the same distribution as the data (or drive a dynamical random walk), and compare with actual data. There may be correlations present in the actual data that differ from random. I could be mistaken, because it is difficult or impossible to draw conclusions from distributions of absolute values, or maybe I am only confused. In any case, I suspect other readers will also have difficulty with this section.</p></disp-quote><p>This has been addressed above in the major comments.</p><disp-quote content-type="editor-comment"><p>p. 9, &quot;average step slope&quot; I think I understand the definition, but I suggest a diagram might be helpful to illustrate this.</p></disp-quote><p>There is a diagram of a single step slope in Figure 6 and a diagram of the average step slope for a path segment in Figure 12.</p><disp-quote content-type="editor-comment"><p>Incidentally, the &quot;straight path slope&quot; is not clearly defined. I suspect &quot;straight&quot; is the view from above, i.e. ignoring height changes.</p></disp-quote><p>Clarified</p><disp-quote content-type="editor-comment"><p>p. 11 The tortuosity metric could use a clearer definition. Should I interpret &quot;length of the chosen path relative to a straight path&quot; as the numerator and denominator? Here does &quot;length&quot; also refer to the view from above? Why is tortuosity defined differently from step slope? Couldn't there be an analogue to step slope, except summing absolute values of direction changes? Or an analogue to tortuosity, meaning the length as viewed from the side, divided by the length of the straight path?</p></disp-quote><p>We followed the literature in the definition of tortuosity. We have clarified the definition of tortuosity in the methods, but yes, you can interpret the length of the chosen path relative to a straight path, as the numerator and denominator, and length refers to 3D length. We agree that there are many interesting ways to look at the data but for clarity we have limited the discussion to a single definition of tortuosity in this paper.</p><disp-quote content-type="editor-comment"><p>Figure 8 could use better labeling. On the left, there is a straight path and a more tortuous path, why not report the metrics for these? On the right, there are nine unlabeled plots. The caption says &quot;turn probability vs. straight path slope&quot; but the vertical axis is clearly not a probability. Perhaps the axis is tortuosity? I presume the horizontal axis is a straight path slope in degrees, but this is not explained. Why are there nine plots, is each one a subject? I would prefer to be informed directly instead of guessing. (As a side note, I like the correlations as a function of leg length, it is interesting, even if slightly unbelievable. I go hiking with people quite a bit shorter and quite a lot taller than me, and anecdotally I don't think they differ so much from each other).</p></disp-quote><p>We have fixed Figure 8 which shows the average “mean slope” as a function of tortuosity. We have added a supplemental figure which shows a scatter plot of the raw data (mean slope vs. tortuosity for each path segment).</p><p>Note that when walking with friends other factors (e.g. social) will contribute to the cost function. As a very short person my experience is that it is a problem. In any case, the data are the data, whatever the underlying reasons. It does not seem so surprising that people of different heights make different tradeoffs. We know that the preferred gait depends on individual’s passive dynamics as described in the paper, and the terrain will change what is energetically optimal as described in the Darici and Kuo paper.</p><disp-quote content-type="editor-comment"><p>Figure 9 presumably shows one data point per subject, but this isn't clear.</p></disp-quote><p>The correlations are reported per subject, and this has been clarified.</p><disp-quote content-type="editor-comment"><p>p. 13 CNN. I like this analysis, but only sort of. It is convincing that there is SOME sort of systematic decision-making about footholds, better than chance. What it lacks is insight. I wonder what drives peoples' decisions. As an idle suggestion, the AlexNet (arXiv: Krizhevsky et al.; see also A. Karpathy's ConvNETJS demo with CIFAR-10) showed some convolutional kernels to give an idea of what the layers learned.</p></disp-quote><p>Further exploration of CNN’s would definitely be interesting, but it is outside the scope of the paper. We use it simply to make a modest point, as described above.</p><disp-quote content-type="editor-comment"><p>p. 15 What is the definition of stability cost? I understand energy cost, but it is unclear how circuitous paths have a higher stability cost. One possible definition is an energetic cost having to do with going around and turning. But if not an energy cost, what is it?</p></disp-quote><p>We meant to say that the longer and flatter paths are presumably more stable because of the smaller height changes. You are correct that we can’t say what the stability cost is and we have clarified this in the discussion.</p><disp-quote content-type="editor-comment"><p>p. 16 &quot;in other data&quot; is not explained or referenced.</p></disp-quote><p>Deleted</p><disp-quote content-type="editor-comment"><p>p. 10 5 step paths and p. 17 &quot;over the next 5 steps&quot;. I feel there is very little information to really support the 5 steps. A p-value only states the significance, not the amount of difference. This could be strengthened by plotting some measures vs. the number of steps ahead. For example, does a CNN looking 1-5 steps ahead predict better than one looking N&lt;5 steps ahead? I am of course inclined to believe the 5 steps, but I do not see/understand strong quantitative evidence here.</p></disp-quote><p>We have weakened the statements about evidence for planning 5 steps ahead.</p><disp-quote content-type="editor-comment"><p>p. 25 CNN. I did not understand the CNN. The list of layers seems incomplete, it only shows four layers. The convolutional-deconvolutional architecture is mentioned as if that is a common term, which I am unfamiliar with but choose to interpret as akin to encoder-decoder. However, the architecture does not seem to have much of a bottleneck (25x25x8 is not greatly smaller than 100x100x4), so what is the driving principle? It's also unclear how the decoder culminates, does it produce some m x m array of probabilities of stepping, where m is some lower dimension than the images? It might be helpful also to illustrate the predictions, for example, show a photo of the terrain view, along with a probability map for that view. I would expect that the reader can immediately say yes, I would likely step THERE but not there.</p></disp-quote><p>We have clarified the description of the CNN. An illustration is shown in Figure 11.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>(This section expands on the points already contained in the Public Review).</p><p>Major issues</p><p>(1) The training and validation data of the CNN are not explained fully making it unclear if the data tells us anything about the visual features used to guide steering. A CNN was used on the depth scenes to identify foothold locations in the images. This is the bit of the methods and the results that remains ambiguous, and the authors may need to revisit the methods/results. It is not clear how or on what data the network was trained (training vs. validation vs. un-peeked test data), and justification of the choices made. There is no discussion of possible overfitting. The network could be learning just for example specific rock arrangements in the particular place you experimented. Training the network on data from one location and then making it generalize to another location would of course be ideal. Your network probably cannot do this (as far as I can tell this was not tried), and so the meaning of the CNN results cannot really be interpreted.</p><p>I really like the idea, of getting actual retinotopic depth field approximations. But then the question would be: what features in this information are relevant and useful for visual guidance (of foot placement)? But this question is not answered by your method.</p><p>&quot;If a CNN can predict these locations above chance using depth information, this would indicate that depth features can be used to explain some variation in foothold selection.&quot; But there is no analysis of what features they are. If the network is overfitting they could be very artefactual, pixel-level patterns and not the kinds of &quot;features&quot; the human reader immediately has in mind. As you say &quot;CNN analysis shows that subject perspective depth features are predictive of foothold locations&quot;, well, yes, with 50,000 odd parameters the foothold coordinates can be associated with the 3D pixel maps, but what does this tell us?</p></disp-quote><p>See previous discussion of these issues.</p><p>It is true that we do not know the precise depth features used. We established that information about height changes was being used, but further work is needed to specify how the visual system does this. This is mentioned in the Discussion.</p><disp-quote content-type="editor-comment"><p>You open the introduction with a motivation to understand the visual features guiding path selection, but what features the CNN finds/uses or indeed what features are there is not much discussed. You would need to bolster this, or down-emphasize this aspect in the Introduction if you cannot address it.</p><p>&quot;These depth image features may or may not overlap with the step slope features shown to be predictive in the previous analysis, although this analysis better approximates how subjects might use such information.&quot; I do not think you can say this. It may be better to approximate the kind of (egocentric) environment the subjects have available, but as it is I do not see how you can say anything about how the subject uses it. (The results on the path selection with respect to the terrain features, viewpoint viewpoint-independent allocentric properties of the previous analyses, are enough in themselves!)</p></disp-quote><p>We have rewritten the section on the CNN to make clearer what it can and cannot do and its role in the manuscript. See previous discussion.</p><disp-quote content-type="editor-comment"><p>(2) The use of descriptive terminology should be made systematic. Overall the rest of the methodology is well explained, and the workflow is impressive. However, to interpret the results the introduction and discussion seem to use terminology somewhat inconsistently. You need to dig into the methods to figure out the exact operationalizations, and even then you cannot be quite sure what a particular term refers to. Specifically, you use the following terms without giving a single, clear definition for them (my interpretation in parentheses):</p><p>foothold (a possible foot plant location where there is an &quot;affordance&quot;? or a foot plant location you actually observe for this individual? or in the sample?)</p><p>step (foot trajectory between successive step locations)</p><p>step location (the location where the feet are placed)</p><p>path are they lines projected on the ground, or are they sequences of foot plants? The figure suggests lines but you define a path in terms of five steps.</p><p>foot plant (occurs when the foot comes in contact with step location?)</p><p>future foothold (?)</p><p>foot location (?)</p><p>future foot location (?)</p><p>foot position (?)</p><p>I think some terms are being used interchangeably here? I would really highly recommend a diagrammatic cartoon sketch, showing the definitions of all these terms in a single figure, and then sticking to them in the main text. Also, are &quot;gaze location&quot; and &quot;fixation&quot; the same? I.e. is every gaze-ground intersection a &quot;gaze location&quot; (I take it it is not a &quot;fixation&quot;, which you define by event identification by speed and acceleration thresholds in the methods)?</p></disp-quote><p>We have cleaned up the language. A foothold is the location in the terrain representation (mesh) where the foot was placed. A step is the transition from one foothold to the next. A path is the sequences of 5 steps. The lines simply illustrate the path in the Figures. A gaze location is the location in the terrain representation where the walker is holding gaze still (the act of fixating). See Muller et al (2023) for further explanation.</p><disp-quote content-type="editor-comment"><p>(3) More coverage of different interpretations / less interpretation in the abstract/introduction would be prudent. You discuss the path selection very much on the basis of energetic costs and gait stability. At least mention should be given to other plausible parameters the participants might be optimizing (or that indeed they may be just satisficing). Temporal cost (more circuitous route takes longer) and uncertainty (the more step locations you sample the more chance that some of them will not be stable) seem equally reasonable, given the task ecology / the type of environment you are considering. I do not know if there is literature on these in the gait-scene, but even if not then saying you are focusing on just one explanation because that's where there is literature to fall back on would be the thing to do.</p><p>Also in the abstract and introduction you seem to take some of this &quot;for granted&quot;. E.g. you end the abstract saying &quot;are planning routes as well as particular footplants. Such planning ahead allows the minimization of energetic costs. Thus locomotor behavior in natural environments is controlled by decision mechanisms that optimize for multiple factors in the context of well-calibrated sensory and motor internal models&quot;. This is too speculative to be in the abstract, in my opinion. That is, you take as &quot;given&quot; that energetic cost is the major driver of path selection in your task, and that the relevant perception relies on internal models. Neither of these is a priori obvious nor is it as far as I can tell shown by your data (optimizing other variables, satisficing behavior, or online &quot;direct perception&quot; cannot be ruled out).</p></disp-quote><p>We have rewritten the abstract and Discussion with these concerns in mind.</p><disp-quote content-type="editor-comment"><p>You should probably also reference:</p><p>Warren, W. H. (1984). Perceiving affordances: Visual guidance of stair climbing. Journal of Experimental Psychology: Human Perception and Performance, 10(5), 683-703. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0096-1523.10.5.683">https://doi.org/10.1037/0096-1523.10.5.683</ext-link></p><p>Warren WH Jr, Young DS, Lee DN. Visual control of step length during running over irregular terrain. J Exp Psychol Hum Percept Perform. 1986 Aug;12(3):259-66. doi: 10.1037//0096-1523.12.3.259. PMID: 2943854.</p></disp-quote><p>We have added these references to the introduction.</p><disp-quote content-type="editor-comment"><p>Minor point</p><p>Related to (2) above, the path selection results are sometimes expressed a bit convolutedly, and the gist can get lost in the technical vocabulary. The generation of alternative &quot;paths&quot; and comparison of their slope and tortuousness parameters show that the participants preferred smaller slope/shorter paths. So, as far as I can tell, what this says is that in rugged terrain people like paths that are as &quot;flat&quot; as possible. This is common sense so hardly surprising. Do not be afraid to say so, and to express the result in plain non-technical terms. That an apple falls from a tree is common sense and hardly surprising. Yet quantifying the phenomenon, and carefully assessing the parameters of the path that the apple takes, turned out to be scientifically valuable - even if the observation itself lacked &quot;novelty&quot;.</p></disp-quote><p>Thanks. We have tried to clarify the methods/results with this in mind.</p></body></sub-article></article>