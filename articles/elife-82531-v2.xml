<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">82531</article-id><article-id pub-id-type="doi">10.7554/eLife.82531</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Fast rule switching and slow rule updating in a perceptual categorization task</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-151333"><name><surname>Bouchacourt</surname><given-names>Flora</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8893-0143</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-289543"><name><surname>Tafazoli</surname><given-names>Sina</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1926-0227</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-220348"><name><surname>Mattar</surname><given-names>Marcelo G</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3303-2490</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-59885"><name><surname>Buschman</surname><given-names>Timothy J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1298-2761</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-41719"><name><surname>Daw</surname><given-names>Nathaniel D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5029-1430</contrib-id><email>ndaw@princeton.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hx57361</institution-id><institution>Princeton Neuroscience Institute and the Department of Psychology</institution></institution-wrap><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>Department of Cognitive Science, University of California, San Diego</institution></institution-wrap><addr-line><named-content content-type="city">San Diego</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Badre</surname><given-names>David</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>14</day><month>11</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e82531</elocation-id><history><date date-type="received" iso-8601-date="2022-08-08"><day>08</day><month>08</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-11-13"><day>13</day><month>11</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-01-30"><day>30</day><month>01</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.01.29.478330"/></event></pub-history><permissions><copyright-statement>© 2022, Bouchacourt, Tafazoli et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Bouchacourt, Tafazoli et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-82531-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-82531-figures-v2.pdf"/><abstract><p>To adapt to a changing world, we must be able to switch between rules already learned and, at other times, learn rules anew. Often we must do both at the same time, switching between known rules while also constantly re-estimating them. Here, we show these two processes, rule switching and rule learning, rely on distinct but intertwined computations, namely fast inference and slower incremental learning. To this end, we studied how monkeys switched between three rules. Each rule was compositional, requiring the animal to discriminate one of two features of a stimulus and then respond with an associated eye movement along one of two different response axes. By modeling behavior, we found the animals learned the axis of response using fast inference (<italic>rule switching</italic>) while continuously re-estimating the stimulus–response associations within an axis (<italic>rule learning</italic>). Our results shed light on the computational interactions between rule switching and rule learning, and make testable neural predictions for these interactions.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>computational neuroscience</kwd><kwd>rule learning</kwd><kwd>behavioral analysis</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000183</institution-id><institution>U.S. Army Research Office</institution></institution-wrap></funding-source><award-id>ARO W911NF-16-1-047</award-id><principal-award-recipient><name><surname>Daw</surname><given-names>Nathaniel D</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>NIMH</institution></institution-wrap></funding-source><award-id>R01MH129492</award-id><principal-award-recipient><name><surname>Buschman</surname><given-names>Timothy J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Flexible cognition relies on a combination of fast inferential learning and slow incremental learning.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Intelligence requires learning from the environment, allowing one to modify their behavior in light of experience. A long tradition of research in areas like Pavlovian and instrumental conditioning has focused on elucidating general-purpose trial-and-error learning mechanisms – especially error-driven learning rules associated with dopamine and the basal ganglia (<xref ref-type="bibr" rid="bib27">Daw and O’Doherty, 2014</xref>; <xref ref-type="bibr" rid="bib26">Daw and Shohamy, 2008</xref>; <xref ref-type="bibr" rid="bib28">Daw and Tobler, 2014</xref>; <xref ref-type="bibr" rid="bib32">Dolan and Dayan, 2013</xref>; <xref ref-type="bibr" rid="bib34">Doya, 2007</xref>; <xref ref-type="bibr" rid="bib63">O’Doherty et al., 2004</xref>; <xref ref-type="bibr" rid="bib64">O’Reilly and Frank, 2006</xref>; <xref ref-type="bibr" rid="bib73">Rescorla, 1988</xref>; <xref ref-type="bibr" rid="bib80">Schultz et al., 1997</xref>; <xref ref-type="bibr" rid="bib89">Yin and Knowlton, 2006</xref>; <xref ref-type="bibr" rid="bib29">Day et al., 2007</xref>; <xref ref-type="bibr" rid="bib8">Bayer and Glimcher, 2005</xref>; <xref ref-type="bibr" rid="bib52">Lau and Glimcher, 2008</xref>; <xref ref-type="bibr" rid="bib77">Samejima et al., 2005</xref>; <xref ref-type="bibr" rid="bib65">Padoa-Schioppa and Assad, 2006</xref>). This type of learning works by incremental adjustment that can allow animals to gradually learn an arbitrary task – such as a new stimulus–response discrimination rule. However, in other circumstances, learning can also be quicker and more specialized: for instance, if two different stimulus–response rules are repeatedly reinforced in alternation, animals can come to switch between them more rapidly (<xref ref-type="bibr" rid="bib2">Asaad et al., 1998</xref>; <xref ref-type="bibr" rid="bib74">Rougier et al., 2005</xref>; <xref ref-type="bibr" rid="bib48">Harlow, 1949</xref>). These more task-specialized dynamics are often modeled by a distinct computational mechanism, such as Bayesian latent state inference, where animals accumulate evidence about which of several ‘latent’ (i.e., not directly observable) rules currently applies (<xref ref-type="bibr" rid="bib78">Sarafyazd and Jazayeri, 2019</xref>; <xref ref-type="bibr" rid="bib23">Collins and Koechlin, 2012</xref>; <xref ref-type="bibr" rid="bib10">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib42">Gershman et al., 2014</xref>; <xref ref-type="bibr" rid="bib7">Bartolo and Averbeck, 2020</xref>; <xref ref-type="bibr" rid="bib70">Qi et al., 2022</xref>; <xref ref-type="bibr" rid="bib83">Stoianov et al., 2016</xref>). Such inference is associated with activity in the prefrontal cortex (<xref ref-type="bibr" rid="bib36">Durstewitz et al., 2010</xref>; <xref ref-type="bibr" rid="bib57">Milner, 1963</xref>; <xref ref-type="bibr" rid="bib12">Boettiger and D’Esposito, 2005</xref>; <xref ref-type="bibr" rid="bib60">Nakahara et al., 2002</xref>; <xref ref-type="bibr" rid="bib41">Genovesio et al., 2005</xref>; <xref ref-type="bibr" rid="bib13">Boorman et al., 2009</xref>; <xref ref-type="bibr" rid="bib50">Koechlin and Hyafil, 2007</xref>; <xref ref-type="bibr" rid="bib49">Koechlin et al., 2003</xref>; <xref ref-type="bibr" rid="bib76">Sakai and Passingham, 2003</xref>; <xref ref-type="bibr" rid="bib4">Badre et al., 2010</xref>; <xref ref-type="bibr" rid="bib56">Miller and Cohen, 2001</xref>; <xref ref-type="bibr" rid="bib1">Antzoulatos and Miller, 2011</xref>; <xref ref-type="bibr" rid="bib72">Reinert et al., 2021</xref>; <xref ref-type="bibr" rid="bib54">Mansouri et al., 2020</xref>), suggesting that its neural mechanisms are distinct from incremental learning. Lesioning prefrontal cortex impairs performance on tasks that require rule inference (<xref ref-type="bibr" rid="bib57">Milner, 1963</xref>; <xref ref-type="bibr" rid="bib31">Dias et al., 1996</xref>) and neurons in prefrontal cortex track the currently inferred rule (<xref ref-type="bibr" rid="bib53">Mansouri et al., 2006</xref>).</p><p>In its simplest form, this type of latent state inference process presupposes that the animals have previously learned about the structure of the task. They must know the set of possible rules, how often they switch, etc. For this reason, latent state inference has typically been studied in well-trained animals (<xref ref-type="bibr" rid="bib78">Sarafyazd and Jazayeri, 2019</xref>; <xref ref-type="bibr" rid="bib3">Asaad et al., 2000</xref>; <xref ref-type="bibr" rid="bib88">White and Wise, 1999</xref>). There has been increasing theoretical interest – but relatively little direct empirical evidence – in the mechanisms by which the brain learns the broader structure of the task in order to build task-specialized inference mechanisms for rapid rule switching. For Bayesian latent state inference models, this problem corresponds to learning the generative model of the task, for example inferring a mixture model over latent states (rules or task conditions) and their properties (e.g., stimulus–response–reward contingencies) (<xref ref-type="bibr" rid="bib78">Sarafyazd and Jazayeri, 2019</xref>; <xref ref-type="bibr" rid="bib23">Collins and Koechlin, 2012</xref>; <xref ref-type="bibr" rid="bib24">Collins and Frank, 2016</xref>; <xref ref-type="bibr" rid="bib79">Schuck et al., 2016</xref>; <xref ref-type="bibr" rid="bib22">Chan et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Hampton et al., 2006</xref>; <xref ref-type="bibr" rid="bib37">Frank and Badre, 2012</xref>; <xref ref-type="bibr" rid="bib69">Purcell and Kiani, 2016</xref>). In principle, this too could be accomplished by hierarchical Bayesian inference over which of several already specified rules currently applies and over the space of possible rules, as in Chinese restaurant process models. However, such hierarchical inference is less tractable and hard to connect to neural mechanisms.</p><p>Perhaps most intriguingly, one solution to the lack of a full process-level model of latent state learning is that rule learning and rule switching involve an interaction between both major classes of learning mechanisms – latent state inference and incremental trial-and-error learning. Thus, in Bayesian inference models, it is often hypothesized that an inferential process decides which latent state is in effect (e.g., in prefrontal cortex), while the properties of each state are learned, conditional on this, by downstream error-driven incremental learning (e.g, in the striatum) (<xref ref-type="bibr" rid="bib65">Padoa-Schioppa and Assad, 2006</xref>; <xref ref-type="bibr" rid="bib23">Collins and Koechlin, 2012</xref>; <xref ref-type="bibr" rid="bib37">Frank and Badre, 2012</xref>; <xref ref-type="bibr" rid="bib81">Seo et al., 2012</xref>; <xref ref-type="bibr" rid="bib75">Rushworth et al., 2011</xref>; <xref ref-type="bibr" rid="bib6">Balewski et al., 2022</xref>). However, these two learning mechanisms have mostly been studied in regimes where they operate in isolation (<xref ref-type="bibr" rid="bib78">Sarafyazd and Jazayeri, 2019</xref>; <xref ref-type="bibr" rid="bib3">Asaad et al., 2000</xref>; <xref ref-type="bibr" rid="bib88">White and Wise, 1999</xref>; <xref ref-type="bibr" rid="bib51">Lak et al., 2020</xref>; <xref ref-type="bibr" rid="bib19">Busse et al., 2011</xref>; <xref ref-type="bibr" rid="bib39">Fründ et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Gold et al., 2008</xref>; <xref ref-type="bibr" rid="bib85">Tsunada et al., 2019</xref>) and, apart from a few examples in human rule learning (<xref ref-type="bibr" rid="bib23">Collins and Koechlin, 2012</xref>; <xref ref-type="bibr" rid="bib24">Collins and Frank, 2016</xref>; <xref ref-type="bibr" rid="bib33">Donoso et al., 2014</xref>; <xref ref-type="bibr" rid="bib5">Badre and Frank, 2012</xref>; <xref ref-type="bibr" rid="bib15">Bouchacourt et al., 2020</xref>; <xref ref-type="bibr" rid="bib38">Franklin and Frank, 2018</xref>), their interaction has been limited to theoretical work.</p><p>To study rule switching and rule learning, we trained non-human primates to perform a rule-based category-response task. Depending on the rule in effect, the animals needed to attend to and categorize either the color or the shape of a stimulus, and then respond with a saccade along one of two different response axes. We observe a combination of both fast and slow learning during the task: monkeys rapidly switched into the correct response axis, consistent with inferential learning of the response state, while, within a state, the animals slowly learned category-response mappings, consistent with incremental (re)learning. This was true even though the animals were well trained on the task beforehand. To quantify the learning mechanisms underlying the animals’ behavior, we tested whether inference or incremental classes of models, separately, could explain the behavior. Both classes of models reproduced learning-like effects – that is dynamic, experience-driven changes in behavior. However, neither model could, by itself, explain the combination of both fast and slow learning. Importantly, the fact that a fully informed inferential learner could not explain the behavior also indicated that the observed fast and slow learning was not simply driven by informed adjustment to the task structure itself. Instead, we found that key features of behavior were well explained by a hybrid rule-switching and rule-learning model, which inferred which response axis was active while continually performing slower, incremental relearning of the consequent stimulus–response mappings within an axis. These results support the hypothesis that there are multiple, interacting, mechanisms that guide behavior in a contextually appropriate manner.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Task design and performance</title><p>Two rhesus macaques were trained to perform a rule-based category-response task. On each trial, the monkeys were presented with a stimulus that was composed of a color and shape (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). The shape and color of each stimulus were drawn from a continuous space and, depending on the current rule in effect, the animals categorized the stimulus according to either its color (red vs. green) or its shape (‘bunny’ vs.‘tee’, <xref ref-type="fig" rid="fig1">Figure 1b</xref>). Then, as a function of the category of the stimulus and the current rule, the animals made one of four different responses (an upper-left, upper-right, lower-left, or lower-right saccade).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Task design and performance (including all trials).</title><p>(<bold>a</bold>) Schematic of a trial. (<bold>b</bold>) Stimuli were drawn from a two-dimensional feature space, morphing both color (left) and shape (right). Stimulus categories are indicated by vertical lines and labels. (<bold>c</bold>) The stimulus–response mapping for the three rules, and an example of a block timeline. (<bold>d</bold>) Venn diagram showing the overlap between rules. Average performance (sample mean and standard error of the mean) for each rule, for (<bold>e</bold>) Monkey S and (<bold>f</bold>) Monkey C. Proportion of responses on the incorrect axis for the first 50 trials of each block for (<bold>g</bold>) Monkey S and (<bold>h</bold>) Monkey C. Insets: Trial number of the first response on the correct axis after a block switch, respectively, for Monkeys S and C.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig1-v2.tif"/></fig><p>Animals were trained on three different category-response rules (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). Rule 1 required the animal to categorize the shape of the stimulus, making a saccade to the upper-left location when the shape was categorized as a ‘bunny’ and a saccade to the lower-right location when the shape was categorized as a ‘tee’. These two locations – upper-left and lower-right – formed an ‘axis’ of response (<italic>Axis 1</italic>). Rule 2 was similar but required the animal to categorize the color of the stimulus and then respond on the opposite axis (<italic>Axis 2</italic>; red = upper-right, green = lower-left). Finally, Rule 3 required categorizing the color of the stimulus and responding on <italic>Axis 1</italic> (red = lower-right, green = upper-left). Note that these rules are compositional in nature, with overlapping dimensions (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). Rule 1 required categorizing the shape of the stimulus, while Rules 2 and 3 required categorizing the color of the stimulus. Similarly, Rules 1 and 3 required responding on the same axis (<italic>Axis 1</italic>), while Rule 2 required a different set of responses (<italic>Axis 2</italic>). In addition, the overlap in the response axis for Rules 1 and 3 meant certain stimuli had congruent responses for both rules (e.g., red-tee and green-bunny stimuli) while other stimuli had incongruent responses between rules (e.g., red-bunny and green-tee). For all rules, when the animal made a correct response, it received a reward (an incorrect response led to a short ‘time-out’).</p><p>Animals performed the same rule during a block of trials. Critically, the animals were not explicitly cued as to which rule was in effect for that block. Instead, they had to use information about the stimulus, their response, and reward feedback, to infer which rule was in effect. After the animals discovered the rule and were performing it at a high level (defined as &gt;70%, see Methods) the rule would switch. Although unpredictable, the moment of switching rules was cued to the animals (with a flashing screen). Importantly, this switch cue did not indicate which rule was now in effect (just that a switch had occurred).</p><p>To facilitate learning and performance, the sequence of rules across blocks was semi-structured such that the axis of response always changed following a block switch. This means that the animals always alternated between a Rule 2 block and either a Rule 1 or 3 block (chosen pseudo randomly), and that Rule 2 thus occurred twice as frequently as the other rules. Note that the cue implied an axis switch but did not instruct which axis to use, thus the animals must learn and continually track on which axis to respond.</p><p>Overall, both monkeys performed the task well above chance (<xref ref-type="fig" rid="fig1">Figure 1e, f</xref>). When the rule switched to Rule 2, the animals quickly switched their behavior: Monkey S responded correctly on the first trial in 81%, confidence interval (CI) = [0.74,0.87] of Rule 2 blocks, and reached 91%, CI = [0.85,0.95] after only 20 trials (Monkey C being, respectively, at 78%, CI = [0.65,0.88]; and 85%, CI = [0.72,0.92]). In Rules 1 and 3, their performance also exceeded chance level quickly. In Rule 1, although the performance of Monkey S was below chance on the first trial (0%, CI = [0,0.052]; 46%, CI = [0.28,0.65] for Monkey C), reflecting perseveration on the previous rule, performance quickly climbed above chance (77% after 50 trials, CI = [0.66,0.85]; 63%, CI = [0.43,0.79] for Monkey C). A similar pattern was seen for Rule 3 (initial performance of 1.5%, CI = [0.0026,0.079] and 78%, CI = [0.67,0.86] after 50 trials for Monkey S; 41%, CI = [0.25,0.59] and 67%, CI = [0.48,0.81] for Monkey C, respectively).</p><p>While the monkeys performed all three rules well, there were two interesting behavioral phenomena. First, the monkeys were slower to switch to Rules 1 and 3 than to switch to Rule 2. On the first 20 trials, the difference in average percent performance of Monkey S was <italic>Δ</italic> = 35 between Rules 2 and 1, and <italic>Δ</italic> = 22 between Rules 2 and 3 (significant Fisher’s test comparing Rule 2 to Rules 1 and 3, with p &lt; 10<sup>−4</sup> in both conditions; respectively, <italic>Δ</italic> = 35, <italic>Δ</italic> = 24 and p &lt; 10<sup>−4</sup> for Monkey C).</p><p>Second, both monkeys learned the axis of response nearly instantaneously. After a switch cue, Monkey S almost always responded on Axis 2 (the response axis consistent with Rule 2; 97%, CI = [0.90,0.99] in Rule 1; 97%, CI = [0.92,0.98] in Rule 2; 97%, CI = [0.90,0.99] in Rule 3; see <xref ref-type="fig" rid="fig1">Figure 1g</xref>). Then, if this was incorrect, it switched to the correct axis within five trials on 97%, CI = [0.90,0.99] of blocks of Rule 1, and 94% CI = [0.86,0.98] of blocks of Rule 3. Monkey C instead tended to alternate the response axis on the first trial following a switch cue (it made a response on the correct axis on the first trial with a probability of 71%, CI = [0.51,0.85] in Rule 1; 85%, CI = [0.72,0.92] in Rule 2; and 84%, CI = [0.55,0.87] in Rule 3), implying an understanding of the pattern of axis changes with block switches (<xref ref-type="fig" rid="fig1">Figure 1h</xref>). Both monkeys maintained the correct axis with very few off-axis responses throughout the block (at trial 20, Monkey S: 1.4%, CI = [0.0025,0.077] in Rule 1; 2.1%, CI = [0.0072,0.060] in Rule 2; 0%, CI = [0,0.053] in Rule 3; Monkey C: 0%, CI = [0,0.14] in Rule 1; 4.3%, CI = [0.012,0.15] in Rule 2; 3.7%, CI = [0.0066,0.18] in Rule 3). These results suggest the animals were able to quickly identify the axis of response but took longer (particularly for Rules 1 and 3) to learn the correct mapping between stimulus features and responses within an axis.</p></sec><sec id="s2-2"><title>Learning rules de novo cannot capture the behavior</title><p>To perform the task, the animals had to learn which rule was in effect during each block of trials. This required determining both the response axis and the relevant feature. As noted above, the monkeys’ behavior suggests learning was a mixture of fast switching, reminiscent of inference models, and slow refinement, as in error-driven incremental learning. Given this, we began by testing whether the inference or incremental classes of models could capture the animals’ behavior. As in previous work (<xref ref-type="bibr" rid="bib30">Dayan and Daw, 2008</xref>; <xref ref-type="bibr" rid="bib68">Pouget et al., 2016</xref>; <xref ref-type="bibr" rid="bib67">Pouget et al., 2013</xref>; <xref ref-type="bibr" rid="bib43">Gold and Shadlen, 2001</xref>; <xref ref-type="bibr" rid="bib11">Bichot and Schall, 1999</xref>), all our models shared common noisy perceptual input and action selection stages (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, and Methods). As we detail next, the intervening mechanism for mapping stimulus to action value differed between models.</p><p>First, we fit an error-driven learning model, which gradually relearns the stimulus–response mappings de novo at the start of each block, to the monkeys’ behavior. This class of models works by learning the reward expected for different stimulus–response combinations, using incremental running averages to smooth out trial-to-trial stochasticity in reward realization – here, due to perceptual noise in the stimulus classification. In particular, we fit a variant of Q learning (model QL, see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> and Methods) that was elaborated to improve its performance in this task: for each action, model QL parameterized the mapping from stimulus to reward linearly using two basis functions over the feature space (one binary indicator each for color and shape), and used error-driven learning to estimate the appropriate weights on these for each block. This scheme effectively builds-in the two relevant feature-classification rules (shape and color), making the generous assumption that the animals had already learned the categories. In addition, the model resets the weights to fixed initial values at each block switch, allowing the model to start afresh and avoiding the need to unlearn. Yet, even with these built-in advantages, the model was unable to match the animals’ ability to rapidly switch axes, but instead relearned the feature–response associations after each block switch (simulations under best-fitting parameters shown in <xref ref-type="fig" rid="fig2">Figure 2</xref> for Monkey S, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> for Monkey C). Several tests verified the model learned more slowly than the animals (<xref ref-type="fig" rid="fig2">Figure 2a, b</xref>). For instance, the model fitted on Monkey S’s behavior responded on Axis 2 on the first trial of the block only 50% of the time in all three rules (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, Fisher’s test of model simulations against data: p &lt; 10<sup>−4</sup> for the three rules). The model thus failed to capture the initial bias of Monkey S for Rule 2 discussed above. Importantly, the model switched to the correct axis within five trials on only 58% of blocks of Rule 1, and 57% of blocks of Rule 3 (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, Fisher’s test against monkey behavior: p &lt; 10<sup>−4</sup>). Finally, the model performed 24% of off-axis responses after 20 trials in Rule 1, 21% in Rule 2, and 21% in Rule 3, all much higher than what was observed in the monkey’s behavior (Fisher’s test p &lt; 10<sup>−4</sup>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Incremental learner (QL) model fitted on Monkey S behavior (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> for Monkey C).</title><p>(<bold>a</bold>) Trial number of the first response of the model on the correct axis after a block switch (compare to <xref ref-type="fig" rid="fig1">Figure 1g</xref>, inset). (<bold>b</bold>) Proportion of responses of the model on the incorrect axis for the first 50 trials of each block (compare to <xref ref-type="fig" rid="fig1">Figure 1g</xref>). (<bold>c</bold>) Model performance for each rule (averaged over blocks, compare to <xref ref-type="fig" rid="fig1">Figure 1e</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>The three models.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Incremental learner (QL) model fitted on Monkey C behavior (see <xref ref-type="fig" rid="fig2">Figure 2</xref> for Monkey S).</title><p>(<bold>a</bold>) Trial number of the first response of the model on the correct axis after a block switch (compare to <xref ref-type="fig" rid="fig1">Figure 1h</xref>, inset). (<bold>b</bold>) Proportion of responses of the model on the incorrect axis for the first 50 trials of each block (compare to <xref ref-type="fig" rid="fig1">Figure 1h</xref>). (<bold>c</bold>) Model performance for each rule (averaged over blocks, compare to <xref ref-type="fig" rid="fig1">Figure 1f</xref>). <italic>Statistics of QL model fitted on Monkey C</italic>: First, the model made a response on the correct axis on the first trial with a probability of only 50% in Rules 1, 2, and 3. The model performed 28% of off-axis responses after 20 trials in Rule 1, and 25% in Rules 2 and 3. Second, the model performed correctly on the first trial in only 25% of Rule 2 blocks, and reached only 51% after 20 trials. As a result, on the first 20 trials, the difference in average percent performance was only <italic>Δ</italic> = 3.3 between Rules 2 and 1, and only <italic>Δ</italic> = −0.39 between Rules 2 and 3.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig2-figsupp2-v2.tif"/></fig></fig-group><p>In addition, because of the need to relearn feature–response associations after each block switch, the incremental QL model was unable to capture the dichotomy between the monkeys’ slower learning in Rules 1 and 3 (which share Axis 1) and the faster learning of Rule 2 (using Axis 2). As noted above, the monkeys performed Rule 2 at near asymptotic performance from the beginning of the block but were slower to learn which feature to attend to on blocks of Rules 1 and 3 (<xref ref-type="fig" rid="fig1">Figure 1g, h</xref>). In contrast, because the model learned each rule in the same way, the incremental learner performed similarly on all three rules (<xref ref-type="fig" rid="fig2">Figure 2c</xref> for Monkey S, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2c</xref> for Monkey C). In particular, it performed correctly on the first trial in only 25% of Rule 2 blocks (Fisher’s test against behavior: p &lt; 10<sup>−4</sup>), and reached only 62% after 20 trials (p &lt; 10<sup>−4</sup>). As a result, on the first 20 trials, the difference in average percent performances was only <italic>Δ</italic> = 4.0 between Rules 2 and 1, and was only <italic>Δ</italic> = 0.76 between Rules 2 and 3 (similar results were seen when fitting the model to Monkey C, see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). The same pattern of results was seen when the initial weights were free parameters (see Methods).</p><p>Altogether, these results argue simple incremental relearning of the axes and features de novo cannot reproduce the animals’ ability to instantaneously relearn the correct axis after a block switch or the observed differences in learning speed between the rules.</p></sec><sec id="s2-3"><title>Pure inference of previously learned rules cannot capture the behavior</title><p>The results above suggest that incremental learning is too slow to explain the quick switch between response axes displayed by the monkeys. So, we tested whether a model that leverages Bayesian inference can capture the animals’ behavior. A fully informed Bayesian ideal observer model (IO, see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> and Methods) uses statistical inference to continually estimate which of the three rules is in effect, accumulating evidence (‘beliefs’) for each rule based on the history of previous stimuli, actions, and rewards. The IO model chooses the optimal action for any given stimulus, by averaging the associated actions’ values under each rule, weighted by the estimated likelihood that each rule is in effect. Like incremental learning, the IO model learns and changes behavior depending on experience. However, unlike incremental models, this model leverages perfect knowledge of the rules to learn rapidly, limited only by stochasticity in the evidence. Here, noisy stimulus perception is the source of such stochasticity, limiting both the speed of learning and asymptotic performance. Indeed, the IO model predicts the speed of initial (re)learning after a block switch should be coupled to the asymptotic level of performance. Furthermore, given that perceptual noise is shared across rules, the IO model also predicts the speed of learning will be the same for rules that use the same features.</p><p>As expected, when fit to the animal’s behavior, the IO model reproduced the animals’ ability to rapidly infer the correct axis (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a, b, d, e</xref>). For example, when fitted to Monkey S behavior, the model initially responded on Axis 2 almost always immediately after each block switch cue (96% in all rules, Fisher’s test against monkey’s behavior p &gt; 0.4). Then, if this was incorrect, the model typically switched to the correct axis within five trials on 89% of blocks of Rule 1, and 95% of blocks of Rule 3 (Fisher’s test against monkey behavior: p &gt; 0.2 in both rules). The model maintained the correct axis with very few off-axis responses throughout the block (after trial 20, 1.3% in Rule 1; 1.1% in Rule 2; 1.2% in Rule 3; Fisher’s test against monkey’s behavior: p &gt; 0.6 in all rules).</p><p>However, the IO model could not capture the observed differences in learning speed for the different rules (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1c, f</xref>). To understand why, we looked at performance as a function of stimulus difficulty. As expected, the monkey’s performance depended on how difficult it was to categorize the stimulus (i.e., the morph level; psychometric curves shown in <xref ref-type="fig" rid="fig3">Figure 3a–c</xref> for Monkey S, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2a–c</xref> for Monkey C). For example, in color blocks (Rules 2 and 3), the monkeys performed better for a ‘prototype’ red stimulus than for a ‘morphed’ orange stimulus (<xref ref-type="fig" rid="fig3">Figure 3a–c</xref>). Indeed, on ‘early trials’ (first 50 trials) of Rule 2, Monkey S correctly responded to 96% (CI = [0.95,0.97]) of prototype stimuli, and only to 91%, CI = [0.90,0.92] of ‘morphed’ stimuli (p &lt; 10<sup>−4</sup>; similar results for Monkey C in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Rule 3 had a similar ordering: Monkey S correctly responded to 80% (CI = [0.77,0.82]) of prototype stimuli, and only 62%, CI = [0.60,0.64] of ‘morphed’ stimuli (p &lt; 10<sup>−4</sup>). This trend continued as the animal learned Rule 3 (trials 50–200; 89%, CI = [0.88,0.90] and 74%, CI = [0.73,0.75], respectively, for prototype and morphed stimuli, p &lt; 10<sup>−4</sup>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The ideal observer (IO), slow or fast, but not both.</title><p>Fitted on Monkey S behavior (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for Monkey C). Note that data were collapsed across 50%/150%; 30%/70%/130%/170%; and 0%/100% (non-collapsed psychometric functions can be seen in <xref ref-type="fig" rid="fig5">Figure 5</xref>). (<bold>a–c</bold>) Performance for Rules 1, 2, and 3, as a function of the morphed version of the relevant feature. (<bold>d–f</bold>) Performance for Rules 1, 2, and 3, for IO model with high color noise. This parameter regime corresponds to the case where the model is fitted to the monkey’s behavior (see Methods). (<bold>g–i</bold>) Performance for Rules 1, 2, and 3, for IO model with low color noise. Here, we fixed <italic>κ</italic><sub>C</sub> = 6.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Ideal observer (IO) model fitted on Monkeys S and C.</title><p>(<bold>a–c</bold>) IO model fitted on Monkey S behavior. (<bold>a</bold>) Trial number of the first response of the model on the correct axis after a block switch (compare to <xref ref-type="fig" rid="fig1">Figure 1e</xref>, inset). (<bold>b</bold>) Proportion of responses of the model on the incorrect axis for the first 50 trials of each block (compare to <xref ref-type="fig" rid="fig1">Figure 1e</xref>). (<bold>c</bold>) Model performance for each rule (averaged over blocks, compare to <xref ref-type="fig" rid="fig1">Figure 1g</xref>). (<bold>d–f</bold>) Same, but for Monkey C. <italic>Statistics of IO model fitted on Monkey C</italic>: The model responded on the correct axis on the first trial with a probability of 63% in Rule 1, 38% in Rule 2, and 60% in Rule 3. The model maintained the correct axis with very few off-axis responses throughout the block (after trial 20, 1.3% in Rule 1; 6.8%, in Rule 2; 1.3% in Rule 3, Fisher’s test against monkey behavior: p &gt; 0.5 in all rules).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>The ideal observer (IO), slow or fast, but not both.</title><p>Fitted on Monkey C behavior (see <xref ref-type="fig" rid="fig3">Figure 3</xref> for Monkey S). (<bold>a–c</bold>) Performance for Rules 1, 2, and 3, as a function of the morphed version of the relevant feature. (<bold>d–f</bold>) Performance for Rules 1, 2, and 3, for IO model with high color noise. This parameter regime corresponds to the case where the model is fitted to the monkey’s behavior (see Methods). (<bold>g–i</bold>) Performance for Rules 1, 2, and 3, for IO model with low color noise. Here, we fixed <italic>κ</italic><sub>C</sub> = 6. <italic>Statistics on Monkey C</italic>: There was a discrepancy between the performance of ‘morphed’ stimuli in Rule 2 versus Rule 3, with a difference in average percent performances of <italic>Δ</italic> = 33 for the first 50 trials in both rules (p &lt; 10<sup>−4</sup>), and still <italic>Δ</italic> = 24 if we considered Rule 2 against the last trials of Rule 3 (p &lt; 10<sup>−4</sup>). The same discrepancy was observed between the performance of ‘prototype’ stimuli in Rule 2 versus Rule 3, with a difference in average percent performances of <italic>Δ</italic> = 27 for the first 50 trials in both rules (p &lt; 10<sup>−4</sup>), and still <italic>Δ</italic> = 21 if we considered the last trials of Rule 3 (p &lt; 10<sup>−4</sup>). <italic>Statistics of IO model fitted on Monkey C</italic>: While the IO model, using best-fit parameters, reproduced poor asymptotic performance in Rule 3 by increasing color noise (low concentration), it then failed to capture the high performance on Rule 2 early on. The resulting difference in performance for ‘morphed’ stimuli was only <italic>Δ</italic> = 4.8 for the first 50 trials and <italic>Δ</italic> = −7.2 if we considered the last trials of Rule 3 (respectively, <italic>Δ</italic> = 5.1 and <italic>Δ</italic> = −8.6 for ‘prototype’).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>The ideal observer model including a correct generative prior on the transition between axes given by the specific task structure.</title><p>This is defined by fixing the values of the initial belief states over rules to (<italic>b</italic><sub>1</sub> = 0; <italic>b</italic><sub>2</sub> = 1; <italic>b</italic><sub>3</sub> = 0) for a Rule 2 block, and (<italic>b</italic><sub>1</sub> = 0.5; <italic>b</italic><sub>2</sub> = 0; <italic>b</italic><sub>3</sub> = 0.5) for a Rule 1 or 3 block. The model is fitted on Monkeys S and C. (a,b,c,g,h,i) are similar to Figure 2a,b,c. (d,e,f,j,k,l) are similar to Figure 3d,e,f.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig3-figsupp3-v2.tif"/></fig></fig-group><p>Importantly, there was a discrepancy between the performance on ‘morphed’ stimuli in Rule 2 versus Rule 3, with a difference in average percent performance of <italic>Δ</italic> = 28 for the first 50 trials in both rules (p &lt; 10<sup>−4</sup>). This was still true, even if we considered Rule 2 against the last trials of Rule 3 (<italic>Δ</italic> = 17, p &lt; 10<sup>−4</sup>). The same discrepancy was observed between the performance on ‘prototype’ stimuli in Rule 2 versus Rule 3, with a difference in average percent performance on <italic>Δ</italic> = 16 for the first 50 trials in both rules (p &lt; 10<sup>−4</sup>), and <italic>Δ</italic> = 6.8 if comparing to the last trials of Rule 3 (p &lt; 10<sup>−4</sup>).</p><p>The IO model captured the performance ordering on morphed and prototype stimuli for each rule (<xref ref-type="fig" rid="fig3">Figure 3d–i</xref>, similar results for the model reproducing Monkey C, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). However, the model performed similarly for morphed stimuli on Rules 2 and 3. This is because both rules involve categorizing color and so they shared the same perceptual noise, leading to the same likelihood of errors. Furthermore, because perceptual noise limits learning and asymptotic performance in the IO model, it predicts the speed of learning should be shared across Rules 2 and 3, and initial learning in both rules on the first 50 trials should be coupled to the asymptotic performance. Given this, the model had to trade-off between behavioral performance in Rules 2 and 3. Using best-fit parameters, the model reproduced the animals’ lower asymptotic performance in Rule 3 by increasing color noise, and so it failed to capture the high performance on Rule 2 early on (<xref ref-type="fig" rid="fig3">Figure 3e, f</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2e, f</xref>). The resulting difference in average percent performance for ‘morphed’ stimuli was only <italic>Δ</italic> = 4.0 for the first 50 trials and <italic>Δ</italic> = 0.0044 if we considered the last trials of Rule 3 (respectively, <italic>Δ</italic> = 4.2 and <italic>Δ</italic> = 0.032 for ‘prototype’). Conversely, if we forced the model to improve color perception (by reducing perceptual noise, <xref ref-type="fig" rid="fig3">Figure 3h, i</xref>, and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2h, i</xref>), then it was able to account for the monkeys’ performance on Rule 2, but failed to match the animals’ behavior on Rule 3. The resulting difference in average percent performance was again only <italic>Δ</italic> = 3.4 for the first 50 trials, and <italic>Δ</italic> = −0.17 if we considered the last trials of Rule 3 (respectively, <italic>Δ</italic> = 3.2 and <italic>Δ</italic> = −0.16 for ‘prototype’).</p><p>One might be concerned that including a correct generative prior on the transition between axes given by the specific task structure would solve this issue, as a Rule 2 block is always following a Rule 1 or 3 block, hence possibly creating an inherent discrepancy in learning Rule 2 versus Rules 1 and 3. However, the limiting factor was not the speed for axis discovery (which was nearly instantaneous, cf above), but the shared perceptual color noise between Rules 2 and 3, coupling initial learning to asymptotic performance. Such a modified IO could not account for the behavior (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>).</p></sec><sec id="s2-4"><title>The key features of monkeys’ behavior are reproduced by a hybrid model composing inference over axes and incremental relearning over features</title><p>To summarize, the main characteristics of the animals’ behavior were (1) rapid learning of the axis of response after a block switch, (2) immediately high behavioral performance of Rule 2, the only rule on Axis 2, and (3) slower relearning of Rules 1 and 3, which mapped different features onto Axis 1. Altogether, these results suggest that the animals learned axes and features separately, with fast learning of the axes and slower learning of the features. One way to conceive this is as a Bayesian inference model (similar to IO), but relaxing the assumption that the animal had perfect knowledge of the underlying rules (i.e., all of the stimulus–action–reward contingencies). We propose that the animals maintained two latent states (e.g., one corresponding to each axis of response) instead of the three rules we designed. Assuming each state had its own stimulus–action–reward mappings, the mappings would be stable for Rule 2 (Axis 2) but continually re-estimated for Rules 1 and 3 (Axis 1). To test this hypothesis, we implemented a hybrid model that inferred the axis of response while incrementally learning which features to attend for that response axis (Hybrid Q Learner, ‘HQL’ in the Methods, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). In the model, the current axis of response was inferred through Bayesian evidence accumulation (as in the IO model), while feature–response weights were incrementally learned for each axis of response.</p><p>Intuitively, this model could explain all three core behavioral observations. First, inference allows for rapid switching between axes. Second, because only Rule 2 mapped to Axis 2, the weights for Axis 2 did not change and so the model was able to perform well on Rule 2 immediately. Third, because Rules 1 and 3 shared an axis of response, and, thus, a single set of feature–response association weights, this necessitated relearning associations for each block, reflected in the animal’s slower learning for these rules.</p><p>Consistent with this intuition, the HQL model provided an accurate account of the animals’ behavior. First, unlike the QL model, the HQL model reproduced the fast switch to the correct axis (<xref ref-type="fig" rid="fig4">Figure 4a, b</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1a, bCited</xref>, and <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2a–c and g–i</xref>). Fitted to Monkey S behavior, the model initially responded on Axis 2 immediately after each block switch cue (91% in Rule 1, 89% in Rules 2 and 3, Fisher’s test against monkey’s behavior p &gt; 0.05). Then, if this was incorrect, the model switched to the correct axis within five trials on 91% of blocks of Rules 1 and 3 (Fisher’s test against behavior: p &gt; 0.2 in both rules). Similar to the animals, the model maintained the correct axis with very few off-axis responses throughout the block (on trial 20, 1.4% in Rule 1; 1.3%, in Rule 2; 1.5% in Rule 3, Fisher’s test against monkey’s behavior: p &gt; 0.7 in all rules).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>The hybrid learner (HQL) accounts both for fast switching to the correct axis, and slow relearning of Rules 1 and 3.</title><p>Model fit on Monkey S, see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for Monkey C. (<bold>a</bold>) Trial number for the first response on the correct axis after a block switch, for the model (compare to <xref ref-type="fig" rid="fig1">Figure 1e</xref> inset). (<bold>b</bold>) Proportion of responses on the incorrect axis for the first 50 trials of each block, for the model (compare to <xref ref-type="fig" rid="fig1">Figure 1e</xref>). (<bold>c</bold>) Performance of the model for the three rules (compare to <xref ref-type="fig" rid="fig1">Figure 1g</xref>). (<bold>d–f</bold>) Performance for Rules 1, 2, and 3, as a function of the morphed version of the relevant feature.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>The hybrid learner (HQL) accounts both for fast switching to the correct axis, and slow relearning of Rules 1 and 3.</title><p>Model fit on Monkey C, see <xref ref-type="fig" rid="fig4">Figure 4</xref> for Monkey S. (<bold>a</bold>) Trial number for the first response on the correct axis after a block switch, for the model (compare to <xref ref-type="fig" rid="fig1">Figure 1f</xref> inset). (<bold>b</bold>) Proportion of responses on the incorrect axis for the first 50 trials of each block, for the model (compare to <xref ref-type="fig" rid="fig1">Figure 1f</xref>). (<bold>c</bold>) Performance of the model for the three rules (compare to <xref ref-type="fig" rid="fig1">Figure 1h</xref>). (<bold>d–f</bold>) Performance for Rules 1, 2, and 3, as a function of the morphed version of the relevant feature. <italic>Statistics of HQL model fitted on Monkey C</italic>: First, the model responded on the correct axis on the first trial with a probability of 50% in Rule 1, 54% in Rule 2, and 47% in Rule 3. The model maintained the correct axis with very few off-axis responses throughout the block (after trial 20, 1.5% in Rule 1; 1.7%, in Rule 2; 1.5% in Rule 3, Fisher’s test against monkey’s behavior: p &gt; 0.5 in all rules). Second, the HQL model could capture the animal’s fast performance on Rule 2 and slower performance on Rules 1 and 3: the difference in average percent performances on the first 20 trials was <italic>Δ</italic> = 28 both between Rules 2 and 1 and between Rules 2 and 3. Third, not only the HQL model captured the performance ordering on morphed and prototype stimuli for each rule separately, but the model was able to trade-off between initial and asymptotic behavioral performance in Rules 2 and 3, for both ‘morphed’ and ‘prototype’ stimuli. The resulting difference in performance for ‘morphed’ stimuli was <italic>Δ</italic> = 29 for the first 50 trials and <italic>Δ</italic> = 24 if we considered the last trials of Rule 3 (respectively, <italic>Δ</italic> = 29 and <italic>Δ</italic> = 22 for ‘prototype’).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>The hybrid learner, beliefs, and weights.</title><p>(<bold>a–c</bold>) Belief over axes when the model is fitted on Monkey S. (<bold>d–f</bold>) Feature weights values when the model is fitted on Monkey S. (<bold>g–i</bold>) Belief over axes when the model is fitted on Monkey C. (<bold>j–l</bold>) Feature weights values when the model is fitted on Monkey C.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig4-figsupp2-v2.tif"/></fig></fig-group><p>Second, contrary to the IO model, the HQL model captured the animals’ fast performance on Rule 2 and slower performance on Rules 1 and 3 (<xref ref-type="fig" rid="fig4">Figure 4c</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1c</xref>). As detailed above, animals were significantly better on Rule 2 than Rules 1 and 3 on the first 20 trials. The model captured this difference: fitted on Monkey S’s behavior, the difference in average percent performance on the first 20 trials was <italic>Δ</italic> = 31 between Rules 2 and 1, and <italic>Δ</italic> = 29 between Rules 2 and 3 (a Fisher’s test against monkey’s behavior gave p &gt; 0.05 for the first trial, p &gt; 0.1 for trial 20).</p><p>Third, the HQL model captured the trade-off between the animals’ initial learning rate and asymptotic behavioral performance in Rules 2 and 3 (<xref ref-type="fig" rid="fig4">Figure 4d–f</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1d–f</xref>). Similar to the animals, the resulting difference in average percent performance for ‘morphed’ stimuli was <italic>Δ</italic> = 26 for the first 50 trials (<italic>Δ</italic> = 16 if we considered the last trials of Rule 3; <italic>Δ</italic> = 19 and <italic>Δ</italic> = 9.9 for early and late ‘prototype’ stimuli, respectively). The model was able to match the animals’ performance because the weights for Axis 2 did not change from one Rule 2 block to another (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2e, k</xref>), and the estimated perceptual noise of color was low in order to account for the high performance of both morphed and prototype stimuli (<xref ref-type="fig" rid="fig4">Figure 4e</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1e</xref>). To account for the slow re-learning observed for Rules 1 and 3, the best-fitting learning rate for feature–response associations was relatively low (<xref ref-type="fig" rid="fig4">Figure 4d, f</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1d, f</xref>, <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2d, f, j, l</xref>, <xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Models parameters.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">Monkey S</th><th align="left" valign="bottom" colspan="2">Noise perception</th><th align="left" valign="bottom">Learning rate</th><th align="left" valign="bottom">Initial belief R1</th><th align="left" valign="bottom">Initial belief R3</th><th align="left" valign="bottom">Initial belief Axis 1</th><th align="left" valign="bottom">Weight decay</th><th align="left" valign="bottom">Initial weights</th></tr><tr><th align="left" valign="bottom"><italic>κ</italic> (color)</th><th align="left" valign="bottom"><italic>κ</italic> (shape)</th><th align="left" valign="bottom"><italic>α</italic></th><th align="left" valign="bottom"><italic>b</italic><sub>1</sub></th><th align="left" valign="bottom"><italic>b</italic><sub>3</sub></th><th align="left" valign="bottom">bax</th><th align="left" valign="bottom"><italic>η</italic></th><th align="left" valign="bottom">w<sub>0</sub></th></tr></thead><tbody><tr><td align="center" valign="middle">QL model</td><td align="center" valign="middle">Mean = 2.2<break/>std = 0.44</td><td align="center" valign="middle">Mean = 1.3<break/>std = 0.27</td><td align="center" valign="middle">Mean = 0.23<break/>std = 0.039</td><td align="center" valign="middle"/><td align="center" valign="middle"/><td align="center" valign="middle"/><td align="center" valign="middle"/><td align="center" valign="middle"/></tr><tr><td align="center" valign="middle">IO model</td><td align="center" valign="middle">Mean = 2.4<break/>std = 0.46</td><td align="center" valign="middle">Mean = 1.3<break/>std = 0.31</td><td align="center" valign="middle"/><td align="center" valign="middle">Mean = 0.091<break/>std = 0.089</td><td align="center" valign="middle">Mean = 0.14<break/>std = 0.10</td><td align="center" valign="middle"/><td align="center" valign="middle"/><td align="center" valign="middle"/></tr><tr><td align="center" valign="middle">HQL model</td><td align="center" valign="middle">Mean = 11<break/>std = 1.2</td><td align="center" valign="middle">Mean = 5.1<break/>std = 2.2</td><td align="center" valign="middle">Mean = 0.23<break/>std = 0.10</td><td align="center" valign="middle"/><td align="center" valign="middle"/><td align="center" valign="middle">Mean = 0.29<break/>std = 0.076</td><td align="center" valign="middle">Mean = 0.046<break/>std = 0.022</td><td align="center" valign="middle">Mean = [−0.61,0.79,0.77,−0.64,<break/>−0.83,0.035,0.74,0.040]<break/>std = [0.23,0.089,0.13,0.17,<break/>0.053,0.59,0.19,0.57]</td></tr><tr><th align="left" valign="bottom" rowspan="2">Monkey C</th><th align="left" valign="bottom" colspan="2">Noise perception</th><th align="left" valign="bottom">Learning rate</th><th align="left" valign="bottom">Initial belief R1</th><th align="left" valign="bottom">Initial belief R3</th><th align="left" valign="bottom">Initial belief Axis 1</th><th align="left" valign="bottom">Weight decay</th><th align="left" valign="bottom">Initial weights</th></tr><tr><th align="left" valign="bottom"><italic>κ</italic> (color)</th><th align="left" valign="bottom"><italic>κ</italic> (shape)</th><th align="left" valign="bottom"><italic>α</italic></th><th align="left" valign="bottom"><italic>b</italic><sub>1</sub></th><th align="left" valign="bottom"><italic>b</italic><sub>3</sub></th><th align="left" valign="bottom">bax</th><th align="left" valign="bottom"><italic>η</italic></th><th align="left" valign="bottom"><italic>w</italic><sub>0</sub></th></tr><tr><td align="center" valign="middle">QL model</td><td align="center" valign="middle">Mean = 1.2<break/>std = 0.13</td><td align="center" valign="middle">Mean = 0.71<break/>std = 0.090</td><td align="center" valign="middle">Mean = 0.18<break/>std = 0.010</td><td align="center" valign="middle"/><td align="center" valign="middle"/><td align="center" valign="middle"/><td align="center" valign="middle"/><td align="center" valign="middle"/></tr><tr><td align="center" valign="middle">IO model</td><td align="center" valign="middle">Mean = 1.3<break/>std = 0.21</td><td align="center" valign="middle">Mean = 0.71<break/>std = 0.18</td><td align="center" valign="middle"/><td align="center" valign="middle">Mean = 0.35<break/>std = 0.16</td><td align="center" valign="middle">Mean = 0.32<break/>std = 0.16</td><td align="center" valign="middle"/><td align="center" valign="middle"/><td align="center" valign="middle"/></tr><tr><td align="center" valign="middle">HQL model</td><td align="center" valign="middle">Mean = 12<break/>std = 2.4</td><td align="center" valign="middle">Mean = 7.0<break/>std = 3.4</td><td align="center" valign="middle">Mean = 0.12<break/>std = 0.10</td><td align="center" valign="middle"/><td align="center" valign="middle"/><td align="center" valign="middle">Fixed to 0.5</td><td align="center" valign="middle">Mean = 0.067<break/>std = 0.060</td><td align="center" valign="middle">Mean = [−0.45,0.56,0.60,−0.56,<break/>−0.83,−0.12,0.70,−0.19]<break/>std = [0.16,0.12,0.093,0.16,<break/>0.051,0.45,0.25,0.41]</td></tr></tbody></table></table-wrap></sec><sec id="s2-5"><title>The effect of stimulus congruency (and incongruency) provides further evidence for the hybrid model</title><p>To further understand how the HQL model outperforms the QL and IO models, we examined the animal’s behavioral performance as a function of the relevant and irrelevant stimulus features. The orthogonal nature of the features and rules meant that stimuli could fall into two general groups: congruent stimuli had features that required the same response for both Rules 1 and 3 (e.g., a green bunny, <xref ref-type="fig" rid="fig1">Figure 1</xref>) while incongruent stimuli had features that required opposite responses between the two rules (e.g., a red bunny). Consistent with previous work (<xref ref-type="bibr" rid="bib62">Noppeney et al., 2010</xref>; <xref ref-type="bibr" rid="bib87">Venkatraman et al., 2009</xref>; <xref ref-type="bibr" rid="bib17">Bugg et al., 2008</xref>; <xref ref-type="bibr" rid="bib21">Carter et al., 1995</xref>; <xref ref-type="bibr" rid="bib59">Musslick and Cohen, 2021</xref>), the animals performed better on congruent stimuli than incongruent stimuli (<xref ref-type="fig" rid="fig5">Figure 5a</xref> for Monkey S, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a</xref> for Monkey C). This effect was strongest during learning, but persisted throughout the block (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2a, e</xref>): during early trials of Rules 1 and 3, the monkeys’ performance was significantly higher for congruent stimuli than for incongruent stimuli (gray vs. red squares in <xref ref-type="fig" rid="fig5">Figure 5b</xref>; 94%, CI = [0.93,0.95] vs. 57%, CI = [0.55,0.58], respectively; <italic>Δ</italic> = 37, Fisher’s test p &lt; 10<sup>−4</sup>; see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1b</xref> for Monkey C). Similarly, the animals were slower to respond to incongruent stimuli (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>, <italic>Δ</italic> = 25 ms in reaction time for incongruent and congruent stimuli, <italic>t</italic>-test, p &lt; 10<sup>−4</sup>). In contrast, the congruency of stimuli had no effect during Rule 2 – behavior depended only on the stimulus color, suggesting the monkeys ignored the shape of the stimulus during Rule 2, even when the morph level of the color was more difficult (gray vs. red squares in <xref ref-type="fig" rid="fig5">Figure 5c</xref>; performance was 92%, CI = [0.90,0.93], and 93%, CI = [0.92,0.93] for congruent and incongruent stimuli, respectively; with <italic>Δ</italic> = −0.73; Fisher’s test, p = 0.40; see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1c</xref> for Monkey C).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Comparison of incongruency effects in Monkey S and behavioral models (QL, IO, and HQL models).</title><p>(<bold>a</bold>) Performance as a function of trial number for Rules 1 and 3 (combined), for congruent and incongruent trials. (<bold>b</bold>) Performance for Rules 1 and 3 (combined, first 50 trials), as a function of the morph level for both color (relevant) and shape (irrelevant) features. Gray boxes highlight congruent stimuli, red boxes highlight incongruent stimuli. (<bold>c</bold>) Performance for Rule 2, as a function of the morph level for both color (relevant) and shape (irrelevant) features. Note the lack of an incongruency effect. (<bold>d–f</bold>) Same as a–c but for the QL model. (<bold>g–i</bold>) Same as a–c for the IO model. (<bold>j–l</bold>) Same as a–c but for the HQL model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Comparison of incongruency effects in Monkey C and behavioral models (QL, IO, and HQL models).</title><p>(<bold>a</bold>) Performance as a function of trial number for Rules 1 and 3 (combined), for congruent and incongruent trials. (<bold>b</bold>) Performance for Rules 1 and 3 (combined, first 50 trials), as a function of the morph level for both color (relevant) and shape (irrelevant) features. Gray boxes highlight congruent stimuli, red boxes highlight incongruent stimuli. (<bold>c</bold>) Performance for Rule 2, as a function of the morph level for both color (relevant) and shape (irrelevant) features. Note the lack of an incongruency effect. (<bold>d–f</bold>) Same as a–c but for the QL model. (<bold>g–i</bold>) Same as a–c for the IO model. (<bold>j–l</bold>) Same as a–c but for the HQL model. <italic>Statistics on Monkey C</italic>: During early trials of Rules 1 and 3, the monkeys’ performance was significantly higher for congruent trials than for incongruent trials (gray vs. red squares; 93%, confidence interval, CI = [0.91,0.95] vs. 49%, CI = [0.47,0.51], respectively; with <italic>Δ</italic> = 44; Fisher’s test p &lt; 10<sup>−4</sup>). There was no difference in performance between congruent and incongruent stimuli during Rule 2 (gray vs. red squares; performance was 94%, CI = [0.91,0.95], and 91%, CI = [0.89,0.92], respectively; with <italic>Δ</italic> = 2.7; Fisher’s test p = 0.07). Statistics of QL model fitted on Monkey C: The model performed worse on congruent than incongruent trials in Rules 1 and 3 (41% and 52%, respectively; <italic>Δ</italic> = −10; Fisher’s test p &lt; 10<sup>−4</sup>), against our behavioral observations. Furthermore, the model produced a difference in performance during Rule 2 (48% for congruent vs. 54% for incongruent; <italic>Δ</italic> = −6.1; Fisher’s test p &lt; 10<sup>−4</sup>). Statistics of IO model fitted on Monkey C: Learning quickly reached a low asymptotic performance in Rules 1 and 3, for both congruent and incongruent trials (69% and 67% respectively; <italic>Δ</italic> = 2.5 only). Statistics of HQL model fitted on Monkey C: The model reproduced the greater performance on congruent than incongruent stimuli in Rules 1 and 3 (94% and 53%, respectively; <italic>Δ</italic> = 41). It also captured the absence of incongruency effect in Rule 2 (green vs. red squares; 91% and 91%, respectively; <italic>Δ</italic> = 0.081).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Incongruency effect.</title><p>For Monkey S (<bold>a</bold>), Monkey C (<bold>e</bold>), and models (respectively, fitted on Monkey S: <bold>b–d</bold>; and on Monkey C: <bold>f–h</bold>), for trials 50–200. Each plot represents the performance for Rules 1 and 3 (combined), as a function of morphs for both relevant and irrelevant features. Gray corners for congruent stimuli, red corners for incongruent stimuli.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Reaction times.</title><p>For Rule 2 blocks (<bold>a,d</bold>), the first 50 trials of Rule 1/3 blocks (<bold>b,e</bold>), and the trials 50–200 of Rule 1/3 blocks (<bold>c,f</bold>) as a function of the relevant and irrelevant features of the morphed stimulus presented. Top row: Monkey S, bottom row: Monkey C. Statistics on Monkey C: <italic>Δ</italic> (ms) = 14 between incongruent and congruent, <italic>t</italic>-test p &lt; 10<sup>−4</sup>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig5-figsupp3-v2.tif"/></fig><fig id="fig5s4" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 4.</label><caption><title>Choice probabilities for the three models fitted on Monkey S.</title><p>(<bold>a,d,g</bold>) Rule 1; (<bold>b,e,h</bold>) Rule 2; (<bold>c,f,i</bold>) Rule 3.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig5-figsupp4-v2.tif"/></fig><fig id="fig5s5" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 5.</label><caption><title>Behavior across days.</title><p>(<bold>a</bold>) Averaged performance over the first 200 trials for Rules 1 and 3, and the first 50 trials for Rule 2, for Monkey S. (<bold>b</bold>) Noise perception parameter, for color and shape, from the HQL model fit, for Monkey S. (<bold>c</bold>) Learning rate from the HQL model fit, for Monkey S. (<bold>d–f</bold>) Same as (<bold>a–c</bold>) but for Monkey C. Overall, we found no significant trends in the behavior and model parameters across days, with the exception of the learning rate in Monkey C which was moderately significant (not surviving multiple comparison correction).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82531-fig5-figsupp5-v2.tif"/></fig></fig-group><p>This incongruency effect provided further evidence for the HQL model. First, pure incremental learning by the QL model did not capture this result, but instead predicted an opposite effect. This is because incongruent trials were four times more likely than congruent trials (see Methods). As the QL model encodes the statistics of the task through error-driven updating of action values, the proportion of congruent versus incongruent trials led to an anti-incongruency effect – the QL model fit to Monkey S predicted worse performance on congruent than incongruent trials (<xref ref-type="fig" rid="fig5">Figure 5d, e</xref>; 45% and 62%, respectively; <italic>Δ</italic> = −16; Fisher’s test p &lt; 10<sup>−4</sup>; see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1d, e</xref> for Monkey C). Furthermore, for the same reason, the QL model produced a difference in performance during Rule 2 (<xref ref-type="fig" rid="fig5">Figure 5f</xref>; 54% for congruent vs. 64% for incongruent; <italic>Δ</italic> = −10; Fisher’s test p &lt; 10<sup>−4</sup>; see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1f</xref> for Monkey C, see also <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2b, f</xref> for this effect throughout the block).</p><p>Second, the IO model also did not capture the incongruency effect. In principle, incongruency effects can be seen in this type of model when perceptual noise is large, because incongruent stimuli are more ambiguous when the correct rule is not yet known. However, given the statistics of the task, learning in the IO model quickly reached asymptotic performance, for both congruent and incongruent trials (<xref ref-type="fig" rid="fig5">Figure 5g, h</xref>; 75% and 72%, respectively; <italic>Δ</italic> = 3.8 only; <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1g, h</xref> for Monkey C), hence not reproducing the incongruency effect.</p><p>In contrast to the QL and IO models, the hybrid HQL model captured the incongruency effect. In the HQL model, the weights for mapping congruent stimuli to responses were the same for Rules 1 and 3. In contrast, the weights for incongruent stimuli must change for Rules 1 and 3. Therefore, the animals’ performance was immediately high on congruent stimuli, while the associations for incongruent stimuli had to be relearned on each block (<xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref>). The model fitted to Monkey S behavior reproduced the greater performance on congruent than incongruent stimuli (<xref ref-type="fig" rid="fig5">Figure 5g, h</xref>; 92% and 61%, respectively, <italic>Δ</italic> = 31; see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1g, h</xref> for Monkey C). As with the monkey’s behavior, this effect persisted throughout the block (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2d, h</xref>). Finally, the HQL model captured the absence of incongruency effect in Rule 2 (<xref ref-type="fig" rid="fig5">Figure 5i</xref>, green vs. red squares; 92% and 93%, respectively; <italic>Δ</italic> = −1.0; see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1i</xref> for Monkey C), as there was no competing rule, there was no need to update the Axes 2 weights between blocks. As a result, only a hybrid model performing both rule switching of axis and rule learning of features could account for the incongruency effect observed in the behavior.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In the present study, we investigated rule learning in two monkeys trained to switch between three category-response rules. Critically, the animals were not instructed as to which rule was in effect (only that the rule had changed). We compared two classes of models that were able to perform the task: incremental learning and inferential rule switching. Our results suggested that neither model fit the animals’ performance well. Incremental learning was too slow to capture the monkeys’ rapid learning of the response axis after a block switch. It was also unable to explain the high behavioral performance on Rule 2, which was the only rule requiring responses along the second axis. On the other hand, inferential learning was unable to reproduce the difference in performance for the two rules that required attending to the same feature of the stimulus (color), but responding on different axes (Rules 2 and 3). Finally, when considered separately, neither of these two classes of models could explain the monkeys’ difficulty in responding to stimuli that had incongruent responses on the same axes (Rules 1 and 3). Instead, we found that a hybrid model that inferred axes quickly and relearned features slowly was able to capture the monkeys’ behavior. This suggests the animals were learning the current axis of response using fast inference while continuously re-estimating the stimulus–response mappings within an axis.</p><p>We assessed the generative performance of the hybrid model and falsified (<xref ref-type="bibr" rid="bib66">Palminteri et al., 2017</xref>) incremental learning and inferential rule switching considered separately. The superior explanatory power of the hybrid model suggests that the animals performed both rule switching and rule learning – even in a well-trained regime in which they could, in principle, have discovered perfect rule knowledge. The model suggests that the monkeys discovered only two latent states (corresponding to the two axes of response) instead of the three rules we designed, forcing them to perpetually relearn Rules 1 and 3. These two latent states effectively encode Rule 2 (alone on its response axis) on the one hand, and a combination of Rules 1 and 3 (sharing a response axis) on the other hand. The combination of rules in the second latent state caused the monkeys to continuously update their knowledge of the rules’ contingencies (mapping different stimulus features to actions). At first glance, one might be concerned that our major empirical finding about the discrepancy between fast switching and the slow updating of rules was inherent to the task structure. Indeed, a block switch predictably corresponds to a switch of axis, but does not always switch the relevant stimulus feature. Moreover, the features were themselves morphed, creating ambiguity when trying to categorize them and use past rewards for feature discovery. We can thus expect inherently the slower learning of the relevant feature. However, our experiments with IO models demonstrate that an inference process reflecting the different noise properties of axes and features cannot by itself explain the two timescales of learning. The key insight is that, if slow learning about features is simply driven by their inherent noisiness, then the asymptotic performance level with these features should reflect the same degree of ambiguity. However, these do not match. This indicates that the observed fast and slow learning was not a mere representation of the generative model of the task. One caveat about this interpretation is that it assumes that other factors governing asymptotic performance (e.g., motivation or attention, which we do not explicitly control) are comparable between Rule 2 versus Rules 1 and 3 blocks.</p><p>The particular noise properties of the task may, however, shed light on a related question raised by our account: why the animals failed to discover the correct three-rule structure, which would clearly support better performance in Rule 1 and 3 blocks. We believe their failure to do so was not merely a function of insufficient training but would have remained stable even with more practice, as we trained the monkeys until their behavior was stable (and we verified the lack of significant trends in the behavior across days, <xref ref-type="fig" rid="fig5s5">Figure 5—figure supplement 5</xref>). Their failure to do so could shed light on the brain’s mechanisms for representing task properties and for discovering, splitting, or differentiating different latent states on the basis of their differing stimulus–action–response contingencies. One possible explanation is that the overlap between Rules 1 and 3 (sharing an axis of response) makes them harder to differentiate than Rule 2. In particular, the axis is the most discriminatory feature (being discrete and also under the monkey’s own explicit control), whereas the stimulus–reward mappings are noisier and continuous. Alternatively, a two-latent state regime may be rational given the cognitive demands of this task, including the cost of control or constraints on working memory when routinely switching between latent states (stability–flexibility trade-off) (<xref ref-type="bibr" rid="bib59">Musslick and Cohen, 2021</xref>; <xref ref-type="bibr" rid="bib61">Nassar and Troiani, 2021</xref>; <xref ref-type="bibr" rid="bib58">Musslick et al., 2018</xref>).</p><p>Thus, all this suggests that the particulars of the behavior may well depend on the details of the task or training protocol. For instance, the monkeys may have encoded each rule as a separate latent state (and shown only fast learning asymptotically) with a different task (e.g., including the missing fourth rule so as to counterbalance axes with stimuli) or training protocol (e.g., longer training, random presentation of the three rules with equal probability of occurrence instead of Rule 2 being interleaved and occurring twice more often, a higher ratio of incongruent versus congruent trials, or less morphed and more prototyped stimuli). Another interesting possibility, which requires future experiments to rule out, is that the disadvantage for Rules 1 and 3 arises not because their sharing response axis necessitates continual relearning, but instead because this sharing per se causes some sort of interference. This alternative view suggests that the relative disadvantage would persist even in a modified design in which the rule was explicitly signaled at each trial, making inference unnecessary.</p><p>Understanding how the brain discovers and manipulates latent states would give insight into how the brain avoids catastrophic interference. In artificial neural networks, sequentially learning tasks causes catastrophic interference, such that the new task interferes (overwrites) the representation of the previously learned task (<xref ref-type="bibr" rid="bib55">McCloskey and Cohen, 1989</xref>). In our task, animals partially avoided catastrophic interference by creating two latent states where learning was independent. For example, learning stimulus–response mappings for Rules 1 and 3 did not interfere with the representation of Rule 2. In contrast, Rules 1 and 3 did interfere – behavior was re-learned on each block. Several solutions to this problem have been proposed in machine learning literature such as orthogonal subspaces (<xref ref-type="bibr" rid="bib35">Duncker et al., 2020</xref>) and generative replay (<xref ref-type="bibr" rid="bib86">van de Ven and Tolias, 2019</xref>). Similarly, recent advances in deep reinforcement learning have started to elucidate the importance of incorporating metalearning in order to speed up learning and to avoid catastrophic interference (<xref ref-type="bibr" rid="bib14">Botvinick et al., 2019</xref>; <xref ref-type="bibr" rid="bib46">Hadsell et al., 2020</xref>). Our results suggest the brain might solve this problem by creating separate latent states where learning is possible within each latent state. How these latent states are instantiated in the brain is an open question, and discovering those computations promises exciting new insights for algorithms of learning.</p><p>Finally, our characterization of the computational contributions of rule switching and rule learning, and the fortuitous ability to observe both interacting in a single task, leads to a number of testable predictions about their neural interactions. First, our results make the strong prediction that there should be two latent states represented in the brain – the representation for the two rules competing on one axis (Rules 1 and 3) should be more similar to one another than to the neural representation of the rule alone on the other axis (Rule 2). This would not be the case if the neural activity was instead representing three latent causes. Furthermore, our hybrid model suggests there may be a functional dissociation for rule switching and rule learning, such that they are represented in distinct networks. One hypothesis is that this dissociation is between cortical and subcortical regions. Prefrontal cortex may carry information about the animal’s trial beliefs (i.e., over the two latent states) in a similar manner as perceptual decision making when accumulating evidence from noisy stimuli (<xref ref-type="bibr" rid="bib44">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib82">Shadlen and Kiani, 2013</xref>; <xref ref-type="bibr" rid="bib71">Rao, 2010</xref>; <xref ref-type="bibr" rid="bib9">Beck et al., 2008</xref>). Basal ganglia may, in turn, be engaged in the learning of rule-specific associations (<xref ref-type="bibr" rid="bib27">Daw and O’Doherty, 2014</xref>; <xref ref-type="bibr" rid="bib26">Daw and Shohamy, 2008</xref>; <xref ref-type="bibr" rid="bib28">Daw and Tobler, 2014</xref>; <xref ref-type="bibr" rid="bib32">Dolan and Dayan, 2013</xref>; <xref ref-type="bibr" rid="bib34">Doya, 2007</xref>; <xref ref-type="bibr" rid="bib63">O’Doherty et al., 2004</xref>; <xref ref-type="bibr" rid="bib64">O’Reilly and Frank, 2006</xref>; <xref ref-type="bibr" rid="bib73">Rescorla, 1988</xref>; <xref ref-type="bibr" rid="bib80">Schultz et al., 1997</xref>; <xref ref-type="bibr" rid="bib89">Yin and Knowlton, 2006</xref>). Alternatively, despite their functional dissociation, future work may find both rule switching and rule learning are represented in the same brain regions (e.g., prefrontal cortex).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experimental design and model notation</title><p>Two adult (8- to 11-year-old) male rhesus macaques (<italic>Macaca mulatta</italic>) participated in a category-response task. Monkeys S and C weighed 12.7 and 10.7 kg, respectively. All experimental procedures were approved by Princeton University Institutional Animal Care and Use Committee (protocol #3055) and were in accordance with the policies and procedures of the National Institutes of Health.</p><p>Stimuli were rendering of three dimensional models that were built using POV-Ray and MATLAB (Mathworks). They were presented on a Dell U2413 LCD monitor positioned at a viewing distance of 58 cm. Each stimulus was generated with a morph-level drawn from a circular continuum between two prototype colors <italic>C</italic> (red and green) and two prototype shapes <italic>S</italic> (‘bunny’ and ‘tee’; <xref ref-type="fig" rid="fig1">Figure 1b</xref>).<disp-formula id="equ1"><mml:math id="m1"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></disp-formula></p><p>where <italic>X</italic> is the parameter value in a feature dimension for example <italic>L</italic>, <italic>a</italic>, <italic>b</italic> values in CIELAB color space. Radius (<italic>P</italic>) was chosen such that there was enough visual discriminability between morph levels. Morph levels in shape dimension were built by circular interpolation of the parameters defining the lobes of the first prototype with the parameters defining the corresponding lobes of the second prototype. Morph levels in the color dimension were built by selecting points along photometrically isoluminant circle in CIELAB color space that connected red and green prototype colors. We used percentage to quantify the deviation of each morph level from prototypes (0% and 100%) on the circular space. Morph levels between 0% and 100% correspond to −<italic>π</italic> to 0, and morph levels 100% and 200% correspond to 0 to <italic>π</italic> on the circular space. Morph levels for color and shape dimension were generated at eight levels: 0%, 30%, 50%, 70%, 100%, 130%, 150%, and 170%. 50% morph levels for one feature (color or shape) were only generated for prototypes for the other feature (shape or color, respectively). The total stimulus set consisted of 48 images. By creating a continuum of morphed stimuli, we could independently manipulate stimulus difficulty along each dimension.</p><p>Monkeys were trained to perform three different rules <italic>R</italic> = {<italic>R</italic><sub>1</sub>, <italic>R</italic><sub>2</sub>, <italic>R</italic><sub>3</sub>} (<xref ref-type="fig" rid="fig1">Figure 1c, d</xref>). All of the rules had the same general structure: the monkeys categorized a visual stimulus according to its shape or color, and then responded with a saccade, <inline-formula><mml:math id="inf1"><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula>, to one of four locations <italic>Actions =</italic> {<italic>Upper-Left,Upper-Right,Lower-Left,Lower-Right</italic>} (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Each rule required the monkeys to attend-to and categorize either the color or shape feature of the stimulus, and then respond with a saccade along an axis (<italic>A =</italic> {<italic>Axis 1,Axis 2</italic>}). <italic>Axis 1</italic> corresponded to Upper-Left, Lower-Right locations and <italic>Axis 2</italic> corresponded to Upper-Right, Lower-Left locations. As such, the correct response depended on the rule in effect and stimulus presented during the trial. Rule 1 (<italic>R</italic><sub>1</sub>) required a response on <italic>Axis 1</italic>, to the Upper-Left or Lower-Right locations when the stimulus was categorized as ‘bunny’ or ‘tee’, respectively. Rule 2 (<italic>R</italic><sub>2</sub>) required a response on <italic>Axis 2</italic>, to the Upper-Right or Lower-Left locations when the stimulus was categorized as red or green, respectively. Rule 3 (<italic>R</italic><sub>3</sub>) required a response on <italic>Axis 1</italic>, to the Upper-Left or Lower-Right locations when the stimulus was categorized as green or red, respectively. In this way, the three rules were compositional: Rules 2 and 3 shared the response to the same feature of the stimulus (color) but different axes. Similarly, Rules 1 and 3 shared the response to same axis (<italic>Axis 1</italic>), but to different features.</p><p>The monkeys initiated each trials by fixating a dot on the center of the screen. During a fixation period (lasting 500–700 ms), the monkeys were required to maintain their gaze within a circle with radius of 3.25 degrees of visual angle around the fixation dot. After the fixation period, the stimulus and all four response locations were displayed simultaneously. The monkeys made their response by breaking fixation and saccading to one of the four response locations. Each response location was 6 degrees of visual angle from the fixation point, located at 45°, 135°, 225°, and 315° degrees relative to vertical. The stimulus diameter was 2.5 degrees of visual angle. The animal’s reaction time was taken as the moment of leaving the fixation window, relative to the onset of the stimulus. Trials with a reaction time lower than 150 ms were aborted, and the monkey received a brief timeout. Following a correct response, monkeys were provided with a small reward, while incorrect responses led to a brief timeout (<inline-formula><mml:math id="inf2"><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>0,1</mml:mn><mml:mo>}</mml:mo></mml:math></inline-formula>). Following all trials, there was an inter-trial interval of 2–2.5 s before the next trial began. The time distributions were adjusted according to task demands and previous literature (<xref ref-type="bibr" rid="bib18">Buschman et al., 2012</xref>).</p><p>Note that both Rules 1 and 3 required a response along the same axis (<italic>Axis 1</italic>). Half of the stimuli were ‘congruent’, such that they led to the same response for both rules (e.g., a green bunny is associated with an Upper-Left response for both rules). The other half of stimuli were ‘incongruent’, such that they led to different responses for both rules (e.g., a red bunny is associated with a Upper-Left and Lower-Right response, for Rules 1 and 3, respectively). To ensure the animals were performing the rule, incongruent stimuli were presented on 80% of the trials.</p><p>Animals followed a single rule for a ‘block’ of trials. After the animal’s behavioral performance on that rule reached a threshold, the task would switch to a new block with a different rule. The switch between blocks was triggered when the monkeys’ performance was greater than or equal to 70% on the last 102 trials of Rules 1 and 3 or the last 51 trials of Rule 2. For Monkey S, each performance, for ‘morphed’ and ‘prototype’ stimuli independently, had to be above threshold. Monkey C’s performance was weaker, and a block switch occurred when the average performance for all stimuli was above threshold. Also, to avoid that Monkey C perseverated on one rule for an extended period on a subset of days, the threshold was reduced to 65% over the last 75 trials for Rules 1 and 3, after 200 or 300 trials. Switches between blocks of trials were cued by a flashing screen, a few drops of juice, and a long time out (50 s). Importantly, the rule switch cue did not indicate the identity of the rule in effect or the upcoming rule. Therefore, the animal still had to infer the current rule based on its history.</p><p>Given the limited number of trials performed each day and to simplify the task structure for the monkeys, the axis of response always changed following a block switch. During <italic>Axis 1</italic> blocks, whether Rule 1 or 3 was in effect was pseudo-randomly selected. These blocks were interleaved by <italic>Axis 2</italic> blocks, which were always Rule 2. Pseudo-random selection of Rules 1 and 3 within <italic>Axis 1</italic> blocks was done to ensure the animal performed at least one block of each rule during each session (accomplished by never allowing for three consecutive blocks of the same rule).</p><p>As expected, given their behavioral performance, the average block length varied across monkeys: 50–300 trials for Monkey S, and 50–435 trials for Monkey C. Rule 1 and 3 blocks were on average 199 trials for Monkey S and 222 trials for Monkey C. Rule 2 blocks were shorter because they were performed more frequently and were easier given the task structure. They were on average 56 trials for Monkey S and 52 trials for Monkey C. Overall, the behavioral data include 20 days of behavior from Monkey S, with an average of 14 blocks per day, and 15 days for Monkey C, with an average of 6.5 blocks per day.</p></sec><sec id="s4-2"><title>Statistical details on the study design</title><p>The sample size is two animals, determined by what is typical in the field. All animals were assigned to the same group, with no blinding necessary.</p></sec><sec id="s4-3"><title>Additional details on training</title><p>Given the complex structure of the task, monkeys were trained for months until they fully learned the structure of the task and they could consistently perform at least five blocks each day. Monkeys learned the structure of the task in multiple steps. They were first trained to hold fixation and to associate stimuli with reward by making saccades to target locations. To begin with shape categorization (Rule 1), monkeys learned to associate monochrome versions of prototype stimuli with two response locations on <italic>Axis 1</italic>. Stimuli were then gradually colored by using an adaptive staircase procedure. To begin training on color categorization (Rule 2), monkeys learned to associate red and green squares with two response locations on <italic>Axis 2</italic>. Prototype stimuli gradually appeared on the square and finally replaced the square using an adaptive staircase procedure. After this stage, monkeys were trained to generalize across morph levels in 5% morph-level steps using an adaptive staircase method until they could generalize up to 20% morph level away from the prototypes, for color and shape features. Rule 3 was added at this stage with a cue (purple screen background). Once the monkey was able to switch between Rules 1 and 3, the cue was gradually faded and finally removed. After monkeys learned to switch between three rules, the morph levels 30% and 40% were introduced. Monkeys S and C were trained for 36 and 60 months, respectively. Behavioral data reported here are part of data acquisition during electrophysiological recording sessions. From this point, only behavioral sessions in which monkeys performed at least five blocks were included for further analysis. In order to encourage generalization for shape and color features, during non-recording days, monkeys were trained on the larger number of morph levels (0%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 100%, 120%, 130%, 140%, 150%, 160%, 170%, and 180%).</p></sec><sec id="s4-4"><title>Modeling noisy perception of color and shape</title><p>All the models studied below model stimulus perception in the same way (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), via a signal-detection-theory like account by which objective stimulus features are modeled as magnitudes corrupted by continuously distributed perceptual noise. In particular, the color and shape of each stimulus presented to the animals are either the prototype features <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula>, or a morphed version of them. The feature continuous spaces are projected onto the unit circle from −<italic>π</italic> to <italic>π</italic>, such that each stimulus feature has a unique radius angle, with prototype angles being 0 and <italic>π</italic>. The presented stimulus is denoted <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>. We hypothesize that the monkeys perceive a noisy version of it, denoted <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>. We model it by drawing two independent samples, one from each of two Von Mises distributions, centered around each feature (<inline-formula><mml:math id="inf7"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , for color and shape), and parameterized by the concentrations <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, respectively. The models estimate each initial feature presented by computing its posterior distribution, given the perceived stimulus, that is by Von Mises distributions centered on <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , with same concentrations <inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> .<disp-formula id="equ2"><mml:math id="m2"><mml:mo>∀</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>}</mml:mo></mml:math></disp-formula><disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>M</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and so<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>M</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mi>V</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>M</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-5"><title>Modeling action selection</title><p>All the models studied below use the same action-selection stage. Given the perceived stimulus at each trial <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>, an action is chosen so as to maximize the expected reward <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> by computing <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> which corresponds to maximizing the probability of getting a reward, given the perceived stimulus. We use the notation <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> as in previous work (<xref ref-type="bibr" rid="bib30">Dayan and Daw, 2008</xref>; <xref ref-type="bibr" rid="bib25">Daw et al., 2006</xref>) and refer to these values as <italic>Q</italic> values. Note that even a deterministic ‘max’ choice rule at this stage does not, in practice, imply noiseless choices, since <italic>Q</italic> depends on <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , and the perceptual noise in this quantity gives rise to variability in the maximizing action that is graded in action value, analogous to a softmax rule. For that reason, we do not include a separate choice-stage softmax noise parameter (which would be unidentifiable relative to perceptual noise <italic>κ</italic>), though we do nevertheless approximate the max choice rule with a softmax (but using a fixed temperature parameter), for implementational purposes (specifically, to make the choice model differentiable). To control the asymptotic error rate, we also include an additional probability of lapse (equivalently, adding ‘epsilon-greedy’ choice). Altogether, two fixed parameters implement an epsilon-greedy softmax action-selection rule: the lapse rate <italic>ε</italic> and the inverse temperature <italic>β</italic>.</p><p>The action-selection rule is:<disp-formula id="equ6"><mml:math id="m6"><mml:mo>∀</mml:mo><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:math></disp-formula><disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mi>ϵ</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The lapse rate <italic>ε</italic> is directly estimated from the data, by computing the proportion of trials where the incorrect axis of response is chosen, asymptotically. It is evaluated to 0.02 for both Monkeys S and C. The inverse temperature of the softmax <italic>β</italic> is also fixed (<italic>β</italic> = 10), to allow the algorithm to approximate the max while remaining differentiable (cf. use of Stan below).</p></sec><sec id="s4-6"><title>Fit with Stan</title><p>All our models shared common noisy perceptual input and action selection stages (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, and Methods). The models however differed in the intervening mechanism for dynamically mapping stimulus to action value (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Because of noise perception at each trial, and because the cumulative distribution function of a Von Mises is not analytic, the models are fitted with Monte Carlo Markov chains (MCMC) using Stan (<xref ref-type="bibr" rid="bib20">Carpenter, 2017</xref>). Each day of recording is fitted separately, and the mean and standard deviation reported in <xref ref-type="table" rid="table1">Table 1</xref> are between days. We validated convergence by monitoring the potential scale reduction factor R-hat (which was &lt;1.05 for all simulations) and an estimate of the effective sample size (all effective sample sizes &gt;100) of the models’ fits (<xref ref-type="bibr" rid="bib40">Gelman and Rubin, 1992</xref>).</p><p>Models’ plots correspond to an average of 1000 simulations of each day of the dataset (with the same order of stimuli presentation). Statistics reported in the article were done with Fisher’s exact test (except a <italic>t</italic>-test for reaction times, <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>).</p></sec><sec id="s4-7"><title>Incremental learner: QL model</title><p>In this model, the agent is relearning each rule after a block as a mapping between stimuli and actions, by computing a stimulus-action value function as a linear combination of binary feature–response functions <inline-formula><mml:math id="inf20"><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> with feature–response weights <bold>w</bold>. This implements incremental learning while allowing for some generalization across actions. The weights are updated by the delta rule (<italic>Q</italic> learning with linear function approximation; <xref ref-type="bibr" rid="bib84">Sutton and Barto, 2018</xref>). The weights are reset from one block to the other, and the initial values for each reset are set to zero. Fitting them does not change the results (see Parameter values’).</p></sec><sec id="s4-8"><title>Computation of the feature–response matrix</title><p>Given a morph perception <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:math></inline-formula> a feature–response matrix is defined as:<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf22"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mo>-</mml:mo><mml:mn>1,1</mml:mn><mml:mo>}</mml:mo></mml:math></inline-formula> depends on whether the perceived morph for color <inline-formula><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is classified as green or red, and <inline-formula><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mo>-</mml:mo><mml:mn>1,1</mml:mn><mml:mo>}</mml:mo></mml:math></inline-formula> whether the perceived morph for shape <inline-formula><mml:math id="inf25"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is classified as tee or bunny (see ‘Modeling noisy perception of color and shape’). For the algorithm to remain differentiable, we approximate {−1,1} with a sum of sigmoids. Each column of the matrix <inline-formula><mml:math id="inf26"><mml:mi>ϕ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> is written <inline-formula><mml:math id="inf27"><mml:mi>ϕ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> below and corresponds to an action <inline-formula><mml:math id="inf28"><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s4-9"><title>Linear computation of <italic>Q</italic> values and action selection</title><p>In order to compute <italic>Q</italic> values, the feature–response functions <inline-formula><mml:math id="inf29"><mml:mi>ϕ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> are weighted by the feature–response weight vector <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> :<disp-formula id="equ9"><mml:math id="m9"><mml:mo>∀</mml:mo><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:math></disp-formula><disp-formula id="equ10"><mml:math id="m10"><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>∙</mml:mo><mml:mi>ϕ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>Action selection is done through the epsilon-greedy softmax rule above.</p><p>Thus asymptotic learning of Rule 1 would require <inline-formula><mml:math id="inf31"><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0,1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>-</mml:mo><mml:mn>1,0</mml:mn><mml:mo>,</mml:mo><mml:mn>0,0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>. Learning Rule 2 would require <inline-formula><mml:math id="inf32"><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0,0</mml:mn><mml:mo>,</mml:mo><mml:mn>0,0</mml:mn><mml:mo>,</mml:mo><mml:mo>-</mml:mo><mml:mn>1,0</mml:mn><mml:mo>,</mml:mo><mml:mn>1,0</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>. Learning Rule 3 would require <inline-formula><mml:math id="inf33"><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>1,0</mml:mn><mml:mo>,</mml:mo><mml:mn>1,0</mml:mn><mml:mo>,</mml:mo><mml:mn>0,0</mml:mn><mml:mo>,</mml:mo><mml:mn>0,0</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>.</p></sec><sec id="s4-10"><title>Weight vector update</title><p>Once an action <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is chosen and a reward <inline-formula><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is received at trial <inline-formula><mml:math id="inf36"><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:math></inline-formula> the weights are updated by the delta rule (<xref ref-type="bibr" rid="bib84">Sutton and Barto, 2018</xref>) with learning rate <italic>α</italic>.<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-11"><title>Parameter values</title><p><italic>β</italic> and <italic>ε</italic> are fixed to, respectively, 10 and 0.02.</p><p>Parameters values are reported in <xref ref-type="table" rid="table1">Table 1</xref>. As predicted from the behavior, noise perception is higher for shape than for color (<inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). The initial weight vector <inline-formula><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is set to zero at the beginning of each block day. Fitting these weights instead gives the same results (as then <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> has a mean of [−0.070,0.031,0.093,−0.054,−0.081,−0.022,0.062,−0.0048] for Monkey S and <inline-formula><mml:math id="inf40"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> has a mean of [−0.099,0.070,0.069,−0.089,−0.053,−0.011,0.030,−0.0098] for Monkey C).</p></sec><sec id="s4-12"><title>Optimal Bayesian inference over rules: IO model</title><p>In this model, we assume a perfect knowledge of combination mappings between prototype stimuli and actions as <italic>rules</italic>. Learning is discovering which rule is in effect by Bayesian inference. This is done through learning, over the trials, the probability for each rule to be in effect in a block (or <italic>belief</italic>) from the history of stimuli, actions, and rewards. At each trial, this belief is linearly combined to the likelihood of a positive reward given the stimulus to compute a value for each action. This likelihood encapsulates knowledge of the three experimental rules. An action is chosen as per described above (see ‘Modeling action selection’). The beliefs over rules are then updated through Bayes rule using the likelihood of the reward received, given the chosen action and the stimulus perception. Once the rule is discovered, potential errors thus only depend on the possible miscategorization of the stimulus features (see ‘Modeling noisy perception of color and shape’), or eventually on exploration (see ‘Modeling action selection’).</p></sec><sec id="s4-13"><title>Belief over rules</title><p>The posterior probability of rule <inline-formula><mml:math id="inf41"><mml:mi>R</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">R</mml:mi></mml:math></inline-formula> to be in effect in the block is called the belief over the rule <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> given the perceived stimulus <inline-formula><mml:math id="inf43"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , the action <inline-formula><mml:math id="inf44"><mml:mi>a</mml:mi></mml:math></inline-formula> and the reward <inline-formula><mml:math id="inf45"><mml:mi>r</mml:mi></mml:math></inline-formula>. The beliefs <inline-formula><mml:math id="inf46"><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> at the beginning of each block are initialized to <inline-formula><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math></inline-formula> where <inline-formula><mml:math id="inf48"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf49"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are fitted, to test for a systematic initial bias toward one rule.</p></sec><sec id="s4-14"><title>Computation of values</title><p>The beliefs are used to compute the <italic>Q</italic> values for the trial:<disp-formula id="equ12"><mml:math id="m12"><mml:mo>∀</mml:mo><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:math></disp-formula><disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with (marginalization over the possible morph stimuli presented):<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:mi mathvariant="normal">∀</mml:mi><mml:mtext> </mml:mtext><mml:mi>R</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">M</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">M</mml:mi></mml:msub></mml:mrow><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">M</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">M</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">M</mml:mi></mml:msub></mml:mrow><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Noting <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>y</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> gives:<disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ18"><mml:math id="m18"><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ19"><mml:math id="m19"><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-15"><title>Belief update</title><p>From making an action <inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula>, the agent receives a reward <inline-formula><mml:math id="inf53"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>0,1</mml:mn><mml:mo>}</mml:mo></mml:math></inline-formula>, and the beliefs over rules are updated:<disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:mi mathvariant="normal">∀</mml:mi><mml:mtext> </mml:mtext><mml:mi>R</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ21"><mml:math id="m21"><mml:mrow><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> the likelihood of observing reward <inline-formula><mml:math id="inf55"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for the chosen action <inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p>Note that because of the symmetry of the task, <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">¬</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> .</p></sec><sec id="s4-16"><title>Parameter values</title><p><italic>β</italic> and <italic>ε</italic> are, respectively, fixed to 10 and 0.02.</p><p>Parameter values are reported in <xref ref-type="table" rid="table1">Table 1</xref>. As predicted from the behavior, there is an initial bias for Rule 2 for the model fitted on Monkey S behavior (<inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). Also, noise perception is higher for shape than for color for both monkeys (<inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). In the version of the model with low perceptual color noise (<xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), all the parameters remain the same, except that we fix <inline-formula><mml:math id="inf60"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:math></inline-formula> for all simulated days.</p></sec><sec id="s4-17"><title>Hybrid incremental learner: HQL model</title><p>The hybrid incremental learner combines inference over axes with incremental learning, using a <italic>Q</italic> learning with function approximation to relearn the likelihood of rewards given stimuli per axis of response.</p></sec><sec id="s4-18"><title>Belief over axes</title><p>The posterior probability of an axis <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to be the correct axis of response in a block is called the belief over axis <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, given the perceived stimulus <inline-formula><mml:math id="inf63"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , the action <inline-formula><mml:math id="inf64"><mml:mi>a</mml:mi></mml:math></inline-formula> and the reward <inline-formula><mml:math id="inf65"><mml:mi>r</mml:mi></mml:math></inline-formula>.</p><p>The beliefs over axes are initialized at the beginning of each block to <inline-formula><mml:math id="inf66"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></inline-formula></p></sec><sec id="s4-19"><title>Computation of the feature–response matrix</title><p>As for the incremental learner above, given a morph perception <inline-formula><mml:math id="inf67"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> at trial <inline-formula><mml:math id="inf68"><mml:mi>t</mml:mi></mml:math></inline-formula>, a feature–response matrix is defined as:<disp-formula id="equ22"><mml:math id="m22"><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf69"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mo>-</mml:mo><mml:mn>1,1</mml:mn><mml:mo>}</mml:mo></mml:math></inline-formula> depends on whether the perceived morph for color <inline-formula><mml:math id="inf70"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is classified as green or red, and <inline-formula><mml:math id="inf71"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mo>-</mml:mo><mml:mn>1,1</mml:mn><mml:mo>}</mml:mo></mml:math></inline-formula> whether the perceived morph for shape <inline-formula><mml:math id="inf72"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is classified as tee or bunny (see ‘Modeling noisy perception of color and shape’). Each column of the matrix <inline-formula><mml:math id="inf73"><mml:mi>ϕ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> is written <inline-formula><mml:math id="inf74"><mml:mi>ϕ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> below and corresponds to an action <inline-formula><mml:math id="inf75"><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s4-20"><title>Computation of values</title><p>The beliefs are used to compute the <italic>Q</italic> values for the trial:<disp-formula id="equ23"><mml:math id="m23"><mml:mo>∀</mml:mo><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:math></disp-formula><disp-formula id="equ24"><mml:math id="m24"><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Contrary to the ideal observer, here the likelihood of reward per action <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is learned through function approximation.<disp-formula id="equ25"><mml:math id="m25"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Action selection is done through the epsilon-greedy softmax rule.</p></sec><sec id="s4-21"><title>Weight vector update</title><p>Once an action <inline-formula><mml:math id="inf77"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is chosen and a reward <inline-formula><mml:math id="inf78"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is received at trial <inline-formula><mml:math id="inf79"><mml:mi>t</mml:mi></mml:math></inline-formula>, the weights are updated through gradient descent with learning rate <italic>α</italic>.<disp-formula id="equ26"><mml:math id="m26"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ27"><mml:math id="m27"><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>⟵</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mi>ϕ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>As learning improved steadily in this model contrary to the asymptotic behavior of monkeys, we implemented a weight decay to asymptotic values <inline-formula><mml:math id="inf80"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>:<disp-formula id="equ28"><mml:math id="m28"><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>⟵</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Note that resetting the weights at the beginning of each block and adding a weight decay (or a learning rate decay) provide similar fits to the dataset. Also, this decay can be included in the previous two models without any change of our results and conclusions.</p></sec><sec id="s4-22"><title>Belief update</title><p>From making an action <inline-formula><mml:math id="inf81"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula>, the agent receives a reward <inline-formula><mml:math id="inf82"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> and the beliefs over axes are updated:<disp-formula id="equ29"><mml:math id="m29"><mml:mrow><mml:mi mathvariant="normal">∀</mml:mi><mml:mtext> </mml:mtext><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ30"><mml:math id="m30"><mml:mrow><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-23"><title>Parameter values</title><p><italic>β</italic> and <italic>ε</italic> are fixed to, respectively, 10 and 0.02. As predicted from the behavior, noise perception is higher for shape than for color for both monkeys (<inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). Also, the model fitted on Monkey S behavior has an initial bias for <italic>Axis 2</italic> (<inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). For fitting the model on Monkey C behavior, we fix <inline-formula><mml:math id="inf85"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>. Finally, the fitted values of <inline-formula><mml:math id="inf86"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> correspond to an encoding of an average between Rules 1 and 3 on <italic>Axis 1</italic>, and an encoding of Rule 2 on <italic>Axis 2</italic>, for both monkeys (see <xref ref-type="table" rid="table1">Table 1</xref>).</p></sec><sec id="s4-24"><title>Research standards</title><p>Codes and data supporting the findings of this study are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/buschman-lab/FastRuleSwitchingSlowRuleUpdating">https://github.com/buschman-lab/FastRuleSwitchingSlowRuleUpdating</ext-link>, copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:2eb8d9709127da026e75d0f5368a493d56bc9076;origin=https://github.com/buschman-lab/FastRuleSwitchingSlowRuleUpdating;visit=swh:1:snp:6a3e1506289ca64b25b4ce9a8068159cc4761651;anchor=swh:1:rev:9a7cde4a06e8571d7b955750b599221c40acfac5">swh:1:rev:9a7cde4a06e8571d7b955750b599221c40acfac5</ext-link>; <xref ref-type="bibr" rid="bib16">Bouchacourt, 2022</xref>).</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Resource</th><th align="left" valign="bottom">Source</th><th align="left" valign="bottom">Identifier</th></tr></thead><tbody><tr><td align="left" valign="bottom"><italic>Macaca mulatta</italic></td><td align="left" valign="bottom">Mannheimer Foundation</td><td align="char" char="ndash" valign="bottom">10-52,10-153</td></tr><tr><td align="left" valign="bottom">PyStan 2.19</td><td align="left" valign="bottom">Stan Development Team</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://mc-stan.org/users/interfaces/pystan.html">https://mc-stan.org/users/interfaces/pystan.html</ext-link></td></tr><tr><td align="left" valign="bottom">POV-ray</td><td align="left" valign="bottom">Persistence of Vision Pty Ltd</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="http://www.povray.org/">http://www.povray.org/</ext-link></td></tr><tr><td align="left" valign="bottom">MATLAB R2015a</td><td align="left" valign="bottom">Mathworks</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com">https://www.mathworks.com</ext-link></td></tr><tr><td align="left" valign="bottom">Python 3.6</td><td align="left" valign="bottom">Python software foundation</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://www.python.org/">https://www.python.org/</ext-link></td></tr></tbody></table></table-wrap></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Validation, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Validation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Data curation, Supervision, Funding acquisition, Validation, Methodology, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Validation, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experimental procedures were approved by Princeton University Institutional Animal Care and Use Committee (protocol #3055) and were in accordance with the policies and procedures of the National Institutes of Health.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-82531-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Codes and data supporting the findings of this study are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/buschman-lab/FastRuleSwitchingSlowRuleUpdating">https://github.com/buschman-lab/FastRuleSwitchingSlowRuleUpdating</ext-link>, copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:2eb8d9709127da026e75d0f5368a493d56bc9076;origin=https://github.com/buschman-lab/FastRuleSwitchingSlowRuleUpdating;visit=swh:1:snp:6a3e1506289ca64b25b4ce9a8068159cc4761651;anchor=swh:1:rev:9a7cde4a06e8571d7b955750b599221c40acfac5">swh:1:rev:9a7cde4a06e8571d7b955750b599221c40acfac5</ext-link>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors thank Sam Zorowitz for helpful discussions on the statistical modeling platform Stan.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antzoulatos</surname><given-names>EG</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Differences between neural activity in prefrontal cortex and striatum during learning of novel Abstract categories</article-title><source>Neuron</source><volume>71</volume><fpage>243</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.05.040</pub-id><pub-id pub-id-type="pmid">21791284</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asaad</surname><given-names>WF</given-names></name><name><surname>Rainer</surname><given-names>G</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Neural activity in the primate prefrontal cortex during associative learning</article-title><source>Neuron</source><volume>21</volume><fpage>1399</fpage><lpage>1407</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80658-3</pub-id><pub-id pub-id-type="pmid">9883732</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asaad</surname><given-names>WF</given-names></name><name><surname>Rainer</surname><given-names>G</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Task-Specific neural activity in the primate prefrontal cortex</article-title><source>Journal of Neurophysiology</source><volume>84</volume><fpage>451</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.84.1.451</pub-id><pub-id pub-id-type="pmid">10899218</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname><given-names>D</given-names></name><name><surname>Kayser</surname><given-names>AS</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Frontal cortex and the discovery of abstract action rules</article-title><source>Neuron</source><volume>66</volume><fpage>315</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.03.025</pub-id><pub-id pub-id-type="pmid">20435006</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname><given-names>D</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mechanisms of hierarchical reinforcement learning in cortico-striatal circuits 2: evidence from fmri</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>527</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr117</pub-id><pub-id pub-id-type="pmid">21693491</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balewski</surname><given-names>ZZ</given-names></name><name><surname>Knudsen</surname><given-names>EB</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Fast and slow contributions to decision-making in corticostriatal circuits</article-title><source>Neuron</source><volume>110</volume><fpage>2170</fpage><lpage>2182</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.04.005</pub-id><pub-id pub-id-type="pmid">35525242</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartolo</surname><given-names>R</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Prefrontal cortex predicts state switches during reversal learning</article-title><source>Neuron</source><volume>106</volume><fpage>1044</fpage><lpage>1054</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.03.024</pub-id><pub-id pub-id-type="pmid">32315603</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bayer</surname><given-names>HM</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Midbrain dopamine neurons encode a quantitative reward prediction error signal</article-title><source>Neuron</source><volume>47</volume><fpage>129</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.05.020</pub-id><pub-id pub-id-type="pmid">15996553</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Hanks</surname><given-names>T</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Roitman</surname><given-names>J</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Probabilistic population codes for Bayesian decision making</article-title><source>Neuron</source><volume>60</volume><fpage>1142</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.09.021</pub-id><pub-id pub-id-type="pmid">19109917</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning the value of information in an uncertain world</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1214</fpage><lpage>1221</lpage><pub-id pub-id-type="doi">10.1038/nn1954</pub-id><pub-id pub-id-type="pmid">17676057</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bichot</surname><given-names>NP</given-names></name><name><surname>Schall</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effects of similarity and history on neural mechanisms of visual selection</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>549</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.1038/9205</pub-id><pub-id pub-id-type="pmid">10448220</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boettiger</surname><given-names>CA</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Frontal networks for learning and executing arbitrary stimulus-response associations</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>2723</fpage><lpage>2732</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3697-04.2005</pub-id><pub-id pub-id-type="pmid">15758182</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boorman</surname><given-names>ED</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>How green is the grass on the other side? frontopolar cortex and the evidence in favor of alternative courses of action</article-title><source>Neuron</source><volume>62</volume><fpage>733</fpage><lpage>743</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.05.014</pub-id><pub-id pub-id-type="pmid">19524531</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Ritter</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>JX</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Blundell</surname><given-names>C</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reinforcement learning, fast and slow</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>408</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.02.006</pub-id><pub-id pub-id-type="pmid">31003893</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouchacourt</surname><given-names>F</given-names></name><name><surname>Palminteri</surname><given-names>S</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Temporal chunking as a mechanism for unsupervised learning of task-sets</article-title><source>eLife</source><volume>9</volume><elocation-id>e50469</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.50469</pub-id><pub-id pub-id-type="pmid">32149602</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bouchacourt</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>FastRuleSwitchingSlowRuleUpdating</data-title><version designator="swh:1:rev:9a7cde4a06e8571d7b955750b599221c40acfac5">swh:1:rev:9a7cde4a06e8571d7b955750b599221c40acfac5</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:2eb8d9709127da026e75d0f5368a493d56bc9076;origin=https://github.com/buschman-lab/FastRuleSwitchingSlowRuleUpdating;visit=swh:1:snp:6a3e1506289ca64b25b4ce9a8068159cc4761651;anchor=swh:1:rev:9a7cde4a06e8571d7b955750b599221c40acfac5">https://archive.softwareheritage.org/swh:1:dir:2eb8d9709127da026e75d0f5368a493d56bc9076;origin=https://github.com/buschman-lab/FastRuleSwitchingSlowRuleUpdating;visit=swh:1:snp:6a3e1506289ca64b25b4ce9a8068159cc4761651;anchor=swh:1:rev:9a7cde4a06e8571d7b955750b599221c40acfac5</ext-link></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bugg</surname><given-names>JM</given-names></name><name><surname>Jacoby</surname><given-names>LL</given-names></name><name><surname>Toth</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Multiple levels of control in the stroop task</article-title><source>Memory &amp; Cognition</source><volume>36</volume><fpage>1484</fpage><lpage>1494</lpage><pub-id pub-id-type="doi">10.3758/MC.36.8.1484</pub-id><pub-id pub-id-type="pmid">19015507</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buschman</surname><given-names>TJ</given-names></name><name><surname>Denovellis</surname><given-names>EL</given-names></name><name><surname>Diogo</surname><given-names>C</given-names></name><name><surname>Bullock</surname><given-names>D</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Synchronous oscillatory neural ensembles for rules in the prefrontal cortex</article-title><source>Neuron</source><volume>76</volume><fpage>838</fpage><lpage>846</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.09.029</pub-id><pub-id pub-id-type="pmid">23177967</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Busse</surname><given-names>L</given-names></name><name><surname>Ayaz</surname><given-names>A</given-names></name><name><surname>Dhruv</surname><given-names>NT</given-names></name><name><surname>Katzner</surname><given-names>S</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Schölvinck</surname><given-names>ML</given-names></name><name><surname>Zaharia</surname><given-names>AD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The detection of visual contrast in the behaving mouse</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>11351</fpage><lpage>11361</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6689-10.2011</pub-id><pub-id pub-id-type="pmid">21813694</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carpenter</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stan: A probabilistic programming language</article-title><source>Journal of Statistical Software</source><volume>76</volume><elocation-id>v076.i01</elocation-id><pub-id pub-id-type="doi">10.18637/jss.v076.i01</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carter</surname><given-names>CS</given-names></name><name><surname>Mintun</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Interference and facilitation effects during selective attention: an H215O PET study of stroop task performance</article-title><source>NeuroImage</source><volume>2</volume><fpage>264</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1006/nimg.1995.1034</pub-id><pub-id pub-id-type="pmid">9343611</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>SCY</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A probability distribution over latent causes, in the orbitofrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>7817</fpage><lpage>7828</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0659-16.2016</pub-id><pub-id pub-id-type="pmid">27466328</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>A</given-names></name><name><surname>Koechlin</surname><given-names>ER</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Reasoning, learning, and creativity: frontal lobe function and human decision-making</article-title><source>PLOS Biology</source><volume>10</volume><elocation-id>e1001293</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001293</pub-id><pub-id pub-id-type="pmid">22479152</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AGE</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural signature of hierarchically structured expectations predicts clustering and transfer of rule sets in reinforcement learning</article-title><source>Cognition</source><volume>152</volume><fpage>160</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2016.04.002</pub-id><pub-id pub-id-type="pmid">27082659</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortical substrates for exploratory decisions in humans</article-title><source>Nature</source><volume>441</volume><fpage>876</fpage><lpage>879</lpage><pub-id pub-id-type="doi">10.1038/nature04766</pub-id><pub-id pub-id-type="pmid">16778890</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The cognitive neuroscience of motivation and learning</article-title><source>Social Cognition</source><volume>26</volume><fpage>593</fpage><lpage>620</lpage><pub-id pub-id-type="doi">10.1521/soco.2008.26.5.593</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Chapter 21 - multiple systems for value learning</chapter-title><person-group person-group-type="editor"><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Fehr</surname><given-names>E</given-names></name></person-group><source>Neuroeconomics</source><publisher-name>Academic Press</publisher-name><fpage>393</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-416008-8.00021-8</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Tobler</surname><given-names>PN</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Chapter 15 - value learning through reinforcement: the basics of dopamine and reinforcement learning</chapter-title><person-group person-group-type="editor"><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Fehr</surname><given-names>E</given-names></name></person-group><source>Neuroeconomics</source><publisher-name>Academic Press</publisher-name><fpage>283</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-416008-8.00015-2</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Day</surname><given-names>JJ</given-names></name><name><surname>Roitman</surname><given-names>MF</given-names></name><name><surname>Wightman</surname><given-names>RM</given-names></name><name><surname>Carelli</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Associative learning mediates dynamic shifts in dopamine signaling in the nucleus accumbens</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1020</fpage><lpage>1028</lpage><pub-id pub-id-type="doi">10.1038/nn1923</pub-id><pub-id pub-id-type="pmid">17603481</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision theory, reinforcement learning, and the brain</article-title><source>Cognitive, Affective &amp; Behavioral Neuroscience</source><volume>8</volume><fpage>429</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.3758/CABN.8.4.429</pub-id><pub-id pub-id-type="pmid">19033240</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dias</surname><given-names>R</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name><name><surname>Roberts</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Primate analogue of the Wisconsin card sorting test: effects of excitotoxic lesions of the prefrontal cortex in the marmoset</article-title><source>Behavioral Neuroscience</source><volume>110</volume><fpage>872</fpage><lpage>886</lpage><pub-id pub-id-type="doi">10.1037//0735-7044.110.5.872</pub-id><pub-id pub-id-type="pmid">8918991</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Goals and habits in the brain</article-title><source>Neuron</source><volume>80</volume><fpage>312</fpage><lpage>325</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.007</pub-id><pub-id pub-id-type="pmid">24139036</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoso</surname><given-names>M</given-names></name><name><surname>Collins</surname><given-names>AGE</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Human cognition foundations of human reasoning in the prefrontal cortex</article-title><source>Science</source><volume>344</volume><fpage>1481</fpage><lpage>1486</lpage><pub-id pub-id-type="doi">10.1126/science.1252254</pub-id><pub-id pub-id-type="pmid">24876345</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Reinforcement learning: computational theory and biological mechanisms</article-title><source>HFSP Journal</source><volume>1</volume><fpage>30</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.2976/1.2732246/10.2976/1</pub-id><pub-id pub-id-type="pmid">19404458</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Duncker</surname><given-names>L</given-names></name><name><surname>Driscoll</surname><given-names>L</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Organizing recurrent network dynamics by task-computation to enable continual learning</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>14387</fpage><lpage>14397</lpage></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durstewitz</surname><given-names>D</given-names></name><name><surname>Vittoz</surname><given-names>NM</given-names></name><name><surname>Floresco</surname><given-names>SB</given-names></name><name><surname>Seamans</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Abrupt transitions between prefrontal neural ensemble states accompany behavioral transitions during rule learning</article-title><source>Neuron</source><volume>66</volume><fpage>438</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.03.029</pub-id><pub-id pub-id-type="pmid">20471356</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname><given-names>MJ</given-names></name><name><surname>Badre</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: computational analysis</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>509</fpage><lpage>526</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr114</pub-id><pub-id pub-id-type="pmid">21693490</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franklin</surname><given-names>NT</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Compositional clustering in task structure learning</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006116</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006116</pub-id><pub-id pub-id-type="pmid">29672581</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fründ</surname><given-names>I</given-names></name><name><surname>Wichmann</surname><given-names>FA</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Quantifying the effect of intertrial dependence on perceptual decisions</article-title><source>Journal of Vision</source><volume>14</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.1167/14.7.9</pub-id><pub-id pub-id-type="pmid">24944238</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Inference from iterative simulation using multiple sequences</article-title><source>Statistical Science</source><volume>7</volume><fpage>457</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1214/ss/1177011136</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Genovesio</surname><given-names>A</given-names></name><name><surname>Brasted</surname><given-names>PJ</given-names></name><name><surname>Mitz</surname><given-names>AR</given-names></name><name><surname>Wise</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Prefrontal cortex activity related to Abstract response strategies</article-title><source>Neuron</source><volume>47</volume><fpage>307</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.06.006</pub-id><pub-id pub-id-type="pmid">16039571</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Radulescu</surname><given-names>A</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Statistical computations underlying the dynamics of memory updating</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003939</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003939</pub-id><pub-id pub-id-type="pmid">25375816</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural computations that underlie decisions about sensory stimuli</article-title><source>Trends in Cognitive Sciences</source><volume>5</volume><fpage>10</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(00)01567-9</pub-id><pub-id pub-id-type="pmid">11164731</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Law</surname><given-names>CT</given-names></name><name><surname>Connolly</surname><given-names>P</given-names></name><name><surname>Bennur</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The relative influences of priors and sensory evidence on an oculomotor decision variable during perceptual learning</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>2653</fpage><lpage>2668</lpage><pub-id pub-id-type="doi">10.1152/jn.90629.2008</pub-id><pub-id pub-id-type="pmid">18753326</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hadsell</surname><given-names>R</given-names></name><name><surname>Rao</surname><given-names>D</given-names></name><name><surname>Rusu</surname><given-names>AA</given-names></name><name><surname>Pascanu</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Embracing change: continual learning in deep neural networks</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>1028</fpage><lpage>1040</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.09.004</pub-id><pub-id pub-id-type="pmid">33158755</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hampton</surname><given-names>AN</given-names></name><name><surname>Bossaerts</surname><given-names>P</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The role of the ventromedial prefrontal cortex in Abstract state-based inference during decision making in humans</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>8360</fpage><lpage>8367</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1010-06.2006</pub-id><pub-id pub-id-type="pmid">16899731</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harlow</surname><given-names>HF</given-names></name></person-group><year iso-8601-date="1949">1949</year><article-title>The formation of learning sets</article-title><source>Psychological Review</source><volume>56</volume><fpage>51</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1037/h0062474</pub-id><pub-id pub-id-type="pmid">18124807</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koechlin</surname><given-names>E</given-names></name><name><surname>Ody</surname><given-names>C</given-names></name><name><surname>Kouneiher</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The architecture of cognitive control in the human prefrontal cortex</article-title><source>Science</source><volume>302</volume><fpage>1181</fpage><lpage>1185</lpage><pub-id pub-id-type="doi">10.1126/science.1088545</pub-id><pub-id pub-id-type="pmid">14615530</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koechlin</surname><given-names>E</given-names></name><name><surname>Hyafil</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Anterior prefrontal function and the limits of human decision-making</article-title><source>Science</source><volume>318</volume><fpage>594</fpage><lpage>598</lpage><pub-id pub-id-type="doi">10.1126/science.1142995</pub-id><pub-id pub-id-type="pmid">17962551</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lak</surname><given-names>A</given-names></name><name><surname>Hueske</surname><given-names>E</given-names></name><name><surname>Hirokawa</surname><given-names>J</given-names></name><name><surname>Masset</surname><given-names>P</given-names></name><name><surname>Ott</surname><given-names>T</given-names></name><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Tonegawa</surname><given-names>S</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Reinforcement biases subsequent perceptual decisions when confidence is low, a widespread behavioral phenomenon</article-title><source>eLife</source><volume>9</volume><elocation-id>e49834</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49834</pub-id><pub-id pub-id-type="pmid">32286227</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname><given-names>B</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Value representations in the primate striatum during matching behavior</article-title><source>Neuron</source><volume>58</volume><fpage>451</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.02.021</pub-id><pub-id pub-id-type="pmid">18466754</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mansouri</surname><given-names>FA</given-names></name><name><surname>Matsumoto</surname><given-names>K</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Prefrontal cell activities related to monkeys’ success and failure in adapting to rule changes in a Wisconsin card sorting test analog</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>2745</fpage><lpage>2756</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5238-05.2006</pub-id><pub-id pub-id-type="pmid">16525054</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mansouri</surname><given-names>FA</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Buckley</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Emergence of Abstract rules in the primate brain</article-title><source>Nature Reviews. Neuroscience</source><volume>21</volume><fpage>595</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0364-5</pub-id><pub-id pub-id-type="pmid">32929262</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McCloskey</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="1989">1989</year><chapter-title>Catastrophic interference in connectionist networks: the sequential learning problem</chapter-title><person-group person-group-type="editor"><name><surname>Bower</surname><given-names>G</given-names></name></person-group><source>Psychology of Learning and Motivation</source><publisher-name>Academic Press</publisher-name><fpage>109</fpage><lpage>165</lpage></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An integrative theory of prefrontal cortex function</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>167</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.167</pub-id><pub-id pub-id-type="pmid">11283309</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milner</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Effects of different brain lesions on card sorting: the role of the frontal lobes</article-title><source>Archives of Neurology</source><volume>9</volume><fpage>90</fpage><lpage>100</lpage></element-citation></ref><ref id="bib58"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Musslick</surname><given-names>S</given-names></name><name><surname>Jang</surname><given-names>SJ</given-names></name><name><surname>Shvartsman</surname><given-names>M</given-names></name><name><surname>Shenhav</surname><given-names>A</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Constraints associated with cognitive control and the stability-ﬂexibility dilemma</article-title><conf-name>Annual Conference of the Cognitive Science Society. Cognitive Science Society (U.S.). Conference</conf-name></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musslick</surname><given-names>S</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Rationalizing constraints on the capacity for cognitive control</article-title><source>Trends in Cognitive Sciences</source><volume>25</volume><fpage>757</fpage><lpage>775</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2021.06.001</pub-id><pub-id pub-id-type="pmid">34332856</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakahara</surname><given-names>K</given-names></name><name><surname>Hayashi</surname><given-names>T</given-names></name><name><surname>Konishi</surname><given-names>S</given-names></name><name><surname>Miyashita</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Functional MRI of macaque monkeys performing a cognitive set-shifting task</article-title><source>Science</source><volume>295</volume><fpage>1532</fpage><lpage>1536</lpage><pub-id pub-id-type="doi">10.1126/science.1067653</pub-id><pub-id pub-id-type="pmid">11859197</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Troiani</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The stability flexibility tradeoff and the dark side of detail</article-title><source>Cognitive, Affective &amp; Behavioral Neuroscience</source><volume>21</volume><fpage>607</fpage><lpage>623</lpage><pub-id pub-id-type="doi">10.3758/s13415-020-00848-8</pub-id><pub-id pub-id-type="pmid">33236296</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noppeney</surname><given-names>U</given-names></name><name><surname>Ostwald</surname><given-names>D</given-names></name><name><surname>Werner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Perceptual decisions formed by accumulation of audiovisual evidence in prefrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>7434</fpage><lpage>7446</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0455-10.2010</pub-id><pub-id pub-id-type="pmid">20505110</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Doherty</surname><given-names>J</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Schultz</surname><given-names>J</given-names></name><name><surname>Deichmann</surname><given-names>R</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Dissociable roles of ventral and dorsal striatum in instrumental conditioning</article-title><source>Science</source><volume>304</volume><fpage>452</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1126/science.1094285</pub-id><pub-id pub-id-type="pmid">15087550</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>RC</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia</article-title><source>Neural Computation</source><volume>18</volume><fpage>283</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1162/089976606775093909</pub-id><pub-id pub-id-type="pmid">16378516</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name><name><surname>Assad</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neurons in the orbitofrontal cortex encode economic value</article-title><source>Nature</source><volume>441</volume><fpage>223</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1038/nature04676</pub-id><pub-id pub-id-type="pmid">16633341</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palminteri</surname><given-names>S</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The importance of falsification in computational cognitive modeling</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>425</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.03.011</pub-id><pub-id pub-id-type="pmid">28476348</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Probabilistic brains: knowns and unknowns</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1170</fpage><lpage>1178</lpage><pub-id pub-id-type="doi">10.1038/nn.3495</pub-id><pub-id pub-id-type="pmid">23955561</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Confidence and certainty: distinct probabilistic quantities for different goals</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>366</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1038/nn.4240</pub-id><pub-id pub-id-type="pmid">26906503</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Purcell</surname><given-names>BA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Hierarchical decision processes that operate over distinct timescales underlie choice and changes in strategy</article-title><source>PNAS</source><volume>113</volume><fpage>E4531</fpage><lpage>E4540</lpage><pub-id pub-id-type="doi">10.1073/pnas.1524685113</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qi</surname><given-names>G</given-names></name><name><surname>Fang</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Neural dynamics of causal inference in the macaque frontoparietal circuit</article-title><source>eLife</source><volume>11</volume><elocation-id>e76145</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.76145</pub-id><pub-id pub-id-type="pmid">36279158</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RPN</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Decision making under uncertainty: a neural model based on partially observable Markov decision processes</article-title><source>Frontiers in Computational Neuroscience</source><volume>4</volume><elocation-id>146</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2010.00146</pub-id><pub-id pub-id-type="pmid">21152255</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinert</surname><given-names>S</given-names></name><name><surname>Hübener</surname><given-names>M</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Goltstein</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Mouse prefrontal cortex represents learned rules for categorization</article-title><source>Nature</source><volume>593</volume><fpage>411</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03452-z</pub-id><pub-id pub-id-type="pmid">33883745</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rescorla</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Pavlovian conditioning it’s not what you think it is</article-title><source>The American Psychologist</source><volume>43</volume><fpage>151</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1037//0003-066x.43.3.151</pub-id><pub-id pub-id-type="pmid">3364852</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rougier</surname><given-names>NP</given-names></name><name><surname>Noelle</surname><given-names>DC</given-names></name><name><surname>Braver</surname><given-names>TS</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>O’Reilly</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Prefrontal cortex and flexible cognitive control: rules without symbols</article-title><source>PNAS</source><volume>102</volume><fpage>7338</fpage><lpage>7343</lpage><pub-id pub-id-type="doi">10.1073/pnas.0502455102</pub-id><pub-id pub-id-type="pmid">15883365</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rushworth</surname><given-names>MFS</given-names></name><name><surname>Noonan</surname><given-names>MP</given-names></name><name><surname>Boorman</surname><given-names>ED</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Frontal cortex and reward-guided learning and decision-making</article-title><source>Neuron</source><volume>70</volume><fpage>1054</fpage><lpage>1069</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.05.014</pub-id><pub-id pub-id-type="pmid">21689594</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sakai</surname><given-names>K</given-names></name><name><surname>Passingham</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Prefrontal interactions reflect future task operations</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>75</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1038/nn987</pub-id><pub-id pub-id-type="pmid">12469132</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samejima</surname><given-names>K</given-names></name><name><surname>Ueda</surname><given-names>Y</given-names></name><name><surname>Doya</surname><given-names>K</given-names></name><name><surname>Kimura</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Representation of action-specific reward values in the striatum</article-title><source>Science</source><volume>310</volume><fpage>1337</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.1126/science.1115270</pub-id><pub-id pub-id-type="pmid">16311337</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarafyazd</surname><given-names>M</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hierarchical Reasoning by neural circuits in the frontal cortex</article-title><source>Science</source><volume>364</volume><elocation-id>eaav8911</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav8911</pub-id><pub-id pub-id-type="pmid">31097640</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuck</surname><given-names>NW</given-names></name><name><surname>Cai</surname><given-names>MB</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Human orbitofrontal cortex represents a cognitive map of state space</article-title><source>Neuron</source><volume>91</volume><fpage>1402</fpage><lpage>1412</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.08.019</pub-id><pub-id pub-id-type="pmid">27657452</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seo</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>E</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Action selection and action value in frontal-striatal circuits</article-title><source>Neuron</source><volume>74</volume><fpage>947</fpage><lpage>960</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.037</pub-id><pub-id pub-id-type="pmid">22681697</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Decision making as a window on cognition</article-title><source>Neuron</source><volume>80</volume><fpage>791</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.047</pub-id><pub-id pub-id-type="pmid">24183028</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoianov</surname><given-names>I</given-names></name><name><surname>Genovesio</surname><given-names>A</given-names></name><name><surname>Pezzulo</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prefrontal goal codes emerge as latent states in probabilistic value learning</article-title><source>Journal of Cognitive Neuroscience</source><volume>28</volume><fpage>140</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00886</pub-id><pub-id pub-id-type="pmid">26439267</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning, Second Edition: An Introduction</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsunada</surname><given-names>J</given-names></name><name><surname>Cohen</surname><given-names>Y</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Post-decision processing in primate prefrontal cortex influences subsequent choices on an auditory decision-making task</article-title><source>eLife</source><volume>8</volume><elocation-id>e46770</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.46770</pub-id><pub-id pub-id-type="pmid">31169495</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>van de Ven</surname><given-names>GM</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Generative Replay with Feedback Connections as a General Strategy for Continual Learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1809.10635">https://arxiv.org/abs/1809.10635</ext-link></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venkatraman</surname><given-names>V</given-names></name><name><surname>Rosati</surname><given-names>AG</given-names></name><name><surname>Taren</surname><given-names>AA</given-names></name><name><surname>Huettel</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Resolving response, decision, and strategic control: evidence for a functional topography in dorsomedial prefrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>13158</fpage><lpage>13164</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2708-09.2009</pub-id><pub-id pub-id-type="pmid">19846703</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>IM</given-names></name><name><surname>Wise</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Rule-dependent neuronal activity in the prefrontal cortex</article-title><source>Experimental Brain Research</source><volume>126</volume><fpage>315</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1007/s002210050740</pub-id><pub-id pub-id-type="pmid">10382618</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>HH</given-names></name><name><surname>Knowlton</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The role of the basal ganglia in habit formation</article-title><source>Nature Reviews. Neuroscience</source><volume>7</volume><fpage>464</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1038/nrn1919</pub-id><pub-id pub-id-type="pmid">16715055</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82531.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Badre</surname><given-names>David</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.01.29.478330" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.01.29.478330"/></front-stub><body><p>This important study modeled monkeys' behavior in a stimulus-response rule-learning task to show that animals can adopt mixed strategies involving inference for learning latent states and incremental updating for learning action-outcome associations. The task is cleverly designed, the modeling is rigorous, and importantly there are clear distinctions in the behavior generated by different models, which makes the conclusions convincing.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82531.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Badre</surname><given-names>David</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.01.29.478330">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.01.29.478330v3">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Fast rule switching and slow rule updating in a perceptual categorization task&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a David Badre as Reviewing Editor and Michael Frank as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The reviewers and editors were in agreement that this is a strong contribution. In consultation, it was agreed that to strengthen the impact, there are some points that could be revised. In particular:</p><p>1) The design should be clarified to address some points of confusion about the specifics.</p><p>2) Both reviewers raised some alternatives that should be considered. And, if not ruled out, they should be discussed in relation to the findings and in comparison to the favored account.</p><p>These points are detailed in the reviewers' comments below. We have left them unedited as they are clear in the points that could be strengthened through revision.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>As indicated in the discussion, the regularity in the stimulus-response mappings makes it possible for the animals to learn the task structure and behave according to the ideal observer model, which would be an optimal strategy. However, they didn't fully accomplish this, likely due to the difficulty of learning the full task structure compared to the repeated switches between response sets. On one hand, settling on a hybrid strategy might be the subjects' preferred trade-off between effort and optimal outcomes. The authors seem to favor this as a possible explanation . However, it could also be that monkeys would eventually learn to use rule inference with more practice. With this in mind, it would be helpful if more detail about the duration of the monkeys' training in the task could be supplied in the methods. It would also be of interest to know whether there are any changes in behavior across sessions included in this study.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. I found the introduction to be a little confusing. Reference is made obliquely to a &quot;general purpose incremental learning mechanism&quot; but I was initially a bit unclear about what is meant here. It would have been helpful for me as a reader if the authors had used more standard terminology, e.g. introducing trial and error learning and latent state inference; and highlighting explicitly work that has focussed on understanding how these processes are implemented, and how they work together, and especially highlighting work in monkeys. There is quite a lot of reference to RNN models and/or meta-learning methods but these seem less relevant here given the modelling approach. Generally, I thought the introduction could have been clearer, more focused, and spoken more directly to the specific questions addressed in the paper.</p><p>2. Although the paper is generally quite clear, I found the description of the task a little bit hard to follow initially. In particular, the essential feature of the design – that rule 2 occurs twice as often as the other rules and rules 1 and 3 never occur in sequence – is visible in Figure 1c but is otherwise not explicitly mentioned in the main text (or was not sufficiently highlighted for me). This is not an incidental feature of the task (&quot;to facilitate learning and performance&quot;) but is the key driver of the main result, so I think it needs a bit more introduction/foregrounding for reader clarity.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82531.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The reviewers and editors were in agreement that this is a strong contribution. In consultation, it was agreed that to strengthen the impact, there are some points that could be revised. In particular:</p><p>1) The design should be clarified to address some points of confusion about the specifics.</p><p>2) Both reviewers raised some alternatives that should be considered. And, if not ruled out, they should be discussed in relation to the findings and in comparison to the favored account.</p><p>These points are detailed in the reviewers' comments below. We have left them unedited as they are clear in the points that could be strengthened through revision.</p></disp-quote><p>We are grateful to the editors and the reviewers for the thoughtful comments. As discussed below, we have addressed the suggested revisions in the new version.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>As indicated in the discussion, the regularity in the stimulus-response mappings makes it possible for the animals to learn the task structure and behave according to the ideal observer model, which would be an optimal strategy. However, they didn't fully accomplish this, likely due to the difficulty of learning the full task structure compared to the repeated switches between response sets. On one hand, settling on a hybrid strategy might be the subjects' preferred trade-off between effort and optimal outcomes. The authors seem to favor this as a possible explanation. However, it could also be that monkeys would eventually learn to use rule inference with more practice. With this in mind, it would be helpful if more detail about the duration of the monkeys' training in the task could be supplied in the methods. It would also be of interest to know whether there are any changes in behavior across sessions included in this study.</p></disp-quote><p>We agree (and now make clear in the discussion) that the monkeys’ strategies might be different depending on the details of training. Regarding reporting what was actually done here, the Methods paragraph “Additional details on training” reports the length and protocol used for training the two monkeys. As mentioned in the methods, our intent was to train the monkeys until their behavior was stable. Consistent with this, in response to the reviewer’s comment, we examined this formally but found no significant trends in the averaged performance across days (Figure 5 —figure supplement 5a,d), or in the per day-by-day estimated model parameters (noise perception and learning rate Figure 5 —figure supplement 5b,c,e), with the exception of the learning rate in Monkey C which was moderately significant (Figure 5 —figure supplement 5f, though not surviving multiple comparison correction). We now discuss this in the manuscript.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1. I found the introduction to be a little confusing. Reference is made obliquely to a &quot;general purpose incremental learning mechanism&quot; but I was initially a bit unclear about what is meant here. It would have been helpful for me as a reader if the authors had used more standard terminology, e.g. introducing trial and error learning and latent state inference; and highlighting explicitly work that has focussed on understanding how these processes are implemented, and how they work together, and especially highlighting work in monkeys. There is quite a lot of reference to RNN models and/or meta-learning methods but these seem less relevant here given the modelling approach. Generally, I thought the introduction could have been clearer, more focused, and spoken more directly to the specific questions addressed in the paper.</p></disp-quote><p>We have streamlined and revised the introduction with the reviewer’s points and terminology in mind.</p><disp-quote content-type="editor-comment"><p>2. Although the paper is generally quite clear, I found the description of the task a little bit hard to follow initially. In particular, the essential feature of the design – that rule 2 occurs twice as often as the other rules and rules 1 and 3 never occur in sequence – is visible in Figure 1c but is otherwise not explicitly mentioned in the main text (or was not sufficiently highlighted for me). This is not an incidental feature of the task (&quot;to facilitate learning and performance&quot;) but is the key driver of the main result, so I think it needs a bit more introduction/foregrounding for reader clarity.</p></disp-quote><p>We thank the reviewer for this comment and have highlighted this essential feature of the design in the paragraph “Task design and performance”.</p></body></sub-article></article>