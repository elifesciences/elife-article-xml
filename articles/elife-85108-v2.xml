<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">85108</article-id><article-id pub-id-type="doi">10.7554/eLife.85108</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Large-scale electrophysiology and deep learning reveal distorted neural signal dynamics after hearing loss</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-299714"><name><surname>Sabesan</surname><given-names>Shievanie</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-299715"><name><surname>Fragner</surname><given-names>Andreas</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-315177"><name><surname>Bench</surname><given-names>Ciaran</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-315178"><name><surname>Drakopoulos</surname><given-names>Fotios</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-62401"><name><surname>Lesica</surname><given-names>Nicholas A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5238-4462</contrib-id><email>n.lesica@ucl.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Ear Institute, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution>Perceptual Technologies</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Herrmann</surname><given-names>Björn</given-names></name><role>Reviewing Editor</role><aff><institution>Baycrest</institution><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>10</day><month>05</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e85108</elocation-id><history><date date-type="received" iso-8601-date="2022-11-22"><day>22</day><month>11</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-04-27"><day>27</day><month>04</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-10-07"><day>07</day><month>10</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.10.04.510811"/></event></pub-history><permissions><copyright-statement>© 2023, Sabesan et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Sabesan et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-85108-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-85108-figures-v2.pdf"/><abstract><p>Listeners with hearing loss often struggle to understand speech in noise, even with a hearing aid. To better understand the auditory processing deficits that underlie this problem, we made large-scale brain recordings from gerbils, a common animal model for human hearing, while presenting a large database of speech and noise sounds. We first used manifold learning to identify the neural subspace in which speech is encoded and found that it is low-dimensional and that the dynamics within it are profoundly distorted by hearing loss. We then trained a deep neural network (DNN) to replicate the neural coding of speech with and without hearing loss and analyzed the underlying network dynamics. We found that hearing loss primarily impacts spectral processing, creating nonlinear distortions in cross-frequency interactions that result in a hypersensitivity to background noise that persists even after amplification with a hearing aid. Our results identify a new focus for efforts to design improved hearing aids and demonstrate the power of DNNs as a tool for the study of central brain structures.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>gerbil</kwd><kwd>hearing loss</kwd><kwd>deep learning</kwd><kwd>neural coding</kwd><kwd>neural dynamics</kwd><kwd>speech</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>200942/Z/16/Z</award-id><principal-award-recipient><name><surname>Sabesan</surname><given-names>Shievanie</given-names></name><name><surname>Lesica</surname><given-names>Nicholas A</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/W004275/1</award-id><principal-award-recipient><name><surname>Bench</surname><given-names>Ciaran</given-names></name><name><surname>Drakopoulos</surname><given-names>Fotios</given-names></name><name><surname>Lesica</surname><given-names>Nicholas A</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Deep neural network modeling of auditory processing identifies distorted cross-frequency interactions as the key problem for the processing of speech in noise after hearing loss.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Hearing loss is a widespread problem with far-reaching consequences ranging from lost productivity and social isolation to decreases in quality of life and mental health (<xref ref-type="bibr" rid="bib57">Wilson et al., 2017</xref>). It also imposes a growing societal burden with associated costs approaching $1 trillion annually (<xref ref-type="bibr" rid="bib58">World Health Organization, 2021</xref>). Hearing aids are the only widely available treatment for hearing loss, but, unfortunately, current devices provide limited benefit in many workplace and social settings (<xref ref-type="bibr" rid="bib28">Lesica, 2018</xref>).</p><p>The term ‘hearing loss’ does not capture the full spectrum of the effects of cochlear damage on auditory processing. One common consequence of cochlear damage is a loss of sensitivity that renders low-intensity sounds inaudible. This is the ‘hearing loss’ that is assessed in standard clinical tests and is addressed through amplification with hearing aids. But cochlear damage has other consequences beyond lost sensitivity that cause many people to struggle with high-intensity sounds that are well above their audibility threshold (<xref ref-type="bibr" rid="bib38">Moore, 2007</xref>). For example, people with hearing loss often have difficulties understanding speech in noisy settings, with or without a hearing aid (<xref ref-type="bibr" rid="bib27">Larson et al., 2000</xref>). The pathophysiology underlying these high-intensity deficits remains poorly understood, and, as a result, they are largely ignored by clinicians and hearing aid designers.</p><p>The consequences of cochlear damage for the processing of both low- and high-intensity sounds have been well described at the level of the auditory nerve (AN; <xref ref-type="bibr" rid="bib59">Young, 2008</xref>). In addition to the general decrease in neural activity resulting from lost sensitivity, there are also complex changes in the spatiotemporal structure of the neural activity patterns that encode acoustic information, such as lost synchrony capture (<xref ref-type="bibr" rid="bib36">Miller et al., 1997</xref>) and distorted tonotopy (<xref ref-type="bibr" rid="bib21">Henry et al., 2016</xref>). Many theories have attempted to explain how the peripheral changes associated with hearing loss might lead to perceptual deficits (<xref ref-type="bibr" rid="bib22">Humes and Dubno, 2010</xref>; <xref ref-type="bibr" rid="bib42">Plomp, 1986</xref>). But, with few explicit studies comparing neural coding in central auditory areas before and after hearing loss, it has been difficult to differentiate between competing theories or to identify which peripheral changes are most important to address.</p><p>One recent study of individual neurons in the inferior colliculus (IC) with and without hearing loss and hearing aids found that some properties, such as phoneme selectivity, were impacted while others, such as frequency selectivity and trial-to-trial variability, were not (<xref ref-type="bibr" rid="bib2">Armstrong et al., 2022</xref>). But, while characterizing the impact of hearing loss on neural coding in individual neurons may be sufficient at the level of the AN (a set of largely homogeneous fibers driven by a few thousand inner hair cells), neural coding in downstream areas such as the IC, which are much larger and more complex, likely involves emergent network-level properties that are not readily apparent in the activity of individual neurons.</p><p>One of the challenges in characterizing neural coding at the network level is the high dimensionality of the activity patterns. If the goal is to gain insight into the underlying computations that the activity reflects, it can be useful to find more compact representations of the full activity that retain its important features. This process, often termed ‘manifold learning’ (<xref ref-type="bibr" rid="bib37">Mitchell-Heggs et al., 2023</xref>; <xref ref-type="bibr" rid="bib55">Williamson et al., 2019</xref>), typically utilizes techniques such as principal component analysis (PCA) that identify a projection of the full activity into a lower dimensional space that retains as much of its variance as possible. In this study, we use manifold learning to investigate the impact of hearing loss on the neural coding of speech in gerbils, a common animal model for the study of human hearing. We employ large-scale intracranial recordings that allow us to achieve comprehensive sampling of activity from individual animals at the fine spatial and temporal scales that are critical for encoding speech (<xref ref-type="bibr" rid="bib15">Garcia-Lazaro et al., 2013</xref>).</p><p>We begin with the traditional approach to manifold learning using PCA to identify and analyze the low-dimensional subspace in which most of the variance in the full network activity patterns resides. We focus on the signal manifold, which captures the features of neural activity that are sound-evoked, first establishing that our recordings are sufficient to identify the signal manifold in individual animals and then that the changes in signal dynamics with hearing loss are fundamental, that is, that the dynamics within the signal manifold are not simply attenuated by hearing loss, but instead are truly distorted. We then continue our analysis using deep neural networks (DNNs) to perform manifold learning within the framework of a stimulus encoding model, which allows us to investigate the impact of hearing loss on the coding of novel sounds.</p><p>We demonstrate that training DNNs on our recordings allows for accurate prediction of neural activity in conjunction with identification of the signal manifold. We use the trained DNNs to probe the processing of basic acoustic features and show that hearing loss predominantly affects spectral, rather than temporal, processing. We then probe the processing of speech and find that this impaired spectral processing creates a hypersensitivity to background noise that persists even with a hearing aid and appears to arise from aberrant cross-frequency interactions. Our results demonstrate the power of DNNs to provide new insights into neural coding at the network level and suggest that new approaches to hearing aid design are required to address the highly nonlinear nature of the effects of hearing loss on spectral processing.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We recorded neural activity from the IC of anesthetized gerbils using electrode arrays with a total of 512 channels (<xref ref-type="bibr" rid="bib2">Armstrong et al., 2022</xref>), allowing us to sample widely from neurons that were sensitive to the full range of speech frequencies (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). We recorded activity in response to more than 10 hr of speech in each animal, presenting the entire TIMIT speech database (<xref ref-type="bibr" rid="bib16">Garofolo, 1993</xref>) twice – once in quiet and once in background noise, with the overall intensity, speech-to-noise ratio, and noise type varied from sentence to sentence (with a small number of sentences repeated multiple times under identical conditions to assess trial-to-trial variability). We processed the recordings to extract multi-unit activity (MUA) spike counts for each recording channel, using 1.3 ms time bins to account for the fact that neurons in the IC can encode information about speech with millisecond temporal precision (<xref ref-type="bibr" rid="bib15">Garcia-Lazaro et al., 2013</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Neural signal and noise in the gerbil inferior colliculus (IC).</title><p>(<bold>a</bold>) Schematic diagram showing the geometry of custom-designed electrode arrays for large-scale recordings in relation to the gerbil IC (center), along with the speech syllable ‘sa’ (left) and the neural activity that it elicited during an example recording (right). Each image of the neural activity corresponds to one hemisphere, with each row showing the average multi-unit activity recorded on one electrode over repeated presentations of the syllable, with the units arranged according to their location within the IC. The activity for three units with different center frequencies (CF; frequency for which sensitivity to pure tones is maximal) are shown in detail. (<bold>b</bold>) Schematic diagram showing the method for separating signal and noise in neural activity. The signal is obtained by averaging responses across repeated presentations of identical sounds. The noise is the residual activity that remains after subtracting the signal from the response to each individual presentation. (<bold>c</bold>) Signal and noise in neural activity. Left: total, signal, and noise variance in neural activity for units recorded from normal hearing animals (horizontal line indicates median, thick vertical line indicates 25th through 75th percentile, thin vertical line indicates 5th through 95th percentile; n = 2556). Right: total, signal, and noise correlation in neural activity for pairs of units recorded from normal hearing animals (n = 544,362).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85108-fig1-v2.tif"/></fig><sec id="s2-1"><title>The neural signal manifold is low dimensional</title><p>We began by analyzing activity from animals with normal hearing. As a first step toward characterizing the neural code for speech at the network level, we determined whether there was shared variance across units that would allow us to reduce the dimensionality of the activity patterns. Previous work has shown that the correlations in IC activity are dominated by ‘signal’ (features of activity that are reproducible across repeated trials and, thus, convey acoustic information) rather than ‘noise’ (features of activity that vary from trial-to-trial and reflect intrinsic noise or fluctuations in brain state) (<xref ref-type="fig" rid="fig1">Figure 1b</xref>; <xref ref-type="bibr" rid="bib15">Garcia-Lazaro et al., 2013</xref>).</p><p>Signal correlations were also dominant in our recordings: although signal variance (the covariance in activity across repeated trials) accounted for only 40% of the overall variance in activity, signal correlations accounted for 95% of the total correlation between units (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). For a network operating in such a regime, with each neuron having more than half of its variance uncorrelated with that of its neighbors, there is limited scope for reducing the dimensionality of the full activity patterns. However, given the large signal correlations, it may be possible to identify a low-dimensional subspace in which the acoustic information represented by the signal is embedded.</p><p>We partitioned the recordings from each animal into two sets: a training set that was used to identify the principal components (PCs) of the activity (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, step 1) and a test set with two repeated trials that was used to measure the variance that could be explained by each PC. To measure the total variance explained (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, steps 2a–c), we projected the activity from test trial 1 onto the PCs, then reconstructed the original activity from the same trial using the PC projection and compared the reconstruction to the original activity. The overall dimensionality of the neural activity was high, as expected, with a large number of PCs required for the reconstruction to explain 95% of the total variance in the original activity (<xref ref-type="fig" rid="fig2">Figure 2b</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Neural signal dynamics identified via classical methods.</title><p>(<bold>a</bold>) Schematic diagram showing the method for identifying the neural signal manifold from recordings of neural activity. Step 1: principal component analysis (PCA) is performed on a subset of the recordings allocated for training. Step 2a: a subset of recordings allocated for testing are projected onto the principal components (PCs) from step 1. Step 2b: The projections from step 2a are used to reconstruct the test recordings. Step 2c: the reconstructions from step 2b are compared to the test recordings from step 2a to determine the total variance explained. Step 3: the reconstructions from step 2b are compared to another set of test recordings (made during a second presentation of the same sounds) to determine the signal variance explained. Step 4: the projections from step 2a for one animal are compared to the projections for another animal to determine the similarity of the signal dynamics between animals. (<bold>b</bold>) Total variance explained in step 2c as a function of the number of PCs used for the reconstruction. Each thick line shows the results for one normal hearing animal (n = 6). The thin line denotes 95% variance explained. (<bold>c</bold>) Signal variance explained in step 3 as a function of the number of PCs used for the reconstruction. (<bold>d</bold>) Percent of variance explained by each PC that corresponds to neural signal (rather than neural noise) for an example animal. (<bold>e</bold>) Variance explained in step 4 for each pair of normal hearing animals. (<bold>f, g</bold>) Total variance explained in step 2c and signal variance explained in step 3 for animals with hearing loss (n = 6). (<bold>h</bold>) Variance explained in step 4 for each pair of animals with hearing loss and each pair of animals with different hearing status. (<bold>i</bold>) Distributions of variance explained in step 4 for each pair of normal hearing animals (n = 15), each pair of animals with hearing loss (n = 15), and each pair of animals with different hearing status (n = 36). Median values were compared via Kruskal–Wallis one-way ANOVA and Tukey–Kramer post hoc tests, ***p&lt;0.001, **p&lt;0.01, *p&lt;0.05, ns indicates not significant. For full details of statistical tests, see <xref ref-type="table" rid="table1">Table 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85108-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Sloping mild-to-moderate sensorineural hearing loss.</title><p>(<bold>a</bold>) ABR traces for two example ears. Each small box shows the average response to a tone at the specified frequency and intensity. The time points (30 ms) in each response that were used for threshold estimation are colored black. The boxes corresponding to intensities below the threshold for each frequency are colored gray. (<bold>b</bold>) ABR thresholds as a function of frequency in normal-hearing (dark blue) and noise-exposed (light blue) animals (each line shows one ear from one animal). The thick red line shows the average ABR threshold shift for noise-exposed animals relative to the mean of all animals with normal hearing. (<bold>c</bold>) Hearing aid gain as a function of frequency for speech at 60 dB SPL with gain and compression parameters fit to the average hearing loss after noise exposure. The values shown are the average across 5 min of continuous speech.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85108-fig2-figsupp1-v2.tif"/></fig></fig-group><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Details of statistical analyses.</title><p>This table provides the details for the statistical analyses in this study, including sampling unit, sample sizes, and p-values. All comparisons were made using Kruskal–Wallis one-way ANOVA with post hoc Tukey–Kramer tests to compute pairwise p-values.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"><xref ref-type="fig" rid="fig2">Figure 2</xref></th><th align="left" valign="bottom"/><th align="left" valign="bottom"/><th align="left" valign="bottom"/><th align="left" valign="bottom"><xref ref-type="fig" rid="fig6">Figure 6</xref></th><th align="left" valign="bottom"/><th align="left" valign="bottom"/><th align="left" valign="bottom"/></tr><tr><th align="char" char="." valign="bottom"><xref ref-type="fig" rid="fig2">Figure 2i</xref></th><th align="left" valign="bottom" colspan="3">Sampling unit: pairs of animals</th><th align="char" char="." valign="bottom"><xref ref-type="fig" rid="fig6">Figure 6e</xref></th><th align="left" valign="bottom" colspan="3">Sampling unit: pairs of animals</th></tr><tr><th align="left" valign="bottom">Groups:</th><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="2">Comparisons:</th><th align="left" valign="bottom">Groups:</th><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="2">Comparisons:</th></tr></thead><tbody><tr><td align="left" valign="bottom" colspan="2">1. NH\NH (n = 15)</td><td align="char" char="." valign="bottom">1 vs. 2</td><td align="left" valign="bottom">p=0.08</td><td align="left" valign="bottom" colspan="2">1. NH\NH (n = 15)</td><td align="char" char="." valign="bottom">1 vs. 2</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="left" valign="bottom" colspan="2">2. HL\HL (n = 15)</td><td align="char" char="." valign="bottom">1 vs. 3</td><td align="left" valign="bottom">p&lt;1e-7</td><td align="left" valign="bottom" colspan="2">2. NH\HL (n = 36)</td><td align="char" char="." valign="bottom">1 vs. 3</td><td align="left" valign="bottom">p=0.007</td></tr><tr><td align="left" valign="bottom" colspan="2">3. NH\HL (n = 36)</td><td align="char" char="." valign="bottom">2 vs. 3</td><td align="left" valign="bottom">p&lt;1e-7</td><td align="left" valign="bottom" colspan="2">3. NH\HL* (n = 36)</td><td align="char" char="." valign="bottom">1 vs. 4</td><td align="left" valign="bottom">p=0.23</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2">4. NH\HA (n = 36)</td><td align="char" char="." valign="bottom">2 vs. 3</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig3">Figure 3</xref></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">2 vs. 4</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">3 vs. 4</td><td align="left" valign="bottom">p=0.29</td></tr><tr><td align="char" char="." valign="bottom"><xref ref-type="fig" rid="fig3">Figure 3e</xref></td><td align="left" valign="bottom" colspan="3"><bold>Sampling unit: pairs of animals</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom"><xref ref-type="fig" rid="fig6">Figure 6f</xref></td><td align="left" valign="bottom" colspan="3"><bold>Sampling unit: pairs of animals</bold></td></tr><tr><td align="left" valign="bottom"><bold>Groups:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2"><bold>Comparisons:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom" colspan="2">1. NH\NH (n = 15)</td><td align="char" char="." valign="bottom">1 vs. 2</td><td align="left" valign="bottom">p=0.71</td><td align="left" valign="bottom"><bold>Groups:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2"><bold>Comparisons:</bold></td></tr><tr><td align="left" valign="bottom" colspan="2">2. HL\HL (n = 15)</td><td align="char" char="." valign="bottom">1 vs. 3</td><td align="left" valign="bottom">p&lt;1e-10</td><td align="left" valign="bottom" colspan="2">1. NH\NH (n = 15)</td><td align="char" char="." valign="bottom">1 vs. 2</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="left" valign="bottom" colspan="2">3. NH\HL (n = 36)</td><td align="char" char="." valign="bottom">2 vs. 3</td><td align="left" valign="bottom">p&lt;1e-10</td><td align="left" valign="bottom" colspan="2">2. NH\HL (n = 36)</td><td align="char" char="." valign="bottom">1 vs. 3</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2">3. NH\HL* (n = 36)</td><td align="char" char="." valign="bottom">1 vs. 4</td><td align="left" valign="bottom">p=0.002</td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig4">Figure 4</xref></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2">4. NH\HA (n = 36)</td><td align="char" char="." valign="bottom">2 vs. 3</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">2 vs. 4</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="char" char="." valign="bottom"><xref ref-type="fig" rid="fig4">Figure 4f</xref></td><td align="left" valign="bottom" colspan="3"><bold>Sampling unit: pairs of animals</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">3 vs. 4</td><td align="left" valign="bottom">p&lt;1e-4</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>Groups:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2"><bold>Comparisons:</bold></td><td align="char" char="." valign="bottom"><xref ref-type="fig" rid="fig6">Figure 6g</xref></td><td align="left" valign="bottom" colspan="3"><bold>Sampling unit: animals</bold></td></tr><tr><td align="left" valign="bottom" colspan="2">1. NH\NH (n = 15)</td><td align="char" char="." valign="bottom">1 vs. 2</td><td align="left" valign="bottom">p&lt;1e-7</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom" colspan="2">2. NH\HL (n = 36)</td><td align="char" char="." valign="bottom">1 vs. 3</td><td align="left" valign="bottom">p&lt;1e-3</td><td align="left" valign="bottom"><bold>Groups:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2"><bold>Comparisons:</bold></td></tr><tr><td align="left" valign="bottom" colspan="2">3. NH\HL* (n = 36)</td><td align="char" char="." valign="bottom">2 vs. 3</td><td align="left" valign="bottom">p&lt;1e-7</td><td align="char" char="." valign="bottom" colspan="2">1. NH (n = 6)</td><td align="char" char="." valign="bottom">1 vs. 2</td><td align="left" valign="bottom">p&lt;1e-6</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom" colspan="2">2. HL (n = 6)</td><td align="char" char="." valign="bottom">1 vs. 3</td><td align="left" valign="bottom">p=0.011</td></tr><tr><td align="char" char="." valign="bottom"><xref ref-type="fig" rid="fig4">Figure 4f</xref></td><td align="left" valign="bottom" colspan="3"><bold>Sampling unit: pairs of animals</bold></td><td align="char" char="." valign="bottom" colspan="2">3. HL* (n = 6)</td><td align="char" char="." valign="bottom">1 vs. 4</td><td align="left" valign="bottom">p=0.057</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2">4. HA (n = 6)</td><td align="char" char="." valign="bottom">2 vs. 3</td><td align="left" valign="bottom">p&lt;1e-3</td></tr><tr><td align="left" valign="bottom"><bold>Groups:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2"><bold>Comparisons:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">2 vs. 4</td><td align="left" valign="bottom">p&lt;1e-4</td></tr><tr><td align="left" valign="bottom" colspan="2">1. NH\NH (n = 15)</td><td align="char" char="." valign="bottom">1 vs. 2</td><td align="left" valign="bottom">p&lt;1e-7</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">3 vs. 4</td><td align="left" valign="bottom">p=0.86</td></tr><tr><td align="left" valign="bottom" colspan="2">2. NH\HL (n = 36)</td><td align="char" char="." valign="bottom">1 vs. 3</td><td align="left" valign="bottom">p&lt;1e-4</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom" colspan="2">3. NH\HL* (n = 36)</td><td align="char" char="." valign="bottom">2 vs. 3</td><td align="left" valign="bottom">p&lt;1e-7</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig7">Figure 7</xref></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig5">Figure 5</xref></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom"><xref ref-type="fig" rid="fig7">Figure 7e</xref></td><td align="left" valign="bottom" colspan="3"><bold>Sampling unit: pairs of animals</bold></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="char" char="." valign="bottom"><xref ref-type="fig" rid="fig5">Figure 5f</xref></td><td align="left" valign="bottom" colspan="3"><bold>Sampling unit: pairs of animals</bold></td><td align="left" valign="bottom"><bold>Groups:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2"><bold>Comparisons:</bold></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2">1. NH\NH (n = 15)</td><td align="char" char="." valign="bottom">1 vs. 2</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="left" valign="bottom"><bold>Groups:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2"><bold>Comparisons:</bold></td><td align="left" valign="bottom" colspan="2">2. NH\HL (n = 36)</td><td align="char" char="." valign="bottom">1 vs. 3</td><td align="left" valign="bottom">p&lt;1e-4</td></tr><tr><td align="left" valign="bottom" colspan="2">1. NH\NH (n = 15)</td><td align="char" char="." valign="bottom">1 vs. 2</td><td align="left" valign="bottom">p&lt;1e-7</td><td align="left" valign="bottom" colspan="2">3. NH\HL* (n = 36)</td><td align="char" char="." valign="bottom">1 vs. 4</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="left" valign="bottom" colspan="2">2. NH\HL (n = 36)</td><td align="char" char="." valign="bottom">1 vs. 3</td><td align="left" valign="bottom">p=0.99</td><td align="left" valign="bottom" colspan="2">4. NH\HA (n = 36)</td><td align="char" char="." valign="bottom">2 vs. 3</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="left" valign="bottom" colspan="2">3. NH\HL* (n = 36)</td><td align="char" char="." valign="bottom">2 vs. 3</td><td align="left" valign="bottom">p&lt;1e-7</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">2 vs. 4</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">3 vs. 4</td><td align="left" valign="bottom">p=0.056</td></tr><tr><td align="char" char="." valign="bottom"><xref ref-type="fig" rid="fig5">Figure 5g</xref></td><td align="left" valign="bottom" colspan="3"><bold>Sampling unit: pairs of animals</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom"><xref ref-type="fig" rid="fig7">Figure 7f</xref></td><td align="left" valign="bottom" colspan="3"><bold>Sampling unit: pairs of animals</bold></td></tr><tr><td align="left" valign="bottom"><bold>Groups:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2"><bold>Comparisons:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom" colspan="2">1. NH\NH (n = 15)</td><td align="char" char="." valign="bottom">1 vs. 2</td><td align="left" valign="bottom">p&lt;1e-7</td><td align="left" valign="bottom"><bold>Groups:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2"><bold>Comparisons:</bold></td></tr><tr><td align="left" valign="bottom" colspan="2">2. NH\HL (n = 36)</td><td align="char" char="." valign="bottom">1 vs. 3</td><td align="left" valign="bottom">p=0.64</td><td align="left" valign="bottom" colspan="2">1. NH\NH (n = 15)</td><td align="char" char="." valign="bottom">1 vs. 2</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="left" valign="bottom" colspan="2">3. NH\HL* (n = 36)</td><td align="char" char="." valign="bottom">2 vs. 3</td><td align="left" valign="bottom">p&lt;1e-7</td><td align="left" valign="bottom" colspan="2">2. NH\HL (n = 36)</td><td align="char" char="." valign="bottom">1 vs. 3</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2">3. NH\HL* (n = 36)</td><td align="char" char="." valign="bottom">1 vs. 4</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="char" char="." valign="bottom"><xref ref-type="fig" rid="fig5">Figure 5j</xref></td><td align="left" valign="bottom" colspan="3"><bold>Sampling unit: pairs of animals</bold></td><td align="left" valign="bottom" colspan="2">4. NH\HA (n = 36)</td><td align="char" char="." valign="bottom">2 vs. 3</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">2 vs. 4</td><td align="left" valign="bottom">p&lt;1e-7</td></tr><tr><td align="left" valign="bottom"><bold>Groups:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2"><bold>Comparisons:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">3 vs. 4</td><td align="left" valign="bottom">p=0.99</td></tr><tr><td align="left" valign="bottom" colspan="2">1. NH\NH (n = 15)</td><td align="char" char="." valign="bottom">1 vs. 2</td><td align="left" valign="bottom">p&lt;1e-7</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom" colspan="2">2. NH\HL (n = 36)</td><td align="char" char="." valign="bottom">1 vs. 3</td><td align="left" valign="bottom">p=0.89</td><td align="char" char="." valign="bottom"><xref ref-type="fig" rid="fig7">Figure 7g</xref></td><td align="left" valign="bottom" colspan="3"><bold>Sampling unit: animals</bold></td></tr><tr><td align="left" valign="bottom" colspan="2">3. NH\HL* (n = 36)</td><td align="char" char="." valign="bottom">2 vs. 3</td><td align="left" valign="bottom">p&lt;1e-7</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><bold>Groups:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2"><bold>Comparisons:</bold></td></tr><tr><td align="char" char="." valign="bottom"><xref ref-type="fig" rid="fig5">Figure 5k</xref></td><td align="left" valign="bottom" colspan="3"><bold>Sampling unit: pairs of animals</bold></td><td align="char" char="." valign="bottom" colspan="2">1. NH (n = 6)</td><td align="char" char="." valign="bottom">1 vs. 2</td><td align="left" valign="bottom">p&lt;1e-9</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom" colspan="2">2. HL (n = 6)</td><td align="char" char="." valign="bottom">1 vs. 3</td><td align="left" valign="bottom">p&lt;1e-5</td></tr><tr><td align="left" valign="bottom"><bold>Groups:</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2"><bold>Comparisons:</bold></td><td align="char" char="." valign="bottom" colspan="2">3. HL* (n = 6)</td><td align="char" char="." valign="bottom">1 vs. 4</td><td align="left" valign="bottom">p&lt;1e-6</td></tr><tr><td align="left" valign="bottom" colspan="2">1. NH\NH (n = 15)</td><td align="char" char="." valign="bottom">1 vs. 2</td><td align="left" valign="bottom">p&lt;1e-7</td><td align="left" valign="bottom" colspan="2">4. HA (n = 6)</td><td align="char" char="." valign="bottom">2 vs. 3</td><td align="left" valign="bottom">p&lt;1e-4</td></tr><tr><td align="left" valign="bottom" colspan="2">2. NH\HL (n = 36)</td><td align="char" char="." valign="bottom">1 vs. 3</td><td align="left" valign="bottom">p=0.97</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">2 vs. 4</td><td align="left" valign="bottom">p&lt;1e-3</td></tr><tr><td align="left" valign="bottom" colspan="2">3. NH\HL* (n = 36)</td><td align="char" char="." valign="bottom">2 vs. 3</td><td align="left" valign="bottom">p&lt;1e-7</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">3 vs. 4</td><td align="left" valign="bottom">p=0.59</td></tr></tbody></table><table-wrap-foot><fn><p>NH: normal hearing; HL: hearing loss; HL*: hearing loss at best intensity; HA: hearing aid.</p></fn></table-wrap-foot></table-wrap><p>To measure the signal variance explained (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, step 3), we compared the same reconstructed activity to the original activity from test trial 2 (when using activity from one trial to reconstruct activity on another, only those features that reliably encode acoustic information across trials can be successfully reconstructed and, thus, only signal variance can be explained). The signal variance explained saturated quickly, indicating that the signal dimensionality was much lower than the overall dimensionality, with only a small number of PCs (between 5 and 10) required to explain 95% of the signal variance in the original activity (<xref ref-type="fig" rid="fig2">Figure 2c</xref>).</p><p>These results suggest that the acoustic information in IC activity is restricted to a low-dimensional subspace, which we term the neural <italic>signal manifold</italic>, and that PCA is able to identify the dimensions that define this manifold. To confirm that PCA preferentially identified the signal manifold, we computed the fraction of the variance explained by each PC that was signal rather than noise (measured as the covariance between the activity on the two test trials when projected onto each PC relative to the overall variance after the same projection) and verified that it decreased with each successive PC (<xref ref-type="fig" rid="fig2">Figure 2d</xref>).</p><p>If the signal manifold reflects something fundamental about auditory processing, then we should expect the activity within it, which we term the <italic>signal dynamics</italic>, to be similar across animals with the same hearing status. To measure the similarity of the signal dynamics across animals (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, step 4), we projected the original activity for each animal onto its respective signal manifold and then determined how much of the variance in the activity from one animal could be explained by the activity from another (allowing for additional linear transformation). We found that the signal dynamics for different animals were remarkably similar, with the signal dynamics from one animal accounting for, on average, 96% of the variance in the signal dynamics from other animals (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). This result gives us confidence that the signal manifold is indeed fundamental, and that our methods are sufficient to identify it robustly in individual animals.</p></sec><sec id="s2-2"><title>Hearing loss distorts neural signal dynamics</title><p>We next sought to use analysis of the signal manifold to better understand the impact of hearing loss on the neural code for speech at the network level. We induced sloping mild-to-moderate sensorineural hearing loss (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) by exposing gerbils (n = 6) to broadband noise using established protocols (<xref ref-type="bibr" rid="bib2">Armstrong et al., 2022</xref>; <xref ref-type="bibr" rid="bib49">Suberman et al., 2011</xref>). After waiting at least 1 mo for the effects of the hearing loss to stabilize, we made neural recordings while presenting the same speech and noise sounds and then performed the same manifold learning.</p><p>The results for animals with hearing loss were similar to those for animals with normal hearing: the overall dimensionality of the neural activity was high (<xref ref-type="fig" rid="fig2">Figure 2f</xref>); the dimensionality of the signal manifold was low (<xref ref-type="fig" rid="fig2">Figure 2g</xref>; between 4 and 7 PCs required to explain 95% of the signal variance); and the signal dynamics were similar across animals (<xref ref-type="fig" rid="fig2">Figure 2h</xref>; 95% average variance explained), demonstrating again that the signal manifold is fundamental and robust. But the similarity between the signal dynamics of normal hearing animals and animals with hearing loss was much lower than that between animals with the same hearing status (<xref ref-type="fig" rid="fig2">Figure 2h and i</xref>; 78% average variance explained). This result indicates that the activity within the signal manifold of an animal with hearing loss is not linearly predictable from the activity within the signal manifold of a normal hearing animal and, thus, that the impact of hearing loss at the network level is a true nonlinear distortion that reshapes the neural code in a complex way.</p></sec><sec id="s2-3"><title>DNNs enable accurate simulation of neural signal dynamics</title><p>To develop an understanding of exactly how hearing loss impacts signal dynamics, further investigation is required. However, traditional approaches to manifold learning such as PCA are limited by the fact that they can only be applied to existing recordings. To overcome this limitation, we designed a DNN that allowed us to identify the signal manifold within the framework of an encoding model that maps sound to neural activity (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). If the DNN can be trained to replicate neural activity with high accuracy for a wide range of sounds, it can then be used to probe the effects of hearing loss on signal dynamics using new sounds as needed.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Neural signal dynamics identified via deep learning.</title><p>(<bold>a</bold>) Schematic diagram of the deep neural network (DNN) used to predict inferior colliculus (IC) activity. (<bold>b</bold>) Example images of neural activity elicited by a speech syllable from an example recording (left), reconstructed as in step 2b of <xref ref-type="fig" rid="fig2">Figure 2</xref> (center), and predicted by the DNN (right). (<bold>c</bold>) Left: predictive power of the DNN for each unit from one animal. The value on the horizontal axis is the fraction of the variance in the neural activity that is explainable (i.e., that is consistent across trials of identical speech). The value on the vertical axis is the percent of this explainable variance that was captured by the DNN. Analogous values for linear–nonlinear (LN) models trained and tested on the same activity are shown for comparison. Right: average predictive power for all units as a function of the number of channels in the bottleneck layer. Each line shows the results for one animal. (<bold>d</bold>) Fraction of the variance in one set of bottleneck activations explained by another set for each pair of normal hearing animals (left), each pair of animals with hearing loss (center), and each pair of animals with different hearing status (right). (<bold>e</bold>) Distributions of variance in one set of bottleneck activations explained by another set for each pair of normal hearing animals (n = 15), each pair of animals with hearing loss (n = 15), and each pair of animals with different hearing status (n = 36). Median values were compared via Kruskal–Wallis one-way ANOVA and Tukey–Kramer post hoc tests, ***p&lt;0.001, **p&lt;0.01, *p&lt;0.05, ns indicates not significant. For full details of statistical tests, see <xref ref-type="table" rid="table1">Table 1</xref>. (<bold>f</bold>) Top: example images of neural activity elicited by pure tones from an example recording (left) and predicted by the DNN (right). Each subplot shows the frequency response area for one unit (the average activity recorded during the presentation of tones with different frequencies and sound levels). The colormap for each plot is normalized to the minimum and maximum activity level across all frequencies and intensities. Bottom: predictive power of the DNN for tone responses across all frequencies at two different intensities. Each point indicates the average percent of the explainable variance that was captured by the DNN for all units from one animal (with horizontal jitter added for visibility). (<bold>g</bold>) Top left: schematic diagram of transfer learning for new animals. Bottom: example images of neural activity elicited by sinusoidally amplitude modulated (SAM) noise with two different modulation frequencies from an example recording (left; average over 128 repeated trials) and predicted by the DNN (right). Top right: predictive power of the DNN for SAM noise responses across all modulation frequencies and modulation depths at two different intensities. Each point indicates the average percent of the explainable variance that was captured by the DNN for all units from one of four new animals after transfer learning using a frozen encoder from one of six animals in the original training set.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85108-fig3-v2.tif"/></fig><p>The DNN first projects sound into a high-dimensional feature space using an encoder with a cascade of convolutional layers. It then reduces the dimensionality of its feature space through a bottleneck layer and uses a simple (i.e., not convolutional) linear readout to transform the activations in the bottleneck into the neural activity for each recorded unit. During training, the DNN learns to use the bottleneck to identify the low-dimensional feature space that captures as much of the explainable variance in the recorded neural activity as possible, that is, the signal manifold. (Note that the structure of the DNN is not meant to reflect the anatomy of the auditory system; it is simply a tool for identifying latent dynamics and predicting neural activity.)</p><p>Once trained, the DNN can be used to simulate neural activity for any sound, whether it was presented during neural recordings or not, with the activations in the bottleneck layer reflecting the underlying signal dynamics. This supervised approach to identifying the signal manifold also has the added advantage that it eliminates the residual noise that is inevitable with unsupervised methods such as PCA (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). (See decreasing SNR with successive PCs in <xref ref-type="fig" rid="fig2">Figure 2d</xref>.).</p><p>The utility of the DNN rests, of course, on its ability to faithfully reproduce the recorded neural activity. We trained and tested a separate DNN for each animal (after partitioning the recordings into training and test sets as described above) and found that they performed with remarkable accuracy. The explainable variance explained for activity in the test set approached 100% for the units with highest explainable variance and was far beyond that achieved by a standard single-layer linear–nonlinear model (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). We varied the size of the bottleneck layer and found that performance plateaued with more than eight channels for both normal hearing animals and those with hearing loss, consistent with the dimensionality of the signal manifold identified through PCA (<xref ref-type="fig" rid="fig3">Figure 3c</xref>).</p><p>We also assessed the similarity of the DNN-derived signal dynamics across animals by measuring how much of the variance in the bottleneck activations from one animal could be explained by the bottleneck activations from another (<xref ref-type="fig" rid="fig3">Figure 3d and e</xref>; allowing for additional linear transformation). We found that the signal dynamics for animals with the same hearing status were nearly identical (97% average variance explained for both normal hearing and hearing loss), while the similarity between the signal dynamics for animals with normal hearing and those with hearing loss was much lower (87% average variance explained). Thus, the signal manifold as identified by the DNN appears to have similar properties to that identified through PCA, with the added advantages of reduced residual noise and the ability to probe the signal dynamics using novel sounds.</p><p>To examine the degree to which the DNNs trained on speech were capable of predicting responses to other sounds, we compared recorded and DNN-generated responses to pure tones with different frequencies and intensities (<xref ref-type="fig" rid="fig3">Figure 3f</xref>). The DNNs performed well, explaining an average of 83% of the explainable variance in the recorded activity across animals. To further test the generality of the DNN models, we used transfer learning to test their ability to predict responses to new sounds for a new set of animals. If the DNN encoder really does capture transformations that are common to all animals with the same hearing status, then it should be possible to use a trained encoder from one animal to predict responses for a new animal after learning only a new linear readout (<xref ref-type="fig" rid="fig3">Figure 3g</xref>). For each of the DNN models trained on activity from one of the six normal hearing animals in our original dataset, we froze the encoder and retrained the linear readout for each of four new normal hearing animals. We initialized the readout weights for each unit in a new animal using the readout weights for a random unit from the original animal, and then optimized the weights using a relatively small sample (between 2 and 3.5 hr) of activity recorded from the new animal during the presentation of speech and moving ripples. We then used the new DNN model (the frozen encoder and the optimized readout) to predict responses from the new animal to sinusoidally amplitude modulated (SAM) broadband noise sounds with different modulation frequencies, modulation depths, and intensities. The new DNN models performed well, explaining an average of 85% of the explainable variance in the recorded activity across animals. While pure tones and SAM noise are only two of many possible sounds, these results provide encouraging evidence of the generality of the DNN models.</p></sec><sec id="s2-4"><title>Hearing loss distorts spectral processing</title><p>Before continuing our investigation of the neural coding of speech, we first used the DNN to examine the impact of hearing loss on the processing of basic acoustic features. To assess spectral processing, we presented the DNN for each animal with a stream of pure tones with different frequencies and intensities and extracted the activations from the bottleneck layer (<xref ref-type="fig" rid="fig4">Figure 4a</xref>; we set the dimensionality of the bottleneck layer to 8 for this and all subsequent analyses). The frequency response areas (FRAs) for individual bottleneck channels resembled those that are typically observed for individual neurons in the IC: some exhibited a clear preferred frequency at low intensities and broader tuning at high intensities, while others had more complex shapes (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). For animals with hearing loss, elevated intensity thresholds were also evident.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Neural signal dynamics for pure tones.</title><p>(<bold>a</bold>) Schematic diagram showing pure tone sounds with different frequencies and intensities and corresponding bottleneck activations. (<bold>b</bold>) Frequency response areas (FRAs) for the eight bottleneck channels from a normal hearing animal (top) and an animal with hearing loss (bottom). Each subplot shows the average activity for one channel in response to tones with different frequencies and intensities. The colormap for each plot is normalized to the minimum and maximum activity level across all frequencies and intensities. (<bold>c</bold>) Dimensionality reduction of bottleneck activations via principal component analysis (PCA). Each line shows the variance explained by the top two principal components (PCs) for one animal as a function of the intensity of the tones. (<bold>d</bold>) Signal dynamics for pure tones for a normal hearing animal (left) and an animal with hearing loss (right). The top two rows show the projection of the bottleneck activations onto each of the top two PCs as a function of time. The inset value indicates the percent of the variance in the bottleneck activations explained by each PC. The bottom row shows the projections from the top two rows plotted against one another. Each line shows the dynamics for a different tone frequency. Each column shows the dynamics for a different tone intensity. (<bold>e</bold>) Representational dissimilarity matrices (RDMs) computed from bottleneck activations. The left image shows the average RDM for normal hearing animals for tones at 55 dB SPL. The value of each pixel is proportional to the point-by-point correlation between the activations for a pair of tones with different frequencies. The center image shows the same lower half of the RDM for normal hearing animals along with the upper half of the RDM for animals with hearing loss at the same intensity. The right image shows the same lower half of the RDM for normal hearing animals along with the upper half of the RDM for animals with hearing loss at the best intensity (that which produced the highest point-by-point correlation between the normal hearing and hearing loss RDMs). (<bold>f</bold>) The point-by-point correlation between RDMs for each pair of normal hearing animals (n = 15), and each pair of animals with different hearing status compared at either the same intensity or the best intensity (n = 36). Median values were compared via Kruskal–Wallis one-way ANOVA and Tukey–Kramer post hoc tests, ***p&lt;0.001, **p&lt;0.01, * p&lt;0.05, ns indicates not significant. For full details of statistical tests, see <xref ref-type="table" rid="table1">Table 1</xref>. (<bold>g</bold>) Average signal dynamics for pure tones for normal hearing animals (left) and animals with hearing loss (right) after alignment via multiway canonical correlation analysis (MCCA). (<bold>h</bold>) The similarity between dynamics after alignment via pairwise canonical correlation analysis (CCA) (see ‘Methods’) for each pair of normal hearing animals, and each pair of animals with different hearing status compared at either the same intensity or the best intensity (that which produced the highest similarity between the dynamics).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85108-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Distorted spectral processing in the inferior colliculus (IC) and auditory nerve (AN).</title><p>Representational dissimilarity matrices (RDMs) for simulated and recorded IC responses to tones display a simple block-like structure after hearing loss, indicating a clustering of response trajectories for different tones within the signal manifold. The effects of hearing loss on tone responses at the level of the AN appear to be much more complex and differ between full responses and response envelopes (i.e., with and without phase locking to tone fine structure). Methodological details are provided below. (<bold>a</bold>) RDMs computed from bottleneck activations. The left image shows the average RDM for normal hearing animals for tones at 55 dB SPL. The value of each pixel is proportional to the point-by-point correlation between the activations for a pair of tones with different frequencies. The center image shows the same lower half of the RDM for normal hearing animals along with the upper half of the RDM for animals with hearing loss at the same intensity. The right image shows the same lower half of the RDM for normal hearing animals along with the upper half of the RDM for animals with hearing loss at the best intensity (that which produced the highest point-by-point correlation between the normal hearing and hearing loss RDMs). Reproduced from <xref ref-type="fig" rid="fig4">Figure 4</xref> for reference. (<bold>b</bold>) RDMs computed from original IC activity. The left image shows the average RDM for normal hearing animals for tones at 60 dB SPL. The center image shows the same lower half of the RDM for normal hearing animals along with the upper half of the RDM for animals with hearing loss at the same intensity. The right image shows the same lower half of the RDM for normal hearing animals along with the upper half of the RDM for animals with hearing loss at 85 dB SPL. The tones were 50 ms in duration with frequencies ranging from 500 Hz to 8000 Hz in 0.5 octave steps with 5 ms cosine on and off ramps. Tones were presented 128 times each in random order with 175 ms between presentations. IC multi-unit activity (MUA) was averaged across presentations of each tone. Principal component analysis (PCA) was performed and RDMs were computed using the top 3 principal components (PCs), which explained more than 90% of the variance in all cases. (<bold>c, d</bold>) RDMs computed from simulated AN activity. The left image shows the average RDM for normal hearing animals for tones at 60 dB SPL. The center image shows the same lower half of the RDM for normal hearing animals along with the upper half of the RDM for animals with hearing loss at the same intensity. The right image shows the same lower half of the RDM for normal hearing animals along with the upper half of the RDM for animals with hearing loss at the best intensity (that which produced the highest point-by-point correlation between the normal hearing and hearing loss RDMs). The tones were the same as those presented to the IC deep neural network (DNN) models: 100 ms in duration with frequencies ranging from 500 Hz to 8000 Hz in 0.2 octave steps; intensities ranging from 25 dB SPL to 100 dB SPL in 5 dB steps; 10 ms cosine on and off ramps; and a 100 ms pause between tones. AN responses were simulated using the model of Bruce et al. (2018) with default parameters at 48 CFs ranging from 200 Hz to 10 kHz. To simulate mild-to-moderate sloping sensorineural hearing loss, we modified the parameter controlling outer hair cell function (<inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) as needed to create a threshold shift ranging from 20 dB at 1 kHz to 40 dB at 8 kHz. Responses to low-, medium-, and high-threshold fibers were simulated and summed together to create MUA in each CF channel. PCA was performed and RDMs were computed using the top 8 PCs, which explained more than 90% of the variance in all cases. The envelope of the AN responses was extracted using a Hilbert transform (MATLAB <italic>envelope</italic>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85108-fig4-figsupp1-v2.tif"/></fig></fig-group><p>To visualize the signal dynamics, we applied PCA to the bottleneck activations (for each intensity separately) and projected the full dynamics onto the top two PCs, which explained more than 90% of the variance for these simple sounds (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). For normal hearing animals, the paths traced by the dynamics within the signal manifold for different sounds, which we term <italic>trajectories</italic>, were distinct and formed an orderly arrangement, but with a clear change in geometry across intensities (<xref ref-type="fig" rid="fig4">Figure 4d</xref>). At low intensities, the trajectories for different frequencies were distinct across both PCs, each of which accounted for substantial variance (the percent of the variance in the signal dynamics explained by each PC at each intensity is indicated on each panel). But at high intensities, the trajectories for all frequencies were similar along the first PC, which accounted for the vast majority of the variance, and varied only along the second PC. These intensity-dependent changes in the geometry of the signal dynamics are consistent with the known effects of intensity on spectral tuning in IC neurons. At low intensities, tuning is narrow and, thus, different tone frequencies elicit distinct population activity patterns. But at high intensities, because tuning is broader, the population activity patterns elicited by different frequencies are less distinct.</p><p>For animals with hearing loss, the signal dynamics were dramatically different. At moderate intensity, only the lowest frequencies elicited any activation (as expected, given the larger elevation in intensity thresholds at high frequencies; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). At higher intensity, all tones elicited activation, but rather than forming an orderly arrangement within the manifold, the trajectories for different frequencies clustered into two groups, one for low frequencies and one for high frequencies. At the highest intensity, the trajectories for different frequencies became more distinct, but clear clustering remained. The increased similarity in the signal trajectories for different frequencies is consistent with the increased similarity of spectral tuning between individual IC neurons in animals with hearing loss (<xref ref-type="bibr" rid="bib4">Barsz et al., 2007</xref>; <xref ref-type="bibr" rid="bib56">Willott, 1986</xref>), but the clustering of dynamics within the signal manifold is an emergent network-level phenomenon that has not previously been observed.</p><p>To further analyze the dynamics and allow for direct comparisons across animals, we first turned to representational similarity analysis (RSA) (<xref ref-type="bibr" rid="bib26">Kriegeskorte et al., 2008</xref>). RSA uses only the relative distances between trajectories within a manifold and, thus, does not require the dynamics for different animals to be aligned. The first step in RSA is to form a representational dissimilarity matrix (RDM) for each set of trajectories, with each entry in the RDM equal to one minus the point-by-point correlation between a pair of trajectories for two different sounds (<xref ref-type="fig" rid="fig4">Figure 4e</xref>, left).</p><p>The structure of the RDMs was consistent with the observations about the dynamics made above. For normal hearing animals, the RDMs had a diagonal symmetry with a smooth gradient, indicating that the similarity of the trajectories for two frequencies decreased gradually as the difference between the frequencies increased. For animals with hearing loss, the RDMs had a block structure, indicating that the trajectories formed two clusters (<xref ref-type="fig" rid="fig4">Figure 4e</xref>, center; note that RDMs are symmetric, so the lower half of the normal hearing RDM is shown with the upper half of the hearing loss RDM for comparison).</p><p>Because we were interested in identifying the effects of hearing loss beyond those related to audibility, we also compared the normal hearing RDMs at a given intensity to the hearing loss RDMs at the <italic>best intensity,</italic> that is, at whatever intensity resulted in the highest similarity to the normal hearing RDM for each pair of animals (measured as the point-by-point correlation between the RDMs). Amplification to the best intensity softened the block structure and shifted the transition between clusters to a lower frequency, but did not fully restore the diagonal structure present in the normal hearing RDMs (<xref ref-type="fig" rid="fig4">Figure 4e</xref>, right). Overall, the similarity between the RDMs for different normal hearing animals at moderate intensity (55 dB SPL) was high (0.91 ± 0.01; mean ± SEM; n = 15 pairwise comparisons; <xref ref-type="fig" rid="fig4">Figure 4f</xref>; for full details of all statistical tests, see <xref ref-type="table" rid="table1">Table 1</xref>). The similarity between the normal hearing and hearing loss RDMs at the same moderate intensity was much lower (0.59 ± 0.02; n = 36) and remained relatively low even at the best intensity (0.78 ± 0.02; n = 36).</p><p>The results of RSA are easy to interpret, but, because it uses only the relative distances between trajectories, it can be insensitive to distortions that impact the overall structure of the dynamics (e.g., a change in temporal dynamics that is common across all sound frequencies). To allow for direct comparisons of overall structure, we used canonical correlation analysis (CCA) (<xref ref-type="bibr" rid="bib11">Dabagia et al., 2022</xref>). CCA identifies a series of linear projections, known as canonical components (CCs), that attempt to align two sets of dynamics such that the point-by-point correlation between trajectories after projection onto their respective CCs is maximized. The set of CCs for a given set of dynamics are required to be orthogonal to each other and to explain all of the variance in the original trajectories. CCA can also be extended to simultaneously align dynamics for an entire group of animals through multiway canonical correlation analysis (MCCA) (<xref ref-type="bibr" rid="bib12">de Cheveigné et al., 2019</xref>).</p><p>The average dynamics after alignment via CCA exhibited phenomena that were similar to those that were evident for individual animals (<xref ref-type="fig" rid="fig4">Figure 4g</xref>). For normal hearing animals, the trajectories for different tone frequencies were distinct and formed an orderly arrangement with frequency-dependent variation across two dimensions at low intensities, while at high intensities the variation across frequencies was largely confined to the second CC. For animals with hearing loss, the trajectories for different frequencies clustered into two groups at moderate intensities and remained clustered, albeit more weakly, at high intensities. This clustering was also evident when a similar analysis was performed directly on recorded neural activity (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>To measure the similarity between the dynamics for different animals after alignment via CCA, we used a weighted sum of the point-by-point correlations between the two sets of dynamics after projection onto each pair of CCs, with the weight for the correlation associated with each pair of CCs given by the average variance in the original dynamics that those CCs explained (see ‘Methods’ for equation). Overall, the similarity between the dynamics for different normal hearing animals at moderate intensity after alignment via CCA was extremely high (0.98 ± 0.01; n = 15; <xref ref-type="fig" rid="fig4">Figure 4h</xref>). The similarity between the aligned dynamics for normal hearing and hearing loss animals at the same moderate intensity was much lower (0.37 ± 0.02; n = 36) and remained below normal even when compared at the best intensity (0.88 ± 0.01; n = 36). Taken together, the RSA and CCA results suggest that hearing loss results in a fundamental disruption of spectral processing at the network level.</p></sec><sec id="s2-5"><title>Hearing loss does not distort temporal processing</title><p>We next assessed temporal processing by performing a similar analysis on the bottleneck activations elicited by a stream of SAM broadband noise sounds with different modulation frequencies and intensities (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). For these sounds, two dimensions were again enough to capture almost all of the variance in the full signal dynamics across all intensities (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). For both normal hearing animals and those with hearing loss, the explicit tracking of envelope modulations in the signal dynamics decreased with increasing modulation frequency and increasing intensity (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). But when compared at the same intensity, the dynamics for animals with hearing loss clearly differed from those for animals with normal hearing (<xref ref-type="fig" rid="fig5">Figure 5d and e</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Neural signal dynamics for amplitude modulated noise.</title><p>(<bold>a</bold>) Schematic diagram showing amplitude-modulated noise sounds with different intensities and modulation frequencies. (<bold>b</bold>) Variance in bottleneck activations explained by top two principal components (PCs) for each animal as function of sound intensity. (<bold>c</bold>) Envelope tracking in signal dynamics measured as the coefficient of variation of the bottleneck activations for sounds with different modulation frequencies at an intensity of 55 dB SPL (left) and sounds at different intensities with a modulation frequency of 100 Hz (right). Values shown are the average across animals. (<bold>d</bold>) Signal dynamics for a normal hearing animal (left) and an animal with hearing loss (right). Each line shows the dynamics for a different modulation frequency. (<bold>e</bold>) Signal dynamics for a modulation frequency of 100 Hz after projection onto the top PC. The top panel shows the dynamics for a normal hearing animal and an animal with hearing loss at 55 dB SPL. The bottom panel shows the same dynamics for the normal hearing animal along with the dynamics for the animal with hearing loss at the best intensity. (<bold>f</bold>) Representational dissimilarity matrices (RDMs) computed from bottleneck activations and the point-by-point correlation between RDMs for different pairs of animals at 55 dB SPL or best intensity. Median values were compared via Kruskal–Wallis one-way ANOVA and Tukey–Kramer post hoc tests, ***p&lt;0.001, **p&lt;0.01, *p&lt;0.05, ns indicates not significant. For full details of statistical tests, see <xref ref-type="table" rid="table1">Table 1</xref>. (<bold>g</bold>) The similarity between dynamics after alignment via pairwise canonical correlation analysis (CCA) for different pairs of animals at 55 dB SPL or best intensity. (<bold>h</bold>) Schematic diagram showing amplitude modulated noise sounds with different intensities and modulation depths. (<bold>i</bold>) Signal dynamics for a normal hearing animal at 55 dB SPL (left) and an animal with hearing loss at the best intensity (right). Each line shows the dynamics for a different modulation depth. (<bold>j</bold>) The point-by-point correlation between RDMs for different pairs of animals at 55 dB SPL or best intensity. (<bold>k</bold>) The similarity between dynamics after alignment via pairwise CCA for different pairs of animals at 55 dB SPL or best intensity.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85108-fig5-v2.tif"/></fig><p>This was confirmed by RSA (<xref ref-type="fig" rid="fig5">Figure 5f</xref>), which indicated that while the similarity between normal hearing RDMs at moderate intensity (55 dB SPL) was extremely high (0.99 ± 0.01; n = 15), the similarity between normal hearing and hearing loss RDMs was lower (0.76 ± 0.01; n = 36). But when compared at the best intensity to eliminate differences related to audibility, the dynamics for animals with hearing loss became nearly identical to those for animals with normal hearing (<xref ref-type="fig" rid="fig5">Figure 5e</xref>), and this was reflected in the similarity between RDMs (0.99 ± 0.01; n = 36).</p><p>Comparing the similarity of the dynamics after alignment via CCA yielded similar results (<xref ref-type="fig" rid="fig5">Figure 5g</xref>). The similarity between the dynamics for different normal hearing animals at moderate intensity after alignment via CCA was high (0.97 ± 0.01; n = 15). The similarity between the aligned dynamics for normal hearing and hearing loss animals was much lower when compared at the same moderate intensity (0.44 ± 0.02; n = 36) but increased to normal levels when the comparison was made at the best intensity (0.95 ± 0.01; n = 36). Thus, it appears that hearing loss has little impact on temporal processing beyond that which results from decreased audibility.</p><p>We verified that this was also true for the processing of sounds with different modulation depths. We performed the same analysis on the bottleneck activations elicited by a stream of SAM noise sounds with different modulation depths and intensities (and a fixed modulation frequency of 30 Hz; <xref ref-type="fig" rid="fig5">Figure 5h</xref>). When compared at the best intensity, the signal dynamics for normal hearing animals and animals with hearing loss were nearly identical (<xref ref-type="fig" rid="fig5">Figure 5i</xref>), with the explicit tracking of envelope modulations decreasing with decreasing modulation depth. The overall similarity measured at the best intensity both by RSA (0.99 ± 0.01; n = 36; <xref ref-type="fig" rid="fig5">Figure 5j</xref>) and after alignment via CCA (0.96 ± 0.01; n = 36; <xref ref-type="fig" rid="fig5">Figure 5k</xref>) confirmed that the impact of hearing loss on temporal processing beyond that which results from decreased audibility was negligible.</p></sec><sec id="s2-6"><title>Distortions in the neural code for speech in quiet are largely corrected by amplification</title><p>Having established that the distortions in neural signal dynamics caused by hearing loss affect primarily spectral, rather than temporal, processing for simple sounds, we next returned to speech. We focused on consonants, which vary widely in their spectral properties and are the primary contributor to the perceptual deficits exhibited by people with hearing loss when listening to ongoing speech (<xref ref-type="bibr" rid="bib14">Fogerty et al., 2012</xref>). We presented the DNN with a stream of isolated consonants (diphone syllables with the vowel removed), each uttered multiple times by multiple talkers (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). The consonants can be divided into three broad groups: the vowel-like consonants (nasals and approximants), which are dominated by low frequencies; the plosives, which are broadband; and the fricatives, which are dominated by high frequencies (<xref ref-type="fig" rid="fig6">Figure 6b</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Neural signal dynamics for speech in quiet.</title><p>(<bold>a</bold>) Schematic diagram showing different consonants. (<bold>b</bold>) Average power spectra for three classes of consonants. The individual consonants are shown in the inset in panel (<bold>d</bold>). (<bold>c</bold>) Variance in bottleneck activations explained by top two principal components (PCs) for each animal as a function of sound intensity. (<bold>d</bold>) Signal dynamics for a normal hearing animal (left) and an animal with hearing loss (right). Each line shows the dynamics for a different consonant, averaged over all instances. (<bold>e</bold>) Representational dissimilarity matrices (RDMs) computed from bottleneck activations and the point-by-point correlation between RDMs for different pairs of animals at 60 dB SPL, best intensity, or with a hearing aid. Median values were compared via Kruskal–Wallis one-way ANOVA and Tukey–Kramer post hoc tests, ***p&lt;0.001, **p&lt;0.01, *p&lt;0.05, ns indicates not significant. For full details of statistical tests, see <xref ref-type="table" rid="table1">Table 1</xref>. (<bold>f</bold>) The similarity between dynamics after alignment via pairwise canonical correlation analysis (CCA) for different pairs of animals at 60 dB SPL, best intensity, or with a hearing aid. (<bold>g</bold>) Performance of a support vector machine classifier trained to identify consonants based on bottleneck activations for each normal hearing animal (n = 6) at 60 dB SPL and each animal with hearing loss (n = 6) at either 60 dB SPL, best intensity, or with a hearing aid.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85108-fig6-v2.tif"/></fig><p>For both normal hearing animals and animals with hearing loss, two dimensions were again sufficient to explain nearly all of the variance in the bottleneck activations (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). For normal hearing animals, the dynamics elicited by different consonants followed distinct trajectories that were organized within the signal manifold according to consonant type (<xref ref-type="fig" rid="fig6">Figure 6d</xref>; the dynamics shown are the average across all instances of each consonant). For animals with hearing loss, only the vowel-like consonants elicited responses at moderate intensity (as expected, given the larger elevation in intensity thresholds at high frequencies; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). At higher intensities, all consonants elicited responses but the trajectories were not as distinct as with normal hearing and exhibited a clustering similar to that observed for pure tones (<xref ref-type="fig" rid="fig4">Figure 4d</xref>), which softened at the highest intensity.</p><p>The differences in the dynamics for normal hearing animals and those with hearing loss were evident in the RDMs (<xref ref-type="fig" rid="fig6">Figure 6e</xref>). When compared at a typical conversational intensity (60 dB SPL), the similarity between normal hearing RDMs was high (0.94 ± 0.01; n = 15), but the similarity between normal hearing and hearing loss RDMs was low (0.23 ± 0.02; n = 36). The normal hearing and hearing loss RDMs were much more similar when compared at the best intensity, though some differences remained (0.87 ± 0.01; n = 36).</p><p>Comparing the similarity of the dynamics after alignment via CCA yielded similar results (<xref ref-type="fig" rid="fig6">Figure 6f</xref>). The similarity between the dynamics for different normal hearing animals after alignment via CCA was high (0.96 ± 0.01; n = 15). The similarity between normal hearing and hearing loss animals when compared at the same conversational intensity was much lower (0.31 ± 0.02; n = 36) and increased when the comparison was made at the best intensity, but not to normal levels (0.77 ± 0.01; n = 36).</p><p>Given that hearing loss seems to impact primarily spectral processing, we investigated whether the distortions in the neural code for speech could be corrected by providing frequency-dependent amplification using a simulated hearing aid (<xref ref-type="bibr" rid="bib2">Armstrong et al., 2022</xref>; <xref ref-type="bibr" rid="bib1">Alexander and Masterson, 2015</xref>). We used the measured ABR threshold shifts for each animal to set the parameters for the amplification, which resulted in a gain of approximately 10 dB at low frequencies and 30 dB at high frequencies for speech at conversational intensity (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), and presented the same stream of consonants again after processing with the hearing aid. The frequency-dependent amplification was effective in reducing the distortion in the dynamics for animals with hearing loss. The overall similarity between normal hearing and hearing loss animals as measured by RSA was restored to normal levels (0.91 ± 0.01; n = 36; <xref ref-type="fig" rid="fig6">Figure 6e</xref>), and the similarity measured after alignment via CCA was also increased, though some residual distortion remained (0.86 ± 0.01; n = 36; <xref ref-type="fig" rid="fig6">Figure 6f</xref>). (Note that we did not record neural activity in response to speech through the simulated hearing aid; while we have no specific reason to doubt the accuracy of the DNN model for this class of sounds, the fact that it has not been explicitly validated should be considered.)</p><p>To evaluate the functional consequences of the remaining distortion, we turned to decoding. We trained a support vector machine to classify consonants based on the signal dynamics for each animal. For normal hearing animals, the decoder identified 55% of consonants correctly (±1%; n = 6; chance = 4.5%) when the consonants were presented at conversational intensity (<xref ref-type="fig" rid="fig6">Figure 6g</xref>). For animals with hearing loss, performance at the same intensity was lower (41 ± 1%; n = 6) but increased substantially at best intensity (47 ± 1%; n = 6), and increased further still with frequency-dependent amplification by the hearing aid (49 ± 1%; n = 6). Taken together, these results suggest that while amplification cannot completely restore the neural code for speech in quiet to normal, the residual distortions are relatively minor.</p></sec><sec id="s2-7"><title>Distortions in the neural code for speech in noise persist even after frequency-weighted amplification</title><p>Given that the perceptual difficulties experienced by listeners with hearing loss are most pronounced in noisy environments, we expected that the addition of background noise to the speech would create larger distortions in the neural code. We presented the same consonant stream with added speech babble (background noise formed by adding together the voices of many different talkers; <xref ref-type="fig" rid="fig7">Figure 7a and b</xref>) at a speech-to-noise ratio of 3 dB, which is typical of real-world settings experienced by hearing aid users (<xref ref-type="bibr" rid="bib8">Christensen et al., 2021</xref>). The addition of background noise increased the dimensionality of the signal dynamics relative to simple sounds or speech in quiet, especially at high overall intensities; three PCs were often required to capture more than 90% of the variance (<xref ref-type="fig" rid="fig7">Figure 7c</xref>). (Note that both the speech and the background noise contribute to the signal dynamics, which encode all incoming sounds without distinction.)</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Neural signal dynamics for speech in noise.</title><p>(<bold>a</bold>) Schematic diagram showing different consonants in speech babble. (<bold>b</bold>) Average power spectra for three classes of consonants and speech babble. (<bold>c</bold>) Variance in bottleneck activations explained by top two (left) or three (right) principal components (PCs) for each animal as a function of sound intensity. (<bold>d</bold>) Signal dynamics for a normal hearing animal (top) and an animal with hearing loss (bottom). Each line shows the dynamics for a different consonant, averaged over all instances. Dynamics for speech in quiet are shown alongside those for speech in noise for comparison. (<bold>e</bold>) Point-by-point correlation between representational dissimilarity matrices (RDMs) for different pairs of animals at 70 dB SPL, best intensity, or with a hearing aid. Median values were compared via Kruskal–Wallis one-way ANOVA and Tukey–Kramer post hoc tests, ***p&lt;0.001, ** p&lt;0.01, *p&lt;0.05, ns indicates not significant. For full details of statistical tests, see <xref ref-type="table" rid="table1">Table 1</xref>. (<bold>f</bold>) The similarity between dynamics after alignment via pairwise canonical correlation analysis (CCA) for different pairs of animals at 70 dB SPL, best intensity, or with a hearing aid. (<bold>g</bold>) Performance of a support vector machine classifier trained to identify consonants based on bottleneck activations for each normal hearing animal at 70 dB SPL, and each animal with hearing loss at 70 dB SPL, best intensity, or with a hearing aid.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85108-fig7-v2.tif"/></fig><p>For normal hearing animals, the signal dynamics for speech-in-noise and speech-in-quiet were nearly identical at the lowest intensities, but differed strongly at higher intensities (<xref ref-type="fig" rid="fig7">Figure 7d</xref>; the dynamics shown are the average across all instances of each consonant). For speech-in-noise at the highest intensity, there was a clear distinction between the first PC, which provided a clean reflection of each consonant (though not the same as for speech in quiet), and the other PCs, which were dominated by the background noise (despite averaging across all instances of each consonant with independent noise). The trends were similar for animals with hearing loss, though the background noise was reflected in the signal dynamics even more strongly.</p><p>When compared at the same high intensity (70 dB SPL), typical of a social setting, both RSA and CCA indicated strong effects of hearing loss on the signal dynamics for speech in noise (<xref ref-type="fig" rid="fig7">Figure 7e and f</xref>). The similarity between normal hearing RDMs was high (0.89 ± 0.02; n = 15), but the similarity between normal hearing and hearing loss RDMs was much lower (0.15 ± 0.03; n = 36). Amplification to best intensity increased the similarity between normal hearing and hearing loss RDMs (0.62 ± 0.02; n = 36), as did the frequency-weighted amplification provided by the hearing aid (0.56 ± 0.03; n = 36), but neither was sufficient to bring the similarity close to normal levels. For both forms of amplification, the similarity of the signal dynamics for speech in noise to those with normal hearing was much lower than for speech in quiet (best intensity: 0.62 vs. 0.87, n = 36; p&lt;1e-10, paired <italic>t</italic>-test; hearing aid: 0.56 vs. 0.91, n = 36, p&lt;1e-10, paired <italic>t</italic>-test). Comparing the similarity of the dynamics after alignment via CCA yielded similar results. The similarity between the dynamics for different normal hearing animals after alignment via CCA was high (0.92 ± 0.01; n = 15). The similarity between normal hearing and hearing loss animals when compared at the same intensity was much lower (0.49 ± 0.02; n = 36) and increased when the comparison was made at the best intensity (0.68 ± 0.01; n = 36) or after processing with the hearing aid (0.70 ± 0.02; n = 36), but remained well below normal levels. Again, for both forms of amplification, the similarity of the signal dynamics for speech in noise to those with normal hearing was much lower than for speech in quiet (best intensity: 0.68 vs. 0.77, n = 36; p&lt;1e-6, paired <italic>t</italic>-test; hearing aid: 0.70 vs. 0.86, n = 36, p&lt;1e-10, paired <italic>t</italic>-test).</p><p>Decoding the signal dynamics for each animal suggested that the distortions in the signal dynamics for speech in noise had functional consequences (<xref ref-type="fig" rid="fig7">Figure 7g</xref>). For normal hearing animals, the decoder identified 32% of consonants correctly (±1%; n = 6). For animals with hearing loss, performance at the same intensity was lower (15 ± 1%; n = 6) and remained well below normal levels both at the best intensity (23 ± 1%; n = 6) or after processing with the hearing aid (22 ± 1%; n = 6).</p></sec><sec id="s2-8"><title>Hearing loss causes hypersensitivity to background noise</title><p>To gain a better understanding of the differential impact of the background noise with and without hearing loss, we used MCCA to jointly align the signal dynamics for all animals with normal hearing and hearing loss so that we could make direct comparisons. We first analyzed the results for speech in quiet. When compared at best intensity (<xref ref-type="fig" rid="fig8">Figure 8a</xref>), there was good alignment between the dynamics for animals with normal hearing and hearing loss. The correlation for pairs of animals after projection onto the first CC, which accounted for 88 ± 2% and 71 ± 7% of the variance in animals with normal hearing (n = 6) and hearing loss (n = 6), respectively, was 0.94 ± 0.01 (n = 36). The correlation after projection onto the second CC, which accounted for the remaining variance, was lower (0.44 ± 0.03; n = 36).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Hypersensitivity to background noise after hearing loss.</title><p>(<bold>a</bold>) Average signal dynamics for speech in quiet at 60 dB SPL for normal hearing animals (left) and at best intensity for animals with hearing loss (right) after joint alignment via multiway canonical correlation analysis (MCCA). The inset value within each panel indicates the average percent of the variance in the bottleneck activations explained by each canonical component (CC). The inset value between columns indicates the average correlation between the dynamics after projection onto each pair of CCs. (<bold>b</bold>) Average signal dynamics for speech in quiet at 60 dB SPL for normal hearing animals (left) and with a hearing aid for animals with hearing loss (right) after joint alignment via MCCA. (<bold>c</bold>) Average signal dynamics for speech in noise at 70 dB SPL for normal hearing animals (left) and at best intensity for animals with hearing loss (right) after joint alignment via MCCA. (<bold>d</bold>) Average signal dynamics for speech in noise at 70 dB SPL for normal hearing animals (left) and with a hearing aid for animals with hearing loss (right) after joint alignment via MCCA. (<bold>e</bold>) Schematic diagram showing amplitude modulated narrowband noise target and masker sounds. (<bold>f</bold>) Correlation between bottleneck activations for target noise with and without masker noise at 70 dB SPL. Each line shows the correlation for a different target center frequency as a function of the masker center frequency, averaged across individual animals. (<bold>g</bold>) Change in correlation between bottleneck activations for target noise with and without masker noise at 70 dB SPL for animals with hearing loss relative to animals with normal hearing. (<bold>h</bold>) Change in correlation for animals with hearing loss at best intensity relative to animals with normal hearing at 70 dB SPL. (<bold>i</bold>) Change in correlation for animals with hearing loss with a hearing aid relative to animals with normal hearing at 70 dB SPL.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85108-fig8-v2.tif"/></fig><p>With the hearing aid (<xref ref-type="fig" rid="fig8">Figure 8b</xref>), the alignment between the dynamics for animals with normal hearing and hearing loss was even better. The correlation after projection onto the first CC remained high (0.94 ± 0.01; n = 36) and the correlation after projection onto the second CC rose substantially (0.73 ± 0.01; n = 36). These results are consistent with the analysis for speech in quiet above that suggested that only minimal distortions in the signal dynamics for animals with hearing loss remain after frequency-weighted amplification is provided (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><p>For speech in noise, the alignment was generally worse overall. When compared at best intensity (<xref ref-type="fig" rid="fig8">Figure 8c</xref>), the correlations after projection onto the CCs were 79 ± 1%, 74 ± 3%, and 18 ± 3%, respectively (n = 36). But even for the first two CCs for which the alignment was relatively good, the percent of the variance accounted for by each was substantially different for normal hearing animals and those with hearing loss. The first CC, which provided a clean reflection of each consonant, accounted for 56 ± 1% of the variance for normal hearing animals (n = 6), but only 38 ± 7% of the variance for animals with hearing loss (n = 6). Conversely, the second CC, which was dominated by the background noise, accounted for 41 ± 6% of the variance for animals with hearing loss (n = 6), but only 28 ± 1% of the variance for animals with normal hearing. Thus, while the neural subspaces encoding the speech and the background noise seem to be similar for all animals, the balance of variance between these subspaces is tilted much more toward background noise in animals with hearing loss.</p><p>Given the nature of the hearing loss (larger elevation in intensity thresholds at high frequencies; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) and the spectral properties of speech babble (higher power at low frequencies; <xref ref-type="fig" rid="fig7">Figure 7b</xref>), the hypersensitivity of animals with hearing loss to the background noise is somewhat expected. However, the problem was, if anything, exacerbated by the selective amplification of high frequencies provided by the hearing aid (<xref ref-type="fig" rid="fig8">Figure 8d</xref>). The CC that was dominated by the background noise (now the first CC, since it produced the highest correlation) accounted for 46 ± 6% of the variance for animals with hearing loss (n = 6), but only 31 ± 1% of the variance for animals with normal hearing, while the CC that provided a clean reflection of each consonant (now the second CC) accounted for 56 ± 1% of the variance for normal hearing animals (n = 6), but only 33 ± 6% of the variance for animals with hearing loss (n = 6). Thus, it appears that hearing loss allows the signal dynamics to be captured by background noise at the expense of foreground speech in a way that cannot be corrected by simple frequency-dependent amplification.</p><p>To characterize the distortions in spectral processing that underlie this effect, we examined how the processing of one narrowband sound is impacted by the presence of another. We used narrowband noise modulated by a 20 Hz sinusoidal envelope as the target sound and narrowband noise modulated by a pink noise envelope as the masker (<xref ref-type="fig" rid="fig8">Figure 8e</xref>). We varied the center frequency of the target and masker separately and the intensity of the target and masker together (to maintain a constant target-to-masker ratio). We presented the target sounds to the DNN for each animal with and without the masker sounds and measured the differences between the signal dynamics across the two conditions by computing their correlation.</p><p>For animals with normal hearing, the correlation between the target only and target plus masker dynamics ranged from 0.25 to 0.75, with the masker having the largest impact when its center frequency was similar to that of the target (<xref ref-type="fig" rid="fig8">Figure 8f</xref>). When compared at the same high intensity (70 dB SPL), the correlation for animals with hearing loss was typically lower than normal when the masker center frequency was low, especially when the target center frequency was high (<xref ref-type="fig" rid="fig8">Figure 8g</xref>). This regime (high-frequency target, low-frequency masker) is comparable to the speech-in-noise scenario analyzed above (many consonants contain substantial high-frequency content, while speech babble is dominated by low frequencies, see <xref ref-type="fig" rid="fig7">Figure 7b</xref>), and the increased impact of the masker with hearing loss is consistent with observed hypersensitivity to background noise at the expense of foreground speech. For high-frequency maskers, the correlation for animals with hearing loss was often higher than normal, especially for low-frequency targets. This is unsurprising given the sloping nature of the hearing loss, but this regime (low-frequency target, high-frequency masker) is uncommon in real-world speech.</p><p>When the correlation for animals with normal hearing and hearing loss was compared either at the best intensity (<xref ref-type="fig" rid="fig8">Figure 8h</xref>) or after frequency-weighted amplification with the hearing aid (<xref ref-type="fig" rid="fig8">Figure 8i</xref>), the pattern of results was similar. In the regime that is most comparable to consonants in speech babble (low-frequency masker, high-frequency target), the correlation for animals with hearing loss was lower than normal. Thus, hearing loss appears to create deficits in spectral processing that manifest as highly nonlinear distortions in cross-frequency interactions that are particularly problematic for real-world speech in noise and cannot be corrected by simply compensating for lost sensitivity.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we took advantage of recently developed tools for large-scale neural recordings and nonlinear modeling that allowed us to gain new insights into the impact of hearing loss on auditory processing at the network level. We first used a traditional approach to manifold learning to establish that the neural code for speech in the IC can be well described by low-dimensional latent signal dynamics that are common across animals with similar hearing status but fundamentally altered by hearing loss. We then trained a DNN to replicate neural coding in the IC with high accuracy using a framework that also facilitated manifold learning. The DNN exhibited dynamics in response to speech that were similar to those identified directly from IC recordings, and further probing of the DNN dynamics with novel sounds allowed us to identify changes in cross-frequency interactions as a key contributor to the distorted neural coding of speech-in-noise with hearing loss.</p><sec id="s3-1"><title>Suprathreshold effects of hearing loss</title><p>The effects of hearing loss beyond increased detection thresholds are often ignored in clinical assessment and treatment. But these suprathreshold effects are, in fact, the main problem for many people in real-world settings, such as busy workplaces or social gatherings, where sound intensities are high and amplification via a hearing aid provides little benefit. The clinical neglect of suprathreshold effects is not due to a lack of awareness, but rather to a lack of effective treatments. And the lack of effective treatments stems from a lack of understanding of how the many physiological changes that accompany hearing loss contribute to complex perceptual deficits.</p><p>Many specific suprathreshold impairments with plausible links to speech-in-noise perception have been identified, such as decreased frequency selectivity, dynamic range, or temporal resolution (<xref ref-type="bibr" rid="bib38">Moore, 2007</xref>). But the extent to which each of these specific impairments contributes to real-world hearing problems has been difficult to determine. Our results suggest that changes in spectral processing, particularly in the interactions between different frequencies, are the primary problem. While impaired spectral processing was evident for simple tones as a clustering of dynamical trajectories within the signal manifold, this impairment does not appear to be a major problem for the neural coding of speech per se as suitable amplification corrected many of the distortions caused by hearing loss for speech in quiet. For speech in noise, however, there was a hypersensitivity to background noise with hearing loss that amplification (simple or frequency-weighted) did little to alleviate, and this was consistent with observed interactions between narrowband sounds, which revealed an increased disruption of high-frequency targets by low-frequency maskers both with and without amplification.</p><p>These results are consistent with a recent study that found that hearing loss caused the IC activity patterns elicited by different phonemes to become less distinct, and that a hearing aid failed to correct this problem for speech in noise (<xref ref-type="bibr" rid="bib2">Armstrong et al., 2022</xref>). They are also consistent with a body of work demonstrating that listeners with hearing loss struggle to combine temporal envelope cues across frequency channels (<xref ref-type="bibr" rid="bib19">Healy and Bacon, 2002</xref>; <xref ref-type="bibr" rid="bib20">Healy and Carson, 2010</xref>; <xref ref-type="bibr" rid="bib48">Souza and Boike, 2006</xref>; <xref ref-type="bibr" rid="bib18">Grant et al., 2007</xref>). When speech is reduced to a single amplitude-modulated band, speech recognition performance is similar for all listeners, independent of their hearing status, suggesting that temporal processing of the speech envelope per se is unaffected by hearing loss. But as additional amplitude-modulated bands are added, performance increases more for normal hearing listeners than for those with hearing loss, suggesting that the latter group are less able to make use of complementary temporal information across multiple frequency channels. This difference is most pronounced when comparing the ability to make use of temporal modulations in high-frequency channels (4–6 kHz) in the presence of temporal modulations in lower frequency channels (1–2 kHz) (<xref ref-type="bibr" rid="bib18">Grant et al., 2007</xref>), and it does not appear to be a simple consequence of broadened frequency tuning but rather a specific deficit in cross-frequency interactions (<xref ref-type="bibr" rid="bib20">Healy and Carson, 2010</xref>).</p></sec><sec id="s3-2"><title>Distorted spectral processing from cochlea to cortex</title><p>Understanding exactly what is going wrong with spectral processing after hearing loss at a mechanistic level remains a challenge. The effects of hearing loss on spectral processing in the cochlea have been well described in terms of the observed changes in the frequency tuning curves of individual AN fibers. After hearing loss, the tuning curve ‘tip’ (corresponding to the characteristic frequency [CF] to which the fiber is most sensitive) becomes less sensitive and may shift toward lower frequencies while the ‘tail’ (corresponding to frequencies below CF) may become more sensitive (<xref ref-type="bibr" rid="bib60">Young, 2012</xref>). It is difficult to know the degree to which these changes distort the basic tonotopic map in the cochlea (i.e., the relationship between CF and cochlear position) because few studies have identified the cochlear position from which recorded fibers originate. The limited data that exist suggest that the effect of hearing loss on CF tonotopy is modest (<xref ref-type="bibr" rid="bib30">Liberman and Kiang, 1984</xref>), but the effect on the tonotopic map of best frequency (BF; the frequency that elicits the strongest response from a fiber at higher intensities) can be much larger (<xref ref-type="bibr" rid="bib21">Henry et al., 2016</xref>), and can be accompanied by more complex changes in spectral processing such as decreased synchrony capture (<xref ref-type="bibr" rid="bib60">Young, 2012</xref>).</p><p>One recent study has provided insight into how the complex spectral distortions in the cochlea impact the neural coding of speech in noise in the AN (<xref ref-type="bibr" rid="bib40">Parida and Heinz, 2022</xref>). In animals with mild-to-moderate sensorineural hearing loss, fibers with high CFs responded excessively to low-frequency sounds and it was this effect (rather than changes in frequency selectivity per se or temporal processing) that appeared to be most responsible for the disrupted neural coding of speech in noise. These results are consistent with our observations of hypersensitivity to background noise and increased disruption of high-frequency targets by low-frequency maskers in the IC. The distortion of spectral processing that we observe as clustering of network dynamics for simple tones in the IC, however, does not appear to be present in the AN. It is difficult to be certain since no directly comparable analysis has been performed on experimental AN responses, but our analysis of simulated AN responses suggests that hearing loss has more complex effects on the signal dynamics in the AN than simply causing clustering (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). This transformation from complex distortions in signal dynamics for tones in the AN to the simple clustering observed in the IC could arise from normal central processing of distorted peripheral inputs, but it is also possible that plastic changes in central processing that follow hearing loss facilitate a shift into a new dynamical regime.</p><p>Distorted spectral processing has also recently been observed in the auditory cortex after mild-to-moderate sloping sensorineural hearing loss (<xref ref-type="bibr" rid="bib35">McGill et al., 2022</xref>). Animals displayed behavioral hypersensitivity for detection of tones at the edge frequencies around which the hearing loss increased from mild to moderate as well as an overrepresentation of these frequencies in the cortical tonotopic map of BF. The mechanisms underlying these phenomena are not entirely clear. Some cortical neurons tuned to these edge frequencies exhibited increased neural gain and synchrony, and direct stimulation of thalamocortical afferents demonstrated that hearing loss caused an increase in gain within the local cortical circuit. But the frequency tuning of the stimulated afferents was unknown and, thus, it is difficult to separate the effects that were cortical in origin from those that were inherited from lower levels. It is possible that the altered neural representation of spectral features that we observed in the IC results in changes in the coactivation patterns across the cortical network, prompting the plastic reorganization in the cortex. Future research should be focused on developing a coherent model of how peripheral and central changes combine to create auditory processing deficits, perhaps through coordinated experiments across many brain areas in a single species.</p></sec><sec id="s3-3"><title>A new focus for hearing aid design</title><p>The complex suprathreshold effects of hearing loss that are evident in the distorted neural signal dynamics observed in this study present a difficult challenge for hearing aid designers. Current hearing aids compensate for changes in the threshold and dynamic range of auditory processing using a framework built around a bank of bandpass filters with automatic gain control. The signal processing that can be performed by such a framework is highly constrained, and it is difficult to imagine how it could be used to compensate for problems such as hypersensitivity to background noise that involve highly nonlinear interactions across frequency bands. It is possible that with a better understanding of exactly how different frequencies are interacting, new signal processing frameworks can be designed to offset the more complex effects of hearing loss. But engineering such a framework that is flexible enough to provide benefit in a wide range of real-world settings will require conceptual advances that may not be forthcoming in the near term.</p><p>One alternative approach to improving the perception of speech in noise that is already showing promise is speech enhancement, that is, the suppression of background noise. Hearing aids have offered noise suppression for many years, but the simple filters based on low-order statistics that are used by current devices often provide little real-world benefit (<xref ref-type="bibr" rid="bib6">Brons et al., 2014</xref>; <xref ref-type="bibr" rid="bib10">Cox et al., 2014</xref>). Recent research on speech enhancement via DNNs has demonstrated the potential to yield dramatic improvements in performance (<xref ref-type="bibr" rid="bib53">Wang, 2017</xref>; <xref ref-type="bibr" rid="bib31">Luo and Mesgarani, 2019</xref>). This ‘deep denoising’ may prove effective in situations with a single talker where it is obvious which sound is of interest. But in others, for example, with multiple talkers, a sound that is of primary interest one minute may become a distraction the next. It may be possible to implement cognitive control to direct enhancement toward the sound of interest but there are many significant technical challenges that must be overcome before this approach can be widely applied (<xref ref-type="bibr" rid="bib17">Geirnaert et al., 2021</xref>).</p><p>A more flexible alternative is to identify optimal processing algorithms for hearing aids empirically by providing DNNs with the data they need to learn how best to transform sounds in order to elicit normal neural activity from an impaired auditory system (<xref ref-type="bibr" rid="bib28">Lesica, 2018</xref>; <xref ref-type="bibr" rid="bib13">Drakopoulos and Verhulst, 2022</xref>). By taking advantage of the nonlinear capacity of DNNs with minimal assumptions, it should be possible to identify novel general-purpose algorithms that go well beyond the hand-designed processing in current devices. Such algorithms would be especially valuable in important contexts such as listening to music – a major problem for hearing aid users (<xref ref-type="bibr" rid="bib32">Madsen and Moore, 2014</xref>) – in which denoising cannot help. There are, of course, limits to the degree of hearing restoration that any hearing aid can provide in cases of severe hearing loss. But the vast majority of people with hearing loss have only mild-to-moderate cochlear damage (<xref ref-type="bibr" rid="bib57">Wilson et al., 2017</xref>), and there should be sufficient functionality remaining within the auditory system for a hearing aid to leverage when attempting elicit the necessary patterns of neural activity.</p></sec><sec id="s3-4"><title>Modeling biological neural networks with DNNs</title><p>Building computational models of sensory processing has been a long-standing goal in systems neuroscience. Current models of the sensory periphery can be highly accurate. For example, there are numerous models of the cochlea that faithfully capture the transformation of incoming sound into basilar membrane motion and AN activity (<xref ref-type="bibr" rid="bib47">Saremi et al., 2016</xref>; <xref ref-type="bibr" rid="bib50">Verhulst et al., 2018</xref>). Models of sensory processing in the brain, however, have generally been much less accurate, with even the best models missing out on a significant fraction of the explainable variance in subcortical and cortical neural activity (<xref ref-type="bibr" rid="bib54">Williamson et al., 2016</xref>; <xref ref-type="bibr" rid="bib44">Rahman et al., 2020</xref>; <xref ref-type="bibr" rid="bib34">McFarland et al., 2013</xref>; <xref ref-type="bibr" rid="bib51">Vintch et al., 2015</xref>).</p><p>Until recently, efforts to model central sensory processing were constrained by the difficulties of fitting high-capacity models with limited experimental data. But deep learning has provided a new approach to fitting models that are well matched to sensory processing, and new methods for large-scale electrophysiology can provide the required big data. Initial efforts to use DNNs to model the sensory periphery have shown that they can be as accurate as hand-designed biophysical models. In one recent study, a DNN trained to replicate an established model of the cochlea provided accurate and generalizable predictions of the model’s response to speech and tones (<xref ref-type="bibr" rid="bib3">Baby et al., 2021</xref>). In another study, a DNN trained on retinal ganglion cell activity predicted nearly all of the explainable variance in responses to natural images (<xref ref-type="bibr" rid="bib33">Maheswaranathan et al., 2019</xref>).</p><p>DNN models of sensory processing in the brain have also been shown to outperform traditional models. DNN models of V1 responses to natural images explained 50 and 80% of the explainable variance in single-unit spike counts and calcium activity, respectively (<xref ref-type="bibr" rid="bib7">Cadena et al., 2019</xref>; <xref ref-type="bibr" rid="bib52">Walker et al., 2019</xref>), while DNN models of V4 explained 90% of the explainable variance in multi-unit spike counts (<xref ref-type="bibr" rid="bib5">Bashivan et al., 2019</xref>). DNN models of A1 responses to speech and other natural sounds perform similarly well, explaining much of the explainable variance in high-gamma activity, fMRI voxel responses, or time-varying spike rates (<xref ref-type="bibr" rid="bib25">Keshishian et al., 2020</xref>; <xref ref-type="bibr" rid="bib24">Kell et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">Pennington and David, 2022</xref>).</p><p>Our results improve on these initial successes in several important ways. Firstly, our models simulate neural activity with full temporal resolution, that is, spike times with millisecond precision. While lower temporal resolution may be sufficient to describe sensory processing in some contexts, precise spike timing carries critical information about speech (<xref ref-type="bibr" rid="bib15">Garcia-Lazaro et al., 2013</xref>). Secondly, the activity produced by our models is nearly indistinguishable from that recorded experimentally, capturing more than 95% of the explainable variance in many cases. This is especially remarkable considering the full temporal resolution (with lower resolution, variance at fine time scales, which is typically the most difficult to capture, is simply ignored). Finally, our use of a low-dimensional bottleneck allows us to achieve this performance within a framework that also provides a compact and interpretable description of the latent dynamics that underlie the full network activity patterns.</p><p>With these advances, it should now be possible to use computational models of the brain for exploratory basic research, with benefits that are both scientific (studies are no longer data limited) and ethical (animal experiments can be limited to confirmatory studies), as well as for technology development (such as improved hearing aids, as described above). With further effort, it may be possible to build models that are not only black-box simulators but also mirror the underlying structure of biological systems (<xref ref-type="bibr" rid="bib23">Jazayeri and Ostojic, 2021</xref>; <xref ref-type="bibr" rid="bib9">Chung and Abbott, 2021</xref>). Such models would provide powerful platforms for testing mechanistic hypotheses and developing new ways to address complex network-level dysfunctions that remain difficult to treat (such as tinnitus).</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Experimental protocol</title><p>Experiments were performed on 12 young-adult gerbils of both sexes that were born and raised in standard laboratory conditions. Six of the animals were exposed to noise when they were 16–18 weeks old. (These six were chosen from among many that were noise exposed based on the pattern of hearing loss that they exhibited: sloping mild-to-moderate in both ears.) The number of animals used was not predetermined. Because of the investigative nature of the study, the key outcome measures were not known in advance and, thus, a pre-study power analysis based on anticipated effect sizes was not possible. The duration of the data collection from each animal was predetermined based on the results of preliminary experiments in which the amount of neural activity required for manifold analysis and deep learning to yield stable results was assessed. Assignment to the control and hearing loss groups was random on a per-animal basis (i.e., animals from the same litter were often assigned to different groups). Investigators were not blinded during data collection or analysis (since the difference between animals with normal hearing and hearing loss is immediately apparent upon the observation of sound-evoked neural activity), but all analyses were fully automated and objective. ABR recordings and large-scale IC recordings were made from all animals when they were 20–24 weeks old. All experimental protocols were approved by the UK Home Office (PPL P56840C21).</p></sec><sec id="s4-2"><title>Noise exposure</title><p>Mild-to-moderate sensorineural hearing loss was induced by exposing anesthetized gerbils to high-pass filtered noise with a 3 dB/octave roll-off below 2 kHz at 118 dB SPL for 3 hr (<xref ref-type="bibr" rid="bib2">Armstrong et al., 2022</xref>; <xref ref-type="bibr" rid="bib49">Suberman et al., 2011</xref>). For anesthesia, an initial injection of 0.2 ml per 100 g body weight was given with fentanyl (0.05 mg per ml), medetomidine (1 mg per ml), and midazolam (5 mg per ml) in a ratio of 4:1:10. A supplemental injection of approximately 1/3 of the initial dose was given after 90 min. Internal temperature was monitored and maintained at 38.7°C.</p></sec><sec id="s4-3"><title>Preparation for large-scale IC recordings</title><p>Animals were placed in a sound-attenuated chamber and anesthetized for surgery with an initial injection of 1 ml per 100 g body weight of ketamine (100 mg per ml), xylazine (20 mg per ml), and saline in a ratio of 5:1:19. The same solution was infused continuously during recording at a rate of approximately 2.2 μl per min. Internal temperature was monitored and maintained at 38.7°C. A small metal rod was mounted on the skull and used to secure the head of the animal in a stereotaxic device. The pinnae were removed and speakers (Etymotic ER-2) coupled to tubes were inserted into both ear canals along with microphones (Etymotic ER-10B+) for calibration. The frequency response of these speakers measured at the entrance of the ear canal was flat (±5 dB) between 0.2 and 8 kHz. Two craniotomies were made along with incisions in the dura mater, and a 256-channel multi-electrode array was inserted into the central nucleus of the IC in each hemisphere (<xref ref-type="bibr" rid="bib2">Armstrong et al., 2022</xref>). The arrays were custom-designed to maximize coverage of the portion of the gerbil IC that is sensitive to the frequencies that are present in speech.</p></sec><sec id="s4-4"><title>Auditory brainstem responses</title><p>Before beginning the IC recordings, ABRs were measured. Subdermal needles were used as electrodes with the active electrodes placed behind the ear over the bulla (one on each side), the reference placed over the nose, and the ground placed in a rear leg. Recordings were bandpass-filtered between 300 and 3000 Hz. The parallel ABR method (<xref ref-type="bibr" rid="bib43">Polonenko and Maddox, 2019</xref>) was used, with randomly timed tones at multiple frequencies presented simultaneously and independently to each ear. The tone frequencies were 500, 1000, 2000, 4000, and 8000 Hz. Each tone was five cycles long and multiplied by a Blackman window of the same duration. Tones were presented at a rate of 40 per s per frequency with alternating polarity for 100 s at each intensity. The activity recorded in the 30 ms following each tone was extracted and thresholds for each frequency were defined as the lowest intensity at which the root mean square (RMS) of the median response across presentations was more than twice the RMS of the median activity recorded in the absence of sound.</p></sec><sec id="s4-5"><title>Sounds presented during IC recordings</title><sec id="s4-5-1"><title>Speech</title><p>Sentences were taken from the TIMIT corpus (<xref ref-type="bibr" rid="bib16">Garofolo, 1993</xref>) that contains speech read by a wide range of American English speakers. The entire corpus excluding ‘SA’ sentences was used (approximately 4.5 hr) and split into training and test sets (4.25 hr and 0.25 hr, respectively; not to be confused with the suggested training/test subdivisions in the TIMIT documentation). The training set was presented twice, once on its own and once with background noise. The test set was presented four times, twice in quiet and twice with the same background noise. The intensity for each sentence was chosen at random from 55, 65, 75, or 85 dB SPL. The speech-to-noise ratio (SNR) was chosen at random from either 0 or 10 when the speech intensity was 55 or 65 dB SPL (as is typical of a quiet setting such as a home or an office) or –10 or 0 when the speech intensity was 75 or 85 dB SPL (as is typical of a noisy setting such as a pub). The intensity of the sentences for the two presentations of the test set in quiet were identical, as were the intensity, SNR, and specific noise used for the two presentations of the test set with background noise.</p></sec><sec id="s4-5-2"><title>Noise</title><p>Background noise sounds were taken from the Microsoft Scalable Noisy Speech Dataset (<xref ref-type="bibr" rid="bib46">Reddy et al., 2019</xref>), which includes recordings of environmental sounds from a large number of different settings (e.g., café, office, roadside) and specific noises (e.g., washer-dryer, copy machine, public address announcements). A total of 4.5 hr of unique noises were used to match the duration of the presented speech. The intensity of the noise presented with each sentence was determined by the intensity of the speech and the SNR as described above.</p></sec></sec><sec id="s4-6"><title>Multi-unit activity</title><p>MUA was measured from recordings on each channel of the electrode array as follows: (1) a bandpass filter was applied with cutoff frequencies of 700 and 5000 Hz; (2) the standard deviation of the background noise in the bandpass-filtered signal was estimated as the median absolute deviation/0.6745 (this estimate is more robust to outlier values, e.g., neural spikes, than direct calculation); (3) times at which the bandpass-filtered signal made a positive crossing of a threshold of 3.5 standard deviations were identified and grouped into bins with a width of 1.3 ms. Only units with a signal correlation (across repeated trials of the speech in the test set) of 0.2 or higher were used for manifold learning and DNN training (420 ± 24 [mean ± SD] units from each animal out of 512 total channels).</p></sec><sec id="s4-7"><title>Analysis of recorded neural activity</title><p>For each animal, the MUA was represented as an <inline-formula><mml:math id="inf2"><mml:mi>M</mml:mi><mml:mi> </mml:mi><mml:mi>x</mml:mi><mml:mi> </mml:mi><mml:mi>T</mml:mi></mml:math></inline-formula> matrix, where <inline-formula><mml:math id="inf3"><mml:mi>M</mml:mi></mml:math></inline-formula> is the number of units and <inline-formula><mml:math id="inf4"><mml:mi>T</mml:mi></mml:math></inline-formula> is the number of time bins. Separate matrices <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn><mml:mi> </mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , and <inline-formula><mml:math id="inf7"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn><mml:mi> </mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were formed for the training set and each repetition of the test set (see ‘Speech<italic>’</italic> above).</p></sec><sec id="s4-8"><title>Dimensionality of signal manifold</title><p>We applied PCA to <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (after subtracting the mean from each row) to obtain the PCs, ranked in order of the amount of neural variance they explain. We projected the activity in <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn><mml:mi> </mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> onto a chosen number of PCs to obtain the latent dynamics within the manifold spanned by those PCs, yielding a new <inline-formula><mml:math id="inf10"><mml:mi>D</mml:mi><mml:mi> </mml:mi><mml:mi>x</mml:mi><mml:mi> </mml:mi><mml:mi>T</mml:mi></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , where <inline-formula><mml:math id="inf12"><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math></inline-formula> is the <inline-formula><mml:math id="inf13"><mml:mi>D</mml:mi><mml:mi> </mml:mi><mml:mi>x</mml:mi><mml:mi> </mml:mi><mml:mi>M</mml:mi></mml:math></inline-formula> matrix containing the first <inline-formula><mml:math id="inf14"><mml:mi>D</mml:mi></mml:math></inline-formula> PCs. We reconstructed the activity in <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn><mml:mi> </mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> from the latent dynamics as <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (plus the originally subtracted means) and measured the total variance explained as the ratio of the covariance between <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and the square root of the product of their variances. We reconstructed the activity in <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn><mml:mi> </mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> from the same latent dynamics as <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (plus the means of the rows of <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn><mml:mi> </mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and measured the signal variance explained as the ratio of the covariance between <inline-formula><mml:math id="inf22"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and the square root of the product of their variances. We defined the dimensionality of the signal manifold for each animal based on the number PCs required to explain 95% of the signal variance.</p></sec><sec id="s4-9"><title>Similarity of signal dynamics</title><p>We measured the similarity between the signal dynamics for different animals as the variance explained after linear regression of one set of dynamics <inline-formula><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> onto another <inline-formula><mml:math id="inf25"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>ε</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf26"><mml:mi>β</mml:mi></mml:math></inline-formula> is a matrix of regression coefficients and <inline-formula><mml:math id="inf27"><mml:mi>ε</mml:mi></mml:math></inline-formula> is a vector of error terms.</p></sec><sec id="s4-10"><title>Deep neural network models</title><p>DNNs were used to transform sound input into neural activity across four stages: (1) a SincNet layer (<xref ref-type="bibr" rid="bib45">Ravanelli and Bengio, 2018</xref>) with 48 bandpass filters of length 32 samples, each with two learnable parameters (center frequency, bandwidth), followed by symmetric log activations <inline-formula><mml:math id="inf28"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mtext>sgn </mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi> </mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>; (2) a stack of five 1-D convolutional layers, each with 128 filters of length 32 samples and stride 2, followed by PReLU activations; (3) a 1-D bottleneck convolutional layer with a specified number of filters of length 32 and stride 1, followed by PReLU activations; and (4) a linear readout layer followed by exponential activations. The only hyperparameter that was varied was the number of filters in the bottleneck layer. For comparison with a linear–nonlinear (LN) model, we used a network with the same stages 1 and 4 and a single convolutional layer between them with 128 filters of length 256 samples and stride 1, followed by PReLU activations.</p></sec><sec id="s4-11"><title>Training</title><p>Models were trained to transform 24,414.0625 kHz sound input frames of length 8192 samples into 762.9395 Hz neural activity frames of length 192 samples (corresponding to temporal decimation by a factor of 5 via the strided convolutions in the encoder block plus a final cropping layer that removed 32 samples at the start and end of each frame to eliminate convolutional edge effects). Sound inputs were scaled such that an RMS of 1 corresponded to a level of 94 dB SPL. Training was performed in MATLAB on a local PC with GPUs (2x NVIDIA RTX 3080) with a batch size of 64 for 10 epochs and took about 8 hr for a typical model. The Adam optimizer was used with a learning rate of 0.0001. The optimization was framed as Poisson regression with loss function <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>,</bold> where <inline-formula><mml:math id="inf30"><mml:mi mathvariant="bold-italic">R</mml:mi></mml:math></inline-formula> is the recorded neural activity, <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the network output, <inline-formula><mml:math id="inf32"><mml:mi>M</mml:mi></mml:math></inline-formula> is the number of units, and <inline-formula><mml:math id="inf33"><mml:mi>T</mml:mi></mml:math></inline-formula> is the number of time bins.</p></sec><sec id="s4-12"><title>Validation</title><p>For each animal, data were split into training and test sets (see ‘Speech’ above). The training set was used to learn the optimal values of the DNN parameters. The final performance of the optimized network was measured on the test set by calculating the percent of the explainable variance in the recorded responses that was explained by the network outputs based on the ratio of the covariance between <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and the covariance between <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , where <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the network output and <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf40"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are the recorded responses to the two presentations of the test speech.</p></sec><sec id="s4-13"><title>Analysis of bottleneck activations</title><p>For each animal, the activations in the bottleneck layer for all sounds from a given class (e.g., all pure tones or all consonants in noise) were extracted to form the <inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mi>x</mml:mi><mml:mi> </mml:mi><mml:mi>T</mml:mi></mml:math></inline-formula> signal dynamics matrix <inline-formula><mml:math id="inf42"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf43"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of bottleneck channels and <inline-formula><mml:math id="inf44"><mml:mi>T</mml:mi></mml:math></inline-formula> is the number of time bins. For visualization, we applied PCA to the dynamics in <inline-formula><mml:math id="inf45"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math></inline-formula> and projected them onto a chosen number of PCs.</p></sec><sec id="s4-14"><title>Sounds presented to trained DNNs</title><sec id="s4-14-1"><title>Pure tones</title><p>100 ms tones with frequencies ranging from 500 Hz to 8000 Hz in 0.2 octave steps; intensities ranging from 25 dB SPL to 100 dB SPL in 5 dB steps; 10 ms cosine on and off ramps; and a 100 ms pause between tones.</p></sec><sec id="s4-14-2"><title>SAM noise, fixed modulation depth</title><p>100 ms bursts of bandpass noise with cutoff frequencies of 500 and 8000 Hz; a sinusoidal envelope with frequencies ranging from 10 Hz to 240 Hz in 10 Hz steps and a modulation depth of 1; intensities ranging from 25 dB SPL to 100 dB SPL in 5 dB steps; 10 ms cosine on and off ramps; and a 100 ms pause between tones.</p></sec><sec id="s4-14-3"><title>SAM noise, fixed modulation frequency</title><p>100 ms bursts of bandpass noise with cutoff frequencies of 500 and 8000 Hz; a sinusoidal envelope with a modulation depth ranging from 0.1 to 1 in 20 logarithmic steps and a frequency of 30 Hz; intensities ranging from 25 dB SPL to 100 dB SPL in 5 dB steps; 10 ms cosine on and off ramps; and a 100 ms pause between tones.</p></sec><sec id="s4-14-4"><title>Isolated consonants</title><p>Speech utterances were taken from the Articulation Index LSCP (LDC Cat# LDC2015S12). Utterances were from 10 American English speakers (five males, five females). Each speaker pronounced consonant-vowel syllables made from all possible combinations of 22 consonants and 13 vowels. For each utterance, the border between the consonant and vowel was identified in a semi-automated manner (a clustering algorithm [MATLAB <italic>linkage</italic>] was applied to the spectrogram time bins to identify two clusters based on a correlation metric and the border between them was inspected and corrected if needed), values after the end of the consonant were set to zero (with a 2 ms linear ramp), and the utterance was truncated to 200 ms. Utterances were presented in random order with a 175 ms pause between sounds at intensities ranging from 25 dB SPL to 100 dB SPL in 5 dB steps.</p></sec><sec id="s4-14-5"><title>Multi-talker speech babble noise</title><p>Continuous speech from 16 different British English speakers from the UCL Scribe database (<ext-link ext-link-type="uri" xlink:href="https://www.phon.ucl.ac.uk/resource/scribe">https://www.phon.ucl.ac.uk/resource/scribe</ext-link>) was summed to create speech babble. The intensity of the babble was set based on the intensity of the isolated consonants to achieve a speech-to-noise ratio of 3 dB.</p></sec><sec id="s4-14-6"><title>Narrowband target</title><p>100 ms bursts of bandpass noise with center frequencies ranging from 500 Hz to 8000 Hz in 0.5 octave steps and a bandwidth of 0.5 octaves; a sinusoidal envelope with a modulation depth of 1 and a frequency of 20 Hz; intensities ranging from 25 dB SPL to 100 dB SPL in 5 dB steps; 10 ms cosine on and off ramps; and a 100 ms pause between tones.</p></sec><sec id="s4-14-7"><title>Narrowband noise</title><p>100 ms bursts of bandpass noise with center frequencies ranging from 500 Hz to 8000 Hz in 0.5 octave steps and a bandwidth of 0.5 octaves; a pink noise envelope (power scaled as inverse of frequency) with a modulation depth ((peak – trough)/peak) of 1; an intensity matched to that of the narrowband target; 10 ms cosine on and off ramps; and a 100 ms pause between tones.</p></sec></sec><sec id="s4-15"><title>Hearing aid simulation</title><p>A 10-channel wide-dynamic range compression hearing aid was simulated using a program provided by Prof. Johsua Alexander (Purdue University) (<xref ref-type="bibr" rid="bib1">Alexander and Masterson, 2015</xref>). The crossover frequencies between channels were 200, 500, 1000, 1750, 2750, 4000, 5500, 7000, and 8500 Hz. The intensity thresholds below which amplification was linear for each channel were 45, 43, 40, 38, 35, 33, 28, 30, 36, and 44 dB SPL. The attack and release times (the time constants of the changes in gain following an increase or decrease in the intensity of the incoming sound, respectively) for all channels were 5 and 40 ms, respectively. The gain and compression ratio for each channel were fit individually for each ear of each gerbil using the Cam2B.v2 software provided by Prof. Brian Moore (Cambridge University) (<xref ref-type="bibr" rid="bib39">Moore et al., 2010</xref>). The gain before compression typically ranged from 10 dB at low frequencies to 30 dB at high frequencies. The compression ratios typically ranged from 1 to 2.5, that is, the increase in sound intensity required to elicit a 1 dB increase in the hearing output ranged from 1 dB to 2.5 dB when compression was engaged.</p></sec><sec id="s4-16"><title>Representational similarity analysis</title><p>For each animal, the signal dynamics matrix <inline-formula><mml:math id="inf46"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math></inline-formula> was reshaped to yield <inline-formula><mml:math id="inf47"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> , an <inline-formula><mml:math id="inf48"><mml:mi>S</mml:mi><mml:mi> </mml:mi><mml:mi>x</mml:mi><mml:mi> </mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mi>x</mml:mi><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> matrix, where <inline-formula><mml:math id="inf49"><mml:mi>S</mml:mi></mml:math></inline-formula> is the number of sounds from a given class and <inline-formula><mml:math id="inf50"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of times bins associated with an individual sound. An <inline-formula><mml:math id="inf51"><mml:mi>S</mml:mi><mml:mi> </mml:mi><mml:mi>x</mml:mi><mml:mi> </mml:mi><mml:mi>S</mml:mi></mml:math></inline-formula> representational dissimilarity matrix (RDM) was formed, with each entry equal to one minus the correlation between a pair of rows in <inline-formula><mml:math id="inf52"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> . To compute overall representational similarity, the upper triangular values (excluding the diagonal) from two <inline-formula><mml:math id="inf53"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> matrices were reshaped into vectors and the correlation between them was computed. For speech, dynamics were averaged across all instances of each consonant before RDMs were computed.</p></sec><sec id="s4-17"><title>Canonical correlation analysis</title><p>To align two sets of signal dynamics, <inline-formula><mml:math id="inf54"><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf55"><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mi mathvariant="bold-italic">B</mml:mi></mml:math></inline-formula> were computed using QR factorization and singular value decomposition (MATLAB <italic>cannoncorr</italic>), where <inline-formula><mml:math id="inf56"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf57"><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:math></inline-formula> are the matrices containing the original dynamics, <inline-formula><mml:math id="inf58"><mml:mi mathvariant="bold-italic">A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf59"><mml:mi mathvariant="bold-italic">B</mml:mi></mml:math></inline-formula> are the matrices containing the canonical components, and <inline-formula><mml:math id="inf60"><mml:mi mathvariant="bold-italic">U</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf61"><mml:mi mathvariant="bold-italic">V</mml:mi></mml:math></inline-formula> are the aligned dynamics. Overall similarity after alignment was computed as <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ρ</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, with the second term in the product acting as the ‘weight for the correlation associated with each pair of CCs’ that is referred to in the ‘Results.’ <inline-formula><mml:math id="inf63"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf64"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the projections of <inline-formula><mml:math id="inf65"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf66"><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:math></inline-formula> onto the <inline-formula><mml:math id="inf67"><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> pair of canonical components, <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∗</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Y</mml:mi><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∗</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are the reconstructions of <inline-formula><mml:math id="inf70"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf71"><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:math></inline-formula> from the <inline-formula><mml:math id="inf72"><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> pair of canonical components and <inline-formula><mml:math id="inf73"><mml:mi>ρ</mml:mi></mml:math></inline-formula> denotes point-by-point correlation. To jointly align more than two sets of dynamics, multiway CCA was used (<xref ref-type="bibr" rid="bib12">de Cheveigné et al., 2019</xref>) (MATLAB NoiseTools <italic>nt_mcca</italic>).</p></sec><sec id="s4-18"><title>Decoding signal dynamics</title><p>For each animal, the signal dynamics matrix <inline-formula><mml:math id="inf74"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math></inline-formula> was reshaped such that each row contained the dynamics for one consonant instance. A support vector machine was trained (MATLAB <italic>fitcecoc</italic>) to identify consonants from signal dynamics with a max-wins voting strategy based on all possible combinations of binary classifiers and tenfold cross-validation.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>is a co-founder of Perceptual Technologies</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Supervision, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Software, Investigation, Methodology</p></fn><fn fn-type="con" id="con4"><p>Software, Investigation, Methodology</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Software, Supervision, Funding acquisition, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experimental protocols were approved by the UK Home Office (PPL P56840C21). Every effort was made to minimize suffering.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-85108-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The metadata, ABR recordings, and a subset of the IC recordings analyzed in this study are available on figshare (DOI:<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.845654">10.6084/m9.figshare.845654</ext-link>). We have made only a subset of the IC recordings available because they are also being used for commercial purposes. These purposes (to develop improved assistive listening technologies) are distinct from the purpose for which the recordings are used in this manuscript (to better understand the fundamentals of hearing loss). Researchers seeking access to the full set of neural recordings for research purposes should contact the corresponding author via e-mail to set up a material transfer agreement. The custom code used for training the deep neural network models for this study is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/nicklesica/dnn">https://github.com/nicklesica/dnn</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib29">Lesica, 2023</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Lesica</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Data from Sabesan et al., 2023</data-title><source>figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.845654</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors thank A de Cheveigné, S Ostojic, JÁ Gallego, and F Bruford for their advice. This work was supported by a Wellcome Trust Senior Research Fellowship (200942/Z/16/Z) and a grant from the UK Engineering and Physical Sciences Research Council (EP/W004275/1). The funding sources were not involved in study design, data collection and interpretation, or the decision to submit the work for publication.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname><given-names>JM</given-names></name><name><surname>Masterson</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Effects of WDRC release time and number of channels on output SNR and speech recognition</article-title><source>Ear and Hearing</source><volume>36</volume><fpage>e35</fpage><lpage>e49</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000115</pub-id><pub-id pub-id-type="pmid">25470368</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Armstrong</surname><given-names>AG</given-names></name><name><surname>Lam</surname><given-names>CC</given-names></name><name><surname>Sabesan</surname><given-names>S</given-names></name><name><surname>Lesica</surname><given-names>NA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Compression and amplification algorithms in hearing aids impair the selectivity of neural responses to speech</article-title><source>Nature Biomedical Engineering</source><volume>6</volume><fpage>717</fpage><lpage>730</lpage><pub-id pub-id-type="doi">10.1038/s41551-021-00707-y</pub-id><pub-id pub-id-type="pmid">33941898</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baby</surname><given-names>D</given-names></name><name><surname>Van Den Broucke</surname><given-names>A</given-names></name><name><surname>Verhulst</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A convolutional neural-network model of human cochlear mechanics and filter tuning for real-time applications</article-title><source>Nature Machine Intelligence</source><volume>3</volume><fpage>134</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1038/s42256-020-00286-8</pub-id><pub-id pub-id-type="pmid">33629031</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barsz</surname><given-names>K</given-names></name><name><surname>Wilson</surname><given-names>WW</given-names></name><name><surname>Walton</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Reorganization of receptive fields following hearing loss in inferior colliculus neurons</article-title><source>Neuroscience</source><volume>147</volume><fpage>532</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2007.04.031</pub-id><pub-id pub-id-type="pmid">17540507</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural population control via deep image synthesis</article-title><source>Science</source><volume>364</volume><elocation-id>eaav9436</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav9436</pub-id><pub-id pub-id-type="pmid">31048462</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brons</surname><given-names>I</given-names></name><name><surname>Houben</surname><given-names>R</given-names></name><name><surname>Dreschler</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Effects of noise reduction on speech intelligibility, perceived listening effort, and personal preference in hearing-impaired listeners</article-title><source>Trends in Hearing</source><volume>18</volume><elocation-id>2331216514553924</elocation-id><pub-id pub-id-type="doi">10.1177/2331216514553924</pub-id><pub-id pub-id-type="pmid">25315377</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Gatys</surname><given-names>LA</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep convolutional models improve predictions of macaque V1 responses to natural images</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006897</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006897</pub-id><pub-id pub-id-type="pmid">31013278</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christensen</surname><given-names>JH</given-names></name><name><surname>Saunders</surname><given-names>GH</given-names></name><name><surname>Porsbo</surname><given-names>M</given-names></name><name><surname>Pontoppidan</surname><given-names>NH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The everyday acoustic environment and its association with human heart rate: evidence from real-world data logging with hearing aids and wearables</article-title><source>Royal Society Open Science</source><volume>8</volume><elocation-id>201345</elocation-id><pub-id pub-id-type="doi">10.1098/rsos.201345</pub-id><pub-id pub-id-type="pmid">33972852</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>S</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural population geometry: an approach for understanding biological and artificial neural networks</article-title><source>Current Opinion in Neurobiology</source><volume>70</volume><fpage>137</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2021.10.010</pub-id><pub-id pub-id-type="pmid">34801787</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RM</given-names></name><name><surname>Johnson</surname><given-names>JA</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Impact of advanced hearing aid technology on speech understanding for older listeners with mild to moderate, adult-onset, sensorineural hearing loss</article-title><source>Gerontology</source><volume>60</volume><fpage>557</fpage><lpage>568</lpage><pub-id pub-id-type="doi">10.1159/000362547</pub-id><pub-id pub-id-type="pmid">25139516</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dabagia</surname><given-names>M</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name><name><surname>Dyer</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Comparing High-Dimensional Neural Recordings by Aligning Their Low-Dimensional Latent Representations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2205.08413">https://arxiv.org/abs/2205.08413</ext-link></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Cheveigné</surname><given-names>A</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Arzounian</surname><given-names>D</given-names></name><name><surname>Wong</surname><given-names>DDE</given-names></name><name><surname>Hjortkjær</surname><given-names>J</given-names></name><name><surname>Fuglsang</surname><given-names>S</given-names></name><name><surname>Parra</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Multiway canonical correlation analysis of brain data</article-title><source>NeuroImage</source><volume>186</volume><fpage>728</fpage><lpage>740</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.11.026</pub-id><pub-id pub-id-type="pmid">30496819</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Drakopoulos</surname><given-names>F</given-names></name><name><surname>Verhulst</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A Differentiable Optimisation Framework for The Design of Individualised DNN-based Hearing-Aid Strategies</article-title><conf-name>ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</conf-name><pub-id pub-id-type="doi">10.1109/ICASSP43922.2022.9747683</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fogerty</surname><given-names>D</given-names></name><name><surname>Kewley-Port</surname><given-names>D</given-names></name><name><surname>Humes</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The relative importance of consonant and vowel segments to the recognition of words and sentences: effects of age and hearing loss</article-title><source>The Journal of the Acoustical Society of America</source><volume>132</volume><fpage>1667</fpage><lpage>1678</lpage><pub-id pub-id-type="doi">10.1121/1.4739463</pub-id><pub-id pub-id-type="pmid">22978895</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcia-Lazaro</surname><given-names>JA</given-names></name><name><surname>Belliveau</surname><given-names>LAC</given-names></name><name><surname>Lesica</surname><given-names>NA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Independent population coding of speech with sub-millisecond precision</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>19362</fpage><lpage>19372</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3711-13.2013</pub-id><pub-id pub-id-type="pmid">24305831</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Garofolo</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="1993">1993</year><data-title>TIMIT acoustic-phonetic continuous speech corpus</data-title><source>Linguistic Data Consortium</source></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geirnaert</surname><given-names>S</given-names></name><name><surname>Vandecappelle</surname><given-names>S</given-names></name><name><surname>Alickovic</surname><given-names>E</given-names></name><name><surname>de Cheveigne</surname><given-names>A</given-names></name><name><surname>Lalor</surname><given-names>E</given-names></name><name><surname>Meyer</surname><given-names>BT</given-names></name><name><surname>Miran</surname><given-names>S</given-names></name><name><surname>Francart</surname><given-names>T</given-names></name><name><surname>Bertrand</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Electroencephalography-based auditory attention decoding: toward neurosteered hearing devices</article-title><source>IEEE Signal Processing Magazine</source><volume>38</volume><fpage>89</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1109/MSP.2021.3075932</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grant</surname><given-names>KW</given-names></name><name><surname>Tufts</surname><given-names>JB</given-names></name><name><surname>Greenberg</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Integration efficiency for speech perception within and across sensory modalities by normal-hearing and hearing-impaired individuals</article-title><source>The Journal of the Acoustical Society of America</source><volume>121</volume><fpage>1164</fpage><lpage>1176</lpage><pub-id pub-id-type="doi">10.1121/1.2405859</pub-id><pub-id pub-id-type="pmid">17348537</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Healy</surname><given-names>EW</given-names></name><name><surname>Bacon</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Across-frequency comparison of temporal speech information by listeners with normal and impaired hearing</article-title><source>Journal of Speech, Language, and Hearing Research</source><volume>45</volume><fpage>1262</fpage><lpage>1275</lpage><pub-id pub-id-type="doi">10.1044/1092-4388(2002/101)</pub-id><pub-id pub-id-type="pmid">12546492</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Healy</surname><given-names>EW</given-names></name><name><surname>Carson</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Influence of broad auditory tuning on across-frequency integration of speech patterns</article-title><source>Journal of Speech, Language, and Hearing Research</source><volume>53</volume><fpage>1087</fpage><lpage>1095</lpage><pub-id pub-id-type="doi">10.1044/1092-4388(2010/09-0185)</pub-id><pub-id pub-id-type="pmid">20689025</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henry</surname><given-names>KS</given-names></name><name><surname>Kale</surname><given-names>S</given-names></name><name><surname>Heinz</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Distorted tonotopic coding of temporal envelope and fine structure with noise-induced hearing loss</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>2227</fpage><lpage>2237</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3944-15.2016</pub-id><pub-id pub-id-type="pmid">26888932</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Humes</surname><given-names>LE</given-names></name><name><surname>Dubno</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2010">2010</year><chapter-title>Factors affecting speech understanding in older adults</chapter-title><person-group person-group-type="editor"><name><surname>Gordon-Salant</surname><given-names>S</given-names></name><name><surname>Frisina</surname><given-names>RD</given-names></name><name><surname>Popper</surname><given-names>AN</given-names></name><name><surname>Fay</surname><given-names>RR</given-names></name></person-group><source>The Aging Auditory System</source><publisher-name>Springer</publisher-name><fpage>211</fpage><lpage>257</lpage><pub-id pub-id-type="doi">10.1007/978-1-4419-0993-0_8</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jazayeri</surname><given-names>M</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Interpreting neural computations by examining intrinsic and embedding dimensionality of neural activity</article-title><source>Current Opinion in Neurobiology</source><volume>70</volume><fpage>113</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2021.08.002</pub-id><pub-id pub-id-type="pmid">34537579</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kell</surname><given-names>AJE</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Shook</surname><given-names>EN</given-names></name><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy</article-title><source>Neuron</source><volume>98</volume><fpage>630</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.044</pub-id><pub-id pub-id-type="pmid">29681533</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshishian</surname><given-names>M</given-names></name><name><surname>Akbari</surname><given-names>H</given-names></name><name><surname>Khalighinejad</surname><given-names>B</given-names></name><name><surname>Herrero</surname><given-names>JL</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Estimating and interpreting nonlinear receptive field of sensory neural responses with deep neural network models</article-title><source>eLife</source><volume>9</volume><elocation-id>e53445</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.53445</pub-id><pub-id pub-id-type="pmid">32589140</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis-connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larson</surname><given-names>VD</given-names></name><name><surname>Williams</surname><given-names>DW</given-names></name><name><surname>Henderson</surname><given-names>WG</given-names></name><name><surname>Luethke</surname><given-names>LE</given-names></name><name><surname>Beck</surname><given-names>LB</given-names></name><name><surname>Noffsinger</surname><given-names>D</given-names></name><name><surname>Wilson</surname><given-names>RH</given-names></name><name><surname>Dobie</surname><given-names>RA</given-names></name><name><surname>Haskell</surname><given-names>GB</given-names></name><name><surname>Bratt</surname><given-names>GW</given-names></name><name><surname>Shanks</surname><given-names>JE</given-names></name><name><surname>Stelmachowicz</surname><given-names>P</given-names></name><name><surname>Studebaker</surname><given-names>GA</given-names></name><name><surname>Boysen</surname><given-names>AE</given-names></name><name><surname>Donahue</surname><given-names>A</given-names></name><name><surname>Canalis</surname><given-names>R</given-names></name><name><surname>Fausti</surname><given-names>SA</given-names></name><name><surname>Rappaport</surname><given-names>BZ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Efficacy of 3 commonly used hearing aid circuits</article-title><source>JAMA</source><volume>284</volume><fpage>1806</fpage><lpage>1813</lpage><pub-id pub-id-type="doi">10.1001/jama.284.14.1806</pub-id><pub-id pub-id-type="pmid">11025833</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lesica</surname><given-names>NA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Why do hearing aids fail to restore normal auditory perception?</article-title><source>Trends in Neurosciences</source><volume>41</volume><fpage>174</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2018.01.008</pub-id><pub-id pub-id-type="pmid">29449017</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Lesica</surname><given-names>NA</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Dnn</data-title><version designator="swh:1:rev:b03c6ade3fd3dfbfd62da9cacebc8fcebc6da8e8">swh:1:rev:b03c6ade3fd3dfbfd62da9cacebc8fcebc6da8e8</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:f0f134c9341bdda250e9c9286c5203714ef72be4;origin=https://github.com/nicklesica/dnn;visit=swh:1:snp:f7ce0b7cf3bf9b6bdcd12c49c9a9e0fc36343a19;anchor=swh:1:rev:b03c6ade3fd3dfbfd62da9cacebc8fcebc6da8e8">https://archive.softwareheritage.org/swh:1:dir:f0f134c9341bdda250e9c9286c5203714ef72be4;origin=https://github.com/nicklesica/dnn;visit=swh:1:snp:f7ce0b7cf3bf9b6bdcd12c49c9a9e0fc36343a19;anchor=swh:1:rev:b03c6ade3fd3dfbfd62da9cacebc8fcebc6da8e8</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname><given-names>MC</given-names></name><name><surname>Kiang</surname><given-names>NY</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Single-Neuron labeling and chronic cochlear pathology. IV. stereocilia damage and alterations in rate- and phase-level functions</article-title><source>Hearing Research</source><volume>16</volume><fpage>75</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1016/0378-5955(84)90026-1</pub-id><pub-id pub-id-type="pmid">6511674</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>Y</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Conv-tasnet: surpassing ideal time–frequency magnitude masking for speech separation</article-title><source>IEEE/ACM Transactions on Audio, Speech, and Language Processing</source><volume>27</volume><fpage>1256</fpage><lpage>1266</lpage><pub-id pub-id-type="doi">10.1109/TASLP.2019.2915167</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madsen</surname><given-names>SMK</given-names></name><name><surname>Moore</surname><given-names>BCJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Music and hearing aids</article-title><source>Trends in Hearing</source><volume>18</volume><elocation-id>2331216514558271</elocation-id><pub-id pub-id-type="doi">10.1177/2331216514558271</pub-id><pub-id pub-id-type="pmid">25361601</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>McIntosh</surname><given-names>LT</given-names></name><name><surname>Tanaka</surname><given-names>H</given-names></name><name><surname>Grant</surname><given-names>S</given-names></name><name><surname>Kastner</surname><given-names>DB</given-names></name><name><surname>Melander</surname><given-names>JB</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Brezovec</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Baccus</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The Dynamic Neural Code of the Retina for Natural Scenes</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/340943</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McFarland</surname><given-names>JM</given-names></name><name><surname>Cui</surname><given-names>Y</given-names></name><name><surname>Butts</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Inferring nonlinear neuronal computation based on physiologically plausible inputs</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003143</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003143</pub-id><pub-id pub-id-type="pmid">23874185</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGill</surname><given-names>M</given-names></name><name><surname>Hight</surname><given-names>AE</given-names></name><name><surname>Watanabe</surname><given-names>YL</given-names></name><name><surname>Parthasarathy</surname><given-names>A</given-names></name><name><surname>Cai</surname><given-names>D</given-names></name><name><surname>Clayton</surname><given-names>K</given-names></name><name><surname>Hancock</surname><given-names>KE</given-names></name><name><surname>Takesian</surname><given-names>A</given-names></name><name><surname>Kujawa</surname><given-names>SG</given-names></name><name><surname>Polley</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Neural signatures of auditory hypersensitivity following acoustic trauma</article-title><source>eLife</source><volume>11</volume><elocation-id>e80015</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.80015</pub-id><pub-id pub-id-type="pmid">36111669</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>RL</given-names></name><name><surname>Schilling</surname><given-names>JR</given-names></name><name><surname>Franck</surname><given-names>KR</given-names></name><name><surname>Young</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Effects of acoustic trauma on the representation of the vowel `` EH'' in cat auditory nerve fibers</article-title><source>The Journal of the Acoustical Society of America</source><volume>101</volume><fpage>3602</fpage><lpage>3616</lpage><pub-id pub-id-type="doi">10.1121/1.418321</pub-id><pub-id pub-id-type="pmid">9193048</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mitchell-Heggs</surname><given-names>R</given-names></name><name><surname>Prado</surname><given-names>S</given-names></name><name><surname>Gava</surname><given-names>GP</given-names></name><name><surname>Go</surname><given-names>MA</given-names></name><name><surname>Schultz</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neural Manifold Analysis of Brain Circuit Dynamics in Health and Disease</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2203.11874">https://arxiv.org/abs/2203.11874</ext-link></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>BCJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Cochlear Hearing Loss: Physiological, Psychological and Technical Issues</source><publisher-name>John Wiley &amp; Sons</publisher-name><pub-id pub-id-type="doi">10.1002/9780470987889</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>BCJ</given-names></name><name><surname>Glasberg</surname><given-names>BR</given-names></name><name><surname>Stone</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Development of a new method for deriving initial fittings for hearing aids with multi-channel compression: CAMEQ2-HF</article-title><source>International Journal of Audiology</source><volume>49</volume><fpage>216</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.3109/14992020903296746</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parida</surname><given-names>S</given-names></name><name><surname>Heinz</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Distorted tonotopy severely degrades neural representations of connected speech in noise following acoustic trauma</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>1477</fpage><lpage>1490</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1268-21.2021</pub-id><pub-id pub-id-type="pmid">34983817</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pennington</surname><given-names>JR</given-names></name><name><surname>David</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Can Deep Learning Provide a Generalizable Model for Dynamic Sound Encoding in Auditory Cortex?</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.06.10.495698</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Plomp</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>A signal-to-noise ratio model for the speech-reception threshold of the hearing impaired</article-title><source>Journal of Speech, Language, and Hearing Research</source><volume>29</volume><fpage>146</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1044/jshr.2902.146</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polonenko</surname><given-names>MJ</given-names></name><name><surname>Maddox</surname><given-names>RK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The parallel auditory brainstem response</article-title><source>Trends in Hearing</source><volume>23</volume><elocation-id>2331216519871395</elocation-id><pub-id pub-id-type="doi">10.1177/2331216519871395</pub-id><pub-id pub-id-type="pmid">31516096</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahman</surname><given-names>M</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Simple transformations capture auditory input to cortex</article-title><source>PNAS</source><volume>117</volume><fpage>28442</fpage><lpage>28451</lpage><pub-id pub-id-type="doi">10.1073/pnas.1922033117</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ravanelli</surname><given-names>M</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Speaker Recognition from Raw Waveform with SincNet</article-title><conf-name>2018 IEEE Spoken Language Technology Workshop (SLT</conf-name><pub-id pub-id-type="doi">10.1109/SLT.2018.8639585</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Reddy</surname><given-names>CKA</given-names></name><name><surname>Beyrami</surname><given-names>E</given-names></name><name><surname>Pool</surname><given-names>J</given-names></name><name><surname>Cutler</surname><given-names>R</given-names></name><name><surname>Srinivasan</surname><given-names>S</given-names></name><name><surname>Gehrke</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A Scalable Noisy Speech Dataset and Online Subjective Test Framework</article-title><conf-name>Interspeech 2019</conf-name><pub-id pub-id-type="doi">10.21437/Interspeech.2019-3087</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saremi</surname><given-names>A</given-names></name><name><surname>Beutelmann</surname><given-names>R</given-names></name><name><surname>Dietz</surname><given-names>M</given-names></name><name><surname>Ashida</surname><given-names>G</given-names></name><name><surname>Kretzberg</surname><given-names>J</given-names></name><name><surname>Verhulst</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A comparative study of seven human cochlear filter models</article-title><source>The Journal of the Acoustical Society of America</source><volume>140</volume><fpage>1618</fpage><lpage>1634</lpage><pub-id pub-id-type="doi">10.1121/1.4960486</pub-id><pub-id pub-id-type="pmid">27914400</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Souza</surname><given-names>PE</given-names></name><name><surname>Boike</surname><given-names>KT</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Combining temporal-envelope cues across channels: effects of age and hearing loss</article-title><source>Journal of Speech, Language, and Hearing Research</source><volume>49</volume><fpage>138</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1044/1092-4388(2006/011)</pub-id><pub-id pub-id-type="pmid">16533079</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suberman</surname><given-names>TA</given-names></name><name><surname>Campbell</surname><given-names>AP</given-names></name><name><surname>Adunka</surname><given-names>OF</given-names></name><name><surname>Buchman</surname><given-names>CA</given-names></name><name><surname>Roche</surname><given-names>JP</given-names></name><name><surname>Fitzpatrick</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A gerbil model of sloping sensorineural hearing loss</article-title><source>Otology &amp; Neurotology</source><volume>32</volume><fpage>544</fpage><lpage>552</lpage><pub-id pub-id-type="doi">10.1097/MAO.0b013e31821343f5</pub-id><pub-id pub-id-type="pmid">21389900</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verhulst</surname><given-names>S</given-names></name><name><surname>Altoè</surname><given-names>A</given-names></name><name><surname>Vasilkov</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Computational modeling of the human auditory periphery: auditory-nerve responses, evoked potentials and hearing loss</article-title><source>Hearing Research</source><volume>360</volume><fpage>55</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2017.12.018</pub-id><pub-id pub-id-type="pmid">29472062</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vintch</surname><given-names>B</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A convolutional subunit model for neuronal responses in macaque V1</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>14829</fpage><lpage>14841</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2815-13.2015</pub-id><pub-id pub-id-type="pmid">26538653</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Inception loops discover what excites neurons most using deep predictive models</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>2060</fpage><lpage>2065</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0517-x</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Deep learning reinvents the hearing aid</article-title><source>IEEE Spectrum</source><volume>54</volume><fpage>32</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1109/MSPEC.2017.7864754</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williamson</surname><given-names>RS</given-names></name><name><surname>Ahrens</surname><given-names>MB</given-names></name><name><surname>Linden</surname><given-names>JF</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Input-specific gain modulation by local sensory context shapes cortical and thalamic responses to complex sounds</article-title><source>Neuron</source><volume>91</volume><fpage>467</fpage><lpage>481</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.05.041</pub-id><pub-id pub-id-type="pmid">27346532</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williamson</surname><given-names>RC</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name><name><surname>Smith</surname><given-names>MA</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Bridging large-scale neuronal recordings and large-scale network models using dimensionality reduction</article-title><source>Current Opinion in Neurobiology</source><volume>55</volume><fpage>40</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.12.009</pub-id><pub-id pub-id-type="pmid">30677702</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willott</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Effects of aging, hearing loss, and anatomical location on thresholds of inferior colliculus neurons in C57BL/6 and CBA mice</article-title><source>Journal of Neurophysiology</source><volume>56</volume><fpage>391</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1152/jn.1986.56.2.391</pub-id><pub-id pub-id-type="pmid">3760927</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>BS</given-names></name><name><surname>Tucci</surname><given-names>DL</given-names></name><name><surname>Merson</surname><given-names>MH</given-names></name><name><surname>O’Donoghue</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Global hearing health care: new findings and perspectives</article-title><source>Lancet</source><volume>390</volume><fpage>2503</fpage><lpage>2515</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(17)31073-5</pub-id><pub-id pub-id-type="pmid">28705460</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="software"><person-group person-group-type="author"><collab>World Health Organization</collab></person-group><year iso-8601-date="2021">2021</year><data-title>World report on hearing</data-title><source>WHO</source></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Young</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural representation of spectral and temporal information in speech</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>363</volume><fpage>923</fpage><lpage>945</lpage><pub-id pub-id-type="doi">10.1098/rstb.2007.2151</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Young</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>Neural coding of sound with Cochlear damage</chapter-title><source>Noise-Induced Hearing Loss: Scientific Advances</source><publisher-name>Springer</publisher-name><fpage>87</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1007/978-1-4419-9523-0</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85108.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Herrmann</surname><given-names>Björn</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Baycrest</institution><country>Canada</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.10.04.510811" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.10.04.510811"/></front-stub><body><p>This fundamental work uses deep neural networks to simulate activity evoked by a wide range of stimuli and demonstrates systematic differences in latent population representations between hearing-impaired and normal-hearing animals that are consistent with impaired representations of speech in noise. The evidence supporting the conclusions is compelling, and the neural-network approach is novel with potential future applications. The research will be of interest to auditory neuroscientists and computational scientists.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85108.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Herrmann</surname><given-names>Björn</given-names></name><role>Reviewing Editor</role><aff><institution>Baycrest</institution><country>Canada</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>David</surname><given-names>Stephen V</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/009avj582</institution-id><institution>Oregon Health and Science University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.10.04.510811">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.10.04.510811v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Distorted neural signal dynamics create hypersensitivity to background noise after hearing loss&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Barbara Shinn-Cunningham as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Stephen V David (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) The reviewers raised concerns about the generalizability of the study's approach. The approach hinges on the deep neural network getting things right, such that it generalizes across sounds. The study's claims would be substantially more convincing if the authors included data that validates their model's predictions about coding of speech in noise, tones in noise, and SAM noise in NH vs. HL animals. It appears that they have data in hand for speech and speech-in-noise from the same animals that they could analyze using methods already in the manuscript. If they are unable to validate any of these predictions, the authors should revise the manuscript to emphasize that they remain predictions until they can be validated with additional data in a different study.</p><p>2) Several labs have studied changes in inferior colliculus and cortex, but their work is not acknowledged in this manuscript. For example, the work by the Sanes lab at NYU and Polley lab at Harvard have advanced theories around decreased inhibition to accommodate reduced peripheral input. This work has also implicated deficits in temporal processing that do not at the surface appear consistent with the current study (e.g., see Sanes and Yao <italic>eLife</italic> 2018). The authors would want to place their work in the context of these and other works more clearly.</p><p>3) Details about some statistical tests were hard to find (e.g., only in Table S1), but it also appears that the authors still make important statements without statistical justification, for example, related to NH/HL+noise vs. NH/HL+quiet (Figures 6 vs. Figures 7). There are several other cases where conclusions, e.g., about dimensionality, are not supported by a statistical test. The authors would want to make sure that all their conclusions are supported quantitatively.</p><p>4) The authors would also want to flesh out the argument for why the same effects would not be present in the nerve.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Suggestions for authors:</p><p>-at the end of intro, I think you could make it a little more explicit that the DNN is being trained to predict PC responses from sound.</p><p>For people who will not be familiar with the practical constraints that necessitate a design with separate groups of normal-hearing and hearing-impaired gerbils, you might state explicitly early on that you are comparing separate groups of NH and HI gerbils.</p><p>Figure 1 caption – why are there two sample sizes given in the last sentence?</p><p>Line 83 – you might give some flavor as to the noise types that were used</p><p>Figure 2A is really well done -- you made a pretty complicated list of comparisons quite straightforward to follow</p><p>Line 111 and Figure 2 – I must be missing something, but I don't see how you can approach 100% explained signal variance given the way that I think it is calculated. Doesn't the noise variance show up in the denominator?</p><p>Line 130 – I suggest motivating/justifying the additional linear transformation for the reader</p><p>Figure 3 – can anything be said about how the left panel of Figure 3d looks pretty different from Figure 2i?</p><p>I don't understand what constitutes a &quot;recorded unit&quot;. The methods refer to multi-unit activity. Is a unit just an electrode from the array of 512? Or is there spike sorting being performed? How is any non-stationarity in the recordings dealt with (e.g. if neurons die, or the brain moves a little w.r.t. the electrode array)?</p><p>Line 177 – I found myself wondering how the results would compare to a more conventional model with a STRF for each neuron. I suspect lots of people will wonder how the DNN compares to something like that.</p><p>Lines 183-185 – give numbers for the similarity, to be parallel to earlier sections</p><p>Lines 219-220 – the &quot;clustering of dynamics&quot; referred to here was not all that evident to this reviewer from eyeballing the figure – please make what you mean more explicit, and clarify how this is different from refs 18 and 19</p><p>Lines 232-233 – I recommend making the RDMs more conventional and just having more of them in the figure – I think people will find the asymmetry confusing when they page through the paper</p><p>Line 241 – are the numbers mean and SD? Please specify.</p><p>Line 246 – I didn't completely understand what would constitute a distortion to the &quot;overall structure of the dynamics&quot; – could you give an example?</p><p>Multiple figures – I don't think the asterisks are defined clearly, and I believe the mean different things in different figures. Please label more explicitly, and/or mention in each caption.</p><p>Line 287 – I found myself wondering about the possible effect of phase shifts or increases in response latency, which one might imagine could occur with hearing loss. I think the analysis would be highly vulnerable to this, especially given that the encoding of modulation is partly synchrony-based. The fact that the modulation analysis shows pretty similar results for NH and HI suggests there is not much of this, but some readers may wonder about this.</p><p>At several points throughout the paper, I found myself wondering about the effects of compression. I would have been greatly interested to see an analysis that separately manipulated compression (e.g., turning it off), to see how much benefit it produces on restoring normal-like responses. I also would have liked to see some discussion of its effects.</p><p>Line 344 – for this analysis, I was hoping to have chance levels explicitly stated, and ideally labeled on the graph.</p><p>Figure 7d – this panel is pretty confusing, partly because the SPL numbers are inside particular plots, so it is not completely clear what they apply to, and partly because the little numbers in the plots are not labeled or defined anywhere.</p><p>Line 404-406 – how does this jive with the findings of distorted tonotopy from the Heinz lab?</p><p>Line 408 and onwards – the text refers to CCs but the figure is labeled as PCs</p><p>Line 438 – why is this coherence rather than correlation?</p><p>Line 457-459 – these lines state the conclusions of the paper, but I think they could be more explicitly linked to the results that come earlier. Explain why the distortions are nonlinear, and explain why the effects involve cross-frequency interactions.</p><p>Lines 472-473 – the statement here (and the earlier one on line 33) seems a little too strong given the widespread prevalence of noise reduction, and the widespread use of speech in noise diagnostics in audiometric evaluations</p><p>Line 484 and earlier – can the clustering be explained merely by audibility (e.g., all the stimuli that are inaudible cluster together, for the uninteresting reason that they do not evoke a response)?</p><p>Line 496 – the claim here needs a reference</p><p>Line 511 – I wanted to know more about the absence of evidence of clustering in nerve responses. This seems critical.</p><p>Line 586 and onwards – I think the conclusions/suggestions here should be tempered given that there are almost surely going to be limits to how well DNN models trained in this way will generalize to arbitrary stimuli. And you might acknowledge some of these limitations.</p><p>Line 606 – I think it might be helpful to specify what a material transfer agreement would involve – does this just mean someone agrees not to share it with anyone else?</p><p>Line 691 – why is &quot;/ 0.6745&quot; here? Is this a typo?</p><p>Line 697 – what is a &quot;unit&quot;?</p><p>Line 768 – I wondered whether setting values to 0 without any windowing might induce artifacts…</p><p>Line 784 – it seems plausible that the hearing aid settings are suboptimal. In particular, the extent of compression is based on some informal optimization in humans. Could this partly explain the less than complete restoration of normal responses?</p><p>Line 810 – it would help to link this to the weights that are described in the Results section. It took me a couple reads to make the connection.</p><p>Overall, the statistical tests and quantitative comparisons are somewhat buried. There are a lot of statistical comparisons via color map (i.e., Figures 2H-I and 3D) where a scatter or bar plot with error bars might be more helpful.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. While there are many open questions around central deficits following hearing loss, several labs have studied changes in IC and cortex, but their work is not acknowledged in this manuscript. In particular the Sanes lab at NYU and Polley lab at Harvard have advanced theories around decreased inhibitory tone to accommodate diminished bottom-up drive. Relevant to the current study, this work has implicated deficits in temporal processing that do not at the surface appear consistent with the current study (eg, see Sanes and Yao <italic>eLife</italic> 2018). Hearing loss and the neural coding of sound are complex, so the concern is not about the validity of the current results as much as how they fit in with the existing literature. Currently, the manuscript reads as if this previous work was never completed, and that issue should be addressed.</p><p>2. In general, the results of fairly sophisticated analyses are presented clearly, which is great. After some hunting, it was possible to find important details about some statistics in Table S1, but it appears that the authors still make important statements without statistical justification. Of particular importance to the main conclusions, the increased dissimilarity for NH/HL+noise vs. NH/HL+quiet (Figure 6 vs. Figure 7) needs to be demonstrated by a quantitative comparison between them. Table S1 doesn't appear to contain anything about comparisons between data in the different figures. Please provide quantitative support for the statement that &quot;… neither was sufficient to bring the similarity close to normal levels&quot; (Line 379). There are several other cases where conclusions, eg, about dimensionality, are not supported by a statistical test. The authors should make sure that all their conclusions are supported quantitatively. It would also</p><p>3. The performance of the DNN is impressive, providing a reasonable motivation for the subsequent analysis of &quot;bottleneck PCs&quot; for activity simulated by the model. However, one worries that since the models were not fit to stimuli tested in the simulation, that the results may not actually be reciprocated in actual neural activity. One contrast, in particular (speech in quiet vs. speech in noise), was actually collected experimentally, and it seems like the authors could validate their decoding analysis with the actual neural data. Can't the neural responses be projected back into the bottleneck space and be used to decode the same way as the DNN simulations? Such an analysis would substantially strengthen the study. Alternatively, the authors should include a caveat in the Discussion that the DNN simulations may not actually generalize to actual neural activity. The authors may wish to argue that this is a small concern, but the finding of such low-dimensional PC bottleneck is quite surprising, and it's not clear if dimensionality would be as small if the actual stimuli (pure tones, SAM noise) were included in the fit set.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85108.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The reviewers raised concerns about the generalizability of the study's approach. The approach hinges on the deep neural network getting things right, such that it generalizes across sounds. The study's claims would be substantially more convincing if the authors included data that validates their model's predictions about coding of speech in noise, tones in noise, and SAM noise in NH vs. HL animals. It appears that they have data in hand for speech and speech-in-noise from the same animals that they could analyze using methods already in the manuscript. If they are unable to validate any of these predictions, the authors should revise the manuscript to emphasize that they remain predictions until they can be validated with additional data in a different study.</p></disp-quote><p>We understand the concern about generalization to out-of-sample inputs. A model trained on one set of inputs may not necessarily produce accurate simulations of responses to a different set of inputs. Thus, in order for our analysis of model latent representations of non-speech sounds to be compelling, we must demonstrate that the model responses to such sounds are an accurate simulation of the true responses.</p><p>(Some of the reviewer comments seem to suggest that we also need to demonstrate the model’s accuracy for speech and speech-in-noise sounds, but this is already shown in the original Figure 3c; the performance shown is for speech and speech-in-noise sounds that were not part of the training set.)</p><p>The non-speech sounds that we used in the study were pure tones and SAM noise. For pure tones, we were able to verify that the models generalized well by using recordings from the same animals on which the models were originally trained (without including the tone responses in training). For SAM noise, we did not have recordings from the original animals. However, this provided an opportunity to further test the generality of the model by using transfer learning to predict responses from new animals for which we had responses to SAM noise as well as a small sample of speech. We froze the DNN encoder after training on animals from the original dataset and retrained only the linear readout using the speech responses for the new animals. We then tested the ability of the updated models to predict SAM noise responses for the new animals, and they performed well. This new work is described in the revised Results (see new Figure 3f,g and associated text).</p><p>The one class of sounds for which we were not explicitly able to validate the models is hearing aid-amplified speech. While these sounds are not qualitatively different from standard speech and we have no particular reason to believe the model predictions for these sounds would be inaccurate, we have added a note to the text to indicate the lack of validation.</p><disp-quote content-type="editor-comment"><p>2) Several labs have studied changes in inferior colliculus and cortex, but their work is not acknowledged in this manuscript. For example, the work by the Sanes lab at NYU and Polley lab at Harvard have advanced theories around decreased inhibition to accommodate reduced peripheral input. This work has also implicated deficits in temporal processing that do not at the surface appear consistent with the current study (e.g., see Sanes and Yao eLife 2018). The authors would want to place their work in the context of these and other works more clearly.</p></disp-quote><p>We have tried to do more to place our results within the context other related studies. The study from the Polley lab that is most closely related to ours is McGill et al. (2022) in which they study the downstream effects of mild-to-moderate sensorineural hearing loss. (Other studies from the Polley lab use a model of extreme neuropathy, which is too different from mild-to-moderate sensorineural hearing loss to allow for meaningful comparisons.) The key findings of McGill et al. that relate to our study are (1) that hearing loss induces behavioral hypersensitivity for detection of tones at the frequencies around which the hearing loss increases from mild to moderate; (2) that these frequencies are overrepresented after rearrangement of the cortical tonotopic map; and (3) that (some) cortical neurons located in this region of the tonotopic map exhibit increased gain and synchrony in their responses.</p><p>Our work does not investigate the circuit-level mechanisms that underlie the observed effects of hearing loss (e.g., bottom-up drive vs. local E-I balance). The work from the Sanes lab is focused on these mechanisms and it is difficult for us to see how further consideration of our results in conjunction with theirs can lead to additional insights. The specific study suggested by Reviewer 2, Yao and Sanes (2018) is focused on developmental hearing loss, which makes it even more difficult to compare with our work. Also, the reviewer suggests that their results are somehow inconsistent with ours, but they are not. Their abstract states “We found that developmental HL … did not alter brainstem temporal processing.” Our results also suggest that HL does not alter brainstem temporal processing, and this is consistent with many other studies that have found that HL does not impact temporal processing in the early auditory pathway (see Parida and Heinz (2022) for another recent example). Understanding how temporal processing deficits arise at the level of the cortex after hearing loss is not something that our work can help with; for that we must continue to look to the Sanes lab and others who are focused on such questions.</p><p>What we can do is try to synthesize our results with others related to mild-to-moderate sensorineural hearing loss from the auditory nerve and cortex in order to better understand the transformation that takes place along the way. We have added a new section to the Discussion “Distorted spectral processing from cochlea to cortex” along these lines. Perhaps the most salient point we can take from this exercise is the recognition that coordinated studies are needed to develop a coherent picture.</p><disp-quote content-type="editor-comment"><p>3) Details about some statistical tests were hard to find (e.g., only in Table S1), but it also appears that the authors still make important statements without statistical justification, for example, related to NH/HL+noise vs. NH/HL+quiet (Figures 6 vs. Figures 7). There are several other cases where conclusions, e.g., about dimensionality, are not supported by a statistical test. The authors would want to make sure that all their conclusions are supported quantitatively.</p></disp-quote><p>We have added statistical tests to support our assertion that the distortions in signal dynamics caused by hearing loss are more pronounced for speech in noise than for speech in quiet (Figures 6 and 7). Whether compared at best intensity or after amplification with a hearing aid, and whether measured via RSA or CCA, the distortions were much smaller for speech in quiet than for speech in noise, with all differences highly significant (the largest p-value was less than 1e-6). We have included this information in the revised Results.</p><p>We have also added distribution plots and statistical tests to support our assertion that the signal dynamics differ between pairs of animals with normal hearing and hearing loss more than between pairs of animals with the same hearing status (new Figures 2i and 3e). Whether based on the signal manifold as identified via PCA or via DNN, signal dynamics were much more similar for pairs of animals with the same hearing status than for pairs of animals with different hearing status.</p><p>Reviewer 2 also suggested statistical tests in two instances where we did not make explicit comparisons between groups because we did not feel that these comparisons would be informative. But we include the statistical tests here for completeness:</p><p>Figure 2c,g. The dimensionality of the signal manifold</p><p>We assert only that the signal manifold is low dimensional with both normal hearing and hearing loss. We have added the range of values for the dimensionality of the signal manifold for each group of animals to the revised text. A t-test indicates that the average dimensionality with hearing loss is significantly lower than with normal hearing in the statistical sense (p = 0.04, mean = 4.8 for HL and 6.8 for NH). But since we cannot say whether or not this difference is significant in the functional sense in and of itself (as opposed to the many detailed differences in the signal manifold with and without hearing loss that we go on to analyze in the rest of the study), we did not include it in the Results.</p><p>Figure 3c. The predictive power of the DNN model</p><p>We assert only that the model performs well for both normal hearing and hearing loss. In fact, the predictive power was generally higher for hearing loss than for normal hearing: separate t-tests for each bottleneck dimensionality (see Figure 3c) yielded p-vales of 0.08, 4e-4, 1e-3, 3e-3, 1e-3, and 1e-3; all but the first of these indicates significantly better performance for hearing loss even after (Bonferroni correction). But, inasmuch as we do not follow up on this difference to understand how it arises, we don’t think it is appropriate to include it in the Results.</p><disp-quote content-type="editor-comment"><p>4) The authors would also want to flesh out the argument for why the same effects would not be present in the nerve.</p></disp-quote><p>Broadly speaking, many of the effects that we see (e.g., hypersensitivity to background noise) are present in the AN. What does not appear to be present in the AN is the specific form of distorted spectral processing that we observe in the IC as a clustering of signal dynamical trajectories within the latent representation of the DNN model.</p><p>Providing a definitive answer to the question of why these effects are present in the IC and not the AN is beyond us (though we added some speculative ideas to the new section in the revised Discussion). We can, however, provide more explicit evidence that the distortions in spectral processing in the AN and the IC are, in fact, different. To do this, we simulated AN responses to pure tone sounds with and without hearing loss, as suggested by Reviewer 1, and performed the same analyses of the signal dynamics as we did for the IC.</p><p>We found no evidence in the AN of the clustering of dynamical trajectories that is present in the IC. (In fact, the effects of hearing loss on spectral processing in the AN as revealed through this analysis appear to be much more complex than in the IC). We also demonstrated that the clustering of dynamical trajectories that we observed in the latent representation of the IC DNN model was also evident in experimental IC responses. These new analyses are described in Figure 4 —figure supplement 1 of the revised manuscript.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Suggestions for authors:</p><p>Figure 1 caption – why are there two sample sizes given in the last sentence?</p></disp-quote><p>It is just one (large) sample size: n = 544,362.</p><disp-quote content-type="editor-comment"><p>Line 83 – you might give some flavor as to the noise types that were used</p></disp-quote><p>This information is in the Methods.</p><disp-quote content-type="editor-comment"><p>Line 111 and Figure 2 – I must be missing something, but I don't see how you can approach 100% explained signal variance given the way that I think it is calculated. Doesn't the noise variance show up in the denominator?</p></disp-quote><p>With the full complement of PCs, all of the variance in a dataset can be fully explained. So, in general, it should be no surprise that a subset of that variance can also be fully explained; in fact, it would be impossible for this not to be the case. It is, however, potentially surprising that so much of the signal variance can be explained with so few PCs.</p><disp-quote content-type="editor-comment"><p>I don't understand what constitutes a &quot;recorded unit&quot;. The methods refer to multi-unit activity. Is a unit just an electrode from the array of 512? Or is there spike sorting being performed? How is any non-stationarity in the recordings dealt with (e.g. if neurons die, or the brain moves a little w.r.t. the electrode array)?</p></disp-quote><p>Spike sorting was not performed, but the extraction of multi-unit activity involved some processing. This is described in the Methods. Non-stationarity was ignored.</p><disp-quote content-type="editor-comment"><p>Line 287 – I found myself wondering about the possible effect of phase shifts or increases in response latency, which one might imagine could occur with hearing loss. I think the analysis would be highly vulnerable to this, especially given that the encoding of modulation is partly synchrony-based. The fact that the modulation analysis shows pretty similar results for NH and HI suggests there is not much of this, but some readers may wonder about this.</p></disp-quote><p>RSA is insensitive to phase shifts; CCA is not. This difference is part of the motivation for using both methods rather than just one or the other.</p><disp-quote content-type="editor-comment"><p>At several points throughout the paper, I found myself wondering about the effects of compression. I would have been greatly interested to see an analysis that separately manipulated compression (e.g., turning it off), to see how much benefit it produces on restoring normal-like responses. I also would have liked to see some discussion of its effects.</p></disp-quote><p>We did an extensive analysis of the effects of compression on the neural coding of speech in a previous paper (Armstrong et al., Nat Biomed Eng., 2022).</p><disp-quote content-type="editor-comment"><p>Figure 7d – this panel is pretty confusing, partly because the SPL numbers are inside particular plots, so it is not completely clear what they apply to, and partly because the little numbers in the plots are not labeled or defined anywhere.</p></disp-quote><p>We understand that this figure can be confusing, but the formatting and labelling are exactly the same as in all of the previous figures, e.g., 4d and 5d. We tried several other designs for this figure, but none were judged to be better.</p><disp-quote content-type="editor-comment"><p>Line 404-406 – how does this jive with the findings of distorted tonotopy from the Heinz lab?</p></disp-quote><p>The relationship between our findings and the distorted tonotopy that has been observed in the auditory nerve is considered in detail in the Discussion. To our knowledge, the potential for (frequency-weighted) amplification to mitigate the effects of distorted tonotopy on speech coding at the level of the auditory nerve has not been tested.</p><disp-quote content-type="editor-comment"><p>Line 438 – why is this coherence rather than correlation?</p></disp-quote><p>As used here, they are equal. We have relabeled as correlation since that is likely to be more familiar to readers.</p><disp-quote content-type="editor-comment"><p>Line 484 and earlier – can the clustering be explained merely by audibility (e.g., all the stimuli that are inaudible cluster together, for the uninteresting reason that they do not evoke a response)?</p></disp-quote><p>That would be possible, but (1) there is also clustering of responses to low-frequency tones that evoke strong responses and (2) the high-frequency tones do, in fact, elicit a response (see Figure 4d).</p><disp-quote content-type="editor-comment"><p>Line 496 – the claim here needs a reference</p></disp-quote><p>The relevant papers are cited throughout the paragraph.</p><disp-quote content-type="editor-comment"><p>Line 691 – why is &quot;/ 0.6745&quot; here? Is this a typo?</p></disp-quote><p>No. That is the scaling factor required to transform an estimate of mean absolute deviation into an estimate of standard deviation.</p><p>(See https://en.wikipedia.org/wiki/Median_absolute_deviation)</p><disp-quote content-type="editor-comment"><p>Line 697 – what is a &quot;unit&quot;?</p></disp-quote><p>The MUA-processed signal from one recording channel.</p><disp-quote content-type="editor-comment"><p>Line 784 – it seems plausible that the hearing aid settings are suboptimal. In particular, the extent of compression is based on some informal optimization in humans. Could this partly explain the less than complete restoration of normal responses?</p></disp-quote><p>We agree that it is plausible that the hearing aid settings are suboptimal. But we think it is unlikely that this suboptimal fitting is the main reason why hearing aids are unable to restore neural responses to normal. We did an extensive analysis of the effects of hearing aids on the neural coding of speech in a previous paper (Armstrong et al., Nat Biomed Eng., 2022).</p></body></sub-article></article>