<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">98489</article-id><article-id pub-id-type="doi">10.7554/eLife.98489</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98489.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Advance</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The recurrent temporal restricted Boltzmann machine captures neural assembly dynamics in whole-brain activity</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Quiroz Monnens</surname><given-names>Sebastian</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">‚Ä†</xref><xref ref-type="fn" rid="pa1">‚Ä°</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Peters</surname><given-names>Casper</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">‚Ä†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Hesselink</surname><given-names>Luuk Willem</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0003-0735-9913</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">‚Ä†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Smeets</surname><given-names>Kasper</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0008-0621-3118</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa2">¬ß</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Englitz</surname><given-names>Bernhard</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9106-0356</contrib-id><email>bernhard.englitz@donders.ru.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Computational Neuroscience Lab, Donders Center for Neuroscience, Radboud University</institution></institution-wrap><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>‚Ä†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="present-address" id="pa1"><label>‚Ä°</label><p>Center for Neurogenomics andCognitive Research, VrijeUniversiteit Amsterdam, Amsterdam, Netherlands</p></fn><fn fn-type="present-address" id="pa2"><label>¬ß</label><p>√âcole Polytechnique F√©d√©rale deLausanne, Lausanne, Switzerland</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>05</day><month>11</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP98489</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-04-18"><day>18</day><month>04</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-02-04"><day>04</day><month>02</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.02.02.578570"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-06-28"><day>28</day><month>06</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98489.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-10-01"><day>01</day><month>10</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98489.2"/></event></pub-history><permissions><copyright-statement>¬© 2024, Quiroz Monnens, Peters, Hesselink et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Quiroz Monnens, Peters, Hesselink et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-98489-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-98489-figures-v1.pdf"/><related-article related-article-type="article-reference" ext-link-type="doi" xlink:href="10.7554/eLife.83139" id="ra1"/><abstract><p>Animal behaviour alternates between stochastic exploration and goal-directed actions, which are generated by the underlying neural dynamics. Previously, we demonstrated that the compositional Restricted Boltzmann Machine (cRBM) can decompose whole-brain activity of larval zebrafish data at the neural level into a small number (‚àº100-200) of assemblies that can account for the stochasticity of the neural activity (van der Plas et al., eLife, 2023). Here, we advance this representation by extending to a combined stochastic-dynamical representation to account for both aspects using the recurrent temporal RBM (RTRBM) and transfer-learning based on the cRBM estimate. We demonstrate that the functional advantage of the RTRBM is captured in the temporal weights on the hidden units, representing neural assemblies, for both simulated and experimental data. Our results show that the temporal expansion outperforms the stochastic-only cRBM in terms of generalization error and achieves a more accurate representation of the moments in time. Lastly, we demonstrate that we can identify the original time-scale of assembly dynamics by estimating multiple RTRBMs at different temporal resolutions. Together, we propose that RTRBMs are a valuable tool for capturing the combined stochastic and time-predictive dynamics of large-scale data sets.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>dynamical model</kwd><kwd>:arge-scale neural recording</kwd><kwd>statistical model</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Zebrafish</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>016.Vidi.189.052</award-id><principal-award-recipient><name><surname>Englitz</surname><given-names>Bernhard</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>EN919/1-1</award-id><principal-award-recipient><name><surname>Englitz</surname><given-names>Bernhard</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The recurrent temporal restricted Boltzmann machine applied to whole-brain neuronal recordings from larval zebrafish brains provides substantial advantages in discovering the neuronal assembly structure and their connectivity.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>When large groups of neurons exhibit joint activity, they are often assumed to form a functional unit, referred to as a neural assembly (<xref ref-type="bibr" rid="bib18">Harris, 2005</xref>). Neural assemblies are thought to form elementary computational units that are essential for cognitive functions such as short-term memory, sensorimotor computation, and decision-making (<xref ref-type="bibr" rid="bib18">Harris, 2005</xref>; <xref ref-type="bibr" rid="bib20">Hebb, 1949</xref>; <xref ref-type="bibr" rid="bib16">Gerstein et al., 1989</xref>). Recent advancements in neuroimaging methods now enable us to study the role of these neural assemblies in more detail. For example, a large breakthrough is the introduction of light-sheet microscopy which enables functional recordings of whole-brain volumes, thereby allowing the study of how complex computation emerges in the brain (<xref ref-type="bibr" rid="bib2">Ahrens et al., 2013</xref>). It, however, remains a computational challenge to extract neural activation patterns from such datasets comprising ‚àº100.000 neurons or more.</p><p>Recent work leveraged the compositional restricted Boltzmann machine (cRBM) to identify such neural assemblies in large-scale neural data (<xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>). The cRBM is an extension of the restricted Boltzmann machine (RBM) (<xref ref-type="bibr" rid="bib36">Smolensky, 1986</xref>), an undirected graphical model that consists of two layers of random variables, representing the data itself (through a set of visible units) and a lower-dimensional latent representation (through a set of hidden units). The model learns in an unsupervised manner by matching its model distribution to the empirical distribution of the data through maximum likelihood optimization (<xref ref-type="bibr" rid="bib41">Tubiana et al., 2019a</xref>; <xref ref-type="bibr" rid="bib35">Salakhutdinov et al., 2007</xref>). The cRBM extends the classical RBM by adhering to a set of structural conditions (see Materials and methods), pushing it to operate in a state referred to as the compositional phase (<xref ref-type="bibr" rid="bib40">Tubiana and Monasson, 2017</xref>; <xref ref-type="bibr" rid="bib41">Tubiana et al., 2019a</xref>; <xref ref-type="bibr" rid="bib42">Tubiana et al., 2019b</xref>). In the compositional phase, the visible-to-hidden connections in the RBM are sparse, and in our previous work (<xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>) this Research Advance is based on, we show that the associated neural assemblies of the hidden units are localized and span the entire space of the visible units. Furthermore, only a small fraction of the hidden units are active at any point in time, improving model interpretability.</p><p>Although the cRBM can accurately reproduce neural statistics and produce a low-dimensional representation of the high-dimensional neural data, this model is limited in capturing only static dependencies and is unable to specifically account for temporal dependencies. Neural activity driving animal behavior is expressed in both stochastic and deterministic states, thus requiring dynamics to be explicitly included to capture most of the variance in the data. To tackle this problem, we here include temporal dependencies directly into the model by applying the recurrent temporal RBM (RTRBM) (<xref ref-type="bibr" rid="bib38">Sutskever et al., 2008</xref>). We utilize a type of transfer learning to retain the sparsity advantages of the cRBM, while the model can additionally account for the deterministic dynamics underlying the neural activity it‚Äôs trained on.</p><p>In short, the RTRBM is a recurrent neural network constructed by chaining multiple RBMs in time (<xref ref-type="bibr" rid="bib27">Mittelman et al., 2014</xref>). Each RBM has a hidden state that is conditioned on the expected hidden state of the RBM at the preceding time-step. While temporal connections are constrained to single time-steps, the recurrency in the model indirectly accounts for multi-time-step dependencies. Previous studies using RTRBMs in other domains have highlighted the value of including such temporal dependencies in extracting spatiotemporal features from high-dimensional data (<xref ref-type="bibr" rid="bib9">Boulanger-Lewandowski et al., 2012</xref>; <xref ref-type="bibr" rid="bib25">Li et al., 2018</xref>; <xref ref-type="bibr" rid="bib48">Zhang et al., 2018</xref>). A more detailed discussion on the RTRBM and its implementation can be found in RTRBM.</p><p>Results In this work, we apply the RTRBM to both simulated and real data. First, we show that the model is capable of retrieving the artificial neural assemblies and their temporal connections in a fully simulated networks with only a few hidden populations. We compare the resulting RTRBM with an RBM that is trained on the same data, and shows that it outperforms in terms of generalization error, pairwise moments, and time-shifted pairwise moments. We then use a combined approach of the RTRBM and the cRBM to model the temporal connections of different neural assemblies in whole-brain neuronal zebrafish data through transfer learning by initializing RTRBM weights with their cRBM counterparts. The resulting RTRBM model extends upon the neural assemblies identified by the cRBM models by additionally capturing their temporal dependencies. We demonstrate that this extension improves the reconstructive power in terms of the estimated moments and provides additional temporal information regarding the underlying structure of the brain.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In this work, we investigated whether the inclusion of temporal dependencies between neural assemblies improve the representation of neuronal activity in the context of simulated data and whole-brain light-sheet recordings from zebrafish larvae (<inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf2"><mml:mrow><mml:mn>40709</mml:mn><mml:mo>¬±</mml:mo><mml:mn>13854</mml:mn></mml:mrow></mml:math></inline-formula> neurons, <xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>). For this purpose, we first introduce and compare the (RTRBM) to the compositional RBM (<xref ref-type="bibr" rid="bib41">Tubiana et al., 2019a</xref>) and then use two-step transfer learning to arrive at an estimate of the RTRBM that stays in the compositional phase and maintains the locally restricted assemblies identified by the cRBM.</p><sec id="s2-1"><title>The RTRBM extends the RBM by temporal dynamics on the assembly level</title><p>The principal structural difference between the RBM and the RTRBM is the addition of recurrent connections through a set of weights <inline-formula><mml:math id="inf3"><mml:mi>U</mml:mi></mml:math></inline-formula> in the RTRBM that connect the hidden unit states at time-steps <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:mi>t</mml:mi></mml:math></inline-formula>. These connections allow the RTRBM to incorporate temporal dynamics from the data, while the (c)RBM is only able to represent time-independent, statistical relationships. This difference is illustrated in a small example of neural assemblies in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The recurrent temporal RBM (RTRBM) extends the restricted Boltzmann machine (RBM) by additionally accounting for temporal interactions of the neural assemblies.</title><p>(<bold>A</bold>) A schematic depiction of an RBM with visible units (neurons) on the left, and hidden units (neural assemblies) on the right. The visible and hidden units are connected through a set of weights <inline-formula><mml:math id="inf6"><mml:mi>W</mml:mi></mml:math></inline-formula>. (<bold>B</bold>) An example <inline-formula><mml:math id="inf7"><mml:mi>W</mml:mi></mml:math></inline-formula> matrix where a subset of visible units is connected to one hidden unit. Details of the equations in panel B and E are given in Materials and methods. (<bold>C</bold>) Hidden and visible activity traces were generated by sampling from the RBM. Due to its static nature, the RBM samples do not exhibit any sequential activation pattern, but merely show a stochastic exploration of the population activity patterns. (<bold>D</bold>) Schematic depiction of an RTRBM. The RTRBM formulation matches the static connectivity of the RBM, but extends it with the weight matrix <inline-formula><mml:math id="inf8"><mml:mi>U</mml:mi></mml:math></inline-formula> to model temporal dependencies between the hidden units. (<bold>E</bold>) In the present example, assembly 1 excites assembly 2, assembly 2 excites assembly 3, and assembly 3 excites assembly 1, while the remaining connections were set to 0. (<bold>F</bold>) Hidden and visible activity traces were generated by sampling from the RTRBM. In contrast to the RBM samples, the RTRBM generates samples featuring a sequential firing pattern. It is able to do so due to the temporal weight matrix <inline-formula><mml:math id="inf9"><mml:mi>U</mml:mi></mml:math></inline-formula> which enables modeling temporal dependencies.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98489-fig1-v1.tif"/></fig><p>When applying the RBM to neural data, the neurons are represented by the <italic>visible units</italic>, while the underlying neural assemblies are represented by the <italic>hidden units</italic> (see <xref ref-type="fig" rid="fig1">Figure 1</xref> for a visualization). In this basic example, we connect each assembly to an exclusive set of neurons for simplicity (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In the case of the RTRBM, there are additional, direct connections between the assemblies (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, red arrows, defined by the weights <inline-formula><mml:math id="inf10"><mml:mi>U</mml:mi></mml:math></inline-formula>). To emphasize the resulting difference in temporal dynamics, we initialize an RBM and an RTRBM model with matching hidden to visible connections <inline-formula><mml:math id="inf11"><mml:mi>W</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1B and E</xref>). We set these connections up so that each neural assembly is connected to a single hidden unit. In this manner, the hidden unit dynamics act as assembly dynamics, where each hidden unit represents a distinct assembly. In addition, the RTRBM has a weight matrix <inline-formula><mml:math id="inf12"><mml:mi>U</mml:mi></mml:math></inline-formula> that models the inter-assembly temporal dynamics. The activation function applied to the sum of the inputs is a sigmoid function, which enforces the output to be in the range <inline-formula><mml:math id="inf13"><mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. The weights that connect the visible to the hidden units outside their assigned assembly must, therefore, be negative to prevent their participation in other assemblies.</p><p>To demonstrate the resulting difference in temporal dynamics, we sample from the RBM and RTRBM and compare the resulting activity traces. In the RBM, as expected from the model definition, the stochastic sampling between the hidden and visible units does not lead to systematic sequential activations of the assemblies occur, aside from some persistence due to the reactivation of similar ensembles through the weights <inline-formula><mml:math id="inf14"><mml:mi>W</mml:mi></mml:math></inline-formula>.</p><p>Conversely for the RTRBM, a combination of temporal sequences and stochastic exploration can be realized: In this simple example of an RTRBM, assembly 1 excites assembly 2, assembly 2 excites assembly 3 and assembly 3 again excites assembly 1 (see <xref ref-type="fig" rid="fig1">Figure 1E</xref>). As a result, the hidden activity traces of data sampled from the RTRBM show matching sequential activation of hidden units (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). Because each hidden unit is connected to a subset of visible units, the results in a sequential activation of the assemblies, where typically only one assembly is strongly active at each time-step. As the representation of the RTRBM is still probabilistic, the dynamics display a mixture of dynamic, and stochastic properties, which we consider a hallmark of neural activity.</p><p>In summary, the RTRBM maintains the features of the RBM to provide an interpretable and probabilistic representation of neural data, but extends it to include temporal dependencies between neural assemblies.</p></sec><sec id="s2-2"><title>RTRBMs learn assembly dynamics from simulated neural data</title><p>In the above example, the neural assembly connectivity was predefined. Next, we demonstrate that the RTRBM can be trained on simulated neural data to learn a set of weights <inline-formula><mml:math id="inf15"><mml:mi>W</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:mi>U</mml:mi></mml:math></inline-formula> that correctly captures the underlying temporal dynamics on the assembly level. Initially, we aimed to compare the performance of the cRBM with the cRTRBM. However, we did not manage to get the RTRBM to reach the compositional phase. To ensure a fair and robust comparison, we opted to compare the RBM with the RTRBM. In this test case, we indeed find the RTRBM to outperform the RBM in the representation of the underlying moments.</p><p>We devised a method for generating artificial data sets mimicking neural population activity using a simplified neural network model. Here, neural activity is driven by the population activity of underlying neural assemblies. These activities of assemblies were determined by two factors: endogenous, assembly-specific activations, and recurrent activations through the connections between assemblies (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, left). The activity of the neurons was then generated from a Poisson process whose time-dependent rate was given by the activations of a single assembly population. For clarity of the presentation, we here again implement a direct match between assemblies and neurons, and thus expect the estimated weight matrix to be a ‚Äòdiagonal‚Äô matrix between assemblies and neurons.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The recurrent temporal RBM (RTRBM) outperforms the restricted Boltzmann machine (RBM) on sequential statistics on simulated data.</title><p>(<bold>A</bold>) Simulated data generation: Hidden Units (<inline-formula><mml:math id="inf17"><mml:msub><mml:mi>N</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula>) interact over time to generate firing rate traces which are used to sample a Poisson train. For example, assembly 1 drives assembly 2 and inhibits assembly 10, both at a single time-step delay. (<bold>B</bold>) Schematic depiction of the RBM and RTRBM trained on the simulated data. (<bold>C</bold>) For the RBM, the aligned estimated weight matrix <inline-formula><mml:math id="inf18"><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> contains spurious off-diagonal weights, while the RTRBM identifies the correct diagonal structure (top). For the assembly weights <inline-formula><mml:math id="inf19"><mml:mi>U</mml:mi></mml:math></inline-formula> (left), the RTRBM also converges to similar aligned estimated temporal weights <inline-formula><mml:math id="inf20"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> (right). (<bold>D</bold>) The RTRBM attributes only a single strong weight to each visible unit ((<inline-formula><mml:math id="inf21"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn><mml:mi>œÉ</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf22"><mml:mi>œÉ</mml:mi></mml:math></inline-formula> is the standard deviation of <inline-formula><mml:math id="inf23"><mml:mi>W</mml:mi></mml:math></inline-formula>)), consistent with the specification in <inline-formula><mml:math id="inf24"><mml:mi>W</mml:mi></mml:math></inline-formula>, while in the RBM multiple significant weights get assigned per visible units. (<bold>E</bold>) The RBM and RTRBM perform similarly for concurrent (<inline-formula><mml:math id="inf25"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf26"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>) statistics, but the RTRBM provides more accurate estimates for sequential (<inline-formula><mml:math id="inf27"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>v</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf28"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>) statistics. In all panels, the abscissa refers to the data statistics in the test set, while the ordinate shows data sampled from the two models,, respectively. (<bold>F</bold>) The trained RTRBM and the RBM yield similar concurrent moments, but the RTRBM significantly outperformed the RBM on time-shifted moments (see text for details on statistics). (<bold>G</bold>) The RTRBM achieved significantly lower normalized mean squared error (nMSE) when predicting ahead in time from the current state in comparison to RBMs for up to four time-steps.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98489-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2‚Äîfigure supplement 1.</label><caption><title>Alignment of weight matrices after learning.</title><p>An alignment procedure is used to be able to enable comparison between the estimated temporal weight matrix <inline-formula><mml:math id="inf29"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> and the original temporal matrix used to generate the data. Details on the alignment procedure can be found in alignment of the estimated temporal weight matrix. (<bold>A</bold>) The link between hidden units and assemblies of visible units can often be clearly identified within the weight matrix, making it possible to define an ordering of the hidden units such that it is in line with the ordering of the assemblies. This leads to a shuffling of the rows in the <inline-formula><mml:math id="inf30"><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> matrix. The sign of the mean weight between an assembly and its matched hidden unit is used to identify inverse relations. (<bold>B</bold>) The reordering of the hidden units leads to a reshuffling of both the rows and the columns of the <inline-formula><mml:math id="inf31"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> matrix. The sign of an entry is switched when the two hidden units have an opposing sign to their matched assembly of visible units.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98489-fig2-figsupp1-v1.tif"/></fig></fig-group><p>An RBM (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) trained on the simulated data recovers a non-diagonal weight matrix (<xref ref-type="fig" rid="fig2">Figure 2C</xref>), which is composed of both the true assembly-to-neuron weights on the diagonal, but in addition has multiple off-diagonal weights, which partially account for the dependencies between the assemblies.</p><p>In contrast, the RTRBM correctly segments all ten assemblies, recovering a clean ‚Äòdiagonal‚Äô estimated connectivity matrix <inline-formula><mml:math id="inf32"><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, right), in addition to providing a close estimate <inline-formula><mml:math id="inf33"><mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> to the true assembly connectivity matrix <inline-formula><mml:math id="inf34"><mml:mi>U</mml:mi></mml:math></inline-formula>, i.e., it recovers the underlying hidden connections from the activation patterns of neurons alone. Consistently, each visible neuron has only a single dominant weight (<inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>|</mml:mi><mml:msub><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>|</mml:mi></mml:mrow></mml:math></inline-formula>) in the RTRBM and thus produces a diagonal weight matrix, while the RBM assigns multiple strong weights to an RBM to address the time-dependencies (<xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p><p>As expected, the RBM performs very well in capturing the average activations of the visible units (<inline-formula><mml:math id="inf36"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>) and their correlations (<inline-formula><mml:math id="inf37"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>), referred to as first and second order moments, respectively (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, top). However, it cannot accurately capture the time-shifted moments of the visible or hidden units (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, bottom). The RTRBM performs similarly for the simultaneous moments (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, top), but provides a more accurate account of the time-shifted moments (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, bottom). This behavior is consistent for the different moments across multiple runs on independent simulated data sets and model estimates (<inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig2">Figure 2F</xref>), with significant improvements of the RTRBM observed for the time-shifted moments (p-values 0.993 for <inline-formula><mml:math id="inf39"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, 0.312 for <inline-formula><mml:math id="inf40"><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf41"><mml:mrow><mml:mn>9.13</mml:mn><mml:mo>‚ãÖ</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo lspace="0em" rspace="0em">‚àí</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf42"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:msubsup><mml:mi>v</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf43"><mml:mrow><mml:mn>4.55</mml:mn><mml:mo>‚ãÖ</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo lspace="0em" rspace="0em">‚àí</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf44"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, one-sided Mann-Whitney U test).</p><p>Lastly, the RTRBM also exhibited a significantly lower normalized mean squared error (nMSE, see Materials and methods) (<inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>4.1</mml:mn><mml:mo>‚ãÖ</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo lspace="0em" rspace="0em">‚àí</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, two-way ANOVA with time-steps and model type as factors, <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig2">Figure 2G</xref>) when predicting ahead in time inside the simulated data (not used in training). The RTRBM‚Äôs advantage in prediction stayed significant up to four time-steps (<inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, two group t-tests per time-step with Bonferroni correction for the number of time-steps). This decay of differences between the models is expected, as the probabilistic basis of the RBM/RTRBM as well as the simulated data by design leads to non-deterministic trajectories, similar to the divergence of trajectories in non-linear dynamic systems where small noise eventually leads to large differences (<xref ref-type="bibr" rid="bib37">Strogatz, 2000</xref>) (see discussion for a relation to animal behavior).</p><p>These results indicate that the RTRBM provides a more accurate account of the model structure and data statistics in particular for sequential activations. The RBM can partially account for the temporal structure, but only by conflating it with its non-temporal weights in <inline-formula><mml:math id="inf48"><mml:mi>W</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s2-3"><title>The RTRBM outperforms the cRBM on whole-brain zebrafish data</title><p>Next, we trained the RTRBM on whole-brain data recorded in zebrafish larvae (<inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula>, same data as in <xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>). To obtain binarized spike traces that can be used by the RTRBM, the individual fluorescence traces were deconvolved by means of blind sparse deconvolution (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Model training was performed using a trained cRBM as the basis for the assembly-to-neuron weights <inline-formula><mml:math id="inf50"><mml:mi>W</mml:mi></mml:math></inline-formula>, and then training the temporal assembly-to-assembly weights <inline-formula><mml:math id="inf51"><mml:mi>U</mml:mi></mml:math></inline-formula>, while allowing <inline-formula><mml:math id="inf52"><mml:mi>W</mml:mi></mml:math></inline-formula> to only change slightly (the learning rate for these weights is reduced by two orders of magnitude, see Materials and methods for more details on model training). For each animal, model training was successful and the weight changes converged to small values. This approach of using pre-learned weights can be seen as a variant of transfer learning (<xref ref-type="bibr" rid="bib39">Tan et al., 2018</xref>). We chose for this training procedure as the weight matrix <inline-formula><mml:math id="inf53"><mml:mi>W</mml:mi></mml:math></inline-formula> inferred by the RTRBM is rarely able to identify localized receptive fields for a large portion of hidden units within its present, non-compositional, formulation.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Recurrent temporal RBM (RTRBM) often outperforms the compositional restricted Boltzmann machine (cRBM) on zebrafish data.</title><p>(<bold>A</bold>) Whole-brain neural activity of larval zebrafish was imaged via Calcium-indicators using light-sheet microscopy at single neuron resolution (left). Calcium activity (middle, blue) is deconvolved by blind, sparse deconvolution to obtain a binarized spike train (middle, black). The binarized neural activity of 1000 randomly chosen neurons (right). (<bold>B</bold>) Left: Distribution of all visible-to-hidden weights. Here, a strong weight is determined by proportional thresholding, <inline-formula><mml:math id="inf54"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf55"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is set such that 5000 neurons have a strong connection towards the hidden layer. Right: log-weight distribution of the visible to hidden connectivity. (<bold>C</bold>) The RTRBM extracts sample assemblies (color indicates assembly) by selecting neurons based on the previously mentioned threshold. Visible units with stronger connections than this threshold for a given hidden unit are included. Temporal connections (inhibitory: blue, excitatory: red) between assemblies are depicted across time-steps. (<bold>D</bold>) Temporal connections between the assemblies are sorted by agglomerative clustering (dashed lines separate clusters, colormap is clamped to <inline-formula><mml:math id="inf56"><mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mo form="prefix" stretchy="false">‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>). Details on the clustering method can be found in Materials and methods. (<bold>E</bold>) Corresponding receptive fields of the clusters identified in (<bold>D</bold>), where the visible units with strong weights are selected similarly to (<bold>B</bold>). The receptive field of cluster 5 has been left out as it contains only a very small number of neurons with strong weights based on the proportional threshold. (<bold>F</bold>) Comparative analysis between the cRBM (bottom row) and RTRBM (top row) on inferred model statistics and data statistics (test dataset). Compared in terms of Spearman correlations and sum square difference. From left to right: the RTRBM significantly outperformed the cRBM on the mean activations <inline-formula><mml:math id="inf57"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:math></inline-formula>), pairwise neuron-neuron interactions <inline-formula><mml:math id="inf59"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:math></inline-formula>), time-shifted pairwise neuron-neuron interactions <inline-formula><mml:math id="inf61"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>v</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:math></inline-formula>), and time-shifted pairwise hidden-hidden interactions <inline-formula><mml:math id="inf63"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:math></inline-formula>) for example fish 4. (<bold>G</bold>) The methodology in panel F is extended to analyze datasets from eight individual fish, each color representing one individual fish. Spearman correlation and the assessment of significant differences between both models are determined using a bootstrap method (see Materials and methods for details).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98489-fig3-v1.tif"/></fig><p>The mean square reconstruction error <inline-formula><mml:math id="inf65"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:munderover><mml:mo stretchy="true">‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>ùêØ</mml:mi><mml:mrow><mml:mtext>data</mml:mtext><mml:mo separator="true">,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>‚àí</mml:mo><mml:msub><mml:mi>ùêØ</mml:mi><mml:mrow><mml:mtext>model</mml:mtext><mml:mo separator="true">,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> in the initial phase of model training was <inline-formula><mml:math id="inf66"><mml:mrow><mml:mo>‚àº</mml:mo><mml:mn>0.40</mml:mn></mml:mrow></mml:math></inline-formula>, the RTRBM was able to reduce this to <inline-formula><mml:math id="inf67"><mml:mrow><mml:mo>‚àº</mml:mo><mml:mn>0.072</mml:mn></mml:mrow></mml:math></inline-formula>, which it achieves predominantly by adjusting the temporal weights. The trained RTRBM model maintained the localised neural assemblies inherited from the cRBM (<xref ref-type="fig" rid="fig3">Figure 3C</xref>) as quantified by a sparse weight distribution (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, left) and a comparable, lower number of typically 1‚Äì3 strong weights per neuron as in the cRBM (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, right).</p><p>Most hidden units showed self-excitation, i.e., indicated as a positive value on the diagonal of <inline-formula><mml:math id="inf68"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula>. The overall pattern of temporal connections between the assemblies in <inline-formula><mml:math id="inf69"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> could be divided into several groups. To this end, agglomerative clustering (for details see Materials and methods) can be applied to the incoming (row) or outgoing (columns) connections of the matrix <inline-formula><mml:math id="inf70"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> to identify assemblies with similar temporal structures. We here focus on the incoming connections/receptive fields, as their grouping was more clear (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, dashed lines indicate boundaries between clusters). The clustering for outgoing connections was similar, however, not identical as <inline-formula><mml:math id="inf71"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> are generally not symmetric, due to the directedness of the temporal connections. The identified clusters showed characteristic patterns of connectivity, e.g., clusters 1 and 5 show a diverse connectivity pattern with relatively strong intra-cluster connections (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). Clusters 2 and 3 exhibit a diverse connectivity pattern as well, but do not show the same strong intra-cluster connectivity. Cluster 4 has very strong recurrent intra-cluster connectivity, but also excites all other clusters. Cluster 5 is dominated by strong inhibitory connectivity to itself and all other clusters. We thus see a range of different connectivity patterns appearing, identified by the clustering method. By using a lower clustering threshold, an even further refined clustering structure appears (data not shown here). The sets of strongly connected visible units corresponding to the hidden unit clusters furthermore formed spatially localized sets of neurons (<xref ref-type="fig" rid="fig3">Figure 3E</xref>).</p><p>To compare the performance between the inferred RTRBM and cRBM models, we analyzed the reconstruction quality. To this end, we sampled data from both models and again compared the model statistics to the data statistics, using an unseen test set (matched to the test set in <xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>, see Materials and methods for details). For the example fish in <xref ref-type="fig" rid="fig3">Figure 3B‚ÄìF</xref>, the first order moments between neurons <inline-formula><mml:math id="inf72"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula> are strongly correlated for the RTRBM (<inline-formula><mml:math id="inf73"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.92</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf74"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf75"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:math></inline-formula> denotes machine precision), that is an improvement compared to the performance of the cRBM (<inline-formula><mml:math id="inf76"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:math></inline-formula>). The second-order moments between neurons <inline-formula><mml:math id="inf78"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula> of the RTRBM (<inline-formula><mml:math id="inf79"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.58</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:math></inline-formula>) also correlates better compared to the cRBM (<inline-formula><mml:math id="inf81"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.27</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:math></inline-formula>). To establish how well both models can capture the temporal dynamics of the data, we compared the time-shifted moments of the visible <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‚ü®</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and hidden <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‚ü®</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> units. The time-shifted moments of the visible units of the RTRBM (<inline-formula><mml:math id="inf85"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.56</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:math></inline-formula>) correlates better than the cRBM (<inline-formula><mml:math id="inf87"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.27</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:math></inline-formula>). While a direct comparison of the hidden unit activations between the cRBM and the RTRBM is hindered by the inherent discrepancy in their activation functions (unbounded and bounded, respectively), the analysis of time-shifted moments reveals a stronger correlation for the RTRBM hidden units (<inline-formula><mml:math id="inf89"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.92</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf90"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:math></inline-formula>) compared to the cRBM (<inline-formula><mml:math id="inf91"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.88</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:math></inline-formula>).</p><p>The Spearman correlation is scale-free, i.e., even if one variable is doubled, the correlation can stay the same. However, in many cases, the sampled RTRBM was much closer to the test data in absolute terms (indicated by densities in <xref ref-type="fig" rid="fig3">Figure 3F</xref> that are closer to the diagonal). To quantify this difference, we also compared the sum square difference (SSD) between the sampled statistics of the RTRBM and cRBM with the statistics of the test set, to determine how well both models accounted for the real data on a quantitative, absolute level. The RTRBM had a lower SSD for all first-, second-, and time-shifted moments compared to the cRBM. This suggests that the statistics of the RTRBM are better matched in an absolute sense, and can be considered as better behaved than the statistics of the cRBM when compared to the test set.</p><p>Over the whole dataset, the RTRBM outperforms the cRBM on 5 out of 8 fish for <inline-formula><mml:math id="inf93"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>, and on 4 out of 8 fish for <inline-formula><mml:math id="inf94"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>, while the performance was similar on the remaining fish, except for the green fish (<xref ref-type="fig" rid="fig3">Figure 3G</xref>). To establish how well both models can capture the temporal dynamics of the data, we compared the <inline-formula><mml:math id="inf95"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>v</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf96"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>. Here, the RTRBM outperformed the cRBM on 4 out of 8 fish for the visible units and on 6 out of 8 fish for the hidden units. Specifically, the performance of the RTRBM is consistently improved compared to the cRBM for the same fish across the different moments (<xref ref-type="fig" rid="fig3">Figure 3G</xref>). Note, that the second order statistics are statistics the algorithms are not explicitly trained on to replicate (see also Materials and methods).</p><p>In summary, the transfer learning approach was able to successfully expand the cRBM model to include the temporal connections, while maintaining a high level of sparsity in the hidden-to-visible layer connections. It is beyond the scope of this study to evaluate the detailed differences between the two models, but this transfer learning approach appears a promising avenue to enable the RTRBM to be estimated on large scale data sets.</p></sec><sec id="s2-4"><title>Identification of the underlying time-scale of assembly interactions</title><p>The sampling rate in an experiment will generally not match the effective interaction time between neural assemblies. If the mismatch is too large, it may prevent the RTRBM from making a correct estimate of the temporal connections. It is therefore important to be able to estimate the interaction time of assemblies in relation to the sampling rate.</p><p>To investigate this issue, simulated data was generated where the interaction time between assemblies was set to <inline-formula><mml:math id="inf97"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Œî</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> time-steps (relative to the sampling rate of the simulated data, <xref ref-type="fig" rid="fig4">Figure 4A</xref>, left). This simulated data was then down-sampled to different rates using integer steps on the range <inline-formula><mml:math id="inf98"><mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>10</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, middle). Down-sampling was performed by selecting the value of the data at the sampling interval, rather than averaging or interpolating over all points in the interval. This choice was motivated by the fact that light-sheet imaging only has access to the neural activity at particular time-points and cannot average the entire duration between these time-points (as it is imaging at different planes in depth in between). As the size of the system should not make a qualitative difference for this analysis, we generated simulated data with only <inline-formula><mml:math id="inf99"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> hidden assemblies and <inline-formula><mml:math id="inf100"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> visible units per assembly. Different runs (<inline-formula><mml:math id="inf101"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>) were inferred on independently drawn assembly dynamics and subsequently drawn spike-times, but with identical temporal and static weight matrices <inline-formula><mml:math id="inf102"><mml:mi>W</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf103"><mml:mi>U</mml:mi></mml:math></inline-formula>.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Neural interaction timescale can be identified via recurrent temporal RBM (RTRBM) estimates over multiple timescales.</title><p>(<bold>A</bold>) Training paradigm. Simulated data is generated as in <xref ref-type="fig" rid="fig2">Figure 2</xref>, but with temporal interactions between populations at a delay of <inline-formula><mml:math id="inf104"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Œî</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> time-steps. This data is downsampled according to a downsampling rate <inline-formula><mml:math id="inf105"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Œî</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> by taking every <inline-formula><mml:math id="inf106"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Œî</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>-th time-step (shown here is <inline-formula><mml:math id="inf107"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Œî</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>), and used for training different RTRBMs. (<bold>B</bold>) Performance of the RTRBM for various down-sampling rates measured as the normalized mean squared error (MSE) in predicting the visible units one time-step ahead (<inline-formula><mml:math id="inf108"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> models per <inline-formula><mml:math id="inf109"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Œî</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>). Dotted line shows the mean estimate of the lower bound ¬± SEM (<inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10,000</mml:mn></mml:mrow></mml:math></inline-formula>) due to inherent variance in the way the data is generated (see Materials and methods). Dashed gray line indicates the theoretical performance of an uninformed, unbiased estimator <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msubsup><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true" form="postfix">‚ü©</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>C</bold>) Cosine similarity between the interaction matrix <inline-formula><mml:math id="inf112"><mml:mi>U</mml:mi></mml:math></inline-formula> and the aligned learned matrices <inline-formula><mml:math id="inf113"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula>, both z-scored. Bars and errorbars show mean and standard deviation, respectively, across the <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> models per <inline-formula><mml:math id="inf115"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Œî</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Dark lines show absolute values of the mean cosine similarity. Shown above are the <inline-formula><mml:math id="inf116"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> matrices with the largest absolute cosine similarity per down-sampling rate. (<bold>D</bold>) The same procedure as in (<bold>A</bold>) is performed on neural data in order to find the effect of down-sampling here. (<bold>E</bold>) Spearman correlation of three important model statistics across different down-sampling rates for neural data from example fish 4, similar to <xref ref-type="fig" rid="fig3">Figure 3F</xref>. Dots and shaded areas indicate mean and two times standard deviation, determined using a bootstrap method (see Materials and methods for details).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98489-fig4-v1.tif"/></fig><p>To evaluate the reconstruction performance of the trained RTRBMs, the reconstruction nMSE one time-step ahead is calculated on a single test data set and compared for different down-sampling rates (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, see Materials and methods for details). The trained RTRBM performs significantly better than an unbiased random estimator, i.e., <inline-formula><mml:math id="inf117"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msubsup><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true" form="postfix">‚ü©</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (nMSE <inline-formula><mml:math id="inf118"><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>), when the down-sampling rate is a multiple of the simulated interaction time (<inline-formula><mml:math id="inf119"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mi>.</mml:mi><mml:mn>0098</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf120"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Œî</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>‚àà</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">{</mml:mo><mml:mn>4</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>8</mml:mn><mml:mo fence="true" form="postfix">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, one-sided Wilcoxon signed rank test, <inline-formula><mml:math id="inf121"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> with Bonferroni correction for the number of down-sampling rates, effect size <inline-formula><mml:math id="inf122"><mml:mrow><mml:mo>‚â•</mml:mo><mml:mn>8.93</mml:mn></mml:mrow></mml:math></inline-formula>). The performance is best when the down-sampling factor matches this interaction time at <inline-formula><mml:math id="inf123"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Œî</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf124"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.00082</mml:mn></mml:mrow></mml:math></inline-formula>, one-sided Wilcoxon rank-sum test, <inline-formula><mml:math id="inf125"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> with Bonferroni correction for the number of comparisons, effect size <inline-formula><mml:math id="inf126"><mml:mrow><mml:mo>‚â•</mml:mo><mml:mn>7.05</mml:mn></mml:mrow></mml:math></inline-formula>). The optimal estimator at <inline-formula><mml:math id="inf127"><mml:mrow><mml:mtext>nMSE</mml:mtext><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> is obtained from knowing the underlying model precisely, only limited by the unpredictable variance from the random factors of Poisson sampling and intrinsic assembly dynamics (see Materials and methods for details). Visual inspection of the inferred data shows that only the sampled data from <inline-formula><mml:math id="inf128"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Œî</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> contains the characteristic temporal sequences generated by the connections in <inline-formula><mml:math id="inf129"><mml:mi>U</mml:mi></mml:math></inline-formula>.</p><p>To verify that the correct temporal connections between the hidden units are identified, the estimated temporal weights <inline-formula><mml:math id="inf130"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> after alignment of assemblies (see Materials and methods) are compared with the true temporal connections <inline-formula><mml:math id="inf131"><mml:mi>U</mml:mi></mml:math></inline-formula> of the simulated data using cosine similarity (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Correspondingly, the similarity peaks when the down-sampling rate matches the interaction time. While neighboring values around the simulation step size have similar absolute correlations, the correct step size still outperforms them. Conversely, this indicates that it may be sufficient to be close to the true step size in order to correctly estimate the temporal dependence in <inline-formula><mml:math id="inf132"><mml:mi>U</mml:mi></mml:math></inline-formula>.</p><p>We applied this analysis as a refinement to the training of the RTRBM on zebrafish data, and found that the Spearman correlation of important moments between neural activity and model activity was highest at the natural sampling rate (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). Therefore, the analysis in <xref ref-type="fig" rid="fig3">Figure 3</xref> was conducted without down-sampling. Each RTRBM was trained with the same number of gradient updates to ensure a fair comparison. However, due to the down-sampling procedure the amount of training data available is drastically decreased for large down-sampling rates. We did not retrain the cRBM on the downsampled data because the cRBM model does not account for time dependencies.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here, we introduced the RTRBM as a powerful dynamical statistical model for the analysis of large-scale neural data, demonstrating that it can uncover temporal dependencies between neural assemblies. We achieve this through transfer learning on the basis of the static assembly structure estimated by a cRBM trained on the same data (<xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>). The estimated RTRBM models are structurally more fitting and provide more accurate accounts of the activity dynamics than those of the cRBM, as we demonstrate on simulated and experimentally acquired, whole-brain zebrafish data. The resulting temporal connectivity structure on the assembly level provides a compact description of the neural dynamics, which decomposes into dynamical networks of assemblies. Training of an RTRBM/cRBM model can be completed in a few hours on current hardware, and could thus lend itself for within experiment, interventional studies. The RTRBM therefore provides an effective and practically feasible model formalism for accounting for temporal dynamics as well as stochastic properties of whole-brain zebrafish activity.</p><sec id="s3-1"><title>Relation with previous studies on large-scale assemblies</title><p>Estimating functional divisions and connectivity from large-scale activity data can be considered one of the key objectives of computational neuroscience, as it would allow to automatically extract interpretable structure from datasets of (human-level) uninterpretable complexity. While it is generally recognized that this poses a difficult analytical challenge, in particular in highly connected systems (<xref ref-type="bibr" rid="bib13">Das and Fiete, 2020</xref>), whole-brain recordings have brought a critical advance to this endeavor. However, due to the relatively recent introduction of whole-brain measurements in zebrafish larvae (<xref ref-type="bibr" rid="bib2">Ahrens et al., 2013</xref>), surprisingly few studies exist in this system that have attempted the investigation of neural activity in this type of data at the whole-brain scale (<xref ref-type="bibr" rid="bib28">Nguyen et al., 2018</xref>; <xref ref-type="bibr" rid="bib11">Chen et al., 2018</xref>; <xref ref-type="bibr" rid="bib5">Betzel, 2020</xref>; <xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>).</p><p>These studies have all focused on extracting functional groupings from the neural activity, without directly attempting to perform temporal predictions of neural activity. In <xref ref-type="bibr" rid="bib11">Chen et al., 2018</xref>, a clustering approach was introduced that identified a set of clusters of neurons, which showed responses to specific visual stimuli or motor behaviors. (<xref ref-type="bibr" rid="bib5">Betzel, 2020</xref>) estimates instantaneous functional connectivity from spontaneous activity and identifies groups of local nodes that form a hierarchical, modular structure, however, without the possibility of using this model in a generative way. In our previous study (<xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>) we identified neural assemblies from spontaneous activity using a generative, probabilistic approach, i.e., the cRBM, but without explicitly modeling any time-dependencies. Lastly, (<xref ref-type="bibr" rid="bib28">Nguyen et al., 2018</xref>) uses Gaussian mixture modelling to cluster on the activity level, but again, this method does not yield any insight into the time-dependencies between the identified clusters.</p><p>Many other studies have focused on the analysis of subsystems, but also without directly modelling the temporal dependence, e.g., sensorimotor transformations in the visual (<xref ref-type="bibr" rid="bib6">Bianco and Engert, 2015</xref>) and the auditory system (<xref ref-type="bibr" rid="bib33">Privat et al., 2019</xref>), the representation and maintenance of spatial location (<xref ref-type="bibr" rid="bib47">Yang et al., 2022</xref>), decision making (<xref ref-type="bibr" rid="bib3">Bahl and Engert, 2020</xref>), or the neural circuit underlying heading direction (<xref ref-type="bibr" rid="bib32">Petrucco et al., 2023</xref>).</p><p>Since the current temporal resolution of light-sheet imaging is rather low, i.e., limited of a few volumes per second (but alternative scanning approaches might change this soon, see e.g., <xref ref-type="bibr" rid="bib8">Bouchard et al., 2015</xref>), estimating the temporal dynamics on the level of individual neurons is still difficult. Therefore, our present approach focuses on the dynamics between assemblies, which are expected to develop on slower timescales. Temporal connections are generally more insightful than instantaneous function connections, as they are directed and therefore provide a better basis for separating correlation from causation.</p><p>We demonstrate that the RTRBM was able to capture functional temporal connections between neural assemblies while maintaining localized receptive fields of the hidden units. Additionally, as the RTRBM yields insight into the temporal dynamics of these identified neural assemblies, it provides a way of identifying which assemblies are similar in their dynamics and thereby can suggest distinct large-scale dynamical networks spanning one or multiple brain areas (see <xref ref-type="fig" rid="fig3">Figure 3</xref>). Moreover, the RTRBM outperforms the classical RBM on the artificial data in terms of reconstruction statistics, and also outperforms cRBM in accounting for temporal dynamics.</p><p>To our knowledge, only two earlier studies have attempted to predict dynamics or estimate dynamical relations between neural assemblies on the whole brain level (<xref ref-type="bibr" rid="bib46">Watanakeesuntorn et al., 2020</xref>; <xref ref-type="bibr" rid="bib29">Pao et al., 2021</xref>). In both studies Empirical Dynamical Modelling/Convergent Cross Mapping is used to estimate neural dynamics, however, the zebrafish data is mostly utilized as a usage case for demonstrating that the methods scale to large datasets, without providing insight into the resulting ensembles or prediction quality. Another approach used is to apply dynamical modeling of the behavioral level and then use concurrently acquired whole-brain activity to identify corresponding structures in the zebrafish brain (<xref ref-type="bibr" rid="bib14">Dunn et al., 2016</xref>).</p></sec><sec id="s3-2"><title>Limitations and future improvements</title><p>The RTRBM introduces temporal dependencies in a constrained way that effectively limit the number of additional parameters. This feature is important to avoid overfitting on the limited amount of data generally available in each experiment. However, the increased complexity involved with the addition of these temporal dependencies limits the analytical tractability of the model. This added complexity had a number of consequences on the estimation procedure, which should ideally be resolved in future work.</p><p>Specifically, the RTRBM in its current form is not intrinsically driven toward the compositional phase, which is an important property that pushes the model to identify localized neural assemblies. Specifically, the use of dReLU hidden unit potentials within the RTRBM framework was not analytically tractable in our hands. We therefore opted for a transfer learning approach, where the cRBM first estimates the assemblies, and then we initialize the model with these identified assemblies. The RTRBM then infers the temporal connectivity between the identified localized assemblies, while only slightly modifying their assembly structure. This approach could be limiting in multiple ways. For example, the cRBM-estimated assembly structure could contain an amalgam of static and dynamic connectivities (see <xref ref-type="fig" rid="fig2">Figure 2</xref> for simulated data). Furthermore, it might be necessary to estimate the assembly structure jointly with the temporal connectivities between them for optimal decomposition. Extending the current work, we aim to refine the RTRBM by introducing other sparsity constraints on the hidden-hidden connections (similar to <xref ref-type="bibr" rid="bib27">Mittelman et al., 2014</xref>), or by realizing the compositional properties in the RTRBM to allow single-step, direct estimation.</p><p>Another limitation of the current RTRBM framework is that all assemblies are interacting on a single time-scale. While we have demonstrated (<xref ref-type="fig" rid="fig4">Figure 4</xref>) that a single time-scale can be identified through the estimation of and subsequent selection from multiple RTRBMs on different timescales, the more general case of multiple interaction time-scales between different assembles remains unaddressed. Partly, this issue is alleviated by the compounding effect of temporal interactions over multiple time-steps, which, therefore, suggests to err on the side of shorter time-steps in estimation. In preliminary explorations we noticed that the estimated RTRBM generates alternating dynamics between clusters of assemblies (see <xref ref-type="fig" rid="fig3">Figure 3</xref>) on time-scales that are much longer than the single interaction step. In subsequent work, the RTRBM could be generalized to include multiple time-scales of interaction for different assemblies.</p><p>Related to the time-scale issue, light-sheet imaging is currently limited to ‚àº100 Hz, which means that the ‚àº30‚Äì40 imaging planes are sampled at only 2‚Äì4 Hz, depending on the specific system. At these low imaging rates it is likely that some assembly dynamics are missed or appear simultaneous. Improvements in the speed of stepping between imaging planes will increase the sampling rate per cell. Together with brighter fluorescent indicators (<xref ref-type="bibr" rid="bib49">Zhang et al., 2023</xref>), this will provide a more reliable basis for estimating models that incorporate temporal dependencies.</p></sec><sec id="s3-3"><title>Conclusions</title><p>The RTRBM formalism is the logical next step in the analysis of whole-brain recordings, as it accounts for the static and dynamic aspects using a probabilistic formalism, which captures both the stochastic and deterministic aspects that are hallmarks of neural activity. Followup studies need to attempt to extend the RTRBM into the compositional phase directly, thus speeding up learning and ensuring matched assemblies and temporal connectivities (<xref ref-type="bibr" rid="bib4">Bargmann and Marder, 2013</xref>). Recordings at higher temporal resolutions and for longer durations will be instrumental in allowing convergence of the <italic>cRTRBM</italic> (<xref ref-type="bibr" rid="bib21">Helmstaedter, 2015</xref>). Together with advancements in computing hardware this should allow for interventional studies based on the estimated dynamics to directly verify the estimated temporal connectivity through modulation techniques such as optogenetic control or laser ablation.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>The RBM</title><p>The RBM (<xref ref-type="bibr" rid="bib35">Salakhutdinov et al., 2007</xref>) is an undirected graphical model that defines a probability distribution over a set of binary visible units carrying the data configurations <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>ùêØ</mml:mi><mml:mo>‚àà</mml:mo><mml:mo form="prefix" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo form="postfix" stretchy="false">}</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></inline-formula>, and real-valued latent representations are given by a set of hidden units <inline-formula><mml:math id="inf134"><mml:mrow><mml:mi>ùê°</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></inline-formula>. In contrast to the definition of the classical Boltzmann Machine, the RBM has no direct couplings between pairs of units within the same layer, making it a bipartite graph that allows for more efficient model training. The joint probability distribution of the model is defined as:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mtable displaystyle="true" style="width:100%;"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"/><mml:mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mml:mrow><mml:mi>P</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>ùêØ</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>ùê°</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>‚Å°</mml:mo></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mo form="prefix" stretchy="false">‚àí</mml:mo><mml:mi>E</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>ùêØ</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>ùê°</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>‚Å°</mml:mo></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="true">‚àë</mml:mo><mml:mi>i</mml:mi></mml:munder></mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>‚àí</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="true">‚àë</mml:mo><mml:mi>j</mml:mi></mml:munder></mml:mrow><mml:msub><mml:mi class="MJX-tex-caligraphic" mathvariant="script">U</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="true">‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo separator="true">,</mml:mo></mml:mrow></mml:mtd><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are visible-to-hidden unit weights, <inline-formula><mml:math id="inf136"><mml:mi>E</mml:mi></mml:math></inline-formula> is the global energy of the RBM, <inline-formula><mml:math id="inf137"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="true">‚àë</mml:mo><mml:mi>ùêØ</mml:mi></mml:munder></mml:mrow><mml:mo movablelimits="false">‚à´</mml:mo><mml:mi>d</mml:mi><mml:mi>ùê°</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">‚àí</mml:mo><mml:mi>E</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>ùêØ</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>ùê°</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> the partition sum, and <inline-formula><mml:math id="inf138"><mml:mrow><mml:msub><mml:mi class="MJX-tex-caligraphic" mathvariant="script">U</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the hidden unit potential. The choice of the hidden unit potential shapes the energy landscape of the model and thus states the hidden units in the the model can take, making it an important model choice. For a more detailed discussion on the choice of the shape of the hidden unit potential, we refer to our previous paper (<xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>). In this work, we use the Bernoulli hidden unit potential, which is defined as <inline-formula><mml:math id="inf139"><mml:mrow><mml:msub><mml:mi class="MJX-tex-caligraphic" mathvariant="script">U</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">‚àí</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf140"><mml:msub><mml:mi>ùêõ</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula> is the hidden bias and <inline-formula><mml:math id="inf141"><mml:mrow><mml:mi>ùê°</mml:mi><mml:mo>‚àà</mml:mo><mml:mo form="prefix" stretchy="false">{</mml:mo><mml:mn>0,1</mml:mn><mml:msup><mml:mo form="postfix" stretchy="false">}</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf142"><mml:mi>i</mml:mi></mml:math></inline-formula> is used to index the visible units and <inline-formula><mml:math id="inf143"><mml:mi>j</mml:mi></mml:math></inline-formula> to index the hidden units. The full set of model parameters is given by the visible bias <inline-formula><mml:math id="inf144"><mml:mrow><mml:msub><mml:mi>ùêõ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></inline-formula>, the hidden bias <inline-formula><mml:math id="inf145"><mml:mrow><mml:msub><mml:mi>ùêõ</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></inline-formula> and weights <inline-formula><mml:math id="inf146"><mml:mrow><mml:mi>W</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>√ó</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Note that, while the visible units <inline-formula><mml:math id="inf147"><mml:mi>ùêØ</mml:mi></mml:math></inline-formula> are observed variables, the hidden units <inline-formula><mml:math id="inf148"><mml:mi>ùê°</mml:mi></mml:math></inline-formula> are unobserved (latent) variables and must therefore be sampled, conditioned on the state of the visible units <inline-formula><mml:math id="inf149"><mml:mi>ùêØ</mml:mi></mml:math></inline-formula>.</p><p>The posterior distributions over the hidden and visible units allow sequential sampling of the hidden and visible unit states. As both layers contain no within-layer dependencies, the posterior distributions over <inline-formula><mml:math id="inf150"><mml:mi>ùêØ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf151"><mml:mi>ùê°</mml:mi></mml:math></inline-formula> are conditioned only on the other variable and factorize as:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mtable displaystyle="true" style="width:100%;"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"/><mml:mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd style="padding:0.7ex 0em 0.7ex 0em;"><mml:mrow><mml:mi>P</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>ùê°</mml:mi><mml:mo lspace="0.22em" rspace="0.22em" stretchy="false">|</mml:mo><mml:mi>ùêØ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">‚àè</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo lspace="0.22em" rspace="0.22em" stretchy="false">|</mml:mo><mml:mi>ùêØ</mml:mi><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>‚àù</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">‚àè</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mi>exp</mml:mi><mml:mo>‚Å°</mml:mo></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:mo form="prefix" stretchy="false">‚àí</mml:mo><mml:msub><mml:mi class="MJX-tex-caligraphic" mathvariant="script">U</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>‚ãÖ</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="true">‚àë</mml:mo><mml:mi>i</mml:mi></mml:munder></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo separator="true">,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd style="padding:0.7ex 0em 0.7ex 0em;"><mml:mrow><mml:mi>P</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>ùêØ</mml:mi><mml:mo lspace="0.22em" rspace="0.22em" stretchy="false">|</mml:mo><mml:mi>ùê°</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">‚àè</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="0.22em" rspace="0.22em" stretchy="false">|</mml:mo><mml:mi>ùê°</mml:mi><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>‚àù</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">‚àè</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mi>exp</mml:mi><mml:mo>‚Å°</mml:mo></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>‚ãÖ</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="true">‚àë</mml:mo><mml:mi>j</mml:mi></mml:munder></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>The choice of the Bernoulli hidden unit potential reduces these equations to:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mtable displaystyle="true" style="width:100%;"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"/><mml:mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd style="padding:0.7ex 0em 0.7ex 0em;"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo lspace="0.22em" rspace="0.22em" stretchy="false">|</mml:mo><mml:mi>ùêØ</mml:mi><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>œÉ</mml:mi><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="true">‚àë</mml:mo><mml:mi>i</mml:mi></mml:munder></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd style="padding:0.7ex 0em 0.7ex 0em;"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo lspace="0.22em" rspace="0.22em" stretchy="false">|</mml:mo><mml:mi>ùê°</mml:mi><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>œÉ</mml:mi><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="true">‚àë</mml:mo><mml:mi>j</mml:mi></mml:munder></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo separator="true">,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf152"><mml:mi>œÉ</mml:mi></mml:math></inline-formula> denotes the logistic function, defined as <inline-formula><mml:math id="inf153"><mml:mrow><mml:mi>œÉ</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>‚Å°</mml:mo></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mo form="prefix" stretchy="false">‚àí</mml:mo><mml:mi>x</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. For a detailed derivation, we refer to <xref ref-type="bibr" rid="bib17">Goodfellow et al., 2016</xref>.</p></sec><sec id="s4-2"><title>Training the RBM</title><p>The RBM model parameters are learned by maximizing the log-likelihood of the target data, denoted as <inline-formula><mml:math id="inf154"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>‚Å°</mml:mo></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>ùêØ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo><mml:mtext>data</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib1">Ackley et al., 1985</xref>). This learning procedure ensures an accurate representation of the underlying distribution of the target dataset through the Boltzmann-Gibbs energy distribution (<xref ref-type="bibr" rid="bib7">Boltzmann, 1868</xref>). Stochastic gradient-based methods are employed to minimize the Kullback‚ÄìLeibler divergence (<xref ref-type="bibr" rid="bib24">Kullback and Leibler, 1951</xref>) between the model and data distributions. These steps write:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:msub><mml:mi mathvariant="normal">‚àá</mml:mi><mml:mrow><mml:mi>Œ∏</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo>‚àí</mml:mo><mml:msub><mml:mrow><mml:mo>‚ü®</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">‚àá</mml:mi><mml:mrow><mml:mi>Œ∏</mml:mi></mml:mrow></mml:msub><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>‚ü©</mml:mo></mml:mrow><mml:mrow><mml:mtext>data¬†</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mo>‚ü®</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">‚àá</mml:mi><mml:mrow><mml:mi>Œ∏</mml:mi></mml:mrow></mml:msub><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>‚ü©</mml:mo></mml:mrow><mml:mrow><mml:mtext>model¬†</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf155"><mml:mi>E</mml:mi></mml:math></inline-formula> is the energy of the RBM, as given in probrbm. We here let <inline-formula><mml:math id="inf156"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula> denote the expectation value over the data and the model distributions, respectively. As the visible states of the data are given, <inline-formula><mml:math id="inf157"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:mi>E</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>ùêØ</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>ùê°</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo><mml:mtext>data</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> is straightforward to compute. In contrast, the computation <inline-formula><mml:math id="inf158"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:mi>E</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>ùêØ</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>ùê°</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo><mml:mtext>model</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> is intractable for sufficiently large systems due to the exponentially large state space over the visible unit states in the partition sum. To address this problem of intractability, a common solution is the use of a <inline-formula><mml:math id="inf159"><mml:mi>K</mml:mi></mml:math></inline-formula>-step Markov Chain Monte Carlo sampling scheme (Gibbs sampling) (<xref ref-type="bibr" rid="bib22">Hinton, 2002</xref>). This approach starts from an initial configuration and aims to approximate the expectation values of the model distribution. This is also known as contrastive divergence (CD). In this learning scheme, samples are sequentially drawn from the visible and hidden posterior distributions, respectively (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>), up to <inline-formula><mml:math id="inf160"><mml:mi>K</mml:mi></mml:math></inline-formula> time. For sufficiently large values of <inline-formula><mml:math id="inf161"><mml:mi>K</mml:mi></mml:math></inline-formula>, this procedure yields unbiased samples of the underlying model distribution (<xref ref-type="bibr" rid="bib22">Hinton, 2002</xref>). In practice, a value of <inline-formula><mml:math id="inf162"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> during training is generally sufficient to obtain reasonable expectation values (<xref ref-type="bibr" rid="bib10">Carreira-Perpinan and Hinton, 2005</xref>). Increasing the number of samples will result in better approximations, and thus generally in improved model estimation.</p></sec><sec id="s4-3"><title>The compositional phase</title><p>In the classical definition of the RBM training scheme, there is no regularization on the hidden-unit activation sparsity. This means that a visible unit can have a proportionally strong connection to a large set of hidden units. Such a non-localized visible-hidden unit connectivity can hinder the interpretation of the model‚Äôs learned latent representation. Previous work (<xref ref-type="bibr" rid="bib40">Tubiana and Monasson, 2017</xref>; <xref ref-type="bibr" rid="bib41">Tubiana et al., 2019a</xref>) introduced a method to regularize the RBM such that it is pushed towards a sparse visible-hidden unit connectivity, termed the compositional phase. The resulting cRBM was employed to discover compact neural assemblies in our previous study (<xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>). This sparsity in connections allows for a direct and interpretable relationship between the weights in the cRBM and the generated data configurations. The compositional phase is observed when RBMs are constrained to a specified set of structural conditions (<xref ref-type="bibr" rid="bib40">Tubiana and Monasson, 2017</xref>):</p><list list-type="order"><list-item><p>The hidden units are unbound and real-valued with ReLU like activation function.</p></list-item><list-item><p>The weight matrix <inline-formula><mml:math id="inf163"><mml:mi>W</mml:mi></mml:math></inline-formula> is sparse.</p></list-item><list-item><p>The columns of the <inline-formula><mml:math id="inf164"><mml:mi>W</mml:mi></mml:math></inline-formula> have similar norms.</p></list-item></list><p>A key implementation detail of this cRBM model is the use of the double-Rectified Linear Unit (dReLU) defining the hidden-unit potential (<xref ref-type="bibr" rid="bib40">Tubiana and Monasson, 2017</xref>; <xref ref-type="bibr" rid="bib41">Tubiana et al., 2019a</xref>; <xref ref-type="bibr" rid="bib42">Tubiana et al., 2019b</xref>). Detailed analyses of RBMs operating in the compositional phase have exemplified the dynamical consequences and the advantages of this regime for learning complex data manifolds. In-depth discussions on the cRBM and the compositional phase can be found in related literature (<xref ref-type="bibr" rid="bib40">Tubiana and Monasson, 2017</xref>; <xref ref-type="bibr" rid="bib41">Tubiana et al., 2019a</xref>; <xref ref-type="bibr" rid="bib42">Tubiana et al., 2019b</xref>). In this work, we use the implementation from <xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref> for our cRBM model. A detailed description of the implementation and accompanying code can be found there.</p></sec><sec id="s4-4"><title>The recurrent temporal restricted Boltzmann machine (RTRBM)</title><p>The RTRBM can be conceptualized as a series of RBMs unfolded across the temporal dimension, i.e., a recurrent network. The model state of the RTRBM at each time-step <inline-formula><mml:math id="inf165"><mml:mi>t</mml:mi></mml:math></inline-formula> is essentially an RBM, where the hidden state is conditioned on the contextual hidden state of the RBM at time-step <inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Mathematically, this involves augmenting the hidden bias of the RBM at time <inline-formula><mml:math id="inf167"><mml:mi>t</mml:mi></mml:math></inline-formula> with an additional term dependent on the previous expected hidden states <inline-formula><mml:math id="inf168"><mml:mrow><mml:msup><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>ùê≠</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>ùüè</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">‚ü®</mml:mo><mml:msup><mml:mi>ùê°</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>ùê≠</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>ùüè</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msup><mml:mo fence="true" form="postfix">‚ü©</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Furthermore, the weights <inline-formula><mml:math id="inf169"><mml:mrow><mml:mi>U</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>√ó</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are introduced which directly connect the previous to the current hidden unit states. This setup allow the model to directly capture latent-state temporal dependencies that cross multiple time-steps. The statistical distribution of the RTRBM at time-step <inline-formula><mml:math id="inf170"><mml:mi>t</mml:mi></mml:math></inline-formula> is given by:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>‚à£</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>‚Å°</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">‚ä§</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mi>W</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">‚ä§</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">‚ä§</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>U</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where we dropped the indexing subscripts <inline-formula><mml:math id="inf171"><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> for conciseness. At <inline-formula><mml:math id="inf172"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, the term <inline-formula><mml:math id="inf173"><mml:mrow><mml:mi>U</mml:mi><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is replaced by <inline-formula><mml:math id="inf174"><mml:mrow><mml:msub><mml:mi>ùêõ</mml:mi><mml:mtext>init</mml:mtext></mml:msub><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></inline-formula>, which denotes the learnable initial bias for the hidden units. The joint probability distribution of an RTRBM model with length <inline-formula><mml:math id="inf175"><mml:mi>T</mml:mi></mml:math></inline-formula> is found by factoring over the RBM stack over all time-steps <inline-formula><mml:math id="inf176"><mml:mrow><mml:mi>t</mml:mi><mml:mo separator="true">,</mml:mo><mml:mo>‚Ä¶</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>. This procedure is written as:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>‚à£</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>‚Å°</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>‚à£</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo>‚ãÖ</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>‚ãØ</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>T</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mtable displaystyle="true" style="width:100%;"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"/><mml:mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd style="padding:0.7ex 0em 0.7ex 0em;"><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mrow><mml:mo fence="true" form="prefix">{</mml:mo><mml:msub><mml:mi>ùêØ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>ùê°</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo fence="true" form="postfix">}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo lspace="0.22em" rspace="0.22em" stretchy="false">|</mml:mo><mml:msubsup><mml:mrow><mml:mo fence="true" form="prefix">{</mml:mo><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo fence="true" form="postfix">}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd style="padding:0.7ex 0em 0.7ex 0em;"><mml:mrow><mml:mo form="prefix" stretchy="false">‚àí</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msubsup><mml:mi>ùê°</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow><mml:mi>‚ä§</mml:mi></mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>ùêØ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ùêõ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi>ùêØ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ùêõ</mml:mi><mml:mtext>init</mml:mtext></mml:msub><mml:msub><mml:mi>ùê°</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">‚àë</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msubsup><mml:mi>ùê°</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow><mml:mi>‚ä§</mml:mi></mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>ùêõ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ùêõ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi>ùêØ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ùêõ</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:msub><mml:mi>ùê°</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ùê°</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow><mml:mi>‚ä§</mml:mi></mml:msubsup><mml:mi>U</mml:mi><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Importantly, instead of using binary hidden unit states <inline-formula><mml:math id="inf177"><mml:msub><mml:mi>ùê°</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>, sampled from the expected real-valued hidden states <inline-formula><mml:math id="inf178"><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>, the RTRBM propagates these real-valued hidden unit states directly. This approach constitutes the mean-field approximation of the hidden states of the temporally preceding RBMs, resulting in an efficient and easily computible approximation of the temporal state at each time-step <inline-formula><mml:math id="inf179"><mml:mi>t</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib22">Hinton, 2002</xref>; <xref ref-type="bibr" rid="bib38">Sutskever et al., 2008</xref>). The inputs <inline-formula><mml:math id="inf180"><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> of the RTRBM at time-step <inline-formula><mml:math id="inf181"><mml:mi>t</mml:mi></mml:math></inline-formula>, given the visible unit state <inline-formula><mml:math id="inf182"><mml:mi>v</mml:mi></mml:math></inline-formula>, are then calculated as:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mi>œÉ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>¬†if¬†</mml:mtext><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>œÉ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>¬†if¬†</mml:mtext><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-5"><title>Training the RTRBM</title><p>The model parameters <inline-formula><mml:math id="inf183"><mml:mrow><mml:mi>Œ∏</mml:mi><mml:mo>‚àà</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">{</mml:mo><mml:mi>W</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>U</mml:mi><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>ùêõ</mml:mi><mml:mi>ùêØ</mml:mi></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>ùêõ</mml:mi><mml:mtext>init</mml:mtext></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>ùêõ</mml:mi><mml:mi>ùê°</mml:mi></mml:msub><mml:mo fence="true" form="postfix">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are learned by maximization of the log-likelihood using (stochastic) gradient ascent:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi>Œ∏</mml:mi><mml:mo>:=</mml:mo><mml:mi>Œ∏</mml:mi><mml:mo>+</mml:mo><mml:mi>Œ∑</mml:mi><mml:msub><mml:mi mathvariant="normal">‚àá</mml:mi><mml:mrow><mml:mi>Œ∏</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf184"><mml:mi>Œ∑</mml:mi></mml:math></inline-formula> denotes the learning rate and where<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="normal">‚àá</mml:mi><mml:mrow><mml:mi>Œ∏</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>‚àí</mml:mo><mml:mo fence="false" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi mathvariant="normal">‚àá</mml:mi><mml:mrow><mml:mi>Œ∏</mml:mi></mml:mrow></mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>‚à£</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‚ü©</mml:mo><mml:mrow><mml:mtext>data¬†</mml:mtext></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi mathvariant="normal">‚àá</mml:mi><mml:mrow><mml:mi>Œ∏</mml:mi></mml:mrow></mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>‚à£</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‚ü©</mml:mo><mml:mrow><mml:mtext>model¬†</mml:mtext></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>are the partial derivatives of the log-likelihood with respect to the model parameters. The energy function of the RTRBM can be split up into two components: a static component <inline-formula><mml:math id="inf185"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">H</mml:mi></mml:math></inline-formula> and a temporal component <inline-formula><mml:math id="inf186"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">Q</mml:mi></mml:math></inline-formula>. The gradients of the static component <inline-formula><mml:math id="inf187"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">H</mml:mi></mml:math></inline-formula> with respect to <inline-formula><mml:math id="inf188"><mml:mi>Œ∏</mml:mi></mml:math></inline-formula> are calculated by summing over the gradients of the RBM at each time-step. The calculation of the gradients of <inline-formula><mml:math id="inf189"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">Q</mml:mi></mml:math></inline-formula> with respect to <inline-formula><mml:math id="inf190"><mml:mi>Œ∏</mml:mi></mml:math></inline-formula> is more complex. First, observe that <inline-formula><mml:math id="inf191"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">Q</mml:mi></mml:math></inline-formula> can be computed recursively as:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mtable displaystyle="true" style="width:100%;"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"/><mml:mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mml:mrow><mml:msub><mml:mi class="MJX-tex-caligraphic" mathvariant="script">Q</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">‚àë</mml:mo><mml:mrow><mml:mi>œÑ</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:munderover></mml:mrow><mml:msubsup><mml:mi>ùê°</mml:mi><mml:mi>œÑ</mml:mi><mml:mi>‚ä§</mml:mi></mml:msubsup><mml:mi>U</mml:mi><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>œÑ</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi class="MJX-tex-caligraphic" mathvariant="script">Q</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ùê°</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow><mml:mi>‚ä§</mml:mi></mml:msubsup><mml:mi>U</mml:mi><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Hence, the backpropagation-through-time algorithm (<xref ref-type="bibr" rid="bib34">Rumelhart et al., 1986</xref>) can be employed to recursively compute gradients for <inline-formula><mml:math id="inf192"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">Q</mml:mi></mml:math></inline-formula> with respect to <inline-formula><mml:math id="inf193"><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mtable displaystyle="true" style="width:100%;"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"/><mml:mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mml:mrow><mml:msub><mml:mo>‚àá</mml:mo><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:msub><mml:msub><mml:mi class="MJX-tex-caligraphic" mathvariant="script">Q</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>U</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mo>‚àá</mml:mo><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:msub><mml:msub><mml:mi class="MJX-tex-caligraphic" mathvariant="script">Q</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo>‚äô</mml:mo><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo>‚äô</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:mn>ùüè</mml:mn><mml:mo>‚àí</mml:mo><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ùê°</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf194"><mml:mo lspace="0em" rspace="0em">‚äô</mml:mo></mml:math></inline-formula> denotes the element-wise product. The final step to obtain the derivative with respect to <inline-formula><mml:math id="inf195"><mml:mi>Œ∏</mml:mi></mml:math></inline-formula> involves applying the chain rule:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mtable displaystyle="true" style="width:100%;"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"/><mml:mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mml:mrow><mml:msub><mml:mo>‚àá</mml:mo><mml:mi>Œ∏</mml:mi></mml:msub><mml:mi class="MJX-tex-caligraphic" mathvariant="script">Q</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">‚àë</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mo>‚àá</mml:mo><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:msub><mml:msub><mml:mi class="MJX-tex-caligraphic" mathvariant="script">Q</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo>‚äô</mml:mo><mml:msub><mml:mo form="prefix" stretchy="false">‚àá</mml:mo><mml:mi>Œ∏</mml:mi></mml:msub><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mo form="prefix" stretchy="false">‚àá</mml:mo><mml:mi>Œ∏</mml:mi></mml:msub><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msubsup><mml:mi>ùê°</mml:mi><mml:mi>t</mml:mi><mml:mi>‚ä§</mml:mi></mml:msubsup><mml:mi>U</mml:mi><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:mtd><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>For a more detailed derivation of these equations, we refer to <xref ref-type="bibr" rid="bib27">Mittelman et al., 2014</xref>.</p></sec><sec id="s4-6"><title>Inference in the RTRBM</title><p>The RTRBM‚Äôs sequential sampling scheme, using preceding hidden- and visible states to predict subsequent time-steps, enables generating data from its learned model distribution. Initially, <inline-formula><mml:math id="inf196"><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> is calculated using the previous expected hidden states and current visible states. The contrastive divergence sampling scheme is then used to sample <inline-formula><mml:math id="inf197"><mml:msub><mml:mi>ùêØ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>. Consequently, <inline-formula><mml:math id="inf198"><mml:msub><mml:mi>ùêØ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> is utilized to get the following expected hidden unit states <inline-formula><mml:math id="inf199"><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>, and so forth. This sequential process enables the RTRBM to statistically predict events in future time-steps.</p><p>During model training, all time-steps <inline-formula><mml:math id="inf200"><mml:mi>T</mml:mi></mml:math></inline-formula> are available and it is not required to infer longer sequences. At the start of each training epoch <inline-formula><mml:math id="inf201"><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> can be computed recursively up to time point <inline-formula><mml:math id="inf202"><mml:mi>T</mml:mi></mml:math></inline-formula>, followed by contrastive divergence performed for all time-steps in parallel. At model inference, we initialize <inline-formula><mml:math id="inf203"><mml:msub><mml:mi>ùê´</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> on the first time-step of each test batch and sample multiple time-steps into the future using the Gibbs sampling scheme (see <xref ref-type="fig" rid="fig2">Figure 2G</xref>). By comparing the inferred data with the remainder of the test data batch, we quantify the extent to which the RTRBM accurately captures the statistics of neural data across time (see also Performance metrics).</p></sec><sec id="s4-7"><title>Model-generated simulated data</title><p>The simulated data is generated according to the following principles: <inline-formula><mml:math id="inf204"><mml:msub><mml:mi>N</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula> hidden units represent population activities that are temporally connected through a set of weights <inline-formula><mml:math id="inf205"><mml:mi>U</mml:mi></mml:math></inline-formula>. The activity of a single hidden unit is represented as a time-varying firing rate. Each hidden unit activity is the combination of two sources: (1) An intrinsic, time-varying firing rate, generated by two instances of randomly timed peaks (where the inter-peak interval is randomly drawn as ISI <inline-formula><mml:math id="inf206"><mml:mrow><mml:mo>‚àº</mml:mo><mml:mtext>Uniform</mml:mtext><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) convolved with normalized Gaussian-shaped signals (<inline-formula><mml:math id="inf207"><mml:mrow><mml:mi>œï</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo separator="true">;</mml:mo><mml:mi>t</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>œÉ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf208"><mml:mrow><mml:mi>œÉ</mml:mi><mml:mo>‚àº</mml:mo><mml:mtext>Uniform</mml:mtext><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>œÉ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>œÉ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and where <inline-formula><mml:math id="inf209"><mml:mrow><mml:mi>œï</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> signifies the standard Gaussian cumulative distribution function). Both of the resulting instances are subsequently renormalized to be in the intervals <inline-formula><mml:math id="inf210"><mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mi>/</mml:mi><mml:mn>10</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf211"><mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, respectively, and are summed together. The final signal is denoted <inline-formula><mml:math id="inf212"><mml:mrow><mml:msubsup><mml:mi>Œª</mml:mi><mml:mi>i</mml:mi><mml:mtext>init</mml:mtext></mml:msubsup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and represents the firing rates of hidden unit <inline-formula><mml:math id="inf213"><mml:mi>i</mml:mi></mml:math></inline-formula>. (2) Recurrent interactions of firing rates between the hidden units at a delay of <inline-formula><mml:math id="inf214"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Œî</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, according to<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mtable displaystyle="true" style="width:100%;"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"/><mml:mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mml:mrow><mml:msub><mml:mi>Œª</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>œï</mml:mi><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msubsup><mml:mi>Œª</mml:mi><mml:mi>i</mml:mi><mml:mtext>init</mml:mtext></mml:msubsup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">‚àë</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:munderover></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Œª</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mrow><mml:mi mathvariant="normal">Œî</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf215"><mml:mi>œï</mml:mi></mml:math></inline-formula> limits the output to the interval <inline-formula><mml:math id="inf216"><mml:mrow><mml:mo fence="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>3</mml:mn><mml:msub><mml:mi>f</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mo fence="true" form="postfix">]</mml:mo></mml:mrow></mml:math></inline-formula>. The strength of the temporal interactions between hidden units are given by the entries in <inline-formula><mml:math id="inf217"><mml:mi>U</mml:mi></mml:math></inline-formula>, with positive and negative entries representing excitatory and inhibitory interactions, respectively.</p><p>Each hidden unit connects to a distinct set of <inline-formula><mml:math id="inf218"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mtext> per unit</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> visible units, yielding a total of <inline-formula><mml:math id="inf219"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>‚ãÖ</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mtext> per unit</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> visible units, and their activity <inline-formula><mml:math id="inf220"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is independently drawn from a Poisson distribution whose rate over time is given by the rate <inline-formula><mml:math id="inf221"><mml:msub><mml:mi>Œª</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula> of the corresponding hidden unit, scaled by a different constant for each visible unit taken from the range <inline-formula><mml:math id="inf222"><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow></mml:math></inline-formula>. The first <inline-formula><mml:math id="inf223"><mml:msub><mml:mi>t</mml:mi><mml:mtext>settle</mml:mtext></mml:msub></mml:math></inline-formula> time points are removed from the simulated data as the initial activity is different from the long-term interacting dynamics. The parameters for the data generating model were set to <inline-formula><mml:math id="inf224"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf225"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mtext> per unit</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf226"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf227"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf228"><mml:mrow><mml:msub><mml:mi>œÉ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf229"><mml:mrow><mml:msub><mml:mi>œÉ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf230"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf231"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1.4</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf232"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>settle</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf233"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>. The temporal connectivity matrix <inline-formula><mml:math id="inf234"><mml:mi>U</mml:mi></mml:math></inline-formula> was configured such that each population is both excited and inhibited by one of the other populations, and scaled in such a way that the resulting activity is mainly determined by the interactions (<xref ref-type="fig" rid="fig2">Figure 2A</xref> shows a graphical representation of the data generation pipeline).</p><p>For model training and evaluation, the generated simulated data was divided into a train- and test-set. Model inference is used for generating samples from trained RBM/RTRBM models to evaluate their performance. At model inference, the models are initialized on the first time-step of the test batch, and the subsequent time-steps are inferred through Gibbs sampling. The inferred data is then compared to the ground-truth test data through several statistics: (1) The MSE between inferred and test data, which measures how well the models can reproduce unseen activity of visible units. (2) The mean activation of the visible units <inline-formula><mml:math id="inf235"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>, also referred to as the first order moments. (3) Pairwise moments between the visible units <inline-formula><mml:math id="inf236"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>, also denoted as second-order statistics. These pairwise moments assess the model‚Äôs ability to capture data statistics it is not explicitly trained on. (4) Time-shifted pairwise moments of the visible and hidden units, i.e., <inline-formula><mml:math id="inf237"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>v</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf238"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">[</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">]</mml:mo></mml:mrow></mml:msubsup><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>, respectively. More details on how these statistics are calculated can be found in performance metrics.</p></sec><sec id="s4-8"><title>Zebrafish data</title><p>Whole-brain single-cell functional recordings for 15 zebrafish larvae were recorded by means of light-sheet microscopy in the lab of G. Debr√©geas, 8 of which are used in this study (see <xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref> for a more detailed description of both the data acquisition process and subsequent data processing). These datasets are publicly available and can be found at <ext-link ext-link-type="uri" xlink:href="https://gin.g-node.org/vdplasthijs/cRBM_zebrafish_spontaneous_data">https://gin.g-node.org/vdplasthijs/cRBM_zebrafish_spontaneous_data</ext-link>. In summary, the datasets consist on average of <inline-formula><mml:math id="inf239"><mml:mrow><mml:mn>40,709</mml:mn><mml:mo>¬±</mml:mo><mml:mn>13,854</mml:mn></mml:mrow></mml:math></inline-formula> neurons, recorded for <inline-formula><mml:math id="inf240"><mml:mrow><mml:mn>5836</mml:mn><mml:mo>¬±</mml:mo><mml:mn>1183</mml:mn></mml:mrow></mml:math></inline-formula> time-steps, at a frequency of 3.9 ¬± 0.8 Hz. The zebrafish larvae are 5‚Äì7 days post-fertilization and expressed GCaMP6s or GCaMP6f calcium reporters for imaging. The experimental procedure is described in more detail in <xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>. The process from data segregation to analysis is displayed in <xref ref-type="fig" rid="fig3">Figure 3</xref>. To obtain binarized spike traces that can be used by the RTRBM the individual fluorescence traces are deconvolved by blind sparse deconvolution (<xref ref-type="fig" rid="fig3">Figure 3A</xref>; <xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>).</p></sec><sec id="s4-9"><title>RTRBM initialization through transfer learning</title><p>Through empirical evaluation, we found de novo learning of the RTRBM to not converge to a solution that satisfies the compositional-phase criteria (see The compositional phase). This is to be expected as the formulation of the RTRBM as used in this work does not feature the required elements necessary to push the model solution towards such a solution. To overcome this issue, we make use of a transfer learning strategy (<xref ref-type="bibr" rid="bib39">Tan et al., 2018</xref>). More specifically, the visible-to-hidden weights <inline-formula><mml:math id="inf241"><mml:mi>W</mml:mi></mml:math></inline-formula> of the RTRBM models are initialized by their estimated counterparts from trained cRBM models (<xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>). During subsequent training of the RTRBM, we let the model to update the values of these weights with a reduced learning rate (see below). This initialization strategy is able to bias the resulting weight matrix <inline-formula><mml:math id="inf242"><mml:mi>W</mml:mi></mml:math></inline-formula> inferred by the RTRBM to contain a strong prior towards the localized receptive fields of the hidden units of the cRBM.</p><p>The hyperparameters, including the total number of hidden units for the cRBM model, were optimized by evaluating the model‚Äôs performance over a grid of hyperparameter values using one dataset. This process identified the optimal hyperparameter values, which were subsequently applied to all recordings. In our study, the number of hidden units is fixed in the RTRBM model due to the use of transfer learning. Further details on the cross-validation process can be found in <xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>.</p><p>For model training and evaluation, the functional data for each animal was divided into a train- and test-set. To this end, the functional recordings of each animal were subdivided into 10 segments of consecutive time-steps. In all cases, temporal segments 2, 6, and 7 are labeled as test sets, the remaining temporal segments are labeled as training sets. This data division strategy mirrors that of <xref ref-type="bibr" rid="bib43">van der Plas et al., 2023</xref>. In practice, each segment was further subdivided into smaller batches of size <inline-formula><mml:math id="inf243"><mml:mrow><mml:mn>16</mml:mn><mml:mo>¬±</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>, enabling their computation on an Nvidia GeForce RTX 3090 24 GB GPU. This resulted on average in <inline-formula><mml:math id="inf244"><mml:mrow><mml:mn>243</mml:mn><mml:mo>¬±</mml:mo><mml:mn>67</mml:mn></mml:mrow></mml:math></inline-formula> train batches and <inline-formula><mml:math id="inf245"><mml:mrow><mml:mn>104</mml:mn><mml:mo>¬±</mml:mo><mml:mn>29</mml:mn></mml:mrow></mml:math></inline-formula> test batches. Using transfer learning, we trained each RTRBM for 10,000 epochs with a learning rate of <inline-formula><mml:math id="inf246"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo lspace="0em" rspace="0em">‚àí</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>. For each epoch we calculated the gradients for 20 batches in parallel before updating the model parameters. This strategy reduced the training time to a relatively short duration of 3 hr for 10,000 epochs calculated on an <italic>NVIDIA RTX3090</italic> GPU. To maintain neural assemblies we enforced an L1-norm sparsity regulator with its constant set to <inline-formula><mml:math id="inf247"><mml:mrow><mml:mi>Œª</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo lspace="0em" rspace="0em">‚àí</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (with <inline-formula><mml:math id="inf248"><mml:mrow><mml:mi>Œª</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo lspace="0em" rspace="0em">‚àí</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for two of the animals), and a reduced learning rate for the matrix <inline-formula><mml:math id="inf249"><mml:mi>W</mml:mi></mml:math></inline-formula> by two orders of magnitude (i.e. 10<sup>-5</sup>).</p></sec><sec id="s4-10"><title>Performance evaluation and significance testing</title><p>The performance of both the cRBM and RTRBM models after training was evaluated through inspection of the learning moments, similar to that of the simulated data (see see Performance metric for more details). Furthermore, to test the significance of the difference in performance between the RTRBM and the cRBM, we made use of a bootstrap method. To this end, we randomly selected 1000 neurons (<inline-formula><mml:math id="inf250"><mml:mrow><mml:mo>‚àº</mml:mo><mml:mn>2</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></inline-formula> of the total population) and calculated the neural statistics for each model. This process was repeated <inline-formula><mml:math id="inf251"><mml:mi>n</mml:mi></mml:math></inline-formula> times with non-overlapping subsets such that each neuron only gets sampled once. For each repetition, the Spearman correlation between data and model sampled moments was calculated for each <inline-formula><mml:math id="inf252"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). We then calculated a confidence interval given by two standard deviations from the mean for the Spearman correlation. If the confidence intervals for the RTRBM and the cRBM did not overlap, the difference in performance was considered significant.</p></sec><sec id="s4-11"><title>Clustering of hidden-hidden unit weights</title><p>To identify neural assemblies with similar temporal connectivity, agglomerative clustering is employed on the matrix <inline-formula><mml:math id="inf253"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> after model training. To this end, we utilize the function <italic>AgglomerativeClustering</italic> from <italic>scikit-learn</italic> and use Ward‚Äôs method to evaluate the distance dendrogram (<xref ref-type="bibr" rid="bib31">Pedregosa et al., 2011</xref>). Clustering can be applied to the incoming (row) or outgoing (column) connections of the matrix <inline-formula><mml:math id="inf254"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula>, in this work we apply it to the outgoing connections only. For fish 4, as shown in <xref ref-type="fig" rid="fig3">Figure 3D and E</xref>, a distance threshold of 20 was used to threshold the resulting distance dendrogram, which identified six functional clusters. Receptive fields were determined by assigning neurons to the identified clusters based on the strength of their weight (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). Here, a strong weight is determined by proportional thresholding, <inline-formula><mml:math id="inf255"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf256"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is set so that at least 5000 neurons have a strong connection towards the hidden layer. This mirrors the thresholding strategy used in <xref ref-type="fig" rid="fig3">Figure 3B</xref>.</p></sec><sec id="s4-12"><title>Alignment of the estimated temporal weight matrix</title><p>In the context of the simulated data, the activity of the visible units are systematically generated in assemblies of size <inline-formula><mml:math id="inf257"><mml:msubsup><mml:mi>N</mml:mi><mml:mi>v</mml:mi><mml:mi>S</mml:mi></mml:msubsup></mml:math></inline-formula>. Nevertheless, during the training process of the RTRBM, which solely relies on the state of the visible units, the link between each assembly and a particular hidden unit becomes arbitrary. This is, of course, under the assumption that all assemblies are retrieved correctly during the training process. Consequently, the estimated temporal weights <inline-formula><mml:math id="inf258"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> can have any arbitrary ordering and is often not matched with that of the original matrix <inline-formula><mml:math id="inf259"><mml:mi>U</mml:mi></mml:math></inline-formula> used to generate the data, generally resulting in an invalid match between both matrices. To enable a valid comparison, the ordering of the hidden units in the estimated matrix <inline-formula><mml:math id="inf260"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> is matched to those in the original matrix <inline-formula><mml:math id="inf261"><mml:mi>U</mml:mi></mml:math></inline-formula> by leveraging the learned visible-to-hidden unit weight matrix <inline-formula><mml:math id="inf262"><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula>. Here, as the assemblies of the visible units are known, assemblies are matched with hidden units under the assumption that an assembly of visible units is most linked to the hidden unit with the largest mean absolute visible-to-hidden weights between them (<xref ref-type="fig" rid="fig2s1">Figure 2‚Äîfigure supplement 1A</xref>, left), calculated using the row-wise correlations with the ideal (‚Äôdiagonal‚Äô) weight matrix <inline-formula><mml:math id="inf263"><mml:mi>W</mml:mi></mml:math></inline-formula>. The ordering of the hidden units can then be set such that it is aligned with the ordering of the assemblies of visible units. Cases where two assemblies have the same initially matched hidden unit are resolved by sequentially assigning assemblies by order of their correlation strength to the hidden unit. These cases, however, generally indicate that the assemblies of visible units were not correctly captured by the model.</p><p>In addition to any arbitrary ordering of hidden units in the estimated matrix <inline-formula><mml:math id="inf264"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula>, it is possible for assemblies to form an inverse but correct match with a hidden unit. This means that the corresponding hidden unit is inactive when the corresponding assembly is active and vice versa. This phenomenon is enabled through the anti-symmetry of the activation function, and generally occurs when the weights between the assembly and the hidden unit are negative. An inverse match is identified based on the sign of the mean weight between the hidden unit and the matched assembly of visible units. In the case of an inverse match with a hidden unit, the temporal weights between this hidden unit and all the other units are inverted, resulting in sign switches in the estimated matrix <inline-formula><mml:math id="inf265"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2s1">Figure 2‚Äîfigure supplement 1</xref>, right). In the case of two inverted hidden units, the two temporal weights between them are unaffected as the sign switches cancel each other out.</p><p>Together, these corrections yield an aligned estimated temporal weight matrix <inline-formula><mml:math id="inf266"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> that is more similar to the original matrix <inline-formula><mml:math id="inf267"><mml:mi>U</mml:mi></mml:math></inline-formula>, allowing element-wise comparisons (<xref ref-type="fig" rid="fig2s1">Figure 2‚Äîfigure supplement 1B</xref>). All presented <inline-formula><mml:math id="inf268"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> matrices for simulated data are aligned according to this procedure.</p></sec><sec id="s4-13"><title>Performance metrics</title><p>We evaluated model performance using multiple metrics throughout this study. More specifically, we compared between first and second order model and data statistics for both the visible and hidden units and we used the MSE on the inferred states of the visible units.</p></sec><sec id="s4-14"><title>Comparison of model and data statistics</title><p>The cRBM and RTRBM models are trained to optimize multiple statics. These include the mean activity of the visible units (neurons) <inline-formula><mml:math id="inf269"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>, the mean activity of the hidden units <inline-formula><mml:math id="inf270"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>Œº</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula> and their paired interaction <inline-formula><mml:math id="inf271"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>Œº</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>. Furthermore, one can evaluate the second-order statistics <inline-formula><mml:math id="inf272"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf273"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>Œº</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>ŒΩ</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula> that are not directly optimized by the models.</p><p>To evaluate model performance, we take these statistics and compare then between those of the empirical data <inline-formula><mml:math id="inf274"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and the model <inline-formula><mml:math id="inf275"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Data statistics are calculated based on a withheld test-data set consisting of ‚àº30% of the data. To evaluate hidden-unit statistics for the empirical data, we use the expected value of <inline-formula><mml:math id="inf276"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> conditioned on the visible unit state <inline-formula><mml:math id="inf277"><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> at time point <inline-formula><mml:math id="inf278"><mml:mi>t</mml:mi></mml:math></inline-formula> for the model under evaluation. Model statistics are not explicitly available and must be sampled from the model under study. To this end, model statistics <inline-formula><mml:math id="inf279"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are approximated through Gibbs sampling of the visible and hidden unit states. For the simulated data, 15 steps of Gibbs sampling are used with a burn-in period of 4000 steps. This is done for 100 time points of data with a chain length of 20 time-steps. For the zebrafish data, again 15 steps of Gibbs sampling are used with a burn-in period of 4000 time-steps. Here 68‚Äì150 random time points in the test set were chosen each with a sampling chain of 14‚Äì16 time-steps. The number of time points and chain length depended on the length of the functional recording of a single data-batch, and was empirically chosen by comparing resulting performance.</p><p>In all cases, we subsequently measure correspondence between pairs of data and model statistics <inline-formula><mml:math id="inf280"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf281"><mml:mrow><mml:mo form="prefix" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mo form="postfix" stretchy="false">‚ü©</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> by evaluating their Spearman correlation.</p></sec><sec id="s4-15"><title>Predictive quality</title><p>The performance of the model in predicting the activity of visible units ahead in time is assessed through a measure of the mean squared error (MSE) between reconstructed data and a test data set. The MSE is defined as<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mtable displaystyle="true" style="width:100%;"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"/><mml:mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mml:mrow><mml:mtext>MSE</mml:mtext><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mfrac><mml:mrow><mml:munderover><mml:mo stretchy="true">‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:munderover></mml:mrow><mml:msup><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>‚àí</mml:mo><mml:msub><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf282"><mml:msub><mml:mi>N</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math></inline-formula> denotes the number of visible units, and <inline-formula><mml:math id="inf283"><mml:mrow><mml:msub><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the estimated state of visible unit <inline-formula><mml:math id="inf284"><mml:mi>i</mml:mi></mml:math></inline-formula> at time-step <inline-formula><mml:math id="inf285"><mml:mi>t</mml:mi></mml:math></inline-formula>. When predicting <inline-formula><mml:math id="inf286"><mml:mi>T</mml:mi></mml:math></inline-formula> time-steps ahead, <inline-formula><mml:math id="inf287"><mml:mrow><mml:msub><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> implicitly depends on <inline-formula><mml:math id="inf288"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mi>T</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> as discussed above. As the visible units only take on binary states, the MSE is equal to the mean absolute error (MAE) and is also equal to 1 minus the accuracy.</p><p>For interpretability, the MSE is normalized (nMSE) such that <inline-formula><mml:math id="inf289"><mml:mrow><mml:mtext>nMSE</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> corresponds to a naive unbiased estimator and 0 to an optimal estimator when considering the MSE arising from inherent stochasticity.<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mtext>nMSE</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>MSE</mml:mtext><mml:mo>‚àí</mml:mo><mml:msub><mml:mtext>MSE</mml:mtext><mml:mrow><mml:mtext>var</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mtext>MSE</mml:mtext><mml:mrow><mml:mtext>naive</mml:mtext></mml:mrow></mml:msub><mml:mo>‚àí</mml:mo><mml:msub><mml:mtext>MSE</mml:mtext><mml:mrow><mml:mtext>var</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>With <inline-formula><mml:math id="inf290"><mml:msub><mml:mtext>MSE</mml:mtext><mml:mtext>naive</mml:mtext></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf291"><mml:msub><mml:mtext>MSE</mml:mtext><mml:mtext>var</mml:mtext></mml:msub></mml:math></inline-formula> explained below.</p></sec><sec id="s4-16"><title>Determining error bounds for simulated data</title><p>For the simulated data, the model predictive performance would be directly interpretable if the model generating the data was completely deterministic. However, the model used in this work features multiple stochastic components. To gain insight into model performance for simulated data, it is thus necessary to infer statistically meaningful bounds on performance with respect to the generating model. To this end, we here describe methods used for estimating both a lower- and an upper-bound for the generating model used in this work.</p></sec><sec id="s4-17"><title>Lower-bound estimation</title><p>As described in the methods above, the state of the visible units is sampled through a Poisson distribution from the assembly activity consisting of (1) an intrinsic, time-varying firing rate, generated by randomly timed, Gaussian-shaped variations of firing rate, and (2) recurrent, delayed interactions of firing rates between the hidden populations with interaction time <inline-formula><mml:math id="inf292"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Œî</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. It is straightforward to estimate the variance in a Poisson sampling process under a known distribution of assembly traces. However, the intrinsic firing rates of the artificial neural assemblies are complicated to describe with theoretical distributions. The exact calculation of this theoretical bound is thus rather complex. Furthermore, one must make a distinction between an estimator with perfect knowledge of the assembly state at the previous time-step, an <italic>ideal estimator</italic>, and a <italic>real estimator</italic> that must approximate this assembly state, such as the RTRBM. A representative lower-bound thus includes this source of stochasticity.</p><p>To obtain a lower-bound that includes these notions, we turn to empirical methods. To this end, the assembly activity in a previous time-step is initialized to a fixed, known state. Then, the temporal interactions are calculated to obtain the deterministic assembly activity after an interval <inline-formula><mml:math id="inf293"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Œî</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> time-steps, which are added to different samples from the intrinsic assembly activity to obtain the full assembly state. Poisson sampling is performed on two such states and the MSE is calculated between them. This corresponds to the performance of an ideal model in predicting the state of the visible units ahead by a time equivalent to the interaction delay, with perfect knowledge of the underlying dynamics, and yields sets of MSE samples incorporating all aforementioned sources of stochasticity.</p><p>For a representative distribution of the possible states, 10,000 known assembly activity states are randomly sampled from the original test data, which form the fixed previous time-step. Next, <inline-formula><mml:math id="inf294"><mml:mrow><mml:mn>2</mml:mn><mml:mo>√ó</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> random instances of the intrinsic assembly activities are added as described above per initial state, and samples are taken twice from each of the 200 pairs of states. As in our case <inline-formula><mml:math id="inf295"><mml:mrow><mml:mtext>MSE</mml:mtext><mml:mo>=</mml:mo><mml:mtext>MAE</mml:mtext></mml:mrow></mml:math></inline-formula> is a linear metric, the mean MSE is taken over all initial states and all random intrinsic states, which is our estimate <inline-formula><mml:math id="inf296"><mml:msub><mml:mtext>MSE</mml:mtext><mml:mtext>var</mml:mtext></mml:msub></mml:math></inline-formula> The uncertainty in this estimate is quantified as the standard error of the mean, and is calculated by seeing each average MSE per initial state as an estimate of the mean.</p></sec><sec id="s4-18"><title>Upper-bound estimation</title><p>An upper-bound is determined through a representative naive estimator. Here, we define this naive estimator with the expected firing rate of each visible unit <inline-formula><mml:math id="inf297"><mml:mrow><mml:mi>P</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msubsup><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true" form="postfix">‚ü©</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, such that the estimator is unbiased by definition. The MSE reached by this estimator can be precisely calculated as <inline-formula><mml:math id="inf298"><mml:mrow><mml:msub><mml:mtext>MSE</mml:mtext><mml:mtext>naive</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">‚ü®</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo fence="true" form="prefix">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true" form="postfix">‚ü©</mml:mo></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>‚àí</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true" form="postfix">‚ü©</mml:mo></mml:mrow><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo fence="true" form="postfix">‚ü©</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The choice of this unbiased estimator is especially important in the case of sparse activity traces as dealt with here. It is easy to see that a lower MSE is reached by the (biased) estimator <inline-formula><mml:math id="inf299"><mml:mrow><mml:mi>P</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msubsup><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf300"><mml:mrow><mml:mtext>MSE</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true" form="postfix">‚ü©</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. It is worth noting that in the context of down-sampling experiments, even though all models are assessed using the same test dataset, the act of down-sampling yields a similar but different mean activity <inline-formula><mml:math id="inf301"><mml:mrow><mml:mo fence="true" form="prefix">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true" form="postfix">‚ü©</mml:mo></mml:mrow></mml:math></inline-formula>, resulting in a slightly different theoretical estimate of the MSE of the naive unbiased estimator.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Formal analysis, Investigation, Visualization, Methodology, Writing ‚Äì original draft, Writing ‚Äì review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Investigation, Visualization, Methodology, Writing ‚Äì original draft, Writing ‚Äì review and editing</p></fn><fn fn-type="con" id="con3"><p>Supervision, Investigation, Visualization, Methodology, Writing ‚Äì original draft, Writing ‚Äì review and editing</p></fn><fn fn-type="con" id="con4"><p>Software, Formal analysis, Investigation, Visualization, Methodology, Writing ‚Äì original draft, Writing ‚Äì review and editing</p></fn><fn fn-type="con" id="con5"><p>Supervision, Writing ‚Äì original draft, Project administration, Writing ‚Äì review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-98489-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The RTRBM models described in this study have been implemented in Python 3.7. Packages used for model implementation, analysis, and data visualization include Numpy (<xref ref-type="bibr" rid="bib19">Harris et al., 2020</xref>), Scipy (<xref ref-type="bibr" rid="bib44">Virtanen et al., 2020</xref>), Pytorch (<xref ref-type="bibr" rid="bib30">Paszke et al., 2019</xref>), Scikit-learn (<xref ref-type="bibr" rid="bib31">Pedregosa et al., 2011</xref>), matplotlib (<xref ref-type="bibr" rid="bib23">Hunter, 2007</xref>), Pandas (<xref ref-type="bibr" rid="bib26">McKinney, 2010</xref>), Seaborn (<xref ref-type="bibr" rid="bib45">Waskom et al., 2017</xref>), h5py (<xref ref-type="bibr" rid="bib12">Collette, 2013</xref>). Separate notebooks are available allowing for recreation of each individual figure presented in this paper. All code accompanying this paper is available on <ext-link ext-link-type="uri" xlink:href="https://github.com/benglitz/Zebrafish_RTRBM">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib15">Englitz, 2024</xref>). All accompanying data is hosted on <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/TX2VZ">OSF</ext-link>. The model-generated simulated data is seeded and can easily be generated through the accompanying notebooks. All functional whole-brain datasets were obtained in the lab of G Debr√©geas and are publicly available <ext-link ext-link-type="uri" xlink:href="https://gin.g-node.org/vdplasthijs/cRBM_zebrafish_spontaneous_data">here</ext-link>. Copies of the zebrafish datasets used in this project can additionally be found at the OSF data share accompanying this project.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Hesselink</surname><given-names>LW</given-names></name><name><surname>Englitz</surname><given-names>B</given-names></name><name><surname>Monnens</surname><given-names>SLQ</given-names></name><name><surname>Casper</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Zebrafish RTRBM</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/TX2VZ</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>van der Plas</surname><given-names>TL</given-names></name><name><surname>Tubiana</surname><given-names>J</given-names></name><name><surname>Goc</surname><given-names>GL</given-names></name><name><surname>Migault</surname><given-names>G</given-names></name><name><surname>Kunst</surname><given-names>M</given-names></name><name><surname>Baier</surname><given-names>H</given-names></name><name><surname>Bormuth</surname><given-names>V</given-names></name><name><surname>Englitz</surname><given-names>B</given-names></name><name><surname>Debr√©geas</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Data repository for Van der Plas, Tubiana and colleagues</data-title><source>G-Node</source><pub-id pub-id-type="accession" xlink:href="https://gin.g-node.org/vdplasthijs/cRBM_zebrafish_spontaneous_data">cRBM_zebrafish_spontaneous_data</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We would like to thank T van de Plas and G Tubiana for technical discussions. BE acknowledges funding from an NWO VIDI grant (016.Vidi.189.052) and a DFG Forschungsstipendium (EN919/1-1).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ackley</surname><given-names>DH</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>A learning algorithm for boltzmann machines</article-title><source>Cognitive Science</source><volume>9</volume><fpage>147</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1016/S0364-0213(85)80012-4</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname><given-names>MB</given-names></name><name><surname>Orger</surname><given-names>MB</given-names></name><name><surname>Robson</surname><given-names>DN</given-names></name><name><surname>Li</surname><given-names>JM</given-names></name><name><surname>Keller</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Whole-brain functional imaging at cellular resolution using light-sheet microscopy</article-title><source>Nature Methods</source><volume>10</volume><fpage>413</fpage><lpage>420</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2434</pub-id><pub-id pub-id-type="pmid">23524393</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bahl</surname><given-names>A</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural circuits for evidence accumulation and decision making in larval zebrafish</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>94</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0534-9</pub-id><pub-id pub-id-type="pmid">31792464</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bargmann</surname><given-names>CI</given-names></name><name><surname>Marder</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>From the connectome to brain function</article-title><source>Nature Methods</source><volume>10</volume><fpage>483</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2451</pub-id><pub-id pub-id-type="pmid">23866325</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Betzel</surname><given-names>RF</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Organizing principles of whole-brain functional connectivity in zebrafish larvae</article-title><source>Network Neuroscience</source><volume>4</volume><fpage>234</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1162/netn_a_00121</pub-id><pub-id pub-id-type="pmid">32166210</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname><given-names>IH</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visuomotor transformations underlying hunting behavior in zebrafish</article-title><source>Current Biology</source><volume>25</volume><fpage>831</fpage><lpage>846</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.01.042</pub-id><pub-id pub-id-type="pmid">25754638</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boltzmann</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1868">1868</year><article-title>Studien uber das gleichgewicht der lebenden kraft</article-title><source>Wissenschafiliche Abhandlungen</source><volume>1</volume><fpage>49</fpage><lpage>96</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouchard</surname><given-names>MB</given-names></name><name><surname>Voleti</surname><given-names>V</given-names></name><name><surname>Mendes</surname><given-names>CS</given-names></name><name><surname>Lacefield</surname><given-names>C</given-names></name><name><surname>Grueber</surname><given-names>WB</given-names></name><name><surname>Mann</surname><given-names>RS</given-names></name><name><surname>Bruno</surname><given-names>RM</given-names></name><name><surname>Hillman</surname><given-names>EMC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Swept confocally-aligned planar excitation (SCAPE) microscopy for high speed volumetric imaging of behaving organisms</article-title><source>Nature Photonics</source><volume>9</volume><fpage>113</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1038/nphoton.2014.323</pub-id><pub-id pub-id-type="pmid">25663846</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Boulanger-Lewandowski</surname><given-names>N</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Vincent</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1206.6392">https://arxiv.org/abs/1206.6392</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Carreira-Perpinan</surname><given-names>MA</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>On contrastive divergence learning</article-title><conf-name>Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics</conf-name><fpage>33</fpage><lpage>40</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Mu</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>Y</given-names></name><name><surname>Kuan</surname><given-names>AT</given-names></name><name><surname>Nikitchenko</surname><given-names>M</given-names></name><name><surname>Randlett</surname><given-names>O</given-names></name><name><surname>Chen</surname><given-names>AB</given-names></name><name><surname>Gavornik</surname><given-names>JP</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name><name><surname>Ahrens</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Brain-wide organization of neuronal activity and convergent sensorimotor transformations in larval zebrafish</article-title><source>Neuron</source><volume>100</volume><fpage>876</fpage><lpage>890</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.042</pub-id><pub-id pub-id-type="pmid">30473013</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Collette</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Python and HDF5</source><publisher-name>O‚ÄôReilly. Appendix</publisher-name></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Das</surname><given-names>A</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Systematic errors in connectivity inferred from activity in strongly recurrent networks</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1286</fpage><lpage>1296</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0699-2</pub-id><pub-id pub-id-type="pmid">32895567</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>TW</given-names></name><name><surname>Mu</surname><given-names>Y</given-names></name><name><surname>Narayan</surname><given-names>S</given-names></name><name><surname>Randlett</surname><given-names>O</given-names></name><name><surname>Naumann</surname><given-names>EA</given-names></name><name><surname>Yang</surname><given-names>CT</given-names></name><name><surname>Schier</surname><given-names>AF</given-names></name><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name><name><surname>Ahrens</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Brain-wide mapping of neural activity controlling zebrafish exploratory locomotion</article-title><source>eLife</source><volume>5</volume><elocation-id>e12741</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.12741</pub-id><pub-id pub-id-type="pmid">27003593</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Englitz</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Zebrafish_RTRBM</data-title><version designator="swh:1:rev:de1dec66151af5440d79141fb9d122367ed1a3bd">swh:1:rev:de1dec66151af5440d79141fb9d122367ed1a3bd</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e3b08ff192916bba0522abc4c977e2acb3a8e06f;origin=https://github.com/benglitz/Zebrafish_RTRBM;visit=swh:1:snp:06a2464b6ccccc2613be7b2f5949d444c3dd7df2;anchor=swh:1:rev:de1dec66151af5440d79141fb9d122367ed1a3bd">https://archive.softwareheritage.org/swh:1:dir:e3b08ff192916bba0522abc4c977e2acb3a8e06f;origin=https://github.com/benglitz/Zebrafish_RTRBM;visit=swh:1:snp:06a2464b6ccccc2613be7b2f5949d444c3dd7df2;anchor=swh:1:rev:de1dec66151af5440d79141fb9d122367ed1a3bd</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerstein</surname><given-names>GL</given-names></name><name><surname>Bedenbaugh</surname><given-names>P</given-names></name><name><surname>Aertsen</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Neuronal assemblies</article-title><source>IEEE Transactions on Bio-Medical Engineering</source><volume>36</volume><fpage>4</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1109/10.16444</pub-id><pub-id pub-id-type="pmid">2646211</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Deep Learning</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural signatures of cell assembly organization</article-title><source>Nature Reviews. Neuroscience</source><volume>6</volume><fpage>399</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1038/nrn1669</pub-id><pub-id pub-id-type="pmid">15861182</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Picus</surname><given-names>M</given-names></name><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>van Kerkwijk</surname><given-names>MH</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Haldane</surname><given-names>A</given-names></name><name><surname>Del R√≠o</surname><given-names>JF</given-names></name><name><surname>Wiebe</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>G√©rard-Marchant</surname><given-names>P</given-names></name><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Abbasi</surname><given-names>H</given-names></name><name><surname>Gohlke</surname><given-names>C</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Array programming with NumPy</article-title><source>Nature</source><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebb</surname><given-names>DO</given-names></name></person-group><year iso-8601-date="1949">1949</year><article-title>The first stage of perception: growth of the assembly</article-title><source>The Organization of Behavior</source><volume>4</volume><fpage>60</fpage><lpage>78</lpage></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helmstaedter</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The mutual inspirations of machine learning and neuroscience</article-title><source>Neuron</source><volume>86</volume><fpage>25</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.031</pub-id><pub-id pub-id-type="pmid">25856482</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Training products of experts by minimizing contrastive divergence</article-title><source>Neural Computation</source><volume>14</volume><fpage>1771</fpage><lpage>1800</lpage><pub-id pub-id-type="doi">10.1162/089976602760128018</pub-id><pub-id pub-id-type="pmid">12180402</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Matplotlib: a 2D graphics environment</article-title><source>Computing in Science &amp; Engineering</source><volume>9</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kullback</surname><given-names>S</given-names></name><name><surname>Leibler</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1951">1951</year><article-title>On information and sufficiency</article-title><source>The Annals of Mathematical Statistics</source><volume>22</volume><fpage>79</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177729694</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Yu</surname><given-names>ZL</given-names></name><name><surname>Gu</surname><given-names>Z</given-names></name><name><surname>Wu</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Jin</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A hybrid network for erp detection and analysis based on restricted boltzmann machine</article-title><source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source><volume>26</volume><fpage>563</fpage><lpage>572</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2018.2803066</pub-id><pub-id pub-id-type="pmid">29522400</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McKinney</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Data Structures for Statistical Computing in Python</article-title><conf-name>Python in Science Conference</conf-name><conf-loc>Austin, Texas</conf-loc><fpage>56</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.25080/Majora-92bf1922-00a</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mittelman</surname><given-names>R</given-names></name><name><surname>Kuipers</surname><given-names>B</given-names></name><name><surname>Savarese</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Structured recurrent temporal restricted boltzmann machines</article-title><conf-name>International Conference on Machine Learning</conf-name><fpage>1647</fpage><lpage>1655</lpage></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>HD</given-names></name><name><surname>Ullmann</surname><given-names>JFP</given-names></name><name><surname>McLachlan</surname><given-names>GJ</given-names></name><name><surname>Voleti</surname><given-names>V</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Hillman</surname><given-names>EMC</given-names></name><name><surname>Reutens</surname><given-names>DC</given-names></name><name><surname>Janke</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Whole-volume clustering of time series data from zebrafish brain calcium images via mixture modeling</article-title><source>Statistical Analysis and Data Mining</source><volume>11</volume><fpage>5</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1002/sam.11366</pub-id><pub-id pub-id-type="pmid">29725490</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pao</surname><given-names>GM</given-names></name><name><surname>Smith</surname><given-names>C</given-names></name><name><surname>Park</surname><given-names>J</given-names></name><name><surname>Takahashi</surname><given-names>K</given-names></name><name><surname>Watanakeesuntorn</surname><given-names>W</given-names></name><name><surname>Natsukawa</surname><given-names>H</given-names></name><name><surname>Chalasani</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Experimentally testable whole brain manifolds that recapitulate behavior</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2106.10627">https://arxiv.org/abs/2106.10627</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PyTorch: an imperative style, high-performance deep learning library</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>8024</fpage><lpage>8035</lpage></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petrucco</surname><given-names>L</given-names></name><name><surname>Lavian</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>YK</given-names></name><name><surname>Svara</surname><given-names>F</given-names></name><name><surname>≈†tih</surname><given-names>V</given-names></name><name><surname>Portugues</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neural dynamics and architecture of the heading direction circuit in zebrafish</article-title><source>Nature Neuroscience</source><volume>26</volume><fpage>765</fpage><lpage>773</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01308-5</pub-id><pub-id pub-id-type="pmid">37095397</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Privat</surname><given-names>M</given-names></name><name><surname>Romano</surname><given-names>SA</given-names></name><name><surname>Pietri</surname><given-names>T</given-names></name><name><surname>Jouary</surname><given-names>A</given-names></name><name><surname>Boulanger-Weill</surname><given-names>J</given-names></name><name><surname>Elbaz</surname><given-names>N</given-names></name><name><surname>Duchemin</surname><given-names>A</given-names></name><name><surname>Soares</surname><given-names>D</given-names></name><name><surname>Sumbre</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sensorimotor transformations in the zebrafish auditory system</article-title><source>Current Biology</source><volume>29</volume><fpage>4010</fpage><lpage>4023</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.10.020</pub-id><pub-id pub-id-type="pmid">31708392</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Williams</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Learning representations by back-propagating errors</article-title><source>Nature</source><volume>323</volume><fpage>533</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1038/323533a0</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Salakhutdinov</surname><given-names>R</given-names></name><name><surname>Mnih</surname><given-names>A</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Restricted Boltzmann Machines for Collaborative Filtering</source><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>Corvalis Oregon USA</publisher-name><pub-id pub-id-type="doi">10.1145/1273496.1273596</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Smolensky</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1986">1986</year><chapter-title>Information processing in dynamical systems: foundations of harmony theory</chapter-title><person-group person-group-type="editor"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><source>Parallel Distributed Processing: Explorations in the Microstructure of Cognition</source><publisher-name>MIT Press Direct</publisher-name><fpage>194</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.7551/mitpress/5236.003.0009</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Strogatz</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry and Engineering</source><publisher-name>Westview Press</publisher-name></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Taylor</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The recurrent Temporal Restricted Boltzmann Machine</article-title><conf-name>NIPS‚Äô08: Proceedings of the 21st International Conference on Neural Information Processing Systems</conf-name><fpage>1601</fpage><lpage>1608</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>C</given-names></name><name><surname>Sun</surname><given-names>F</given-names></name><name><surname>Kong</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Yang</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A survey on deep transfer learning</article-title><conf-name>International Conference on Artificial Neural Networks</conf-name><fpage>270</fpage><lpage>279</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tubiana</surname><given-names>J</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Emergence of compositional representations in restricted boltzmann machines</article-title><source>Physical Review Letters</source><volume>118</volume><elocation-id>138301</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.118.138301</pub-id><pub-id pub-id-type="pmid">28409983</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tubiana</surname><given-names>J</given-names></name><name><surname>Cocco</surname><given-names>S</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Learning compositional representations of interacting systems with restricted boltzmann machines: comparative study of lattice proteins</article-title><source>Neural Computation</source><volume>31</volume><fpage>1671</fpage><lpage>1717</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01210</pub-id><pub-id pub-id-type="pmid">31260391</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tubiana</surname><given-names>J</given-names></name><name><surname>Cocco</surname><given-names>S</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Learning protein constitutive motifs from sequence data</article-title><source>eLife</source><volume>8</volume><elocation-id>e39397</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.39397</pub-id><pub-id pub-id-type="pmid">30857591</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Plas</surname><given-names>TL</given-names></name><name><surname>Tubiana</surname><given-names>J</given-names></name><name><surname>Le Goc</surname><given-names>G</given-names></name><name><surname>Migault</surname><given-names>G</given-names></name><name><surname>Kunst</surname><given-names>M</given-names></name><name><surname>Baier</surname><given-names>H</given-names></name><name><surname>Bormuth</surname><given-names>V</given-names></name><name><surname>Englitz</surname><given-names>B</given-names></name><name><surname>Debr√©geas</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neural assemblies uncovered by generative modeling explain whole-brain activity statistics and reflect structural connectivity</article-title><source>eLife</source><volume>12</volume><elocation-id>e83139</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.83139</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>ƒ∞</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Waskom</surname><given-names>M</given-names></name><name><surname>Botvinnik</surname><given-names>O</given-names></name><name><surname>O‚ÄôKane</surname><given-names>D</given-names></name><name><surname>Hobson</surname><given-names>P</given-names></name><name><surname>Lukauskas</surname><given-names>S</given-names></name><name><surname>Gemperline</surname><given-names>DC</given-names></name><name><surname>Augspurger</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Mwaskom/seaborn</data-title><version designator="v0.8.1">v0.8.1</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.883859">https://doi.org/10.5281/zenodo.883859</ext-link></element-citation></ref><ref id="bib46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Watanakeesuntorn</surname><given-names>W</given-names></name><name><surname>Takahashi</surname><given-names>K</given-names></name><name><surname>Ichikawa</surname><given-names>K</given-names></name><name><surname>Park</surname><given-names>J</given-names></name><name><surname>Sugihara</surname><given-names>G</given-names></name><name><surname>Takano</surname><given-names>R</given-names></name><name><surname>Haga</surname><given-names>J</given-names></name><name><surname>Pao</surname><given-names>GM</given-names></name></person-group><article-title>Massively Parallel Causal Inference of Whole Brain Dynamics at Single Neuron Resolution</article-title><conf-name>2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS).</conf-name><year iso-8601-date="2020">2020</year><conf-loc>Hong Kong</conf-loc><fpage>196</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1109/ICPADS51040.2020.00035</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>Zwart</surname><given-names>MF</given-names></name><name><surname>James</surname><given-names>B</given-names></name><name><surname>Rubinov</surname><given-names>M</given-names></name><name><surname>Wei</surname><given-names>Z</given-names></name><name><surname>Narayan</surname><given-names>S</given-names></name><name><surname>Vladimirov</surname><given-names>N</given-names></name><name><surname>Mensh</surname><given-names>BD</given-names></name><name><surname>Fitzgerald</surname><given-names>JE</given-names></name><name><surname>Ahrens</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A brainstem integrator for self-location memory and positional homeostasis in zebrafish</article-title><source>Cell</source><volume>185</volume><fpage>5011</fpage><lpage>5027</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2022.11.022</pub-id><pub-id pub-id-type="pmid">36563666</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Gao</surname><given-names>X</given-names></name><name><surname>Peng</surname><given-names>X</given-names></name><name><surname>Ye</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Attention-based recurrent temporal restricted boltzmann machine for radar high resolution range profile sequence recognition</article-title><source>Sensors</source><volume>18</volume><elocation-id>1585</elocation-id><pub-id pub-id-type="doi">10.3390/s18051585</pub-id><pub-id pub-id-type="pmid">29772725</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>R√≥zsa</surname><given-names>M</given-names></name><name><surname>Liang</surname><given-names>Y</given-names></name><name><surname>Bushey</surname><given-names>D</given-names></name><name><surname>Wei</surname><given-names>Z</given-names></name><name><surname>Zheng</surname><given-names>J</given-names></name><name><surname>Reep</surname><given-names>D</given-names></name><name><surname>Broussard</surname><given-names>GJ</given-names></name><name><surname>Tsang</surname><given-names>A</given-names></name><name><surname>Tsegaye</surname><given-names>G</given-names></name><name><surname>Narayan</surname><given-names>S</given-names></name><name><surname>Obara</surname><given-names>CJ</given-names></name><name><surname>Lim</surname><given-names>J-X</given-names></name><name><surname>Patel</surname><given-names>R</given-names></name><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Ahrens</surname><given-names>MB</given-names></name><name><surname>Turner</surname><given-names>GC</given-names></name><name><surname>Wang</surname><given-names>SS-H</given-names></name><name><surname>Korff</surname><given-names>WL</given-names></name><name><surname>Schreiter</surname><given-names>ER</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Hasseman</surname><given-names>JP</given-names></name><name><surname>Kolb</surname><given-names>I</given-names></name><name><surname>Looger</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Fast and sensitive GCaMP calcium indicators for imaging neural populations</article-title><source>Nature</source><volume>615</volume><fpage>884</fpage><lpage>891</lpage><pub-id pub-id-type="doi">10.1038/s41586-023-05828-9</pub-id><pub-id pub-id-type="pmid">36922596</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98489.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Latham</surname><given-names>Peter</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Useful</kwd></kwd-group></front-stub><body><p>This study introduces a <bold>useful</bold> extension to a recently proposed model of neural assembly activity. The extension was to add recurrent connections to the hidden units of the Restricted Boltzmann Machine. The authors show <bold>solid</bold> evidence that the new model outperforms their earlier model on both a simulated dataset and on whole-brain neural activity from zebrafish.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98489.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Understanding large-scale neural activity remains a formidable challenge in neuroscience. While several methods have been proposed to discover the assemblies from such large-scale recordings, most of previous studies do not explicit modeling the temporal dynamics. This study is an attempt to uncover the temporal dynamics of assemblies using a tool that have been establish in other domains.</p><p>The authors previously introduced the compositional Restricted Boltzmann Machine (cRBM) to identify neuron assemblies in zebrafish brain activity. Building upon this, they now employ the Recurrent Temporal Restricted Boltzmann Machine (RTRBM) to elucidate the temporal dynamics within these assemblies. By introducing recurrent connections between hidden units, RTRBM could retrieve neural assemblies and their temporal dynamics from simulated and zebrafish brain data.</p><p>Strengths:</p><p>The RTRBM has been previously used in other domains. Training the model has been already established. This study is an application of such model to neuroscience. Overall, the paper is well-structured and the methodology is robust, the analysis is solid to support the authors claim.</p><p>Weaknesses:</p><p>The overall degree of advance is very limited. The performance improvement by RTRBM compared to their cRBM is marginal, and insights into assembly dynamics are limited.</p><p>(1) The biological insights from this method are constrained. Though the aim is to unravel neural ensemble dynamics, the paper lacks in-depth discussion on how this method enhances our understanding of zebrafish neural dynamics. For example, the dynamics of assemblies can be analyzed using various tools such as dimensionality reduction methods once we have identified them using cRBM. What information can we gain by knowing the effective recurrent connection between them? It would be more convincing to show this in real data.</p><p>(2) Including predicted and measured neural activity traces could aid readers in evaluating model efficacy. The current version only contains comparison of the statistics, such as mean and covariance.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98489.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this work, the authors propose an extension to some of the last author's previous work, where a compositional restricted Boltzmann machine was considered as a generative model of neuron-assembly interaction. They augment this model by recurrent connections between the Boltzmann machine's hidden units, which allow them to explicitly account for temporal dynamics of the assembly activity. Since their model formulation does not allow the training towards a compositional phase (as in the previous model), they employ a transfer learning approach according to which they initialise their model with a weight matrix that was pre-trained using the earlier model so as to essentially start the actually training in a compositional phase. Finally, they test this model on synthetic and actual data of whole-brain light-sheet-microscopy recordings of spontaneous activity from the brain of larval zebrafish.</p><p>Strengths:</p><p>This work introduces a new model for neural assembly activity. Importantly, being able to capture temporal assembly dynamics is an interesting feature that goes beyond many existing models. While this work clearly focuses on the method (or the model) itself, it opens up an avenue for experimental research where it will be interesting to see if one can obtain any biologically meaningful insights considering these temporal dynamics when one is able to, for instance, relate them to development or behaviour.</p><p>Weaknesses:</p><p>For most of the work, the authors present their RTRBM model as an improvement over the earlier cRBM model. Yet, when considering synthetic data, they actually seem to compare with a &quot;standard&quot; RBM model. This seems odd considering the overall narrative and that when considering whole-brain zebrafish data, the comparisons were made between RTRBM and cRBM models. For that, the RTRBM model was initialised with the cRBM weight matrix to overcome the fact that RTRBM alone does not seem to converge to a compositional phase, so to cite the latter as reason does not really make sense.</p><p>Furthermore, whether the clusters shown in Figure 3E can indeed be described as &quot;spatially localized&quot; is debatable. Especially in view of clusters 3 and 4, this seems a stretch. If receptive fields are described as &quot;spatially localized&quot;, arguably, one would expect that they are contained in some small (compared to the overall size of the brain) or specific anatomical brain region. However, this is clearly not the case here.</p><p>In addition, the performance comparison for the temporal dynamics of the hidden units actually suggests that the RTRBM (significantly) underperforms where the text says (Line 235f) it outperforms the cRBM model.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98489.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Quiroz Monnens</surname><given-names>Sebastian</given-names></name><role specific-use="author">Author</role><aff><institution>Computational Neuroscience Lab, Donders Institute for Cognition, Brain and Behavior , Radboud University</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib><contrib contrib-type="author"><name><surname>Peters</surname><given-names>Casper</given-names></name><role specific-use="author">Author</role><aff><institution>Computational Neuroscience Lab, Donders Institute for Cognition, Brain and Behavior , Radboud University</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib><contrib contrib-type="author"><name><surname>Hesselink</surname><given-names>Luuk Willem</given-names></name><role specific-use="author">Author</role><aff><institution>Computational Neuroscience Lab, Donders Institute for Cognition, Brain and Behavior , Radboud University</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib><contrib contrib-type="author"><name><surname>Smeets</surname><given-names>Kasper</given-names></name><role specific-use="author">Author</role><aff><institution>Computational Neuroscience Lab, Donders Institute for Cognition, Brain and Behavior , Radboud University</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib><contrib contrib-type="author"><name><surname>Englitz</surname><given-names>Bernhard</given-names></name><role specific-use="author">Author</role><aff><institution>Computational Neuroscience Lab, Donders Center for Neuroscience, Radboud University</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors‚Äô response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>Understanding large-scale neural activity remains a formidable challenge in neuroscience. While several methods have been proposed to discover the assemblies from such large-scale recordings, most previous studies do not explicitly model the temporal dynamics. This study is an attempt to uncover the temporal dynamics of assemblies using a tool that has been established in other domains.</p><p>The authors previously introduced the compositional Restricted Boltzmann Machine (cRBM) to identify neuron assemblies in zebrafish brain activity. Building upon this, they now employ the Recurrent Temporal Restricted Boltzmann Machine (RTRBM) to elucidate the temporal dynamics within these assemblies. By introducing recurrent connections between hidden units, RTRBM could retrieve neural assemblies and their temporal dynamics from simulated and zebrafish brain data.</p><p>Strengths:</p><p>The RTRBM has been previously used in other domains. Training in the model has been already established. This study is an application of such a model to neuroscience. Overall, the paper is well-structured and the methodology is robust, the analysis is solid to support the authors' claim.</p><p>Weaknesses:</p><p>The overall degree of advance is very limited. The performance improvement by RTRBM compared to their cRBM is marginal, and insights into assembly dynamics are limited.</p><p>(1) The biological insights from this method are constrained. Though the aim is to unravel neural ensemble dynamics, the paper lacks in-depth discussion on how this method enhances our understanding of zebrafish neural dynamics. For example, the dynamics of assemblies can be analyzed using various tools such as dimensionality reduction methods once we have identified them using cRBM. What information can we gain by knowing the effective recurrent connection between them? It would be more convincing to show this in real data.</p></disp-quote><p>See below in the recommendations section.</p><disp-quote content-type="editor-comment"><p>(2) Despite the increased complexity of RTRBM over cRBM, performance improvement is minimal. Accuracy enhancements, less than 1 in synthetic and zebrafish data, are underwhelming (Figure 2G and Figure 4B). Predictive performance evaluation on real neural activity would enhance model assessment. Including predicted and measured neural activity traces could aid readers in evaluating model efficacy.</p></disp-quote><p>See below in the recommendations section.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations:</bold></p><p>(1) The biological insights from this method are constrained. Though the aim is to unravel neural ensemble dynamics, the paper lacks in-depth discussion on how this method enhances our understanding of zebrafish neural dynamics. For example, the dynamics of assemblies can be analyzed using various tools such as dimensionality reduction methods once we have identified them using cRBM. What information can we gain by knowing the effective recurrent connection between them? It would be more convincing to show this in real data.</p></disp-quote><p>We agree with the reviewer that our analysis does not explore the data far enough to reach the level of new biological insights. For practical reasons unrelated to the science, we cannot further explore the data in this direction at this point, however, funding permitting, we will pick up this question at a later stage. The only change we have made to the corresponding figure at the current stage was to adapt the thresholds, which better emphasizes the locality of the resulting clusters.</p><disp-quote content-type="editor-comment"><p>(2) Despite the increased complexity of RTRBM over cRBM, performance improvement is minimal. Accuracy enhancements, less than 1 in synthetic and zebrafish data, are underwhelming (Figure 2G and Figure 4B). Predictive performance evaluation on real neural activity would enhance model assessment. Including predicted and measured neural activity traces could aid readers in evaluating model efficacy.</p></disp-quote><p>We thank the reviewer kindly for the comments on the performance comparison between the two models. We would like to highlight that the small range of accuracy values for the predictive performance is due to both the sparsity and stochasticity of the simulated data, and is not reflective of the actual percentage in performance improvement. To this end, we have opted to use a rescaled metric that we call the normalised Mean Squared Error (nMSE), where the MSE is equal to 1 minus the accuracy, as the visible units take on binary values. This metric is also more in line with the normalised Log-Likelihood (nLLH) metric used in the cRBM paper in terms of interpretability. The figure shows that the RTRBM can significantly predict the state of the visible units in subsequent time-steps, whereas the cRBM captures the correct time-independent statistics but has no predictive power over time.</p><p>We also thank the reviewer for pointing out that there is no predictive performance evaluation on the neural data. This has been chosen to be omitted for two reasons. First, it is clear from Fig. 2 that the (c)RBM has no temporal dependencies, meaning that the predictive performance is determined mostly by the average activity of the visible units. If this corresponds well with the actual mean activity per neuron, the nMSE will be around 0. This correspondence is already evaluated in the first panel of 3F. Second, as this is real data, we can not make an estimate of a lower bound on the MSE that is due to neural noise. Because of this, the scale of the predictive performance score will be arbitrary, making it difficult to quantitatively assess the difference in performance between both models.</p><disp-quote content-type="editor-comment"><p>(3) The interpretation of the hidden real variable <inline-formula><mml:math id="sa3m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> lacks clarity. Initially interpreted as the expectation of <inline-formula><mml:math id="sa3m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, its interpretation in Eq (8) appears different. Clarification on this link is warranted.</p></disp-quote><p>We thank the reviewer kindly for the suggested clarification. However, we think the link between both values should already be sufficiently clear from the text in lines 469-470:</p><p>‚ÄúImportantly, instead of using binary hidden unit states ùê°[ùë°‚àí1], sampled from the expected real valued hidden states ùê´[ùë°‚àí1], the RTRBM propagates these real-valued hidden unit states directly.‚Äù</p><p>In other words, both indeed are the same, one could sample a binary-valued ùê°[ùë°-1] from the real-valued ùê´[ùë°-1] through e.g. a Bernoulli distribution, where ùê´[ùë°-1] would thus indeed act as an expectation over ùê°[ùë°‚àí1]. However, the RTRBM formulation keeps the real-valued ùê´[ùë°-1] to propagate the hidden-unit states to the next time-step. The motivation for this choice is further discussed in the original RTRBM paper (Sutskever et al. 2008).</p><disp-quote content-type="editor-comment"><p>(4) In Figure 3 panel F, the discrepancy in x-axis scales between upper and lower panels requires clarification. Explanation regarding the difference and interpretation guidelines would enhance understanding.</p></disp-quote><p>Thank you for pointing out the discrepancy in x-axis scales between the upper and lower panels of Figure 3F. The reason why these scales are different is that the activation functions in the two models differ in their range, and showing them on the same scale would not do justice to this difference. But we agree that this could be unclear for readers. Therefore we added an additional clarification for this discrepancy in line 215:</p><p>‚ÄúWhile a direct comparison of the hidden unit activations between the cRBM and the RTRBM is hindered by the inherent discrepancy in their activation functions (unbounded and bounded, respectively), the analysis of time-shifted moments reveals a stronger correlation for the RTRBM hidden units (<inline-formula><mml:math id="sa3m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.92</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="sa3m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) compared to the cRBM (<inline-formula><mml:math id="sa3m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.88</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="sa3m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>)‚Äù</p><disp-quote content-type="editor-comment"><p>(5) Assessing model performance at various down-sampling rates in zebrafish data analysis would provide insights into model robustness.</p></disp-quote><p>We agree that we would have liked to assess this point in real data, to verify that this holds as well in the case of the zebrafish whole-brain data. The main reason why we did not choose to do this in this case is that we would only be able to further downsample the data. Current whole brain data sets are collected at a few Hz (here 4 Hz, only 2 Hz in other datasets), which we consider to be likely slower than the actual interaction speed in neural systems, which is on the order of milliseconds between neurons, and on the order of ~100 ms (~10 Hz) between assemblies. Therefore reducing the rate further, we expect to only see a reduction in quality, which we considered less interesting than finding an optimum. Higher rates of imaging in light-sheet imaging are only achievable currently by imaging only single planes (which defies the goal of whole brain recordings), but may be possible in the future when the limiting factors (focal plane stepping and imaging) are addressed. For completeness, we have now performed the downstepping for the experimental data, which showed the expected decrease in performance. The results have been integrated into Figure 4.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>In this work, the authors propose an extension to some of the last author's previous work, where a compositional restricted Boltzmann machine was considered as a generative model of neuron-assembly interaction. They augment this model by recurrent connections between the Boltzmann machine's hidden units, which allow them to explicitly account for temporal dynamics of the assembly activity. Since their model formulation does not allow the training towards a compositional phase (as in the previous model), they employ a transfer learning approach according to which they initialise their model with a weight matrix that was pre-trained using the earlier model so as to essentially start the actually training in a compositional phase. Finally, they test this model on synthetic and actual data of whole-brain light-sheet-microscopy recordings of spontaneous activity from the brain of larval zebrafish.</p><p>Strengths:</p><p>This work introduces a new model for neural assembly activity. Importantly, being able to capture temporal assembly dynamics is an interesting feature that goes beyond many existing models. While this work clearly focuses on the method (or the model) itself, it opens up an avenue for experimental research where it will be interesting to see if one can obtain any biologically meaningful insights considering these temporal dynamics when one is able to, for instance, relate them to development or behaviour.</p><p>Weaknesses:</p><p>For most of the work, the authors present their RTRBM model as an improvement over the earlier cRBM model. Yet, when considering synthetic data, they actually seem to compare with a &quot;standard&quot; RBM model. This seems odd considering the overall narrative, and it is not clear why they chose to do that. Also, in that case, was the RTRBM model initialised with the cRBM weight matrix?</p></disp-quote><p>Thank you for raising the important point regarding the RTRBM comparison in the synthetic data section. Initially, we aimed to compare the performance of the cRBM with the cRTRBM. However, we encountered significant challenges in getting the RTRBM to reach the compositional phase. To ensure a fair and robust comparison, we opted to compare the RBM with the RTRBM.</p><disp-quote content-type="editor-comment"><p>A few claims made throughout the work are slightly too enthusiastic and not really supported by the data shown. For instance, when the authors refer to the clusters shown in Figure 3D as &quot;spatially localized&quot;, this seems like a stretch, specifically in view of clusters 1, 3, and 4.</p></disp-quote><p>Thanks for pointing out this inaccuracy. When going back to the data/analyses to address the question about locality, we stumbled upon a minor bug in the implementation of the proportional thresholding, causing the threshold to be too low and therefore too many neurons to be considered.</p><p>Fixing this bug reduces the number of neurons, thereby better showing the local structure of the clusters. Furthermore, if one would lower the threshold within the hierarchical clustering, smaller, and more localized, clusters would appear. We deliberately chose to keep this threshold high to not overwhelm the reader with the number of identified clusters. We hope the reviewer agrees with these changes and that the spatial structure in the clusters presented are indeed rather localized.</p><disp-quote content-type="editor-comment"><p>Moreover, when they describe the predictive performance of their model as &quot;close to optimal&quot; when the down-sampling factor coincided with the interaction time scale, it seems a bit exaggerated given that it was more or less as close to the upper bound as it was to the lower bound.</p></disp-quote><p>We thank the reviewer for catching this error. Indeed, the best performing model does not lay very close to the estimated performance of an optimal model. The text has been updated to reflect this.</p><disp-quote content-type="editor-comment"><p>When discussing the data statistics, the authors quote correlation values in the main text. However, these do not match the correlation values in the figure to which they seem to belong. Now, it seems that in the main text, they consider the Pearson correlation, whereas in the corresponding figure, it is the Spearman correlation. This is very confusing, and it is not really clear as to why the authors chose to do so.</p></disp-quote><p>Thank you for identifying the discrepancy between the correlation values mentioned in the text and those presented in the figure. We updated the manuscript to match the correlation coefficient values in the figure with the correct values denoted in the text.</p><disp-quote content-type="editor-comment"><p>Finally, when discussing the fact that the RTRBM model outperforms the cRBM model, the authors state it does so for different moments and in different numbers of cases (fish). It would be very interesting to know whether these are the same fish or always different fish.</p></disp-quote><p>Thank you for pointing this out. Keeping track of the same fish across the different metrics makes sense. We updated the figure to include a color code for each individual fish. As it turns out each time the same fish are significantly better performing.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations:</bold></p><p>Figure 1: While the schematic in A and D only shows 11 visible units (&quot;neurons&quot;), the weight matrices and the activity rasters in B and C and E and F suggest that there should be, in fact, 12 visible units. While not essential, I think it would be nice if these numbers would match up.</p></disp-quote><p>Thank you for pointing out the inconsistency in the number of visible units depicted in Figure 1. We agree that this could have been confusing for readers. The figure has been updated accordingly. As you suggested, the schematic representation now accurately reflects the presence of 12 visible units in both the RBM and RTRBM models.</p><disp-quote content-type="editor-comment"><p>Figure 3: Panel G is not referenced in the main text. Yet, I believe it should be somewhere in lines 225ff.</p></disp-quote><p>Thank you for mentioning this. We added in line 233 a reference to figure 3 panel G to refer to the performance of the cRBM and RTRBM on the different fish.</p><disp-quote content-type="editor-comment"><p>Line 637ff: The authors consider moments <inline-formula><mml:math id="sa3m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>Œº</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa3m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and from the context, it seems they are not the same. However, it is not clear as to why because, judging from the notation, they should be the same.</p></disp-quote><p>The second-order statistic <inline-formula><mml:math id="sa3m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> on line 639 was indeed already mentioned and denoted as <inline-formula><mml:math id="sa3m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‚ü®</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>Œº</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‚ü©</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> on line 638. It has now been removed accordingly in the updated manuscript.</p><disp-quote content-type="editor-comment"><p>I found the usage of √õ and U throughout the manuscript a bit confusing. As far as I understand, √õ is a learned representation of U. However, maybe the authors could make the distinction clearer.</p></disp-quote><p>We understand the usage of √õ and U throughout the text may be confusing for the reader. However, we would like to notify the reviewer that the distinction between these two variables is explained in line 142: ‚Äúin addition to providing a close estimate (√õ) to the true assembly connectivity matrix U‚Äù. However, for added clarification to the reader, we added additional mentions of the estimated nature of √õ throughout the text in the updated manuscript.</p><disp-quote content-type="editor-comment"><p>Equation 3: It would be great if the authors could provide some more explanation of how they arrived at the identities.</p></disp-quote><p>These identities have previously been widely described in literature. For this reason, we decided not to include their derivation in our manuscript. However, for completeness, we kindly refer to:</p><p>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Chapter 20: Deep generative models [In <italic>Deep Learning</italic>]. MIT Press. <ext-link ext-link-type="uri" xlink:href="https://www.deeplearningbook.org/contents/generative_models.html">https://www.deeplearningbook.org/contents/generative_models.html</ext-link></p><disp-quote content-type="editor-comment"><p>Typos:</p><p>- L. 196: &quot;connectiivty&quot; -&gt; &quot;connectivity&quot;</p><p>- L. 197: Does it mean to say &quot;very strong stronger&quot;?</p><p>- L. 339: The reference to Dunn et al. (2016) should appear in parentheses.</p><p>- L. 504f: The colon should probably be followed by a full sentence.</p><p>- Eq. 2: In the first line, the potential V still appears, which should probably be changed to show the concrete form (-b * h) as in the second line.</p><p>- L. 351: Is there maybe a comma missing after &quot;cRBM&quot;?</p><p>- L. 271: Instead of &quot;correlation&quot;, shouldn't it rather be &quot;similarity&quot;? - L. 218: &quot;Figure 3D&quot; -&gt; &quot;Figure 3F&quot;</p></disp-quote><p>We thank the reviewer for pointing out these typos, which have all (except one) been fixed in the text. We do emphasize the potential V to show that there are alternative hidden unit potentials that can be chosen. For instance, the cRBM utilizes dReLu hidden unit potentials.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>With ever-growing datasets, it becomes more challenging to extract useful information from such a large amount of data. For that, developing better dimensionality reduction/clustering methods can be very important to make sense of analyzed data. This is especially true for neuroscience where new experimental advances allow the recording of an unprecedented number of neurons. Here the authors make a step to help with neuronal analyses by proposing a new method to identify groups of neurons with similar activity dynamics. I did not notice any obvious problems with data analyses here, however, the presented manuscript has a few weaknesses:</p><p>(1) Because this manuscript is written as an extension of previous work by the same authors (van der Plas et al., eLife, 2023), thus to fully understand this paper it is required to read first the previous paper, as authors often refer to their previous work for details. Similarly, to understand the functional significance of identified here neuronal assemblies, it is needed to go to look at the previous paper.</p></disp-quote><p>We agree that the present Research Advance has been written in a way that builds on our previous publication. It was our impression that this was the intention of the Research Advance format, as spelled out in its announcement &quot;eLife has introduced an innovative new type of article ‚Äì the Research Advance ‚Äì that invites the authors of any eLife paper to present significant additions to their original research&quot;. In the previous formatting guidelines from eLife this was more evident with a strong limitation on the number of figures and words, however, also for the present, more liberal guidelines, place an emphasis on the relation to the previous article. We have nonetheless tried in several places to fill in details that might simplify the reading experience.</p><disp-quote content-type="editor-comment"><p>(2) The problem of discovering clusters in data with temporal dynamics is not unique to neuroscience. Therefore, the authors should also discuss other previously proposed methods and how they compare to the presented here RTRBM method. Similarly, there are other methods using neural networks for discovering clusters (assemblies) (e.g. t-SNE: van der Maaten &amp; Hinton 2008, Hippocluster: Chalmers et al. 2023, etc), which should be discussed to give better background information for the readers.</p></disp-quote><p>The clustering methods suggested by the reviewer do not include modeling any time dependence, which is the crucial advance presented here by the introduction of the RTRBM, in extending the (c)RBM. In our previous publication on the cRBM (an der Plas et al., eLife, 2023), this comparison was part of the discussion, although it focussed on a different set of methods. While clustering methods like t-SNE, UMAP and others certainly have their value in scientific analysis, we think it might be misleading the reader to think that they achieve the same task as an RTRBM, which adds the crucial dimension of temporal dependence.</p><disp-quote content-type="editor-comment"><p>(3) The above point to better describe other methods is especially important because the performance of the presented here method is not that much better than previous work. For example, RTRBM outperforms the cRBM only on ~4 out of 8 fish datasets. Moreover, as the authors nicely described in the Limitations section this method currently can only work on a single time scale and clusters have to be estimated first with the previous cRBM method. Thus, having an overview of other methods which could be used for similar analyses would be helpful.</p></disp-quote><p>We think that the perception that the RTRBM performs only slightly better is based on a misinterpretation of the performance measure, which we have tried to address (see comments above) in this rebuttal and the manuscript. In addition we would like to emphasize that the structural estimation (which is still modified by the RTRBM, only seeded by the cRBMs output), as shown in the simulated data, makes improved structural estimates, which is important, even in cases where the performance is comparable (which can be the case if the RBM absorbs temporal dependencies of assemblies into modified structure of assemblies). We have clarified this now in the discussion.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations:</bold></p><p>(1) Line 181: it is not explained how a reconstruction error is defined.</p></disp-quote><p>Dear reviewer, thanks for pointing this out. A definition of the (mean square) reconstruction error is added in this line.</p><disp-quote content-type="editor-comment"><p>(2) How was the number of hidden neurons chosen and how does it affect performance?</p></disp-quote><p>Thank you for pointing this out. Due to the fact that we use transfer learning, the number of hidden units used for the RTRBM is given by the number of hidden units used for training the cRBM. In further research, when the RTRBM operates in the compositional phase, we can exploit a grid search over a set of hyper parameters to determine the optimal set of hidden units and other parameters.</p></body></sub-article></article>