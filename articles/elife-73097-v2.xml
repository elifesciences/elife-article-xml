<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">73097</article-id><article-id pub-id-type="doi">10.7554/eLife.73097</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Eye movements reveal spatiotemporal dynamics of visually-informed planning in navigation</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-249937"><name><surname>Zhu</surname><given-names>Seren</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0555-9690</contrib-id><email>lt1686@nyu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-211035"><name><surname>Lakshminarasimhan</surname><given-names>Kaushik J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3932-2616</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-251591"><name><surname>Arfaei</surname><given-names>Nastaran</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-138210"><name><surname>Angelaki</surname><given-names>Dora E</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9650-8962</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Center for Neural Science, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Center for Theoretical Neuroscience, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Department of Psychology, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Department of Mechanical and Aerospace Engineering, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Zhang</surname><given-names>Hang</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>03</day><month>05</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e73097</elocation-id><history><date date-type="received" iso-8601-date="2021-08-16"><day>16</day><month>08</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-05-01"><day>01</day><month>05</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-04-27"><day>27</day><month>04</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.04.26.441482"/></event></pub-history><permissions><copyright-statement>© 2022, Zhu et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Zhu</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-73097-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-73097-figures-v2.pdf"/><abstract><p>Goal-oriented navigation is widely understood to depend upon internal maps. Although this may be the case in many settings, humans tend to rely on vision in complex, unfamiliar environments. To study the nature of gaze during visually-guided navigation, we tasked humans to navigate to transiently visible goals in virtual mazes of varying levels of difficulty, observing that they took near-optimal trajectories in all arenas. By analyzing participants’ eye movements, we gained insights into how they performed visually-informed planning. The spatial distribution of gaze revealed that environmental complexity mediated a striking trade-off in the extent to which attention was directed towards two complimentary aspects of the world model: the reward location and task-relevant transitions. The temporal evolution of gaze revealed rapid, sequential prospection of the future path, evocative of neural replay. These findings suggest that the spatiotemporal characteristics of gaze during navigation are significantly shaped by the unique cognitive computations underlying real-world, sequential decision making.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>navigation</kwd><kwd>eye movements</kwd><kwd>planning</kwd><kwd>active sensing</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U19-NS118246</award-id><principal-award-recipient><name><surname>Zhu</surname><given-names>Seren</given-names></name><name><surname>Arfaei</surname><given-names>Nastaran</given-names></name><name><surname>Angelaki</surname><given-names>Dora E</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01-EY022538</award-id><principal-award-recipient><name><surname>Zhu</surname><given-names>Seren</given-names></name><name><surname>Arfaei</surname><given-names>Nastaran</given-names></name><name><surname>Angelaki</surname><given-names>Dora E</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DBI-1707398</award-id><principal-award-recipient><name><surname>Lakshminarasimhan</surname><given-names>Kaushik Janakiraman</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000324</institution-id><institution>Gatsby Charitable Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Lakshminarasimhan</surname><given-names>Kaushik Janakiraman</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The spatial and temporal patterns of eye movements exhibited by humans in virtual reality reveal how they plan paths when navigating in complex, naturalistic environments.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Planning, the evaluation of prospective future actions using a model of the environment, plays a critical role in sequential decision making (<xref ref-type="bibr" rid="bib34">Hunt et al., 2021</xref>; <xref ref-type="bibr" rid="bib54">Mattar and Lengyel, 2022</xref>). Two-step choice tasks have revealed quantitative evidence that humans are capable of flexible planning (<xref ref-type="bibr" rid="bib57">Momennejad et al., 2017</xref>; <xref ref-type="bibr" rid="bib56">Miller and Venditto, 2021</xref>; <xref ref-type="bibr" rid="bib83">Wunderlich et al., 2012</xref>). Under unfamiliar or uncertain task conditions, planning may depend upon and occur in conjunction with active sensing, the cognitively motivated process of gathering information from the environment (<xref ref-type="bibr" rid="bib40">Kaplan and Friston, 2018</xref>). After all, one cannot make decisions about the future without knowing what options are available. Humans and animals perform near-optimal active sensing via eye movements during binary decision making tasks (<xref ref-type="bibr" rid="bib84">Yang et al., 2016</xref>; <xref ref-type="bibr" rid="bib66">Renninger et al., 2007</xref>) and visual search tasks (<xref ref-type="bibr" rid="bib59">Najemnik and Geisler, 2005</xref>; <xref ref-type="bibr" rid="bib52">Ma et al., 2011</xref>; <xref ref-type="bibr" rid="bib33">Hoppe and Rothkopf, 2019</xref>). Such tasks are typically characterized by an observation model, a mapping between states and observations, and visual information serves to reduce uncertainty about the state. In contrast, sequential decision-making tasks require knowledge about the structure of the environment characterized by state transitions, and visual information can additionally contribute to reducing uncertainty about this structure. Therefore, the principles of visually-informed decision making uncovered in simplified, discrete settings may not generalize to natural behaviors like real-world navigation, which entails planning a sequence of actions rather than a binary choice. How can we study visually-informed planning in structured, naturalistic sequential decision-making ventures such as navigation?</p><p>Theoretical work suggests that information acquisition and navigational planning can be simultaneously achieved through active inference – orienting the sensory apparati to reduce uncertainty about task variables in the service of decision making (<xref ref-type="bibr" rid="bib40">Kaplan and Friston, 2018</xref>). Humans are fortuitously equipped with a highly evolved visual system to perform goal-oriented inference. By swiftly parsing a large, complex scene on a millisecond timescale, the eyes actively interrogate and efficiently gather information from different regions of space to facilitate complex computations (<xref ref-type="bibr" rid="bib48">Leigh and Kennard, 2004</xref>; <xref ref-type="bibr" rid="bib69">Schroeder et al., 2010</xref>). At the same time, eye movements are influenced by the contents of internal deliberation and the prioritization of goals in real-time, providing a faithful readout of important cognitive variables (<xref ref-type="bibr" rid="bib85">Yang et al., 2018</xref>; <xref ref-type="bibr" rid="bib25">Gottlieb and Oudeyer, 2018</xref>; <xref ref-type="bibr" rid="bib35">Hutton, 2008</xref>; <xref ref-type="bibr" rid="bib68">Ryan and Shen, 2020</xref>). Thus, eye tracking lends itself as a valuable tool for investigating how humans and animals gather information to plan action trajectories (<xref ref-type="bibr" rid="bib32">Hoppe et al., 2018</xref>; <xref ref-type="bibr" rid="bib29">Henderson et al., 2013</xref>; <xref ref-type="bibr" rid="bib14">Eckstein et al., 2017</xref>; <xref ref-type="bibr" rid="bib85">Yang et al., 2018</xref>).</p><p>Over the past few decades, research on eye movements has led to a growing consensus that the oculomotor system has evolved to prioritize top-down, cognitive guidance over image salience (<xref ref-type="bibr" rid="bib30">Henderson and Hayes, 2017</xref>; <xref ref-type="bibr" rid="bib28">Hayhoe and Ballard, 2005</xref>; <xref ref-type="bibr" rid="bib70">Schütt et al., 2019</xref>). During routine activities such as making tea, we tend to foveate specifically upon objects relevant to the task being performed (e.g. boiling water) while ignoring salient distractors (<xref ref-type="bibr" rid="bib28">Hayhoe and Ballard, 2005</xref>; <xref ref-type="bibr" rid="bib44">Kowler, 2011</xref>). A current consensus about active sensing is that gaze elucidates how humans mitigate uncertainty in a goal-oriented manner (<xref ref-type="bibr" rid="bib85">Yang et al., 2018</xref>). Therefore, we hypothesize that in the context of navigation, gaze will be directed towards the most informative regions of space, depending upon the specific relationship between the participant’s position and their goal. A candidate framework to formalize this hypothesis is reinforcement learning (RL), whereby the goal of behavior is cast in terms of maximizing total long-term reward (<xref ref-type="bibr" rid="bib77">Sutton and Barto, 2018</xref>). For example, this framework has been previously used to provide a principled account of why neuronal responses in the hippocampal formation depend upon behavioral policies and environmental geometries (<xref ref-type="bibr" rid="bib26">Gustafson and Daw, 2011</xref>; <xref ref-type="bibr" rid="bib76">Stachenfeld et al., 2017</xref>), as well as a unifying account of how the hippocampus samples memories to replay (<xref ref-type="bibr" rid="bib53">Mattar and Daw, 2018</xref>). Incidentally, RL provides a formal interpretation of active sensing, which can be understood as optimizing information sampling for the purpose of improving knowledge about the environment, allowing for better planning and ultimately greater long-term reward (<xref ref-type="bibr" rid="bib85">Yang et al., 2018</xref>). Here, we invoke the RL framework and hypothesize that eye movements should be directed towards spatial locations where small changes in the local structure of the environment can drastically alter the expected reward.</p><p>While active sensing manifests as overt behavior, the planning algorithms which underlie action selection are thought to be more covert (<xref ref-type="bibr" rid="bib34">Hunt et al., 2021</xref>). Researchers have proposed that certain neural codes, such as hierarchical representations, would support efficient navigational planning by exploiting structural redundancies in the environment (<xref ref-type="bibr" rid="bib80">Tomov et al., 2020</xref>; <xref ref-type="bibr" rid="bib74">Solway et al., 2014</xref>). There is also evidence for predictive sequential neural activations during sequence learning and visual motion viewing tasks (<xref ref-type="bibr" rid="bib50">Liu et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Ekman et al., 2017</xref>; <xref ref-type="bibr" rid="bib45">Kurth-Nelson et al., 2016</xref>). This is reminiscent of replay, a well-documented phenomenon in rodents during navigation (<xref ref-type="bibr" rid="bib37">Johnson and Redish, 2007</xref>; <xref ref-type="bibr" rid="bib60">Pfeiffer and Foster, 2013</xref>; <xref ref-type="bibr" rid="bib13">Dragoi and Tonegawa, 2011</xref>; <xref ref-type="bibr" rid="bib6">Brown et al., 2016</xref>). The mechanism by which humans perform visually-informed planning may similarly involve simulating sequences of actions that chart out potential trajectories, and chunking a chosen trajectory into subgoals for efficient implementation. As recent evidence shows that eye movements reflect the dynamics of internal beliefs during sensorimotor tasks (<xref ref-type="bibr" rid="bib46">Lakshminarasimhan et al., 2020</xref>), we hypothesize that participants’ gaze dynamics would also reveal sequential trajectory simulation, and thus reveal the strategies by which humans plan during navigation.</p><p>To test both hypotheses mentioned above, we designed a virtual reality navigation task where participants were asked to navigate to transiently visible targets using a joystick in unfamiliar arenas of varying degrees of complexity. We found that human participants balanced foveating the hidden reward location with viewing highly task-consequential regions of space both prior to and during active navigation, and that environmental complexity mediated a trade-off between the two modes of information sampling. The experiment also revealed that participants’ eyes indeed rapidly traced the trajectories which they subsequently embarked upon, with such sweeps being more prevalent in complex environments. Furthermore, participants seemed to decompose convoluted trajectories by focusing their gaze on one turn at a time until they reached their goal. Taken together, these results suggest that the spatiotemporal dynamics of gaze are significantly shaped by cognitive computations underlying sequential decision making tasks like navigation.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Humans use vision to efficiently navigate to hidden goals in virtual arenas</title><p>To study human eye movements during naturalistic navigation, we designed a virtual reality (VR) task in which participants navigated to hidden goals in hexagonal arenas. As we desired to elicit the most naturally occurring eye movements, we used a head-mounted VR system with a built-in eye tracker to provide a full immersion navigation experience with few artificial constraints. Participants freely rotated in a swivel chair and used an analog joystick to control their forward and backward motion along the direction in which they were facing (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The environment was viewed from a first-person perspective through an HTC Vive Pro headset with a wide field of view, and several eye movement parameters were recorded using built-in software.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Participants exhibit near-optimal navigation performance across multiple environments.</title><p>(<bold>A</bold>) Left: Human participants wore a VR headset and executed turns by rotating in a swivel chair, while translating forwards or backwards using an analog joystick. Right: A screenshot of the first-person view of the display. The headset conferred an immersive field of view of 110°. (<bold>B</bold>) Aerial view showing the layout of the arenas. (<bold>C</bold>) Arenas ranged in complexity, which is related to negative mean state closeness centrality. (<bold>D</bold>) Heatmap showing the value function corresponding to an arbitrary goal state (closed circle) in one of the arenas. The value of each state is related to the geodesic distance between that state and the goal. Dashed line denotes the optimal trajectory from an example starting state (open circle). (<bold>E</bold>) Trajectories from an example trial in each arena, executed by one participant. The optimal trajectory is superimposed in black (dashed line). Time is color-coded. (<bold>F</bold>) Comparison of the empirical path length against the path length predicted by the optimal policy. The gray shaded region denotes the width of the outer reward zone (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). Left: Data points are colored in accordance to the colors of each arena as depicted in B. Right: Unrewarded trials (red) vs rewarded trials (green) had similar path lengths. For both plots, all trials for all participants and all arenas are superimposed. (<bold>G</bold>) Across participants, the average ratio of observed vs optimal (predicted) trajectory lengths is consistently around 1 in all arenas. (<bold>H</bold>) The search epoch was defined as the period between goal stimulus appearance and goal stimulus foveation. A threshold applied on the filtered joystick input (movement velocity) was used to delineate the pre-movement and movement epochs. (<bold>I</bold>) The average duration of the pre-movement (orange) and movement epochs (blue; colored according to the scheme in H) increased with arena complexity, in conjunction with the trial-level effects exerted by path lengths (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3A</xref>). (<bold>J</bold>) The relative planning time, calculated as the ratio of pre-movement to total trial time after goal foveation, was higher for more complex arenas. For G, I, and J, error bars denote ±1 SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Experimental details and behavioral performance.</title><p>(<bold>A</bold>) Arenas were regular hexagons of side length 10 m with a triangular tessellation of side length 2 m. Two points were rewarded if participants reached the goal state (green), and one point was rewarded if participants reached a state neighboring the goal state (light green). (<bold>B</bold>) Mean state closeness centrality of each arena, where higher centrality values correspond to a less complex arena. Error bars denote ±1 SD across states. (<bold>C</bold>) To incorporate twelve degrees of freedom in translation, value functions were computed using dynamic programming, whereby the cost of actions scaled in accordance with the center-to-center distance between states <inline-formula><mml:math id="inf1"><mml:mi>s</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf2"><mml:msup><mml:mi>s</mml:mi><mml:mo mathvariant="normal">′</mml:mo></mml:msup></mml:math></inline-formula> (pertaining to the transition which results from taking action <inline-formula><mml:math id="inf3"><mml:mi>a</mml:mi></mml:math></inline-formula>). (<bold>D</bold>) Top: Across all participants and all trials, the probability of being awarded two points (green) decreased with arena complexity, while the probability of being awarded one point (light green) was relatively constant across all arenas. Middle: The total fraction of points earned decreased with arena complexity. Bottom: Distance between the stopping location and the goal in rewarded (green) and unrewarded (red) trials. Error bars denote ±1 SEM. (<bold>E</bold>) Each color corresponds to a single participant. Variables plotted against arena complexity are, from left to right: the probability that participants scored two points or one point, the percentage of total points scored, and the probability of being rewarded. (<bold>F</bold>) Linear mixed model with random intercepts and slopes for the effect of trial-specific variables (number of turns, length of optimal trajectory, relative bearing, and the number of trajectory options) on the distance between the participant’s stopping position and the goal (error). The path length best predicts stopping errors. The overlaid scatter shows fixed effect slope + participant − specific random effect slope, and all variables were z-scored prior to model fitting.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Effect of arena complexity on behavioral performance.</title><p>(<bold>A</bold>) Performance was stable across each block, as measured by the average probability of being rewarded on each trial (green), as well as the average ratio between the empirical and optimal path lengths (gray). (<bold>B</bold>) Left: Across all arenas (colored according to the coloring scheme introduced in <xref ref-type="fig" rid="fig1">Figure 1B</xref>), the path lengths observed in unrewarded trials were close to the optimal trajectory lengths between the starting state and the state at which participants stopped on these trials, suggesting that unrewarded trials were predominantly caused by participants forgetting the precise location of the target. Right: The ratio of observed to optimal path lengths (to the participants’ stopping location on unrewarded trials) was close to unity in all arenas. (<bold>C</bold>) Arena complexity predicts the fraction of rewarded trials in each arena. (<bold>D</bold>) Distribution of epoch durations across all participants and all trials. Pre-movement and movement occupied a greater fraction of the total trial time for less open arenas. (<bold>E</bold>) Some participants spent less time deliberating before movement, but this did not impact task performance. Relative pre-movement duration was defined as the average ratio of the duration of the pre-movement epoch to the duration of the entire trial after goal detection. The average proportion of time that participants spent making prospective eye movements prior to using the joystick did not correlate with their average path lengths across all arenas (gray), nor with the overall probability of them being rewarded (green). (<bold>F</bold>) The relative pre-movement duration did not differ between rewarded (green) and unrewarded trials (red), except for the two easiest arenas (where planning demands are low). This suggests that unrewarded trials are not merely caused by poor planning. (<bold>G</bold>) Linear mixed models with random intercepts and slopes for the effect of trial-specific variables on epoch durations – left: pre-movement; right: movement; and relative pre-movement duration, top right. (Please see the description for <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F</xref> for further details.) The number of turns and the length of the optimal trajectory had the greatest effect on epoch durations. All error bars denote ±1 SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-fig1-figsupp2-v2.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Effect of path length and arena complexity on epoch durations and gaze.</title><p>(<bold>A</bold>) Slopes obtained by linear regression of the pre-movement epoch duration (left) or relative pre-movement epoch duration (right) against path length and arena complexity, averaged across participants. Overlaid scatter depicts participant-specific slopes. Both arena-level and trial-level variables influence these dependent variables, and notably exert opposing effects in the case of relative pre-movement duration. (<bold>B</bold>) Left: The negative trend in the duration of gaze at the goal vs. arena complexity during pre-movement observed in <xref ref-type="fig" rid="fig2">Figure 2D</xref> can be mostly explained by the longer trajectories in more complex arenas. Right: However, the stronger negative trend during movement can be predicted by both arena complexity and path lengths. (<bold>C</bold>) The fraction of time sweeping in the backwards direction pre-movement can be more strongly explained by arena-level effects than trial path lengths (top left). Both arena complexity and path lengths influence the overall fraction of time sweeping during movement (right) and the fraction of time sweeping forward pre-movement (bottom left).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-fig1-figsupp3-v2.tif"/></fig></fig-group><p>Facilitating quantitative analyses, we designed arenas with a hidden underlying triangular tessellation, where each triangular unit (covering 0.67% of the total area) constituted a <italic>state</italic> in a discrete state space (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). A fraction of the edges of the tessellation was chosen to be impassable barriers, defined as obstacles. Participants could take <italic>actions</italic> to achieve <italic>transitions</italic> between adjacent states which were not separated by obstacles. As participants were free to rotate and/or translate, the space of possible actions was continuous such that participants did not report knowledge about the tessellation. Furthermore, participants experienced a relatively high vantage point and were able to gaze over the tops of all of the obstacles (<xref ref-type="fig" rid="fig1">Figure 1A</xref>).</p><p>On each trial, participants were tasked to collect a <italic>reward</italic> by navigating to a random goal location drawn uniformly from all states in the arena. The goal was a realistic banana which the participants had to locate and foveate in order to unlock the joystick. The banana disappeared 200 ms after foveation, as we wanted to discourage beaconing. Participants were instructed to press a button when they believed that they have arrived at the remembered goal location. Then, feedback was immediately displayed on the screen, showing participants that they had received either two points for stopping within the goal state, one point for stopping in a state sharing a border with the goal state (up to three possible), or zero for stopping in any other state. While participants viewed the feedback, a new goal for the next trial was spawned without breaking the continuity of the task. In separate blocks, participants navigated to 50 goals in each of five different arenas (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). All five arenas were designed by defining the obstacle configurations such that the arenas varied in the average path length between two states, as quantified by the average state closeness centrality (Methods – <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>, <xref ref-type="bibr" rid="bib36">Javadi et al., 2017</xref>). One of the blocks involved an entirely open arena that contained only a few obstacles at the perimeter, such that on most trials, participants could travel in a straight line to the goal location (<xref ref-type="fig" rid="fig1">Figure 1B</xref> – leftmost). On the other extreme was a maze arena in which most pairs of states were connected by only one viable path (<xref ref-type="fig" rid="fig1">Figure 1B</xref> – rightmost). Because lower centrality values correspond to more complex arenas, negative centrality can be interpreted as a measure of arena complexity. For simplicity, we defined complexity using a linear transformation of negative centrality such that the open arena had a complexity value of zero, and we used this scale throughout the paper (see Methods; <xref ref-type="fig" rid="fig1">Figure 1C</xref>). We captured both within-participant and between-participant variability by fitting linear mixed effects (LME) models with random slopes and intercepts to predict trial-specific outcomes, and found consistent effects across participants. Therefore, we primarily show average trends in the main text, but participant-specific effects are included in <xref ref-type="table" rid="app2table1 app2table2 app2table3 app2table4">Appendix 2—Tables 1–4</xref>.</p><p>To quantify behavioral performance, we first computed the optimal trajectory for each trial using dynamic programming, an efficient algorithm with guaranteed convergence. This technique uses two pieces of information — the goal location (<italic>reward function</italic>) and the obstacle configuration (<italic>transition structure</italic>) — to find an optimal <italic>value function</italic> over all states such that the value of each state is equal to the (negative) length of the shortest path between that state and the goal state (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>). The <italic>optimal policy</italic> requires that participants select actions to climb the value function along the direction of steepest ascent, which would naturally bring them to the goal state while minimizing the total distance traveled. <xref ref-type="fig" rid="fig1">Figure 1E</xref> shows optimal (dashed) as well as behavioral (colored) trajectories from an example trial in each arena. Behavioral path lengths were computed by integrating changes in the participants’ position in each trial.</p><p>Although participants occasionally took a suboptimal route (<xref ref-type="fig" rid="fig1">Figure 1E</xref> – second from right), they took near-optimal paths (i.e. optimal to within the width of the reward zone) on most trials (<xref ref-type="fig" rid="fig1">Figure 1F</xref>), scoring (mean±SD across participants) 72 ± 7% of the points across all arenas and stopping within the reward zone on 85 ± 6% of all trials (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1D–E</xref>). We quantified the degree of optimality by computing the ratio of observed vs optimal path lengths to the participants’ stopping location. Across all rewarded trials, this ratio was close to unity (1.1 ± 0.1), suggesting that participants were able to navigate efficiently in all arenas (<xref ref-type="fig" rid="fig1">Figure 1G</xref>). Navigational performance was near-optimal from the beginning, such that there was no visible improvement with experience (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref>). Even on unrewarded trials, participants took a trajectory that is, on average, only 1.2±0.1 times the optimal path length from the participant’s initial state to their stopping location (<xref ref-type="fig" rid="fig1">Figure 1E</xref> – rightmost, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2B</xref>). This suggests that remembering the goal location was not straightforward. In fact, the fraction of rewarded trials decreased with increasing arena complexity (Pearson’s r(63) = –0.64, p=8 × <inline-formula><mml:math id="inf4"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>), suggesting that the ability to remember the goal location is compromised in challenging environments (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2C</xref>). Each trial poses unique challenges for the participant, such as the number of turns in the trajectory, the length of the trajectory, and the angle between the initial direction of heading and the direction of target approach (relative bearing). Among these variables, the length of the trajectory best predicts the error in the participants’ stopping position (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F</xref>).</p><p>In order to understand how participants tackled the computational demands of the task, it is critical to break down each trial into three main epochs: <italic>search</italic> – when participants sought to locate the goal, <italic>pre-movement</italic> – when participants surveyed their route prior to utilizing the joystick, and <italic>movement</italic> – when participants actively navigated to the remembered goal location (<xref ref-type="fig" rid="fig1">Figure 1H</xref>). On some trials, participants did not end the trial via button press immediately after stopping, but this post-movement period constituted a negligible proportion of the total trial time. Although participants spent a major portion of each trial navigating to the target, the relative duration of other epochs was not negligible (mean fraction ± SD – search: 0.27 ± 0.05, pre-movement: 0.11 ± 0.03, movement: 0.60 ± 0.06; <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2D</xref>). There was considerable variability across participants in the fraction of time spent in the pre-movement phase (coefficient of variation (CV) – search: 0.18, pre-movement: 0.31, movement: 0.10), although this did not translate to a significant difference in navigational precision (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2E</xref>). One possible explanation is that some participants were simply more efficient planners or were more skilled at planning on the move. While the duration of the search epoch was similar across arenas, the movement epoch duration increased drastically with increasing arena complexity (<xref ref-type="fig" rid="fig1">Figure 1I</xref>). This was understandable as the more complex arenas posed, on average, longer trajectories and more winding paths by virtue of their lower centrality values. Notably, the pre-movement duration was also higher in more complex arenas, reflecting the participant’s commitment to meet the increased planning demands in those arenas (<xref ref-type="fig" rid="fig1">Figure 1J</xref>). Nonetheless, the relative pre-movement duration was similar for rewarded and unrewarded trials (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2F</xref>). This suggests that the participants’ performance is limited by their success in remembering the reward location, rather than in meeting planning demands. On a finer scale, the duration of the pre-movement and movement epochs were both strongly influenced by the path length and the number of turns, but not by the bearing angle (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2G</xref>). Overall, these results suggest that in the presence of unambiguous visual information, humans are capable of adapting their behavior to efficiently solve navigation problems in complex, unfamiliar environments.</p></sec><sec id="s2-2"><title>A computational analysis supports that human eye movements are task relevant</title><p>Aiming to gain insights from participants’ eye movements, we begin by examining the spatial distribution of gaze positions during different trial epochs (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Within each trial, the spatial spread of the gaze position was much larger during visual search than during the other epochs (mean spread ± SD across participants – search: 5.8 ± 1.1 m, pre-movement: 2.0 ± 0.2 m, movement: 3.0 ± 0.6 m; <xref ref-type="fig" rid="fig2">Figure 2B</xref> – left). This pattern was reversed when examining the spatial spread across trials (mean spread ± SD – search: 5.8 ± 0.7 m, pre-movement: 6.5 ± 0.3 m, movement: 6.4 ± 0.4; <xref ref-type="fig" rid="fig2">Figure 2B</xref> – right). This suggests that participants’ eye movements during pre-movement and movement were chiefly dictated by trial-to-trial fluctuations in task demands. Furthermore, the variance of gaze positions within a trial, both prior to and during movement, was largely driven by the path length (<xref ref-type="fig" rid="fig2">Figure 2C</xref>) and therefore increased with arena complexity (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Eye movements are modulated by goal location and environment complexity.</title><p>(<bold>A</bold>) Eye positions on a representative trial for one participant during the three main trial epochs. Each datapoint corresponds to one frame. An open black circle denotes the start location, while a closed black circle denotes the goal. The color scheme applies to all plots in this figure. (<bold>B</bold>) Left: The median spatial spread of gaze within trial epochs (averaged across trials and arenas) was higher during search than during pre-movement and movement. Right: In contrast, the median spread of the average gaze positions across trials was higher during the pre-movement and movement epochs. Individual participant data are overlaid on top of the bars. (<bold>C</bold>) Left: A linear mixed model for the effect of trial-specific variables (number of turns, length of optimal trajectory, relative bearing) on the variance of gaze within the pre-movement epoch reveals that the expected path length has the greatest effect on gaze spread. The overlaid scatter shows fixed effect slope + participant - specific random effect slope. Right: Similar result for gaze spread within the movement epoch. (<bold>D</bold>) Left: Across participants, the average fraction of time for which gaze was near (within 2 m of) the center of the goal state decreased with arena complexity. The arena-level variable (complexity) and the trial-level equivalent (path length) both independently exert effects on the amount of time subjects looked at the goal (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3B</xref>). Right: Participants spent more time looking near the goal location when fewer turns separated them from the goal. (<bold>E</bold>) Left: A linear mixed model reveals that expected path length had the greatest negative effect on the fraction of time that participants spent gazing at the goal location prior to movement. Right: During movement, all measures of trial difficulty decreased goal-fixation behavior, especially the number of turns. (<bold>F</bold>) Left: The average distance between the gaze position and the goal state increased with arena complexity during pre-movement and movement. Right: The average distance of the point of gaze from the goal location decreases as the participant approaches the target. (<bold>G</bold>) Left: Expected path length best predicted the average distance of gaze to the goal prior to movement. Right: During movement, the number of turns and the expected path length most positively affected this statistic. All error bars denote ±1 SEM, and all variables were z-scored prior to model fitting.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Gaze variance.</title><p>(<bold>A</bold>) The variance of gaze within trials across reaches a peak in the second half of the pre-movement period. Because the pre-movement duration scales with arena complexity, variance was computed using a sliding window of length between 0.32 s (least complex arena) to 0.68 s (most complex arena), linearly spaced between the extremes for arenas of intermediate complexity. Endpoints that fell outside the window were discarded. Results were qualitatively similar when using a fixed window size for all arenas. (<bold>B</bold>) Gaze is increasingly concentrated around the goal/stopping location for easier arenas. The believed goal location was assumed to be the participants’ stopping position, and the point of gaze was visibly more concentrated around the stop location than around the true goal location (especially in the most complex arena). The effects of working memory on gaze were more apparent for easier, more open arenas. Each panel depicts eye movements on a random subset of trials (3 trials × 13 participants) in each arena. The origin (0,0) denotes the goal location or the stopping location. Raw gaze positions relative to these points are depicted during the pre-movement and movement epochs. Axis limits are ±15 m for all panels.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Gaze locations.</title><p>(<bold>A</bold>) Each color corresponds to a single participant. Variables plotted against arena complexity are — left: percent of time participants spent gazing at the target location, right: average distance of gaze to the target location. (<bold>B</bold>) The pre-movement duration in each arena (colored according to the color scheme used for <xref ref-type="fig" rid="fig1">Figure 1B</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>) does not significantly change across trials. (<bold>C</bold>) During search, participants spent a greater fraction of time foveating the arena borders (purple), and after the target was located, participants spent more time foveating the ground (green). While there appears to be a trend in the fraction of time foveating obstacles (orange) vs arena complexity, this is explained by a higher obstacle density in the more complex arenas. (<bold>D</bold>) Across all participants and all trials, the probability of gazing upon each obstacle remains relatively constant across all arenas during each epoch. All error bars denote ±1 SEM across participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-fig2-figsupp2-v2.tif"/></fig></fig-group><p>How did the task demands constrain human eye movements? Studies have shown that reward circuitry tends to orient the eyes toward the most valuable locations in space (<xref ref-type="bibr" rid="bib31">Hikosaka et al., 2006</xref>; <xref ref-type="bibr" rid="bib43">Koenig et al., 2017</xref>). Moreover, when the goal is hidden, it has been argued that fixating the hidden reward zone may allow for the oculomotor circuitry to carry the burden of remembering the latent goal location (<xref ref-type="bibr" rid="bib46">Lakshminarasimhan et al., 2020</xref>; <xref ref-type="bibr" rid="bib63">Postle et al., 2006</xref>). Consistent with this, participants spent a large fraction of time looking at the reward zone, and this statistic was interestingly higher during pre-movement (66 ± 10%) than during movement (54 ± 6%). However, goal fixation decreased with arena complexity (<xref ref-type="fig" rid="fig2">Figure 2D</xref> – left, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref> – left), resulting in a larger mean distance between the gaze and the goal in more complex arenas (<xref ref-type="fig" rid="fig2">Figure 2F</xref> – left, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref> – right). This effect could not be attributed to participants forgetting the goal location in more complex arenas, as we found a similar trend when analyzing gaze in relation to the eventual stopping location (which could be different from the goal location; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>). A more plausible explanation is that looking solely at the goal might prevent participants from efficiently learning the task-relevant transition structure of the environment, as the structure is both more instrumental to solving the task and harder to comprehend in more challenging arenas. If central vision is attracted to the remembered goal location only when planning demands are low, this tendency should become more prevalent as participants approach the target. Indeed, participants spend significantly more time looking at the goal when there is a straight path to the goal than when the obstacle configuration requires that they make at least one turn prior to arriving upon such a straight path (<xref ref-type="fig" rid="fig2">Figure 2D and F</xref> – right). Also in alignment with this explanation, trial-level analyses revealed that during pre-movement, the tendency to look at the goal substantially decreased with greater path lengths (<xref ref-type="fig" rid="fig2">Figure 2E and G</xref> – left). During movement, goal fixation has more diverse influences from affordances linked to navigation, especially as a greater amount of turns also decreased the amount of time participants dedicated to looking at the remembered goal location (<xref ref-type="fig" rid="fig2">Figure 2E and G</xref> – right).</p><p>As mentioned earlier, computing the optimal trajectory requires precisely knowing both the reward function as well as the transition structure. While examining the proximity of gaze to the goal reveals the extent to which eye movements are dedicated to encoding the reward function, how may we assess the effectiveness with which participants interrogate the transition structure of the environment to solve the task of navigating from point A to point B? In the case that a participant has a precise model of the transition structure of the environment, they would theoretically be capable of planning trajectories to the remembered goal location without vision. However, in this experiment, the arena configurations were unfamiliar to the participants, such that they would be quite uncertain about the transition structure. The finding that participants achieved near-optimal performance on even the first few trials in each arena (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref>) indicates that humans are capable of using vision to rapidly reduce their uncertainty about the aspects of the model needed to solve the task. This reduction in uncertainty could be accomplished in two ways: (i) by actively sampling visual information about the structure of the arena in the first few trials and then relying largely on the internal model later on after this information is consolidated, or (ii) by actively gathering visual samples throughout the experiment on the basis of immediate task demands on each trial. We found evidence in support of the second possibility — arena-specific pre-movement epoch durations did not significantly decrease across trials (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref>).</p><p>Did participants look at the most informative locations? Depending on the goal location, misremembering the location of certain obstacles would have a greater effect on the subjective value of actions than for other obstacles (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>, &quot;Relevance simulations&quot; in Appendix 1). We leveraged this insight and defined a metric to quantify the task-<italic>relevance</italic> of each transition by computing the magnitude of the change in value of the participant’s current state, for a given goal location, if the status of that transition was misremembered:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf5"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the relevance of the <inline-formula><mml:math id="inf6"><mml:msup><mml:mi>k</mml:mi><mml:mi>th</mml:mi></mml:msup></mml:math></inline-formula> transition for navigating from state <italic>s</italic><sub>0</sub> to the goal state <inline-formula><mml:math id="inf7"><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf8"><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> denotes the status of that transition (1 if it is passable and 0 if it is an obstacle), and <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the value of state <italic>s</italic><sub>0</sub> computed with respect to the goal state <inline-formula><mml:math id="inf10"><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:math></inline-formula> by setting <inline-formula><mml:math id="inf11"><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> to 1. It turns out that this measure of relevance is directly related to the magnitude of expected change in subjective value of the current state when looking at the <inline-formula><mml:math id="inf12"><mml:msup><mml:mi>k</mml:mi><mml:mi>th</mml:mi></mml:msup></mml:math></inline-formula> transition, provided that the transitions are stationary and the participant’s uncertainty is uniform across transitions (Supplementary Notes). Thus, maximally relevant transitions identified by <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> are precisely those which may engender the greatest changes of mind about the current return (i.e. utility) of the current state. On each trial, the transitions with the highest relevance strikingly correspond to bottleneck transitions that bridge clusters of interconnected states (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>). Relevance was also high for obstacles that precluded a straight path to the goal, as well as for transitions along the optimal trajectory (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1C</xref>). By defining relevance of transitions according to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, we can thus capture multiple task-relevant attributes in a succinct manner. In Appendix 1, we point to a generalization of this relevance measure for settings in which the transition structure is stochastic (e.g. in volatile environments) and the subjective uncertainty is heterogeneous (i.e. the participant is more certain about some transitions than others).</p><p>We quantified the usefulness of participants’ eye position on each frame as the relevance of the transition closest to the point of gaze, normalized by that of the most relevant transition in the entire arena given the goal state on that trial. Then, we constructed a distribution of shuffled relevance values by analyzing gaze with respect to a random goal location. <xref ref-type="fig" rid="fig3">Figure 3A</xref> shows the resulting cumulative distributions across trials for the average participant during the three epochs in an example arena. As expected, the relevance of participants’ gaze was not significantly different from chance during the search epoch, as the participant had not yet determined the goal location. However, relevance values were significantly greater than chance both during pre-movement and movement (median relevance for the most complex arena, pre-movement – true: 0.14, shuffled: 0.006; movement – true: 0.20, shuffled: 0.06; see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, <xref ref-type="table" rid="app2table2">Appendix 2—table 2</xref> for other arenas).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Eye movements reveal a cognitive trade-off between reward and transition encoding.</title><p>(<bold>A</bold>) Left: Cumulative distribution (computed by pooling trials from all participants) of average log (normalized) relevance values (colored line) and the corresponding shuffled distribution (gray) during search (left), pre-movement (center), and movement (right) epochs (data for the most complex arena is shown). Shaded regions denote 95% confidence bounds computed using Greenwood’s formula. Rightmost: ROC curves characterizing the gaze relevance during the three epochs. (<bold>B</bold>) Area under the ROC curves (AUC) for different epochs, colored according to the color scheme in A. (<bold>C-D</bold>) Similar plots as A-B, but for the distributions of the log fraction of the duration in each epoch spent gazing near (within 2 m of) the eventual stopping position (which was assumed to be the participants’ believed goal location). (<bold>E</bold>) AUC values of gaze relevance computed for the distributions of trial-averaged relevances, after excluding fixations within the reward zone, during the pre-movement (orange) and movement (blue) epochs. Black line represents best-fit linear regression model. (<bold>F</bold>) Similar to E, but showing the AUC values of gaze durations within the reward zone. All error bars were computed using bootstrapping.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Gaze relevance across epochs and arenas.</title><p>(<bold>A–E</bold>) Breakdown of <xref ref-type="fig" rid="fig3">Figure 3A, B and E</xref> for arenas 1 (most complex) through 5 (least complex).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-fig3-figsupp1-v2.tif"/></fig></fig-group><p>To concisely describe participants’ tendency to orient their gaze toward relevant transitions in a scale-free manner, we constructed receiver operating characteristic (ROC) curves by plotting the cumulative probability of shuffled gaze relevances against the cumulative probability of true relevances (<xref ref-type="fig" rid="fig3">Figure 3A</xref> – rightmost). An area under the ROC curve (AUC) greater (less) than 0.5 would indicate that the gaze relevance was significantly above (below) what is expected from a random gaze strategy. Across all arenas, the AUC was highest during the pre-movement epoch (<xref ref-type="fig" rid="fig3">Figure 3B</xref>; mean AUC ± SD – search: 0.52 ± 0.03, pre-movement: 0.77 ± 0.03, movement: 0.68 ± 0.07). This suggests that participants were most likely to attend to relevant transitions when contemplating potential actions before embarking upon the trajectory.</p><p>As the most relevant transitions can sometimes be found near the goal (e.g. <xref ref-type="fig" rid="fig1">Figure 1B</xref> – left), we investigated whether our evaluation of gaze relevance was confounded by the observation that participants spent a considerable amount of time looking at the goal location (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Therefore, we first quantified the tendency to look at the goal location in a manner analogous to the analysis of gaze relevance (<xref ref-type="fig" rid="fig3">Figure 3A–B</xref>) by computing the AUC constructed using the true vs shuffled distributions of the duration spent foveating the goal in each epoch (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Across all arenas, AUCs were high during the pre-movement and movement epochs, confirming that there was a strong tendency for participants to look at the goal location (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). When we excluded gaze positions that fell within the reward zone while computing relevance, we found that the degree to which participants looked at task-relevant transitions outside of the reward zone increased with arena complexity: the tendency to look at relevant transitions was greater in more complex arenas, falling to chance for the easiest arena (<xref ref-type="fig" rid="fig3">Figure 3E</xref>; Pearson’s r(63) – pre-movement: 0.40, p=0.004; movement: 0.28, p=0.05). In contrast, the tendency to look at the goal location followed the opposite trend and was greater in easier arenas (<xref ref-type="fig" rid="fig3">Figure 3F</xref>; Pearson’s r(63) – pre-movement: –0.46, p=0.0003; movement: –0.33, p=0.001). These analyses reveal a striking trade-off in the allocation of gaze between encoding the reward function and transition structure that closely mirrors the cognitive requirements of the task. This trade-off is not simply a consequence of directing gaze more/less often at the reward, as such temporal statistics are preserved while shuffling. Instead, it points to a strategy of directing attention away from the reward and towards <italic>task-relevant</italic> transitions in complex arenas. This compromise allowed participants to dedicate more time to surveying the task-relevant structure in complex environments and likely underlies their ability to take near-optimal paths in all environments, albeit at the cost of an increased tendency to forget the precise goal location in complex environments (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2C</xref>). The trade-off reported here is roughly analogous to the trade-off between looking ahead towards where you’re going and having to pay attention to signposts or traffic lights. One could get away with the former strategy while driving on rural highways whereas city streets would warrant paying attention to many other aspects of the environment to get to the destination.</p></sec><sec id="s2-3"><title>The temporal evolution of gaze includes distinct periods of sequential prospection</title><p>So far, we have shown that the spatial distribution of eye movements adapts to trial-by-trial fluctuations in task demands induced by changing the goal location and/or the environment. However, planning and executing optimal actions in this task requires dynamic cognitive computations within each trial. To gain insights into this process, we examined the temporal dynamics of gaze. <xref ref-type="fig" rid="fig4">Figure 4A</xref> shows a participant’s gaze in an example trial which has been broken down into nine epochs (pre-movement: I–VI, movement: VII–IX) for illustrative purposes (see <xref ref-type="video" rid="video1">Video 1</xref> for more examples). The participant initially foveated the goal location (epoch I), and their gaze subsequently traced a trajectory <italic>backwards</italic> from the goal state towards their starting position (II) roughly along a path which they subsequently traversed on that trial (dotted line). This sequential gaze pattern was repeated shortly thereafter (IV), interspersed by periods of non-sequential eye movements (III and V). Just before embarking on their trajectory, the gaze traced the trajectory, now in the <italic>forward</italic> direction, until the end of the first turn (VI). Upon reaching the first turning point in their trajectory (VII), they executed a similar pattern of sequential gaze from their current position toward the goal (VIII), tracing out the path which they navigated thereafter (IX). We refer to the sequential eye movements along the future trajectory in the backwards and forwards direction as <italic>backward sweeps</italic> and <italic>forward sweeps</italic>, respectively. During such sweeps, participants seemed to rapidly navigate their future paths with their eyes, and all participants exhibited sweeping eye movements without being explicitly instructed to plan their trajectories prior to navigating. The fraction of time that participants looked near the trajectories which they subsequently embarked upon increased with arena and trial difficulty (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). To algorithmically detect periods of sweeps, gaze positions on each trial were projected onto the trajectory taken by the participant by locating the positions along the trajectory closest to the point of gaze on each frame (Methods). On each frame, the length of the trajectory up until the point of the gaze projection was divided by the total trajectory length, and this ratio was defined as the ‘fraction of trajectory’. We used the increase/decrease of this variable to determine the start and end times of periods when the gaze traveled sequentially along the trajectory in the forward/backward directions (sweeps) for longer than chance (<xref ref-type="fig" rid="fig4">Figure 4B</xref>; see Methods).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Gaze traveled forwards and backwards along the intended trajectory.</title><p>(<bold>A</bold>) Spatial locations of gaze positions (the arrow of relative time within each window increases from violet to orange) and participant positions (violet to blue) during individual time windows demarcated in panel B. Panels in the bottom row correspond to time periods corresponding to sweeps. The participant’s trajectory from the starting location (open black circle) to the goal (closed black circle) is denoted by a black dashed line. (<bold>B</bold>) Time-series of the points on the trajectory that were closest to the participant’s gaze on each frame, expressed as a fraction (0: start of trajectory, 1: end of trajectory) during one example trial. Only frames during which the gaze position fell within 2 m of the trajectory are plotted. The gray trace shows the movement velocity of the participant during this trial. Red and green shaded regions highlight time windows during which the sweep classification algorithm detected backward and forward sweeps, respectively. In this trial, there were two backward sweeps before movement, and one forward sweep each before and during movement. (<bold>C</bold>) Across all participants, the fraction of time spent sweeping in the forward and backward directions within each epoch reveals an antiparallel effect: more time was spent sweeping forwards during movement than during pre-movement (top), whereas more time was spent sweeping backwards during pre-movement than during movement (bottom). Generally, the arena complexity as well as the trial-specific path lengths, both increase the fraction of time sweeping (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3C</xref>). Error bars denote ±1 SEM. (<bold>D</bold>) Linear mixed models with random intercepts and slopes for the effect of trial-specific variables (number of turns, length of optimal trajectory, relative bearing) on the fraction of time that participants spent sweeping their trajectory in the backward direction, separated for pre-movement and movement epochs. The overlaid scatter shows fixed effect slope + participant − specific random effect slope. (<bold>E</bold>) Similar analysis as <bold>D</bold>, but for forward sweeps. All variables were z-scored prior to model fitting.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Effect of arena complexity and alternative trajectories on gaze.</title><p>(<bold>A</bold>) Line plot: The fraction of time that participants spent gazing near (within 2 m from) the trajectory that they took on each trial (excluding points of gaze near the goal location) increased with arena complexity during both pre-movement (gold) and movement (blue). Bar graphs: Linear mixed models with random intercepts and slopes for the effect of trial-specific variables on the fraction of time participants gazed upon the trajectory. (Please see the description for <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F</xref> for further details.) The tendency to make on-trajectory eye movements increased with trial difficulty, and the expected trajectory length had the greatest effect on this statistic prior to movement. (<bold>B</bold>) Left: The fraction of trials with sweeps was lower for less complex arenas. Right: Each color corresponds to a single participant. (<bold>C</bold>) Eye movements on each trial were decomposed into fixations near the target (green), fixations near the participant’s trajectory (excluding the target) during sweeps vs outside of sweeps, and fixations outside of sweeps that were neither made to the target nor trajectory. Participants viewed the hidden target location more in easier arenas, and gazed upon the rest of the trajectory more in difficult arenas. The fraction of time that was spent looking elsewhere was relatively constant across arenas and epochs. (<bold>D</bold>) During pre-movement (top row) and movement (bottom row), participants spent more time foveating alternative trajectories on trials where there were more trajectory options (left), as well as more time foveating the chosen trajectory (middle). Participants spent less time foveating the goal location when there were more trajectory options (right). All error bars denote ±1 SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Properties of sweeping eye movements.</title><p>(<bold>A</bold>) Left: Across all participants and all trials, the speed of backward sweeps (top plot) and forward sweeps (bottom plot) during pre-movement (gold) was greater than that during movement (blue). Center: Forward sweeps before movement were relatively constant in duration across different arenas (bottom plot). Otherwise, sweeps were generally longer in duration in more complex arenas, and forward sweeps were longer than backward sweeps (top plot). Right: The number of saccades during forward sweeps before movement were relatively constant across arenas. Otherwise, the average number of saccades per sweep was higher for more complex arenas. (<bold>B–E</bold>) Linear mixed models with random intercepts and slopes for the effect of trial-specific variables on the dependent variables: (<bold>B</bold>) speed of sweeps (m/s), (<bold>C</bold>) duration of sweeps (s),(<bold>D</bold>) number of saccades during each sweep, and (<bold>E</bold>) saccade rate during sweeps. (Please see the description for <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F</xref> for further details.) All variables were z-scored prior to model fitting. (<bold>F</bold>) During both pre-movement (top plot) and movement (bottom), the saccade rate was generally higher during sweeps than outside of sweeps.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Sweep direction and timing.</title><p>(<bold>A</bold>) The direction of the first sweep was more likely to be backwards if it occurred prior to movement (left plot), and forwards if it occurred during movement (right). (<bold>B</bold>) Same plots as in <bold>A</bold>, but backward and forward sweeps are shown separately, and each color corresponds to a single participant. (<bold>C</bold>) Top left: The average delay between goal detection and the first sweep increased with arena difficulty. Top right: Cumulative distribution of sweep delays across all participants and all trials. Bar graphs: Linear mixed models with random intercepts and slopes for the effect of trial-specific variables on the latency to the first sweep if it occurred in the forward direction (bottom left) or the backward direction (bottom right). (Please see the description for <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F</xref> for additional information.) The number of turns and especially the path length positively influence sweep latency. (<bold>D</bold>) Top left: ROC curves constructed as described in <xref ref-type="fig" rid="fig3">Figure 3A</xref> (rightmost) for the distributions of true vs. shuffled average relevance values for each trial (pooled across all arenas, all participants, and all trials), with periods of sweeping eye movements removed, reveals that during the pre-movement (orange) and movement (blue) epochs, non-sequential eye movements are still directed towards task-relevant locations. Top right: AUC plots were constructed with sweeps removed, as described in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. The AUC values remain well above chance during the pre-movement (orange) and movement (blue) epochs. Bottom left/right: The same analysis was performed with gaze positions falling within 2 m of the participant’s trajectory on each trial removed, revealing that the remaining visual samples were made to relevant locations in space during pre-movement, but not during movement.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-fig4-figsupp3-v2.tif"/></fig></fig-group><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-73097-video1.mp4"><label>Video 1.</label><caption><title>Six representative trials in which participants exhibited sweeping eye movements.</title><p>(Top) Aerial view of the arena with the participant’s dynamically evolving position (lilac) and gaze (green). The target is represented as a black circle. (Bottom) Time-evolving version of the plot described in <xref ref-type="fig" rid="fig4">Figure 4B</xref>. The video speed is veridical, and the search epoch was omitted from each trial.</p></caption></media><p>The environmental structure exerted a strong influence on the probability of sweeping: the fraction of trials in which this phenomenon occurred was significantly correlated with arena complexity (Pearson’s r(63) = 0.73, p=5e<sup>-12</sup>; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>). This suggests that sweeping eye movements could be integral to trajectory planning. Most notably, on average, backward sweeps occupied a greater fraction of time during pre-movement than during movement, but forward sweeps predominantly occurred during movement (backward sweeps – pre-movement: 5.6 ± 3.3%, movement: 1.9 ± 0.6%; forward sweeps – pre-movement: 5.2 ± 3.0%, movement: 10.5 ± 6.3%; <xref ref-type="fig" rid="fig4">Figure 4C</xref>). This suggests that the initial planning is primarily carried out by sweeping backwards from the goal. Furthermore, trials with a greater number of turns and those in which participants initially move away from the direction of the target tend to have more backward sweeps during pre-movement (<xref ref-type="fig" rid="fig4">Figure 4D</xref> – left). In contrast, the number of turns inhibited forward sweeps during pre-movement, which were instead driven largely by the length of the trajectory (<xref ref-type="fig" rid="fig4">Figure 4E</xref> – left). During movement on the other hand, multiple measures of trial difficulty increased the likelihood of forward sweeps (<xref ref-type="fig" rid="fig4">Figure 4E</xref> – right), whereas backward sweeps depended primarily on the number of turns (<xref ref-type="fig" rid="fig4">Figure 4D</xref> – right). Together, these dependencies explain why backward sweeps are more common during pre-movement but forward sweeps dominate during movement.</p><p>Aside from gazing upon the target or the trajectory on each trial, about 20% of eye movements were made to other locations in space (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C</xref>). Besides task-relevant locations such as bottleneck transitions, we wanted to know whether these other locations also comprised alternative trajectories to the goal. To test this, we identified all trajectories whose path lengths were comparable (within about 1 SD; Methods) to the chosen trajectory on each trial. The fraction of time spent looking at alternative trajectories and the chosen trajectory both increased with the number of alternatives (Pearson’s r(14) = 0.91, p=1e<sup>-6</sup> pre-movement; <italic>r</italic>=0.90, p=2e<sup>-6</sup> movement), suggesting that participants engage in some form of deliberation (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1D</xref>). This deliberation happens at the expense of reducing the fraction of time spent looking at the hidden goal location (r(14) = –0.85, p=3e<sup>-5</sup> pre-movement; <italic>r</italic>=–0.64, <italic>P</italic>=7e<sup>-3</sup> movement), revealing that the planning algorithm is likely subject to a cognitive trade-off mediated by the number of available options, in addition to that mediated by model complexity demonstrated earlier.</p><p>When looking at the trajectory, the mean speed of backward sweeps was greater than the speed of forward sweeps across all arenas (backward sweeps: 26 ± 4 m/s, forward sweeps: 21 ± 3 m/s; <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2A</xref>). Notably, sweep velocities were more than 10 × greater than the average participant velocity during the movement epoch (1.4 ± 0.1 m/s). This is reminiscent of the hippocampal replay of trajectories through space, as such sequential neural events are also known to be compressed in time (around 2–20 × the speed of neural sequence activation during navigation) (<xref ref-type="bibr" rid="bib7">Buhry et al., 2011</xref>). Both sweep speeds and durations slightly increased with arena complexity (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2A</xref>). This is because peripheral vision processing must lead the control of central vision to allow for sequential eye movements to trace a viable path (<xref ref-type="bibr" rid="bib8">Caspi et al., 2004</xref>; <xref ref-type="bibr" rid="bib9">Crowe et al., 2000</xref>). In more complex arenas such as the maze where the search tree is narrow and deep, the obstacle configuration is more structured and presents numerous constraints, and thus path tracing computations might occur more quickly. Accordingly, longer path lengths best predicted sweep speeds (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2B</xref>). However, due to the lengthier and more convoluted trajectories in those arenas, the gaze must cover greater distances and make more turns, resulting in sweeps which last longer (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2A, C</xref>). Another property of sweeps is that they comprised more saccades in more difficult trials (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2A, D</xref>), and saccade rates were higher during sweeps than at other times after goal detection (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2F</xref>). This suggests that either visual processing during sweeps was expedited compared to average, or sweeps resulted from eye movements which followed a pre-planned saccade sequence.</p><p>If the first sweep on a trial occurred during pre-movement, the direction of the sweep was more likely to be backwards, while if the first sweep occurred during movement, it was more likely to be in the forwards direction (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3A, B</xref>). The latency between goal detection and the first sweep increased with arena difficulty (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3C</xref> – top row), and more specifically the number of turns and expected path length (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3C</xref> – bottom row), suggesting that sweep initiation is preceded by brief processing of the arena, and more complex tasks elicited longer processing. While the sequential nature of eye movements could constitute a swift and efficient way to perform instrumental sampling, we found that task-relevant eye movements were not necessarily sequential. When we reanalyzed the spatial distribution of gaze positions by removing periods of sweeping, the resulting relevance values remained greater than chance (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3D</xref>).</p><p>What task conditions promote sequential eye movements? To find out, we computed the probability that the participants engaged in sweeping behavior as a function of time and position, during the pre-movement and movement epochs respectively, and focusing on the predominant type of sweep during those periods (backward and forward sweeps respectively; <xref ref-type="fig" rid="fig4">Figure 4C</xref>). During pre-movement, we found that the probability of sweeping gradually increased over time, suggesting that backward sweeps during the initial stages of planning are separated from the time of target foveation by a brief pause, during which participants may be gathering some preliminary information about the environment (<xref ref-type="fig" rid="fig5">Figure 5A</xref> – left). During movement, on the other hand, the probability of sweeping is strongly influenced by whether participants are executing a turn in their trajectory. Obstacles often preclude a straight path to the remembered goal location, and thus participants typically find themselves making multiple turns while actively navigating. Consequently, a trajectory may be divided into a series of straight segments separated by brief periods of elevated angular velocity. We isolated such periods by applying a threshold on angular velocity, designating the periods of turns as <italic>subgoals</italic>, and aligned the participant’s position in all trials with respect to the subgoals. The likelihood of sweeping the trajectory in the forward direction tended to spike precisely when participants reached a subgoal (<xref ref-type="fig" rid="fig5">Figure 5A</xref> – right). There was a concomitant decrease in the average distance of the point of gaze from the goal location in a step-like manner with each subgoal achieved (<xref ref-type="fig" rid="fig5">Figure 5B</xref> – right). In contrast to backward sweeps, which were made predominantly to the most proximal subgoal prior to navigating (<xref ref-type="fig" rid="fig5">Figure 5C</xref> – left), forward sweeps that occurred during movement were not regularly directed toward one particular location. Instead, in a strikingly stereotyped manner, participants appeared to lock their gaze upon the <italic>upcoming</italic> subgoal when rounding each bend in the trajectory (<xref ref-type="fig" rid="fig5">Figure 5C</xref> – right). This suggests that participants likely represented their plan by decomposing it into a series of subgoals, focusing on one subgoal at a time until they reached the final goal location. In contrast to sweeping eye movements, the likelihood of gazing alternative trajectories peaked much earlier during pre-movement (<xref ref-type="fig" rid="fig5">Figure 5D</xref> – left). Likewise, during movement, participants tend to look briefly at alternative trajectories shortly before approaching a subgoal (<xref ref-type="fig" rid="fig5">Figure 5D</xref> – right) which might constitute a form of vicarious trial and error behavior at choice points (<xref ref-type="bibr" rid="bib65">Redish, 2016</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Timing of sweeps reveals task decomposition.</title><p>Trials across all arenas and all participants were aligned and scaled for the purpose of trial-averaging. This process was carried out separately for the pre-movement and movement epochs. (<bold>A</bold>) Left: Prior to movement, the probability of (backward) sweeps increased with time. Right: During movement, the probability of (forward) sweeps transiently increased at the precise moments when participants reached each subgoal. Participant position is defined in relation to the location of subgoals. Subgoals are designated as numbers starting from the goal (subgoal 0) and counting backwards along the trajectory (subgoals 1, 2, 3 etc.) such that greater values correspond to more proximal subgoals. (<bold>B</bold>) Left: Gaze traveled away from the goal location prior to movement. Right: The average distance of gaze from the goal decreased in steps, with steps occurring at each subgoal. (<bold>C</bold>) Distance of gaze from individual subgoals (most proximal in yellow, most distal in cyan). Left: Gaze traveled towards the most proximal subgoal prior to movement, consistent with the increased probability of backward sweeps during this epoch. Right: The average distance of gaze to each individual subgoal (colored lines) was minimized precisely when participants approached that subgoal. (<bold>D</bold>) Left: The probability of gazing at alternative trajectories is relatively constant throughout the pre-movement epoch. Right: Participants gaze at alternative trajectories more frequently when approaching turns. (<bold>E</bold>) A graphical summary of the spatiotemporal dynamics of eye movements in this task. Subgoals are depicted in the same color scheme used in <bold>C</bold>. (<bold>F</bold>) Diagram of a standard Markov Decision Process, augmented with an additional pathway for agent-environment interaction through eye movements (colored arrows). Dashed arrows denote sweeps, and possible paths throughout the arena are depicted in gray. Darker bounds in <bold>A–C</bold> denote ±1 SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-fig5-v2.tif"/></fig><p>To summarize, we found participants made sequential eye movements sweeping forward and/or backward along the intended trajectory, and the likelihood of sweeping increased with environmental complexity. During the pre-movement phase, participants gathered visual information about the arena, evaluated alternative trajectories, and typically traced the chosentrajectory backwards from the goal to the first subgoal (<xref ref-type="fig" rid="fig5">Figure 5E</xref> – orange). While moving through the arena, they tended to lock their gaze upon the upcoming subgoal until shortly before a turn, at which point they exhibit a higher tendency of gazing upon alternative trajectories. During turns, participants often sweep their gaze forward to the next subgoal (<xref ref-type="fig" rid="fig5">Figure 5E</xref> – blue). Via eye movements, navigators could construct well-informed plans to make sequential actions that would most efficiently lead to rewards (<xref ref-type="fig" rid="fig5">Figure 5F</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we highlight the crucial role of eye movements for flexible navigation. We found that humans took trajectories nearly optimal in length through unfamiliar arenas, and spent more time planning prior to navigation in more complex environments. The spatial distribution of gaze was largely concentrated at the hidden goal location in the simplest environment, but participants increasingly interrogated the task-relevant structure of the environment as the arena complexity increased. In the temporal domain, participants often rapidly traced their future trajectory to and from the goal with their eyes (sweeping), and generally concentrated their gaze upon one subgoal (turn) at a time until they reached their destination. In summary, we found evidence that the neural circuitry governing the oculomotor system optimally schedules and allocates resources to tackle the diverse cognitive demands of navigation, producing efficient eye movements through space and time.</p><p>Eye movements provide a natural means for researchers to understand information seeking strategies, in both experimental and real-world settings (<xref ref-type="bibr" rid="bib23">Gottlieb et al., 2013</xref>; <xref ref-type="bibr" rid="bib24">Gottlieb et al., 2014</xref>). Past studies using simple decision making tasks probed whether active sensing, specifically via eye movements, reduces uncertainty about the <italic>state</italic> of the environment (such as whether a change in an image has occurred) (<xref ref-type="bibr" rid="bib84">Yang et al., 2016</xref>; <xref ref-type="bibr" rid="bib66">Renninger et al., 2007</xref>; <xref ref-type="bibr" rid="bib1">Ahmad and Yu, 2013</xref>). But in sequential decision making tasks such as navigation, there is added uncertainty about the task-contingent causal structure (<italic>model</italic>) of the environment (<xref ref-type="bibr" rid="bib40">Kaplan and Friston, 2018</xref>; <xref ref-type="bibr" rid="bib54">Mattar and Lengyel, 2022</xref>). Common paradigms for goal-oriented navigation occlude large portions of the environment from view, usually in the interest of distinguishing model-based strategies from conditioned responses (<xref ref-type="bibr" rid="bib73">Smittenaar et al., 2013</xref>; <xref ref-type="bibr" rid="bib71">Simon and Daw, 2011</xref>; <xref ref-type="bibr" rid="bib11">de Cothi et al., 2020</xref>) or allow very restricted fields of view where eye movements have limited potential for sampling information (<xref ref-type="bibr" rid="bib36">Javadi et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Ghamari and Golshany, 2021</xref>). Occlusions eliminate the possibility of gathering information about the structure of the environment using active sensing. By removing such constraints, we allowed participants to acquire a model of the environment without physically navigating through it, which yielded new insights about how humans perform visually-informed planning.</p><p>In particular, we found that the gaze is distributed between the two components of the model required to plan a path – the transition function, which describes the relationship between states, and the reward function, which describes the relationship between the states and the reward – with the distribution skewed in favor of the former in more complex environments. When alternative paths were available, gaze tended to be directed towards them at the expense of looking at the hidden reward location. These findings suggest a context-dependent mechanism which dictates the dynamic arbitration between competing controllers of the oculomotor system that seek information about complementary aspects of the task. Neurally, this could be implemented by circuits that exert executive control over voluntary eye movements. Candidate substrates include the dorsolateral prefrontal cortex, which is known to be important for contextual information processing and memory-guided saccades (<xref ref-type="bibr" rid="bib61">Pierrot-Deseilligny et al., 1995</xref>; <xref ref-type="bibr" rid="bib38">Johnston and Everling, 2006</xref>; <xref ref-type="bibr" rid="bib62">Pierrot-Deseilligny et al., 2005</xref>), and the anterior cingulate cortex, which is known to be involved in evaluating alternative strategies (<xref ref-type="bibr" rid="bib79">Tervo et al., 2021</xref>; <xref ref-type="bibr" rid="bib20">Gaymard et al., 1998</xref>). To better understand the precise neural mechanisms underlying the spatial gaze patterns we observed, it would be instructive to examine the direction of information flow between the oculomotor circuitry and brain regions with strong spatial and value representations during this task in animal models. Future research may also investigate multi-regional interactions in humans by building on recent advances in data analysis that allow for eye movements to be studied in fMRI scanners (<xref ref-type="bibr" rid="bib19">Frey et al., 2021</xref>). Our analysis of spatial gaze patterns is grounded in the RL framework, which provides an objective way to measure the utility of sampling information from different locations. However, this measure was agnostic to the temporal ordering of those samples. Given that previous work demonstrated evidence for the planning of multiple saccades during simple tasks like visual search (<xref ref-type="bibr" rid="bib33">Hoppe and Rothkopf, 2019</xref>), incorporating chronology into a normative theory of eye movements in sequential decision-making tasks presents an excellent opportunity for future studies.</p><p>Meanwhile, we found that the temporal pattern of eye movements revealed a fine-grained view of how planning computations unfold in time. In particular, participants made sequential eye movements sweeping forward and/or backward along the future trajectory, evocative of forward and reverse replay by place cells in the hippocampus (<xref ref-type="bibr" rid="bib12">Diba and Buzsáki, 2007</xref>; <xref ref-type="bibr" rid="bib60">Pfeiffer and Foster, 2013</xref>). Shortly after fixating on the goal, participants’ gaze often swept backwards along their future trajectory, mimicking reverse replay. Because these sweeps predominantly occurred before movement, they may reflect depth-first tree search, a model-based algorithm for path discovery (<xref ref-type="bibr" rid="bib86">Zhou and Hansen, 2008</xref>). Then, during movement, participants were more likely to make forward sweeps when momentarily slowing down at turning points, analogous to finding that neural replay mainly occurs during periods of relative immobility (<xref ref-type="bibr" rid="bib75">Sosa and Giocomo, 2021</xref>). Several recent studies have also supported that replay serves to consolidate memory and generalize information about rewards (<xref ref-type="bibr" rid="bib22">Gillespie et al., 2021</xref>; <xref ref-type="bibr" rid="bib51">Liu et al., 2021</xref>; <xref ref-type="bibr" rid="bib17">Eldar et al., 2020</xref>). In light of the similarities between sweeps and sequential hippocampal activations, we predict that direct or indirect hippocampal projections to higher oculomotor controllers (e.g. the supplemental eye fields through the orbitofrontal cortex) may allow eye movements to embody the underlying activations of state representations (<xref ref-type="bibr" rid="bib47">Larson and Loschky, 2009</xref>; <xref ref-type="bibr" rid="bib82">Wilming et al., 2017</xref>; <xref ref-type="bibr" rid="bib27">Hannula and Ranganath, 2009</xref>). This would allow replays to influence the active gathering of information. Alternatively, active sensing could be a result of rapid peripheral vision processing which drives saccade generation, such that the eye movements reflect the outcome of sensory processing rather than prior experience. Consistent with this idea, past studies have demonstrated that humans can smoothly trace paths through entirely novel 2D mazes (<xref ref-type="bibr" rid="bib9">Crowe et al., 2000</xref>; <xref ref-type="bibr" rid="bib10">Crowe et al., 2004</xref>). Interestingly, neural modulation does occur in this direction — the contents of gaze have been found to influence activity in the hippocampus and entorhinal cortex (<xref ref-type="bibr" rid="bib81">Turk-Browne, 2019</xref>; <xref ref-type="bibr" rid="bib49">Liu et al., 2017</xref>; <xref ref-type="bibr" rid="bib58">Monaco et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Jun et al., 2016</xref>; <xref ref-type="bibr" rid="bib18">Fotowat et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Ringo et al., 1994</xref>). Therefore, it is conceivable that sequential neural activity could emerge from consolidating temporally extended eye movements such as sweeps. We hope that in future, simultaneous recordings from brain areas involved in visual processing, eye movement control, and the hippocampal formation would uncover the mechanisms underlying trajectory sweeping eye movements and their relationship to perception and memory.</p><p>Value-based decisions are known to involve lengthy deliberation between similar alternatives (<xref ref-type="bibr" rid="bib2">Bakkour et al., 2019</xref>; <xref ref-type="bibr" rid="bib78">Tajima et al., 2016</xref>). Participants exhibited a greater tendency to deliberate between viable alternative trajectories at the expense of looking at the reward location. Likelihood of deliberation was especially high when approaching a turn, suggesting that some aspects of path planning could also be performed on the fly. More structured arena designs with carefully incorporated trajectory options could help shed light on how participants discover a near-optimal path among alternatives. However, we emphasize that deliberative processing accounted for less than one-fifth of the spatial variability in eye movements, such that planning largely involved searching for a viable trajectory.</p><p>Although we have analyzed strategies of active sensing and planning separately, these computations must occur simultaneously and influence each other. This is formalized by the framework of active inference that unifies planning and information seeking by integrating the RL framework, which describes exploiting rewards for their extrinsic value, and the information theoretic framework, which describes exploring new information for its epistemic value (<xref ref-type="bibr" rid="bib40">Kaplan and Friston, 2018</xref>). Using this framework to simulate eye movements in a spatial navigation task, <xref ref-type="bibr" rid="bib40">Kaplan and Friston, 2018</xref> found that gaze is dominated by epistemic (curiosity) rather than pragmatic (reward) considerations in the first few trials, a prediction that is not supported by our results. However, it is possible that participants were able to rapidly resolve uncertainty about the arena structure in our experiments. Future studies must identify the constraints under which active inference models can provide quantitatively good fits to our data. In another highly relevant theoretical work, Mattar and Daw proposed that path planning and structure learning are variants of the same operation, namely the spatiotemporal propagation of memory (<xref ref-type="bibr" rid="bib53">Mattar and Daw, 2018</xref>). The authors show that prioritization of reactivating memories about reward encounters and imminent choices depends upon its utility for future task performance. Through this formulation, the authors provided a normative explanation for the idiosyncrasies of forward and backward replay, the overrepresentation of reward locations and turning points in replayed trajectories, and many other experimental findings in the hippocampus literature. Given the parallels between eye movements and patterns of hippocampal activity, it is conceivable that gaze patterns can be parsimoniously explained as an outcome of such a prioritization scheme. But interpreting eye movements observed in our task in the context of the prioritization theory requires a few assumptions. First, we must assume that traversing a state space using vision yields information that has the same effect on the computation of utility as does information acquired through physical navigation. Second, peripheral vision allows participants to form a good model of the arena such that there is little need for active sensing. In other words, eye movements merely reflect memory access and have no computational role. Finally, long-term statistics of sweeps gradually evolve with exposure, similar to hippocampal replays. These assumptions can be tested in future studies by titrating the precise amount of visual information available to the participants, and by titrating their experience and characterizing gaze over longer exposures. We suspect that a pure prioritization-based account might be sufficient to explain eye movements in relatively uncluttered environments, whereas navigation in complex environments would engage mechanisms involving active inference. Developing an integrative model that features both prioritized memory-access as well as active sensing to refine the contents of memory, would facilitate further understanding of computations underlying sequential decision-making in the presence of uncertainty.</p><p>The tendency of humans to break larger problems into smaller, more tractable subtasks has been previously established in domains outside of navigation (<xref ref-type="bibr" rid="bib41">Killian et al., 2012</xref>; <xref ref-type="bibr" rid="bib55">Meister and Buffalo, 2018</xref>; <xref ref-type="bibr" rid="bib42">Killian and Buffalo, 2018</xref>; <xref ref-type="bibr" rid="bib15">Eckstein and Collins, 2020</xref>; <xref ref-type="bibr" rid="bib80">Tomov et al., 2020</xref>). However, theoretical insights on clustered representations of space have not been empirically validated in the context of navigation (<xref ref-type="bibr" rid="bib3">Balaguer et al., 2016</xref>; <xref ref-type="bibr" rid="bib64">Rasmussen et al., 2017</xref>), primarily due to the difficulty in distinguishing between flat and hierarchical representations from behavior alone. Our observation that participants often gazed upon the upcoming turn during movement supports that participants viewed turns as subgoals of an overall plan. Future work could focus on designing more structured arenas to experimentally separate the effects of path length, number of subgoals, and environmental complexity on participants’ eye movement patterns.</p><p>We hope that the study of visually-informed planning during navigation will eventually generalize to understanding how humans accomplish a variety of sequential decision-making tasks. A major goal in the study of neuroscience is to elucidate the principles of biological computations which allow humans to effortlessly exceed the capabilities of machines. Such computations allow animals to learn environmental contingencies and flexibly achieve goals in the face of uncertainty. However, one of the main barriers to the rigorous study of active, goal-oriented behaviors is the complexity in estimating the participant’s prior knowledge, intentions, and internal deliberations which lead to the actions that they take. Luckily, eye movements reveal a wealth of information about ongoing cognitive processes during tasks as complex and naturalistic as spatial navigation.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Experimental Model and Participant Details</title><p>Thirteen human participants (all &gt;18 years old, ten males) participated in the experiments. All but two participants (S6 and S9) were unaware of the purpose of the study. Four of the participants, including S6 and S9, were exposed to the study earlier than the rest of the participants, and part of the official dataset for two of these participants (S4 and S8) was collected 2 months prior to the rest of data collection as a safety precaution during the COVID-19 pandemic. Eight additional human participant recruits (all &gt;18 years old, four males) were disqualified due to experiencing motion sickness while in the VR environment and not completing a majority of trials. All experimental procedures were approved by the Institutional Review Board at New York University and all participants signed an informed consent form (IRB-FY2019-2599).</p></sec><sec id="s4-2"><title>Stimulus</title><p>Participants were seated on a swivel chair with 360° of freedom in physical rotation and navigated in a full-immersion hexagonal virtual arena with several obstacles. The stimulus was rendered at a frame rate of 90 Hz using the Unity game engine v2019.3.0a7 (programmed in C#) and was viewed through an HTC VIVE Pro virtual reality headset. The subjective vantage point (height of the point between the participants’ eyes with respect to the ground plane) was 1.72 m. The participant had a field of view of 110.1° of visual angle. Forward and backward translation was enabled via a continuous control CTI Electronics M20U9T-N82 joystick with a maximum speed recorded at 4.75 m/s. Participants executed angular rotations inside the arena by turning their head, while the joystick input enabled translation in the direction in which the participant’s head was facing. Obstacles and arena boundaries appeared as gray, rectangular slabs of concrete. The ground plane was grassy, and the area outside of the arena consisted of a mountainous background. Peaks were visible above the outer boundary of the arena to provide crude orientation landmarks. Clear blue skies with a single light source appeared overhead.</p></sec><sec id="s4-3"><title>State space geometry</title><p>The arena was a rectangular hexagon enclosing an area of approximately 260 m<sup>2</sup> of navigable space. For ease of simulation and data analyses, the arena was imparted with a hidden triangular tessellation (<italic>deltille</italic>) composed of 6 <italic>n</italic><sup>2</sup> equilateral triangles, where n determines the state space granularity. We chose <italic>n</italic>=5, resulting in triangles with a side length of 2 meters, each of which constituted a state in the discrete state space (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). The arena contained several obstacles in the form of unjumpable obstacles (0.4 m high) located along the edges between certain triangles (states). Obstacle locations were predetermined offline using MATLAB by either randomly selecting a chosen number of edges of the tessellation or by using a graphical user interface (GUI) to manually select edges of the tessellation; these locations were loaded into Unity. Outer boundary walls of height 2.5 m enclosed the arena. We chose five arenas spanning a large range in <italic>average</italic> state closeness centrality <inline-formula><mml:math id="inf13"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>), where <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as the inverse average path length <inline-formula><mml:math id="inf15"><mml:mi>d</mml:mi></mml:math></inline-formula> from state <inline-formula><mml:math id="inf16"><mml:mi>s</mml:mi></mml:math></inline-formula> to every other state <inline-formula><mml:math id="inf17"><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> (<inline-formula><mml:math id="inf18"><mml:mi>N</mml:mi></mml:math></inline-formula> states in total). On average, arenas with lower centrality will impose greater path lengths between two given states, making them more complex to navigate. We defined a measure of arena complexity by adding an offset to mean centrality and then scaling it, such that the simplest arena had a complexity value of zero (<inline-formula><mml:math id="inf19"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>100</mml:mn><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) where <inline-formula><mml:math id="inf20"><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> denotes mean centrality across states and max is taken over arenas. Such a transformation would preserve the correlation and p-values between dependent variables and arena centrality, while the new metric would allow for the graphic representation of arenas in an intuitive order (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). A complexity value of zero corresponds to the simplest arena that we designed. The order of arenas presented to each participant was randomly permuted but not entirely counterbalanced due to the large number of permutations (<xref ref-type="table" rid="app2table1">Appendix 2—table 1</xref>).<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-4"><title>Eye tracking</title><p>At the beginning of each block of trials, participants calibrated the VIVE Pro eye tracker using inbuilt Tobii software which prompted participants to foveate several points tiling a 2D plane in the VR environment. Both eyes were tracked, and the participant’s point of foveation (<italic>x-y</italic> coordinates), object of foveation (ground, obstacles, boundaries, etc.), eye openness, and other variables of interest were recorded on each frame using the inbuilt software. <xref ref-type="bibr" rid="bib72">Sipatchin et al., 2020</xref> reported that during free head movements, point-of-gaze measurements using the VIVE Pro eye tracker has a spread of 1.15° ± 0.69° (SE) (<xref ref-type="bibr" rid="bib72">Sipatchin et al., 2020</xref>). This means that when the participant fixates a point on the ground five meters away, the 95% confidence interval (CI) for the measurement error in the reported gaze location would be 0–23 cm (roughly one-tenth of the length of one transition or obstacle) and 0–67 cm (one-third of a transition length) for points fifteen meters away. While machine precision was not factored into the analyses, the fraction of eye positions that may have been misclassified due to hardware and software limitations is likely very small. Furthermore, Sipatchin et al. reported that the system latency was 58.1 ms. While there is reason to suspect that the participant’s position was recorded with a similar latency of around 5 frames, even if the gaze data lagged the position data, the participant would only have moved 28.5 cm if they were translating at the maximum possible velocity over this interval.</p></sec><sec id="s4-5"><title>Behavioral task</title><p>At the beginning of each trial, a target in the form of a realistic banana from the Unity Asset store appeared hovering 0.4 m over a state randomly drawn from a uniform distribution over all possible states. The joystick input was disabled until the participant foveated the target, but the participant was free to scan the environment by rotating in the swivel chair during the visual search period. About 200 ms after target foveation, the banana disappeared and participants were tasked with navigating to the remembered target location without time constraints. Participants were not given instructions on what strategy to use to complete the task. After reaching the target, participants pressed a button on the joystick to indicate that they have completed the trial. Alternatively, they could press another button to indicate that they wished to skip the trial. Feedback was displayed immediately after pressing either button (see section below). Skipping trials was discouraged except when participants did not remember seeing the target before it disappeared, and these trials were recorded and excluded from the analyses (&lt; 1%).</p></sec><sec id="s4-6"><title>Reward</title><p>If participants stopped within the triangular state which contained the target, they were rewarded with two points. If they stopped in a state sharing a border with the target state, they were rewarded with one point. After the participant’s button press, the number of points earned on the current trial was displayed for one second at the center of the screen. The message displayed was ‘You earned p points!’; the font color was blue if p=1 or p=2, and red if p=0. On skipped trials, the screen displayed ‘You passed the trial’ in red. In each experimental session, after familiarizing themselves with the movement controls by completing ten trials in a simplistic six-compartment arena (granularity n=1), participants completed one block of 50 trials in each of five arenas (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). At the end of each block, a blue message stating ‘You have completed all trials!’ prompted them to prepare for the next block. Session durations were determined by the participant’s speed and the length of the breaks that they needed from the virtual environment, ranging from 1.5–2 hr, sometimes spread across more than 1 day. Participants were paid $0.02/point for a maximum of 5 arenas × 50 trials/arena × 2 points/trial × $0.02/point = $10, in addition to a base pay of $10 /hr for their time (the average payment was $27.55).</p></sec><sec id="s4-7"><title>RL formulation</title><p>Navigation can be formulated as a Markov decision process (MDP) described by the tuple <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>&gt;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> whose elements denote, respectively, a finite state space <inline-formula><mml:math id="inf22"><mml:mi>S</mml:mi></mml:math></inline-formula>, a finite action space <inline-formula><mml:math id="inf23"><mml:mi>A</mml:mi></mml:math></inline-formula>, a state transition distribution <inline-formula><mml:math id="inf24"><mml:mi>P</mml:mi></mml:math></inline-formula>, a reward function <inline-formula><mml:math id="inf25"><mml:mi>R</mml:mi></mml:math></inline-formula>, and a temporal discount factor γ that captures the relative preference of distal over proximal rewards (<xref ref-type="bibr" rid="bib5">Bermudez-Contreras et al., 2020</xref>). Given that an agent is in state <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula>, the agent may execute an action <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula> in order to bring about a change in state <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>s</mml:mi><mml:mo>→</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and harvest a reward <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To relate this formalism to the structure of the arena, it is instructive to consider the <italic>possibility</italic> of traversal from state <inline-formula><mml:math id="inf31"><mml:mi>s</mml:mi></mml:math></inline-formula> to any state <inline-formula><mml:math id="inf32"><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> in a single time step, as described by the adjacency matrix <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> if there exists an available action which would bring about the change in state <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>s</mml:mi><mml:mo>→</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> with a non-zero probability, and <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> otherwise. By definition, <inline-formula><mml:math id="inf36"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> if there is an obstacle between <inline-formula><mml:math id="inf37"><mml:mi>s</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf38"><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula>. A state not bordered by any obstacle would have three non-zero entries in the corresponding row of <inline-formula><mml:math id="inf39"><mml:mi>T</mml:mi></mml:math></inline-formula>. Thus, the arena structure is fully encapsulated in the adjacency matrix.</p><p>In the case that an agent is tasked with navigating to a goal location <inline-formula><mml:math id="inf40"><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:math></inline-formula> where the agent would receive a reward, the reward function <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> if and only if the action <inline-formula><mml:math id="inf42"><mml:mi>a</mml:mi></mml:math></inline-formula> allows for the transition <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>s</mml:mi><mml:mo>→</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in one time step, and <inline-formula><mml:math id="inf44"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> otherwise. Given this formulation, we may compute the optimal policy <inline-formula><mml:math id="inf45"><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which describes the actions that an agent should take from each state in order to reach the target state in the fewest possible number of time steps. The optimal policy may be derived by computing optimal state values <inline-formula><mml:math id="inf46"><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, defined as the expected future rewards to be earned when an agent begins in state <inline-formula><mml:math id="inf47"><mml:mi>s</mml:mi></mml:math></inline-formula> and acts in accordance with the policy <inline-formula><mml:math id="inf48"><mml:msup><mml:mi>π</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>. The optimal value function can be computed by solving the Bellman Equation (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>) via dynamic programming (specifically value iteration) — an efficient algorithm for path-finding — which iteratively unrolls the recursion in this equation (<xref ref-type="bibr" rid="bib4">Bellman, 1954</xref>). The optimal policy is given by the argument <inline-formula><mml:math id="inf49"><mml:mi>a</mml:mi></mml:math></inline-formula> that maximizes the right-hand side of <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>. Intuitively, following the optimal policy requires that agents take actions to ascend the value function where the value gradient is most steep (<xref ref-type="fig" rid="fig1">Figure 1D</xref>).<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:munder><mml:mtext> </mml:mtext><mml:mo stretchy="false">[</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For the purposes of computing the optimal trajectory, we considered twelve possible degrees of freedom in the action space, such that one-step transitions could result in relocating to a state that is 0°, 30°, 60°, …, 300°, or 330° with respect to the previous state. However, the center-to-center distances between states for a given transition depends on the angle of transition. Specifically, as shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>, if a step in the 0° direction requires translating 1 m, then a step in the 60°, 120°, 180°, 240°, and 300° directions would also require translating 1 m, but a step in the 30°, 150°, and 270° directions would require translating <inline-formula><mml:math id="inf50"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt></mml:mrow><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> m, and a step in the 90°, 210°, and 330° directions would require translating <inline-formula><mml:math id="inf51"><mml:mrow><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> m. Therefore, in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, <inline-formula><mml:math id="inf52"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf53"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt></mml:mrow><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, or <inline-formula><mml:math id="inf54"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, depending on the step size required in taking an action <inline-formula><mml:math id="inf55"><mml:mi>a</mml:mi></mml:math></inline-formula>. The value of the goal state <inline-formula><mml:math id="inf56"><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:math></inline-formula> was set to zero on each iteration. Value functions were computed for each goal location, and the relative value of states describes the relative minimum number of time steps required to reach <inline-formula><mml:math id="inf57"><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:math></inline-formula> from each state. The lower the value of a state, the greater the geodesic separation between the state and the goal state. We set <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> during all simulations and performed 100 iterations before calculating optimal trajectory lengths from an initial state <italic>s</italic><sub><italic>i</italic></sub> to the target state <inline-formula><mml:math id="inf59"><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:math></inline-formula>, as this number of iterations allowed for the algorithm to converge.</p></sec><sec id="s4-8"><title>Relevance computation</title><p>To compute the relevance <inline-formula><mml:math id="inf60"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the <inline-formula><mml:math id="inf61"><mml:msup><mml:mi>k</mml:mi><mml:mi>th</mml:mi></mml:msup></mml:math></inline-formula> transition to the task of navigating from a specific initial state <italic>s</italic><sub>0</sub> to a specific goal <inline-formula><mml:math id="inf62"><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:math></inline-formula>, we calculated the absolute change induced in the optimal value of the initial state after toggling the navigability of that transition by changing the corresponding element in the adjacency matrix from 1 to 0 or from 0 to 1 (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>). For the simulations described in Appendix 1, we also tested a non-myopic, path-dependent metric <inline-formula><mml:math id="inf63"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> defined as the sum of squared differences induced in the values of all states along the optimal path (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>). Furthermore, we tested the robustness of the measure to the precise algorithm used to compute state values by computing value functions using the successor representation (SR) algorithm, which caches future state occupancy probabilities learned with a specific policy (<xref ref-type="bibr" rid="bib76">Stachenfeld et al., 2017</xref>). (While SR is more efficient than value iteration, it is less flexible.) As we used a random walk policy, we computed the matrix of probabilities <inline-formula><mml:math id="inf64"><mml:mi>M</mml:mi></mml:math></inline-formula> analytically by temporally abstracting a one-step transition matrix <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The cached probabilities can then be combined with a one-hot reward vector <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> to yield state values <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. We set the temporal discount factor <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and integrated over 100 time steps.<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-9"><title>Relation to bottlenecks</title><p>In order to assess whether the relevance metric is predictive of the degree to which transitions are bottlenecks in the environment, we correlated normalized relevance values (averaged across all target locations and normalized via dividing by the maximum relevance value across all transitions for each target location) with the average betweenness centrality <inline-formula><mml:math id="inf69"><mml:mi>G</mml:mi></mml:math></inline-formula> of the two states on either side of a transition (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>). Betweenness centrality essentially calculates the degree to which a state controls the traffic flowing through the arena. <inline-formula><mml:math id="inf70"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the number of shortest paths between states <inline-formula><mml:math id="inf71"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf72"><mml:mi>j</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf73"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the number of such paths which pass through state <inline-formula><mml:math id="inf74"><mml:mi>s</mml:mi></mml:math></inline-formula>. For this analysis, transitions within 1 m from the goal state were excluded due to their chance of having spuriously high relevance values.<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>s</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-10"><title>Simulations</title><p>Behavior of three artificial agents with qualitatively different planning capacities was simulated. All agents were initialized with a noisy model of the environment. Representational noise was simulated by toggling 50% of randomly selected unavailable transitions from <inline-formula><mml:math id="inf75"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> to 1, and the equivalent number of randomly selected available transitions from <inline-formula><mml:math id="inf76"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to . This is analogous to the agents misplacing obstacles in their memories, or equivalently, a subjective-objective model mismatch induced by volatility in the environment. The blind agent was unable to correct its model during a planning period. On each trial, eight transitions (out of 210 available) were drawn for each sighted agent; the agent’s model was compared with the true arena structure at these transitions and, if applicable, corrected prior to navigation. Visual samples were drawn uniformly from all possible transitions (without replacement) for the random exploration agent. For the goalward looking agent, the probability of drawing a transition was determined by a circular normal (von Mises) distribution with <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>θ</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:math></inline-formula> is the angle of the goal w.r.t. the agent’s heading), <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and concentration parameter <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>. In contrast, the directed sampling agent gathered information specifically about the eight transitions that were calculated to be most relevant for that trial. After the model updates, if any, the agents’ subjective value functions were recomputed, and agents took actions according to the resulting policies. When an agent encountered a situation in which no action was subjectively available, they attempted a random action. In the case that a new action is discovered, the agents temporarily updated <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from 0 to 1 for that action. Conversely, in the case that an agent attempted to take an action but discovered that it was not actually feasible, they temporarily updated their subjective models to account for the transition block which they had just learned about. In both cases, value functions were recomputed using the updated model. Simulations were conducted with 25 arenas of granularity n=3 (state space size = 54 for computational tractability) and 100 trials per arena. Furthermore, we tested the agents’ performance using a range of gaze samples evenly spaced between 2 and 14 foveations.</p></sec><sec id="s4-11"><title>Data processing</title><p>In order to identify moving and non-moving epochs within each trial, movement onset and offset times were detected by applying a moving average filter of window size 5 frames on the absolute value of the joystick input function. When the smoothed joystick input exceeded the threshold of 0.2 m/s (approximately 10% of the maximum velocity), the participant was deemed to be moving, and when the input fell below this threshold for the last time on each trial, the participant was deemed to have stopped moving. Participants’ relative planning time was defined as the ratio of pre-movement time to the total trial duration, minus the search period (which was roughly constant across arenas). Prior to any eye movement analyses, blinks were filtered from the eye movements by detecting when the fraction of the pupil visible dipped below 0.8. The spread in the <inline-formula><mml:math id="inf82"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> gaze positions within trials was calculated as the expectation of variance, <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msqrt><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:msqrt><mml:mtext> </mml:mtext><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf84"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the variance across time <inline-formula><mml:math id="inf85"><mml:mi>t</mml:mi></mml:math></inline-formula> within a trial and <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes expectation across trials denoted by <inline-formula><mml:math id="inf87"><mml:mi>n</mml:mi></mml:math></inline-formula>. The spread across trials was calculated as the variance of the expectation, <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes expectation across time and <inline-formula><mml:math id="inf90"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the variance across trials.</p><p>For <xref ref-type="fig" rid="fig1">Figure 1f, g</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A, B, E</xref> the first trial of each run was removed from the analyses due to an occasional rapid teleportation of the participant to a random starting location associated with the software starting up. While there were a few instances where more than one run occurred per block due to participants adjusting the headset, at least 51 trials were actually collected during each block such that most blocks consisted of 50 trials when the first trial of each run was omitted. For analyses such as epoch duration, gaze distribution, relevance, sweep detection, and subgoal detection, the first trial was not discarded since the teleportation only affected the recorded path length, but as the teleportation was virtually instantaneous, the new starting locations on such trials could be used for analyses which do not depend upon the path length variable.</p></sec><sec id="s4-12"><title>Linear mixed effects models</title><p>To separately examine how various aspects of the navigation task contribute to the behavioral and eye movement patterns that we observed, we fit linear mixed effects models of the form <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>, where each datapoint either corresponds to one trial (for trial-level analyses) or one participant in one arena (for analyses like % trials). In the former case, the number of predictor variables was <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>, and a fixed effect slope <inline-formula><mml:math id="inf92"><mml:msub><mml:mi>β</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> and participant-specific random effect slope <inline-formula><mml:math id="inf93"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, quantified the effect of each variable in a linear combination with a fixed intercept <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> and a participant-specific random intercept <inline-formula><mml:math id="inf95"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to produce an estimate of the dependent variable <italic>y</italic><sub><italic>i</italic></sub> on that trial for participant <inline-formula><mml:math id="inf96"><mml:mi>i</mml:mi></mml:math></inline-formula>. The three variables were the number of turns (described in the ‘Subgoal analysis’ section), the length of the optimal trajectory (described in the ‘RL formulation’ section), and the unsigned angle between the direction of participants’ initial heading vs the optimal direction of target approach (relative bearing). The ranges of the predictors were – number of turns: 0–17, length of optimal trajectory: 0–45 m, relative bearing: 0–90 degrees. <xref ref-type="disp-formula" rid="equ10">Equation 8</xref> describes the specific form used to examine trial-specific effects on the output. For the case where analyses required pooling trials for each participant in each arena, <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and the single set of fixed/random slopes correspond to the arena complexity variable. All variables were z-scored for each participant prior to model fittings, so the intercepts were close to zero for most cases and not shown in the bar plots.<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>J</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo>∼</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-13"><title>Relevance estimation</title><p>Prior to estimating the task-relevance of the participants’ gaze positions at each time point, the closest transition <inline-formula><mml:math id="inf98"><mml:mi>k</mml:mi></mml:math></inline-formula> to the participant’s point of gaze was identified and the effect of toggling the transition on the value function was computed as <inline-formula><mml:math id="inf99"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In order to construct a null distribution of relevance values, we paired the eye movements on each trial with the goal location for a random trial, given the participant’s position in the current trial. This shuffled average is not task-specific, and therefore may be compared with the true <inline-formula><mml:math id="inf100"><mml:mi mathvariant="normal">Ω</mml:mi></mml:math></inline-formula> values to probe whether the spatial distribution of gaze positions was sensitive to the goal location on each trial. Similarly, the shuffled fraction of time looking at the goal was computed with a goal state randomly chosen from all states.</p></sec><sec id="s4-14"><title>Sweep classification</title><p>Forward and backward eye movements (sweeps) along the intended trajectory were classified by first calculating the point <inline-formula><mml:math id="inf101"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> on the trajectory closest to the location of gaze in each frame. For each trial, the fraction of the total trajectory length corresponding to each point was stored as a variable <inline-formula><mml:math id="inf102"><mml:mi>f</mml:mi></mml:math></inline-formula>, and periods when <inline-formula><mml:math id="inf103"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> consecutively ascended or descended were identified. For each period, we determined <inline-formula><mml:math id="inf104"><mml:mi>m</mml:mi></mml:math></inline-formula>, an integer whose magnitude denoted the sequence length and whose sign denoted the sequence direction (+/− for ascending/descending sequences). We then constructed a null distribution <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> describing the chance-level frequency of <inline-formula><mml:math id="inf106"><mml:mi>m</mml:mi></mml:math></inline-formula> by selecting 20 random trials and recomputing <inline-formula><mml:math id="inf107"><mml:mi>f</mml:mi></mml:math></inline-formula> based on the participant’s trajectories on those trials. Sequential eye movements of length <inline-formula><mml:math id="inf108"><mml:mi>m</mml:mi></mml:math></inline-formula> where the CDF of <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> was less than <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>α</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> or greater than <inline-formula><mml:math id="inf111"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> were classified as backward and forward sweeps, respectively. The significance threshold α was chosen to be 0.02. Compensating for noise in the gaze position, we applied a median filter of length 20 frames to both the true and shuffled <inline-formula><mml:math id="inf112"><mml:mi>f</mml:mi></mml:math></inline-formula> functions. During post-processing, sweeps in the same direction that were separated by less than 25 frames were merged, and sweeps for which the gaze fell outside of 2 meters from the intended trajectory on &gt; 30% of the frames pertaining to the sweep were eliminated. Sweeps were required to be at least 25 frames in length. To remove periods of fixation, the minimum variance in <inline-formula><mml:math id="inf113"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> values for all time points corresponding to the sweep was required to be 0.001. Finally, sweeps which did not cover at least 20% of the total trajectory length were removed from the analyses. This algorithm allowed for the automated detection of sequential eye movements pertaining to the prospective evaluation of trajectories which participants subsequently took.</p></sec><sec id="s4-15"><title>Alternative trajectories</title><p>To find the number of trajectory options for each trial, we identified all paths between the initial and goal states that were comparable within a factor of 1.25 to the optimal trajectory length and shared no more than 50% of the states with each other. The factor of 1.25 ensured that the trajectories were within 1 SD of the trajectory chosen by the participants. The gaze was classified to be exploring an alternative trajectory only when it was disjoint from the trajectory that the participant executed.</p></sec><sec id="s4-16"><title>Saccade detection</title><p>Saccade times were classified to be the times at which eye movement speeds <inline-formula><mml:math id="inf114"><mml:mi>v</mml:mi></mml:math></inline-formula> crossed a threshold of 50 °/s from below, where speeds were computed using <xref ref-type="disp-formula" rid="equ11">Equation 9</xref>, where <inline-formula><mml:math id="inf115"><mml:mi>x</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf116"><mml:mi>y</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf117"><mml:mi>z</mml:mi></mml:math></inline-formula> correspond to the coordinates of the point of gaze (averaged across both eyes), and α and β respectively correspond to the lateral and vertical displacement of the pupil in degrees.<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>tan</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="260%" minsize="260%">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac><mml:mo maxsize="260%" minsize="260%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">,</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>tan</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="260%" minsize="260%">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac><mml:mo maxsize="260%" minsize="260%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">,</mml:mo><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-17"><title>Subgoal analysis</title><p>Turns in the participants’ trajectories (defined as subgoals) were isolated by applying a threshold of 60 °/s on their angular velocity (smoothed with a median filter; window size = 8 frames). The first and last frames for periods of elevated angular velocity were recorded. For the purposes of the analysis in <xref ref-type="fig" rid="fig5">Figure 5</xref>, all trials (for all participants in all arenas) were broken down into periods of turns vs periods of navigating straight segments. Starting from the stopping location, these periods were independently interpolated to fit an arbitrarily defined common timeline of 25 time points per turn and 100 time points per straight segment. For example, if a trial had two turns, then eye movement variables from the last turn to the stopping location were interpolated to the points –100 to –1. The last turn was represented by –125 to –101, and the second last turn was represented by –250 to –226. The segment between the two turns was represented by –225 to –126. Finally, the segment between the participant’s starting position and the first turn was represented by –350 to –251. Note that as a consequence, the number of trials for which there were (for example) more than four turns in the trajectory was substantially fewer than the number of trials for which there were one or no turns, such that the quantity of raw data contributing to each normalized position value in <xref ref-type="fig" rid="fig5">Figure 5</xref> increases from left to right.</p></sec><sec id="s4-18"><title>Data and code availability</title><p>The dataset is available at <ext-link ext-link-type="uri" xlink:href="https://gin.g-node.org/neuro-sci/gaze-navigation">https://gin.g-node.org/neuro-sci/gaze-navigation</ext-link>, and the MATLAB code used to produce the analysis figures has been published at <ext-link ext-link-type="uri" xlink:href="https://github.com/neuro-sci/gaze-navigation">https://github.com/neuro-sci/gaze-navigation</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:7e4f7c529ca4ec260472377b433566d12b66c81c;origin=https://github.com/neuro-sci/gaze-navigation;visit=swh:1:snp:bb633cd882775574bf7a7cf25abf594b929dbbc2;anchor=swh:1:rev:91870d7384c539b656f5dcab69bc24b83eece161">swh:1:rev:91870d7384c539b656f5dcab69bc24b83eece161</ext-link>; <xref ref-type="bibr" rid="bib87">Zhu, 2022</xref>).</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Methodology, Supervision, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Methodology, Software</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Funding acquisition, Methodology, Project administration, Resources, Supervision, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All experimental procedures were approved by the Institutional Review Board at New York University and all participants signed an informed consent form (IRB-FY2019-2599).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-73097-mdarchecklist1-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Links to data and code are included in the manuscript.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>SL</given-names></name><name><surname>Lakshminarasimhan</surname><given-names>KJ</given-names></name><name><surname>Arfaei</surname><given-names>N</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Gaze-navigation</data-title><source>G-node</source><pub-id pub-id-type="accession" xlink:href="https://gin.g-node.org/neuro-sci/gaze-navigation">neuro-sci/gaze-navigation</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank the members of the Angelaki Lab and Professor Wei Ji Ma for insightful discussions. This work was supported by the NIH (1U19-NS118246 – BRAIN Initiative, 1R01-EY022538), the NSF NeuroNex Award (DBI-1707398) and the Gatsby Charitable Foundation.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ahmad</surname><given-names>S</given-names></name><name><surname>Yu</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Active sensing as bayes-optimal sequential decision-making</article-title><conf-name>Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013</conf-name><fpage>12</fpage><lpage>21</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakkour</surname><given-names>A</given-names></name><name><surname>Palombo</surname><given-names>DJ</given-names></name><name><surname>Zylberberg</surname><given-names>A</given-names></name><name><surname>Kang</surname><given-names>YH</given-names></name><name><surname>Reid</surname><given-names>A</given-names></name><name><surname>Verfaellie</surname><given-names>M</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The hippocampus supports deliberation during value-based decisions</article-title><source>eLife</source><volume>8</volume><elocation-id>e46080</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.46080</pub-id><pub-id pub-id-type="pmid">31268419</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balaguer</surname><given-names>J</given-names></name><name><surname>Spiers</surname><given-names>H</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural Mechanisms of Hierarchical Planning in a Virtual Subway Network</article-title><source>Neuron</source><volume>90</volume><fpage>893</fpage><lpage>903</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.03.037</pub-id><pub-id pub-id-type="pmid">27196978</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellman</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1954">1954</year><article-title>Some Problems in the Theory of Dynamic Programming</article-title><source>Econometrica : Journal of the Econometric Society</source><volume>22</volume><elocation-id>37</elocation-id><pub-id pub-id-type="doi">10.2307/1909830</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bermudez-Contreras</surname><given-names>E</given-names></name><name><surname>Clark</surname><given-names>BJ</given-names></name><name><surname>Wilber</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The Neuroscience of Spatial Navigation and the Relationship to Artificial Intelligence</article-title><source>Frontiers in Computational Neuroscience</source><volume>14</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.3389/fncom.2020.00063</pub-id><pub-id pub-id-type="pmid">32848684</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>TI</given-names></name><name><surname>Carr</surname><given-names>VA</given-names></name><name><surname>LaRocque</surname><given-names>KF</given-names></name><name><surname>Favila</surname><given-names>SE</given-names></name><name><surname>Gordon</surname><given-names>AM</given-names></name><name><surname>Bowles</surname><given-names>B</given-names></name><name><surname>Bailenson</surname><given-names>JN</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prospective representation of navigational goals in the human hippocampus</article-title><source>Science (New York, N.Y.)</source><volume>352</volume><fpage>1323</fpage><lpage>1326</lpage><pub-id pub-id-type="doi">10.1126/science.aaf0784</pub-id><pub-id pub-id-type="pmid">27284194</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buhry</surname><given-names>L</given-names></name><name><surname>Azizi</surname><given-names>AH</given-names></name><name><surname>Cheng</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reactivation, replay, and preplay: how it might all fit together</article-title><source>Neural Plasticity</source><volume>2011</volume><elocation-id>203462</elocation-id><pub-id pub-id-type="doi">10.1155/2011/203462</pub-id><pub-id pub-id-type="pmid">21918724</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caspi</surname><given-names>A</given-names></name><name><surname>Beutter</surname><given-names>BR</given-names></name><name><surname>Eckstein</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The time course of visual information accrual guiding eye movement decisions</article-title><source>PNAS</source><volume>101</volume><fpage>13086</fpage><lpage>13090</lpage><pub-id pub-id-type="doi">10.1073/pnas.0305329101</pub-id><pub-id pub-id-type="pmid">15326284</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crowe</surname><given-names>DA</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Chafee</surname><given-names>MV</given-names></name><name><surname>Anderson</surname><given-names>JH</given-names></name><name><surname>Georgopoulos</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Mental maze solving</article-title><source>Journal of Cognitive Neuroscience</source><volume>12</volume><fpage>813</fpage><lpage>827</lpage><pub-id pub-id-type="doi">10.1162/089892900562426</pub-id><pub-id pub-id-type="pmid">11054923</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crowe</surname><given-names>DA</given-names></name><name><surname>Chafee</surname><given-names>MV</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Georgopoulos</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Neural activity in primate parietal area 7a related to spatial analysis of visual mazes</article-title><source>Cerebral Cortex (New York, N.Y</source><volume>14</volume><fpage>23</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhg088</pub-id><pub-id pub-id-type="pmid">14654454</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>de Cothi</surname><given-names>W</given-names></name><name><surname>Nyberg</surname><given-names>N</given-names></name><name><surname>Griesbauer</surname><given-names>EM</given-names></name><name><surname>Ghanamé</surname><given-names>C</given-names></name><name><surname>Zisch</surname><given-names>F</given-names></name><name><surname>Lefort</surname><given-names>JM</given-names></name><name><surname>Fletcher</surname><given-names>L</given-names></name><name><surname>Newton</surname><given-names>C</given-names></name><name><surname>Renaudineau</surname><given-names>S</given-names></name><name><surname>Bendor</surname><given-names>D</given-names></name><name><surname>Grieves</surname><given-names>R</given-names></name><name><surname>Duvelle</surname><given-names>É</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Predictive Maps in Rats and Humans for Spatial Navigation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.09.26.314815</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diba</surname><given-names>K</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Forward and reverse hippocampal place-cell sequences during ripples</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1241</fpage><lpage>1242</lpage><pub-id pub-id-type="doi">10.1038/nn1961</pub-id><pub-id pub-id-type="pmid">17828259</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dragoi</surname><given-names>G</given-names></name><name><surname>Tonegawa</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Preplay of future place cell sequences by hippocampal cellular assemblies</article-title><source>Nature</source><volume>469</volume><fpage>397</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1038/nature09633</pub-id><pub-id pub-id-type="pmid">21179088</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eckstein</surname><given-names>MK</given-names></name><name><surname>Guerra-Carrillo</surname><given-names>B</given-names></name><name><surname>Miller Singley</surname><given-names>AT</given-names></name><name><surname>Bunge</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Beyond eye gaze: What else can eyetracking reveal about cognition and cognitive development?</article-title><source>Developmental Cognitive Neuroscience</source><volume>25</volume><fpage>69</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/j.dcn.2016.11.001</pub-id><pub-id pub-id-type="pmid">27908561</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eckstein</surname><given-names>MK</given-names></name><name><surname>Collins</surname><given-names>AGE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Computational evidence for hierarchically structured reinforcement learning in humans</article-title><source>PNAS</source><volume>117</volume><fpage>29381</fpage><lpage>29389</lpage><pub-id pub-id-type="doi">10.1073/pnas.1912330117</pub-id><pub-id pub-id-type="pmid">33229518</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>M</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Time-compressed preplay of anticipated events in human primary visual cortex</article-title><source>Nature Communications</source><volume>8</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/ncomms15276</pub-id><pub-id pub-id-type="pmid">28534870</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eldar</surname><given-names>E</given-names></name><name><surname>Lièvre</surname><given-names>G</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The roles of online and offline replay in planning</article-title><source>eLife</source><volume>9</volume><elocation-id>e56911</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.56911</pub-id><pub-id pub-id-type="pmid">32553110</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fotowat</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>C</given-names></name><name><surname>Jun</surname><given-names>JJ</given-names></name><name><surname>Maler</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural activity in a hippocampus-like region of the teleost pallium is associated with active sensing and navigation</article-title><source>eLife</source><volume>8</volume><elocation-id>e44119</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.44119</pub-id><pub-id pub-id-type="pmid">30942169</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frey</surname><given-names>M</given-names></name><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Magnetic resonance-based eye tracking using deep neural networks</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1772</fpage><lpage>1779</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00947-w</pub-id><pub-id pub-id-type="pmid">34750593</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaymard</surname><given-names>B</given-names></name><name><surname>Rivaud</surname><given-names>S</given-names></name><name><surname>Cassarini</surname><given-names>JF</given-names></name><name><surname>Dubard</surname><given-names>T</given-names></name><name><surname>Rancurel</surname><given-names>G</given-names></name><name><surname>Agid</surname><given-names>Y</given-names></name><name><surname>Pierrot-Deseilligny</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Effects of anterior cingulate cortex lesions on ocular saccades in humans</article-title><source>Experimental Brain Research</source><volume>120</volume><fpage>173</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.1007/s002210050391</pub-id><pub-id pub-id-type="pmid">9629959</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghamari</surname><given-names>H</given-names></name><name><surname>Golshany</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Wandering Eyes: Using Gaze-Tracking Method to Capture Eye Fixations in Unfamiliar Healthcare Environments</article-title><source>HERD</source><volume>15</volume><fpage>115</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1177/19375867211042344</pub-id><pub-id pub-id-type="pmid">34477015</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillespie</surname><given-names>AK</given-names></name><name><surname>Astudillo Maya</surname><given-names>DA</given-names></name><name><surname>Denovellis</surname><given-names>EL</given-names></name><name><surname>Liu</surname><given-names>DF</given-names></name><name><surname>Kastner</surname><given-names>DB</given-names></name><name><surname>Coulter</surname><given-names>ME</given-names></name><name><surname>Roumis</surname><given-names>DK</given-names></name><name><surname>Eden</surname><given-names>UT</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Hippocampal replay reflects specific past experiences rather than a plan for subsequent choice</article-title><source>Neuron</source><volume>109</volume><fpage>3149</fpage><lpage>3163</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.07.029</pub-id><pub-id pub-id-type="pmid">34450026</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gottlieb</surname><given-names>J</given-names></name><name><surname>Oudeyer</surname><given-names>PY</given-names></name><name><surname>Lopes</surname><given-names>M</given-names></name><name><surname>Baranes</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Information-seeking, curiosity, and attention: computational and neural mechanisms</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>585</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.09.001</pub-id><pub-id pub-id-type="pmid">24126129</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gottlieb</surname><given-names>J</given-names></name><name><surname>Hayhoe</surname><given-names>M</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name><name><surname>Rangel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Attention, Reward, and Information Seeking</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>15497</fpage><lpage>15504</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3270-14.2014</pub-id><pub-id pub-id-type="pmid">25392517</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gottlieb</surname><given-names>J</given-names></name><name><surname>Oudeyer</surname><given-names>PY</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Towards a neuroscience of active sampling and curiosity</article-title><source>Nature Reviews. Neuroscience</source><volume>19</volume><fpage>758</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1038/s41583-018-0078-0</pub-id><pub-id pub-id-type="pmid">30397322</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gustafson</surname><given-names>NJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Grid cells, place cells, and geodesic generalization for spatial reinforcement learning</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002235</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002235</pub-id><pub-id pub-id-type="pmid">22046115</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hannula</surname><given-names>DE</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The eyes have it: hippocampal activity predicts expression of memory in eye movements</article-title><source>Neuron</source><volume>63</volume><fpage>592</fpage><lpage>599</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.08.025</pub-id><pub-id pub-id-type="pmid">19755103</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayhoe</surname><given-names>M</given-names></name><name><surname>Ballard</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Eye movements in natural behavior</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>188</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.02.009</pub-id><pub-id pub-id-type="pmid">15808501</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Shinkareva</surname><given-names>SV</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Luke</surname><given-names>SG</given-names></name><name><surname>Olejarczyk</surname><given-names>J</given-names></name><name><surname>Paterson</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Predicting Cognitive State from Eye Movements</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e64937</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0064937</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Hayes</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Meaning-based guidance of attention in scenes as revealed by meaning maps</article-title><source>Nature Human Behaviour</source><volume>1</volume><fpage>743</fpage><lpage>747</lpage><pub-id pub-id-type="doi">10.1038/s41562-017-0208-0</pub-id><pub-id pub-id-type="pmid">31024101</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hikosaka</surname><given-names>O</given-names></name><name><surname>Nakamura</surname><given-names>K</given-names></name><name><surname>Nakahara</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Basal ganglia orient eyes to reward</article-title><source>Journal of Neurophysiology</source><volume>95</volume><fpage>567</fpage><lpage>584</lpage><pub-id pub-id-type="doi">10.1152/jn.00458.2005</pub-id><pub-id pub-id-type="pmid">16424448</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoppe</surname><given-names>S</given-names></name><name><surname>Loetscher</surname><given-names>T</given-names></name><name><surname>Morey</surname><given-names>SA</given-names></name><name><surname>Bulling</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Eye Movements During Everyday Behavior Predict Personality Traits</article-title><source>Frontiers in Human Neuroscience</source><volume>12</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.3389/fnhum.2018.00105</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoppe</surname><given-names>D</given-names></name><name><surname>Rothkopf</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Multi-step planning of eye movements in visual search</article-title><source>Scientific Reports</source><volume>9</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41598-018-37536-0</pub-id><pub-id pub-id-type="pmid">30644423</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Kaanders</surname><given-names>P</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name><name><surname>Mugan</surname><given-names>U</given-names></name><name><surname>Procyk</surname><given-names>E</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name><name><surname>Russo</surname><given-names>E</given-names></name><name><surname>Scholl</surname><given-names>J</given-names></name><name><surname>Stachenfeld</surname><given-names>K</given-names></name><name><surname>Wilson</surname><given-names>CRE</given-names></name><name><surname>Kolling</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Formalizing planning and information search in naturalistic decision-making</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1051</fpage><lpage>1064</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00866-w</pub-id><pub-id pub-id-type="pmid">34155400</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hutton</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cognitive control of saccadic eye movements</article-title><source>Brain and Cognition</source><volume>68</volume><fpage>327</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1016/j.bandc.2008.08.021</pub-id><pub-id pub-id-type="pmid">19028265</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Javadi</surname><given-names>AH</given-names></name><name><surname>Emo</surname><given-names>B</given-names></name><name><surname>Howard</surname><given-names>LR</given-names></name><name><surname>Zisch</surname><given-names>FE</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Knight</surname><given-names>R</given-names></name><name><surname>Pinelo Silva</surname><given-names>J</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hippocampal and prefrontal processing of network topology to simulate the future</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>14652</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms14652</pub-id><pub-id pub-id-type="pmid">28323817</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>A</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neural ensembles in CA3 transiently encode paths forward of the animal at a decision point</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>12176</fpage><lpage>12189</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3761-07.2007</pub-id><pub-id pub-id-type="pmid">17989284</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnston</surname><given-names>K</given-names></name><name><surname>Everling</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Monkey dorsolateral prefrontal cortex sends task-selective signals directly to the superior colliculus</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>12471</fpage><lpage>12478</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4101-06.2006</pub-id><pub-id pub-id-type="pmid">17135409</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>JJ</given-names></name><name><surname>Longtin</surname><given-names>A</given-names></name><name><surname>Maler</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Active sensing associated with spatial learning reveals memory-based attention in an electric fish</article-title><source>Journal of Neurophysiology</source><volume>115</volume><fpage>2577</fpage><lpage>2592</lpage><pub-id pub-id-type="doi">10.1152/jn.00979.2015</pub-id><pub-id pub-id-type="pmid">26961107</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaplan</surname><given-names>R</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Planning and navigation as active inference</article-title><source>Biological Cybernetics</source><volume>112</volume><fpage>323</fpage><lpage>343</lpage><pub-id pub-id-type="doi">10.1007/s00422-018-0753-2</pub-id><pub-id pub-id-type="pmid">29572721</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Killian</surname><given-names>NJ</given-names></name><name><surname>Jutras</surname><given-names>MJ</given-names></name><name><surname>Buffalo</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A map of visual space in the primate entorhinal cortex</article-title><source>Nature</source><volume>491</volume><fpage>761</fpage><lpage>764</lpage><pub-id pub-id-type="doi">10.1038/nature11587</pub-id><pub-id pub-id-type="pmid">23103863</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Killian</surname><given-names>NJ</given-names></name><name><surname>Buffalo</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Grid cells map the visual world</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>161</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1038/s41593-017-0062-4</pub-id><pub-id pub-id-type="pmid">29371658</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koenig</surname><given-names>S</given-names></name><name><surname>Kadel</surname><given-names>H</given-names></name><name><surname>Uengoer</surname><given-names>M</given-names></name><name><surname>Schubö</surname><given-names>A</given-names></name><name><surname>Lachnit</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reward Draws the Eye, Uncertainty Holds the Eye: Associative Learning Modulates Distractor Interference in Visual Search</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>11</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.3389/fnbeh.2017.00128</pub-id><pub-id pub-id-type="pmid">28744206</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kowler</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Eye movements: the past 25 years</article-title><source>Vision Research</source><volume>51</volume><fpage>1457</fpage><lpage>1483</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2010.12.014</pub-id><pub-id pub-id-type="pmid">21237189</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Economides</surname><given-names>M</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fast Sequences of Non-spatial State Representations in Humans</article-title><source>Neuron</source><volume>91</volume><fpage>194</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.05.028</pub-id><pub-id pub-id-type="pmid">27321922</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakshminarasimhan</surname><given-names>KJ</given-names></name><name><surname>Avila</surname><given-names>E</given-names></name><name><surname>Neyhart</surname><given-names>E</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Tracking the Mind’s Eye: Primate Gaze Behavior during Virtual Visuomotor Navigation Reflects Belief Dynamics</article-title><source>Neuron</source><volume>106</volume><fpage>662</fpage><lpage>674</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.02.023</pub-id><pub-id pub-id-type="pmid">32171388</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larson</surname><given-names>AM</given-names></name><name><surname>Loschky</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The contributions of central versus peripheral vision to scene gist recognition</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.1167/9.10.6</pub-id><pub-id pub-id-type="pmid">19810787</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leigh</surname><given-names>RJ</given-names></name><name><surname>Kennard</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Using saccades as a research tool in the clinical neurosciences</article-title><source>Brain</source><volume>127</volume><fpage>460</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1093/brain/awh035</pub-id><pub-id pub-id-type="pmid">14607787</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>ZX</given-names></name><name><surname>Shen</surname><given-names>K</given-names></name><name><surname>Olsen</surname><given-names>RK</given-names></name><name><surname>Ryan</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Visual Sampling Predicts Hippocampal Activity</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>599</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2610-16.2016</pub-id><pub-id pub-id-type="pmid">28100742</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Human Replay Spontaneously Reorganizes Experience</article-title><source>Cell</source><volume>178</volume><fpage>640</fpage><lpage>652</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.06.012</pub-id><pub-id pub-id-type="pmid">31280961</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Mattar</surname><given-names>MG</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Experience replay is associated with efficient nonlocal learning</article-title><source>Science (New York, N.Y.)</source><volume>372</volume><elocation-id>eabf1357</elocation-id><pub-id pub-id-type="doi">10.1126/science.abf1357</pub-id><pub-id pub-id-type="pmid">34016753</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Navalpakkam</surname><given-names>V</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Berg</surname><given-names>R</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Behavior and neural basis of near-optimal visual search</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>783</fpage><lpage>790</lpage><pub-id pub-id-type="doi">10.1038/nn.2814</pub-id><pub-id pub-id-type="pmid">21552276</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattar</surname><given-names>MG</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Prioritized memory access explains planning and hippocampal replay</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1609</fpage><lpage>1617</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0232-z</pub-id><pub-id pub-id-type="pmid">30349103</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattar</surname><given-names>MG</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Planning in the brain</article-title><source>Neuron</source><volume>110</volume><fpage>914</fpage><lpage>934</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.12.018</pub-id><pub-id pub-id-type="pmid">35041804</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meister</surname><given-names>MLR</given-names></name><name><surname>Buffalo</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neurons in Primate Entorhinal Cortex Represent Gaze Position in Multiple Spatial Reference Frames</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>2430</fpage><lpage>2441</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2432-17.2018</pub-id><pub-id pub-id-type="pmid">29386260</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Venditto</surname><given-names>SJC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Multi-step planning in the brain</article-title><source>Current Opinion in Behavioral Sciences</source><volume>38</volume><fpage>29</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.07.003</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Russek</surname><given-names>EM</given-names></name><name><surname>Cheong</surname><given-names>JH</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The successor representation in human reinforcement learning</article-title><source>Nature Human Behaviour</source><volume>1</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1038/s41562-017-0180-8</pub-id><pub-id pub-id-type="pmid">31024137</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monaco</surname><given-names>JD</given-names></name><name><surname>Rao</surname><given-names>G</given-names></name><name><surname>Roth</surname><given-names>ED</given-names></name><name><surname>Knierim</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Attentive scanning behavior drives one-trial potentiation of hippocampal place fields</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>725</fpage><lpage>731</lpage><pub-id pub-id-type="doi">10.1038/nn.3687</pub-id><pub-id pub-id-type="pmid">24686786</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Najemnik</surname><given-names>J</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Optimal eye movement strategies in visual search</article-title><source>Nature</source><volume>434</volume><fpage>387</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1038/nature03390</pub-id><pub-id pub-id-type="pmid">15772663</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname><given-names>BE</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Hippocampal place-cell sequences depict future paths to remembered goals</article-title><source>Nature</source><volume>497</volume><fpage>74</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1038/nature12112</pub-id><pub-id pub-id-type="pmid">23594744</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pierrot-Deseilligny</surname><given-names>C</given-names></name><name><surname>Rivaud</surname><given-names>S</given-names></name><name><surname>Gaymard</surname><given-names>B</given-names></name><name><surname>Müri</surname><given-names>R</given-names></name><name><surname>Vermersch</surname><given-names>AI</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Cortical control of saccades</article-title><source>Annals of Neurology</source><volume>37</volume><fpage>557</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1002/ana.410370504</pub-id><pub-id pub-id-type="pmid">7755349</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pierrot-Deseilligny</surname><given-names>C</given-names></name><name><surname>Müri</surname><given-names>RM</given-names></name><name><surname>Nyffeler</surname><given-names>T</given-names></name><name><surname>Milea</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The role of the human dorsolateral prefrontal cortex in ocular motor behavior</article-title><source>Annals of the New York Academy of Sciences</source><volume>1039</volume><fpage>239</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1196/annals.1325.023</pub-id><pub-id pub-id-type="pmid">15826978</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Postle</surname><given-names>BR</given-names></name><name><surname>Idzikowski</surname><given-names>C</given-names></name><name><surname>Sala</surname><given-names>SD</given-names></name><name><surname>Logie</surname><given-names>RH</given-names></name><name><surname>Baddeley</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The selective disruption of spatial working memory by eye movements</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>59</volume><fpage>100</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1080/17470210500151410</pub-id><pub-id pub-id-type="pmid">16556561</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rasmussen</surname><given-names>D</given-names></name><name><surname>Voelker</surname><given-names>A</given-names></name><name><surname>Eliasmith</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A neural model of hierarchical reinforcement learning</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0180234</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0180234</pub-id><pub-id pub-id-type="pmid">28683111</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Vicarious trial and error</article-title><source>Nature Reviews Neuroscience</source><volume>17</volume><fpage>147</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1038/nrn.2015.30</pub-id><pub-id pub-id-type="pmid">26891625</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renninger</surname><given-names>LW</given-names></name><name><surname>Verghese</surname><given-names>P</given-names></name><name><surname>Coughlan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Where to look next? Eye movements reduce local uncertainty</article-title><source>Journal of Vision</source><volume>7</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1167/7.3.6</pub-id><pub-id pub-id-type="pmid">17461684</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ringo</surname><given-names>JL</given-names></name><name><surname>Sobotka</surname><given-names>S</given-names></name><name><surname>Diltz</surname><given-names>MD</given-names></name><name><surname>Bunce</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Eye movements modulate activity in hippocampal, parahippocampal, and inferotemporal neurons</article-title><source>Journal of Neurophysiology</source><volume>71</volume><fpage>1285</fpage><lpage>1288</lpage><pub-id pub-id-type="doi">10.1152/jn.1994.71.3.1285</pub-id><pub-id pub-id-type="pmid">8201422</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ryan</surname><given-names>JD</given-names></name><name><surname>Shen</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The eyes are a window into memory</article-title><source>Current Opinion in Behavioral Sciences</source><volume>32</volume><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2019.12.014</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Wilson</surname><given-names>DA</given-names></name><name><surname>Radman</surname><given-names>T</given-names></name><name><surname>Scharfman</surname><given-names>H</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Dynamics of Active Sensing and perceptual selection</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>172</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.02.010</pub-id><pub-id pub-id-type="pmid">20307966</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schütt</surname><given-names>HH</given-names></name><name><surname>Rothkegel</surname><given-names>LOM</given-names></name><name><surname>Trukenbrod</surname><given-names>HA</given-names></name><name><surname>Engbert</surname><given-names>R</given-names></name><name><surname>Wichmann</surname><given-names>FA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Disentangling bottom-up versus top-down and low-level versus high-level influences on eye movements over time</article-title><source>Journal of Vision</source><volume>19</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/19.3.1</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>DA</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural Correlates of Forward Planning in a Spatial Decision Task in Humans</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>5526</fpage><lpage>5539</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4647-10.2011</pub-id><pub-id pub-id-type="pmid">21471389</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sipatchin</surname><given-names>A</given-names></name><name><surname>Wahl</surname><given-names>S</given-names></name><name><surname>Rifai</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Eye-Tracking for Low Vision with Virtual Reality (VR): Testing Status Quo Usability of the HTC Vive Pro Eye2</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.07.29.220889</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smittenaar</surname><given-names>P</given-names></name><name><surname>FitzGerald</surname><given-names>THB</given-names></name><name><surname>Romei</surname><given-names>V</given-names></name><name><surname>Wright</surname><given-names>ND</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Disruption of dorsolateral prefrontal cortex decreases model-based in favor of model-free control in humans</article-title><source>Neuron</source><volume>80</volume><fpage>914</fpage><lpage>919</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.08.009</pub-id><pub-id pub-id-type="pmid">24206669</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solway</surname><given-names>A</given-names></name><name><surname>Diuk</surname><given-names>C</given-names></name><name><surname>Córdova</surname><given-names>N</given-names></name><name><surname>Yee</surname><given-names>D</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimal behavioral hierarchy</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003779</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003779</pub-id><pub-id pub-id-type="pmid">25122479</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sosa</surname><given-names>M</given-names></name><name><surname>Giocomo</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Navigating for reward</article-title><source>Nature Reviews. Neuroscience</source><volume>22</volume><fpage>472</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1038/s41583-021-00479-z</pub-id><pub-id pub-id-type="pmid">34230644</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The hippocampus as a predictive map</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1643</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1038/nn.4650</pub-id><pub-id pub-id-type="pmid">28967910</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning: An Introduction</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tajima</surname><given-names>S</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Optimal policy for value-based decision-making</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>12400</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms12400</pub-id><pub-id pub-id-type="pmid">27535638</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tervo</surname><given-names>DGR</given-names></name><name><surname>Kuleshova</surname><given-names>E</given-names></name><name><surname>Manakov</surname><given-names>M</given-names></name><name><surname>Proskurin</surname><given-names>M</given-names></name><name><surname>Karlsson</surname><given-names>M</given-names></name><name><surname>Lustig</surname><given-names>A</given-names></name><name><surname>Behnam</surname><given-names>R</given-names></name><name><surname>Karpova</surname><given-names>AY</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The anterior cingulate cortex directs exploration of alternative strategies</article-title><source>Neuron</source><volume>109</volume><fpage>1876</fpage><lpage>1887</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.03.028</pub-id><pub-id pub-id-type="pmid">33852896</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tomov</surname><given-names>MS</given-names></name><name><surname>Yagati</surname><given-names>S</given-names></name><name><surname>Kumar</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>W</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Pascucci</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Discovery of hierarchical representations for efficient planning</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007594</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007594</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The hippocampus as A visual area organized by space and time: A spatiotemporal similarity hypothesis</article-title><source>Vision Research</source><volume>165</volume><fpage>123</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2019.10.007</pub-id><pub-id pub-id-type="pmid">31734633</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wilming</surname><given-names>N</given-names></name><name><surname>König</surname><given-names>P</given-names></name><name><surname>König</surname><given-names>S</given-names></name><name><surname>Buffalo</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Entorhinal Cortex Receptive Fields Are Modulated by Spatial Attention, Even without Movement</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/183327</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wunderlich</surname><given-names>K</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mapping value based planning and extensively trained choice in the human brain</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>786</fpage><lpage>791</lpage><pub-id pub-id-type="doi">10.1038/nn.3068</pub-id><pub-id pub-id-type="pmid">22406551</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>SCH</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Active sensing in the categorization of visual patterns</article-title><source>eLife</source><volume>5</volume><elocation-id>e12215</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.12215</pub-id><pub-id pub-id-type="pmid">26880546</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>SCH</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Theoretical perspectives on active sensing</article-title><source>Current Opinion in Behavioral Sciences</source><volume>11</volume><fpage>100</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2016.06.009</pub-id><pub-id pub-id-type="pmid">30175197</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>R</given-names></name><name><surname>Hansen</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Combining breadth-first and depth-first strategies in searching for treewidth</article-title><conf-name>International Symposium on Combinatorial Search, SoCS 2008</conf-name><fpage>162</fpage><lpage>168</lpage></element-citation></ref><ref id="bib87"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>neuro-sci/gaze-navigation</data-title><version designator="swh:1:rev:91870d7384c539b656f5dcab69bc24b83eece161">swh:1:rev:91870d7384c539b656f5dcab69bc24b83eece161</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/neuro-sci/gaze-navigation">https://github.com/neuro-sci/gaze-navigation</ext-link></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><p>Relevance simulations: Motivating the quantitative characterization of the task relevance of visual samples, we show an illustration of the consequence of mistaken beliefs about the passability of specific transitions on the subjective value function. Transition <italic>toggling</italic> can be defined as the act of removing an obstacle between two states if an obstacle was previously present, or adding an obstacle at that location if one was previously absent. In alignment with intuition, some transitions are more important to veridically represent (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref> – middle), as toggling them results in a dramatic change to the value function (which is essential to computing the optimal set of actions to reach the goal), while toggling some other transitions causes a relatively minimal change (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref> – right).</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Simulations validate the utility of precisely knowing the status of theoretically important transitions.</title><p>(<bold>A</bold>) Value functions corresponding to an arbitrary goal location (closed circle) in an example arena (left) and in the arenas resulting from blocking either a bottleneck transition (center) or a transition that was not a bottleneck (right). (<bold>B</bold>) Theoretical relevance of all transitions (circles) for the example arena for four different pairs of start (open black circle) and goal (closed black circle) states. (<bold>C</bold>) The betweenness centrality of a state describes the degree to which the state controls the traffic flowing through the area (see Methods). Left: Across all possible start and end locations, the mean normalized relevance of non-obstacle transitions (across all possible start and goal state pairings) was positively correlated with betweenness centrality values. Middle: The mean normalized relevance of obstacles was negatively correlated with the eccentricity of each obstacle from the straight line connecting the current state to the goal state. Right: Transitions that fell on the optimal trajectory had greater relevance than those that fell outside of it. (<bold>D</bold>) Simulation results of an agent instantiated with a perfect transition model (orange), and four agents with imperfect transition models, three of whom were endowed with the ability to correct their model according to different rules (see text). Those three agents were allowed to make eight ’saccades’, each of which could update one transition. Left: Cumulative distributions of the path lengths of various agents (100 trials each from 25 different arenas; see Methods). Right: Median of trial-averaged path lengths across all simulated arenas; data points denote trial-averaged path lengths in individual arenas. (<bold>E</bold>) Results of simulations similar to D but with a variable budget of ’saccades’. Each line denotes the average path length (across arenas) of one agent as a function of the number of ’saccades’. For each trial, path lengths of different agents were normalized by the optimal path length before trial-averaging. Error bars denote ±1 SEM. (<bold>F</bold>) Example simulated trajectories, as well as the gaze samples (red dots, if applicable), taken by each agent. The configuration of the arena reflects the agent’s subjective model at the <italic>end</italic> of all eye movements. Note that the subjective model of the ‘smart’ agent was still quite mismatched with the true world model after eight eye movements, but the visual samples allowed for the correction of the model at crucial locations such that the trajectory of the ‘smart’ agent was closer to optimal than that of the other agents.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-app1-fig1-v2.tif"/></fig><p>By defining relevance of transitions according to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, we can thus capture multiple task-relevant attributes in a succinct manner. Theoretically investigating whether looking at task-relevant transitions improves navigational efficiency, we simulated artificial agents performing the same task that we imposed upon our human participants. One agent (‘perfect’) had a veridical subjective model of the environment, and thus was capable of computing the optimal trajectory (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1f</xref>). Its antithesis (‘blind’) had an incorrect subjective model where half of the obstacle positions were ‘misremembered’ (toggled) to simulate a predicament where the agent had previous exposure to the arena, but were only halfway to learning the precise transition structure. The blind agent was incapable of using vision to correct their model prior to taking actions according to their subjectively computed value functions. Performance at these two extremes was compared against the performance of three agents that were allocated a fixed budget of ‘saccades’ to rectify their incorrect models. These agents either randomly interrogated transitions (‘random’), preferentially sampled transitions along the direction connecting the agent’s starting location to the goal location (‘goalward’), or chose the most task-relevant transitions as defined by the relevance metric in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> (‘smart’).</p><p>While all three agents showed an improvement over the ‘blind’ agent, the agent with knowledge about the most task-relevant transitions resulted in much shorter average path lengths than agents looking at transitions along the general direction of the goal or looking at random transitions (mean path length ±SE – perfect: 3.9 ± 0.1, blind: 9.6 ± 0.7, random: 8.9 ± 0.6, goalward: 8.6 ± 0.7, smart: 5.8 ± 0.3; <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1D</xref>). Moreover, the performance of the smart sampling agent quickly approached optimality as the number of sampled transitions increased (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1E</xref>). The rate of performance improvement was substantially slower for the goalward and random samplers (linear rather than exponential). These results were robust to the precise algorithm used to compute the value function in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. In particular, the successor representation (SR) has been proposed as a computationally efficient, biologically plausible alternative to pure model-based algorithms like value iteration for responding to changing goal locations (<xref ref-type="bibr" rid="bib57">Momennejad et al., 2017</xref>; <xref ref-type="bibr" rid="bib76">Stachenfeld et al., 2017</xref>). We found that estimating the task-relevance of transitions using values implied by SR resulted in a similar performance improvement (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>). Nevertheless, we emphasize that our objective was to use the relevance metric simply as a means to probe whether humans preferentially looked at task-relevant transitions. Understanding how the brain might compute such metrics is outside the scope of this study.</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Simulations reveal that foveating ‘relevant transitions’ reduces path length.</title><p>Results were robust to the precise algorithm (value iteration vs. successor representation) as well as the degree of temporal abstraction (current state vs optimal trajectory) used to estimate the relevance of transitions. Plots similar to <xref ref-type="fig" rid="fig3">Figure 3C and D</xref> are shown for relevance values calculated with A value iteration, current state, <bold>B</bold> value iteration, entire trajectory, <bold>C</bold> successor representation, current state, and <bold>D</bold> successor representation, entire trajectory.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73097-app1-fig2-v2.tif"/></fig><p>Relevance derivation: In this section, we derive a general measure to quantify the relevance of transitions with respect to the task of navigating between two given states. The following derivation focuses on the general setting when external noise (stochastic transitions) is present and internal noise (model uncertainty) is inhomogeneous. As we will show, the measure used to quantify transition relevance in the main text (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) corresponds to the special case where transitions are deterministic and uncertainty is homogeneous. Let <inline-formula><mml:math id="inf118"><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> denote the status of the <inline-formula><mml:math id="inf119"><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> stochastic transition (1 or 0) and <italic>p</italic><sub><italic>k</italic></sub> be the parameter of the true probability distribution (p.d.) of that transition such that <inline-formula><mml:math id="inf120"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf121"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="inf122"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> be the parameter of the subjective probability distribution of the transition. That is the agent thinks that <inline-formula><mml:math id="inf123"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf124"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Given a particular goal state, let <inline-formula><mml:math id="inf125"><mml:msubsup><mml:mi>V</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> denote the value of the agent’s current state <inline-formula><mml:math id="inf126"><mml:mi>s</mml:mi></mml:math></inline-formula> evaluated using the true transition status <inline-formula><mml:math id="inf127"><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> such that <inline-formula><mml:math id="inf128"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> if <inline-formula><mml:math id="inf129"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf130"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> if <inline-formula><mml:math id="inf131"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="inf132"><mml:msubsup><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> denote the expectation of the value of state <inline-formula><mml:math id="inf133"><mml:mi>s</mml:mi></mml:math></inline-formula> evaluated using the subjective transition p.d. of the <inline-formula><mml:math id="inf134"><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> transition such that <inline-formula><mml:math id="inf135"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+2.2pt"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo rspace="4.7pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Since looking at a transition will dramatically reduce the uncertainty about the status of that transition, this can impact the subjective value of the current state, provided that transition is critical to the task at hand. For instance, discovering a subway line linking your neighborhood and downtown will increase the value of your neighborhood if your workplace is located downtown, but will have no impact if your workplace is located crosstown. Therefore, we define relevance (<inline-formula><mml:math id="inf136"><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>) of the <inline-formula><mml:math id="inf137"><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> transition as the expectation of the (log) change in subjective value about the current state <inline-formula><mml:math id="inf138"><mml:mi>s</mml:mi></mml:math></inline-formula> induced by looking at that transition. Then, we have:</p><p><inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="normal">V</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="normal">V</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>.</mml:mo><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes expectation taken w.r.t. the true p.d.<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf141"><mml:mrow><mml:mtext>H</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the cross entropy between <inline-formula><mml:math id="inf142"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf143"><mml:mi>Y</mml:mi></mml:math></inline-formula><disp-formula id="equ18"><mml:math id="m18"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf144"><mml:mrow><mml:mtext>H</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the entropy of <inline-formula><mml:math id="inf145"><mml:mi>X</mml:mi></mml:math></inline-formula><disp-formula id="equ19"><mml:math id="m19"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">K</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>V</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf146"><mml:msub><mml:mover accent="true"><mml:mi>T</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> denotes the subjective knowledge about the status of the <inline-formula><mml:math id="inf147"><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> transition</p><p>Observe that <inline-formula><mml:math id="inf148"><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> is comprised of four factors: (I) <inline-formula><mml:math id="inf149"><mml:mrow><mml:mtext>H</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the entropy of <italic>p</italic><sub><italic>k</italic></sub>, which captures transition volatility, (II) <inline-formula><mml:math id="inf150"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mtext>KL</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the Kullback-Leibler divergence between true and subjective p.d., which captures the degree of mismatch between the subjective and true transition models, (III) <inline-formula><mml:math id="inf151"><mml:mrow><mml:mtext>log</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>T</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the log variance of the subjective status of the transition, which captures the agent’s uncertainty, and (IV) <inline-formula><mml:math id="inf152"><mml:mrow><mml:mrow><mml:mtext>log</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mtext>log</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the log change in the value of the current state induced by changing the transition status, which captures the sensitivity of the value function to the transition. The first and third terms suggest that an agent should prioritize looking at transitions with high volatility and high subjective uncertainty. The second term suggests that it is best to look at transitions whose subjective status is known to be wrong. Although this is mathematically correct, agents would not know the true model to begin with and therefore cannot direct their attention at such transitions. Therefore, if external and internal noise are homogeneous, the best strategy would be to look at transitions which the value function is highly sensitive to, as postulated by <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> in the main text. Note that in deriving <inline-formula><mml:math id="inf153"><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>, we have neglected the contribution of model mismatches that may exist at other transitions. This approximation will be valid if the model mismatch is small, and the solution works well in practice as demonstrated by the simulations (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p></app><app id="appendix-2"><title>Appendix 2</title><table-wrap id="app2table1" position="float"><label>Appendix 2—table 1.</label><caption><title>The order of arena presentation was randomized across participants.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Participant ID</th><th align="left" valign="bottom">Block 1</th><th align="left" valign="bottom">Block 2</th><th align="left" valign="bottom">Block 3</th><th align="left" valign="bottom">Block 4</th><th align="left" valign="bottom">Block 5</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">5</td></tr><tr><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">3</td></tr><tr><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">4</td></tr><tr><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">1</td></tr><tr><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">1</td></tr><tr><td align="char" char="." valign="bottom">6</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">4</td></tr><tr><td align="char" char="." valign="bottom">7</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">4</td></tr><tr><td align="char" char="." valign="bottom">8</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">2</td></tr><tr><td align="char" char="." valign="bottom">9</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">3</td></tr><tr><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">4</td></tr><tr><td align="char" char="." valign="bottom">11</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">1</td></tr><tr><td align="char" char="." valign="bottom">12</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">4</td></tr><tr><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">4</td></tr></tbody></table></table-wrap><table-wrap id="app2table2" position="float"><label>Appendix 2—table 2.</label><caption><title>Median true relevance values (vs median shuffled relevance) for each arena (×10<sup>-3</sup>).</title><p>Arenas 1–5 are in the order of least to greatest complexity.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Epoch</th><th align="left" valign="bottom">Arena 1</th><th align="left" valign="bottom">Arena 2</th><th align="left" valign="bottom">Arena 3</th><th align="left" valign="bottom">Arena 4</th><th align="left" valign="bottom">Arena 5</th></tr></thead><tbody><tr><td align="left" valign="bottom">Search</td><td align="char" char="." valign="bottom">0 (vs 0)</td><td align="char" char="." valign="bottom">0 (vs 0)</td><td align="char" char="." valign="bottom">0 (vs 0)</td><td align="char" char="." valign="bottom">1.4 (vs 0.1)</td><td align="char" char="." valign="bottom">5.3 (vs 1.4)</td></tr><tr><td align="left" valign="bottom">Pre-movement</td><td align="char" char="." valign="bottom">32 (vs 0)</td><td align="char" char="." valign="bottom">31 (vs 0)</td><td align="char" char="." valign="bottom">45 (vs 0)</td><td align="char" char="." valign="bottom">47 (vs 0)</td><td align="char" char="." valign="bottom">137 (vs 6.2)</td></tr><tr><td align="left" valign="bottom">Movement</td><td align="char" char="." valign="bottom">0 (vs 0)</td><td align="char" char="." valign="bottom">9.5 (vs 0)</td><td align="char" char="." valign="bottom">47 (vs 0.7)</td><td align="char" char="." valign="bottom">51 (vs 4.3)</td><td align="char" char="." valign="bottom">201 (vs 60)</td></tr></tbody></table></table-wrap><table-wrap id="app2table3" position="float"><label>Appendix 2—table 3.</label><caption><title>Number of participants (13 total) with a significant Pearson’s correlation (p≤0.05) between the dependent variable and each independent variable (number of turns, path length, bearing angle, number of trajectory options).</title><p>Note: This correlation analyses does not characterize conditional dependencies, which may also be present in the data. Such dependencies are factored into the LME model described elsewhere.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Figure(s)</th><th align="left" valign="bottom">Dependent variable</th><th align="left" valign="bottom"># Turns</th><th align="left" valign="bottom">Length</th><th align="left" valign="bottom">Bearing</th><th align="left" valign="bottom"># Options</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">1i</td><td align="left" valign="bottom">Pre-movement epoch duration</td><td align="char" char="." valign="bottom">12</td><td align="char" char="." valign="bottom">12</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">1</td></tr><tr><td align="char" char="." valign="bottom">1i</td><td align="left" valign="bottom">Movement epoch duration</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">12</td><td align="char" char="." valign="bottom">5</td></tr><tr><td align="char" char="." valign="bottom">2b</td><td align="left" valign="bottom">Variance of gaze pre-move.</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">11</td><td align="char" char="." valign="bottom">5</td></tr><tr><td align="char" char="." valign="bottom">2b</td><td align="left" valign="bottom">Variance of gaze move.</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">7</td><td align="char" char="." valign="bottom">6</td></tr><tr><td align="char" char="." valign="bottom">2d</td><td align="left" valign="bottom">Gaze @ goal duration pre-move.</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">8</td></tr><tr><td align="char" char="." valign="bottom">2d</td><td align="left" valign="bottom">Gaze @ goal duration move.</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">12</td><td align="char" char="." valign="bottom">10</td></tr><tr><td align="char" char="." valign="bottom">2 f</td><td align="left" valign="bottom">Gaze distance from goal pre-move.</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">11</td><td align="char" char="." valign="bottom">7</td></tr><tr><td align="char" char="." valign="bottom">2 f</td><td align="left" valign="bottom">Gaze distance from goal move.</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">11</td><td align="char" char="." valign="bottom">7</td></tr><tr><td align="char" char="." valign="bottom">4 c, 4d</td><td align="left" valign="bottom">% Time sweeping backward pre-move.</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">8</td><td align="char" char="." valign="bottom">7</td><td align="char" char="." valign="bottom">2</td></tr><tr><td align="char" char="." valign="bottom">4 c, 4d</td><td align="left" valign="bottom">% Time sweeping backward move.</td><td align="char" char="." valign="bottom">12</td><td align="char" char="." valign="bottom">12</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">3</td></tr><tr><td align="char" char="." valign="bottom">4 c, 4e</td><td align="left" valign="bottom">% Time sweeping forward pre-move.</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">2</td></tr><tr><td align="char" char="." valign="bottom">4 c, 4e</td><td align="left" valign="bottom">% Time sweeping forward move.</td><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">12</td><td align="char" char="." valign="bottom">12</td><td align="char" char="." valign="bottom">4</td></tr></tbody></table></table-wrap><table-wrap id="app2table4" position="float"><label>Appendix 2—table 4.</label><caption><title>Number of participants (13 total) with a significant Pearson’s correlation (p≤0.05) between the dependent variable and arena complexity (for analyses which require pooling trials).</title><p>A linear mixed effects model (LME) with random slopes and intercepts yielded participant-specific slopes, from which we computed the mean and coefficient of variation (CV). Note that results showed low between-participant variability. All variables were z-scored prior to model fitting.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Figure(s)</th><th align="left" valign="bottom">Dependent variable</th><th align="left" valign="bottom">Mean slope</th><th align="left" valign="bottom">CV slope</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">2d</td><td align="left" valign="bottom">Gaze @ goal duration move. by # turns remaining</td><td align="char" char="." valign="bottom">0.80</td><td align="char" char="hyphen" valign="bottom">5.6e<sup>-15</sup></td></tr><tr><td align="char" char="." valign="bottom">2 f</td><td align="left" valign="bottom">Gaze distance from goal move. by # turns remaining</td><td align="char" char="." valign="bottom">–0.86</td><td align="char" char="hyphen" valign="bottom">–2.2e<sup>-3</sup></td></tr></tbody></table></table-wrap></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.73097.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Hang</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.04.26.441482" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.04.26.441482"/></front-stub><body><p>This beautiful piece of work demonstrates the power of eye movement analysis in understanding the cognitive algorithms for navigation, and more generally, for real-time planning and decision making. Its sophisticated computational measures of the multiple dimensions of eye movement data can potentially inspire discoveries in many fields of cognitive neuroscience concerning rich human behavior.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.73097.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Zhang</surname><given-names>Hang</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Spiers</surname><given-names>Hugo J</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.04.26.441482">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.04.26.441482v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Eye movements reveal spatiotemporal dynamics of active sensing and planning in navigation&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Hugo J Spiers (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>Below is a summary of what we think are most important for the authors to address if you choose to submit a revised manuscript.</p><p>1) Most of the conclusions reached by the authors seem to rely on fixed-effect analysis (i.e., with all participants pooled as one), which basically neglects between-participant variability and may thus inflate the Type-I error in statistics. May the same conclusions be reached when the random effects of participants are appropriately considered (e.g., using linear mixed-effects model analysis with random intercept and slopes)?</p><p>2) A related concern is the relatively small sample size compared to contemporary navigation studies, which should be justified in the paper. The current sample size would be acceptable if the major conclusions hold on the group level when random participant effects are appropriately considered or hold on the individual level for at least 8 of all 9 participants. Otherwise, the authors are recommended to increase the sample size of the study.</p><p>3) A few factors that covary with the complexity of the maze may influence eye movement and error patterns, which include the length of the optimal path, the number of alternative plausible paths, and the affordances linked to turning biases and momentum. These factors should receive some treatments in data analysis or at least in discussion. Please see Reviewer #2's comments for details.</p><p>4) Alternative theoretical accounts for participants' eye movement patterns, other than active sensing, should be considered.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>It is worth being careful defining preplay and replay and pre-activation of activity. The pre-play of Dragoi and Tonegawa includes pre-play for visible places but also de novo preplay where rats have never seen the environment. There is distinct difference between non-local replay during sharp-wave-ripples (SWRs) and the theta oscillation state during running. It is worth making these patterns of hippocampal dynamics clearer for the readers. The findings of Johnson and Redish occur during the theta state, whereas the replay or pre-play of distant locations occur during SWRs.</p><p>What wasn't obvious what the advantage of using dynamic programming was over other methods to determine the optimal path.</p><p>I didn't see any discussion of how this approach could be applied across species. It seems a great strategy to explore planning in non-human primates and I assume this is the plan given the expertise in the research group.</p><p>It may be useful to cite this work as it is a real-world example, but lacks the computational approach taken by the authors: Ghamari, H., and Golshany, N. (2021). Wandering Eyes: Using Gaze-Tracking Method to Capture Eye Fixations in Unfamiliar Healthcare Environments. HERD: Health Environments Research and Design Journal, 19375867211042344.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.73097.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Below is a summary of what we think are most important for the authors to address if you choose to submit a revised manuscript.</p><p>1) Most of the conclusions reached by the authors seem to rely on fixed-effect analysis (i.e., with all participants pooled as one), which basically neglects between-participant variability and may thus inflate the Type-I error in statistics. May the same conclusions be reached when the random effects of participants are appropriately considered (e.g., using linear mixed-effects model analysis with random intercept and slopes)?</p></disp-quote><p>The revised manuscript includes new results from fitting linear mixed-effects models which consider the participant-specific random effects. We find that all major trends are shared across participants even after accounting for variability across participants.</p><disp-quote content-type="editor-comment"><p>2) A related concern is the relatively small sample size compared to contemporary navigation studies, which should be justified in the paper. The current sample size would be acceptable if the major conclusions hold on the group level when random participant effects are appropriately considered or hold on the individual level for at least 8 of all 9 participants. Otherwise, the authors are recommended to increase the sample size of the study.</p></disp-quote><p>We conducted a new round of experiments by recruiting more participants, increasing our sample size to 13. We also share data in the tables in Appendix 2 showing that main results also hold on the individual level for a vast majority of the participants.</p><disp-quote content-type="editor-comment"><p>3) A few factors that covary with the complexity of the maze may influence eye movement and error patterns, which include the length of the optimal path, the number of alternative plausible paths, and the affordances linked to turning biases and momentum. These factors should receive some treatments in data analysis or at least in discussion. Please see Reviewer #2's comments for details.</p></disp-quote><p>Since path length and turns covary with maze complexity, they have now been added as covariates in a linear mixed effects model to predict trial-by-trial variability in error patterns, pre-movement duration, as well as spatial and temporal features of eye movements. These results are presented in the revised manuscript (bar graphs in Figures 2, 4 Figure 1 —figure supplement 1, Figure 1 —figure supplement 2, Figure 4 —figure supplements 1, 2, and 3). We found that the number of alternative paths does not covary with complexity – both the simplest and the most complex maze have no non-trivial alternative trajectories. Therefore, we separately analyzed the effect of the number of alternative paths on eye movements both across time within a trial (Figure 5D) and across trials (Figure 4 —figure supplement 1D).</p><disp-quote content-type="editor-comment"><p>4) Alternative theoretical accounts for participants' eye movement patterns, other than active sensing, should be considered.</p></disp-quote><p>In the revised manuscript, we reinterpret our results in the light of the alternative account and propose testable predictions for future experiments. To achieve a more balanced treatment, we also deemphasize the dichotomy between active sensing and planning throughout the revised manuscript, starting with the title.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>It is worth being careful defining preplay and replay and pre-activation of activity. The pre-play of Dragoi and Tonegawa includes pre-play for visible places but also de novo preplay where rats have never seen the environment. There is distinct difference between non-local replay during sharp-wave-ripples (SWRs) and the theta oscillation state during running. It is worth making these patterns of hippocampal dynamics clearer for the readers. The findings of Johnson and Redish occur during the theta state, whereas the replay or pre-play of distant locations occur during SWRs.</p></disp-quote><p>We agree that there are major differences between the two major types of sequential activity observed in the medial temporal lobe. As we currently do not have enough evidence to speculate about whether gaze sweeps are similar to SWRs or theta-timed activation, we refrained from citing any literature regarding the theta rhythm and avoided discussing replay in detail, but we plan to investigate the similarities in greater detail in future.</p><disp-quote content-type="editor-comment"><p>What wasn't obvious what the advantage of using dynamic programming was over other methods to determine the optimal path.</p></disp-quote><p>We tried to improve the clarity of the sentence starting with “the optimal value function can be computed... ” by specifying that dynamic programming is a very efficient method for such a computation. Concretely, this technique returns the value function, which can be reused to compute the optimal path from any location to that particular goal location by ascending the value function without re-running the algorithm. (The computation times were manageable for medium-sized state spaces like the arenas we used, but this algorithm can become tedious for more complex tasks, and in this case, we would use other algorithms which approximate the optimal solution.)</p><disp-quote content-type="editor-comment"><p>I didn't see any discussion of how this approach could be applied across species. It seems a great strategy to explore planning in non-human primates and I assume this is the plan given the expertise in the research group.</p></disp-quote><p>Your assumption is correct. In the third and fourth paragraphs of the Discussion (line 407 and 431), we added that we would be interested in studying neural information flow between brain areas in different species, and elaborated upon which brain regions we would be interested in recording from and why.</p><p>“Neurally, this could be implemented by circuits that exert executive control over voluntary eye movements. Candidate substrates include the dorsolateral prefrontal cortex, which is known to be important for contextual information processing and memory-guided saccades […], and the anterior cingulate cortex, which is known to be involved in evaluating alternative strategies […]. To better understand the precise neural mechanisms underlying the spatial gaze patterns we observed, it would be instructive to examine the direction of information flow between the oculomotor circuitry and brain regions with strong spatial and value representations during this task in animal models.</p><p>In light of the similarities between sweeps and sequential hippocampal activations, we predict that direct or indirect hippocampal projections to higher oculomotor controllers (e.g. the supplemental eye fields through the orbitofrontal cortex) may allow eye movements to embody the underlying activations of state representations… the contents of gaze have been found to influence activity in the hippocampus […] and entorhinal cortex […]. Therefore, it is conceivable that sequential neural activity could emerge from consolidating temporally extended eye movements such as sweeps. We hope that in future, simultaneous recordings from brain areas involved in visual processing, eye movement control, and the hippocampal formation would uncover the mechanisms underlying trajectory sweeping eye movements and their relationship to perception and memory.”</p><disp-quote content-type="editor-comment"><p>It may be useful to cite this work as it is a real-world example, but lacks the computational approach taken by the authors: Ghamari, H., and Golshany, N. (2021). Wandering Eyes: Using Gaze-Tracking Method to Capture Eye Fixations in Unfamiliar Healthcare Environments. HERD: Health Environments Research and Design Journal, 19375867211042344.</p></disp-quote><p>Thank you for the reference. This is cited in the discussion as an example of a study that only allows a restricted field of view.</p></body></sub-article></article>