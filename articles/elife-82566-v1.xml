<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">82566</article-id><article-id pub-id-type="doi">10.7554/eLife.82566</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Statistical inference on representational geometries</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-288979"><name><surname>Schütt</surname><given-names>Heiko H</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2491-5710</contrib-id><email>hs3110@columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-290957"><name><surname>Kipnis</surname><given-names>Alexander D</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa2">‡</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-290958"><name><surname>Diedrichsen</surname><given-names>Jörn</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0264-8532</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-154647"><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7433-9005</contrib-id><email>N.Kriegeskorteg@columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Zuckerman Institute, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Western University</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Serences</surname><given-names>John T</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>University of California, San Diego</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Université du Luxembourg, Esch-Belval, Luxembourg</p></fn><fn fn-type="present-address" id="pa2"><label>‡</label><p>Max Planck Institute for Biological Cybernetics, Tuebingen, Germany</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>23</day><month>08</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e82566</elocation-id><history><date date-type="received" iso-8601-date="2022-08-09"><day>09</day><month>08</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-08-07"><day>07</day><month>08</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-12-16"><day>16</day><month>12</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2112.09200"/></event></pub-history><permissions><copyright-statement>© 2023, Schütt et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Schütt et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-82566-v1.pdf"/><abstract><p>Neuroscience has recently made much progress, expanding the complexity of both neural activity measurements and brain-computational models. However, we lack robust methods for connecting theory and experiment by evaluating our new big models with our new big data. Here, we introduce new inference methods enabling researchers to evaluate and compare models based on the accuracy of their predictions of representational geometries: A good model should accurately predict the distances among the neural population representations (e.g. of a set of stimuli). Our inference methods combine novel 2-factor extensions of crossvalidation (to prevent overfitting to either subjects or conditions from inflating our estimates of model accuracy) and bootstrapping (to enable inferential model comparison with simultaneous generalization to both new subjects and new conditions). We validate the inference methods on data where the ground-truth model is known, by simulating data with deep neural networks and by resampling of calcium-imaging and functional MRI data. Results demonstrate that the methods are valid and conclusions generalize correctly. These data analysis methods are available in an open-source Python toolbox (<ext-link ext-link-type="uri" xlink:href="https://rsatoolbox.readthedocs.io/en/stable/">rsatoolbox.readthedocs.io</ext-link>).</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>statistical inference</kwd><kwd>representational similarity analysis</kwd><kwd>toolbox</kwd><kwd>human</kwd><kwd>mouse</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>Forschungsstipendium SCHU 3351/1-1</award-id><principal-award-recipient><name><surname>Schütt</surname><given-names>Heiko H</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>New methods for statistical inference in representational similarity analysis were developed, tested thoroughly, and are made available in an open-source Python toolbox.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Experimental neuroscience has recently made rapid progress with technologies for measuring neural population activity. Spatial and temporal resolution, as well as the coverage of measurements across the brains of animals and humans have all improved considerably (<xref ref-type="bibr" rid="bib86">Parvizi and Kastner, 2018</xref>; <xref ref-type="bibr" rid="bib2">Abbott et al., 2020</xref>; <xref ref-type="bibr" rid="bib109">Wang and Xu, 2020</xref>; <xref ref-type="bibr" rid="bib6">Allen et al., 2021</xref>; <xref ref-type="bibr" rid="bib45">Guo et al., 2021</xref>; <xref ref-type="bibr" rid="bib106">Uğurbil, 2021</xref>; <xref ref-type="bibr" rid="bib10">Bandettini et al., 2021</xref>). Activity is measured using a wide range of techniques, including electrode recordings (<xref ref-type="bibr" rid="bib51">Jun et al., 2017</xref>; <xref ref-type="bibr" rid="bib98">Steinmetz et al., 2018</xref>; <xref ref-type="bibr" rid="bib86">Parvizi and Kastner, 2018</xref>), calcium imaging (<xref ref-type="bibr" rid="bib109">Wang and Xu, 2020</xref>), functional magnetic resonance imaging (fMRI; <xref ref-type="bibr" rid="bib6">Allen et al., 2021</xref>; <xref ref-type="bibr" rid="bib106">Uğurbil, 2021</xref>; <xref ref-type="bibr" rid="bib10">Bandettini et al., 2021</xref>), and scalp electro- and magnetoencephalography (EEG and MEG; <xref ref-type="bibr" rid="bib9">Baillet, 2017</xref>; <xref ref-type="bibr" rid="bib26">Craik et al., 2019</xref>). In parallel to the advances in measuring brain activity, theoretical neuroscience has substantially scaled up brain-computational models that implement computational theories (e.g. <xref ref-type="bibr" rid="bib69">Kriegeskorte, 2015</xref>; <xref ref-type="bibr" rid="bib53">Kell et al., 2018</xref>; <xref ref-type="bibr" rid="bib76">Kubilius et al., 2019</xref>; <xref ref-type="bibr" rid="bib119">Zhuang et al., 2021</xref>). The engineering advances associated with deep learning (e.g. <xref ref-type="bibr" rid="bib87">Paszke et al., 2019</xref>; <xref ref-type="bibr" rid="bib1">Abadi et al., 2015</xref>) provide powerful tools for modeling brain information processing for complex, naturalistic tasks (<xref ref-type="bibr" rid="bib78">LeCun et al., 2015</xref>). How to leverage the new big data to evaluate the new big models, however, is an open problem (<xref ref-type="bibr" rid="bib99">Stevenson and Kording, 2011</xref>; <xref ref-type="bibr" rid="bib95">Sejnowski et al., 2014</xref>; <xref ref-type="bibr" rid="bib97">Smith and Nichols, 2018</xref>; <xref ref-type="bibr" rid="bib71">Kriegeskorte and Douglas, 2018</xref>).</p><p>An important concept for understanding neural population codes is the concept of <italic>representational geometry</italic> (<xref ref-type="bibr" rid="bib96">Shepard and Chipman, 1970</xref>; <xref ref-type="bibr" rid="bib34">Edelman et al., 1998</xref>; <xref ref-type="bibr" rid="bib33">Edelman, 1998</xref>; <xref ref-type="bibr" rid="bib85">Norman et al., 2006</xref>; <xref ref-type="bibr" rid="bib29">Diedrichsen and Kriegeskorte, 2017</xref>; <xref ref-type="bibr" rid="bib65">Kriegeskorte et al., 2008a</xref>; <xref ref-type="bibr" rid="bib66">Kriegeskorte et al., 2008b</xref>; <xref ref-type="bibr" rid="bib24">Connolly et al., 2012</xref>; <xref ref-type="bibr" rid="bib113">Xue et al., 2010</xref>; <xref ref-type="bibr" rid="bib56">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib114">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib20">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Haxby et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Freeman et al., 2018</xref>; <xref ref-type="bibr" rid="bib58">Kietzmann et al., 2019</xref>; <xref ref-type="bibr" rid="bib102">Stringer et al., 2019</xref>; <xref ref-type="bibr" rid="bib18">Chung et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Chung and Abbott, 2021</xref>; <xref ref-type="bibr" rid="bib74">Kriegeskorte and Wei, 2021</xref>). Neural activity patterns that represent particular pieces of mental content, such as the stimuli presented in a neurophysiological experiment, can be viewed as points in the multivariate neural population response space of a brain region. The representational geometry is the geometry of these points. The geometry is characterized by the matrix of distances among the points. This distance matrix abstracts from the roles of individual neurons and provides a summary characterization of the neural population code that can be directly compared among animals and between brain and model representations (e.g. a cortical area and a layer of a neural network model). The representational geometry provides a multivariate characterization of a neural population code that can be motivated as a generalization of linear decoding analyses. A linear decoder reveals a single projection of the geometry. The full distance matrix (when measured after a transform that renders the noise isotropic) captures what information is available in any linear projection (<xref ref-type="bibr" rid="bib72">Kriegeskorte and Diedrichsen, 2019a</xref>).</p><p>A popular method for analyzing representational geometries (<xref ref-type="bibr" rid="bib68">Kriegeskorte and Kievit, 2013</xref>) on which we build here is representational similarity analysis (RSA; <xref ref-type="bibr" rid="bib65">Kriegeskorte et al., 2008a</xref>; <xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref>). RSA is a three-step process (<xref ref-type="fig" rid="fig1">Figure 1</xref>): In the first step, RSA characterizes the representational geometry of the brain region of interest (ROI) by estimating the representational distance for each pair of experimental conditions (e.g. different stimuli). The distance estimates are assembled in a representational dissimilarity matrix (RDM). We use the more general term ‘dissimilarity’ here to include dissimilarity measures that are not distances or metrics in the mathematical sense, such as crossvalidated distance estimators that can return negative values. This relaxation enables inclusion of measures that are not biased by the noise in the data (<xref ref-type="bibr" rid="bib64">Kriegeskorte et al., 2007</xref>; <xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref>; <xref ref-type="bibr" rid="bib107">Walther et al., 2016</xref>; <xref ref-type="bibr" rid="bib72">Kriegeskorte and Diedrichsen, 2019a</xref>), returning values distributed symmetrically about 0, when the true distance is 0, but patterns are noisy estimates. In the second step, each model is evaluated by the accuracy of its prediction of the data RDM. To this end, an RDM is computed for each model representation. Each model’s prediction of the data RDM is evaluated using an RDM comparator, such as a correlation coefficient. In the third step, models are inferentially compared to each other in terms of their RDM prediction accuracy to guide computational theory.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview of model-comparative inference.</title><p>(<bold>a</bold>) Multiple conditions are presented to observers and to models (here different stimulus images). The brain measurements during the presentation produce a set of measurements for each stimulus and subject, potentially with repetitions; a model yields a feature vector per stimulus. Importantly, no mapping between brain measurement channels and model features is required. (<bold>b</bold>) To compare the two representations, we compute a representational dissimilarity matrix (RDM) measuring the pairwise dissimilarities between conditions for each subject and each model. For model comparison, we perform 2-factor crossvalidation within a 2-factor bootstrap loop to estimate our uncertainty about the model performances. On each fold of crossvalidation, flexible models are fitted to the representational dissimilarities for a set of fitting stimuli estimated in a set of fitting subjects (blue fitting dissimilarities). The fitted models must then predict the representational dissimilarities among held-out test stimuli for held-out test subjects (red test dissimilarities). The resulting performance estimates are not biased by overfitting to either subjects or stimuli. (<bold>c</bold>) Based on our uncertainty about model performances (error bars indicate estimated standard errors of measurement), we can perform various statistical tests, which are marked in the graphical display. Dew drops (gray) clinging to the lower bound of the noise ceiling mark models performing significantly below the noise ceiling. White dew drops on the horizontal axis mark models whose performance significantly exceeds 0 or chance performance. Pairwise differences are summarized by arrows. Each arrow indicates that the model marked with the dot performed significantly better than the model the arrow points at and all models further away in the direction of the arrow.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82566-fig1-v1.tif"/><attrib>Image credit: Ecoset (<xref ref-type="bibr" rid="bib80">Mehrer et al., 2017</xref>) and <ext-link ext-link-type="uri" xlink:href="https://commons.wikimedia.org/wiki/File:Human_Brain_sketch_with_eyes_and_cerebrellum.svg">Wiki Commons</ext-link>.</attrib></fig><p>RSA is widely used (<xref ref-type="bibr" rid="bib68">Kriegeskorte and Kievit, 2013</xref>; <xref ref-type="bibr" rid="bib46">Haxby et al., 2014</xref>; <xref ref-type="bibr" rid="bib72">Kriegeskorte and Diedrichsen, 2019a</xref>) and has gained additional popularity with the rise of image-computable representational models like deep neural networks (e.g. <xref ref-type="bibr" rid="bib75">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="bib114">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib56">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib80">Mehrer et al., 2017</xref>; <xref ref-type="bibr" rid="bib69">Kriegeskorte, 2015</xref>; <xref ref-type="bibr" rid="bib115">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="bib112">Xu and Vaziri-Pashkam, 2021</xref>; <xref ref-type="bibr" rid="bib61">Konkle and Alvarez, 2022</xref>; <xref ref-type="bibr" rid="bib21">Cichy et al., 2016</xref>). There has been important recent progress with methods for estimating representational distances (step 1) as well as measures of RDM prediction accuracy (step 2). For RDM estimation, biased and unbiased distance estimators with improved reliability have been proposed (<xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref>; <xref ref-type="bibr" rid="bib15">Cai et al., 2019</xref>; <xref ref-type="bibr" rid="bib107">Walther et al., 2016</xref>). For quantification of the RDM prediction accuracy, the sampling distribution of distance estimators has been derived and measures of RDM prediction accuracy that take the dependencies between dissimilarity estimates into account have been proposed (<xref ref-type="bibr" rid="bib31">Diedrichsen et al., 2020</xref>). However, existing statistical inference methods for RSA (step 3) have important limitations. Established RSA inference methods (<xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref>) provide a noise ceiling and enable comparisons of fixed models with generalization to new subjects and conditions. However, they cannot handle flexible models, can be severely suboptimal in terms of statistical power, and have not been thoroughly validated using simulated or real data where ground truth is known. Addressing these shortcomings poses three substantial challenges. (1) Model-comparative inference with generalization to new conditions is not trivial because new conditions extend an RDM and the evaluation depends on pairwise dissimilarities, thus violating independence assumptions. (2) Standard methods for statistical inference do not handle multiple random factors — subjects and conditions in RSA. (3) Flexible models, that is models that have parameters enabling them to predict different RDMs, are essential for RSA (<xref ref-type="bibr" rid="bib30">Diedrichsen et al., 2018</xref>; <xref ref-type="bibr" rid="bib70">Kriegeskorte and Diedrichsen, 2016</xref>). Evaluation of such models requires methods that are unaffected by overfitting to either subjects or conditions to avoid a bias in favor of more flexible models.</p><p>Here, we introduce a comprehensive methodology for statistical inference on models that predict representational geometries (<xref ref-type="fig" rid="fig1">Figure 1</xref>). We introduce novel bootstrapping methods that support generalization of model-comparative statistical inferences to new subjects, new conditions, or both simultaneously, as required to support the theoretical claims researchers wish to make. We also introduce a novel crossvalidation method for estimation of the RDM prediction accuracy of flexible models, that is models with parameters fitted to the data (<xref ref-type="bibr" rid="bib56">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib70">Kriegeskorte and Diedrichsen, 2016</xref>). This is important, because theories do not always make a specific prediction for the representational geometry. There may be unknown parameters, such as the relative prevalences of different tuning functions (<xref ref-type="bibr" rid="bib56">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib50">Jozwik et al., 2016</xref>) in the neural population or properties of the measurement process (<xref ref-type="bibr" rid="bib70">Kriegeskorte and Diedrichsen, 2016</xref>). The combination of our 2-factor bootstrap and 2-factor crossvalidation methods enables statistical comparisons among fixed and flexible models that generalize across subjects and conditions.</p><p>We thoroughly validate the new inference methods using simulations and neural activity data. Extensive simulations based on deep neural network models and models of the measurement process enable us to test model-comparative inference in a setting where the ground-truth model (the one that actually generated the data) is known. These simulations confirm the validity of the inference procedures and their ability to generalize to the populations of subjects and/or conditions. We also validated the methods on real data from calcium imaging (mouse) and functional MRI (human). For both datasets, we confirm that conclusions generalize from an experimental dataset (a subset of the real data) to the entire dataset (which serves as a stand-in for the population). The statistical inference methodology described in this paper is available in a new open-source RSA toolbox written in Python (<ext-link ext-link-type="uri" xlink:href="https://github.com/rsagroup/rsatoolbox">https://github.com/rsagroup/rsatoolbox</ext-link>, copy archived at <xref ref-type="bibr" rid="bib94">Schütt, 2023</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We now introduce the 2-factor bootstrap procedure for model-comparative inference and the 2-factor crossvalidation procedure for unbiased evaluation of flexible models. This paper also introduces a new representational dissimilarity estimator for electrophysiological recordings of patterns of firing rates across a population of neurons, based on the KL-divergence between Poisson distributions (Appendix 2) and a faster alternative to the rank correlation <inline-formula><mml:math id="inf1"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> as an RDM comparator (<xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref>), which we call <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> (Appendix 3). The proposed inferential methods work for any representational dissimilarity measure and any RDM comparator. We evaluate alternative RDM comparators in terms of their power in Appendix 6. A complete description of all steps of the new methodology can be found in the Materials and methods (Full description of the RSA method).</p><sec id="s2-1"><title>Methods for inference on representational geometries</title><p>A simple approach to inferential comparison of two models is to compute the difference between the models’ performance estimates for each subject and use Student’s <inline-formula><mml:math id="inf3"><mml:mi>t</mml:mi></mml:math></inline-formula>-test (or a nonparametric alternative). However, inference then only takes the variability over subjects into account and thus does not justify generalization to different experimental conditions (e.g. different stimuli). Computational neuroscience usually pursues insights that generalize not only to a population of subjects but also to a population of conditions (<xref ref-type="bibr" rid="bib116">Yarkoni, 2020</xref>). To support generalization to the population of conditions statistically, we require uncertainty estimates that treat the experimental conditions as a random sample from a population (<xref ref-type="bibr" rid="bib65">Kriegeskorte et al., 2008a</xref>), whether or not the subjects are treated as a random sample.</p><p>For frequentist inference, the challenge is to estimate how variable the model-performance estimates would be if we repeated the experiment many times with new subjects and/or conditions. We would like to know (1) the variance of each model’s performance estimate and (2) the variance of the estimated performance difference for each pair of models. The variance of model-performance estimates enables us to statistically compare each model to a fixed value such as an RDM correlation of 0. The variance of our estimate of model-performance difference enables us to statistically compare two models to each other (see Frequentist tests for model evaluation and model comparison for details).</p><sec id="s2-1-1"><title>Estimating the variance of model-performance estimates for generalization to new subjects and conditions</title><p>To estimate the variance of model-performance estimates across repetitions of the experiment with new conditions, we use a bootstrap method. Bootstrap methods estimate the variance of experimental outcomes by sampling from the measured data with replacement, treating the measured data as an approximation to the population (<xref ref-type="bibr" rid="bib35">Efron and Tibshirani, 1994</xref>). The population here is the set of experimental conditions of which the actual experimental conditions can be considered a random sample. Because the conditions do not have independent influences on the model evaluations, we cannot compute a sample variance across conditions as we can across subjects to replace the bootstrap.</p><p>When we bootstrap-resample conditions, we obtain RDMs of the same size as the original RDMs, but some of the conditions will be repeated. Here, we exclude the entries that correspond to the dissimilarity of any condition with itself from the comparisons between RDMs. Simulations confirm that this procedure yields a good estimate of how variable the results are when we sample new conditions with the same subjects (Figures 4a and 6g).</p><p>For simultaneous generalization to the populations of both conditions and subjects, we can employ a 2-factor bootstrap (<xref ref-type="fig" rid="fig1">Figure 1b</xref>) as introduced previously (<xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref>; <xref ref-type="bibr" rid="bib101">Storrs et al., 2021</xref>). However, our simulations and theory here show that a naive 2-factor bootstrap approach triple-counts the variance contributed by the measurement noise (Methods, Estimating the uncertainty of our model-performance estimates, Figures 4c and 7c). This effect is not unique to RSA; a naive 2-factor bootstrap will triple-count variance related to the measurement noise for any type of experiment in which two factors (here subject and condition) jointly determine the experimental outcome. The true variance <inline-formula><mml:math id="inf4"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> of the experimental outcome when sampling both factors can be separated into a contribution from condition sampling (<inline-formula><mml:math id="inf5"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>), a contribution from subject sampling (<inline-formula><mml:math id="inf6"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>), and a contribution of the interaction of subjects and conditions or measurement noise (<inline-formula><mml:math id="inf7"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>).<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This decomposition is for the actual variance <inline-formula><mml:math id="inf8"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> across repeated experiments with new subjects and conditions. The variance <inline-formula><mml:math id="inf9"><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> of the naive 2-factor bootstrap can likewise be decomposed into three additive terms (Online Methods, Estimating the uncertainty of our model-performance estimates), corresponding to subject sampling, condition sampling, and the interaction and/or noise. However, in the naive 2-factor bootstrap estimate <inline-formula><mml:math id="inf10"><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, the independent noise contribution enters not only its own term, but also the two others. Thus, the original bootstrap estimate contains the noise variance component three times instead of once:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This problem can be understood by considering the 1-factor bootstraps, which also contain the independent noise component although it has not been added explicitly:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>When we bootstrap two factors, this automatic inclusion of the noise component happens three times. We confirmed this by both theory and simulation. The overestimate of the variance renders the naive 2-factor bootstrap conservative and not optimally powerful.</p><p>To correct the variance estimate, we introduce a novel corrected 2-factor bootstrap procedure to estimate the variance: We first compute the 1-factor bootstrap variance estimates <inline-formula><mml:math id="inf11"><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf12"><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>. We also compute the naive 2-factor bootstrap estimate <inline-formula><mml:math id="inf13"><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>. We can then linearly combine the variances from these three bootstraps to cancel the surplus contribution from the measurement noise. This procedures yields a corrected 2-factor bootstrap estimate <inline-formula><mml:math id="inf14"><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> that has approximately the right expected value:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mn>2</mml:mn><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The approximations in these equations are due to <inline-formula><mml:math id="inf15"><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:math></inline-formula> factors that apply to the individual terms. We give the exact formulae including these factors in the methods section (Estimating the uncertainty of our model-performance estimates). We show in multiple simulations that this estimate approximates the correct variance better than the uncorrected 2-factor bootstrap (Figures 4c and 7c).</p><p>To stabilize the estimator and eliminate the possibility of a negative variance estimate, we bound the estimate from above and below. We use both <inline-formula><mml:math id="inf16"><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf17"><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> as lower bounds for the estimate as the variances they estimate are always smaller than the true variance. As an upper bound, we use <inline-formula><mml:math id="inf18"><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, the naive, conservative estimate. Bounding slightly biases the variance estimate, but reduces its variability and ensures that it is strictly positive.</p></sec><sec id="s2-1-2"><title>Evaluating the performance of flexible models</title><p>We often want to test <italic>flexible models</italic>, that is models that have parameters to be fitted to the brain-activity data. Two elements that often require fitting are weights for the model features and parameters of a measurement model. Feature weighting is required when a model is not meant to specify a priori how prevalent different tuning profiles are in the neural population or in the measured signals. For example, for deep neural network representations to match brain responses well, it is usually necessary to weight the features (e.g. <xref ref-type="bibr" rid="bib114">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib56">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib57">Khaligh-Razavi et al., 2017</xref>; <xref ref-type="bibr" rid="bib101">Storrs et al., 2021</xref>). A flexible measurement model may be necessary to account for the process of measurement, which may subsample, average, or distort neural responses. For example, fMRI voxels average the neural activity locally, which can be modeled with a parameter for the local averaging range, and electrophysiological recordings may preferentially sample certain classes of neurons (<xref ref-type="bibr" rid="bib70">Kriegeskorte and Diedrichsen, 2016</xref>).</p><p>To avoid the bias in the model-performance estimates that can result from overfitting of flexible models, we use crossvalidation. Crossvalidation means that we partition the dataset into separate test sets. In each fold of crossvalidation, we then fit the models to all but one set and evaluate on the held-out set. Taking the average over the folds yields a single performance estimate. As for bootstrapping, crossvalidation is performed over both conditions and subjects so as to avoid overestimating the generalization performance of flexible models when tested on new subjects and new conditions drawn from the populations of subjects and conditions sampled in the actual experiment (<xref ref-type="fig" rid="fig1">Figure 1b</xref>).</p><p>Because the RDM for the test set must contain multiple values to allow a sensible comparison, the smallest possible number of conditions to perform crossvalidation is 6, which would yield three test conditions for twofold crossvalidation. For small numbers of conditions, we use twofolds. We use threefolds for ≥12 conditions, fourfolds for ≥24 conditions, and fivefolds for ≥40 conditions. These numbers seem to work reasonably well, but were chosen ad hoc.</p><p>To estimate our uncertainty about the crossvalidated model performances, we use the same bootstrap methods as for fixed models. To do so, we need to perform crossvalidation on each bootstrap sample. We call this procedure bootstrap-wrapped crossvalidation.</p><p>In any crossvalidation, different ways to partition the data into test sets lead to different overall evaluations of the models. When we partition the conditions set into disjoint test sets in RSA, this effect is particularly strong, because dissimilarities between conditions in separate test sets do not contribute to the evaluation in any fold. The variance in the evaluations created by this random assignment is generated by our analysis and would vanish if we performed repeated cycles of crossvalidation with all possible partitionings of the conditions set into test sets. Unfortunately, such exhaustive crossvalidation will usually be prohibitively expensive in terms of computation time, especially in bootstrap-wrapped crossvalidation.</p><p>We can estimate the variance without this surplus by sampling <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> different randomly chosen partitionings of the conditions set into crossvalidation test sets for each bootstrap sample. Each of the <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> partitionings into <inline-formula><mml:math id="inf21"><mml:mi>k</mml:mi></mml:math></inline-formula> subsets defines a complete cycle of <inline-formula><mml:math id="inf22"><mml:mi>k</mml:mi></mml:math></inline-formula>-fold crossvalidation. The bootstrap-wrapped crossvalidation estimate of the variance of the model-performance estimates with <inline-formula><mml:math id="inf23"><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> crossvalidation cycles will be larger than the variance <inline-formula><mml:math id="inf24"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> of the exact mean performance over all possible partitionings of a dataset. When we assume that the variance <inline-formula><mml:math id="inf25"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> of randomly chosen partitionings around the mean is equal for each bootstrap sample, the overall variance <inline-formula><mml:math id="inf26"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>When we have more than one cycle of crossvalidation for each bootstrap sample, it is straightforward to compute an estimate for the variance we would have gotten if we had drawn only a single partitioning <inline-formula><mml:math id="inf27"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>. We can simply use only the <inline-formula><mml:math id="inf28"><mml:mi>i</mml:mi></mml:math></inline-formula> th partitioning for each bootstrap to estimate the variance and average these estimates. Using these two variance estimates for 1 and <inline-formula><mml:math id="inf29"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> partitionings, we can simply solve for the variance contributions of the random partitioning and of the bootstrap:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Thus, we can directly compute an estimate of the variance we expect for exhaustive crossvalidation from two or more crossvalidation cycles using random partitionings for each bootstrap sample. The repetition across bootstrap samples enables a stable estimate even for <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. The average estimate is independent of <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). We could invest computation in increasing either the number of bootstrap samples or the number of crossvalidation cycles per bootstrap sample. Our simulations show that the reliability of the bootstrap estimate of the variance of the model-performance estimate improves more when we increase the number of bootstrap samples than when we increase the number of crossvalidation cycles per bootstrap sample (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). Thus, we recommend using only two crossvalidation cycles per bootstrap sample.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Correction for variance caused by crossvalidation.</title><p>(<bold>a</bold>) Unbiased estimates of the variance of model-performance estimates (dashed line) require either many crossvalidation cycles (light blue dots) or the proposed correction formula (back dots). Each model in each simulated dataset contributes one dot to each point cloud in this plot, corresponding to the average estimated variance across 100 repeated analyses. All variance estimates of a model are divisively normalized by the average corrected variance estimate for this model over all numbers of crossvalidation cycles for the dataset. For many crossvalidation cycles, the uncorrected and corrected estimates converge, but the correction formula yields this value even when we use only two crossvalidation cycles. (<bold>b</bold>) Reliability of the corrected bootstrap variance estimate across multiple estimations on the same dataset, comparing the use of more crossvalidation cycles per bootstrap sample (gray,  2, 4, 8, 16, 32 crossvalidations at 1000 bootstrap samples) to using more bootstrap samples (black, 1000, 2000, 4000, 8000, 16,000 bootstrap samples with 2 crossvalidation cycles per sample). The horizontal axis represents the total number of crossvalidation cycles (number of cycles per bootstrap × number of bootstraps). More bootstrap samples are more efficient at stabilizing our bootstrap estimates of the variance of model-performance estimates. Increasing the number of bootstraps decreases the variance roughly at the <inline-formula><mml:math id="inf32"><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:math></inline-formula> rate expected for sampling approximations indicated by the dashed line.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82566-fig2-v1.tif"/></fig><p>This crossvalidation approach provides model-performance estimates that are not biased by overfitting of flexible models to either subjects or conditions. Fixed and flexible models with different numbers of parameters can be robustly compared with generalization over conditions and/or subjects. The method can handle any model that can be fitted efficiently enough (for the types of flexible models we actually implemented, see Methods, Flexible models).</p></sec></sec><sec id="s2-2"><title>Validation of the statistical inference methods</title><p>We validate the inference methods using simulations, functional MRI data, and neural data. First, we establish that the statistical tests for model comparison are valid, controlling the false-positive rate at the nominal level. This requires simulating data under the null hypothesis, where two models that predict distinct RDMs are exactly equal in their RDM prediction accuracy. We use a matrix-normal model to simulate this null scenario for model comparison. Second, we show that the estimates of our uncertainty about model performance correctly capture the true variability for different generalization schemes in more realistic simulated scenarios based on neural network models. In these simulations, we cannot simulate the null hypothesis of two models that predict the representational geometry equally accurately. We also use these more realistic simulations to evaluate the power afforded by different RDM comparators. Third, we validate the inference procedure for flexible models, confirming that our bootstrap-wrapped crossvalidation scheme correctly accounts for the overfitting of flexible models. Fourth, we validate the methods using real data, acquired with functional MRI in humans and calcium imaging in mice.</p><sec id="s2-2-1"><title>Validity of inferential model comparisons</title><p>A frequentist test is valid when the rate of false positives (i.e. the rate of positive results when the null hypothesis is true) does not exceed the specified error rate <italic>α</italic> (e.g. 5%). Here, we check the validity of model-comparative inference, where the null hypothesis is that the two models perform equally well at explaining the representational geometry. We simulate scenarios where two models predict distinct geometries, but perform equally well on average at predicting the true representational geometry.</p><p>To simulate situations where two different models perform equally well, we generated condition-response matrices (containing an activity level for each combination of condition and response channel) by sampling from matrix-normal density models. A matrix-normal distribution over matrices yields matrices with normally distributed cells whose covariance is separable into a covariance matrix across rows and one across columns. In our case, rows correspond to the experimental conditions (e.g. stimuli) and the columns correspond to measurement channels (e.g. neurons or voxels). For matrix-normal data, the covariance across conditions captures the similarity among condition-related response patterns and determines the expected squared Euclidean-distance RDM (<xref ref-type="bibr" rid="bib29">Diedrichsen and Kriegeskorte, 2017</xref>). The covariance among channels only scales the covariance of the distance estimates. This relationship enables us to generate matrix-normal data for arbitrary choices of the expected squared Euclidean-distance RDM. To model the null hypothesis, we choose two models that predict distinct RDMs and generate data, such that the expected data RDM has equal Pearson correlation to both model RDMs (results in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>; details in Appendix 1).</p><p>We first evaluated the bootstrap in the scenario, where the goal is to generalize across subjects only. All model-comparative subject-only bootstrap tests were found to be valid (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). Inflated false-positive rates were observed for subject-only bootstrap tests only when using a small sample of subjects (&lt;20). For a small number of samples, bootstrapping is known to produce underestimates of the variance by a factor <inline-formula><mml:math id="inf33"><mml:mfrac><mml:mi>n</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula> for <inline-formula><mml:math id="inf34"><mml:mi>n</mml:mi></mml:math></inline-formula> samples (e.g. <xref ref-type="bibr" rid="bib35">Efron and Tibshirani, 1994</xref>, chapter 5.3). In this scenario, we recommend using a <inline-formula><mml:math id="inf35"><mml:mi>t</mml:mi></mml:math></inline-formula>-test across subjects, which is more computationally efficient and more accurate than bootstrap methods for small numbers of subjects.</p><p>Next, we tested bootstrapping for generalization to new conditions. In this scenario, the bootstrap methods were all conservative, showing false-positive rates substantially below 5% (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). This is expected, because we did not include any random selection of conditions in our data simulation, but enforced the <italic>H</italic><sub>0</sub> exactly for the measured conditions.</p><p>To assess how problematic it is to choose an inference method that ignores the variance due to condition sampling, we ran a simulation in which we sampled the conditions from a large pool. We generated two models that perform equally well on 1000 conditions using matrix-normal sampling and then sampled a smaller set of these conditions for the simulated experiment. In these simulations, all techniques that only take subjects into account as a random factor fail catastrophically (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>), with false-positive rates growing with the number of simulated subjects and reaching 60% at 40 simulated subjects. In contrast, our bootstrap tests that include condition sampling all remain valid, including the uncorrected 2-factor bootstrap and our new corrected 2-factor bootstrap with false-positive rates below the nominal 5%. However, the uncorrected 2-factor bootstrap was extremely conservative.</p><p>We also validated the tests against chance performance, where a single model is tested and the null hypothesis is that its performance is at chance level. To do so, we performed similar matrix-normal data simulations, evaluating a model that predicts a specific randomly sampled RDM on matrix-normal data consistent with an independently sampled random expected data RDM. Results show that a <inline-formula><mml:math id="inf36"><mml:mi>t</mml:mi></mml:math></inline-formula>-test across subjects as well as the bootstrap <inline-formula><mml:math id="inf37"><mml:mi>t</mml:mi></mml:math></inline-formula>-test approaches provide valid inference (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>, top row). The subject <inline-formula><mml:math id="inf38"><mml:mi>t</mml:mi></mml:math></inline-formula>-test and the corrected 2-factor bootstrap <inline-formula><mml:math id="inf39"><mml:mi>t</mml:mi></mml:math></inline-formula>-test avoid overly conservative false-positive rates.</p><p>We conclude that the tests are valid in these simple simulated <italic>H</italic><sub>0</sub> scenarios, where we are able to estimate the false-positive rate. In more realistic simulations using neural network models and real data, we can no longer simulate distinct models that predict the data RDM equally well. We therefore restrict ourselves to evaluating our bootstrap estimate of the variance of model-performance estimates, assuming that the false-positive rates are adequately controlled when we use an accurate variance estimate.</p></sec><sec id="s2-2-2"><title>Criteria for evaluation of inference procedures</title><p>To evaluate alternative inference procedures, we perform simulations that reveal (1) whether the estimates of the uncertainty of the model-performance estimates are accurate (ensuring the validity of the inferences), and (2) how sensitive different model comparison methods are to subtle differences between models (determining the power of the inferences). To measure whether our bootstrap methods correctly estimate the uncertainty of the model-performance estimates, we compute the relative uncertainty (RU). The RU is the standard deviation of the bootstrap distribution of model-performance estimates <inline-formula><mml:math id="inf40"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> divided by the true standard deviation of model-performance estimates <inline-formula><mml:math id="inf41"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as observed over repeated simulations:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>RU</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:msqrt><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf42"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is the variance estimator of the bootstrap in simulated dataset <inline-formula><mml:math id="inf43"><mml:mi>i</mml:mi></mml:math></inline-formula> of the <inline-formula><mml:math id="inf44"><mml:mi>N</mml:mi></mml:math></inline-formula> simulations. Ideally, we would like the bootstrap-estimated variance to match the true variance such that the RU is 1.</p><p>To measure how sensitive our analysis is to differences in model performance (e.g. comparing layers of a deep neural network), we define the model discriminability as a signal-to-noise ratio (SNR). The signal is the magnitude of model-performance differences, which is measured as the variance across models of their average of performance estimates across simulations. The noise is the nuisance variation, which includes subject and condition sample variation along with measurement noise. The noise is measured as the average across models of the variance of performance estimates across simulations. This results in the following formula, in which <inline-formula><mml:math id="inf45"><mml:msub><mml:mi>Perf</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the performance of model <inline-formula><mml:math id="inf46"><mml:mi>m</mml:mi></mml:math></inline-formula> of <inline-formula><mml:math id="inf47"><mml:mi>M</mml:mi></mml:math></inline-formula> in repetition <inline-formula><mml:math id="inf48"><mml:mi>i</mml:mi></mml:math></inline-formula> of <inline-formula><mml:math id="inf49"><mml:mi>N</mml:mi></mml:math></inline-formula> repetitions of the simulation:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>SNR</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>Var</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>Perf</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msub><mml:mi>Var</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Perf</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>A higher SNR indicates greater sensitivity to differences in model performance: differences between models are larger relative to the variation of model-performance estimates over repeated simulations. Note that this measure does not depend on the accuracy of the bootstrap because the bootstrap estimates of the variances do not enter this statistic. The SNR exclusively measures how large differences between models are compared to the level of nuisance variation we simulate, which may include random sampling of conditions, subjects, or both (in addition to measurement noise).</p></sec><sec id="s2-2-3"><title>Validity of generalization to new subjects and conditions</title><p>To test whether our inference methods correctly generalize to new subjects and conditions, we performed a simulation that includes random sampling of both subjects and conditions (<xref ref-type="fig" rid="fig3">Figure 3</xref>). We used the internal representations of the deep convolutional neural network model AlexNet (<xref ref-type="bibr" rid="bib75">Krizhevsky et al., 2012</xref>) to generate fMRI-like simulated data. In each simulated scenario, one of the layers of AlexNet served as the true (data-generating) model, while all layers were considered as candidate models in the inferential model comparisons. We simulated true voxel responses as local averages of the activities of close-by units in the feature maps of layers of the model. The response of each simulated voxel was a local average of unit responses, weighted according to a 2D Gaussian kernel over the locations of the feature map multiplied by a vector of nonnegative random weights (drawn uniformly from the unit interval) across the features. We then simulated hemodynamic-response timecourses and added measurement noise. The covariance structure of the noise was determined by the overlap of the simulated voxels’ averaging regions over space and a first-order autoregressive model over time. The simulated data were subjected to a standard general linear model (GLM) analysis to estimate the condition-response matrix. Variation over conditions was generated by using randomly sampled natural images from ecoset (<xref ref-type="bibr" rid="bib80">Mehrer et al., 2017</xref>) as input to the AlexNet model. Variation over subjects was generated by randomly choosing a new location and a new vector of feature weights for each voxel of a new simulated subject.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Illustration of the deep-neural-network-based simulations for functional magnetic resonance imaging (fMRI)-like data.</title><p>The aim of the analyses was always to infer which layer of AlexNet the simulation was based on. (<bold>a</bold>) Stimuli are chosen randomly from ecoset (<xref ref-type="bibr" rid="bib80">Mehrer et al., 2017</xref>) and we simulate a simple rapid event-related experimental design. (<bold>b</bold>) ‘True’ average response per voxel to a stimulus are based on local averages of the internal representations of AlexNet. To simulate the response of a voxel to a stimulus we choose a (<italic>x</italic>,<italic>y</italic>)-position uniformly randomly and take a weighted average of the activities around that location. As weights we choose a Gaussian in space and independently draw a weight per feature between 0 and 1. (<bold>c</bold>) To generate a simulated voxel timecourse we generate the undistorted timecourses of voxel activities, convolve them with a standard hemodynamic-response function and add temporally correlated normal noise. (<bold>d</bold>) To estimate the response of a voxel to a stimulus we estimate a standard general linear model (GLM) to arrive at a noisy estimate of the true channel responses we started with in C. (<bold>e</bold>) From the estimated channel responses we compute the stimulus by stimulus dissimilarity matrices. These dissimilarity matrices can then be compared to the dissimilarity matrices computed based on the full deep neural network representations from the different layers.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82566-fig3-v1.tif"/></fig><p>We simulated <italic>N</italic> = 100 datasets for each parameter setting to estimate how variable the model-performance estimates truly are. In analysis, we must estimate our uncertainty about model performance from a single dataset. To estimate how accurate these estimates were, we compared the uncertainty estimates used by different inference procedures (including different bootstrap methods) to the true variability. This comparison is a enables us to validate our inference despite the fact that we cannot compute false-positive rates of the model comparison tests. Our neural-network-based simulations do not contain situations that correspond to the <italic>H</italic><sub>0</sub> of two different models with equal performance, which would require that the data-generating neural network layer predicts an RDM equally similar to those predicted by two other model layers. As expected, the rate of erroneously finding an alternative model outperforming the true data-generating model was very low (not shown) whenever the type of bootstrap matches the simulated level of generalization because the true layer has a higher average performance than the other models. At the 5% uncorrected significance level, the proportion of cases where any other layer performed significantly better than the true (data-generating) layer was only 1.524%. This rate reflects the differences between the layers of AlexNet, the simulated variability due to subject, stimulus, and voxel sampling, the simulated noise level, and the number of layers. Tests against the best other layer (chosen based on all data) significantly favor this other layer in only 0.694% of cases. Multiple-comparison correction would reduce these model-selection error rates even further.</p><p>To test generalization to either new conditions or new subjects (but not both simultaneously), we kept the other dimension constant. When simulating condition sampling, the true variance across conditions is accurately estimated for 40 or more conditions (<xref ref-type="fig" rid="fig4">Figure 4a</xref>) and is overestimated by 1-factor bootstrap resampling of conditions (rendering the inference conservative) when we have less than about 40 conditions (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). When simulating subject sampling, the true variance across subjects is accurately estimated for 20 or more subjects (<xref ref-type="fig" rid="fig4">Figure 4b</xref>) and is underestimated by 1-factor bootstrap resampling of subjects (invalidating the inference) when we have very few subjects (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). This downward bias corresponds to the <inline-formula><mml:math id="inf50"><mml:mfrac><mml:mi>n</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula> factor between the sample variance and the unbiased estimate for the population variance. Our implementation in the RSA toolbox uses this factor to correct the variance estimate.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Results of the deep-neural-network-based simulations.</title><p>(<bold>a–c</bold>) Relative uncertainty, that is the bootstrap estimate of the standard deviation of model-performance estimates divided by the true standard deviation over repeated simulations. Dashed line and gray box indicate the expected value and standard deviation due to the number of simulations per condition. (<bold>a</bold>) Bootstrap resampling of conditions when repeated simulations use random samples of conditions and a fixed set of subjects. (<bold>b</bold>) Bootstrap resampling of subjects when simulations use random samples of subjects (simulated voxel placements) and a fixed set of conditions. (<bold>c</bold>) Direct comparison of the uncorrected and corrected 2-factor bootstraps (see Estimating the uncertainty of our model-performance estimates for details) for simulations that varied both conditions and subjects. (<bold>d–h</bold>) Signal-to-noise ratio (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref>), a measure of sensitivity to differences in model performance, for the different inference procedures and simulated scenarios. Infinite voxel averaging range refers to voxels averaging across the whole feature map. All error bars indicate standard deviations across simulation types that fall into the category.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82566-fig4-v1.tif"/></fig><p>To test our corrected 2-factor bootstrap method’s ability to generalize to new subjects and new conditions simultaneously, we simulated sampling of both conditions (stimuli) and subjects in our simulations. The corrected 2-factor bootstrap estimates the overall variation caused by random sampling of subjects and conditions and by measurement noise much more accurately than the naive 2-factor bootstrap (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). Cases where an incorrect model (not the data-generating model) significantly outperformed the true model occurred in only 0.3% of simulations with the corrected 2-factor bootstrap, even without any multiple-comparison correction. This proportion would be larger if the alternative models performed more similar to the true model than simulated here. The RSA toolbox adjusts for multiple comparisons, controlling either the familywise error rate or the false-discovery rate across all pairwise model comparisons.</p><p>Overall, we found that the new more powerful corrected 2-factor bootstrap method yields accurate estimates of the variance across the simulated populations of subjects and conditions when the dataset is large enough (≥20 subjects, ≥40 conditions) and the type of bootstrap matches the population sampling simulated (subject, condition, or both).</p><p>The model discriminability (SNR) increases monotonically with the number of measurements, affording greater power for model-comparative inference. Model discriminability increases with the amount of data according to a power law (straight line in log–log plot; <xref ref-type="fig" rid="fig4">Figure 4d–f</xref>). Such a relationship holds whether we increase the number of conditions, the number of subjects, or the number of repetitions per condition. This result is expected and validates the SNR as an indicator of model-comparison power. In general, increasing the number of measurements helps most for the factor that causes most variability of the performance estimates, rendering generalization harder. For example, in our deep-neural-network-based simulations, the variability over subjects is smaller than the variability across conditions (<xref ref-type="fig" rid="fig4">Figure 4g</xref>). In this simulation, it thus increases statistical power more to collect data for more conditions. When there is more variability across subjects, the opposite is expected to hold. An intermediate voxel size (Gaussian kernel width) yielded the highest model-performance discriminability as measured by the SNR (<xref ref-type="fig" rid="fig4">Figure 4h</xref>, see Appendix 5 for more discussion on this topic).</p></sec><sec id="s2-2-4"><title>Validity of inference on flexible models</title><p>To validate inferential model comparisons involving flexible models, we made a variant of the deep neural network simulation in which we do not assume to know how voxels average local neural responses. As the simulated ground truth, we set the spatial weights for each voxel to a Gaussian with a standard deviation of 5% of the image size (full width at half maximum FWHM ≈ 11.77%) and randomly weighted the feature maps (with weights drawn independently for each voxel and feature map uniformly at random from the unit interval; details in Methods, Neural-network-based simulation).</p><p>We then used models that a researcher could generate without knowing the ground truth of how voxels average local features. As building blocks for the models, we computed RDMs for different voxel averaging pool sizes and for different methods to deal with averaging across feature maps. To capture voxel averaging across retinotopic locations, we smoothed the feature maps with Gaussians of different sizes. To capture voxel averaging across feature maps, we (1) generated RDMs computed after taking the average across feature maps at each location (avg), (2) computed the expected RDM for the weight sampling implemented in the simulation (weighted), or (3) computed RDMs without any feature-map averaging (full).</p><p>We combined these building blocks into two types of flexible model: <italic>selection models</italic> and <italic>nonnegative linear-combination models</italic>. In a selection model, fitting is implemented as selection of the best among a finite set of RDMs. Here we defined one selection model for each method of combining the feature maps. Each selection model contained RDMs computed for different sizes of the local averaging pool. In linear-combination models, fitting consists in finding nonnegative weights for a set of basis RDMs, so as to maximize RDM prediction accuracy. The RDMs contain estimates of the squared Mahalanobis distances, which sum across sets of tuned neurons that jointly form a population code. As component RDMs, we chose the four extreme cases of RDM generation: no pooling across space or averaging across the whole image, each paired with either ‘full’ or ‘avg’ treatment of the feature maps. The resulting four-RDM-component linear model approximates the effect of computing the RDM from voxels that reflect the average activity over retinotopic patches of different sizes (<xref ref-type="bibr" rid="bib70">Kriegeskorte and Diedrichsen, 2016</xref>). For the averaging across feature maps, which uses random weights, there is a strong motivation for using a linear model: When the voxel activities are nonnegatively weighted averages of the underlying neurons with the weights drawn independently from the same distribution, the expected squared Euclidean RDM is exactly a linear combination of the RDM computed based on the univariate population-average responses and the RDM based on all neurons (Appendix 4; see also <xref ref-type="bibr" rid="bib16">Carlin and Kriegeskorte, 2017</xref>). For comparison, we also included fixed RDM models, corresponding to component RDMs of the fitted models.</p><p>We found that our bootstrap-wrapped crossvalidation (corrected 2-factor bootstrap with adjustment for excess crossvaldation variance) yielded accurate estimates of the uncertainty. The relative uncertainties were close to 1 (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). The model-performance discriminability (SNR) was primarily determined by how accurately the different models were able to recreate the true measurement model (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). The highest SNRs were achieved when the assumed model matched exactly (weighted feature treatment and voxel size 0.05), but the model variants which allowed for some fitting still yield high SNRs. Analyses that take the averaging across space and features into account yielded the highest average model performance for the true model. In contrast, analyses that ignore averaging over space or features (the full feature set selection model and some of the fixed models) not only lead to lower SNRs (as seen in <xref ref-type="fig" rid="fig5">Figure 5c</xref>), but also systematically selected the wrong layer, because a higher average performance was achieved by a different layer than the one we used for generating the data (not shown).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Validation of flexible model tests using bootstrap crossvalidation.</title><p>(<bold>a</bold>) MDS arrangement of the representational dissimilarity matrices (RDMs) for one simulated dataset. Colored circles show the predictions based on one correct and one wrong layer changing the voxel averaging region and the treatment of features (‘full’, ‘weighted’, and ‘avg’, as described in the text). Fixed models correspond to single choice of model RDM for each layer. Selection models select the best fitting voxel size from the RDMs presented in one color (or two for ‘both’). Crosses mark the four components of the linear model for Layer 2. The small black dots represent simulated subject RDMs without functional magnetic resonance imaging (fMRI) noise. (<bold>b</bold>) Histogram of relative uncertainties <inline-formula><mml:math id="inf51"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, showing that the bootstrap-wrapped crossvalidation accurately estimates the variance of the performance estimates across many different inference scenarios. (<bold>c</bold>) Model discriminability as signal-to-noise ratios for different model types.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82566-fig5-v1.tif"/></fig><p>We conclude that when the true voxel sampling is unknown, flexible models are needed to account for voxel sampling, so as to enable us recover the underlying data-generating computational model with our model-comparative inference. Fixed models based on incorrect assumptions about the voxel sampling can lead to low model-performance discriminability (SNR) and even to incorrect inferences as to which model is the true model.</p></sec><sec id="s2-2-5"><title>Validation with functional MRI data</title><p>The simulations presented so far validated all statistical inference procedures, but may not capture all aspects of the structure of real measurements of brain activity. To test our methods under realistic conditions, we used real human fMRI (this section) and mouse calcium-imaging data (next section). We resampled data from a large openly available fMRI experiment in which humans viewed pictures from ImageNet (<xref ref-type="bibr" rid="bib48">Horikawa and Kamitani, 2017</xref>). These data contain various noise sources, individual differences, signal shapes, and distributions that are difficult to simulate accurately without using measured data. We therefore implemented a data-based simulation to create realistic synthetic data, whose ground-truth RDM we knew (<xref ref-type="fig" rid="fig6">Figure 6</xref>). By subsampling from this dataset, we generated smaller datasets to test inference with bootstrapping over conditions. We used the entire dataset as a stand-in for the population a researcher might wish to generalize to. For each cortical area, we computed the mean RDM using all data (all runs and subjects). Each area’s mean RDM served as a ground-truth RDM for datasets sampled from that area and as a model RDM for datasets sampled from all areas. The model comparison we attempted aims to recover which cortical area a dataset was subsampled from. The simulation enables us to check whether our uncertainty estimates are correct for model-performance estimates based on real data.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Functional magnetic resonance imaging (fMRI)-data-based simulation.</title><p>(<bold>a</bold>) These simulations are based on a dataset of neural recordings for 50 stimuli in 5 human observers (<xref ref-type="bibr" rid="bib48">Horikawa and Kamitani, 2017</xref>), which were each shown 35 times. To extract stimulus responses from these data we perform two general linear model (GLM) steps as in the original publication. (<bold>b</bold>) In the first step, we regress out diverse noise estimators (provided by fMRIprep) from pooled fMRI runs. (<bold>c</bold>) We then apply a second GLM separately on each run to extract the stimulus responses. (<bold>d</bold>) We then extract regions of interest (ROIs) based on an atlas (<xref ref-type="bibr" rid="bib41">Glasser et al., 2016</xref>), randomly chose differently sized subsets of runs resp. stimuli to enter further analyses. To simulate realistic noise, we estimate an AR(2) model on the second GLM’s residuals, permute and filter them to keep their original autocorrelation structure, and finally scale them by the factors 0.1, 1, and 10. To generate simulated timecourses, we add these altered residuals to the GLM prediction. We then rerun the second GLM on the simulated data and use the Beta-coefficient maps for following steps. (<bold>e</bold>) Finally, we compute crossnobis representational dissimilarity matrices (RDMs) and perform RSA based on the overall RDM across all subjects. (<bold>f</bold>) Results of the simulations, separately for each noise scaling factor. The signal-to-noise ratio shows the same increase as for our abstract simulation. (<bold>g</bold>) The relative uncertainty converges to 1 for increasing stimulus numbers. Error bars indicate standard deviations across different simulation types.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82566-fig6-v1.tif"/></fig><p>We varied the strength of noise, the number of runs, and the number of conditions (i.e. viewed images). We did not vary the number of subjects because the original dataset contains only five subjects, which precludes informative resampling of subjects. To increase the variability of the resampled datasets beyond sampling from the 35 measurement runs and to vary the noise strength, we created new voxel timecourses for each sampled run while preserving the spatial structure and serial autocorrelation of the noise. To achieve this, we estimated a second-order autoregressive model (<inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) separately for each run’s GLM residuals, permuted the AR-model’s residuals and added the results to the GLM’s predicted timecourse (see <xref ref-type="fig" rid="fig6">Figure 6a–e</xref> and Methods, fMRI-data-based simulation for details). We repeated each simulated experiment 24 times and used the RU and the model-performance discriminability (SNR) as our evaluation criteria.</p><p>Results were largely similar to those of the neural-network-based simulations (<xref ref-type="fig" rid="fig6">Figure 6f, g</xref>). For the RU, which measures the accuracy of our bootstrap variance estimates, we see a convergence toward the expected ratio (dashed line at 1), validating the bootstrap procedure for real fMRI data. For the model-performance discriminability (SNR), we find the same power-law increase with the number of conditions and the number of runs used as data. These results suggest that the regions are discriminable on the basis of their RDMs estimated from fMRI given five subjects’ data when a sufficient number of stimuli (≥30) and runs (≥16) is used.</p></sec><sec id="s2-2-6"><title>Validation with calcium-imaging data</title><p>We can also adjudicate among models of the representational geometry on the basis of direct neural measurements, such as electrophysiological recordings or calcium-imaging data. These measurement modalities have very different statistical properties than fMRI. To test our methods for this kind of data, we performed a resampling simulation based on a large calcium-imaging dataset of responses of mouse visual cortex to natural images (<xref ref-type="bibr" rid="bib28">de Vries et al., 2020</xref>). This dataset contains recordings from six visual cortical areas: primary visual cortex (V1), laterointermediate (LM), posteromedial (PM), rostrolateral (RL), anteromedial (AM), and anterolateral (AL) visual area (<xref ref-type="fig" rid="fig7">Figure 7a</xref>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Results in mice with calcium-imaging data.</title><p>(<bold>a</bold>) Mouse visual cortex areas used for analyses and resampling simulations. (<bold>b</bold>) Overall similarities of the representations in different cortical areas in terms of their representational dissimilarity matrix (RDM) correlations. For each mouse and cortical area (‘data RDM’, vertical), the RDM was correlated with the average RDM across all other mice (‘model RDM’, horizontal), for each other cortical area. We plot the average across mice of the crossvalidated RDM correlation (leave-one-mouse-out crossvalidation). The prominent diagonal shows the replicability across mice and the distinctness between cortical areas of the representational geometries. (<bold>c</bold>) Relative uncertainty for the 2-factor bootstrap methods. The gray box indicates the range of results expected from simulation variability if the bootstrap estimates were perfectly accurate. The correction is clearly advantageous here although the method is still slightly conservative (overestimating the true standard deviation <inline-formula><mml:math id="inf53"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of model-performance evaluations) for small numbers of stimuli. For 40 or more stimuli, the corrected 2-factor bootstrap correctly estimates the variance of model-performance evaluations. (<bold>d</bold>) Signal-to-noise ratio validation: The signal-to-noise ratio (SNR) grows with the number of cells per subject and the number of repeats per stimulus. (<bold>e</bold>) Signal-to-noise ratio for different noise covariance estimates. Taking a diagonal covariance estimate into account, that is normalizing cell responses by their standard deviation is clearly advantageous. The shrinkage estimates provide a marginal improvement over that. (<bold>f</bold>) Signal-to-noise ratio for data sampled from different areas. (<bold>g</bold>) Which measure is optimal for discriminating the models depends on the data-generating area. On average there is an advantage of the cosine similarity over the RDM correlation and of the whitened measures over the unwhitened ones. Error bars indicate standard deviations across different simulation types.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82566-fig7-v1.tif"/><attrib>Image credit: Allen Institute.</attrib></fig><p>As in the previous section, we used the overall mean RDM for each area as a ground-truth model and subsampled the data to create simulated datasets for which we know the ground-truth RDM. We used different numbers of stimulus repetitions, neurons, mice, and stimuli to vary the amount of information afforded by each simulated dataset. We used the crossnobis estimator of representational dissimilarity for all analyses here. We repeated each simulated experiment 100 times and computed the RU to assess the correctness of our bootstrap uncertainty estimates and the model-discriminability SNR to determine which noise covariance estimators and RDM comparators afford most sensitivity to model differences.</p><p>We analyzed the overall discriminability of the brain areas (<xref ref-type="fig" rid="fig7">Figure 7b</xref>). Although cortical areas vary in the reliability of the estimated RDMs, they can be discriminated reliably when using all data. We used the RU to assess whether our bootstrap variance estimates are correct for these data (<xref ref-type="fig" rid="fig7">Figure 7c</xref>). We resampled all factors (subjects, stimuli, runs, and cells) to generate simulted datasets. Correspondingly, the analysis used bootstrapping over both subjects and stimuli. We observed correct variance estimates for the corrected 2-factor bootstrap. The uncorrected 2-factor bootstrap was conservative, substantially overestimating the true variance.</p><p>To understand how the model-comparative power depends on experimental parameters and analysis choices, we analyzed the model-discriminability SNR. We found that more subjects, more stimuli, more runs, and more cells all increased the SNR just as in our fMRI and neural-network-based simulations (<xref ref-type="fig" rid="fig7">Figure 7d</xref>). Furthermore, we find that taking the noise covariance into account for computing the crossnobis RDMs in the first-level analysis improves the SNR (<xref ref-type="fig" rid="fig7">Figure 7e</xref>). Univariate noise normalization (implemented by using a diagonal noise covariance matrix) is better than no noise normalization. Multivariate noise normalization is slightly better than univariate noise normalization (<xref ref-type="bibr" rid="bib107">Walther et al., 2016</xref>). For multivariate noise normalization, we tested two different shrinkage estimators with different targets: a multiple of the identity and the diagonal matrix of variances. These two variants perform similarly. In addition, we find that different RDM comparators yield the best model discriminability for different cortical areas (<xref ref-type="fig" rid="fig7">Figure 7g</xref>). For some, cosine RDM similarity performs better, for others, Pearson RDM correlation performs better. The whitened RDM comparators are better on average, but there are cases where the unwhitened RDM comparators perform slightly better. Thus, it remains dependent on the concrete experiment (with a particular choice of conditions, tested models and underlying representational geometry), which RDM comparator affords the best power for model comparison (<xref ref-type="bibr" rid="bib31">Diedrichsen et al., 2020</xref>).</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We present new methods for inferential evaluation and comparison of models that predict brain representational geometries. The inference procedures enable generalization to new measurements, new subjects, and new conditions, treat flexible models correctly using crossvalidation, and work for any representational dissimilarity estimator and RDM comparator. For fixed as well as flexible models, our inference methods support all combinations of generalization: to new measurements using the same subjects and conditions, to new subjects, to new conditions, and to both new subjects and new conditions simultaneously. We validated the methods using simulated data as well as calcium-imaging and fMRI data, showing that the inferences are correct. The methods are available as part of an open-source Python toolbox (<ext-link ext-link-type="uri" xlink:href="https://rsatoolbox.readthedocs.io/en/stable/">rsatoolbox.readthedocs.io</ext-link>).</p><sec id="s3-1"><title>Generalizing to new measurements, new subjects, and/or new conditions</title><p>Inferential statistics is about generalization from the experimental random samples to the underlying populations. We must carefully consider the level of generalization, both at the stage of designing our experiments and analyses and at the stage of interpreting the results. The lowest level of inferential generalization is to new measurements. Our conclusions in this scenario are expected to hold only for replications of the experiment in the same animals using the same conditions. Inferential generalization to new subjects may not be possible, for example, in case studies or when the number of animals (e.g. two macaques) is insufficient. Generalization to new conditions is not needed when all conditions relevant to our claims have been sampled. For example, <xref ref-type="bibr" rid="bib36">Ejaz et al., 2015</xref> studied the representational similarity of finger movements in primary motor cortex. All five fingers were sampled in the experiments and there are no other fingers to generalize to. When generalizing to replications with the same subjects and conditions, we need separate data partitions to estimate the variability of the model-performance estimates. We can then use a <inline-formula><mml:math id="inf54"><mml:mi>t</mml:mi></mml:math></inline-formula>-test or rank-sum test to test for significant differences between models.</p><p>If generalization to the population of subjects is desired, we need a sufficiently large sample of subjects. We can then evaluate each model for each subject and use a <inline-formula><mml:math id="inf55"><mml:mi>t</mml:mi></mml:math></inline-formula>-test or rank-sum test, treating the subjects as a random sample from a population. We showed that this method is valid, controlling false-positive rates at their nominal values in our matrix-normal simulations (Methods, Frequentist tests for model evaluation and model comparison). The variance across subjects here is a good estimate of the variance across the population of subjects. However, the interpretation of the results must be restricted to the exact set of experimental conditions used in the experiment.</p><p>We often would like our inferences to generalize to a population of conditions. For example, when evaluating computational models of vision, we are not usually interested in determining which models dominate just for the particular visual stimuli presented in our experiment. We are interested in models that dominate for a population of visual stimuli. Model-comparative inference can generalize to the population of conditions that the experimental conditions were randomly sampled from. The inference requires bootstrapping, because RDM prediction accuracy cannot be assessed for single conditions. We bootstrap-resample the conditions set and evaluate all models on each sample. This procedure correctly estimates our uncertainty about model-performance differences, and <inline-formula><mml:math id="inf56"><mml:mi>t</mml:mi></mml:math></inline-formula>-tests based on the estimated bootstrap variances provide valid frequentist inference.</p><p>If we want to generalize simultaneously across conditions and subjects, then the corrected 2-factor bootstrap approach provides accurate estimates of our uncertainty about model performances. These uncertainty estimates support valid inferential model comparisons, comparisons to the lower bound of the noise ceiling, and tests against chance performance. We expect the results to generalize to new subjects and conditions drawn from the respective populations sampled randomly in the experiment.</p></sec><sec id="s3-2"><title>Inference on fixed and flexible models</title><p>Our performance estimates for flexible models must not be biased by overfitting to measurement noise, subjects, or conditions. To avoid this bias, we use a novel 2-factor crossvalidation scheme that enables us to evaluate models’ predictive accuracy when simultaneously generalizing to new subjects and/or new conditions. The 2-factor crossvalidation is nested in our 2-factor bootstrap procedure for estimating uncertainty. By using two crossvalidation cycles with different data partitionings for each bootstrap sample, we can accurately remove the excess variance introduced by crossvalidation. Our method provides a computationally efficient estimate of the variances and covariances of model-performance estimates for flexible models, which enables us to use a <inline-formula><mml:math id="inf57"><mml:mi>t</mml:mi></mml:math></inline-formula>-test to inferentially compare models to each other, to the lower bound of the noise ceiling, and to chance performance.</p><p>Our methods are fully general in that inference can be performed on any model for which the user provides a fitting and an RDM prediction method. In practice, the complexity of the models is limited by the requirement that we need to fit each model thousands of times in our bootstrap-wrapped crossvalidation scheme. Thus, we need a sufficiently fast and reliable fitting method for the model.</p><p>If fitting the model so often is not feasible or if the data RDMs do not provide sufficient constraints, one solution is to fit all models using a separate set of neural data before the inferential analyses. This approach is appropriate when many parameters are to be fitted, as is the case in nonlinear systems identification approaches as well as linear encoding models (<xref ref-type="bibr" rid="bib111">Wu et al., 2006</xref>), where a large set of neural fitting data is required. All conclusions are then conditional on the fitting data: Inference will generalize to new test data assuming models are fitted on the same fitting data. Our methods support fitting of lower-parametric models as part of the model-comparative inference. When applicable, this approach obviates the need for separate neural data for fitting and supports stronger generalization (not conditional on the neural fitting data).</p></sec><sec id="s3-3"><title>Supported tests and implications of test results</title><p>Our methods enable comparison of a model’s RDM prediction performance (1) against other models, (2) against the noise ceiling, and (3) against chance performance. The first two of these tests are central to the evaluation of models. The test against chance performance is often also reported, but represents a low bar that we should expect most models to pass. In practice, RDM correlations tend to be positive even for very different representations, because physically highly similar stimuli or conditions tend to be similar in all representations. Just like a significant Pearson correlation indicates a dependency, but does not demonstrate that the dependency is linear, a significant RDM prediction result indicates the presence of stimulus information, but does not lend strong support to the particular model. We should resist interpreting significant prediction performance per se as evidence for a particular model (the single-model-significance fallacy; <xref ref-type="bibr" rid="bib73">Kriegeskorte and Douglas, 2019b</xref>). Theoretical progress instead requires that each model be compared to alternative models and to the noise ceiling. An additional point to note is that the interpretation of chance performance, where the RDM comparator equals 0, depends on the chosen RDM comparator, differing, for example, between the Pearson correlation coefficient and the cosine similarity (<xref ref-type="bibr" rid="bib31">Diedrichsen et al., 2020</xref>).</p><p>RDM comparators like the Pearson correlation and the cosine similarity are related to the distance correlation (<xref ref-type="bibr" rid="bib103">Székely et al., 2007</xref>), a general indicator of mutual information. Like a significant distance correlation, a significant RDM correlation mainly demonstrates that there is some mutual information between the brain region in question and the model representation. For a visual representation, for example, all that is required is for the two representations to contain some shared information about the input images. In contrast to the distance correlation (and other nonnegative estimates of mutual information), however, negative RDM correlations can occur, indicating simply that pairs of stimuli close in one representation tend to be far in the other and vice versa. For any RDM, there is even a valid perfectly anti-correlated RDM (Pearson <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), which can be found by flipping the sign of all dissimilarities and adding a large enough value to make the RDM conform to the triangle inequality (which ensures the existence of an embedding of points that is consistent with the anti-correlated RDM). The existence of valid negative RDM correlations is important to the inferential methods presented here because it is required for our assumption of symmetric (<inline-formula><mml:math id="inf59"><mml:mi>t</mml:mi></mml:math></inline-formula>-)distributions around the true RDM correlation.</p><p>Omnibus tests for the presence of information about the experimental conditions in a brain region have been introduced in previous studies (e.g. <xref ref-type="bibr" rid="bib63">Kriegeskorte et al., 2006</xref>; <xref ref-type="bibr" rid="bib5">Allefeld et al., 2016</xref>; <xref ref-type="bibr" rid="bib84">Nili et al., 2020</xref>). Whether stimulus information is present in a region is closely related to the question whether the noise ceiling is significantly larger than 0, indicating RDM replicability. Such tests can sensitively detect small amounts of information in the measured activity patterns and can be helpful to assess whether there is any signal for model comparisons. If we are uncertain whether there is a reliable representational geometry to be explained, we need not bother with model comparisons.</p><p>The question whether an individual dissimilarity is significantly larger than zero is equivalent to the question whether the distinction between the two conditions can be decoded from the brain activity. Decoding analyses can be used for this purpose (<xref ref-type="bibr" rid="bib82">Naselaris et al., 2011</xref>; <xref ref-type="bibr" rid="bib47">Hebart et al., 2014</xref>; <xref ref-type="bibr" rid="bib104">Tong and Pratte, 2012</xref>; <xref ref-type="bibr" rid="bib73">Kriegeskorte and Douglas, 2019b</xref>). Such tests require care because the discriminability of two conditions cannot be systematically negative (<xref ref-type="bibr" rid="bib5">Allefeld et al., 2016</xref>). This is in contrast to comparisons between RDMs, which can be systematically negative (although, as mentioned above, they tend to be positive in practice).</p></sec><sec id="s3-4"><title>How many subjects, conditions, repetitions, and measurement channels?</title><p>Statistical inference gains power when more data are collected along any dimension. More independent measurement channels, more subjects, more conditions, and more repetitions all help. How much data is needed along each of these dimensions depends on the experiment. The most helpful dimension to extend is the one that currently limits generalization. When crossvalidation across repeated measurements is used to eliminate the bias of the distance estimates (as in the crossnobis estimator), using more repetitions brings an additional performance bonus because it reduces the variance increase associated with unbiased estimates (<xref ref-type="bibr" rid="bib31">Diedrichsen et al., 2020</xref>, Appendix 5).</p></sec><sec id="s3-5"><title>Which distance estimator and RDM comparator?</title><p>The statistical inference procedures introduced here work for any choice of representational-distance estimator and RDM comparator. However, the choice of distance estimator and RDM comparator affects the power of model-comparative inference and the meaning of the inferential results.</p><p>For computing the RDM, we tested only variations of the crossnobis (crossvalidated Mahalanobis) distance estimator, as recommended based on earlier research (<xref ref-type="bibr" rid="bib107">Walther et al., 2016</xref>). The crossnobis estimator can use different noise covariance estimates to normalize patterns, such that the noise distribution becomes approximately isotropic. The noise covariance matrix can be the identity (no normalization), diagonal (univariate normalization), or a full estimate (multivariate normalization). Consistent with previous findings (<xref ref-type="bibr" rid="bib107">Walther et al., 2016</xref>; <xref ref-type="bibr" rid="bib91">Ritchie et al., 2021</xref>), our results suggest that univariate noise normalization is always preferable to no normalization, and that multivariate noise normalization using a shrinkage estimate of the noise covariance (<xref ref-type="bibr" rid="bib79">Ledoit and Wolf, 2004</xref>; <xref ref-type="bibr" rid="bib93">Schäfer and Strimmer, 2005</xref>) helps in some circumstances and never hurts model discrimination.</p><p>For evaluating RDM predictions, we can distinguish RDM comparison methods by the scale they assume for the distance estimates: ordinal, interval, or ratio. For ordinal comparisons, the different rank correlation coefficients perform similarly. We recommend <inline-formula><mml:math id="inf60"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> for its computational efficiency and analytically derived noise ceiling. For interval- and ratio-scale comparisons, a more complex pattern emerges. In particular whether cosine similarities (ratio scale) or Pearson correlations (interval scale) work better depends on the structure of the model RDMs to be compared. We recently proposed whitened variants of the cosine similarity and Pearson correlation, which take into account that the distance estimates in an RDM are not independent (<xref ref-type="bibr" rid="bib31">Diedrichsen et al., 2020</xref>). The whitened RDM comparators were more sensitive to subtle differences in model performance when evaluated on fixed models (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). In the simulations based on the calcium-imaging data, whitened RDM comparators still performed better on average, but there were some cortical areas that were easier to identify by using the unwhitened comparison measures.</p></sec><sec id="s3-6"><title>Alternative approaches</title><p>We present a frequentist inference methodology that uses crossvalidation to obtain point estimates of model performance and bootstrapping to estimate our uncertainty about them. Bayesian alternatives deserve consideration. For example, a Bayesian approach has been proposed to alleviate the bias of distance estimates (<xref ref-type="bibr" rid="bib15">Cai et al., 2019</xref>). This Bayesian estimate makes more detailed assumptions about the trial dependencies than our crossvalidated distance estimators, which remove the bias. The Bayesian estimate might be preferable for its higher stability when its assumptions hold and could be used in combination with our model-comparative inference methods. For model comparisons, Bayesian inference is also an interesting alternative to the frequentist methods we discuss here (<xref ref-type="bibr" rid="bib70">Kriegeskorte and Diedrichsen, 2016</xref>). Our whitened RDM comparison methods can be motivated as approximations to the likelihood for a model and we reported recently that they afford similar power as likelihood-based inference with normal assumptions (<xref ref-type="bibr" rid="bib31">Diedrichsen et al., 2020</xref>). Thus, frequentist inference using the whitened RDM comparators is related to Bayesian inference with a uniform prior across models. In the Bayesian framework, generalization to the populations of subjects and conditions would require a model of how RDMs vary across subjects and conditions. We currently do not have such a model. Until such models and Bayesian inference procedures for them are developed, the frequentist methods we present here remain the only method for generalization to the populations of subjects and conditions.</p><p>Another strongly related method for comparing models to data in terms of their geometry is pattern component modeling (<xref ref-type="bibr" rid="bib30">Diedrichsen et al., 2018</xref>), which compares conditions in terms of their covariance over measurement channels instead of their representational dissimilarities. This approach is deeply related to representational similarity analysis (<xref ref-type="bibr" rid="bib29">Diedrichsen and Kriegeskorte, 2017</xref>). Pattern component modeling is somewhat more rigid than RSA as the theory is based on normal distributions, but it has advantages in terms of analytical solutions. In particular, the likelihood of models can be directly evaluated, enabling tests based on the likelihood ratio. Due to the direct evaluation of likelihoods, this framework can be combined with Bayesian inference more easily and recently a variational Bayesian analysis was presented for this model (<xref ref-type="bibr" rid="bib40">Friston et al., 2019</xref>).</p><p>Another powerful approach to inference on brain-computational models is to fit encoding models that predict measured brain-activity data instead of representational geometries (e.g. <xref ref-type="bibr" rid="bib111">Wu et al., 2006</xref>; <xref ref-type="bibr" rid="bib52">Kay et al., 2008</xref>; <xref ref-type="bibr" rid="bib32">Dumoulin and Wandell, 2008</xref>; <xref ref-type="bibr" rid="bib82">Naselaris et al., 2011</xref>; <xref ref-type="bibr" rid="bib108">Wandell and Winawer, 2015</xref>; <xref ref-type="bibr" rid="bib29">Diedrichsen and Kriegeskorte, 2017</xref>; <xref ref-type="bibr" rid="bib13">Cadena et al., 2019a</xref>). This approach was originally developed in the context of low-dimensional models and measurements. When models and measurements are both high dimensional, even a linear encoding model can be severely under-constrained (<xref ref-type="bibr" rid="bib14">Cadena et al., 2019b</xref>; <xref ref-type="bibr" rid="bib62">Kornblith et al., 2019</xref>). As a result, an encoding model requires a combination of substantial fitting data and strong priors on the weights. The predictive model that is being evaluated comprises the encoding model and the priors on its weights (<xref ref-type="bibr" rid="bib29">Diedrichsen and Kriegeskorte, 2017</xref>), which complicates the interpretation of the results (<xref ref-type="bibr" rid="bib14">Cadena et al., 2019b</xref>; <xref ref-type="bibr" rid="bib73">Kriegeskorte and Douglas, 2019b</xref>). Both model performances and the fitted weights can then be highly uncertain and/or dependent on the details of the assumed encoding model. The additional data and assumptions needed to fit complex encoding models motivate the consideration of methods as proposed here that do not require fitting of a high-parametric mapping from model to measured brain activity.</p><p>The generalization challenges that we tackle here for RSA apply equally to encoding models and pattern component modeling. Inferences are often meant to generalize to new subjects and/or experimental conditions. The alternative approaches, in their current implementations, do not yet enable simultaneous generalization to the populations of experimental conditions and subjects. By default pattern component modeling and its Bayesian variants assume a single geometry and thus do not take either subject or condition variability into account. Variability across subjects can be taken into account in a group-level analysis (see e.g. <xref ref-type="bibr" rid="bib30">Diedrichsen et al., 2018</xref>, 2.7.3), but this approach does not account for uncertainty due to the sample of experimental conditions. Encoding models usually follow the machine learning approach with training, validation, and test sets (e.g. <xref ref-type="bibr" rid="bib82">Naselaris et al., 2011</xref>; <xref ref-type="bibr" rid="bib22">Cichy et al., 2019</xref>; <xref ref-type="bibr" rid="bib23">Cichy et al., 2021</xref>). Uncertainty about the model evaluations is either not estimated at all or estimated in a secondary analysis based on the variability across subjects, cells, or conditions. Because these secondary analysis is based solely on the test set, results are conditional on the training and validation sets, and so fall short of generalizing model-comparative inferences to the underlying populations. Note that the bootstrapping and crossvalidation approaches we introduce here are not inherently specific to RSA. These methods could be adapted for estimating the uncertainties about other model evaluation measures such as those provided by pattern component and encoding models.</p></sec><sec id="s3-7"><title>Conclusion</title><p>We present a comprehensive new methodology for inference on models of representational geometries that is more powerful than previous approaches, can handle flexible models, and enables neuroscientists to draw conclusions that generalize to new subjects and conditions. The validity of the methods has been established through extensive simulations and using real neural data. These methods enable neuroscientists working with humans and animals to evaluate complex brain-computational models with measurements of neural population activity. As we enter the age of big models and big data, we hope these methods will help connect computational theory to neuroscientific experiment.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>The methods section for this paper is separated into two parts: First, we describe the RSA analysis pipeline we propose in full. In the second part, we describe the simulation methods we used to test our pipeline for this paper.</p><sec id="s4-1"><title>Full description of the RSA method</title><p>The inference method we describe here represents a new pipeline for representational similarity analysis. Nonetheless, some parts of the analysis appeared in earlier or concurrent publications (<xref ref-type="bibr" rid="bib66">Kriegeskorte et al., 2008b</xref>; <xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref>; <xref ref-type="bibr" rid="bib107">Walther et al., 2016</xref>; <xref ref-type="bibr" rid="bib100">Storrs et al., 2014</xref>). In this section, we describe the whole pipeline, including both new and established procedures, without requiring familiarity with previous papers.</p><sec id="s4-1-1"><title>Computing representational dissimilarity matrices</title><p>The first step of RSA is the computation of the representational dissimilarity matrices. The main question for this step is which dissimilarity measure shall be computed between conditions.</p><p>In the formal mathematical sense, a distance or metric is a function that takes two points from the space as input and computes a real number from them conforming to the following three rules: (1) <italic>Nonnegativity</italic>: The result is larger than or equal to zero, with equality only if the two points are equal. (2) <italic>Symmetry</italic>: The result is the same if the two points are swapped. (3) <italic>Triangle inequality</italic>: The sum of distances from <inline-formula><mml:math id="inf61"><mml:mi>a</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf62"><mml:mi>b</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf63"><mml:mi>b</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf64"><mml:mi>c</mml:mi></mml:math></inline-formula> is less than or equal to the distance from <inline-formula><mml:math id="inf65"><mml:mi>a</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf66"><mml:mi>c</mml:mi></mml:math></inline-formula> for all choices of the three points. We use the term dissimilarity for symmetric measures that may violate (1) and (3). Dropping requirement (3) admits symmetric divergences between probability distributions, for example. Dropping requirement (1) and allowing measures that may return negative values admits unbiased distance estimators (whose distribution is symmetric about 0 when the true distance is 0). We would still like the dissimilarity to be nonnegative in expectation.</p><p>In principle, any dissimilarity measure on the measured representation vectors can be used to quantify the dissimilarities between conditions. Popular choices in the past were the Pearson correlation distance, squared and unsquared Euclidean distances, cosine distance, and linear-decoding-based measures such as the decoding accuracy, the linear-discriminant contrast (LDC, also known as the crossnobis estimator; <xref ref-type="bibr" rid="bib107">Walther et al., 2016</xref>) and the linear-discriminant <inline-formula><mml:math id="inf67"><mml:mi>t</mml:mi></mml:math></inline-formula> value (LD-<inline-formula><mml:math id="inf68"><mml:mi>t</mml:mi></mml:math></inline-formula>; <xref ref-type="bibr" rid="bib64">Kriegeskorte et al., 2007</xref>; <xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref>). Earlier publications comparing different measures of dissimilarity found correlation distances to be less interpretable in terms of condition decodability and continuous crossvalidated decoding measures (LDC, LD-<inline-formula><mml:math id="inf69"><mml:mi>t</mml:mi></mml:math></inline-formula>) to be more sensitive than decoding accuracy (<xref ref-type="bibr" rid="bib107">Walther et al., 2016</xref>).</p><p>How representational dissimilarity is best quantified and inferred from raw data depends on the type of measurements taken. For fMRI for example, it is beneficial to take the noise covariance across voxels into account by computing Mahalanobis distances (<xref ref-type="bibr" rid="bib107">Walther et al., 2016</xref>). For electrophysiological recordings of individual neurons one should take the Poisson nature of the variability into account. One approach is to transform the measured spike rates so as to stabilize their variance (<xref ref-type="bibr" rid="bib72">Kriegeskorte and Diedrichsen, 2019a</xref>). Here, we introduce a KL-divergence dissimilarity measure based on the Poisson distribution (Appendix 2). Representational dissimilarities can also be inferred from behavioral responses, such as speeded categorizations or explicit judgments of properties or pairwise dissimilarities (<xref ref-type="bibr" rid="bib67">Kriegeskorte and Mur, 2012</xref>).</p><p>Two aspects of the computation of dissimilarities warrant further discussion: crossvalidation of dissimilarities and taking noise covariance into account.</p><sec id="s4-1-1-1"><title>Crossvalidated distance estimators</title><p>One important aspect of the first-level analysis is that naive estimates of representational similarity can be severely biased (<xref ref-type="bibr" rid="bib107">Walther et al., 2016</xref>; <xref ref-type="bibr" rid="bib15">Cai et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Diedrichsen et al., 2020</xref>) toward a structure dictated by the structure of the experiment rather than the structure of the representations. This happens because noise in the underlying patterns biases distance estimates upward. When different conditions are measured with different amounts of noise or the measurements between some pairs of conditions are correlated, this bias will be different for different distances introducing other structure into the RDM.</p><p>To avoid this problem, one can use crossvalidated distances, which combine difference estimates from independent measurements like separate runs, such that the dissimilarity estimate is unbiased. Crossvalidation applies to all dissimilarity measures that are defined based on inner products of the differences with themselves (e.g. squared and unsquared Euclidean distances, Mahalanobis distances, Poisson KL divergences). To compute a crossvalidated dissimilarity one computes two estimates of the difference vector from independently measured parts of the data and takes the inner product of these two independent estimates, averaging over different splits into independent data.</p><p>The most commonly used version of crossvalidated distances are crossvalidated Mahalanobis (<italic>Crossnobis</italic>) dissimilarities, which we use througout our simulations in this paper. For <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≥</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> repeated measurements of response patterns <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (for <inline-formula><mml:math id="inf72"><mml:mi>K</mml:mi></mml:math></inline-formula> conditions), the crossnobis estimator <inline-formula><mml:math id="inf73"><mml:msub><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is defined as:<disp-formula id="equ11"> <label>(11)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>d</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>≠</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>for an estimate noise covariance matrix <inline-formula><mml:math id="inf74"><mml:mi mathvariant="normal">Σ</mml:mi></mml:math></inline-formula>.</p><p>Crossnobis dissimilarities seem to be the most reliable dissimilarity estimate for fMRI-like data (<xref ref-type="bibr" rid="bib107">Walther et al., 2016</xref>). As in the non-crossvalidated Mahalanobis distance, the linear transformation of of the response dimensions (using the noise precision matrix <inline-formula><mml:math id="inf75"><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>) improves reliability (<xref ref-type="bibr" rid="bib107">Walther et al., 2016</xref>; <xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref>) and renders the estimates monotonically related to the linear decoding accuracy for each pair of conditions, when a fixed Gaussian error distribution is assumed. Their sampling distributions can be described analytically (<xref ref-type="bibr" rid="bib31">Diedrichsen et al., 2020</xref>).</p><p>For Poisson distributed measurements as for electrophysiological recordings we can also define a crossvalidated dissimilarity based on the KL-divergence as we introduce in Appendix 2.</p></sec><sec id="s4-1-1-2"><title>Noise covariance estimation</title><p>To take the noise covariance into account (in Mahalanobis and Crossnobis dissimilarities) we first need to estimate it. To do so, we can use one of two sources of information: We either estimate the covariance based on the variation of the repeated measurements around their mean or based on the residuals of a first-level analysis which estimated the patterns from the raw data. For fMRI for example, these residuals would be the residuals of the first-level GLM. In either case, we may have relatively little data to estimate a large noise covariance matrix. Making this feasible usually requires regularization. To do so the following methods are available:</p><list list-type="bullet"><list-item><p>When the estimation task is judged to be entirely impossible one can reduce the Mahalanobis and Crossnobis back to the Euclidean and crossvalidated Euclidean distances by using the identity matrix instead.</p></list-item><list-item><p>As a univariate simplification one can estimate a diagonal matrix which only takes the variances of voxels into account.</p></list-item><list-item><p>For estimating a full covariance one may use a shrinkage estimate, which ‘shrinks’ the sample covariance toward a simpler estimate of the covariance like a multiple of the identity or the diagonal of variances (<xref ref-type="bibr" rid="bib79">Ledoit and Wolf, 2004</xref>; <xref ref-type="bibr" rid="bib93">Schäfer and Strimmer, 2005</xref>). The amount of shrinkage used in these methods fortunately can be estimated quite accurately based on the data directly such that these methods do not require parameter adjustment.</p></list-item></list><p>We implemented these different methods. Overall the shrinkage estimates perform best, while the other techniques are equally good in some situations. Using the sample covariance directly is not advisable unless an unusually large amount of data exists for this estimation, in which case the shrinkage estimates converge toward the sample covariance anyway.</p></sec></sec></sec><sec id="s4-2"><title>Comparing RDMs</title><p>The second-level analysis is comparing a measured data RDM (for each subject) to the RDMs predicted by different models. There are various measures to compare RDMs. The right choice depends on the aspects of the data RDM that the models are meant to predict. A strict measure would be the Euclidean distance (or equivalently the mean squared error) between a model RDM and the data RDM. However, we usually cannot predict the absolute magnitude of the distances because the SNR varies between subjects and measurement sessions. Allowing an overall scaling of the RDMs leads to the cosine similarity between RDMs. If we additionally drop the assumption that a predicted difference of 0 corresponds to a measured dissimilarity of 0, we can use a correlation coefficient between RDMs. For the cosine similarity and Pearson correlation between RDMs we recently proposed whitened variants which take the correlations between the different entries of the RDM into account (<xref ref-type="bibr" rid="bib31">Diedrichsen et al., 2020</xref>). Finally, we can drop the assumption of a linear relationship between RDMs by using rank correlations like Kendall’s <inline-formula><mml:math id="inf76"><mml:mi>τ</mml:mi></mml:math></inline-formula> or Spearman’s <inline-formula><mml:math id="inf77"><mml:mi>ρ</mml:mi></mml:math></inline-formula>. For this lowest bar for a relationship Kendall’s <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> or randomized rank breaking for Spearman’s <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> are usually preferred over a standard Spearman’s <inline-formula><mml:math id="inf80"><mml:mi>ρ</mml:mi></mml:math></inline-formula> or Kendall’s <inline-formula><mml:math id="inf81"><mml:msub><mml:mi>τ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf82"><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>, which all favor RDMs with tied ranks (<xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref>). As we discuss below there is a direct formula for the expected Spearman’s <inline-formula><mml:math id="inf83"><mml:mi>ρ</mml:mi></mml:math></inline-formula> under random tiebraking, which we prefer now for computational efficiency reasons.</p></sec><sec id="s4-3"><title>Estimating the uncertainty of our model-performance estimates</title><p>Additional to the point estimate of model performances we want to estimate how certain we should be about our evaluations. In the frequentist framework, this corresponds to an estimate how variable our evaluation results would be if we repeated the experiment. All variances we need can be computed from the covariance matrix of the model-performance estimates. Thus, we keep an estimate of this matrix as our overall uncertainty estimate.</p><sec id="s4-3-1"><title>Subject variance</title><p>The easiest to estimate variance is the variance our results would have if we repeated the experiment with new subjects, but the same conditions, as all our evaluations are simple averages across subjects. Thus, an estimate of the variance can always be obtained by dividing the sample variance over subjects by the number of subjects.</p></sec><sec id="s4-3-2"><title>Bootstrapping conditions</title><p>The dependence of the evaluation on the conditions is more complicated. Thus, we use bootstrapping (<xref ref-type="bibr" rid="bib35">Efron and Tibshirani, 1994</xref>) to estimate the variance we expect over repetitions of the experiment with new conditions but the same subjects. To do so, we sample sets of conditions with replacement from the set of measured conditions and generate the data RDM and the model RDMs for this sample of conditions. The variance of the model performances on these resampled RDMs is then an estimate of the variance over experiments with different stimulus choices. The bootstrap samples of conditions contain repetitions of the same condition. The dissimilarity between a stimulus and itself will be 0 in the data and any model such that every model would correctly predict these self-dissimilarities. Thus, including these self-dissimilarities would bias all model performances upward. To avoid this, we exclude them from the evaluation.</p></sec><sec id="s4-3-3"><title>2-Factor bootstrap</title><p>If we want to estimate the variance for generalization to new subjects and new stimuli simultaneously, we need to use the correction we introduce in the results section (see Estimating the variance of model-performance estimates for generalization to new subjects and conditions). This yields an estimate of the model evaluation (co-)variances as all other methods for variance estimation.</p><p>As we mention in the main text, the exact formulas for the correction contain factors that depend on the number of subjects <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> and conditions <inline-formula><mml:math id="inf85"><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>, respectively. The expected value for the uncorrected 2-factor bootstrap variance <inline-formula><mml:math id="inf86"><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>If the true variances due to the subject is <inline-formula><mml:math id="inf87"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, the one due to the conditions is <inline-formula><mml:math id="inf88"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, and the one due to measurement noise or interaction of the two is <inline-formula><mml:math id="inf89"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>. Correspondingly, the expectations for the two 1-factor bootstraps <inline-formula><mml:math id="inf90"><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf91"><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> are:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Combining these equations an unbiased estimate of the variance <inline-formula><mml:math id="inf92"><mml:msup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> can be obtained:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mn>2</mml:mn><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>As <inline-formula><mml:math id="inf93"><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> grow, the three ratio factors converge to 1, and the result converges to the one given by the simpler formula in the main text (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>).</p></sec><sec id="s4-3-4"><title>Bootstrap-wrapped crossvalidation</title><p>If we employ any flexible models, we should additionally use crossvalidation, which leads to our new bootstrap-wrapped crossvalidation explained in the results section (Evaluating the performance of flexible models).</p></sec></sec><sec id="s4-4"><title>Frequentist tests for model evaluation and model comparison</title><p>Based on the uncertainty estimates, we construct frequentist tests to compare models to each other. The default method is a <inline-formula><mml:math id="inf95"><mml:mi>t</mml:mi></mml:math></inline-formula>-test based on bootstrap-estimated variances. There is a collection of other tests available to compare model performances against each other, to the noise ceiling, or to chance performance.</p><p>Because we base our uncertainty estimates on a bootstrap, there are two types of tests we can use for these comparisons: A percentile test based on the bootstrap samples or a <inline-formula><mml:math id="inf96"><mml:mi>t</mml:mi></mml:math></inline-formula>-test based on the estimated variances.</p><p>For the percentile test, we calculate the bootstrap distribution for the differences and then test by checking whether the difference expected under the <italic>H</italic><sub>0</sub> (usually 0) lies within the simple percentile bootstrap confidence interval. It is possible to generate more exact confidence intervals like bias-corrected and accelerated intervals based on the bootstrap samples, which might result in better tests. In our simulations and experience with natural data, however, model performances tend to be fairly symmetrically distributed around the true value, suggesting that these corrections are unnecessary.</p><p>For the <inline-formula><mml:math id="inf97"><mml:mi>t</mml:mi></mml:math></inline-formula>-test we use the variance estimated from the bootstrap and use the number of observations minus one as the degrees of freedom. When bootstrapping across both subjects and conditions, we used the smaller number to stay conservative. This approach follows <xref ref-type="bibr" rid="bib35">Efron and Tibshirani, 1994</xref>, chapter 12.4.</p><p>The <inline-formula><mml:math id="inf98"><mml:mi>t</mml:mi></mml:math></inline-formula>-test has some advantages over the percentile bootstrap (<xref ref-type="bibr" rid="bib35">Efron and Tibshirani, 1994</xref>): First, precise <inline-formula><mml:math id="inf99"><mml:mi>p</mml:mi></mml:math></inline-formula> value estimates require many bootstrap samples. Especially, when smaller <inline-formula><mml:math id="inf100"><mml:mi>α</mml:mi></mml:math></inline-formula> levels or corrections for multiple comparisons are used, the percentile bootstrap can become computationally expensive and/or unreliable. Second, for small sample sizes, the bootstrap distribution does not take the uncertainty about the variance of the distribution into account. This is a similar error as taking the standard normal instead of a <inline-formula><mml:math id="inf101"><mml:mi>t</mml:mi></mml:math></inline-formula>-distribution to define confidence intervals. Third, the bootstrap distributions are discrete, which is a bad approximation in the tails of the distribution. For example, a sample of five RDMs which are all positively related to the model is declared significantly related to the model at any <inline-formula><mml:math id="inf102"><mml:mi>α</mml:mi></mml:math></inline-formula> level, because all bootstrapped average evaluations are at least as high as the lowest individual evaluation. Fourth, for the 2-factor bootstrap and the bootstrap-wrapped crossvalidation, we can give corrections for the variance, but lack techniques to generate bootstrap samples directly.</p><p>We should also note that we expect the <inline-formula><mml:math id="inf103"><mml:mi>t</mml:mi></mml:math></inline-formula>-distribution to be a good approximation for our case: We expect fairly symmetric distributions for the differences between models and average them across subjects, which should lead to a quick convergence toward a normal distribution for the model performances and their differences.</p><p>In particular, the model performances we base our tests on are not necessarily positive, allowing us to use the same techniques for the test against 0. This is different from tests that handle the original dissimilarities, where the true distances can only be positive.</p></sec><sec id="s4-5"><title>Noise ceiling for model performance</title><p>In addition to comparing models to each other, we compare models to a noise ceiling and to chance performance. The noise ceiling provides an estimate of the performance the true (data-generating) model would achieve. A model that approaches the noise ceiling (i.e. is not significantly below the noise ceiling) cannot be statistically rejected. We would need more data to reveal any remaining shortcomings of the model. The noise ceiling is not 1, because even the true group RDM would not perfectly predict all subjects’ RDMs because of the intersubject variability and noise affecting the RDM estimates. We estimate an upper and a lower bound for the true model’s performance (<xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref>). The upper bound is constructed by computing the RDM which performs best among all possible RDMs. Obviously, no model can perform better than this best RDM, so it provides a true upper bound. To estimate a lower bound, we use leave-one-out crossvalidation, computing the best performing RDM for all but one of the subjects and evaluating on the held-out subject. We can understand the upper and lower bound of the noise ceiling as uncrossvalidated and crossvalidated estimates of the performance of an overly flexible model that contains the true model. The uncrossvalidated estimate is expected to be higher than the true model’s performance because it is overfitted. The crossvalidated estimate is expected to be lower than the true model’s performance because it is compromised by the noise and subject-sampling variability in the data.</p><p>For most RDM comparators, the best performing RDM can be derived analytically as a mean after adequate normalization of the single subject RDMs. For cosine similarity, they are normalized to unit norm. For Pearson correlation, the RDM vectors are normalized to zero mean and unit standard deviation. For the whitened measures the normalization is based on the norm induced by the noise precision instead, that is subject RDM vectors <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are divided by <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:msup><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> instead of the standard Euclidean norm <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:msup><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the Spearman correlation, subject RDM vectors are first transformed to ranks.</p><p>For Kendall’s <inline-formula><mml:math id="inf107"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula>, there is no efficient method to find the optimal RDM for a dataset, which is one of the reasons for using the Spearman rank correlation for RDM comparisons. If Kendall <inline-formula><mml:math id="inf108"><mml:mi>τ</mml:mi></mml:math></inline-formula> based inference is chosen nonetheless, the problem can be solved approximately by applying techniques for Kemeny–Young voting (<xref ref-type="bibr" rid="bib4">Ali and Meilă, 2012</xref>) or by simply using the average ranks, which is a reasonable approximation, especially if the rank transformed RDMs are similar across subjects. In the toolbox, we currently use this approximation without further adjustment.</p><p>For the lower bound, we use leave-one-out crossvalidation over subjects. To do this, each subject is once selected as the left-out subject and the best RDM to fit all other subjects is computed. The expected average performance of this RDM is a lower bound on the true model’s performance, because fitting all distances independently is technically a very flexible model, which performs the same generalization as the tested models. As all other models it should thus perform worse than or equal to the correct model.</p><p>When flexible models are used, such that crossvalidation over conditions is performed, the computation of noise ceilings needs to take this into account (<xref ref-type="bibr" rid="bib100">Storrs et al., 2014</xref>). Essentially, the computation of the noise ceilings is then restricted to the test sets of the crossvalidation, which takes into account which parts of the RDMs are used for evaluation.</p></sec><sec id="s4-6"><title>Flexible models</title><p>As model types, we implement three types of flexible model, in addition to the standard fixed model, which represents a single RDM to be tested:</p><list list-type="order"><list-item><p>A <italic>selection model</italic>, which states that one of a set of RDMs is the correct one.</p></list-item><list-item><p>A one-dimensional <italic>manifold model</italic>, which consists of an ordered list of RDMs and is allowed to linearly interpolate between neighboring RDMs.</p></list-item><list-item><p>A <italic>weighted representational model</italic>, which states that the RDM is a (positively) weighted sum of a set of RDMs.</p></list-item></list><p>These models aim to provide the flexibility necessary to appropriately represent the uncertainty about the data generation process in different ways. First, we may be uncertain about aspects of the underlying brain-computational model, such as the relative prevalence in the neural population of different subpopulations of tuned neurons (<xref ref-type="bibr" rid="bib56">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib57">Khaligh-Razavi et al., 2017</xref>; <xref ref-type="bibr" rid="bib50">Jozwik et al., 2016</xref>). Second, we may be uncertain about aspects of the measurement process, such as the spatial smoothing and weighted averaging of features due to measurement methods. Measurement with functional MRI voxels, for example, can strongly influence the resulting RDMs, which can lead to wrong conclusions and generally bad model performance when the models are compared to measured RDMs (<xref ref-type="bibr" rid="bib70">Kriegeskorte and Diedrichsen, 2016</xref>).</p><p>The selection model implements flexibility in perhaps the simplest way, by allowing a choice among a set of RDMs produced from the model under different assumptions about the brain computations and/or the measurement process. For training, we can simply evaluate each possible RDM on the training data and choose the best performing one as the model prediction for evaluation. This model implies no structure in which RDMs can be predicted, but can only handle a finite set of RDMs.</p><p>The one-dimensional manifold model implements an ordered set of RDMs, where the model is allowed to interpolate between each pair of consecutive RDMs. This representation is helpful if the uncertainty about the measurements effect can be well summarized by one continuous parameter like the width of a smoothing kernel. Then we can sample a set of values for this parameter and use the simulated results as the basis RDMs for this kind of model. Then the model will provide an approximation to the continuous set of RDMs predicted by changing the parameter without requiring a method to optimize the parameter directly.</p><p>Finally, the weighted representational model represents the effect of weighting orthogonal features. In this case, the overall RDM is a weighted sum of the RDMs generated by the individual features. Whenever the original model has a feature representation, we may be uncertain about the features relative prevalence in the neural population and/or in the measured responses. It can be sensible then to assume that these features are represented with different weights or are differently amplified by the measurement process. The squared Euclidean representational distances that would obtain from a concatenation of feature subsets, each multiplied by a different weight, is equal to a nonnegatively weighted combination of the squared Euclidean RDMs for the individual feature sets. This justifies a nonnegative weighted model at the level of the RDMs.</p><p>A particular application of the weighted representation model is motivated by the local averaging in fMRI voxels, which leads to an overrepresentation of the population-mean dimension of the multivariate response space (<xref ref-type="bibr" rid="bib16">Carlin and Kriegeskorte, 2017</xref>; <xref ref-type="bibr" rid="bib70">Kriegeskorte and Diedrichsen, 2016</xref>; <xref ref-type="bibr" rid="bib90">Ramírez et al., 2014</xref>). The expected RDM for measurements that independently randomly weight features is a linear combination of two RDMs, one based on treating features separately and one based on averaging all features before computing the RDM (see Appendix 4).</p><p>Our methodology is not specific to these types of model and can be easily extended to other types of model. To do so, the only requirement is that there is a reasonably efficient fitting method to infer the best fitting parameters for a given dataset of training RDMs. Indeed, new model types can be slotted into our toolbox by users by implementing only two functions: one that predicts an RDM based on a parameter vector and one that fits the parameter vector to a dataset.</p></sec><sec id="s4-7"><title>Validation of the methodology</title><p>To evaluate our methods we use three kinds of simulations. First, we implement simulations based on deep neural networks and a simple approximation of voxel sampling. By choosing a new random voxel sampling per subject and using different randomly selected input images, we can test our methods with systematic variations across conditions and/or subjects. Second, we implement a simulation based on real fMRI data recombining measurements signals and noise to keep all complications found in true fMRI data. Third, we present simulations based on calcium-imaging data from mice (<xref ref-type="bibr" rid="bib28">de Vries et al., 2020</xref>).</p><p>Additionally, we tested that the tests we implement are in principle valid using a simple simulation based on a normal distribution for the original measurements, which corresponds to the matrix-normal generative model we used for theoretical derivations elsewhere (<xref ref-type="bibr" rid="bib31">Diedrichsen et al., 2020</xref>). These simulations are presented in Appendix 1.</p><sec id="s4-7-1"><title>Neural-network-based simulation</title><p>Our simulations were based on the activities in the convolutional layers of AlexNet (<xref ref-type="bibr" rid="bib75">Krizhevsky et al., 2012</xref>) in response to randomly chosen images from the ecoset validation set (<xref ref-type="bibr" rid="bib81">Mehrer et al., 2021</xref>). For each stimulus, we computed the activities in the convolutional layer and took randomly chosen local averages to simulate the averaging of voxels. We then generated fMRI-like measurement timecourses to a randomly ordered short event-related design by convolution with a hemodynamic-response function and addition of autoregressive noise. We then ran a GLM analysis to estimate the response strength to each stimulus. From these estimated voxel responses, we computed data RDMs per subject and ran our proposed analysis procedures to compute model performances of different models which we also based on the convolutional layers of AlexNet.</p><p>For the network, we used the implementation available for pytorch through the torchvision package (<xref ref-type="bibr" rid="bib87">Paszke et al., 2019</xref>).</p><sec id="s4-7-1-1"><title>Stimuli</title><p>Stimuli were chosen independently from the validation set of ecoset by first choosing a category randomly and then sampling an image randomly from that category. These stimuli are natural images with categories chosen to approximate the relevance for human observers. The validation set contain 565 categories with 50 images each, that is 28,250 images in total.</p></sec><sec id="s4-7-1-2"><title>Noise-free voxel response</title><p>To compute the response strength of a voxel to a stimulus we computed a local average of the feature maps. We first convolved the feature maps with a Gaussian representing the spatial extend of the voxels, whose size we defined by its standard deviation relative to the overall size of the feature map. A voxel with size 0.05 would thus correspond to a Gaussian averaging area whose standard deviation is 5% of the size of the feature map. Voxel locations were then chosen uniformly randomly over the locations within the feature map. To average across features, we chose a weight for each feature and each voxel uniformly between 0 and 1 and then took the weighed sum as the voxel response.</p></sec><sec id="s4-7-1-3"><title>fMRI simulation</title><p>To generate timecourses we assumed a measurement was taken every 2 s and a new stimulus was presented during every second measurement, with no stimulus presented in the measurement intervals between stimulus presentations.</p><p>To generate a simulated fMRI response, we computed the stimulus by voxel response matrix and normalized it per subject to have equal averaged squared value. We then converted this into timecourses following the usual GLM assumptions and convolved the predictions with a hemodynamic-response function. We set the hrf to the standard sum of two gamma distributions as assumed in statistical parametric mapping (SPM; <xref ref-type="bibr" rid="bib88">Pedregosa et al., 2015</xref>), normalized to an overall sum of 1.</p><p>We then added noise from an autoregressive model of rank 1 (AR1) with covariance between pairs of voxels given by the overlap of the weighting functions of their weights. To control the strength of the autocorrelation, we set the coefficient for the previous data point to 0.5. To enforce the covariance between voxels, we multiplied the noise matrix with the cholesky decomposition of the desired covariance. To control the overall noise strength we scaled the final noise by a constant.</p><p>Each stimulus was presented once per run, with multiple stimulus presentations implemented as multiple runs.</p></sec><sec id="s4-7-1-4"><title>Analysis</title><p>To analyse the simulated data we ran a standard GLM analysis which yielded a <inline-formula><mml:math id="inf109"><mml:mi>β</mml:mi></mml:math></inline-formula>-estimate for each presented stimulus for each run of the experiment.</p><p>To compute RDMs we used Crossnobis distances based on leave-one-out crossvalidation over runs and the covariance of the residuals of the GLM. For this step, we used the function implemented in our toolbox.</p></sec><sec id="s4-7-1-5"><title>Fixed-model definition</title><p>As models to be compared we used the different layers of AlexNet. To generate an optimal model RDM we applied two transformations to mimic the average effect of voxel sampling. First we convolved the representation with the spatial receptive field of the voxels to mimic the spatial averaging effect. To capture the effect of pooling the features with nonnegative weights, we then computed a weighted sum of the RDM containing the features separately and one RDM based on the summed response across features weighted with weights 1 and 3.</p><p>This weighting computes the expected Euclidean distance of patterns under our random weighting scheme as we derive in Appendix 4: For our <inline-formula><mml:math id="inf110"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> the expected value is <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> and the variance is <inline-formula><mml:math id="inf112"><mml:mrow><mml:mrow><mml:mi>Var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>12</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula> such that the weights for the RDM based on the individual features is <inline-formula><mml:math id="inf113"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:math></inline-formula> and the weight for the RDM based on the summed feature response is <inline-formula><mml:math id="inf114"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>12</mml:mn></mml:mfrac></mml:math></inline-formula>, that is a 3:1 weighting.</p><p>Based on this weighting we generated a fixed model for each individual processing step in AlexNet including the nonlinearities and pooling operations resulting in 12 models predicting a fixed RDM.</p></sec><sec id="s4-7-1-6"><title>Tested conditions</title><p>For the large deep-neural-network-based simulation underlying the results in <xref ref-type="fig" rid="fig4">Figure 4</xref>, we chose a base set of factors which we crossed with all other conditions and a separate set of factors which were not crossed with each other but only with the base set.</p><p>Into the base set of factors we included the following factors: Which experimental parameters were changed over repetitions of the experiment (none, subjects, conditions, or both) and which bootstrapping method we applied (over conditions, over subjects, over both or applying the bootstrap correction). We applied all four bootstrapping conditions to the simulations in which none of the parameters were varied, the fitting ones to the subject and stimulus varying simulations and the bootstrap with and without correction for to the simulations were both parameters varied over repetitions resulting in 8 conditions for variation and bootstrap. Additionally, we included the number of subjects (5, 10, 20, 40, or 80) and the number of conditions (10, 20, 40, 80, and 160). For each set of conditions we thus ran 8 × 5 × 5 = 200 conditions.</p><p>Other factors we varied were: The number of repeats, which we set to 4 usually and tested 2 and 8. The layer we used to simulate the data, which we usually set to layer number 8 which corresponds to the output of the 3rd convolutional layer, and also tried 2, 5, 10, and 12, which correspond to the other 4 convolutional layers of AlexNet. The size of the voxels which we usually set to 0.05, that is we set the standard deviation of the Gaussian to 5% of the size of the feature map. As variations we tried 0, 0.25, and <inline-formula><mml:math id="inf115"><mml:mi mathvariant="normal">∞</mml:mi></mml:math></inline-formula>, that is no spatial pooling, a quarter of the size of the feature map as standard deviation and an average over the whole feature map. Finally, we varied the number of voxels, which we usually set to 100, but tried 10 and 1000 additionally. In total we thus ran 3 + 5 + 4 + 3 = 15 sets of conditions with 200 conditions each resulting in 3000 conditions, with a grand total of 300,000 simulations.</p></sec><sec id="s4-7-1-7"><title>Bootstrap-wrapped crossvalidation</title><p>To test the precision and consistency of the calculations for the bootstrap-wrapped crossvalidation (<xref ref-type="fig" rid="fig5">Figure 5a, b</xref>), we needed repeated analyses for the same datasets. For this simulations we thus simulated only 10 datasets for the standard conditions, 20 subjects and 40 conditions, while varying both conditions and subjects and then ran repeated analyses on these datasets. For each setting, we ran 100 repeated analysis of each dataset. As conditions we chose 2, 4, 8, 16, and 32 crossvalidation assignments for 1000 bootstrap samples and additionally variants with only 2 crossvalidation folds and 2000, 4000, 8000, or 16,000 bootstrap samples.</p></sec><sec id="s4-7-1-8"><title>Flexible-model treatment</title><p>To test whether our methods are adequate for estimating the variability for model performances of flexible models (<xref ref-type="fig" rid="fig5">Figure 5d–f</xref>), we ran our standard settings for 20 subjects and 40 stimuli and drawing new subjects and new stimuli, while replacing the fixed models per layer with flexible models of different kinds.</p><p>We generated models by combining models with different assumptions about the voxel pooling pattern: We varied two factors: (1) How feature weighting was handled: full, that is predicted distances are Euclidean distances in the original feature space; avg, that is distances are the differences in the average activation across features; or ‘weighted’, that is the weighted average of these two models, that corresponds to the expected RDM under the weight sampling we simulated. (2) How averaging over space was handled.</p><p>We first used different kinds fixed models, which serve as the building blocks for the flexible models. We varied two aspects of the measurement models applied: How large voxels are assumed to be (no pooling, <inline-formula><mml:math id="inf116"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the image size and pooling over the whole feature maps) and how the features pooling is handled (no pooling, average feature, or the correct weighting assumed for the fixed models previously). These 3 × 3 combinations are the 9 fixed-model variants.</p><p>We then generated selection models which had a range of voxel sizes to choose from (no pooling, std = 1%, 2%, 5%, 10%, 20%, 50% of the image size and pooling across the whole feature map). For the treatment of pooling over features we used four variants: For the first three called full, average, and weighted we used one of the types of fixed models to generate the RDMS. For the last, we allowed both the RDMs used by the full models and the ones used by the average models as a choice.</p><p>As an example of a linearly weighted model, we generated a model which was allowed to use a linear weighting of the four corner-case RDMs: no feature pooling and no spatial pooling, average feature and no spatial pooling, a global average per feature map, and the RDM induced by pooling over all locations and features. The model could predict any linear combination of the corner-case RDMs to fit the data RDMs.</p></sec></sec></sec><sec id="s4-8"><title>fMRI-data-based simulation</title><p>With our fMRI-data-based simulation, we aim to show that our analyses are correct and functional for real fMRI data. Real data may contain additional statistical regularities, which we did not take into account in our deep-neural-network-based simulations. To do so, we took a large published dataset of fMRI responses to images and sampled from this dataset to generate hypothetical experimental datasets across which we would like to generalize. All scripts for the fMRI-data-based simulation are openly available on <ext-link ext-link-type="uri" xlink:href="https://github.com/adkipnis/fmri-simulations">https://github.com/adkipnis/fmri-simulations</ext-link>, (<xref ref-type="bibr" rid="bib59">Kipnis, 2023</xref>).</p><sec id="s4-8-1"><title>Dataset</title><p>For these simulations we used data from <xref ref-type="bibr" rid="bib48">Horikawa and Kamitani, 2017</xref> (as available from <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds001246/versions/1.2.1">https://openneuro.org/datasets/ds001246/versions/1.2.1</ext-link>). This dataset contains fMRI data collected from five subjects viewing natural images selected from ImageNet or imagining images from a category. For our simulations, we used only the ‘test’ datasets, which contain 50 different images from distinct categories, which were each presented 35 times to each subject giving us an overall reliable signal and repetitions to resample from.</p><p>We used the automatic MRI preprocessing pipeline implemented in fMRIPrep 1.5.2 (<xref ref-type="bibr" rid="bib37">Esteban et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">Esteban et al., 2019</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_016216">SCR_016216</ext-link>), which is based on <italic>Nipype</italic> 1.3.1 (<xref ref-type="bibr" rid="bib42">Gorgolewski et al., 2011</xref>; <xref ref-type="bibr" rid="bib43">Gorgolewski et al., 2018</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002502">SCR_002502</ext-link>). This program was also used to produce the following description of the preprocesing performed.</p></sec><sec id="s4-8-2"><title>Anatomical-data preprocessing</title><p>The T1-weighted (T1w) image was corrected for intensity non-uniformity (INU) with N4BiasFieldCorrection (<xref ref-type="bibr" rid="bib105">Tustison et al., 2010</xref>), distributed with ANTs 2.2.0 (<xref ref-type="bibr" rid="bib7">Avants et al., 2008</xref>, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_004757">SCR_004757</ext-link>), and used as T1w reference throughout the workflow. The T1w reference was then skull stripped with a <italic>Nipype</italic> implementation of the antsBrainExtraction.sh workflow (from ANTs), using OASIS30ANTs as target template. Brain tissue segmentation of cerebrospinal fluid (CSF), white matter (WM), and gray matter (GM) was performed on the brain-extracted T1w using fast (FSL 5.0.9, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002823">SCR_002823</ext-link>, <xref ref-type="bibr" rid="bib118">Zhang et al., 2001</xref>). Brain surfaces were reconstructed using recon-all (FreeSurfer 6.0.1, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_001847">SCR_001847</ext-link>, <xref ref-type="bibr" rid="bib27">Dale et al., 1999</xref>), and the brain mask estimated previously was refined with a custom variation of the method to reconcile ANTs- and FreeSurfer-derived segmentations of the cortical GM of Mindboggle (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002438">SCR_002438</ext-link>, <xref ref-type="bibr" rid="bib60">Klein et al., 2017</xref>). Volume-based spatial normalization to one standard space (MNI152NLin2009cAsym) was performed through nonlinear registration with antsRegistration (ANTs 2.2.0), using brain-extracted versions of both T1w reference and the T1w template. The following template was selected for spatial normalization: <italic>ICBM 152 Nonlinear Asymmetrical template version 2009c</italic> (<xref ref-type="bibr" rid="bib38">Fonov et al., 2009</xref>, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_008796">SCR_008796</ext-link>; TemplateFlow ID: MNI152NLin2009cAsym).</p></sec><sec id="s4-8-3"><title>Functional data preprocessing</title><p>For each of the 35 blood-oxygen-level-dependent (BOLD) runs found per subject (across all tasks and sessions), the following preprocessing was performed. First, a reference volume and its skull-stripped version were generated using a custom methodology of <italic>fMRIPrep</italic>. The BOLD reference was then co-registered to the T1w reference using bbregister (FreeSurfer) which implements boundary-based registration (<xref ref-type="bibr" rid="bib44">Greve and Fischl, 2009</xref>). Co-registration was configured with six degrees of freedom. Head-motion parameters with respect to the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) are estimated before any spatiotemporal filtering using mcflirt (FSL 5.0.9, <xref ref-type="bibr" rid="bib49">Jenkinson et al., 2002</xref>). BOLD runs were slice-time corrected using 3dTshift from AFNI 20160207 (<xref ref-type="bibr" rid="bib25">Cox and Hyde, 1997</xref>, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_005927">SCR_005927</ext-link>). The BOLD time-series, were resampled to surfaces on the following spaces: <italic>fsaverage5</italic> and <italic>fsaverage6</italic>. The BOLD time-series (including slice-timing correction when applied) were resampled onto their original, native space by applying the transforms to correct for head-motion. These resampled BOLD time-series will be referred to as <italic>preprocessed BOLD in original space</italic>, or just <italic>preprocessed BOLD</italic>. The BOLD time-series were resampled into standard space, generating a <italic>preprocessed BOLD run in [‘MNI152NLin2009cAsym’] space</italic>. First, a reference volume and its skull-stripped version were generated using a custom methodology of <italic>fMRIPrep</italic>. Several confounding time-series were calculated based on the <italic>preprocessed BOLD</italic>: framewise displacement (FD), the spatial root mean square of the data after temporal differencing (DVARS) and three region-wise global signals. FD and DVARS are calculated for each functional run, both using their implementations in <italic>Nipype</italic> (following the definitions by <xref ref-type="bibr" rid="bib89">Power et al., 2014</xref>). The three global signals are extracted within the CSF, the WM, and the whole-brain masks. Additionally, a set of physiological regressors were extracted to allow for component-based noise correction (<xref ref-type="bibr" rid="bib11">Behzadi et al., 2007</xref>). Principal components are estimated after high-pass filtering the <italic>preprocessed BOLD</italic> time-series (using a discrete cosine filter with 128 s cut-off) for the two <italic>CompCor</italic> variants: temporal (tCompCor) and anatomical (aCompCor). tCompCor components are then calculated from the top 5% variable voxels within a mask covering the subcortical regions. This subcortical mask is obtained by heavily eroding the brain mask, which ensures it does not include cortical GM regions. For aCompCor, components are calculated within the intersection of the aforementioned mask and the union of CSF and WM masks calculated in T1w space, after their projection to the native space of each functional run (using the inverse BOLD-to-T1w transformation). Components are also calculated separately within the WM and CSF masks. For each CompCor decomposition, the <italic>k</italic> components with the largest singular values are retained, such that the retained components’ time-series are sufficient to explain 50% of variance across the nuisance mask (CSF, WM, combined, or temporal). The remaining components are dropped from consideration. The head-motion estimates calculated in the correction step were also placed within the corresponding confounds file. The confound time-series derived from head-motion estimates and global signals were expanded with the inclusion of temporal derivatives and quadratic terms for each (<xref ref-type="bibr" rid="bib92">Satterthwaite et al., 2013</xref>). Frames that exceeded a threshold of 0.5 mm FD or 1.5 standardized DVARS were annotated as motion outliers. All resamplings can be performed with <italic>a single interpolation step</italic> by composing all the pertinent transformations (i.e. head-motion transform matrices, susceptibility distortion correction when available, and co-registrations to anatomical and output spaces). Gridded (volumetric) resamplings were performed using antsApplyTransforms (ANTs), configured with Lanczos interpolation to minimize the smoothing effects of other kernels (<xref ref-type="bibr" rid="bib77">Lanczos, 1964</xref>). Non-gridded (surface) resamplings were performed using mri_vol2surf (FreeSurfer).</p><p>Many internal operations of <italic>fMRIPrep</italic> use <italic>Nilearn</italic> 0.5.2 (<xref ref-type="bibr" rid="bib3">Abraham et al., 2014</xref>, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_001362">SCR_001362</ext-link>), mostly within the functional processing workflow. For more details of the pipeline, see <ext-link ext-link-type="uri" xlink:href="https://fmriprep.readthedocs.io/en/1.5.2/workflows.html">the section corresponding to workflows in fMRIPrep’s documentation</ext-link>.</p><p>The above boilerplate text was automatically generated by fMRIPrep with the express intention that users should copy and paste this text into their manuscripts <italic>unchanged</italic>. It is released under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">CC0</ext-link> license.</p></sec><sec id="s4-8-4"><title>Region selection</title><p>Visual areas were defined according to the surface atlas by <xref ref-type="bibr" rid="bib41">Glasser et al., 2016</xref>. For our simulations we used the following 10 visual areas as ROIs, joining areas from the atlas to avoid too small ROIs (the name of the areas in the atlas is given in brackets): V1 (V1), V2 (V2), V3 (V3), V4 (V4), ventral visual complex (VVC), ventromedial visual area (VMV1, VMV2, VMV3), parahippocampal place area (PHA1, PHA2, PHA3), fusiform face area (FFC), inferotemporal cortex (TF, PeEc), and MT/MST (MT, MST). The areas were selected separately for the two hemispheres.</p><p>To map the atlas onto individual subject’s brain space we used the mappings estimated by FreeSurfer with fmriprep’s standard settings. The Glasser Atlas was registered to each participant’s native space with mri_surf2surf, and voxels labeled using mri_annotation2label. Next, each ROI was mapped to native T1w volumetric space with mri_label2vol. To cover as many contiguous voxels as possible, the resulting masks were inflated with mri_binarize and every voxel outside of the volume between the pial surface and WM was eroded with mris_calc. To convert the resulting masks to T2*w space we used custom python scripts: First, masks were smoothed with a Gaussian kernel of FWHM = 3 mm, resampled to T2*w space using nearest neighbor interpolation, and finally thresholded. The threshold for each mask was set to equalize mask volume between T1w and T2*w space. Finally, voxels with multiple ROI assignments were removed from all ROIs but the one with the highest pre-threshold value. Voxels outside the fMRIPrep-generated brain mask were removed from all generated 3d-masks of ROIs.</p></sec><sec id="s4-8-5"><title>General linear modeling</title><p>For extracting response patterns from the measurements we used two GLMs. In the first GLM, we regressed out noise sources and in the second we estimate stimulus responses. This two-step process is advantageous in this case where stimulus predictors and noise predictors are highly collinear (As there is only one presentation of each stimulus per run and as they cover the whole run they can together form almost any sufficiently slow variation.): It allows us to attribute all variance that could be attributed to the noise sources uniquely to them and not to effects of stimulus presentation. This is closer to the original papers analysis, leads to higher reliability and generates the second GLM as a stage at which we can adequately model the noise with a relatively simple AR(2) model.</p><p>GLM was performed in SPM12 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>). No spatial smoothing was applied, models were estimated using Restricted Maximum Likelihood on top of Ordinary Least Squares and auto-correlations were taken into account using SPM’s inbuilt AR(1) method.</p><p>For the first GLM, we used the following noise regressors from the ones provided by fMRIprep: An intercept for each run, the six basic motion parameters and their derivatives, six cosine basis functions to model drift, FD, DVARS, and the first six aCompCors with the largerst eigenvalues. All runs were pooled to get the best noise parameter estimates possible.</p><p>We interpret the residuals from the first GLM as a denoized version of the fMRI signal and use them as input for a second GLM separately for each run to estimate stimulus effects: Stimulus-specific regressors were generated by convolving stimulus onset time-series with the canonical HRF without derivatives. From this GLM, we kept the estimated <inline-formula><mml:math id="inf117"><mml:mi>β</mml:mi></mml:math></inline-formula> coefficients <inline-formula><mml:math id="inf118"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> for each stimulus and the residuals <italic>r</italic><sub><italic>i</italic></sub> for further processing.</p></sec><sec id="s4-8-6"><title>Resampling</title><p>To sample a single run for further analysis, we randomly chose a run from the measured data without replacement. To expand the set of possible datasets, we then generated a new simulated BOLD signal <inline-formula><mml:math id="inf119"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> for each voxel <inline-formula><mml:math id="inf120"><mml:mi>i</mml:mi></mml:math></inline-formula> at the stage of the second-level GLM. To do so, we model the data as a GLM with an AR(2) model for the noise and then generate a new timecourse by permuting the residuals <inline-formula><mml:math id="inf121"><mml:msub><mml:mi>η</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of the AR(2) model. As we apply the same permutation to each voxel, this procedure largely preserves spatial noise covariance.</p><p>In mathematical formulas, this process can be described as follows: Let <inline-formula><mml:math id="inf122"><mml:mi>p</mml:mi></mml:math></inline-formula> be the number of conditions, <inline-formula><mml:math id="inf123"><mml:mi>n</mml:mi></mml:math></inline-formula> be the number of scans per run, and <inline-formula><mml:math id="inf124"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> be the denoized BOLD response of voxel <inline-formula><mml:math id="inf125"><mml:mi>i</mml:mi></mml:math></inline-formula>. We can then use the design matrix of the run <inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> , the point estimate <inline-formula><mml:math id="inf127"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> for the parameter values and corresponding residuals <inline-formula><mml:math id="inf128"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> estimated by SPM to simulate a new data run:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>η</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf129"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf130"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are the estimated parameters of an AR(2) model fitted to the residuals <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Its residuals are denoted <inline-formula><mml:math id="inf132"><mml:msub><mml:mi>η</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and were randomly permuted to give <inline-formula><mml:math id="inf133"><mml:msub><mml:mover accent="true"><mml:mi>η</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> using the same permutation for all voxels in a run.</p><p>Additionally, we sampled the conditions to use with replacement, that is we used the <inline-formula><mml:math id="inf134"><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of a random sample of conditons, which we also used to select the RDMs from the models.</p><p>We saved this dataset in the same format as the original data and re-ran the second-level GLM using SPM on these simulated data to generate noisy estimates stimulus responses in each voxel.</p></sec><sec id="s4-8-7"><title>RDM calculation and comparison</title><p>We use crossnobis RDMs for this simulation, testing four different estimates for the noise covariance: We either use the identity, univariate noise normalization, or a shrinkage estimate of the covariance based on the covariance of the residuals, or based on the covariance of the individual runs’ mean-centered <inline-formula><mml:math id="inf135"><mml:mi>β</mml:mi></mml:math></inline-formula> estimates.</p><p>For comparing RDMs, we use the cosine similarity throughout.</p></sec><sec id="s4-8-8"><title>Model RDMs</title><p>We use the RDMs of different ROIs as models, effectively testing how well our methods recover the data-generating ROI. The model RDM for each ROI is the pooled RDM across all subjects and runs computed by the same noise normalization method as the one used for the data RDMs. Data for these RDMs stemmed from the original results of the second-level GLM, making them less noisy than any RDM stemming from the simulated data.</p></sec></sec><sec id="s4-9"><title>Simulation design</title><p>For each condition, we ran 100 repeats to estimate the true variability of results and ran all combinations of the following conditions: We used 2, 4, 8, 16, or 32 runs per simulation (5 variants). We used 5, 10, 20, 30, or 50 stimuli (5 variants). We scaled the noise by 0.1, 1.0, or 10.0 (3 variants). We used each of the 20 ROIs for data generation once (20 variants). And we used the 4 methods for estimating the noise covariance (4 variants). Resulting in 24· 5· 5· 3· 20·4 = 144,000 different simulations. To save computation time, we ran the fMRI simulation and analyses only once per repeat and noise level and ran all analysis variants on the same data. When fewer runs or conditions were required for a variant, we randomly selected a subset for the analysis without replacement.</p><sec id="s4-9-1"><title>Calcium-imaging-data-based simulation</title><p>For the calcium-imaging-data-based simulation we used the Allen institutes mouse visual coding calcium-imaging data available at <ext-link ext-link-type="uri" xlink:href="https://observatory.brain-map.org/visualcoding/">https://observatory.brain-map.org/visualcoding/</ext-link> (<xref ref-type="bibr" rid="bib28">de Vries et al., 2020</xref>). Detailed information on the recording techniques can be obtained from the original publications and with the dataset.</p><p>We used the ‘natural scenes’ data, which consists of measured calcium responses to 118 natural scenes. The natural scenes were shown for 250 ms each without an inter stimulus interval in random order. In each session, each image was present 50 times.</p><p>From this dataset, we selected all experimental sessions, which contained a natural scenes experiment. Additionally, we restricted ourselves to three relatively broad cre driver lines, which target excitatory neurons relatively broadly: ’Cux2-CreERT2’, ’Emx1-IRES-Cre’, and ’Slc17a7-IRES2-Cre’. For further analyses, we ignore which driver line was used to achieve enough data for resampling. This resulted in 174 experimental sessions from 91 mice with 146 cells recorded on average (range: 18–359). Of these recordings, 35 came from laterointermediate area, 32 from posteromedial visual area, 23 from rostrolateral visual area, 46 from primary visual cortex, 16 from anteromedial area, and 22 from anterolateral area.</p><p>To quantify the response of a neuron to the stimuli, we used the fully preprocessed <inline-formula><mml:math id="inf136"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mi>F</mml:mi></mml:mfrac></mml:math></inline-formula> traces as provided by the dataset. We then extract the measurements from the frame after the one marked as stimulus onset till the stated stimulus endframe resulting in six or seven frames per stimulus presentation. As a response per neuron we then simply took the average of these frames.</p><p>To compute RDMs based on these data, we used Crossnobis distances based on different estimates of the noise covariance matrix based on the variance of the stimulus repetitions around the average neural response for each stimulus. We either used: an identity matrix, effectively calculating a crossvalidated Euclidean distance; a diagonal matrix of variances, corresponding to univariate noise normalization; a shrinkage estimate toward a constant diagonal matrix (<xref ref-type="bibr" rid="bib79">Ledoit and Wolf, 2004</xref>), or a shrinkage estimate shrunk toward the diagonal of sample variances (<xref ref-type="bibr" rid="bib93">Schäfer and Strimmer, 2005</xref>).</p><p>To generate new datasets, we randomly sampled subsets of stimuli, mice, runs, and cells from a brain area without replacement. To exclude possible interactions we avoided sampling multiple sessions recorded from the same mouse by sampling the mice and then randomly sampling from the sessions of each mouse, if there were more than one. For this dataset, we did not use any further processing of the data.</p><p>As variants for this simulation, we performed all combinations of the following factors: 20, 40, or 80 cells per experiment; 5, 10, or 15 mice; 10, 20, or 40 stimuli; 10, 20, or 40 stimulus repeats; the four types of noise covariance estimates; four types of rdm comparison: cosine similarity, correlation, whitened cosine similarity, and whitened correlation; whether the bootstrap was corrected; and the six brain areas. This resulted in 3 × 3 × 3 × 3 × 4 × 4 × 2 × 6 = 15,552 simulation conditions for which we simulated 100 simulations each.</p><p>As models for the simulations, we used the average RDM for each brain area as a fixed RDM model for that brain area. Thus the models are not independent from the data in our main simulation. This is not problematic for checking the integrity of our inference methods, but does not show that we can indeed differentiate brain areas based on their RDMs. To show that retrieving the brain area is possible as displayed in <xref ref-type="fig" rid="fig7">Figure 7b</xref> in the main text, we performed leave-one-out crossvalidation across mice, that is we chose the RDM models for the brain areas based on all but one mice and evaluated the RDM correlation with the left out mouse’s RDM.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Formal analysis, Supervision, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-82566-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>No new data were collected for this study. The code to run both the analysis we do and our simulations is available with our rsatoolbox (<ext-link ext-link-type="uri" xlink:href="https://github.com/rsagroup/rsatoolbox">https://github.com/rsagroup/rsatoolbox</ext-link>, copy archived at <xref ref-type="bibr" rid="bib94">Schütt, 2023</xref>). The data for the fMRI-based resampling analysis are available from <xref ref-type="bibr" rid="bib48">Horikawa and Kamitani, 2017</xref> and the data for the calcium imaging are available from the Allen institutes website.</p><p>The following previously published datasets were used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Horikawa</surname><given-names>T</given-names></name><name><surname>Kamitani</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Generic decoding of seen and imagined objects using hierarchical visual features</data-title><source>Open Neuro</source><pub-id pub-id-type="accession" xlink:href="https://openneuro.org/datasets/ds001246">ds001246</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>de Vries</surname><given-names>SEJ</given-names></name><name><surname>Lecoq</surname><given-names>JA</given-names></name><name><surname>Buice</surname><given-names>MA</given-names></name><name><surname>Groblewski</surname><given-names>PA</given-names></name><name><surname>Ocker</surname><given-names>GK</given-names></name><name><surname>Oliver</surname><given-names>M</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Millman</surname><given-names>D</given-names></name><name><surname>Roll</surname><given-names>K</given-names></name><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Keenan</surname><given-names>T</given-names></name><name><surname>Kuan</surname><given-names>L</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Olsen</surname><given-names>S</given-names></name><name><surname>Thompson</surname><given-names>C</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Waters</surname><given-names>J</given-names></name><name><surname>Williams</surname><given-names>D</given-names></name><name><surname>Barber</surname><given-names>C</given-names></name><name><surname>Berbesque</surname><given-names>N</given-names></name><name><surname>Blanchard</surname><given-names>B</given-names></name><name><surname>Bowles</surname><given-names>N</given-names></name><name><surname>Caldejon</surname><given-names>SD</given-names></name><name><surname>Casal</surname><given-names>L</given-names></name><name><surname>Cho</surname><given-names>A</given-names></name><name><surname>Cross</surname><given-names>S</given-names></name><name><surname>Dang</surname><given-names>C</given-names></name><name><surname>Dolbeare</surname><given-names>T</given-names></name><name><surname>Edwards</surname><given-names>M</given-names></name><name><surname>Galbraith</surname><given-names>J</given-names></name><name><surname>Gaudreault</surname><given-names>N</given-names></name><name><surname>Gilbert</surname><given-names>TL</given-names></name><name><surname>Griffin</surname><given-names>F</given-names></name><name><surname>Hargrave</surname><given-names>P</given-names></name><name><surname>Howard</surname><given-names>R</given-names></name><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Jewell</surname><given-names>S</given-names></name><name><surname>Keller</surname><given-names>N</given-names></name><name><surname>Knoblich</surname><given-names>U</given-names></name><name><surname>Larkin</surname><given-names>JD</given-names></name><name><surname>Larsen</surname><given-names>R</given-names></name><name><surname>Lau</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>F</given-names></name><name><surname>Leon</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Long</surname><given-names>F</given-names></name><name><surname>Luviano</surname><given-names>J</given-names></name><name><surname>Mace</surname><given-names>K</given-names></name><name><surname>Nguyen</surname><given-names>T</given-names></name><name><surname>Perkins</surname><given-names>J</given-names></name><name><surname>Robertson</surname><given-names>M</given-names></name><name><surname>Seid</surname><given-names>S</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name><name><surname>Shi</surname><given-names>J</given-names></name><name><surname>Sjoquist</surname><given-names>N</given-names></name><name><surname>Slaughterbeck</surname><given-names>C</given-names></name><name><surname>Sullivan</surname><given-names>D</given-names></name><name><surname>Valenza</surname><given-names>R</given-names></name><name><surname>White</surname><given-names>C</given-names></name><name><surname>Williford</surname><given-names>A</given-names></name><name><surname>Witten</surname><given-names>DM</given-names></name><name><surname>Zhuang</surname><given-names>J</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Farrell</surname><given-names>C</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Phillips</surname><given-names>JW</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex</data-title><source>Allen Brain Atlas</source><pub-id pub-id-type="accession" xlink:href="https://observatory.brain-map.org/visualcoding/">visualcoding</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name><name><surname>Agarwal</surname><given-names>A</given-names></name><name><surname>Barham</surname><given-names>P</given-names></name><name><surname>Brevdo</surname><given-names>E</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Citro</surname><given-names>C</given-names></name><name><surname>Corrado</surname><given-names>GS</given-names></name><name><surname>Davis</surname><given-names>A</given-names></name><name><surname>Dean</surname><given-names>J</given-names></name><name><surname>Devin</surname><given-names>M</given-names></name><name><surname>Ghemawat</surname><given-names>S</given-names></name><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Harp</surname><given-names>A</given-names></name><name><surname>Irving</surname><given-names>G</given-names></name><name><surname>Isard</surname><given-names>M</given-names></name><name><surname>Jia</surname><given-names>Y</given-names></name><name><surname>Jozefowicz</surname><given-names>R</given-names></name><name><surname>Kaiser</surname><given-names>L</given-names></name><name><surname>Kudlur</surname><given-names>M</given-names></name><name><surname>Levenberg</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.04467">https://arxiv.org/abs/1603.04467</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>J</given-names></name><name><surname>Ye</surname><given-names>T</given-names></name><name><surname>Krenek</surname><given-names>K</given-names></name><name><surname>Gertner</surname><given-names>RS</given-names></name><name><surname>Ban</surname><given-names>S</given-names></name><name><surname>Kim</surname><given-names>Y</given-names></name><name><surname>Qin</surname><given-names>L</given-names></name><name><surname>Wu</surname><given-names>W</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Ham</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A nanoelectrode array for obtaining intracellular recordings from thousands of connected neurons</article-title><source>Nature Biomedical Engineering</source><volume>4</volume><fpage>232</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1038/s41551-019-0455-7</pub-id><pub-id pub-id-type="pmid">31548592</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname><given-names>A</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Gervais</surname><given-names>P</given-names></name><name><surname>Mueller</surname><given-names>A</given-names></name><name><surname>Kossaifi</surname><given-names>J</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Machine learning for neuroimaging with scikit-learn</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00014</pub-id><pub-id pub-id-type="pmid">24600388</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ali</surname><given-names>A</given-names></name><name><surname>Meilă</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Experiments with Kemeny ranking: What works when?</article-title><source>Mathematical Social Sciences</source><volume>64</volume><fpage>28</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.1016/j.mathsocsci.2011.08.008</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allefeld</surname><given-names>C</given-names></name><name><surname>Görgen</surname><given-names>K</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Valid population inference for information-based imaging: From the second-level t-test to prevalence inference</article-title><source>NeuroImage</source><volume>141</volume><fpage>378</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.07.040</pub-id><pub-id pub-id-type="pmid">27450073</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>EJ</given-names></name><name><surname>St-Yves</surname><given-names>G</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Breedlove</surname><given-names>JL</given-names></name><name><surname>Dowdle</surname><given-names>LT</given-names></name><name><surname>Caron</surname><given-names>B</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Hutchinson</surname><given-names>JB</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A Massive 7T fMRI Dataset to Bridge Cognitive and Computational Neuroscience</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.02.22.432340</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Epstein</surname><given-names>CL</given-names></name><name><surname>Grossman</surname><given-names>M</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</article-title><source>Medical Image Analysis</source><volume>12</volume><fpage>26</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id><pub-id pub-id-type="pmid">17659998</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural correlations, population coding and computation</article-title><source>Nature Reviews. Neuroscience</source><volume>7</volume><fpage>358</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/nrn1888</pub-id><pub-id pub-id-type="pmid">16760916</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Magnetoencephalography for brain electrophysiology and imaging</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>327</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1038/nn.4504</pub-id><pub-id pub-id-type="pmid">28230841</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Huber</surname><given-names>L</given-names></name><name><surname>Finn</surname><given-names>ES</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Challenges and opportunities of mesoscopic brain mapping with fMRI</article-title><source>Current Opinion in Behavioral Sciences</source><volume>40</volume><fpage>189</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2021.06.002</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behzadi</surname><given-names>Y</given-names></name><name><surname>Restom</surname><given-names>K</given-names></name><name><surname>Liau</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A component based noise correction method (CompCor) for BOLD and perfusion based fMRI</article-title><source>NeuroImage</source><volume>37</volume><fpage>90</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.04.042</pub-id><pub-id pub-id-type="pmid">17560126</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Ye</surname><given-names>F</given-names></name><name><surname>Petridou</surname><given-names>N</given-names></name><name><surname>Murphy</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Mapping the MRI voxel volume in which thermal noise matches physiological noise—Implications for fMRI</article-title><source>NeuroImage</source><volume>34</volume><fpage>542</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.09.039</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Gatys</surname><given-names>LA</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Deep convolutional models improve predictions of macaque V1 responses to natural images</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006897</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006897</pub-id><pub-id pub-id-type="pmid">31013278</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>How well do deep neural networks trained on object recognition characterize the Mouse visual system?</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>MB</given-names></name><name><surname>Schuck</surname><given-names>NW</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Representational structure or task structure? Bias in neural representational similarity analysis and a Bayesian method for reducing bias</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006299</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006299</pub-id><pub-id pub-id-type="pmid">31125335</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlin</surname><given-names>JD</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Adjudicating between face-coding models with individual-face fMRI responses</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005604</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005604</pub-id><pub-id pub-id-type="pmid">28746335</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaimow</surname><given-names>D</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Uğurbil</surname><given-names>K</given-names></name><name><surname>Shmuel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Spatial specificity of the functional MRI blood oxygenation response relative to neuronal activity</article-title><source>NeuroImage</source><volume>164</volume><fpage>32</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.077</pub-id><pub-id pub-id-type="pmid">28882632</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>DD</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Classification and geometry of general perceptual manifolds</article-title><source>Physical Review X</source><volume>8</volume><elocation-id>031003</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevX.8.031003</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>S</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural population geometry: An approach for understanding biological and artificial neural networks</article-title><source>Current Opinion in Neurobiology</source><volume>70</volume><fpage>137</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2021.10.010</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id><pub-id pub-id-type="pmid">24464044</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>27755</elocation-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id><pub-id pub-id-type="pmid">27282108</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Roig</surname><given-names>G</given-names></name><name><surname>Andonian</surname><given-names>A</given-names></name><name><surname>Dwivedi</surname><given-names>K</given-names></name><name><surname>Lahner</surname><given-names>B</given-names></name><name><surname>Lascelles</surname><given-names>A</given-names></name><name><surname>Mohsenzadeh</surname><given-names>Y</given-names></name><name><surname>Ramakrishnan</surname><given-names>K</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The Algonauts Project: A Platform for Communication between the Sciences of Biological and Artificial Intelligence</article-title><conf-name>2019 Conference on Cognitive Computational Neuroscience</conf-name><pub-id pub-id-type="doi">10.32470/CCN.2019.1018-0</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Dwivedi</surname><given-names>K</given-names></name><name><surname>Lahner</surname><given-names>B</given-names></name><name><surname>Lascelles</surname><given-names>A</given-names></name><name><surname>Iamshchinina</surname><given-names>P</given-names></name><name><surname>Graumann</surname><given-names>M</given-names></name><name><surname>Andonian</surname><given-names>A</given-names></name><name><surname>Murty</surname><given-names>NAR</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name><name><surname>Roig</surname><given-names>G</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The Algonauts Project 2021 Challenge: How the Human Brain Makes Sense of a World in Motion</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2104.13714">https://arxiv.org/abs/2104.13714</ext-link><pub-id pub-id-type="doi">10.48550/ARXIV.2104.13714</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Guntupalli</surname><given-names>JS</given-names></name><name><surname>Gors</surname><given-names>J</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Wu</surname><given-names>YC</given-names></name><name><surname>Abdi</surname><given-names>H</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The representation of biological classes in the human brain</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>2608</fpage><lpage>2618</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5547-11.2012</pub-id><pub-id pub-id-type="pmid">22357845</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name><name><surname>Hyde</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Software tools for analysis and visualization of fMRI data</article-title><source>NMR in Biomedicine</source><volume>10</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1002/(sici)1099-1492(199706/08)10:4/5&lt;171::aid-nbm453&gt;3.0.co;2-l</pub-id><pub-id pub-id-type="pmid">9430344</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Craik</surname><given-names>A</given-names></name><name><surname>He</surname><given-names>Y</given-names></name><name><surname>Contreras-Vidal</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep learning for electroencephalogram (EEG) classification tasks: A review</article-title><source>Journal of Neural Engineering</source><volume>16</volume><elocation-id>031001</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/ab0ab5</pub-id><pub-id pub-id-type="pmid">30808014</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical Surface-Based Analysis</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id><pub-id pub-id-type="pmid">9931268</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vries</surname><given-names>SEJ</given-names></name><name><surname>Lecoq</surname><given-names>JA</given-names></name><name><surname>Buice</surname><given-names>MA</given-names></name><name><surname>Groblewski</surname><given-names>PA</given-names></name><name><surname>Ocker</surname><given-names>GK</given-names></name><name><surname>Oliver</surname><given-names>M</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Millman</surname><given-names>D</given-names></name><name><surname>Roll</surname><given-names>K</given-names></name><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Keenan</surname><given-names>T</given-names></name><name><surname>Kuan</surname><given-names>L</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Olsen</surname><given-names>S</given-names></name><name><surname>Thompson</surname><given-names>C</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Waters</surname><given-names>J</given-names></name><name><surname>Williams</surname><given-names>D</given-names></name><name><surname>Barber</surname><given-names>C</given-names></name><name><surname>Berbesque</surname><given-names>N</given-names></name><name><surname>Blanchard</surname><given-names>B</given-names></name><name><surname>Bowles</surname><given-names>N</given-names></name><name><surname>Caldejon</surname><given-names>SD</given-names></name><name><surname>Casal</surname><given-names>L</given-names></name><name><surname>Cho</surname><given-names>A</given-names></name><name><surname>Cross</surname><given-names>S</given-names></name><name><surname>Dang</surname><given-names>C</given-names></name><name><surname>Dolbeare</surname><given-names>T</given-names></name><name><surname>Edwards</surname><given-names>M</given-names></name><name><surname>Galbraith</surname><given-names>J</given-names></name><name><surname>Gaudreault</surname><given-names>N</given-names></name><name><surname>Gilbert</surname><given-names>TL</given-names></name><name><surname>Griffin</surname><given-names>F</given-names></name><name><surname>Hargrave</surname><given-names>P</given-names></name><name><surname>Howard</surname><given-names>R</given-names></name><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Jewell</surname><given-names>S</given-names></name><name><surname>Keller</surname><given-names>N</given-names></name><name><surname>Knoblich</surname><given-names>U</given-names></name><name><surname>Larkin</surname><given-names>JD</given-names></name><name><surname>Larsen</surname><given-names>R</given-names></name><name><surname>Lau</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>F</given-names></name><name><surname>Leon</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Long</surname><given-names>F</given-names></name><name><surname>Luviano</surname><given-names>J</given-names></name><name><surname>Mace</surname><given-names>K</given-names></name><name><surname>Nguyen</surname><given-names>T</given-names></name><name><surname>Perkins</surname><given-names>J</given-names></name><name><surname>Robertson</surname><given-names>M</given-names></name><name><surname>Seid</surname><given-names>S</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name><name><surname>Shi</surname><given-names>J</given-names></name><name><surname>Sjoquist</surname><given-names>N</given-names></name><name><surname>Slaughterbeck</surname><given-names>C</given-names></name><name><surname>Sullivan</surname><given-names>D</given-names></name><name><surname>Valenza</surname><given-names>R</given-names></name><name><surname>White</surname><given-names>C</given-names></name><name><surname>Williford</surname><given-names>A</given-names></name><name><surname>Witten</surname><given-names>DM</given-names></name><name><surname>Zhuang</surname><given-names>J</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Farrell</surname><given-names>C</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Phillips</surname><given-names>JW</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>138</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0550-9</pub-id><pub-id pub-id-type="pmid">31844315</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diedrichsen</surname><given-names>J</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Representational models: A common framework for understanding encoding, pattern-component, and representational-similarity analysis</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005508</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005508</pub-id><pub-id pub-id-type="pmid">28437426</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diedrichsen</surname><given-names>J</given-names></name><name><surname>Yokoi</surname><given-names>A</given-names></name><name><surname>Arbuckle</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Pattern component modeling: A flexible approach for understanding the representational structure of brain activity patterns</article-title><source>NeuroImage</source><volume>180</volume><fpage>119</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.051</pub-id><pub-id pub-id-type="pmid">28843540</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diedrichsen</surname><given-names>J</given-names></name><name><surname>Berlot</surname><given-names>E</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Schütt</surname><given-names>HH</given-names></name><name><surname>Shahbazi</surname><given-names>M</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Comparing representational geometries using whitened unbiased-distance-matrix similarity</article-title><source>Neurons, Behavior, Data Analysis, and Theory</source><volume>5</volume><elocation-id>27664</elocation-id><pub-id pub-id-type="doi">10.51628/001c.27664</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Population receptive field estimates in human visual cortex</article-title><source>NeuroImage</source><volume>39</volume><fpage>647</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.034</pub-id><pub-id pub-id-type="pmid">17977024</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edelman</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Representation is representation of similarities</article-title><source>The Behavioral and Brain Sciences</source><volume>21</volume><fpage>449</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1017/s0140525x98001253</pub-id><pub-id pub-id-type="pmid">10097019</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edelman</surname><given-names>S</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kushnir</surname><given-names>T</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Toward direct visualization of the internal shape representation space by fMRI</article-title><source>Psychobiology</source><volume>26</volume><fpage>309</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.3758/BF03330618</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Efron</surname><given-names>B</given-names></name><name><surname>Tibshirani</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>An Introduction to the Bootstrap</source><publisher-name>CRC press</publisher-name><pub-id pub-id-type="doi">10.1201/9780429246593</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ejaz</surname><given-names>N</given-names></name><name><surname>Hamada</surname><given-names>M</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hand use predicts the structure of representations in sensorimotor cortex</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1034</fpage><lpage>1040</lpage><pub-id pub-id-type="doi">10.1038/nn.4038</pub-id><pub-id pub-id-type="pmid">26030847</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>fMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nature Methods</source><volume>16</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id><pub-id pub-id-type="pmid">30532080</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonov</surname><given-names>V</given-names></name><name><surname>Evans</surname><given-names>A</given-names></name><name><surname>McKinstry</surname><given-names>R</given-names></name><name><surname>Almli</surname><given-names>C</given-names></name><name><surname>Collins</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood</article-title><source>NeuroImage</source><volume>47</volume><elocation-id>S102</elocation-id><pub-id pub-id-type="doi">10.1016/S1053-8119(09)70884-5</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>JB</given-names></name><name><surname>Stolier</surname><given-names>RM</given-names></name><name><surname>Brooks</surname><given-names>JA</given-names></name><name><surname>Stillerman</surname><given-names>BS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The neural representational geometry of social perception</article-title><source>Current Opinion in Psychology</source><volume>24</volume><fpage>83</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/j.copsyc.2018.10.003</pub-id><pub-id pub-id-type="pmid">30388494</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name><name><surname>Holmes</surname><given-names>E</given-names></name><name><surname>Zeidman</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Variational representational similarity analysis</article-title><source>NeuroImage</source><volume>201</volume><elocation-id>115986</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.06.064</pub-id><pub-id pub-id-type="pmid">31255808</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Coalson</surname><given-names>TS</given-names></name><name><surname>Robinson</surname><given-names>EC</given-names></name><name><surname>Hacker</surname><given-names>CD</given-names></name><name><surname>Harwell</surname><given-names>J</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Andersson</surname><given-names>J</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A multi-modal parcellation of human cerebral cortex</article-title><source>Nature</source><volume>536</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1038/nature18933</pub-id><pub-id pub-id-type="pmid">27437579</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K</given-names></name><name><surname>Burns</surname><given-names>CD</given-names></name><name><surname>Madison</surname><given-names>C</given-names></name><name><surname>Clark</surname><given-names>D</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Waskom</surname><given-names>ML</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in python</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id><pub-id pub-id-type="pmid">21897815</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Ziegler</surname><given-names>E</given-names></name><name><surname>Ellis</surname><given-names>DG</given-names></name><name><surname>Notter</surname><given-names>MP</given-names></name><name><surname>Jarecka</surname><given-names>D</given-names></name><name><surname>Johnson</surname><given-names>H</given-names></name><name><surname>Burns</surname><given-names>C</given-names></name><name><surname>Manhães-Savio</surname><given-names>A</given-names></name><name><surname>Hamalainen</surname><given-names>C</given-names></name><name><surname>Yvernault</surname><given-names>B</given-names></name><name><surname>Salo</surname><given-names>T</given-names></name><name><surname>Jordan</surname><given-names>K</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>Waskom</surname><given-names>M</given-names></name><name><surname>Clark</surname><given-names>D</given-names></name><name><surname>Wong</surname><given-names>J</given-names></name><name><surname>Loney</surname><given-names>F</given-names></name><name><surname>Modat</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Nipype</data-title><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.596855">https://doi.org/10.5281/zenodo.596855</ext-link></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate and robust brain image alignment using boundary-based registration</article-title><source>NeuroImage</source><volume>48</volume><fpage>63</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Ji</surname><given-names>B</given-names></name><name><surname>Xi</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>B</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name></person-group><article-title>Flexible, Multi-Shank Stacked Array for High-Density Omini-Directional Intracortical Recording</article-title><conf-name>2021 IEEE 34th International Conference on Micro Electro Mechanical Systems (MEMS).</conf-name><year iso-8601-date="2021">2021</year><fpage>540</fpage><lpage>543</lpage><pub-id pub-id-type="doi">10.1109/MEMS51782.2021.9375160</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Guntupalli</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Decoding neural representational spaces using multivariate pattern analysis</article-title><source>Annual Review of Neuroscience</source><volume>37</volume><fpage>435</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062012-170325</pub-id><pub-id pub-id-type="pmid">25002277</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Görgen</surname><given-names>K</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The Decoding Toolbox (TDT): a versatile software package for multivariate analyses of functional imaging data</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>88</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00088</pub-id><pub-id pub-id-type="pmid">25610393</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horikawa</surname><given-names>T</given-names></name><name><surname>Kamitani</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Generic decoding of seen and imagined objects using hierarchical visual features</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15037</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15037</pub-id><pub-id pub-id-type="pmid">28530228</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Bannister</surname><given-names>P</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title><source>NeuroImage</source><volume>17</volume><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1016/s1053-8119(02)91132-8</pub-id><pub-id pub-id-type="pmid">12377157</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jozwik</surname><given-names>KM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual features as stepping stones toward semantics: Explaining object similarity in IT and perception with non-negative least squares</article-title><source>Neuropsychologia</source><volume>83</volume><fpage>201</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.10.023</pub-id><pub-id pub-id-type="pmid">26493748</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>JJ</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Denman</surname><given-names>DJ</given-names></name><name><surname>Bauza</surname><given-names>M</given-names></name><name><surname>Barbarits</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Anastassiou</surname><given-names>CA</given-names></name><name><surname>Andrei</surname><given-names>A</given-names></name><name><surname>Aydın</surname><given-names>Ç</given-names></name><name><surname>Barbic</surname><given-names>M</given-names></name><name><surname>Blanche</surname><given-names>TJ</given-names></name><name><surname>Bonin</surname><given-names>V</given-names></name><name><surname>Couto</surname><given-names>J</given-names></name><name><surname>Dutta</surname><given-names>B</given-names></name><name><surname>Gratiy</surname><given-names>SL</given-names></name><name><surname>Gutnisky</surname><given-names>DA</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Lopez</surname><given-names>CM</given-names></name><name><surname>Mitelut</surname><given-names>C</given-names></name><name><surname>Musa</surname><given-names>S</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Putzeys</surname><given-names>J</given-names></name><name><surname>Rich</surname><given-names>PD</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Sun</surname><given-names>W-L</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title><source>Nature</source><volume>551</volume><fpage>232</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature24636</pub-id><pub-id pub-id-type="pmid">29120427</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Prenger</surname><given-names>RJ</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Identifying natural images from human brain activity</article-title><source>Nature</source><volume>452</volume><fpage>352</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1038/nature06713</pub-id><pub-id pub-id-type="pmid">18322462</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kell</surname><given-names>AJE</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Shook</surname><given-names>EN</given-names></name><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy</article-title><source>Neuron</source><volume>98</volume><fpage>630</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.044</pub-id><pub-id pub-id-type="pmid">29681533</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemeny</surname><given-names>JG</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Mathematics without numbers</article-title><source>Daedalus</source><volume>88</volume><fpage>577</fpage><lpage>591</lpage></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kendall</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="1948">1948</year><source>Rank Correlation Methods</source><publisher-name>Griffin</publisher-name></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>SM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>SM</given-names></name><name><surname>Henriksson</surname><given-names>L</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fixed versus mixed RSA: Explaining visual representations by fixed and mixed feature sets from shallow and deep computational models</article-title><source>Journal of Mathematical Psychology</source><volume>76</volume><fpage>184</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.10.007</pub-id><pub-id pub-id-type="pmid">28298702</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Sörensen</surname><given-names>LKA</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title><source>PNAS</source><volume>116</volume><fpage>21854</fpage><lpage>21863</lpage><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Kipnis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Fmri-simulations</data-title><version designator="2fd77d1">2fd77d1</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/adkipnis/fmri-simulations">https://github.com/adkipnis/fmri-simulations</ext-link></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>A</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Bao</surname><given-names>FS</given-names></name><name><surname>Giard</surname><given-names>J</given-names></name><name><surname>Häme</surname><given-names>Y</given-names></name><name><surname>Stavsky</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>N</given-names></name><name><surname>Rossa</surname><given-names>B</given-names></name><name><surname>Reuter</surname><given-names>M</given-names></name><name><surname>Chaibub Neto</surname><given-names>E</given-names></name><name><surname>Keshavan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mindboggling morphometry of human brains</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005350</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005350</pub-id><pub-id pub-id-type="pmid">28231282</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A self-supervised domain-general learning framework for human ventral stream representation</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>491</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-28091-4</pub-id><pub-id pub-id-type="pmid">35078981</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Norouzi</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Similarity of Neural Network Representations Revisited</article-title><conf-name>Proceedings of the 36th International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Information-based functional brain mapping</article-title><source>PNAS</source><volume>103</volume><fpage>3863</fpage><lpage>3868</lpage><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name><name><surname>Sorger</surname><given-names>B</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Individual faces elicit distinct response patterns in human anterior temporal cortex</article-title><source>PNAS</source><volume>104</volume><fpage>20600</fpage><lpage>20605</lpage><pub-id pub-id-type="doi">10.1073/pnas.0705654104</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008a</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008b</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Inverse MDS: Inferring dissimilarity structure from multiple item arrangements</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>245</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00245</pub-id><pub-id pub-id-type="pmid">22848204</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational geometry: integrating cognition, computation, and the brain</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks: A new framework for modeling biological vision and brain information processing</article-title><source>Annual Review of Vision Science</source><volume>1</volume><fpage>417</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035447</pub-id><pub-id pub-id-type="pmid">28532370</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Inferring brain-computational mechanisms with models of activity measurements</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>371</volume><elocation-id>20160278</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0278</pub-id><pub-id pub-id-type="pmid">27574316</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Douglas</surname><given-names>PK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cognitive computational neuroscience</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1148</fpage><lpage>1160</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0210-5</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Peeling the onion of brain representations</article-title><source>Annual Review of Neuroscience</source><volume>42</volume><fpage>407</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-080317-061906</pub-id><pub-id pub-id-type="pmid">31283895</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Douglas</surname><given-names>PK</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Interpreting encoding and decoding models</article-title><source>Current Opinion in Neurobiology</source><volume>55</volume><fpage>167</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.04.002</pub-id><pub-id pub-id-type="pmid">31039527</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Wei</surname><given-names>XX</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural tuning and representational geometry</article-title><source>Nature Reviews. Neuroscience</source><volume>22</volume><fpage>703</fpage><lpage>718</lpage><pub-id pub-id-type="doi">10.1038/s41583-021-00502-3</pub-id><pub-id pub-id-type="pmid">34522043</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imagenet classification with deep convolutional neural networks</article-title><conf-name>Advances in neural information processing systems</conf-name></element-citation></ref><ref id="bib76"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Majaj</surname><given-names>N</given-names></name><name><surname>Issa</surname><given-names>E</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Prescott-Roy</surname><given-names>J</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Bear</surname><given-names>D</given-names></name><name><surname>Yamins</surname><given-names>DL</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><chapter-title>Brain-like object recognition with high-performing shallow recurrent Anns</chapter-title><person-group person-group-type="editor"><name><surname>Wallach</surname><given-names>H</given-names></name><name><surname>Larochelle</surname><given-names>H</given-names></name><name><surname>Beygelzimer</surname><given-names>A</given-names></name><name><surname>Alché-Buc</surname><given-names>F</given-names></name><name><surname>Fox</surname><given-names>E</given-names></name><name><surname>Garnett</surname><given-names>R</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><fpage>1</fpage><lpage>12</lpage></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lanczos</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>Evaluation of Noisy Data</article-title><source>Journal of the Society for Industrial and Applied Mathematics Series B Numerical Analysis</source><volume>1</volume><fpage>76</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1137/0701007</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ledoit</surname><given-names>O</given-names></name><name><surname>Wolf</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Honey, i shrunk the sample covariance matrix</article-title><source>The Journal of Portfolio Management</source><volume>30</volume><fpage>110</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.3905/jpm.2004.110</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mehrer</surname><given-names>J</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Kriegeskorte</surname><given-names>N.</given-names></name></person-group><article-title>Deep neural networks trained on ecologically relevant categories better explain human IT</article-title><conf-name>Conference on Cognitive Computational Neuroscience</conf-name><year iso-8601-date="2017">2017</year></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehrer</surname><given-names>J</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Jones</surname><given-names>EC</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>An ecologically motivated image dataset for deep learning yields better models of human vision</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2011417118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2011417118</pub-id><pub-id pub-id-type="pmid">33593900</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Encoding and decoding in fMRI</article-title><source>NeuroImage</source><volume>56</volume><fpage>400</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.073</pub-id><pub-id pub-id-type="pmid">20691790</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Wingfield</surname><given-names>C</given-names></name><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Su</surname><given-names>L</given-names></name><name><surname>Marslen-Wilson</surname><given-names>W</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A toolbox for representational similarity analysis</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003553</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003553</pub-id><pub-id pub-id-type="pmid">24743308</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Inferring exemplar discriminability in brain representations</article-title><source>PLOS ONE</source><volume>15</volume><elocation-id>e0232551</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0232551</pub-id><pub-id pub-id-type="pmid">32520962</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Polyn</surname><given-names>SM</given-names></name><name><surname>Detre</surname><given-names>GJ</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Beyond mind-reading: multi-voxel pattern analysis of fMRI data</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>424</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.07.005</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parvizi</surname><given-names>J</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Promises and limitations of human intracranial electroencephalography</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>474</fpage><lpage>483</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0108-2</pub-id><pub-id pub-id-type="pmid">29507407</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Gimelshein</surname><given-names>N</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name><name><surname>Desmaison</surname><given-names>A</given-names></name><name><surname>Kopf</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>DeVito</surname><given-names>Z</given-names></name><name><surname>Raison</surname><given-names>M</given-names></name><name><surname>Tejani</surname><given-names>A</given-names></name><name><surname>Chilamkurthy</surname><given-names>S</given-names></name><name><surname>Steiner</surname><given-names>B</given-names></name><name><surname>Fang</surname><given-names>L</given-names></name><name><surname>Bai</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><chapter-title>Pytorch: an imperative style, high-performance deep learning library</chapter-title><person-group person-group-type="editor"><name><surname>Wallach</surname><given-names>H</given-names></name><name><surname>Larochelle</surname><given-names>H</given-names></name><name><surname>Beygelzimer</surname><given-names>A</given-names></name><name><surname>d’</surname><given-names>F</given-names></name><name><surname>Fox</surname><given-names>E</given-names></name><name><surname>Garnett</surname><given-names>R</given-names></name></person-group><source>Advances in Neural Information Processing System</source><publisher-name>Curran Associates, Inc</publisher-name><fpage>8024</fpage><lpage>8035</lpage></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Ciuciu</surname><given-names>P</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Data-driven HRF estimation for encoding and decoding models</article-title><source>NeuroImage</source><volume>104</volume><fpage>209</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.09.060</pub-id><pub-id pub-id-type="pmid">25304775</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Mitra</surname><given-names>A</given-names></name><name><surname>Laumann</surname><given-names>TO</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Methods to detect, characterize, and remove motion artifact in resting state fMRI</article-title><source>NeuroImage</source><volume>84</volume><fpage>320</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.048</pub-id><pub-id pub-id-type="pmid">23994314</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramírez</surname><given-names>FM</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Allefeld</surname><given-names>C</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The neural code for face orientation in the human fusiform face area</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>12155</fpage><lpage>12167</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3156-13.2014</pub-id><pub-id pub-id-type="pmid">25186759</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchie</surname><given-names>JB</given-names></name><name><surname>Lee Masson</surname><given-names>H</given-names></name><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The unreliable influence of multivariate noise normalization on the reliability of neural dissimilarity</article-title><source>NeuroImage</source><volume>245</volume><elocation-id>118686</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118686</pub-id><pub-id pub-id-type="pmid">34728244</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Satterthwaite</surname><given-names>TD</given-names></name><name><surname>Elliott</surname><given-names>MA</given-names></name><name><surname>Gerraty</surname><given-names>RT</given-names></name><name><surname>Ruparel</surname><given-names>K</given-names></name><name><surname>Loughead</surname><given-names>J</given-names></name><name><surname>Calkins</surname><given-names>ME</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Hakonarson</surname><given-names>H</given-names></name><name><surname>Gur</surname><given-names>RC</given-names></name><name><surname>Gur</surname><given-names>RE</given-names></name><name><surname>Wolf</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>An improved framework for confound regression and filtering for control of motion artifact in the preprocessing of resting-state functional connectivity data</article-title><source>NeuroImage</source><volume>64</volume><fpage>240</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.08.052</pub-id><pub-id pub-id-type="pmid">22926292</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schäfer</surname><given-names>J</given-names></name><name><surname>Strimmer</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics</article-title><source>Statistical Applications in Genetics and Molecular Biology</source><volume>4</volume><elocation-id>Article32</elocation-id><pub-id pub-id-type="doi">10.2202/1544-6115.1175</pub-id><pub-id pub-id-type="pmid">16646851</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Schütt</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Representational Similarity Analysis 3.0</data-title><version designator="swh:1:rev:01e767c432e77633fe31304201718afce6a6ff9c">swh:1:rev:01e767c432e77633fe31304201718afce6a6ff9c</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:60193eeb851ac071f455e3d5db14cd8baeae20fa;origin=https://github.com/rsagroup/rsatoolbox;visit=swh:1:snp:94465c330bf41b107120efa10a9829c4ba5a2b91;anchor=swh:1:rev:01e767c432e77633fe31304201718afce6a6ff9c">https://archive.softwareheritage.org/swh:1:dir:60193eeb851ac071f455e3d5db14cd8baeae20fa;origin=https://github.com/rsagroup/rsatoolbox;visit=swh:1:snp:94465c330bf41b107120efa10a9829c4ba5a2b91;anchor=swh:1:rev:01e767c432e77633fe31304201718afce6a6ff9c</ext-link></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><name><surname>Churchland</surname><given-names>PS</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Putting big data to good use in neuroscience</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1440</fpage><lpage>1441</lpage><pub-id pub-id-type="doi">10.1038/nn.3839</pub-id><pub-id pub-id-type="pmid">25349909</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepard</surname><given-names>RN</given-names></name><name><surname>Chipman</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>Second-order isomorphism of internal representations: Shapes of states</article-title><source>Cognitive Psychology</source><volume>1</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(70)90002-2</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Statistical challenges in “big data” human neuroimaging</article-title><source>Neuron</source><volume>97</volume><fpage>263</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.12.018</pub-id><pub-id pub-id-type="pmid">29346749</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Challenges and opportunities for large-scale electrophysiology with Neuropixels probes</article-title><source>Current Opinion in Neurobiology</source><volume>50</volume><fpage>92</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.01.009</pub-id><pub-id pub-id-type="pmid">29444488</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevenson</surname><given-names>IH</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>How advances in neural recording affect data analysis</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>139</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1038/nn.2731</pub-id><pub-id pub-id-type="pmid">21270781</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Khaligh-Razavi</surname><given-names>SM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Noise Ceiling on the Crossvalidated Performance of Reweighted Models of Representational Dissimilarity: Addendum to Khaligh-Razavi &amp; Kriegeskorte (2014)</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.03.23.003046</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Mehrer</surname><given-names>J</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Diverse deep neural networks all predict human inferior temporal cortex well, after training and fitting</article-title><source>Journal of Cognitive Neuroscience</source><volume>33</volume><fpage>2044</fpage><lpage>2064</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01755</pub-id><pub-id pub-id-type="pmid">34272948</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>High-dimensional geometry of population responses in visual cortex</article-title><source>Nature</source><volume>571</volume><fpage>361</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1346-5</pub-id><pub-id pub-id-type="pmid">31243367</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Székely</surname><given-names>GJ</given-names></name><name><surname>Rizzo</surname><given-names>ML</given-names></name><name><surname>Bakirov</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Measuring and testing dependence by correlation of distances</article-title><source>The Annals of Statistics</source><volume>35</volume><fpage>2769</fpage><lpage>2794</lpage><pub-id pub-id-type="doi">10.1214/009053607000000505</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>F</given-names></name><name><surname>Pratte</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decoding patterns of human brain activity</article-title><source>Annual Review of Psychology</source><volume>63</volume><fpage>483</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-120710-100412</pub-id><pub-id pub-id-type="pmid">21943172</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>NJ</given-names></name><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Cook</surname><given-names>PA</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Egan</surname><given-names>A</given-names></name><name><surname>Yushkevich</surname><given-names>PA</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>N4ITK: improved N3 bias correction</article-title><source>IEEE Transactions on Medical Imaging</source><volume>29</volume><fpage>1310</fpage><lpage>1320</lpage><pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id><pub-id pub-id-type="pmid">20378467</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uğurbil</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Ultrahigh field and ultrahigh resolution fMRI</article-title><source>Current Opinion in Biomedical Engineering</source><volume>18</volume><elocation-id>100288</elocation-id><pub-id pub-id-type="doi">10.1016/j.cobme.2021.100288</pub-id><pub-id pub-id-type="pmid">33987482</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Ejaz</surname><given-names>N</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title><source>NeuroImage</source><volume>137</volume><fpage>188</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.012</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Computational neuroimaging and population receptive fields</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>349</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.03.009</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Xu</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Three-photon neuronal imaging in deep mouse brain</article-title><source>Optica</source><volume>7</volume><elocation-id>947</elocation-id><pub-id pub-id-type="doi">10.1364/OPTICA.395825</pub-id><pub-id pub-id-type="pmid">32905493</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weldon</surname><given-names>KB</given-names></name><name><surname>Olman</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Forging a path to mesoscopic imaging success with ultra-high field functional magnetic resonance imaging</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>376</volume><elocation-id>20200040</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2020.0040</pub-id><pub-id pub-id-type="pmid">33190599</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>MCK</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Complete functional characterization of sensory neurons by system identification</article-title><source>Annual Review of Neuroscience</source><volume>29</volume><fpage>477</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113024</pub-id><pub-id pub-id-type="pmid">16776594</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Limits to visual representational correspondence between convolutional neural networks and the human brain</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>2065</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-22244-7</pub-id><pub-id pub-id-type="pmid">33824315</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xue</surname><given-names>G</given-names></name><name><surname>Dong</surname><given-names>Q</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Mumford</surname><given-names>JA</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Greater neural pattern similarity across repetitions is associated with better memory</article-title><source>Science</source><volume>330</volume><fpage>97</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1126/science.1193125</pub-id><pub-id pub-id-type="pmid">20829453</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/nn.4244</pub-id><pub-id pub-id-type="pmid">26906502</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The generalizability crisis</article-title><source>The Behavioral and Brain Sciences</source><volume>45</volume><fpage>1</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1017/S0140525X20001685</pub-id><pub-id pub-id-type="pmid">33342451</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Young</surname><given-names>HP</given-names></name><name><surname>Levenglick</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A consistent extension of condorcet’s election principle</article-title><source>SIAM Journal on Applied Mathematics</source><volume>35</volume><fpage>285</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1137/0135023</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm</article-title><source>IEEE Transactions on Medical Imaging</source><volume>20</volume><fpage>45</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1109/42.906424</pub-id><pub-id pub-id-type="pmid">11293691</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>C</given-names></name><name><surname>Yan</surname><given-names>S</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Frank</surname><given-names>MC</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Unsupervised neural network models of the ventral visual stream</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2014196118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2014196118</pub-id><pub-id pub-id-type="pmid">33431673</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Matrix-normal simulations</title><p>To establish the validity of our model-comparative frequentist inference, we need to look at the false-alarm rate for data that is generated under the assumption that the null hypothesis <italic>H</italic><sub>0</sub> is true, that is that a model has chance performance in expectation or that two models, predict distinct RDMs, but achieve equal RDM prediction accuracy in expectation. In the deep-neural-network-based simulations and the data-resampling simulations in the main text, we are not able to generate such data. Here, we used a matrix-normal model, as a simpler simulation scheme for RSA in which we can enforce these null hypotheses.</p><p>We started by specifying a desired RDM for our data that fulfills the null hypothesis for the model(s). We then exploit the relationship between the RDM and the covariance matrix between conditions (<xref ref-type="bibr" rid="bib29">Diedrichsen and Kriegeskorte, 2017</xref>) to find a covariance matrix that results in the given RDM and generate responses with this covariance between conditions. This random pattern will then have the desired (squared Euclidean) RDM.</p><p>Concretely, the second-moment matrix <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> of inner products among condition-related patterns across voxels can be computed from the squared Euclidean-distance matrix <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as follows:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:msub><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a centering matrix with <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> being a square matrix of ones. A dataset with this covariance across conditions has <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as its squared Euclidean RDM (<xref ref-type="bibr" rid="bib29">Diedrichsen and Kriegeskorte, 2017</xref>). We can easily generate Gaussian data with a given second-moment matrix and can thus generate data with any desired RDM.</p><sec sec-type="appendix" id="s8-1"><title>Comparison against 0</title><p>To generate <italic>H</italic><sub>0</sub> data for testing our comparisons of models against 0, we choose both the model and the data RDMs as the distances between independent drawn Gaussian noise samples.</p></sec><sec sec-type="appendix" id="s8-2"><title>Model comparisons</title><p>To generate <italic>H</italic><sub>0</sub> data for model comparisons, we first generate two random model RDMs from independent standard normal noise data. We then normalize the model RDMs to have 0 mean and standard deviation of 1. Then we average the two RDMs, which yields a matrix with equal correlation to the two models. As a last step, we then subtract the minimum, to yield only positive distances and add the maximum distance to all distances once, such that the triangle inequality is guaranteed. As this last step only shifted the distance vector by a constant, the final distance vector still has the exact same correlation with the two model predictions. These methods effectively draw the covariance over conditions from a standardized Wishart distribution with as many degrees of freedom as the number of measurement channels.</p></sec><sec sec-type="appendix" id="s8-3"><title>Random conditions</title><p>To generate <italic>H</italic><sub>0</sub> data for model comparisons with variance due to stimulus selection, we created two models for a large set of 1000 conditions, and generated a data RDM and covariance matrix that would yield equal performance as for the other model-comparison simulations. We then sampled a random subset of the conditions for each simulated experiment.</p></sec><sec sec-type="appendix" id="s8-4"><title>Data generation</title><p>In all cases, we find a new configuration of data points that produce the desired RDM for each subject by converting the RDM into the second-moment matrix via <xref ref-type="disp-formula" rid="equ17">equation 17</xref> and drawing random normal data as described at the beginning of this section. We then add additional i.i.d. normal ‘measurement noise’ to each entry of the data matrix. From this data matrix we then compute a squared Euclidean-distance RDM per subject and use this as the data RDM to enter our inference process. Finally, we run our inference methods on these data RDMs and the original model RDMs to check whether the false-positive rate matches the nominal level.</p></sec><sec sec-type="appendix" id="s8-5"><title>Selected conditions</title><p>For each test and setting we generated 50 randomly drawn model RDMs and 100 datasets for each of these RDMs. We always used 200 measurement dimensions and tested all combinations of the following factors: 5, 10, 20, or 40 subjects, 5, 20, 80, or 160 conditions and all test types. As tests we used percentile tests and <inline-formula><mml:math id="inf142"><mml:mi>t</mml:mi></mml:math></inline-formula>-tests based on bootstrapping both dimensions, subjects only or conditions only, a standard <inline-formula><mml:math id="inf143"><mml:mi>t</mml:mi></mml:math></inline-formula>-test across subjects and a Wilcoxon rank-sum test. For the corrected bootstrap, we only used the <inline-formula><mml:math id="inf144"><mml:mi>t</mml:mi></mml:math></inline-formula>-test based on the estimated variances, because we cannot draw bootstrap samples based on our correction.</p></sec><sec sec-type="appendix" id="s8-6"><title>Results</title><p>All test results are shown in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>. They mostly turned out as expected. The classical <inline-formula><mml:math id="inf145"><mml:mi>t</mml:mi></mml:math></inline-formula>- and Wilcoxon tests performed very similar to the bootstrap tests based on subjects. For the tests against chance performance and the model comparisons with fixed conditions the false-positive rates are all close to the nominal 5%. However, we observed some inflated false-positive rates for the bootstraps at small sample sizes: About 7% when using the <inline-formula><mml:math id="inf146"><mml:mi>t</mml:mi></mml:math></inline-formula>-test and up to 12% for the simple percentile bootstrap test. These slightly too large false-positive rates are due to the bootstrap estimating the biased variance estimate (dividing by <inline-formula><mml:math id="inf147"><mml:mi>N</mml:mi></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf148"><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>). For more than 20 subjects, we cannot distinguish the percentage from 5% anymore. For the <inline-formula><mml:math id="inf149"><mml:mi>t</mml:mi></mml:math></inline-formula>-test and Wilcoxon rank-sum test, there were no such caveats as they consistently achieved a false-positive rate of about 5%.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Evaluation of the tests using normally distributed data simulated under different null hypotheses.</title><p>Each plot shows the false-positive rate plotted as a function of the number of subjects and conditions used. Ideal tests should fall on the dotted line at the nominal alpha level of 5%. Dots below the line indicate tests that are valid but conservative. Dots above the line are invalid. The ‘test against chance’ simulations (top row) evaluate tests of the ability of a model to predict RDMs. Data are simulated under the null hypothesis of no correlation between the data and model RDMs. A positive result would (erroneously) indicate that the model predicts the data RDM better than expected by chance. The ‘model comparison’ simulations (middle and bottom row) evaluate tests that compare the predictive accuracy of two models. Data are simulated under the null hypothesis that both models are equally good matches to the data. For the ‘fixed conditions’ simulations (middle row) this was enforced for the exact measured conditions. For the ‘random conditions’ simulations (bottom row) we instead generated models that are equally good on a large set of 1000 conditions, of which only a random subset of the given size is available for the inferential analysis.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82566-app1-fig1-v1.tif"/></fig><p>Once we introduce variance due to stimulus selection by random sampling of the conditions (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> third row), all methods based solely on the variance across subjects fail catastrophically with false-positive rates of up to 60% that grow with the number of tested subjects. This effect demonstrates the need to include the bootstrap across conditions into the evaluation.</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Sensitivity to model differences of different RDM comparators.</title><p>We used the data simulated on the basis of neural network representations of images to assess how well different models (neural network layer representations) can be discriminated for model-comparative inference when using different RDM comparators. We plot the model discriminability (signal-to-noise ratio, <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>) computed for the same simulated data for each RDM comparator and generalization objective (to new measurements of the same conditions in the same subjects: gray, to new measurements of the same conditions in new subjects: red, to new measurements of new conditions in the same subjects: blue, and to new measurements of new conditions in new subjects: purple). Because the condition-related variability dominated the simulated subject-related variability here, model discriminability is markedly higher (gray, red) when no generalization across conditions is attempted. The different rank-based RDM comparators <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> perform similarly and at least as well as the Pearson correlation (corr) and cosine similarity (cosine), while requiring fewer assumptions. This may motivate the use of the computationally efficient <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, which we introduce in Appendix C. Better sensitivity to model differences can be achieved using the whitened Pearson (whitened corr) and whitened cosine similarity (whitened cosine).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82566-app1-fig2-v1.tif"/></fig><p>When bootstrap resampling the conditions, the tests were conservative, achieving false-positive rates below 1%, lower than the nominal 5% (at the expense of power). This held whether or not subjects were treated as a random effect: The <inline-formula><mml:math id="inf152"><mml:mi>t</mml:mi></mml:math></inline-formula>-tests based on either the corrected or the uncorrected 2-factor bootstrap similarly had false-positive rates below 1%. This conservatism is expected for the tests against chance and the model comparisons with fixed conditions, because these simulations contained no true variation due to sampling of conditions. All techniques that include resampling the conditions also remain valid and conservative in the random conditions simulations that add some variation due to the condition choice. In particular, the corrected bootstrap remains conservative despite yielding strictly lower variance estimates than the uncorrected bootstrap.</p><p>We additionally ran a similar simulation, to test the tests against the noise ceiling, which is not displayed in the figure, but the results of this simulation are quickly summarized: We generated a single random model and used the same RDM also for data generation. In these data, the lower noise ceiling never significantly outperformed the true model indicating that the comparison against the lower noise ceiling is a very conservative test. This is most likely due to the difference between the lower bound on the noise ceiling and the true noise ceiling.</p></sec></sec><sec sec-type="appendix" id="s9"><title>Poisson KL-divergence</title><p>Instead of the Gaussian variability implied by the Euclidean and Mahalanobis dissimilarity measures, noise is often assumed to be Poisson or at least to have its variance increase linearly with mean activation. This is used primarily when the spiking variability of neurons is thought to be the main noise source as in electrophysiological recordings. For this case, we discuss two possible solutions.</p><p>The first alternative, discussed by <xref ref-type="bibr" rid="bib72">Kriegeskorte and Diedrichsen, 2019a</xref> is to use a variance stabilizing transform, that is to apply a square root to all dimensions of all representations and use an RDM based on the transformed values. This has the advantage, that the covariances can be taken into account.</p><p>The second alternative, which we introduce here, is to use a symmetrized KL-divergence of Poisson distributions with mean firing rates given by the feature values. This approach automatically takes the increased variance at larger activation levels into account and inherits nice information-theoretic and decoding-based interpretations from the KL-divergence.</p><p>The KL-divergence of two Poisson distributions with mean rates <inline-formula><mml:math id="inf153"><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf154"><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is given by:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:munderover><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Based on this we can compute the symmetrized version of the KL:<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mi>K</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To get a crossvalidated version of this dissimilarity we can calculate the difference in logarithms from one crossvalidation fold and the difference between raw values for a different fold and average across all pairs of different crossvalidation folds.</p><p>This KL-divergence-based dissimilarity is theoretically more interpretable than the square-root transform, but comes with two small drawbacks: First, the underlying firing rates cannot be 0 as a Poisson distribution which never fires is infinitely different from all others. This can be easily fixed by using a weak prior on the firing rate, which results in a non-zero estimated firing rate. Second, there is no straightforward way to include a noise covariance into the dissimilarity. While such noise correlations are much weaker than correlations between nearby voxels in fMRI or nearby electrodes in MEG, correlated noise may still reduce or enhance discriminability based on large neural populations (<xref ref-type="bibr" rid="bib8">Averbeck et al., 2006</xref>; <xref ref-type="bibr" rid="bib74">Kriegeskorte and Wei, 2021</xref>). There might be situations when the need to model noise correlations is a good reason to prefer the square-root transform.</p></sec><sec sec-type="appendix" id="s10"><title>Spearman’s <inline-formula><mml:math id="inf155"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula></title><p><xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref> recommended Kendall’s <inline-formula><mml:math id="inf156"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> as the RDM comparator over other rank correlation coefficients whenever any of the models predicts tied ranks. Kendall’s <inline-formula><mml:math id="inf157"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> does not prefer predictions with tied ranks over random orderings of the same entries in expectation, making it a valid RDM comparator when any model predicts the same dissimilarity for any pair of conditions. However, Kendall’s <inline-formula><mml:math id="inf158"><mml:mi>τ</mml:mi></mml:math></inline-formula>-type correlation coefficients are considerably slower to compute than Spearman’s <inline-formula><mml:math id="inf159"><mml:mi>ρ</mml:mi></mml:math></inline-formula>-type correlation coefficients. Moreover, finding the RDM with the highest average <inline-formula><mml:math id="inf160"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> for a given set of data RDMs (for computing noise ceilings) is equivalent to the Kemeny–Young method for preference voting (<xref ref-type="bibr" rid="bib54">Kemeny, 1959</xref>; <xref ref-type="bibr" rid="bib117">Young and Levenglick, 1978</xref>), which is NP-hard and in practice too slow to compute for our application (<xref ref-type="bibr" rid="bib4">Ali and Meilă, 2012</xref>).</p><p>Here, we propose using the expectation of Spearman’s <inline-formula><mml:math id="inf161"><mml:mi>ρ</mml:mi></mml:math></inline-formula> under random tie breaking as the RDM comparator instead. The coefficient <inline-formula><mml:math id="inf162"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> was described by <xref ref-type="bibr" rid="bib55">Kendall, 1948</xref>, chapter 3.8 and is derived below. For a vector <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, let <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> be the distribution of random-among-equals rank-transforms, where each unique value in <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is replaced with its integer rank and, in the case of a set of tied values, a random permutation of the corresponding ranks. For each draw <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>∼</mml:mo><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, thus, <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">⇒</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> . However, for pairs <inline-formula><mml:math id="inf168"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf169"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the ranks will fall in order <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> with equal probability. The set of values <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>n</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is <inline-formula><mml:math id="inf173"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. The <inline-formula><mml:math id="inf174"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> correlation coefficient is defined as:<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mstyle scriptlevel="1"><mml:mtable rowspacing="0.1em" columnspacing="0em" displaystyle="false"><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>∼</mml:mo><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>∼</mml:mo><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">[</mml:mo></mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For this expectation, we can derive a direct formula:<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mstyle scriptlevel="1"><mml:mtable rowspacing="0.1em" columnspacing="0em" displaystyle="false"><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>∼</mml:mo><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>∼</mml:mo><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">[</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>12</mml:mn><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi></mml:msup></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mn>12</mml:mn><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>3</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> contain the ranks of <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively, with tied values represented by tied average ranks. Thus, computing <inline-formula><mml:math id="inf179"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> does not require drawing actual random tie breaks to sample <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The RDM comparator <inline-formula><mml:math id="inf182"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> provides a general solution for rank-based evaluation that is correct in the presence of tied predictions and fast to compute. In addition, the mean of rank-transformed RDMs provides the best-fitting RDM, obviating the need for optimization and approximation in computing the noise ceiling.</p></sec><sec sec-type="appendix" id="s11"><title>Expected RDM under random feature weighting</title><p>If measurements weight features identically and independently, we can directly compute the expected squared Euclidean RDM for the measurements. We use this calculation both to justify a linear weighting model and to compute the correct models in some of our simulations.</p><p>Formally, we can show that this is true by the following calculation: Let <inline-formula><mml:math id="inf183"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> be the weighting for the <inline-formula><mml:math id="inf184"><mml:mi>i</mml:mi></mml:math></inline-formula> th feature in the <inline-formula><mml:math id="inf185"><mml:mi>v</mml:mi></mml:math></inline-formula> th voxel for two patterns <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with feature values <italic>x</italic><sub><italic>i</italic></sub> and <italic>y</italic><sub><italic>i</italic></sub>. Then the expected squared Euclidean distance in voxel space can be written as:<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>v</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>Var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>Var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This means that the expected RDM is a linear combination of the RDM based on individual features and the RDM based on the average across features weighted by the variance and the squared expected value of the weight distribution, respectively. As averaging or filtering across space is interchangeable with feature weighting, we can also use this calculation to compute the expected RDM for models that combine averaging over space and across features as in our simulations. Then the RDM at some level of averaging over space is still always a linear combination of the feature-averaged and feature-separate RDMs at that level of spatial averaging.</p></sec><sec sec-type="appendix" id="s12"><title>Choosing experimental design parameters for sensitive model adjudication</title><p>To quantify how much increasing the number of measurements along one of the experimental factors improved SNR for adjudication among models (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref>), we can use the slope of a regression line for the SNR against the number of measurements in log–log space. This slope corresponds to the exponent of the power-law relationship (<xref ref-type="fig" rid="fig4">Figure 4</xref> in the main text). We observe that increasing the number of conditions (<inline-formula><mml:math id="inf188"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.935</mml:mn></mml:mrow></mml:math></inline-formula>) is slightly more effective than increasing the number of subjects (<inline-formula><mml:math id="inf189"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.690</mml:mn></mml:mrow></mml:math></inline-formula>), and increasing the number of repeated measurements is most effective (<inline-formula><mml:math id="inf190"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.581</mml:mn></mml:mrow></mml:math></inline-formula>), probably due to the crossvalidation we employ. The crossvalidation across repeated measurements we use to yield unbiased distance estimates produces <inline-formula><mml:math id="inf191"><mml:mfrac><mml:mi>m</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula> times the variance in the original RDM entries compared to the biased estimates without crossvalidation. This provides an additional benefit for increasing the number of repeated independent measurements.</p><p>The model-discriminability SNR depends on the sources of nuisance variation included in the simulations. In these particular simulated experiments, resampling the conditions set induces more nuisance variation than resampling the subjects set (<xref ref-type="fig" rid="fig4">Figure 4g</xref>). This indicates that inference generalizing across conditions is harder than inference generalizing across subjects in these simulations. For small noise levels, the SNR is much higher when nothing is varied over repetitions or only subjects are varied than when the conditions are also varied. At large measurement noise levels this effect disappears, because the measurement noise becomes the dominant factor.</p><p>The intuition to explain our observations about the SNR is that it is most helpful to take more samples along the dimension which currently causes most variation in the results. Clearly, our variation in conditions caused more variance than our variation in voxel sampling to simulate subject variability. As a result, to boost the model-discriminability SNR, increasing the number of conditions is more effective than increasing the number of subjects by the same factor. Results also reveal that we simulated sufficiently high noise levels for a reduction in measurement noise through more repetitions to remain effective. Beyond noise reduction through averaging, more repetitions are also more profitable due to the crossvalidated distances, which loses less efficiency the larger the sample becomes <xref ref-type="bibr" rid="bib31">Diedrichsen et al., 2020</xref>.</p><p>Additionally, we observe that an intermediate voxel size (Gaussian kernel width) yields the highest model discriminability as measured by the SNR (<xref ref-type="fig" rid="fig4">Figure 4h</xref> in the main text). When each voxel averages over a large area, information in fine-grained patterns of activity is lost, which is detrimental to model selection. The fall-off for very small voxels in our simulations is due to randomly sampled voxels covering the feature map less well leading to greater variability. In real fMRI experiments, we do not expect this effect to play a role, as we expect voxels to always cover the whole-brain area, such that smaller voxels correspond to more voxels, which are clearly beneficial for better model selection. We do nonetheless expect a fall of for small voxel sizes for real fMRI experiments as well, because small voxel sizes lead to a steep increase in instrumental noise for fMRI and the BOLD signal itself is not perfectly local to the neurons that cause it (<xref ref-type="bibr" rid="bib12">Bodurka et al., 2007</xref>; <xref ref-type="bibr" rid="bib17">Chaimow et al., 2018</xref>; <xref ref-type="bibr" rid="bib110">Weldon and Olman, 2021</xref>). Thus, the dependence on voxel averaging size is what we expect for real fMRI experiments as well, albeit for different reasons. Also, it might be informative for other measurement methods like electrophysiology, that a local average can be preferable over perfectly local measurements for model selection, when the number of measured channels is limited.</p></sec><sec sec-type="appendix" id="s13"><title>Choosing an RDM comparator for sensitive model adjudication</title><p>An important question is how to measure RDM prediction accuracy for model evaluation. We ran the same analysis with different RDM comparators on the same datasets in a separate simulation.</p><p>We presented the deep neural network with our standard set of stimuli and simulated data for 10 subjects, 40 conditions, and 2 repeats, changing which parameters varied over repetitions of the experiment as in the main simulation. We omitted all bootstrapping, because the bootstrap variances are not needed to estimate model discriminability (SNR, <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>) for different RDM comparators. To improve comparability between different generalization conditions, we enforced that the first simulation for each generalization condition used the same conditions and subjects. The other 99 simulations then varied conditions and subject according to the required generalization. The different RDM comparators were applied to the same simulated experimental data.</p><p>We found that different types of rank correlation are all similarly good at discriminating models (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>). Proper evaluation of models predicting tied dissimilarities requires Kendall’s <inline-formula><mml:math id="inf192"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref>) or <inline-formula><mml:math id="inf193"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula>, a rarely used variant of Spearman’s rank correlation coefficient without correction for ties, analogous to Kendall’s <inline-formula><mml:math id="inf194"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> (derivation in Appendix 3). We recommend <inline-formula><mml:math id="inf195"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> over <inline-formula><mml:math id="inf196"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> for its lower computational cost and analytically derived noise ceiling.</p><p>If we are willing to assume that the representational dissimilarity estimates are on an interval scale, we expect to be able to achieve greater model-performance discriminability with RDM comparators that are not just sensitive to ranks. In this context, we compare the Pearson correlation and cosine similarity, and their whitened variants, which we introduced recently (<xref ref-type="bibr" rid="bib31">Diedrichsen et al., 2020</xref>). The whitened measures boost the power of inferential model comparisons, by accounting for the anisotropic sampling distribution of RDM estimates. To further increase our model-comparative power, both the whitened and the unwhitened cosine similarity assume a ratio scale for the representational dissimilarities, which requires that indistinguishable conditions have an expected dissimilarity of zero. This assumption is justified when using a crossvalidated distance estimator (<xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref>; <xref ref-type="bibr" rid="bib107">Walther et al., 2016</xref>), which provides unbiased dissimilarity estimates with an interpretable zero point.</p><p>Consistent with the theoretical expectations, we observe greatest model-performance discriminability for the whitened cosine similarity, which assumes ratio-scale dissimilarities, intermediate discriminability for the whitened Pearson correlation, and somewhat lower model-performance discriminability for the unwhitened Pearson correlation and the unwhitened cosine similarity. Rank correlation coefficients performed surprisingly well, matching or even outperforming unwhitened Pearson correlation and unwhitened cosine similarity (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>). They provide an attractive alternative to the whitened criteria when researchers wish to make weaker assumptions about their model predictions.</p></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82566.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Serences</surname><given-names>John T</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>University of California, San Diego</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>Schütt and colleagues introduce a new method for statistical inference on representational geometries based on a cross-validated two-factor bootstrap that allows for generalization across both participants and stimuli while allowing the fitting of flexible models. In a series of elegant simulations and empirical analyses on existing datasets, the authors validate the method statistically. The work provides a fundamental and compelling advance for the analysis of representational geometries.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82566.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Serences</surname><given-names>John T</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>University of California, San Diego</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Statistical Inference on Representational Geometries&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The authors appear to get lost in details and some of the key methods are hard to find in the Methods section. This will make the paper hard to follow for those not super familiar with the analysis approach.</p><p>Expanding the section on testing for the presence of information would be very useful to broaden the appeal.</p><p>Expanding the discussion and exposition of the generalizability of the statistical tests (see Reviewer #1).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Schütt and colleagues introduce a new method for statistical inference on representational geometries based on a cross-validated two-factor bootstrap that allows for generalization across both participants and stimuli while allowing the fitting of flexible models. In a series of elegant simulations and empirical analyses on existing datasets, the authors validate the method statistically.</p><p>Strengths:</p><p>– The authors are clearly experts on the methods, and the statistical approach significantly improves upon the state of the art of existing methods in terms of generalization across participants and stimuli.</p><p>– There is a potential for this method to not only become a new standard for analyses of representational geometries but to be applicable to different methodological approaches that not only aim at generalizing to new participants but also to new stimuli.</p><p>– The treatment of the topic is very thorough, with both extensive simulations as well as validation using functional MRI and calcium imaging data.</p><p>– The authors introduce a number of complex yet highly informative and useful new methodological advances, such as the (re)discovery of Spearman rho_a for improved comparison of dissimilarities as compared to Kendall's tau_a.</p><p>Weaknesses:</p><p>– Overall, while the introduction starts off very nicely, the manuscript ends up being rather difficult to read. The authors appear to get lost in details in the main text. Other critical methodological details are buried in the Methods section. Specifically, the key methodological advance, the two-factor bootstrap, is barely explained in the main text, and in my reading, it is never mentioned what data are bootstrapped (i.e. original data, rows and columns in the RDM, individual cells in the RDM).</p><p>– The authors assume a lot of knowledge from the reader, often referring to very recent work or preprints in a matter-of-fact kind of way. While this can be seen as a strength and highlights the timeliness of the work, the constant mix of more established and recent methods makes it much harder for the reader to understand what is actually introduced in this work. This separation is solved nicely in the introduction but does not appear to continue into the Results section.</p><p>– Representational similarity analysis is recommended by the authors to be used for model comparison. However, a very common, probably even more common, use case is to test for the presence of information (i.e. is the representational similarity &gt; 0), which, however, is only briefly discussed.</p><p>– The validity of the T-test based on bootstrap estimates for tests against chance seem to assume a null distribution for individual model-data comparisons that is centered around zero. However, negative similarities cannot be explained by population variance in the population null distribution, which is currently not discussed by the authors.</p><p>Are the claims of the authors justified?</p><p>– For comparisons between models, the claims of the authors clearly seem to be justified and reflect an important advance in the state-of-the-art statistical evaluation of representational geometries.</p><p>– That said, I believe that it is important to clarify the open statistical issue of generalizability to the population.</p><p>1. I really enjoyed reading this manuscript and believe it will make an important contribution to the field. That said, the authors introduce a lot in this work that is only indirectly related to the statistical analysis framework, and as a consequence, the manuscript is currently quite dense and hard to follow. I think that this manuscript would benefit strongly from a much more focused treatment of the key aspects (the introduction of the new method) and a reduction of the emphasis on advanced methods that are not key to this work (such as the use of reweighting and neuronal population sampling approaches, to name only a few).</p><p>I think the issue is that given the flexibility of analyzing representational geometries with RSA, the authors try to be as general as possible and try to encompass <italic>all</italic> possible use cases in their writing. In addition, the specific use case for a cross-validated two-factor bootstrap seems to be fitting flexible models, which alone is already quite advanced. I know this is difficult to solve, so I would like to provide one specific recommendation for making the manuscript easier to digest: it would perhaps help to first provide the reader with a quick run-through, without justifying all steps in detail but only summarizing the approach and the basic motivation for it. Then a more thorough treatment, including relevant parts from the methods section that explains the motivation behind the two-factor bootstrap could follow, again followed by extensive validation. This is just one suggestion for improving clarity.</p><p>2. Given the very common use of RSA for testing the presence of effects, rather than model comparison, I think the impact of this work would be strengthened if the authors expanded on their specific use case, even if it is comparably simple (they call this &quot;simple dependence test&quot;, which is perhaps a little confusing to the reader).</p><p>3. RSA measures the match of one or more model RDMs with a data RDM. For a test against chance, without very specific biases, a negative representational similarity should not be found empirically for subjects and only for a subset of stimuli. Any such effects should thus only be caused by measurement noise or by stimulus variability. I am wondering to what degree this affects the ability to carry out valid inferences against the null at the population level. See Allefeld et al. (2016) for the treatment of a similar problem with decoding accuracies.</p><p>4. The introduction would benefit from a better motivation of the method. It seems as if the authors discuss previous work on RDMs but then jump to the introduction of the new method. Did no other method exist before? What were the issues with these methods, and what is the gap that needs to be filled? This would help the reader better understand why they should be reading this work.</p><p>5. While valid, the approach appears to be rather conservative, producing very low false positive rates. Are false negatives not a potentially problematic issue in that respect?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>This paper addresses a major question in computational neuroscience by proposing a novel methodology to test models to explain behavioral/brain data that generalize across conditions and subjects using bootstrapping.</p><p>The experiments reported validate the claims of the authors. The methodology is applied and analysed in different available datasets.</p><p>I found particularly helpful and thorough the tests with the simulations. However, I found that the reported analysis is focused mainly on the newly proposed method, and this could bring a wider perspective into the picture.</p><p>It is with such simulated data that I believe a deeper discussion and possibly adding a comparison to existing methods, such as vanilla RSA and/or linear encoding methods could be reported to support the final discussion on the limitations of such existing methods. This would allow showcasing in which cases this method reveals new conclusions and has lower false positive rates, or in which cases there which method is limited to the experimental paradigm used to obtain the data (e.g., how many participants, repetitions, and conditions).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82566.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The authors appear to get lost in details and some of the key methods are hard to find in the Methods section. This will make the paper hard to follow for those not super familiar with the analysis approach.</p></disp-quote><p>We have restructured the manuscript to make it easier to follow. Concretely, we moved the methods parts that are newly introduced here into the main text. We collected all parts that give a general explanation of state of the art representational similarity analysis into a coherent material and methods section (5.1) for readers that are not familiar with the analysis approach yet. And to avoid getting lost in details, we pushed all parts that we did not deem central to our new methods into Appendices. We believe this separation makes the manuscript much more readable.</p><p>Additionally, we redistributed the flexible model figure, which one of the reviewers described as the hardest to follow. This is now separated into an early part that explains the method and math and a later section that reports our tests of the method.</p><disp-quote content-type="editor-comment"><p>Expanding the section on testing for the presence of information would be very useful to broaden the appeal.</p><p>Expanding the discussion and exposition of the generalizability of the statistical tests (see Reviewer #1).</p></disp-quote><p>We discuss this point in a bit more detail in the paper in the new section 3.3. We agree that this is a test that is frequently and sensibly employed and should be mentioned. In the case we are most interested in, this is a relatively low information test, as we should expect most of our models to be distinguishable from pure noise predictions.</p><p>We also discuss in this section, what kind of tests our methods cover exactly and which ones we don’t. This should hopefully make things clearer.</p><p>The new section 3.3. contains the following text:</p><p>“3.3 Supported tests and implications of test results</p><p>Our methods enable comparison of a model’s RDM prediction performance (1) against other models, (2) against the noise ceiling, and (3) against chance performance. The first two of these tests are central to the evaluation of models. The test against chance performance is often also reported, but represents a low bar that we should expect most models to pass. In practice, RDM correlations tend to be positive even for very different representations, because physically highly similar stimuli or conditions tend to be similar in all representations. Just like a significant Pearson correlation indicates a dependency, but does not demonstrate that the dependency is linear, a significant RDM prediction result indicates the presence of stimulus information, but does not lend strong support to the particular model. We should resist interpreting significant prediction performance per se as evidence for a particular model (the single-model-significance fallacy; Kriegeskorte and Douglas (2019)). Theoretical progress instead requires that each model be compared to alternative models and to the noise ceiling. An additional point to note is that the interpretation of chance performance, where the RDM comparator equals 0, depends on the chosen RDM comparator, differing, for example, between the Pearson correlation coefficient and the cosine similarity (Diedrichsen et al., 2020).</p><p>RDM comparators like the Pearson correlation and the cosine similarity are related to the distance correlation (Székely, Rizzo, and Bakirov, 2007), a general indicator of mutual information. Like a significant distance correlation, a significant RDM correlation mainly demonstrates that there is some mutual information between the brain region in question and the model representation. For a visual representation, for example, all that is required is for the two representations to contain some shared information about the input images. In contrast to the distance correlation (and other non-negative estimates of mutual information), however, negative RDM correlations can occur, indicating simply that pairs of stimuli close in one representation tend to be far in the other and vice versa. For any RDM, there is even a valid perfectly anti-correlated RDM (Pearson r = −1), which can be found by flipping the sign of all dissimilarities and adding a large enough value to make the RDM conform to the triangle inequality (which ensures the existence of an embedding of points that is consistent with the anti-correlated RDM). The existence of valid negative RDM correlations is important to the inferential methods presented here because it is required for our assumption of symmetric (t-)distributions around the true RDM correlation.</p><p>Omnibus tests for the presence of information about the experimental conditions in a brain region have been introduced in previous studies (e.g. Allefeld, Görgen, and Haynes, 2016; Kriegeskorte, Goebel, and Bandettini, 2006; Nili, Walther, Alink, and Kriegeskorte, 2020). Whether stimulus information is present in a region is closely related to the question whether the noise ceiling is significantly larger than 0, indicating RDM replicability. Such tests can sensitively detect small amounts of information in the measured activity patterns and can be helpful to assess whether there is any signal for model comparisons. If we are uncertain whether there is a reliable representational geometry to be explained, we need not bother with model comparisons.</p><p>The question whether an individual dissimilarity is significantly larger than zero is equivalent to the question whether the distinction between the two conditions can be decoded from the brain-activity. Decoding analyses can be used for this purpose (Hebart, Görgen, and Haynes, 2015; Kriegeskorte and Douglas, 2019; Naselaris, Kay, Nishimoto, and Gallant, 2011; Tong and Pratte, 2012). Such tests require care because the discriminability of two conditions cannot be systematically negative (Allefeld et al., 2016). This is in contrast to comparisons between RDMs, which can be systematically negative (although, as mentioned above, they tend to be positive in practice).”</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1. I really enjoyed reading this manuscript and believe it will make an important contribution to the field. That said, the authors introduce a lot in this work that is only indirectly related to the statistical analysis framework, and as a consequence, the manuscript is currently quite dense and hard to follow. I think that this manuscript would benefit strongly from a much more focused treatment of the key aspects (the introduction of the new method) and a reduction of the emphasis on advanced methods that are not key to this work (such as the use of reweighting and neuronal population sampling approaches, to name only a few).</p><p>I think the issue is that given the flexibility of analyzing representational geometries with RSA, the authors try to be as general as possible and try to encompass all possible use cases in their writing. In addition, the specific use case for a cross-validated two-factor bootstrap seems to be fitting flexible models, which alone is already quite advanced. I know this is difficult to solve, so I would like to provide one specific recommendation for making the manuscript easier to digest: it would perhaps help to first provide the reader with a quick run-through, without justifying all steps in detail but only summarizing the approach and the basic motivation for it. Then a more thorough treatment, including relevant parts from the methods section that explains the motivation behind the two-factor bootstrap could follow, again followed by extensive validation. This is just one suggestion for improving clarity.</p></disp-quote><p>As described in our response to the essential revisions above, we restructured the manuscript to make it less dense. In particular, we tried to move as many of the only indirectly related parts to appendices.</p><p>We also followed the reviewers suggestion to move the relevant methods sections into the main text and generated a Materials and methods section that gives a proper exposition to the state of the art RSA methods. As most of the content in that section is not new we decided to place this part into the Materials and methods rather than the Results section. Nonetheless, this part should fulfill the purpose of explaining advanced topics like the new evaluations metrics and flexible models to readers that are not familiar with them.</p><disp-quote content-type="editor-comment"><p>2. Given the very common use of RSA for testing the presence of effects, rather than model comparison, I think the impact of this work would be strengthened if the authors expanded on their specific use case, even if it is comparably simple (they call this &quot;simple dependence test&quot;, which is perhaps a little confusing to the reader).</p></disp-quote><p>We hope this point is resolved with our new section 3.3. And the answer we give to the general point above.</p><disp-quote content-type="editor-comment"><p>3. RSA measures the match of one or more model RDMs with a data RDM. For a test against chance, without very specific biases, a negative representational similarity should not be found empirically for subjects and only for a subset of stimuli. Any such effects should thus only be caused by measurement noise or by stimulus variability. I am wondering to what degree this affects the ability to carry out valid inferences against the null at the population level. See Allefeld et al. (2016) for the treatment of a similar problem with decoding accuracies.</p></disp-quote><p>The problem the reviewer highlights here is very important for tests for the presence of any information in the data. We do not cover this kind of test in this manuscript though. We are always testing model performances here and model performances can be reliably negative. We do see that this distinction is fairly subtle and that we did not state this clearly in our previous manuscript. To make this distinction clearer, we added some discussion on which exact tests we cover in this paper in our new section 3.3:</p><p>“In contrast to the distance correlation (and other non-negative estimates of mutual information), however, negative RDM correlations can occur, indicating simply that pairs of stimuli close in one representation tend to be far in the other and vice versa. For any RDM, there is even a valid perfectly anti-correlated RDM (Pearson r = −1), which can be found by flipping the sign of all dissimilarities and adding a large enough value to make the RDM conform to the triangle inequality (which ensures the existence of an embedding of points that is consistent with the anti-correlated RDM). The existence of valid negative RDM correlations is important to the inferential methods presented here because it is required for our assumption of symmetric (t-)distributions around the true RDM correlation.”</p><disp-quote content-type="editor-comment"><p>4. The introduction would benefit from a better motivation of the method. It seems as if the authors discuss previous work on RDMs but then jump to the introduction of the new method. Did no other method exist before? What were the issues with these methods, and what is the gap that needs to be filled? This would help the reader better understand why they should be reading this work.</p></disp-quote><p>This connection does indeed seem important. We added a paragraph that explicitly states the limitations of previous RSA inference methods:</p><p>“However, existing statistical inference methods for RSA (step 3) have important limitations. Established RSA inference methods (Nili et al., 2014) provide a noise ceiling and enable comparisons of fixed models with generalization to new subjects and conditions. However, they cannot handle flexible models, can be severely suboptimal in terms of statistical power, and have not been thoroughly validated using simulated or real data where ground truth is known. Addressing these shortcomings poses three substantial challenges. (1)</p><p>Model-comparative inference with generalization to new conditions is not trivial because new conditions extend an RDM and the evaluation depends on pairwise dissimilarities, thus violating independence assumptions. (2) Standard methods for statistical inference do not handle multiple random factors — subjects and conditions in RSA. (3) Flexible models, that is models that have parameters enabling them to predict different RDMs, are essential for RSA (Diedrichsen, Yokoi, and Arbuckle, 2018; Kriegeskorte and Diedrichsen, 2016). Evaluation of such models requires methods that are unaffected by overfitting to either subjects or conditions to avoid a bias in favor of more flexible models.”</p><disp-quote content-type="editor-comment"><p>5. While valid, the approach appears to be rather conservative, producing very low false positive rates. Are false negatives not a potentially problematic issue in that respect?</p></disp-quote><p>The reviewer here refers to the simple simulations based on matrix normal data, which we now present in Appendix 1—figure 1 for model comparisons. In this case, there were indeed some methods that are overly conservative.</p><p>There are three reasons why we believe that this is less problematic than it seems:</p><p>1) This impression is due to the simplified simulations. In these simulations we enforced that the two models performed equally well for the exact stimuli in the RDM. This is even more equal than one would expect for a random sample of stimuli for models that perform equally on average. For simulations that include variation due to the stimuli, the methods that look good in our earlier simulations can fail catastrophically, as we demonstrate with a new simulation in which we create such models that perform equally only on average (New third row of Figure 8). Thus the existing, less conservative methods are in fact invalid.</p><p>2) The new methods we propose in this manuscript are strictly more progressive than existing methods for the 2-factor bootstrap. We explicitly bound the variance estimates with the uncorrected 2-factor bootstrap. Thus, we produce fewer false negatives than existing valid methods.</p><p>3) We show in a diverse set of more realistic simulations, that we can estimate the variance quite accurately. Given an unbiased variance estimate and fairly simple unimodal sampling distributions for the model evaluations, we believe that there is not that much room for improvement left.</p><p>Overall, we are sure that we improve over existing methods, also in terms of false negative rates. Nonetheless, it is true that there could be other methods that gain even more power than the methods we propose by raising the false positive rate to the nominal value.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>This paper addresses a major question in computational neuroscience by proposing a novel methodology to test models to explain behavioral/brain data that generalize across conditions and subjects using bootstrapping.</p><p>The experiments reported validate the claims of the authors. The methodology is applied and analysed in different available datasets.</p><p>I found particularly helpful and thorough the tests with the simulations. However, I found that the reported analysis is focused mainly on the newly proposed method, and this could bring a wider perspective into the picture.</p><p>It is with such simulated data that I believe a deeper discussion and possibly adding a comparison to existing methods, such as vanilla RSA and/or linear encoding methods could be reported to support the final discussion on the limitations of such existing methods. This would allow showcasing in which cases this method reveals new conclusions and has lower false positive rates, or in which cases there which method is limited to the experimental paradigm used to obtain the data (e.g., how many participants, repetitions, and conditions).</p></disp-quote><p>We believe there are two aspects to this question, which are largely independent:</p><p>First, we can compare to other inference methods for RSA to illustrate their limitations. The primary reason to do this is to show that these limited methods indeed fail for the simultaneous generalization to subjects and stimuli that we cover now.</p><p>We have added one such simulation result in Appendix 1—figure 1 in the Appendix. There we created two models that are equally close to the data across a large set of stimuli and then subsample these stimuli to create simulated experiments. This leads to vastly inflated false alarm rates for differences between the two models, for all inference methods that do not handle variance due to stimulus selection explicitly. Preliminary simulations based on the deep neural networks also showed that all methods based on the variance across subjects do indeed fail. These simulations drive home the point that earlier methods were insufficient, but we felt they were distracting from our main point that the methods we now recommend work. Also, the severity of these failures depends on how much variability is due to subject choice and stimulus choice respectively and we can make all methods that are based on the variability across subjects fail arbitrarily badly, depending on the simulation parameters, such that the precise size of the error is rather meaningless.</p><p>Second, it would be interesting to compare RSA to other methods that are used for comparisons between representational models, like encoding models to estimate which method performs best for model selection. Such comparisons feel out of scope for this manuscript unfortunately. Which model performance metric is best suited for comparing representational models will depend on many technical details of the model evaluations, which we would not want to add to this already dense manuscript. Furthermore, the statistical problems we tackle here apply equally to any other model performance metrics and the question which one works best is largely orthogonal to our questions here.</p></body></sub-article></article>