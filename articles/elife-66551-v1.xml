<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">66551</article-id><article-id pub-id-type="doi">10.7554/eLife.66551</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Interpreting wide-band neural activity using convolutional neural networks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-216543"><name><surname>Frey</surname><given-names>Markus</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0291-3391</contrib-id><email>markus.frey@ntnu.no</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-223586"><name><surname>Tanni</surname><given-names>Sander</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9275-0735</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-175105"><name><surname>Perrodin</surname><given-names>Catherine</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-223587"><name><surname>O'Leary</surname><given-names>Alice</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-118039"><name><surname>Nau</surname><given-names>Matthias</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0956-7815</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-223588"><name><surname>Kelly</surname><given-names>Jack</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-223589"><name><surname>Banino</surname><given-names>Andrea</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-61675"><name><surname>Bendor</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-243091"><name><surname>Lefort</surname><given-names>Julie</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-130273"><name><surname>Doeller</surname><given-names>Christian F</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4120-4600</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-75558"><name><surname>Barry</surname><given-names>Caswell</given-names></name><email>caswell.barry@ucl.ac.uk</email><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Kavli Institute for Systems Neuroscience, Centre for Neural Computation, The Egil and Pauline Braathen and Fred Kavli Centre for Cortical Microcircuits, NTNU, Norwegian University of Science and Technology</institution><addr-line><named-content content-type="city">Trondheim</named-content></addr-line><country>Norway</country></aff><aff id="aff2"><label>2</label><institution>Max-Planck-Insitute for Human Cognitive and Brain Sciences</institution><addr-line><named-content content-type="city">Leipzig</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution>Cell &amp; Developmental Biology, UCL</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution>Institute of Behavioural Neuroscience, UCL</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff5"><label>5</label><institution>Open Climate Fix</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff6"><label>6</label><institution>DeepMind</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff7"><label>7</label><institution>Institute of Psychology, Leipzig University</institution><addr-line><named-content content-type="city">Leipzig</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Deshmukh</surname><given-names>Sachin</given-names></name><role>Reviewing Editor</role><aff><institution>Indian Institute of Science Bangalore</institution><country>India</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>02</day><month>08</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e66551</elocation-id><history><date date-type="received" iso-8601-date="2021-01-15"><day>15</day><month>01</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-07-13"><day>13</day><month>07</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2019-12-11"><day>11</day><month>12</month><year>2019</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/871848"/></event></pub-history><permissions><copyright-statement>© 2021, Frey et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Frey et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-66551-v1.pdf"/><abstract><p>Rapid progress in technologies such as calcium imaging and electrophysiology has seen a dramatic increase in the size and extent of neural recordings. Even so, interpretation of this data requires considerable knowledge about the nature of the representation and often depends on manual operations. Decoding provides a means to infer the information content of such recordings but typically requires highly processed data and prior knowledge of the encoding scheme. Here, we developed a deep-learning framework able to decode sensory and behavioral variables directly from wide-band neural data. The network requires little user input and generalizes across stimuli, behaviors, brain regions, and recording techniques. Once trained, it can be analyzed to determine elements of the neural code that are informative about a given variable. We validated this approach using electrophysiological and calcium-imaging data from rodent auditory cortex and hippocampus as well as human electrocorticography (ECoG) data. We show successful decoding of finger movement, auditory stimuli, and spatial behaviors – including a novel representation of head direction - from raw neural activity.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>deep learning</kwd><kwd>decoding</kwd><kwd>calcium imaging</kwd><kwd>electrophysiology</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>ERC-CoG GEOCOG 724836</award-id><principal-award-recipient><name><surname>Doeller</surname><given-names>Christian F</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>212281/Z/18/Z</award-id><principal-award-recipient><name><surname>Barry</surname><given-names>Caswell</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>110238/Z/15/Z</award-id><principal-award-recipient><name><surname>Perrodin</surname><given-names>Catherine</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A novel deep-learning framework shows how to interpret and decode raw neural recordings, avoiding the need for strong prior hypotheses, revealing a novel representation of head direction.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A central aim of neuroscience is deciphering the neural code, understanding the neural representation of sensory features and behaviors, as well as the computations that link them. The task is complex, and although there have been notable successes – such as the identification of orientation selectivity in V1 (<xref ref-type="bibr" rid="bib25">Hubel et al., 1959</xref>) and the representation of self-location provided by hippocampal place cells (<xref ref-type="bibr" rid="bib40">O'Keefe and Dostrovsky, 1971</xref>) – progress has been slow. Neural activity is high dimensional and often sparse, while the available datasets are typically incomplete, being both temporally and spatially limited. This problem is compounded by the fact that the code is multiplexed and functionally distributed (<xref ref-type="bibr" rid="bib59">Walker et al., 2011</xref>). As such, activity in a single region may simultaneously represent multiple variables, to differing extents, across different elements of the neural population. Taking the entorhinal cortex for example, a typical electrophysiological recording might contain spike trains from distinct cells predominantly encoding head direction, self-location, and movement speed via their firing rates (<xref ref-type="bibr" rid="bib52">Sargolini et al., 2006</xref>; <xref ref-type="bibr" rid="bib34">Kropff et al., 2015</xref>; <xref ref-type="bibr" rid="bib21">Hafting et al., 2005</xref>), while other neurons have more complex composite representations (<xref ref-type="bibr" rid="bib22">Hardcastle et al., 2017</xref>). At the same time, information about speed and location can also be identified from the local field potential (LFP) (<xref ref-type="bibr" rid="bib36">McFarland et al., 1975</xref>) and the relative timing of action potentials (<xref ref-type="bibr" rid="bib41">O'Keefe and Recce, 1993</xref>). Fundamentally, although behavioral states and sensory stimuli can generally be considered to be low dimensional, finding the mapping between noisy neural representations and these less complex phenomena is far from trivial.</p><p>Historically, the approach for identifying the correspondence between neural data and external observable states – stimuli or behavior – has been one of raw discovery. An experimenter, guided by existing knowledge, must recognize the fact that the activity covaries with some other factor. Necessarily this is an incremental process, favoring identification of the simplest and most robust representations, such as the sparse firing fields of place cells (<xref ref-type="bibr" rid="bib38">Muller et al., 1987</xref>). Classical methods, like linear regression and linear-nonlinear-Poisson cascade models (<xref ref-type="bibr" rid="bib12">Corrado et al., 2005</xref>; <xref ref-type="bibr" rid="bib34">Kropff et al., 2015</xref>), provide powerful tools for the characterization of existing representations but are less useful for the identification of novel responses – they typically require highly processed data in conjunction with strong assumptions about the neural response, and in the former cases are limited to one dimensional variables. Recent advances in machine learning suggest an alternative strategy. Artificial neural networks (ANNs) trained using error backpropagation regularly exceed human-level performance on tasks in which high dimensional data is mapped to lower dimensional labels (<xref ref-type="bibr" rid="bib33">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="bib37">Mnih et al., 2015</xref>). Indeed, these tools have successfully been applied to <italic>processed</italic> neural data – accurately decoding behavioral variables from observed neural firing rates (<xref ref-type="bibr" rid="bib19">Glaser et al., 2017</xref>; <xref ref-type="bibr" rid="bib57">Tampuu et al., 2018</xref>). However, the true advantage of ANNs is not their impressive accuracy but rather the fact that they make few assumptions about the structure of the input data and, once trained, can be analyzed to determine which elements of the input, or indeed combination of elements, are most informative (<xref ref-type="bibr" rid="bib11">Cichy et al., 2019</xref>; <xref ref-type="bibr" rid="bib24">Hasson et al., 2020</xref>; <xref ref-type="bibr" rid="bib7">Cammarata et al., 2020</xref>). Moreover, this framework provides full control over the weights, activations, and objective functions of the model, allowing fine-grained analysis of the inner workings of the network. Viewed in this way ANNs potentially provide a means to accelerate the discovery of novel neural representations.</p><p>To test this proposal, we developed a convolutional network (<xref ref-type="bibr" rid="bib35">LeCun et al., 2015</xref>) able to take minimally processed, wide-band neural data as input and predict behaviors or other co-recorded stimuli. In the first instance, we trained the model with unfiltered and unclustered electrophysiological recordings made from the CA1 pyramidal cell layer in freely foraging rodents. As expected, the network accurately decoded the animals’ location, speed, and head direction – without spike sorting or additional user input. Analysis of the trained network showed that it had ‘discovered’ place cells (<xref ref-type="bibr" rid="bib40">O'Keefe and Dostrovsky, 1971</xref>; <xref ref-type="bibr" rid="bib45">O’keefe and Nadel, 1978</xref>) – frequency bands associated with pyramidal waveforms being highly informative about self-location (<xref ref-type="bibr" rid="bib17">Epsztein et al., 2011</xref>). Equally, it successfully recognized that theta-band oscillations in the LFP were informative about running speed (<xref ref-type="bibr" rid="bib36">McFarland et al., 1975</xref>; <xref ref-type="bibr" rid="bib28">Jeewajee et al., 2008</xref>). Unexpectedly, the network also identified a population of putative CA1 interneurons that encoded information about head direction. We corroborated this observation using conventional tools, confirming that the firing rate of these neurons was modulated by facing-direction, a previously unreported relationship. Beyond this we found the trained network provided a means to efficiently conduct analyses which would otherwise have been complex or time consuming. For example, comparison of all frequency bands revealed positive interactions between frequencies associated with waveforms – components of the neural code that convey more information together than when considered individually. Subsequently, to demonstrate the generality of this approach, we applied the same architecture to electrophysiological data from auditory cortex, two-photon calcium imaging data acquired while mice explored a virtual environment as well as ECoG recordings in humans performing finger movements (<xref ref-type="bibr" rid="bib53">Schalk et al., 2007</xref>).</p><p>Our model differs markedly from conventional decoding methods which often use Bayesian estimators (<xref ref-type="bibr" rid="bib64">Zhang et al., 1998</xref>) for hippocampal recordings in conjunction with highly processed neural data or linear methods for movement decoding (<xref ref-type="bibr" rid="bib3">Antelis et al., 2013</xref>). In the case of extracellular recordings, this usually implies that time-series are filtered and processed to detect action potentials and assign them to specific neurons. Necessarily this discards information in frequency bands outside of the spike range, potentially introducing biases implicit in the algorithm used (<xref ref-type="bibr" rid="bib46">Pachitariu et al., 2016</xref>; <xref ref-type="bibr" rid="bib10">Chung et al., 2017</xref>; <xref ref-type="bibr" rid="bib27">Hyung et al., 2017</xref>) and operator’s subjective preferences (<xref ref-type="bibr" rid="bib23">Harris et al., 2000</xref>; <xref ref-type="bibr" rid="bib62">Wood et al., 2004</xref>), and – despite considerable advances – still demands manual input to adjust clusters (<xref ref-type="bibr" rid="bib46">Pachitariu et al., 2016</xref>). Furthermore, accurate calculation of prior expectations regarding the way in which the data varies with the decoded variable – an essential component of Bayesian decoding – requires considerable knowledge about the structure of the neural signal being studied and appropriate noise models. Other authors have attempted to address some of these shortcomings, for example, decoding without assigning action potentials to specific neurons (<xref ref-type="bibr" rid="bib31">Kloosterman et al., 2014</xref>; <xref ref-type="bibr" rid="bib1">Ackermann et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Deng et al., 2015</xref>) or combining LFP and spiking data (<xref ref-type="bibr" rid="bib56">Stavisky et al., 2015</xref>) for cursor control in patients. However, these approaches relied on existing assumptions about neural coding statistics and did not use all available information in the recordings, while their primary focus was simply to improve decoding accuracy. In contrast, the flexible, general-purpose approach we describe here requires few assumptions and – once trained – can be interrogated to inform the discovery of novel neural representations. In addition, as the model does not rely on specific oscillations or spike waveforms, it can easily generalize across domains – a fact we demonstrate with optical imaging data acquired in rodents as well as human ECoG signals.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In the following section, we present our model, the results, and describe how it was applied across different datasets. First, we report results obtained when decoding spatial behaviors from CA1 recordings made in freely moving rats. Second, we describe how the model results are interpreted and which informative features drive decoding performance, including a more detailed analysis of head direction and speed decoding. Third, we show that the model generalizes to different brain regions and recording techniques, including calcium imaging in mice and ECoG recordings in humans. In short, the framework provides a unified way of decoding continuous behaviors or stimuli from neural time-series data. The model first transforms the neural data into frequency space via a wavelet transformation. Next, the outputs (behaviors or stimuli) are resampled to match the sample rate of the neural data. Finally, the convolutional neural network takes the transformed neural data as input and decodes each output separately.</p><sec id="s2-1"><title>Accurate decoding of self-location from CA1 recordings</title><p>In the first instance, we sought to evaluate our network-based decoding approach on well characterised neural data with a clear behavioral correlate. To this end, we used as input extracellular electrophysiological signals recorded from hippocampal region CA1 in five freely moving rats – place cells from this area being noted for their spatially constrained firing fields that convey considerable information about an animal's self-location (<xref ref-type="bibr" rid="bib40">O'Keefe and Dostrovsky, 1971</xref>; <xref ref-type="bibr" rid="bib38">Muller et al., 1987</xref>). Animals were bilaterally implanted with 32 tetrodes and, after recovery and screening, 128 channel wide-band (0 Hz to 15000 Hz sampled at 30 kHz) recordings were made while the rats foraged in a 1.25 x 1.75 m arena for approximately 40 min (see methods). Raw electrophysiological data were decomposed using Morlet wavelets to generate a three-dimensional representation depicting time, channels, and frequencies from 2 Hz to 15,000 Hz (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; <xref ref-type="bibr" rid="bib9">Christopher and Gilbert, 1998</xref>). Using the wavelet coefficients as inputs, the model was trained in a supervised fashion using error backpropagation with the X and Y coordinates of the animal as regression targets. We report test-set performance for fully cross-validated models using five splits across the whole duration of the experiment.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Accurate decoding of self-location from unprocessed hippocampal recordings.</title><p>(<bold>A</bold>) Top, a typical ’raw’ extracellular recording from a single CA1 electrode. Bottom, wavelet decomposition of the same data, power shown for frequency bands from 2 Hz to 15 kHz (bottom to top row). (<bold>B</bold>) At each timestep wavelet coefficients (64 time points, 26 frequency bands, 128 channels) were fed to a deep network consisting of 2D convolutional layers with shared weights, followed by a fully connected layer with a regression head to decode self-location; schematic of architecture shown. (<bold>C</bold>) Example trajectory from R2478, true position (black) and decoded position (blue) shown for 3 s of data. Full test-set shown in <xref ref-type="video" rid="fig1video1">Figure 1—video 1</xref>. (<bold>D</bold>) Distribution of decoding errors from trial shown in (<bold>C</bold>), mean error (14.2 cm ± 12.9 cm, black), chance decoding of self-location from shuffled data (62.2 cm ± 9.09 cm, red). (<bold>E</bold>) Across all five rats, the network (CNN) was more accurate than a machine learning baseline (SVM) and a Bayesian decoder (Bayesian) trained on action potentials. This was also true when the network was limited to high-frequency components (&gt;250Hz, CNN-Spikes). When only local frequencies were used (&lt;250Hz, CNN-LFP), network performance dropped to the level of the Bayesian decoder (distributions show the fivefold cross validated performance across each of five animals, n=25). Note that this likely reflects excitatory spikes being picked up at frequencies between 150 and 250 Hz (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). (<bold>F</bold>) Decoding accuracy for individual animals, the network outperformed the Bayesian decoder in all cases. An overview of the performance of all tested models can be seen in <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>. (<bold>G</bold>) The advantage of the network over the Bayesian decoder increased when the available data was reduced by downsampling the number of channels (data from R2478). Inset shows the difference between the two methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Effect of weight sharing on model performance.</title><p>We evaluated two models on the same dataset, using either shared weights and 2D-convolutions (blue) or independent weights using 3D-convolutions (gray).The model using shared weights reaches a lower validation loss and generalizes better (smaller overfit).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Decoding performance of LFP models.</title><p>We evaluated the decoding performance of the LFP model using different frequency components and show euclidean error scores for positional decoding. Each model was fully cross-validated across all five animals, using a different set of frequencies. LFP (0–150 Hz) uses 12/26 frequencies, LFP (0–250 Hz) uses 15/26 frequencies, LFP (150–250 Hz) uses 3/26 frequencies, Bayesian decoder uses spike sorted data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Decoding performance across different models.</title><p>We calculated the Euclidean distance between the real behavior and decoded behavior across five rats and four different models. Full model has access to all frequency bands from 2 to 15,000 Hz, Spikes model has access to frequencies &gt;250Hz, while LFP model uses frequencies &lt;250Hz. Bayesian decoder was trained on spike sorted data. Chance level is indicated as red line.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Decoding performance across time windows and continuity priors.</title><p>(<bold>A</bold>) We evaluated the performance of our model and a Bayesian decoder using a range of different durations (0.1 s - 1.6 s). Our model outperformed the Bayesian decoder for all lengths of the time window, with both models tending to be more accurate with larger windows. (<bold>B</bold>) We assessed Bayesian decoding with and without a continuity prior. The prior was implemented as a Gaussian distribution centred around the previous decoded location adjusting the standard deviation based on the speed in the previous timestep. Compared to the model with no continuity prior the model with such a prior has a higher variance across animals and makes more ‘catastrophic’ errors though has a marginally lower median error (median decoding error with continuity, 22.51 cm; without continuity, 23.23 cm). We show box plots across five cross-validations and all five animals.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig1-figsupp4-v1.tif"/></fig><fig id="fig1s5" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 5.</label><caption><title>Difference in error distribution between Bayesian decoder and CNN.</title><p>Cumulative error distribution of decoding errors for Bayesian decoder (blue) and CNN (gray) for euclidean errors between true position and decoded position. Inset shows error distribution for decoding errors higher than 50 cm. Data from one animal (R2478).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig1-figsupp5-v1.tif"/></fig><fig id="fig1s6" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 6.</label><caption><title>Decoding performance with different speed thresholds.</title><p>We evaluated both the Bayesian decoder and the convolutional neural network errors across different speed thresholds to investigate if periods of immobility adversely affect decoding performance. For each speed threshold, we discarded samples where the speed of the animal was below the threshold, using speeds from 0 cm/s up to 25 cm/s. We show box plots across five cross-validations and all five animals.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig1-figsupp6-v1.tif"/></fig><fig id="fig1s7" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 7.</label><caption><title>Detection of replay events in CA1 recordings.</title><p>To detect more fine-grained representations, we retrained the model using a lower downsampling rate, resulting in an effective sampling rate of 500 Hz. This allowed us to investigate model behavior at timepoints of manually extracted replay events. (<bold>A</bold>) Two example replay events with model loss indicated in inset. Shaded area across model loss indicates timepoints of replay event. Model loss of 0 indicates accurate decoding of animals position. Real position of the animal is indicated as a red dot (note that the animal was not moving during these times so the real position did not change), while decoded position (smoothed using a 0.02 s kernel) is shown in dark blue, with unsmoothed positions shown as black dots. (<bold>B–D</bold>) We quantified replay events using three different measures to separate them from non-replay events. To obtain a statistical score for all three, we shuffled the replay events in time (keeping the absolute length of each replay) and recalculated each measure 1000 times. Original value is indicated as a dark blue line. (<bold>B</bold>) We evaluated if replay events show a higher overall decoding error by calculating the average loss during the replay event (n=1000, p&lt;0.001). (<bold>C</bold>) We calculated the average length of each replay event to detect if replay events are longer than the average trajectory (n=1000, p=0.003). (<bold>D</bold>) We also calculated if replay events are as coherent as other parts of the decoding by calculating the difference to a polynomial fitted to the trajectory (n=1000, p=0.257).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig1-figsupp7-v1.tif"/></fig><fig id="fig1s8" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 8.</label><caption><title>Influence of decoding across channels.</title><p>(<bold>A</bold>) Channel influence scores for positional decoding (left). The most influential channel for the positional decoding has a high number of place cells (right). Average ratemap of all clusters (top, n=21) and four example clusters (bottom) shown. (<bold>B</bold>) Influence scores per tetrode (average influence over four channels) highly correlates with number of place cells on the tetrode, indicating that the network is correctly identifying place cells as the spatially most informative neural correlate.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig1-figsupp8-v1.tif"/></fig><fig id="fig1s9" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 9.</label><caption><title>A subset of frequency pairs exhibit greater than expected decoding influence.</title><p>We evaluated 325 frequency pairs to investigate the relative influence of conjoint frequencies versus the sum of their individual influence on the decoding of position. (<bold>A</bold>) For each frequency pair, the combined influence – calculated by shuffling both together (<inline-formula><mml:math id="inf1"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) – is compared to the summed influence of each alone (<inline-formula><mml:math id="inf2"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>). Lower triangle shows <inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, upper triangle shows <inline-formula><mml:math id="inf4"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Positive, red, entries in lower triangular indicate frequency pairs with a combined influence greater than the sum of their individual influences. (<bold>B</bold>) Same a left matrix with non-significant entries removed. (<bold>C</bold>) p Values for the data in the left matrix determined using the Wilcoxon signed rank test, Holm-Sidak corrections were applied for n=325 comparisons. There was a limited subset of frequency pairings in which the combinatorial influence on position decoding significantly exceeded the sum of individual frequencies – these were focused on the bands associated with place cell action potential (331.5–937.5 Hz) and to a lesser extent on the ones associated with putative interneurons.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig1-figsupp9-v1.tif"/></fig><fig id="fig1s10" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 10.</label><caption><title>Effect of downsampling on model performance.</title><p>We ran fully cross-validated experiments for different downsampling values of the wavelet transformed electrophysiological signal (z = [10…10000]). Data from one animal (R2478), the shaded area indicates the 95% confidence interval.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig1-figsupp10-v1.tif"/></fig><media id="fig1video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-66551-fig1-video1.mp4"><label>Figure 1—video 1.</label><caption><title>Decoding of multiple behaviors from rodent CA1.</title><p>We show the decoding errors on the fully cross-validated test set of one example animal, from a model which was simultaneously decoding position, head direction and speed. Left part shows the decoding of position on top of the animal trajectory, top right shows the decoding of head direction and bottom right shows the decoding of speed on top of a histogram of decoded speeds.Real behavior is indicated in green, decoded output behavior is shown in red.</p></caption></media></fig-group><p>To reduce computational load and improve test set generalization, we use 2D-convolutions with shared weights applied to the three-dimensional input (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>) – the first eight convolutional layers having weights shared across channels and the final six across time. Implementing weight sharing in this way is desirable as the model is able to efficiently identify features that reoccur across time and channels, for example, prominent oscillations or waveforms, while also drastically reducing model complexity. For comparison, an equivalent architecture trained to decode position from 128 channels of hippocampal electrophysiological but without shared weights had 38,144,900 hyperparameters compared to 5,299,653 – an increase of 720%. The more complex model took 4.7 hr to run per epoch, as opposed to 175 s, and ultimately yielded less accurate decoding (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><p>The model accurately decoded position from the unprocessed neural data in all rats, providing a continuous estimate of location with a mean error less than 10% of the environment’s length. This demonstrates that, as expected, the network was able to identify informative signals in the raw neural data (Mean error 17.31 cm ± 4.46 cm; Median error 11.40 cm ± 3.82 cm; Chance level 65.03 cm ± 6.91 cm; <xref ref-type="fig" rid="fig1">Figure 1C,D</xref>). To provide a familiar benchmark, we applied a standard Bayesian decoder without a continuity prior (<xref ref-type="bibr" rid="bib42">Ólafsdóttir et al., 2015</xref>) to the spiking data from the same datasets (see Materials and methods, see also <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4B</xref> for comparison to Bayesian decoder with continuity prior; <xref ref-type="bibr" rid="bib64">Zhang et al., 1998</xref>). To this end, action potentials were identified, clustered, manually curated, and spike time vectors were used to decode location – data contained in the local field potential (LFP) was discarded. Note that while the Bayesian decoder explicitly incorporates information about the probability with which animals visit each spatial location, the CNN is effectively feed-forward – being presented with ∼2 s windows of data. Our model was consistently more accurate than the Bayesian decoder, exceeding its performance in all animals (Bayesian mean error 23.38 cm ± 4.35 cm; network error 17.31 cm ± 4.46 cm; Wilcoxon signed-rank test: T=18, p=0.0001). The CNN’s advantage over the Bayesian decoder was in part derived from the fact that it made fewer large errors (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>), thus the median errors for the two methods were more similar (all channels; Bayesian decoder median error 13.99 cm ± 1.77 cm, network median error 11.40 cm ± 3.82 cm). The high accuracy and efficiency of the model for these harder samples suggest that the CNN utilizes additional information from sub-threshold spikes and those that were not successfully clustered and/or nonlinear information which is not available to the Bayesian decoder.</p><p>The relative advantage over the Bayesian decoder increased further when the number of channels used for decoding was downsampled to simulate smaller recordings (linear regression Wald-test (n=31), s=-0.65, p=1.83e-10; <xref ref-type="fig" rid="fig1">Figure 1G</xref>). Notably, the model achieved a similar mean decoding performance with twenty tetrodes (80 channels, 23.45 cm ± 3.15 cm) as the Bayesian decoder reached with the full data set (128 channels, 23.25 cm ± 2.79 cm, <xref ref-type="fig" rid="fig1">Figure 1G</xref>). Note that for these comparisons, the Bayesian decoder was only applied to periods when the animal was travelling at &gt;3cm/s, in contrast the CNN did not have a speed threshold (i.e. is trained on moving and stationary periods). To confirm this difference did not favor the CNN, we examined how its performance compared to the Bayesian decoder if both were subject to a range of speed thresholds. As expected, limiting the CNN to only periods of movement actually improves its performance, accentuating the difference between it and the Bayes decoder (0 cm/s speed threshold: Bayesian mean error 22.23 cm ± 3.72 cm; network error 17.37 cm ± 3.58 cm; 25 cm/s speed threshold: Bayesian mean error 17.17 cm ± 2.788 cm; network error 13.40 cm ± 3.59 cm, <xref ref-type="fig" rid="fig1s6">Figure 1—figure supplement 6</xref>). To compare the CNN against standard machine learning tools, we used the wavelet transformed data to train support vector machines (SVMs). Note that in this case, the spatial structure of the input is inevitably lost as the input features are transformed to a one-dimensional representation. Both linear (53.6 cm ± 14.77 cm; <xref ref-type="fig" rid="fig1">Figure 1E</xref>) and non-linear SVMs (61.2 cm ± 15.67 cm) performed considerably worse than the CNN. Note that it is computationally not feasible to use a fully connected neural network as the flattening of the spatial structure will inevitably lead to a multi-million parameter model. These models are notoriously hard to train and will not fit many consumer-grade GPUs, in contrast to convolutional neural networks which share the weights across the spatial dimension.</p><p>To better understand which elements of the raw neural data the network used, we retrained our model using datasets limited to just the LFP (&lt;250Hz) and just the spiking data (&gt;250Hz). In both cases, the network accurately decoded location (spikes-only (CNN-Spikes) mean error 17.23 cm ± 4.69 cm; LFP-only (CNN-LFP) mean error 24.24 cm ± 6.00 cm; <xref ref-type="fig" rid="fig1">Figure 1E</xref>), indicating that this framework is able to extract information from varied electrophysiological sources. Consistent with the higher information content of action potentials, the spikes-only network was considerably more accurate than the LFP-only network (Wilcoxon signed-rank test, two-sided (n=25): T=0, p=1.22e-05), although the LFP-only network was still comparable with the spike-based Bayesian decoder (Bayesian 23.38 cm ± 4.35 cm; LFP 24.24 cm ± 6.00 cm; Wilcoxon signed-rank test, two-sided (n=25): T=136, p=0.475). If we retrain the model on frequency bands 0–150 Hz and 150–250 Hz, we observe that spatial information is predominantly contained in higher band frequencies (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>, see also <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), likely reflecting power from pyramidal cell waveforms reaching these frequencies. Note that previous studies have shown that demodulated theta is informative about the position of an animal in its environment (<xref ref-type="bibr" rid="bib2">Agarwal et al., 2014</xref>). However, in those experiments theta oscillations were converted into a complex-valued signal, which carried both the magnitude and phase of theta – here we only used the magnitude for decoding of position.</p><p>The standard decoding model downsamples the wavelet frequencies to a rate of 30 Hz, potentially discarding transient non-local representations (e.g. replay events and theta sequences). To evaluate the potential of our model to detect these short-lived events, we applied a lower downsampling factor to achieve a sampling rate of 500 Hz. This allowed us to investigate decoded time points during immobility periods in which ripples were detected in the LFP data using standard methods (see Materials and methods). Examining these periods, we saw the CNN often decoded transient, high velocity trajectories that resembled those reported in studies of open field replay which are known to co-occur with ripples (see <xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7A</xref>). Consistent with this interpretation the decoding error for these periods (i.e. Euclidean distance between the animal’s location and decoded location) was larger than for matched periods when the animal was stationary but ripples were not present (n=1000, p&lt;0.001, <xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7B</xref>). We furthermore found these putative replay trajectories were longer than trajectories decoded from the matched stopped periods (n=1000, p=0.003, <xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7C</xref>) but were coherent, similar to trajectories decoded during movement (n=1000, p=0.257, <xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7D</xref>). Taken together, these analyses indicate that our model captures transient high-velocity trajectories occurring during sharp-wave ripples but not during other stationary periods – it is highly likely that these correspond to replay events. Thus, it seems plausible that non-local representations are accessible to this CNN framework.</p></sec><sec id="s2-2"><title>Simultaneous decoding of multiple factors</title><p>The hippocampal representation of self-location is arguably one of the most readily identifiable neural codes – at any instance a small number of sparsely active neurons are highly informative. To provide a more stringent test of the network’s ability to detect and decode behavioral variables from unprocessed neural signals, we retrained with the same data but simultaneously decoded position, speed, and head-direction within a single model. CA1 recordings are known to incorporate information about these additional factors but their representation is less pronounced than that for self-location. Thus, the spatial activity of place cells is known to be weakly modulated by head direction (<xref ref-type="bibr" rid="bib29">Jercog et al., 2019</xref>; <xref ref-type="bibr" rid="bib63">Yoganarasimha et al., 2006</xref>), while place cell firing rates and both the frequency and amplitude of theta, a 7–10 Hz LFP oscillation, are modulated by running speed (<xref ref-type="bibr" rid="bib36">McFarland et al., 1975</xref>; <xref ref-type="bibr" rid="bib28">Jeewajee et al., 2008</xref>). In this more complex scenario, the architecture and hyper-parameters remained the same with just the final fully connected layer of the network being replicated, one layer for each variable, with the provision of appropriate loss functions – cyclical mean absolute error for head direction and mean absolute error for speed (see Materials and methods). All three variables were decoded simultaneously and accurately (Position, 17.78 cm ± 4.96 cm; Head Direction, 0.80 rad ± 0.18 rad; Speed 4.94 cm/s ± 1.00 cm/s; <xref ref-type="fig" rid="fig2">Figure 2A</xref> and <xref ref-type="video" rid="fig1video1">Figure 1—video 1</xref>), with no meaningful decrement in performance relative to the simpler network decoding only position (position-only model 17.31 cm ± 4.46; combined model 17.78 cm ± 4.96 cm; Wilcoxon signed-rank test two-sided (n=25): T=116, p=0.2108). Note that, a Bayesian decoder trained to decode head direction achieves a performance of 0.97 rad ± 0.14 rad using spike sorted neural data, significantly worse than our model (Wilcoxon signed-rank test two-sided (n=25): T=47, p=0.0018) but more accurate than would be expected by chance (Wilcoxon signed-rank test two-sided (n=25): T=0, p=1.22e-5). The <inline-formula><mml:math id="inf5"><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula>-score metric from the fully trained network – a measure which represents the portion of variance explained and is independent of the loss function – indicated that mean decoding performance was above chance for all three behaviors (<inline-formula><mml:math id="inf6"><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula>-score Position 0.86 ± 0.08, Head Direction 0.60 ± 0.12, Speed 0.72 ± 0.14, Chance <inline-formula><mml:math id="inf7"><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula>-score Position −0.14 ± 0.13, Head Direction 0.04 ± 0.11, Speed −0.16 ± 0.22) (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Thus, the network was able to effectively access multiplexed information embedded in minimally processed neural data.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Simultaneous decoding of multiple variables from hippocampal data.</title><p>(<bold>A</bold>) Position, head direction, and running speed were accurately decoded in concert by a single network. Data from all five animals, each point indicates an error for a single sample. The red dashed line indicates the chance level obtained by shuffling the input relative to the output while fully retraining the model. (<bold>B</bold>) <inline-formula><mml:math id="inf8"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>-scores, a loss-invariant measure of model performance – ranging from 1 (perfect decoding) to negative infinity – allowing performance to be compared between dissimilar variables. Data as in (<bold>A</bold>), each point corresponds to one of five cross-validations within each of five rats.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Running speed linearly correlates with power in multiple frequency bands.</title><p>To identify simple relationships between behavioral variables and frequency bands, we calculated the Spearman Rank Order Correlation Coefficient (speed and position) and Circular Correlation (head direction) between the wavelet transformed electrophysiological signal and the position, head direction and speed of the animal. (<bold>A</bold>) In the case of running speed, multiple frequency bands exhibited moderate correlations. In particular, as previously reported, the strongest relationship was present in the theta-band (10.36 Hz, rho = 0.415), a relationship that was present in all five rats (p &lt; 0.01). (<bold>B,C</bold>) In contrast no such relationship was found for the other spatial variables. Indeed, the strongest correlation identified was a negative relationship between x-axis position and power in the theta-band (10.36 Hz, rho=-0.0759, but which was not significant, p = 0.9803). Gray points indicate correlation for each animal (n=5), bold line shows the mean of those.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig2-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Interrogation of electrophysiological recordings</title><p>Although the network supports accurate decoding of self-location from electrophysiological data, this was not our main aim. Indeed, our primary goal for this framework was to provide a flexible tool capable of discovering and characterising sensory and behavioral variables represented in neural data - providing insight about the form and content of encoded information. To this end, in the fully trained network, we used a shuffling procedure to estimate the influence that each element of the 3D input (frequencies x channel x time) had on the accuracy of the decoded variables (see Materials and methods). Since this approach does not require retraining, it provides a rapid and computationally efficient means of assessing the contribution made by different channels, frequency bands, and time points.</p><p>Turning first to position decoding, we saw that the adjacent 469 Hz and 663 Hz frequency bands were by far the most influential (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Since these recordings were made from CA1, we hypothesized that these frequencies corresponded to place cell action potentials. To confirm this hypothesis – and demonstrate that it was possible to objectively use this network-based approach to identify the neural basis of decoded signals – we applied the following approach (see Materials and methods): First, we isolated the waveforms of place cells (n=629) and putative interneurons (n=91) in all animals, which were identified using a conventional approach (<xref ref-type="bibr" rid="bib46">Pachitariu et al., 2016</xref>; <xref ref-type="bibr" rid="bib30">Klausberger et al., 2003</xref>; <xref ref-type="bibr" rid="bib13">Csicsvari et al., 1999</xref>). Second, for these two groups, we calculated the relative representation of the 26 frequency bands in their waveforms. We found that the highly informative 469 Hz and 663 Hz bands were the dominant components of place cell action potentials and that in general the power spectra of these spikes strongly resembled the frequency influence plot for position decoding (Spearman rank-order correlation, two-sided (n=26) ρ=0.84, p=7.63e-08; <xref ref-type="fig" rid="fig3">Figure 3B</xref>). In contrast, putative interneurons – which typically have a shorter after-hyperpolarization than place cells (<xref ref-type="bibr" rid="bib16">English et al., 2017</xref>) – were characterized by higher frequency components (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, Mann-Whitney rank test interneuron (n=91) vs. place cell (n=629), U=1009.5, p=2.47e-13), with the highest power at 5304 Hz and 3750 Hz, bands that were considerably less informative about self-location (<xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Analysis of trained network identifies informative elements of the neural code.</title><p>(<bold>A</bold>) A shuffling procedure was used to determine the relative influence of different frequency bands in the network input. Left, the 469 Hz and 663 Hz components – corresponding to place cell action potentials – were highly informative about animals’ positions. Middle, both place cells and putative interneurons (5304 Hz) carried information about head direction. Right, several frequency bands were informative about running speed, including those associated with the LFP (10.4 Hz) and action potentials. Data from all animals. (<bold>B</bold>) Wavelet coefficients of place cell (top) and interneuron (bottom) waveforms are distinct and correspond to frequencies identified in A. Inset, average waveforms. Data from all animals. (<bold>C</bold>) Frequency bands associated with place cells (469 and 663 Hz) were more informative about position than those associated with putative interneurons (5304 Hz) – their elimination produced a larger decrement in decoding performance (p&lt;0.001). Data from all animals. (<bold>D</bold>) A subset of putative CA1 interneurons encodes head direction. Thirty-three of 91 interneurons from five animals exhibited pronounced directional modulation that was stable throughout the recording (green). Depth of modulation quantified using Kullback-Leibler divergence vs. uniform circle. Stability assessed with the Pearson correlation between polar ratemaps from the first and second half of each trial (dark gray and light gray). Cells with p&lt;0.01 for both measures were considered to be reliably modulated by head direction. Inset, example polar ratemaps. Data from all animals.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Performance of standard model compared with models trained on single frequency bands.</title><p>To determine if the frequencies identified as important by our complete model matched those that were most informative on their own, we compared the influence plots (top row, same as <xref ref-type="fig" rid="fig3">Figure 3A</xref> in manuscript) generated for the standard model with accuracy plots from models trained on individual frequency bands (bottom row). In all cases, 128 channel recordings from rodent CA1 were used to decode position (left column), head direction (middle column), and speed (right column). Influence plots were constructed as before. Accuracy (bottom row) is simply defined as 1/decoding error and is not normalized relative to chance or ceiling performance, values were generated using the same convolutional neural network while only providing a single frequency band for training and testing. Although influence is not expected to be a simple linear function of accuracy, the results from the two methods were highly correlated: position, Spearman’s ρ=0.88 (p&lt;0.001); head direction, ρ=0.82 (p&lt;0.001); running speed, ρ=0.47 (p=0.02). For each frequency band, we show the average cross-validation performance across five animals for three different behaviors and loss functions. Data from all animals, the shaded area indicates the 95% confidence interval.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Decoding influence for downsampled models.</title><p>To measure the robustness of the influence measure we downsampled the training data and retrained the model using cross-validation. We plot the Pearson correlation between the original influence distribution using the full training set and the influence distribution obtained from the downsampled data. Each dot shows one cross-validation split. Inset shows influence plots for two runs, one for 35 min of training data, the other in which model training consisted of only 5 min of training data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Decoding influence for simulated behaviors.</title><p>(<bold>A</bold>) We quantified our influence measure using simulated behaviors. We used the wavelet preprocessed data from one CA1 recording and simulated two behavioral variables which were modulated by two frequencies (58 Hz and 3750 Hz) using different multipliers (<bold>B</bold>). We then trained the model using cross-validation and calculated the influence scores via feature shuffling.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig3-figsupp3-v1.tif"/></fig></fig-group><p>Since the frequencies associated with place cell waveforms were the most informative, this indicated that the network had correctly identified place cells as the primary source of spatial information in these recordings. To corroborate this, we used the same data and for each channel eliminated power in the 469 Hz and 663 Hz frequency bands at time points corresponding to either place cell or interneuron action potentials. As expected, position decoding was most strongly affected by removal of the place cell time points (Mann-Whitney-U-Test (n=629 place cells, n=91 interneurons): U=1497, p=2.86e-08; <xref ref-type="fig" rid="fig3">Figure 3C</xref>). Using the same shuffling method, we also analyzed how informative each channel was about self-location (<xref ref-type="fig" rid="fig1s8">Figure 1—figure supplement 8</xref>). In particular, we found that the number of place cells identified on a tetrode from the spike sorted data was highly correlated with the tetrode’s spatial influence (Spearman rank-order correlation (n=128) ρ=0.71, p=5.11e-06) and that the overall distribution of both number of place cells and spatial influence followed a log-normal distribution (Shapiro-Wilk test on log-transformed data, number of place cells, W=0.79, p=3.59e-05; tetrode influence W=0.59, p=3.04e-08; <xref ref-type="fig" rid="fig1s8">Figure 1—figure supplement 8B</xref>). In sum, this analysis correctly identified that the firing rates of both place cells and putative interneurons are informative about an animal’s location, place cells more so than interneurons (<xref ref-type="bibr" rid="bib60">Wilent and Nitz, 2007</xref>). The analysis also highlighted the spatial activity of place cells, pointing to the stable place fields as a key source of spatial information.</p><p>A potential concern is that our approach might not identify multiple frequency bands if the information they contain is mutually redundant. The previous example, in which place cells and putative interneurons were both found to be informative about self-location, demonstrates this is not entirely the case. However, to further exclude this possibility, we compared the influential frequencies identified from our complete model with models trained on just a single frequency band at a time. Specifically, 26 models were trained, one for each frequency – the performance of each of these models being taken as an indication of the information present in that band. As expected we found both methods identified similar frequencies as indicated by a high correlation between our influence measure and the performance of single frequency band models (Position, Spearman rank-order correlation (n=26) ρ=0.88, p&lt;0.001; Head Direction, ρ=0.82, p&lt;0.001; Speed, ρ=0.47, p=0.02, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Note that, although each model was individually faster to train than the complete model, the time to train all 26 was considerably longer than the single model applied simultaneously to all frequencies (51.2 hr vs. 8.6 hr, ∼6x faster).</p><p>To further assess the robustness of the influence measure, we conducted two additional analyses. First, we varied the volume of data by recalculating the influence measure for different amounts of training data (1–35 min, see Materials and methods). We found that our model achieves an accurate representation of the true influence with as little as 5 min of training data (mean Pearson’s r = 0.89 ± 0.06, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Secondly, we assessed influence accuracy on a simulated behavior in which we varied the ground truth frequency information (see Materials and methods). The model trained on the simulated behavior is able to accurately represent the ground truth information (modulated frequencies 58 Hz and 3750 Hz, <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>).</p><p>Thus, our combined approach provides a fair and efficient means to determine the informative elements of wide-band neural data. More importantly, analyses of the full network enables multiple frequency bands to be considered in-concert, providing a means to identify interactions (e.g.<xref ref-type="fig" rid="fig1s9">Figure 1—figure supplement 9</xref>) that are not accessible to standard single-frequency methods.</p></sec><sec id="s2-4"><title>CA1 interneurons are modulated by head direction</title><p>Next, having validated our approach for spatial decoding, we examined the basis upon which the network was able to decode head direction. Although place cells primarily provide an allocentric spatial code, their infield firing rate is known to be modulated by heading direction (<xref ref-type="bibr" rid="bib39">Muller et al., 1994</xref>; <xref ref-type="bibr" rid="bib50">Rubin et al., 2014</xref>). Consistent with the presence of this directional code, we again saw that the most influential frequencies for head direction decoding were those associated with place cells (469 Hz and 663 Hz; <xref ref-type="fig" rid="fig3">Figure 3A</xref>). However, the distribution also incorporated a secondary peak corresponding to the frequencies typical for interneuron waveforms (Spearman rank-order correlation (n=26) ρ=0.76, p=5.71e-06, <xref ref-type="fig" rid="fig3">Figure 3A&amp;B</xref>). Presubicular interneurons have been shown to be modulated by both head direction and angular velocity (<xref ref-type="bibr" rid="bib48">Preston-Ferrer et al., 2016</xref>) but to the best of our knowledge no similar responses have been noted in CA1. To establish if putative interneurons conveyed information about head direction, we again used an ’elimination’ analysis on data from all five animals – the two frequency bands most strongly associated with interneurons (3750 Hz and 5304 Hz) were scrambled at time points when interneuron spikes were present. Consistent with the influence plots, we found that selectively eliminating putative interneurons degraded the accuracy with which head direction was decoded (relative influence: 0.089 ± 0.043, two-sided t-test (n=91) t=4.16, p=0.014). As a final step, to verify this novel observation we reverted to a standard approach. Specifically, we calculated the directional ratemap for each interneuron using only periods when the animal was in motion (&gt;10cm/s), determined the Kullback-Leibler divergence vs. a uniform circle (<xref ref-type="bibr" rid="bib15">Doeller et al., 2010</xref>), and applied a shuffling procedure to determine significance – as a whole the population exhibited reliable but weak modulation of interneuronal firing rate by head direction (Kullback-Leibler Divergence (n=91): 0.0067 ± 0.009) with 58.2% (53/91) of cells being individually significant (p&lt;0.01). Behaviors that are inhomogeneously distributed or confounded can result in spurious neural correlates (<xref ref-type="bibr" rid="bib39">Muller et al., 1994</xref>). To control for this possibility, we repeated the analysis using only data from the centre of the environment (&gt;25cm from the long sides of the enclosure and &gt;20cm from the short sides). Additionally, to verify stability, we controlled that ratemaps generated from the first and second half of the trial were correlated (Pearson correlation, p&lt;0.01). Under this more rigorous analysis, we confirmed that a sub-population (36.2%, 33/91) of putative hippocampal interneurons were modulated by head direction, a previously unrecognized spatial correlate (<xref ref-type="fig" rid="fig3">Figure 3D</xref>).</p></sec><sec id="s2-5"><title>Multiple electrophysiological features contribute to the decoding of speed</title><p>The frequency influence plots for running speed also showed several local peaks (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), that in all cases corresponded to established neural correlates. In rodents, theta frequency and power are well known to co-vary almost linearly with running speed (<xref ref-type="bibr" rid="bib36">McFarland et al., 1975</xref>; <xref ref-type="bibr" rid="bib28">Jeewajee et al., 2008</xref>), accordingly analysis of the network identified the 10.4 Hz frequency band as the most influential. Similarly, the firing rate of place cells increases with speed, an effect captured by the peak at 663 Hz. Interestingly a clear peak is also evident at 2652 Hz, indicating that interneuron firing rates are also informative – originating either from CA1 speed cells (<xref ref-type="bibr" rid="bib20">Góis and Tort, 2018</xref>) or from theta-locked interneurons (<xref ref-type="bibr" rid="bib26">Huh et al., 2016</xref>). Finally, a 4th peak was evident at 3.66 Hz and 5.17 Hz, a range that corresponds to type 2 (’atropine sensitive’) theta which is present during immobility (<xref ref-type="bibr" rid="bib32">Kramis et al., 1975</xref>; <xref ref-type="bibr" rid="bib51">Sainsbury et al., 1987</xref>). To corroborate this conclusion, we calculated the correlation between power in each frequency band and running speed (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>), confirming that the latter band showed the expected negative correlation – higher power at low speeds – while the other three peaks were positively correlated.</p></sec><sec id="s2-6"><title>Generalization across brain regions and recording techniques</title><p>As a final step, we sought to determine how well our approach generalized to other recording techniques and brain areas. Addressing the latter point first, we trained the network using electrophysiological recordings (64 channels) from the primary auditory cortex of a freely moving mouse while pure tone auditory stimuli (4–64 kHz, duration 200 ms) were played from a speaker (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). As above, the raw electrophysiological data was transformed to the frequency domain using Morlet wavelets and this wide-band frequency representation was used as input. The model architecture and hyperparameters were kept the same, reducing only the number of down-sampling steps because of the smaller input size (64 channels vs. 128 channels for CA1 recordings – each down-sampling layer halves the number of units in the previous layer). The auditory stimuli – training target – was modelled as a continuous variable with ’−1’ indicating no tone present and the log-transformed frequency of the sound at all other time points. As expected, this model architecture was also able to accurately decode tone stimuli from auditory cortex (Performance in original frequency space, <inline-formula><mml:math id="inf9"><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula>-score: 0.734 ± 0.080, chance model: −0.432 ± 0.682, <xref ref-type="fig" rid="fig4">Figure 4B,C</xref>). Informative frequencies were concentrated around 663 Hz and 165 Hz, indicating that information content about tone stimuli comes mostly from pyramidal cell activity.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Model generalizes across recording techniques and brain regions.</title><p>(<bold>A</bold>) Overview of auditory recording. We recorded electrophysiological signals while the mouse is freely moving inside a small enclosure and is presented with pure tone stimuli ranging from 4 kHz to 64 kHz. (<bold>B</bold>) <inline-formula><mml:math id="inf10"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>-score for decoding of frequency tone from auditory cortex (0.73 ± 0.08). Each dot describes the <inline-formula><mml:math id="inf11"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>-score for a 5 s sample of the experiment. Chance level is indicated by the red line. (<bold>C</bold>) An example section for decoding of auditory tone frequencies from auditory electrophysiological recordings, real tone colored in black, decoded tone in green, the line between real and decoded indicates magnitude of error. (<bold>D</bold>) Influence plots for decoding of auditory tone stimuli, same method as used for CA1 recordings. (<bold>E</bold>) Calcium recordings from a mouse running on a linear track in VR. We record from 685 cells and use Suite2p to preprocess the raw images and extract calcium traces which we feed through the model to decode linear position. Overlay shows relative influence for decoding of position calculated for each putative cell. (<bold>F</bold>) <inline-formula><mml:math id="inf12"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>-score for decoding of linear position from two-photon CA1 recordings (0.90 ± 0.03). Each dot describes the <inline-formula><mml:math id="inf13"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>-score for a 5 s sample of the experiment. Chance level is indicated by the red line. (<bold>G</bold>) Example trajectory through the virtual linear track (linearized to <inline-formula><mml:math id="inf14"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> with real position (black) and decoded position (orange)). (<bold>H</bold>) Influence plots for decoding of position from two-photon calcium imaging. Note that the range of frequencies is between 0 Hz and 15 Hz as the sampling rate of the calcium traces is 30 Hz.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Decoding finger movements from human ECoG recordings.</title><p>We applied DeepInsight to a publicly available ECoG dataset in order to decode finger movement from three subjects. (Left) We show Pearson’s r across subjects split into individual fingers. The colored line on the y-axis shows the average performance for our model (blue) and the best competing model (black). (Right) Example decoding results across all subjects. The blue line is the decoded finger position, while the black line shows ground truth information obtained via a data glove. Data from BCI Competition IV, Dataset 4 (<xref ref-type="bibr" rid="bib53">Schalk et al., 2007</xref>), comparison model from Cortex Team, Research Centre INRIA (see competition results). (<bold>C</bold>) Influence plots for the decoding of finger movement from ECoG data, calculated by shuffling each frequency and recalculating the error. Each line depicts one participant, with the average shown in blue. Peak influence is located at 88 Hz, followed by 125 Hz.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66551-fig4-figsupp1-v1.tif"/></fig></fig-group><p>Having shown that the model generalizes across different brain areas, we wanted to further investigate if it generalizes across different recording techniques. Therefore, in the third set of experiments, we acquired two-photon calcium fluorescence data from mouse CA1 while the head-fixed animal explored a 230 cm virtual track. Raw data was preprocessed to generate denoised activity traces for putative cells (n=685 regions of interest), these were then decomposed to a frequency representation using the same wavelet approach as before – only frequency bands between 0 Hz and 15 Hz being used because of the lower data rate (30 Hz) (see Materials and methods, <xref ref-type="fig" rid="fig4">Figure 4E</xref>). As before, wavelet coefficients were provided to the network as input and the only change was an increase in the number of down-sampling steps to account for the large number of ROIs (685 ROIs vs 128 channels for CA1 recordings). The network was able to accurately decode the animal’s position on the track (mean error: 15.87 ± 16.33 cm, <inline-formula><mml:math id="inf15"><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula>-scores of 0.90 ± 0.03 vs. chance model −0.05 ± 0.127, <xref ref-type="fig" rid="fig4">Figure 4F,G</xref>). Using the same shuffling technique as before, we generated influence plots indicating the relative information provided by putative cells (<xref ref-type="fig" rid="fig4">Figure 4E</xref>) and frequencies (<xref ref-type="fig" rid="fig4">Figure 4H</xref>). In the frequency domain, the most informative bands were 0.33 Hz and 0.46 Hz, unsurprisingly mirroring the 1–2 s decay time of GCaMP6s (<xref ref-type="bibr" rid="bib8">Chen et al., 2013</xref>). Interestingly the relative information content of individual cells was highly heterogeneous, a small subset (18.2%) of cells accounted for half (50%) of the influence – these units being distributed across the field of view with no discernible pattern (<xref ref-type="fig" rid="fig4">Figure 4E</xref>).</p><p>To further assess the ability of our model to decode continuous behavior from neural data, we investigated its performance on an Electrocorticography (ECoG) dataset recorded in humans (<xref ref-type="bibr" rid="bib53">Schalk et al., 2007</xref>) made available as part of a BCI competition (BCI Competition IV, Dataset 4). In this dataset, three participants were instructed to move their fingers while ECoG signals were acquired. We used the available training data from three subjects to train and validate our model and report performance on the test set provided by the competition hosts. We used the same model pipeline, adjusting parameters to fit the specifics of the dataset. In particular, we used a downsampling factor of 50, as the original sampling rate is 1000 Hz (in comparison to 30000 Hz in the CA1 recordings) which leads to an effective sampling rate of 20 Hz. We trained the model with 128 timesteps (6.4 s) and used a mean squared error loss function between original finger movement and decoded finger movement. All other model parameters were kept the same. We reach an average Pearson’s r of 0.517 ± 0.160 across three subjects (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>), outperforming the winning performance of the BCI competition which reached a performance of 0.46 (note that in line with the evaluation criteria of the BCI competition we excluded the ring finger for the comparison of correlation coefficients). This allows the model to accurately predict movement of the finger, on a never before seen test set, across three different subjects (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>).</p><p>These results together show that our model can be used on a wide variety of continuous regression problems in both rodents and humans and across a wide range of recording systems, including calcium imaging and electrocorticography datasets.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The neural code provides a complex, non-linear representation of stimuli, behaviors, and cognitive states. Reading this code is one of the primary goals of neuroscience – promising to provide insights into the computations performed by neural circuits. However, decoding is a non-trivial problem, requiring strong prior knowledge about the variables encoded and, crucially, the form in which they are represented. Not only is this information often incomplete or absent but a full characterization of the neural code is precisely the question we seek to solve. Addressing these limitations, we investigated the potential of a deep-learning framework to decode behaviors and stimuli from wide-band, minimally processed neural activity. To this end, we designed a model architecture using simple 2D convolutions with shared weights, omitting recurrent layers (<xref ref-type="bibr" rid="bib6">Bai et al., 2018</xref>). These intentional design choices resulted in a fast, data efficient architecture that could be easily interpreted to discover which elements of the neural code provided information about specific variables – a decrease in network performance was accepted as a trade-off. We showed that this approach generalized well across brain regions and recording techniques capturing both spatial and temporal information in the signal, the only changes necessary to the network being adjustments to handle the number of channels in the input matrix.</p><p>In the first instance, we validated our model using the well-characterized spatial representations of rodent CA1 place cells. Decoding performance amply exceeded a Bayesian framework, as well as a standard machine learning approach that proved ineffective on the non-linear representation of self-location. Importantly, simple analyses of the trained network correctly indicated that place cell action potentials were the most informative spatial signal – confirming that this tool can deliver insights into the nature of the neural code. In a further set of experiments, we showed that the network was able to concurrently identify multiple representations of head direction and running speed, including several that were only recently reported and one - interneuron encoding of head direction – that was previously unreported. Importantly, this framework can also identify interactions between frequency components, an analysis that is intractable to conventional methods which consider features independently. Finally, we demonstrated the flexibility of this approach, applying the same network and hyper-parameters, with adjustments made only to the input and output layers, to two-photon calcium data and extracellular recordings from auditory cortex.</p><p>In sum, we believe deep-learning based frameworks such as this constitute a valuable tool for experimental neuroscientists, being able to provide a general overview as to whether a variable is encoded in time-series data and also providing detailed information about the nature of that encoding – when, where, and in what frequency bands it is present. That is not to say that this approach is a complete substitute for conventional analyses – it merely constrains the search space for variables that might be present and their plausible format. Indeed, we imagine this network might be best used as a first-pass analysis, followed by conventional approaches to determine explicitly if a variable is present – much as we did for the interneuron representation of head direction. While we tested the network with optical and electrophysiological data in rodents, as well as ECoG data in humans, it is highly likely that it will perform well with neural data acquired in most experimental settings, including fMRI, EEG, and MEG.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Tetrode recordings from CA1</title><p>Five male Lister Hooded rats were used for this study. All procedures were approved by the UK Home Office, subject to the restrictions and provisions contained in the Animals Scientific Procedures Act of 1986. All rats (333–386 g/13–17 weeks old at implantation) were implanted with two single-screw microdrives (Axona Ltd.) targeted to the right and left CA1 (ML: 2.5 mm, AP: 3.8 mm posterior to bregma, DV: 1.6 mm from dura). Each microdrive was assembled with two 32 channel Omnetics connectors (A79026-001) and 16 tetrodes of twisted wires (either 17 μm H HL coated platinum iridium, 90% and 10% respectively, or 12.7 μm HM-L coated Stablohm 650; California Fine Wire), platinum plated to reduce impedance to below 150 kΩ at 1 kHz (NanoZ). After surgery, rats were housed individually on a 12 hr light/dark cycle and after one week of recovery rats were maintained at 90% of free-feeding weight with ad libitum access to water.</p><p>Screening was performed from one week after surgery. Electrophysiological data was acquired using Open Ephys recording system (<xref ref-type="bibr" rid="bib54">Siegle et al., 2017</xref>) and a 64-channel amplifier board per drive (Intan RHD2164). Positional tracking performed using a Raspberry Pi with Camera Module V2 (synchronized to Open Ephys system) and custom software, that localized two different brightness infra-red LEDs attached to amplifier boards on camera images acquired at 30 Hz. During successive recording sessions in a separate screening environment 1.4 x 1.4 m the tetrodes were gradually advanced in 62.5 <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi mathsize="80%">μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%" mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula> steps until place cells were identified. During the screening session, the animals were often being trained in a spatial navigation task for projects outside the scope of this study.</p><p>The experiments were run during the animals’ dark period of the L/D cycle. The recording sessions used in this study were around 40 min long, depending on the spatial sampling of the animal, in a rectangular environment of 1.75 x 1.25 m, on the second, third or fourth exposure, varying between animals. The environment floor was black vinyl flooring, it was constructed of 60 cm high boundaries (MDF) colored matt black, surrounded by black curtains on the sides and above. There was one large cue card raised above the boundary and two smaller cue cards distributed on the side of the boundary. Foraging was encouraged with 20 mg chocolate-flavored pellets (LBS Biotechnology) dropped into the environment by custom automated devices. The recordings used in this study were part of a longer session that involved foraging in multiple other different size open field environments.</p><p>Rats were anesthetized with isoflurane and given intraperitoneal injection of Euthanal (sodium pentobarbital) overdose (0.5 ml/100 g) after which they were transcardially perfused with saline, followed by a 10% Formalin solution. Brains were removed and stored in 10% Formalin and 30% Sucrose solution for 3–4 days prior to sectioning. Subsequently, 50 <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi mathsize="80%">μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%" mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula> frozen coronal sections were cut using a cryostat, mounted on gelatine coated or positively charged glass slides, stained with cresyl violet and cleared with clearing agent (Histo-Clear II), before covering with DPX and coverslips. Sections were then inspected using Olympus microscope and tetrode tracks reaching into CA1 pyramidal cell layer were verified.</p><p>Putative interneurons were classified based on waveform shape, minimum firing rate across multiple environments and lack of spatial stability. Specifically, classified interneurons had waveform half-width less than 0.15 ms, maximum ratio of amplitude to trough of 0.4, minimum firing rate of 4 Hz and maximal 0.75 spatial correlation of ratemaps from first and last half of the recording in any environment (<xref ref-type="bibr" rid="bib30">Klausberger et al., 2003</xref>; <xref ref-type="bibr" rid="bib13">Csicsvari et al., 1999</xref>). Note that we used the spatial stability in order to differentiate interneurons from place cells or grid cells, with no influence on the directional stability of the head direction cell analysis.</p></sec><sec id="s4-2"><title>Calcium recordings from CA1</title><p>All procedures were conducted in accordance to UK Home Office regulations.</p><p>One GCaMP6f mouse (C57BL/6J-Tg(Thy1-GCaMP6f)GP5.17Dkim/J, Jacksons) was implanted with an imaging cannula (a 3 mm diameter x 1.5 mm height stainless-steel cannula with a glass coverslip at the base) over CA1 (stereotaxic coordinates: AP=-2.0, ML=-2.0 from bregma). A 3 mm craniotomy was drilled at these coordinates. The cortex was removed via aspiration to reveal the external capsule of the hippocampus. The cannula was inserted into the craniotomy and secured to the skull with dental cement. A metal head-plate was glued to the skull and secured with dental cement. The animal was left to recover for at least 1 week after surgery before diet restriction and habituation to head-fixation commenced.</p><p>Following a period of handling and habituation, the mouse was head-fixed above a styrofoam wheel and trained to run for reward through virtual reality environments, presented on 3 LCD screens that surrounded the animal. ViRMEn software (<xref ref-type="bibr" rid="bib4">Aronov and Tank, 2014</xref>) was used to design and present the animal with virtual reality linear tracks. Movement of the animal on the wheel was recorded with a rotatory encoder and lead to corresponding translation through the virtual track. During the experimental phase of the training, the animal was trained to run down a 230 cm linear track and was required to lick at a reward port at a fixed, unmarked goal location within the environment in order to trigger release of a drop of condensed milk. Licks were detected by an optical lick detector, with an IR LED and sensor positioned on either side of the animal’s mouth. When the animal reached the end of the linear track, a black screen appeared for 2 s and the animal was presented with the beginning of the linear track, starting a new trial.</p><p>Imaging was conducted using a two-photon microscope (resonant scanning vivoscope, Scientifica) using 16x/0.8-NA water-immersion objective (Nikon). GCaMP was excited using a Ti:sapphire laser (Mai Tai HP, Spectra-Physics), operated with an excitation wavelength of 940 nm. ScanImage software was used for data collection / to interface with the microscope hardware. Frames were acquired at a rate of 30 Hz.</p><p>The Suite2p toolbox (<xref ref-type="bibr" rid="bib47">Pachitariu et al., 2017</xref>) was used to motion correct the raw imaging frames and extract regions of interest, putative cells.</p></sec><sec id="s4-3"><title>Tetrode recordings from auditory cortex</title><p>Sound-evoked neuronal responses were obtained via chronically-implanted electrodes in the right hemisphere auditory cortex of one 17-week-old male mouse (<italic>M. musculus</italic>, C57Bl/6, Charles River). All experimental procedures were carried out in accordance with the institutional animal welfare guidelines and a UK Home Office Project License approved under the United Kingdom Animals (Scientific Procedures) Act of 1986.</p><p>During recordings, the animal was allowed to freely move within a 11x21 cm cardboard enclosure, with one wall consisting of an acoustically transparent mesh panel to allow unobstructed sound stimulation. Acoustic stimuli were delivered via two free-field electrostatic speakers (Tucker-Davis Technologies, FL, USA) placed at ear level, 7 cm from the edge of the enclosure. Recordings were performed inside a double-walled soundproof booth (IAC Acoustics), whose interior was covered by 4 cm thick acoustic absorption foam (E-foam, UK). Pure tones were generated using MATLAB (Matlab version R2015a; MathWorks, Natwick, MA, USA), and played via a digital signal processor (RX6, Tucker Davis Technologies, FL, USA). The frequency response of the loudspeaker was ±10 dB across the frequency range used for stimulation. Pure tones of a duration of 200 ms (including 5 ms linear rise and fall times) of variable frequencies (4–64 kHz in 0.1 octave increments) were used for stimulation. The tones were presented at 65 dB SPL at the edge of the testing box. The 41 frequencies were presented pseudo-randomly, separated by a randomly varying inter-stimulus interval ranging from 500 to 510 ms, for a total of 20 repetitions.</p><p>Extracellular electrophysiological recordings were obtained using a custom chronically implanted 64-channel hyperdrive with two 32-channel Omnetics connectors (A79026-001) and 16 individually movable tetrodes (FlexDrive, <xref ref-type="bibr" rid="bib58">Voigts et al., 2013</xref>). Tetrodes were made from 12.7 <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi mathsize="80%">μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%" mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula> tungsten wire (99.95%, HFV insulation, California Fine Wire, USA) gold-plated to reduce impedance to 200 kΩ at 1 kHz (NanoZ, Multichannel Systems). Neuronal signals were collected and amplified using two 32-channel amplifier boards (Intan RHD 2132 headstages) and an Open Ephys recording system (<xref ref-type="bibr" rid="bib54">Siegle et al., 2017</xref>) at 30 kHz.</p></sec><sec id="s4-4"><title>Data preprocessing</title><p>Raw electrophysiological traces as well as calcium traces were transformed to a frequency representation using discrete-wavelet transformation (DWT). We decided to use wavelet transformation instead of windowed Fourier transform (WFT) as we expected a wide range of dominant frequencies in our signal for which the wavelet transformation is more appropriate (<xref ref-type="bibr" rid="bib9">Christopher and Gilbert, 1998</xref>). For the wavelet transformation, we used the morlet wavelet:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo>∗</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext mathvariant="italic">i</mml:mtext></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>η</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>with a non-dimensional frequency constant <inline-formula><mml:math id="inf19"><mml:mrow><mml:msub><mml:mi mathsize="80%">w</mml:mi><mml:mn mathsize="80%">0</mml:mn></mml:msub><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">6</mml:mn></mml:mrow></mml:math></inline-formula>. The full frequency space for tetrode recordings consisted of 26 log-space frequencies with fourier frequencies of: 2.59, 3.66, 5.18, 7.32, 10.36, 14.65, 20.72, 29.3, 41.44, 58.59, 82.88, 117.19, 165.75, 234.38, 331.5, 468.75, 663, 937.5, 1326, 1875, 2652, 3750, 5304, 7500, 15,000 Hz. For calcium imaging the fourier frequencies used are: 0.002, 0.003, 0.005 0.007, 0.01, 0.014, 0.02, 0.03, 0.04, 0.058, 0.08, 0.11, 0.16, 0.23, 0.33, 0.46, 0.66, 0.93, 1.32, 1.87, 2.65, 3.75, 5.3, 7.5, 10.6, 15 Hz. We noticed that downscaling the wavelets improved our model performance, prompting us to use an additional preprocessing step which effectively decreased the sampling rate of the wavelets to <inline-formula><mml:math id="inf20"><mml:mrow><mml:msub><mml:mi mathsize="80%">ψ</mml:mi><mml:mrow><mml:mi mathsize="80%">S</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">R</mml:mi><mml:mrow><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">t</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="80%">ψ</mml:mi><mml:mrow><mml:mi mathsize="80%">S</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">R</mml:mi><mml:mrow><mml:mi mathsize="80%">b</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo mathsize="80%" stretchy="false">/</mml:mo><mml:mtext mathsize="80%">𝑀</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> by a factor of <inline-formula><mml:math id="inf21"><mml:mi mathsize="80%">M</mml:mi></mml:math></inline-formula>. This can also be seen as an additional convolutional layer with a kernel size of <inline-formula><mml:math id="inf22"><mml:mi mathsize="80%">M</mml:mi></mml:math></inline-formula>, a stride of <inline-formula><mml:math id="inf23"><mml:mi mathsize="80%">M</mml:mi></mml:math></inline-formula> and weights fixed to <inline-formula><mml:math id="inf24"><mml:mfrac><mml:mn mathsize="80%">1</mml:mn><mml:mi mathsize="80%">M</mml:mi></mml:mfrac></mml:math></inline-formula>. We performed a hyperparameter search for <inline-formula><mml:math id="inf25"><mml:mi mathsize="80%">M</mml:mi></mml:math></inline-formula> with a simplified model and found the best performing model with <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi mathsize="80%">M</mml:mi><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">1000</mml:mn></mml:mrow></mml:math></inline-formula>, thus effectively decreasing our sampling rate from 30,000 to 30 (<xref ref-type="fig" rid="fig1s10">Figure 1—figure supplement 10</xref>).</p><p>As additional preprocessing steps, we applied channel and frequency wise normalization using a median absolute deviation (MAD) approach. We calculated the median and the corresponding median absolute deviation for each frequency and channel on the training set and normalized our inputs as follows:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf27"><mml:mover accent="true"><mml:msub><mml:mi mathsize="80%">X</mml:mi><mml:mi mathsize="80%">i</mml:mi></mml:msub><mml:mo mathsize="80%" stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> is the median of <inline-formula><mml:math id="inf28"><mml:msub><mml:mi mathsize="80%">X</mml:mi><mml:mi mathsize="80%">i</mml:mi></mml:msub></mml:math></inline-formula>. This approach turned out to be more robust against outliers in the signal than simple mean normalization. Additional min-max scaling did not further improve performance.</p></sec><sec id="s4-5"><title>Bayesian decoder</title><p>As a baseline model, we used a Bayesian decoder which was trained on manually sorted and clustered spikes. Given a time window <inline-formula><mml:math id="inf29"><mml:mi mathsize="80%">T</mml:mi></mml:math></inline-formula> and number of spikes <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi mathsize="80%">K</mml:mi><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="80%" mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mi mathsize="80%">N</mml:mi></mml:msub></mml:mrow><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> fired by <inline-formula><mml:math id="inf31"><mml:mi mathsize="80%">N</mml:mi></mml:math></inline-formula> place cells, we can calculate the probability <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi mathsize="80%">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mrow><mml:mi mathsize="80%">K</mml:mi><mml:mo lspace="2.5pt" mathsize="80%" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathsize="80%">x</mml:mi></mml:mrow><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, estimating the number of spikes <inline-formula><mml:math id="inf33"><mml:mi mathsize="80%">K</mml:mi></mml:math></inline-formula> at location <inline-formula><mml:math id="inf34"><mml:mi mathsize="80%">x</mml:mi></mml:math></inline-formula>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>∏</mml:mo><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>!</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf35"><mml:mrow><mml:msub><mml:mi mathsize="80%">α</mml:mi><mml:mi mathsize="80%">i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">x</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the firing rate of cell <inline-formula><mml:math id="inf36"><mml:mi mathsize="80%">i</mml:mi></mml:math></inline-formula> at position <inline-formula><mml:math id="inf37"><mml:mi mathsize="80%">x</mml:mi></mml:math></inline-formula>. From this we can calculate the probability of the animals location given the observed spikes:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi mathsize="80%">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">x</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the historic position of the animal which we use to constrain <inline-formula><mml:math id="inf39"><mml:mrow><mml:mi mathsize="80%">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mrow><mml:mi mathsize="80%">x</mml:mi><mml:mo lspace="2.5pt" mathsize="80%" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathsize="80%">K</mml:mi></mml:mrow><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to provide a fair comparison to the convolutional decoder. The final estimate of position is based on the peak of <inline-formula><mml:math id="inf40"><mml:mrow><mml:mi mathsize="80%">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mrow><mml:mi mathsize="80%">x</mml:mi><mml:mo lspace="2.5pt" mathsize="80%" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathsize="80%">K</mml:mi></mml:mrow><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We implemented a Bayesian continuity prior using a Gaussian distribution centred around the previous decoded location <inline-formula><mml:math id="inf41"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> of the animal, adjusting the standard deviation based on the speed of the animal in the previous four timesteps, as implemented in <xref ref-type="bibr" rid="bib64">Zhang et al., 1998</xref>. Specifically we calculated the probability of the current location given the previous by:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>with the standard deviation adjusted based on the speed of the animal<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula>with constant V making the fraction dimensionless. We used the previous four timesteps (2 s) to estimate the speed of the animal and used V=1s. We only used the Bayesian decoder with continuity prior for the comparison between Bayesian decoders (see <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4B</xref>). In all other analyses, we used the Bayesian decoder without the continuity prior.</p><p>We used the same cross-validation splits as for the convolutional model and calculated the Euclidean distance between the real and decoded position. We performed grid search on one representative rat to find the optimal parameters regarding bin size and bin length for the Bayesian decoder. The optimized Bayesian decoder uses a Gaussian smoothing kernel with sigma = 1.5, a bin size of 2 cm for binning the ratemaps, and uses a bin length of 0.5 s (see <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4A</xref> for Bayesian performance across different time windows).</p></sec><sec id="s4-6"><title>Convolutional neural network</title><p>The model takes a three-dimensional wavelet transformed signal as input and uses convolutional layers connected to a regression head to decode continuous behavior. We use a kernel size of 3 throughout the model and keep the number of filters constant at 64 for the first eight layers while sharing the weights over the channel axis, while then doubling them for each following layer. For downsampling the input, we use a stride of 2, intermixed between the time and frequency dimension (Table S1, <xref ref-type="fig" rid="fig1">Figure 1B</xref>). As regularization we apply gaussian noise <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic" mathsize="80%">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">μ</mml:mi><mml:mo mathsize="80%" rspace="4.2pt" stretchy="false">,</mml:mo><mml:msup><mml:mi mathsize="80%">σ</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi mathsize="80%">μ</mml:mi><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi mathsize="80%">σ</mml:mi><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">1</mml:mn></mml:mrow></mml:math></inline-formula> to each input sample.</p><p>We extensively investigated the use of a convolutional long-short-term-memory (LSTM) after the initial convolutions, where we used backpropagation through time on the time dimension. In a simplified model, this led to a small decrease in decoding error for the model trained on position. We nevertheless decided to employ a model with only simple convolutions as one important aspect of this model is the simplicity of use for a neuroscientist. Moreover, we experimented with using a wavenet (<xref ref-type="bibr" rid="bib44">Oord et al., 2016</xref>) inspired model directly on the raw electrophysiological signal but noticed that the model using the wavelet transformed input outperformed the wavenet approach by a margin of around 20 cm for the positional decoding. The wavenet inspired model was considerably slower to train and therefore a full hyperparameter search could not be performed.</p><p>Previous models contrasting recurrent vs. convolutional networks (<xref ref-type="bibr" rid="bib6">Bai et al., 2018</xref>), find that convolutional layers outperform recurrent ones when trained directly on minimally processed data. The benchmarks typically used in classical sequence learning are one-dimensional, whereas we record two-dimensional raw input (time x channels) with a high sampling rate, complicating the amount of experimentation we could perform as the unprocessed data for a 2 s time window exceeds the capacity of GPU memory (30,000 x 128 time points per sample). In the related field of speech processing with sampling rates up to 48,000 Hz, the input is processed using log-mel feature banks which are computed with a 25 ms window and a 10 ms shift (<xref ref-type="bibr" rid="bib5">Bahdanau et al., 2016</xref>; <xref ref-type="bibr" rid="bib61">William et al., 2016</xref>; <xref ref-type="bibr" rid="bib49">Rohit et al., 2017</xref>). We therefore opted for a similar approach by using downsampled wavelet transformed signals, resulting in a 33.3 ms window given a downsampling size of z=1000. Note that with further downsampling there might be a risk of losing decoding precision, with some of the behaviors coming close to the downsampled rate (e.g. head direction can be up to 40 deg/s) (<xref ref-type="fig" rid="fig1s10">Figure 1—figure supplement 10</xref>).</p></sec><sec id="s4-7"><title>Model training</title><p>The model takes as input a three-dimensional wavelet transformed signal corresponding to time, frequency and channels, with frequencies logarithmically scaled between 0 Hz and 15.000Hz. An optimal temporal window of T=64 (corresponding to 2.13 s) was established by hyperparameter search taking into account the tradeoff between speed of training and model error. For training the model across the full duration of the experiment, we divided the experiment into five partitions and used cross-validation for testing the model on before unseen data partitions, that is we first used partitions 2 to 5 for training and one for testing, then 1, 3, 4, and 5 for training and two for testing and so on. The last partition uses 1 to 4 for training and five for testing. Importantly, the overlap introduced by using 2 s long samples was accounted for by using gaps (2 s) between the training partitions, making sure that training and test set are fully independent of each other. We then randomly sampled inputs and outputs from the training set. Each input had corresponding outputs for the position (X, Y in cm), head direction (in radians) and speed (in cm/s). We used Adam as our learning algorithm with a learning rate of 0.0007 and stopped training after we sampled 18,000 samples, divided into 150 batches for 15 epochs, each batch consisting of eight samples. During training, we multiplied the learning rate by 0.2 if validation performance did not improve for three epochs. We performed random hyperparameter search for the following parameters: learning rate, dropout, number of units in the fully connected layer and number of input timesteps. For calculating the chance level, we used a shuffling procedure in which the wavelet transformed electrophysiological signal is shifted relative to its corresponding position. After shuffling, we trained the model with the same setting as the unshuffled model and for the same number of epochs. The training was performed on one GTX1060 using Keras with Tensorflow as backend.</p></sec><sec id="s4-8"><title>Model comparison</title><p>In order to compare the performance of the network against the Bayesian decoder we simulated both models in a setting with artificially reduced inputs. We used 1 to 32 tetrodes as input for both decoders, with tetrodes taken top to bottom in order of the given tetrode number. The input of run one was then comprised of tetrodes 1 to 32, while run 2 used tetrodes 1 to 31. The last run uses only the first tetrode as input to both models. We then retrained both models with the artificially reduced number of tetrodes making sure both models have the same cross-validation splits and report decoding errors as the average of each cross-validation split.</p><p>To generate a further baseline measure of performance when decoding using wavelet transformed coefficients, we trained support vector machines to decode position from wavelet transformed CA1 recordings. We used either a linear kernel or a non-linear radial-basis-function (RBF) kernel to train the model, using a regularization factor of C=100. For the non-linear RBF kernel we set gamma to the default <inline-formula><mml:math id="inf45"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>*</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math></inline-formula> as implemented in the sklearn framework. The SVM model was trained on the same wavelet coefficients as the convolutional neural network.</p></sec><sec id="s4-9"><title>Model evaluation</title><p>For adjusting the model weights during training we use different loss terms depending on the behavior or stimuli which we decode.<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic" mathsize="80%">ℒ</mml:mi><mml:mrow><mml:mi mathsize="80%">E</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">D</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:msqrt><mml:mrow><mml:munderover><mml:mo largeop="true" mathsize="80%" movablelimits="false" stretchy="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi mathsize="80%">i</mml:mi><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">1</mml:mn></mml:mrow><mml:mi mathsize="80%">M</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathsize="80%">y</mml:mi><mml:mo mathsize="80%" stretchy="false">^</mml:mo></mml:mover><mml:mi mathsize="80%">i</mml:mi></mml:msub><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:msub><mml:mi mathsize="80%">y</mml:mi><mml:mi mathsize="80%">i</mml:mi></mml:msub></mml:mrow><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow><mml:mo mathsize="80%" mathvariant="italic" separator="true" stretchy="false"> </mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic" mathsize="80%">ℒ</mml:mi><mml:mrow><mml:mi mathsize="80%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">A</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">E</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mrow><mml:mfrac><mml:mn mathsize="80%">1</mml:mn><mml:mi mathsize="80%">M</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" mathsize="80%" movablelimits="false" stretchy="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi mathsize="80%">i</mml:mi><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">1</mml:mn></mml:mrow><mml:mi mathsize="80%">M</mml:mi></mml:munderover><mml:mrow><mml:mo maxsize="80%" minsize="80%">|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathsize="80%">y</mml:mi><mml:mo mathsize="80%" stretchy="false">^</mml:mo></mml:mover><mml:mi mathsize="80%">i</mml:mi></mml:msub><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:msub><mml:mi mathsize="80%">y</mml:mi><mml:mi mathsize="80%">i</mml:mi></mml:msub></mml:mrow><mml:mo maxsize="80%" minsize="80%">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="80%" mathvariant="italic" separator="true" stretchy="false"> </mml:mo><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic" mathsize="80%">ℒ</mml:mi><mml:mrow><mml:mi mathsize="80%">C</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">A</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">E</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mrow><mml:mi mathsize="80%">min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">[</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathsize="80%">y</mml:mi><mml:mo mathsize="80%" stretchy="false">^</mml:mo></mml:mover><mml:mi mathsize="80%">i</mml:mi></mml:msub><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:msub><mml:mi mathsize="80%">y</mml:mi><mml:mi mathsize="80%">i</mml:mi></mml:msub></mml:mrow><mml:mo maxsize="80%" minsize="80%">|</mml:mo></mml:mrow><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="80%" minsize="80%">|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathsize="80%">y</mml:mi><mml:mo mathsize="80%" stretchy="false">^</mml:mo></mml:mover><mml:mi mathsize="80%">i</mml:mi></mml:msub><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:msub><mml:mi mathsize="80%">y</mml:mi><mml:mi mathsize="80%">i</mml:mi></mml:msub></mml:mrow><mml:mo maxsize="80%" minsize="80%">|</mml:mo></mml:mrow><mml:mo mathsize="80%" stretchy="false">+</mml:mo><mml:mi mathsize="80%">π</mml:mi></mml:mrow><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="80%" minsize="80%">|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathsize="80%">y</mml:mi><mml:mo mathsize="80%" stretchy="false">^</mml:mo></mml:mover><mml:mi mathsize="80%">i</mml:mi></mml:msub><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:msub><mml:mi mathsize="80%">y</mml:mi><mml:mi mathsize="80%">i</mml:mi></mml:msub></mml:mrow><mml:mo maxsize="80%" minsize="80%">|</mml:mo></mml:mrow><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:mi mathsize="80%">π</mml:mi></mml:mrow><mml:mo maxsize="80%" minsize="80%">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For decoding of position from tetrode CA1 recordings we try to minimize the Euclidean loss between predicted and ground truth position (<inline-formula><mml:math id="inf46"><mml:msub><mml:mi class="ltx_font_mathcaligraphic" mathsize="80%">ℒ</mml:mi><mml:mrow><mml:mi mathsize="80%">E</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">D</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). We use the mean squared error for the decoding of speed (<inline-formula><mml:math id="inf47"><mml:msub><mml:mi class="ltx_font_mathcaligraphic" mathsize="80%">ℒ</mml:mi><mml:mrow><mml:mi mathsize="80%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">A</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">E</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and the cyclical absolute error for decoding of head direction (<inline-formula><mml:math id="inf48"><mml:msub><mml:mi class="ltx_font_mathcaligraphic" mathsize="80%">ℒ</mml:mi><mml:mrow><mml:mi mathsize="80%">C</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">A</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">E</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). For all other behaviors or stimuli we use <inline-formula><mml:math id="inf49"><mml:msub><mml:mi class="ltx_font_mathcaligraphic" mathsize="80%">ℒ</mml:mi><mml:mrow><mml:mi mathsize="80%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">A</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">E</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as the default optimizer.</p><p>We decided to use <inline-formula><mml:math id="inf50"><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula> scores to measure model performance across different behaviors, brain areas, and recording techniques. We use the formulation of fraction of variance accounted for (FVAF) instead of the squared Pearson’s correlation coefficient. Both terms are based on the fraction of the residual sum of squares and the total sum of squares:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="80%">1</mml:mn><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo largeop="true" mathsize="80%" stretchy="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi mathsize="80%">i</mml:mi><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">1</mml:mn></mml:mrow><mml:mi mathsize="80%">M</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="80%">y</mml:mi><mml:mi mathsize="80%">i</mml:mi></mml:msub><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="80%">α</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathsize="80%">y</mml:mi><mml:mo mathsize="80%" stretchy="false">^</mml:mo></mml:mover><mml:mi mathsize="80%">i</mml:mi></mml:msub></mml:mrow><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:mi mathsize="80%">β</mml:mi></mml:mrow><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" mathsize="80%" stretchy="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi mathsize="80%">i</mml:mi><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">1</mml:mn></mml:mrow><mml:mi mathsize="80%">M</mml:mi></mml:msubsup><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="80%">y</mml:mi><mml:mi mathsize="80%">i</mml:mi></mml:msub><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:mover accent="true"><mml:mi mathsize="80%">y</mml:mi><mml:mo mathsize="80%" stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math id="inf51"><mml:msub><mml:mi mathsize="80%">y</mml:mi><mml:mi mathsize="80%">i</mml:mi></mml:msub></mml:math></inline-formula> the ground truth of sample <inline-formula><mml:math id="inf52"><mml:mi mathsize="80%">i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf53"><mml:msub><mml:mover accent="true"><mml:mi mathsize="80%">y</mml:mi><mml:mo mathsize="80%" stretchy="false">^</mml:mo></mml:mover><mml:mi mathsize="80%">i</mml:mi></mml:msub></mml:math></inline-formula> the predicted value and <inline-formula><mml:math id="inf54"><mml:mover accent="true"><mml:mi mathsize="80%">y</mml:mi><mml:mo mathsize="80%" stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> the mean value. Here, Pearson’s correlation coefficient tries to maximize <inline-formula><mml:math id="inf55"><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula> by adjusting <inline-formula><mml:math id="inf56"><mml:mi mathsize="80%">α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf57"><mml:mi mathsize="80%">β</mml:mi></mml:math></inline-formula> while FVAF uses <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi mathsize="80%">α</mml:mi><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi mathsize="80%">β</mml:mi><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">0</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib18">Fagg et al., 2009</xref>). This provides a more conservative measure of performance as FVAF requires that prediction and ground truth fit without scaling the predicted values. FVAF in turn has no lower bound as the prediction can be arbitrarily worse with a given scaling constant (i.e. given a ground truth value of 10, a prediction of 1000 has a lower (worse) <inline-formula><mml:math id="inf60"><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula> score than a prediction of 100).</p></sec><sec id="s4-10"><title>Detection of replay events</title><p>To investigate if the model can detect replay events we reran the analysis using a lower downsampling factor. In addition, we used a new dataset in which animals performed a goal directed task with specific reward locations and in which we had previously detected sharp-wave ripples using standard methods. We applied the same preprocessing only adjusting the downsampling factor to 60, resulting in a sampling rate of 500 Hz instead of the previously used 30 Hz. We then trained the model using the default 64 samples (128 ms) in order to obtain a better estimate of decoded positions during ripples. All other model parameters were kept the same. We then trained the model using full cross-validation and decoded the animal position every 2 ms, resulting in over 1 million decoded positions for the experiment which lasted ∼35 min. Ripples in the raw neural data were extracted by finding regions in the band-pass filtered ripple band (150–250 Hz) that were above a certain threshold (five standard deviations above the mean). We expanded these until power was back at a lower threshold (0.5 standard deviations above the mean). We only kept ripples which were longer than 60 ms and occurred during stationary periods. To quantify putative replay events detected by the CNN we calculated three measures. First, we evaluated if the events showed a higher overall decoding error by calculating the average loss – Euclidean distance between decoded and true location. Second, we quantified the length of each event to detect if they are longer than the average decoded trajectory during immobility, matched to be the same length as ripple periods. Third, we generated a coherence measure to assess the smoothness or continuity of each trajectory, defined as the absolute distance between each point on the trajectory to a second degree polynomial fitted to the entire trajectory. To obtain a statistical score for all three, we permuted the events in time (keeping the absolute length of each) and recalculated each measure 1000 times. We only used periods where the speed distribution of the original replay events matched the shuffled replay events (stationary periods).</p></sec><sec id="s4-11"><title>Influence maps</title><p>To investigate which frequencies, channels or timepoints were informative for the respective decoding we performed a bootstrapping procedure after training the models. For each sample in time, we calculated the real decoding error <inline-formula><mml:math id="inf61"><mml:msub><mml:mi mathsize="80%">e</mml:mi><mml:mi mathsize="80%">o</mml:mi></mml:msub></mml:math></inline-formula> for each behavior by using the wavelets as input. We then shuffle the wavelets for a particular frequency and re-calculate the error. We then define the influence of a given frequency or channel as the relative change: <inline-formula><mml:math id="inf62"><mml:mfrac><mml:mrow><mml:msub><mml:mi mathsize="80%">e</mml:mi><mml:mi mathsize="80%">s</mml:mi></mml:msub><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:msub><mml:mi mathsize="80%">e</mml:mi><mml:mi mathsize="80%">o</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi mathsize="80%">e</mml:mi><mml:mi mathsize="80%">o</mml:mi></mml:msub></mml:mfrac></mml:math></inline-formula> where <inline-formula><mml:math id="inf63"><mml:msub><mml:mi mathsize="80%">e</mml:mi><mml:mi mathsize="80%">o</mml:mi></mml:msub></mml:math></inline-formula> is the original error and <inline-formula><mml:math id="inf64"><mml:msub><mml:mi mathsize="80%">e</mml:mi><mml:mi mathsize="80%">s</mml:mi></mml:msub></mml:math></inline-formula> the shuffled error. We repeat this for the channel and time dimension to get an estimate of how much influence each channel or timepoint has on the decoding of a given behavior.</p><p>To evaluate if the influence measure accurately captures the true information content, we used simulated behaviors in which ground truth information was known. We used the preprocessed wavelet transformed data from one animal and created a simulated behavior (<inline-formula><mml:math id="inf65"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) using uniform random noise. Two frequency bands were then modulated by the simulated behavior using <inline-formula><mml:math id="inf66"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:mi>β</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. We used β=2 for 58 Hz and β=1 for 3750 Hz. We then retrained the model using five-fold cross validation and evaluated the influence measure as previously described. We report the proportion of frequency bands that fall into the correct frequencies (i.e. the frequencies we chose to be modulated, 58 Hz and 3750 Hz).</p><p>We also tried calculating sample gradients with respect to our inputs (<xref ref-type="bibr" rid="bib55">Simonyan et al., 2013</xref>). For this, we calculated the derivative <inline-formula><mml:math id="inf67"><mml:mi mathsize="80%">w</mml:mi></mml:math></inline-formula> by back-propagation for each sample and with respect to the inputs. In contrast to class saliency maps, we obtain a gradient estimate indicating how much each part of the input strongly drives the regression output. We calculate saliency maps for each sample cross-validated over the entire experiment. For deriving influence maps from the raw gradients we calculate the variance across the time dimension and use this as an estimate of how much influence each frequency band or channel has on the decoding. This method however introduces a lot of high-frequency noise in the gradients, possibly coming from the strides in the convolutional layers used throughout the model (<xref ref-type="bibr" rid="bib43">Olah et al., 2017</xref>).</p></sec><sec id="s4-12"><title>Code availability</title><p>We provide the full code and notebooks demonstrating the usage of our method on electrophysiological and two-photon calcium imaging data. All examples can be adapted to predict one-dimensional (default) or n-dimensional behaviors or stimuli from any desired brain area. The code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/CYHSM/DeepInsight">https://github.com/CYHSM/DeepInsight</ext-link>.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Software, Formal analysis, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Data curation</p></fn><fn fn-type="con" id="con3"><p>Resources, Data curation</p></fn><fn fn-type="con" id="con4"><p>Resources, Data curation</p></fn><fn fn-type="con" id="con5"><p>Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization</p></fn><fn fn-type="con" id="con7"><p>Supervision, Methodology</p></fn><fn fn-type="con" id="con8"><p>Funding acquisition</p></fn><fn fn-type="con" id="con9"><p>Resources, Data curation</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Supervision, Funding acquisition</p></fn><fn fn-type="con" id="con11"><p>Conceptualization, Resources, Data curation, Supervision, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All procedures were approved by the UK Home Office, subject to the restrictions and provisions contained in the Animals Scientific Procedures Act of 1986.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Layer by layer architecture of the convolutional model.</title><p>Note that the first layers 1–8 share the weights over the channel dimension while layers 9–15 share the weights across the time dimension. Layers 9 to 15 depict the kernel sizes and strides for the tetrode recordings with 128 channels. For recordings with different number of channels we adjust the number of downsampling layers to match the dimension of layer 15. Order of dimensions: Time, Frequency, Channels.</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-66551-supp1-v1.xlsx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-66551-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Raw neural recordings from CA1 in rodents using tetrode or two-photon calcium imaging, as well as auditory cortex recordings in mice are available here: <ext-link ext-link-type="uri" xlink:href="https://figshare.com/collections/DeepInsight_-_Data_sharing/5486703">https://figshare.com/collections/DeepInsight_-_Data_sharing/5486703</ext-link>. The ECoG dataset is part of a BCI competition (<ext-link ext-link-type="uri" xlink:href="http://www.bbci.de/competition/iv/">http://www.bbci.de/competition/iv/</ext-link>) and can be obtained from <ext-link ext-link-type="uri" xlink:href="https://purl.stanford.edu/zk881ps0522">https://purl.stanford.edu/zk881ps0522</ext-link> (fingerflex.zip).</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Tanni</surname><given-names>S</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Tetrode recordings from CA1</data-title><source>figshare</source><pub-id assigning-authority="figshare" pub-id-type="doi">10.6084/m9.figshare.14909766.v3</pub-id></element-citation></p><p><element-citation id="dataset2" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>O'Leary</surname><given-names>A</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Calcium traces from CA1</data-title><source>figshare</source><pub-id assigning-authority="figshare" pub-id-type="doi">10.6084/m9.figshare.12687881</pub-id></element-citation></p><p><element-citation id="dataset3" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Perrodin</surname><given-names>C</given-names></name><name><surname>Bendor</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Tetrode recordings from auditory cortex</data-title><source>figshare</source><pub-id assigning-authority="figshare" pub-id-type="doi">10.6084/m9.figshare.14909730.v2</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ackermann</surname> <given-names>E</given-names></name><name><surname>Kemere</surname> <given-names>CT</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Unsupervised clusterless decoding using a switching poisson hidden markov model</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/760470</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agarwal</surname> <given-names>G</given-names></name><name><surname>Stevenson</surname> <given-names>IH</given-names></name><name><surname>Berényi</surname> <given-names>A</given-names></name><name><surname>Mizuseki</surname> <given-names>K</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name><name><surname>Sommer</surname> <given-names>FT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Spatially distributed local fields in the Hippocampus encode rat position</article-title><source>Science</source><volume>344</volume><fpage>626</fpage><lpage>630</lpage><pub-id pub-id-type="doi">10.1126/science.1250444</pub-id><pub-id pub-id-type="pmid">24812401</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antelis</surname> <given-names>JM</given-names></name><name><surname>Montesano</surname> <given-names>L</given-names></name><name><surname>Ramos-Murguialday</surname> <given-names>A</given-names></name><name><surname>Birbaumer</surname> <given-names>N</given-names></name><name><surname>Minguez</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>On the Usage of Linear Regression Models to Reconstruct Limb Kinematics from Low Frequency EEG Signals</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e61976</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0061976</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aronov</surname> <given-names>D</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Engagement of Neural Circuits Underlying 2D Spatial Navigation in a Rodent Virtual Reality System</article-title><source>Neuron</source><volume>84</volume><fpage>442</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.042</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bahdanau</surname> <given-names>D</given-names></name><name><surname>Chorowski</surname> <given-names>J</given-names></name><name><surname>Serdyuk</surname> <given-names>D</given-names></name><name><surname>Brakel</surname> <given-names>P</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>End-to-End Attention-based large vocabulary speech recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1508.04395">https://arxiv.org/abs/1508.04395</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bai</surname> <given-names>S</given-names></name><name><surname>Zico Kolter</surname> <given-names>J</given-names></name><name><surname>Koltun</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.01271">https://arxiv.org/abs/1803.01271</ext-link></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cammarata</surname> <given-names>N</given-names></name><name><surname>Carter</surname> <given-names>S</given-names></name><name><surname>Goh</surname> <given-names>G</given-names></name><name><surname>Olah</surname> <given-names>C</given-names></name><name><surname>Petrov</surname> <given-names>M</given-names></name><name><surname>Schubert</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Thread: circuits</article-title><source>Distill</source><volume>5</volume><elocation-id>e24</elocation-id><pub-id pub-id-type="doi">10.23915/distill.00024</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>TW</given-names></name><name><surname>Wardill</surname> <given-names>TJ</given-names></name><name><surname>Sun</surname> <given-names>Y</given-names></name><name><surname>Pulver</surname> <given-names>SR</given-names></name><name><surname>Renninger</surname> <given-names>SL</given-names></name><name><surname>Baohan</surname> <given-names>A</given-names></name><name><surname>Schreiter</surname> <given-names>ER</given-names></name><name><surname>Kerr</surname> <given-names>RA</given-names></name><name><surname>Orger</surname> <given-names>MB</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name><name><surname>Looger</surname> <given-names>LL</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name><name><surname>Kim</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Ultra-sensitive fluorescent proteins for imaging neuronal activity</article-title><source>Nature</source><volume>499</volume><fpage>295</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1038/nature12354</pub-id><pub-id pub-id-type="pmid">23868258</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christopher</surname> <given-names>T</given-names></name><name><surname>Gilbert</surname> <given-names>PC</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A practical guide to wavelet analysis</article-title><source>Bulletin of the American Meteorological Society</source><volume>79</volume><fpage>61</fpage><lpage>78</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname> <given-names>JE</given-names></name><name><surname>Magland</surname> <given-names>JF</given-names></name><name><surname>Barnett</surname> <given-names>AH</given-names></name><name><surname>Tolosa</surname> <given-names>VM</given-names></name><name><surname>Tooker</surname> <given-names>AC</given-names></name><name><surname>Lee</surname> <given-names>KY</given-names></name><name><surname>Shah</surname> <given-names>KG</given-names></name><name><surname>Felix</surname> <given-names>SH</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name><name><surname>Greengard</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A fully automated approach to spike sorting</article-title><source>Neuron</source><volume>95</volume><fpage>1381</fpage><lpage>1394</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.08.030</pub-id><pub-id pub-id-type="pmid">28910621</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Cichy Radoslaw</surname> <given-names>M</given-names></name><name><surname>Daniel</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep Neural Networks as Scientific Models</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>305</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.01.009</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corrado</surname> <given-names>GS</given-names></name><name><surname>Sugrue</surname> <given-names>LP</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Linear-Nonlinear-Poisson models of primate choice dynamics</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>84</volume><fpage>581</fpage><lpage>617</lpage><pub-id pub-id-type="doi">10.1901/jeab.2005.23-05</pub-id><pub-id pub-id-type="pmid">16596981</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Csicsvari</surname> <given-names>J</given-names></name><name><surname>Hirase</surname> <given-names>H</given-names></name><name><surname>Czurkó</surname> <given-names>A</given-names></name><name><surname>Mamiya</surname> <given-names>A</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Oscillatory coupling of hippocampal pyramidal cells and interneurons in the behaving rat</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>274</fpage><lpage>287</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-01-00274.1999</pub-id><pub-id pub-id-type="pmid">9870957</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname> <given-names>X</given-names></name><name><surname>Liu</surname> <given-names>DF</given-names></name><name><surname>Kay</surname> <given-names>K</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name><name><surname>Eden</surname> <given-names>UT</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Clusterless decoding of position from multiunit activity using A marked point process filter</article-title><source>Neural Computation</source><volume>27</volume><fpage>1438</fpage><lpage>1460</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00744</pub-id><pub-id pub-id-type="pmid">25973549</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doeller</surname> <given-names>CF</given-names></name><name><surname>Barry</surname> <given-names>C</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Evidence for grid cells in a human memory network</article-title><source>Nature</source><volume>463</volume><fpage>657</fpage><lpage>661</lpage><pub-id pub-id-type="doi">10.1038/nature08704</pub-id><pub-id pub-id-type="pmid">20090680</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>English</surname> <given-names>DF</given-names></name><name><surname>McKenzie</surname> <given-names>S</given-names></name><name><surname>Evans</surname> <given-names>T</given-names></name><name><surname>Kim</surname> <given-names>K</given-names></name><name><surname>Yoon</surname> <given-names>E</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Pyramidal Cell-Interneuron circuit architecture and dynamics in hippocampal networks</article-title><source>Neuron</source><volume>96</volume><fpage>505</fpage><lpage>520</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.033</pub-id><pub-id pub-id-type="pmid">29024669</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epsztein</surname> <given-names>J</given-names></name><name><surname>Brecht</surname> <given-names>M</given-names></name><name><surname>Lee</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Intracellular determinants of hippocampal CA1 place and silent cell activity in a novel environment</article-title><source>Neuron</source><volume>70</volume><fpage>109</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.03.006</pub-id><pub-id pub-id-type="pmid">21482360</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fagg</surname> <given-names>AH</given-names></name><name><surname>Ojakangas</surname> <given-names>GW</given-names></name><name><surname>Miller</surname> <given-names>LE</given-names></name><name><surname>Hatsopoulos</surname> <given-names>NG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Kinetic Trajectory Decoding Using Motor Cortical Ensembles</article-title><source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source><volume>17</volume><fpage>487</fpage><lpage>496</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2009.2029313</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Glaser</surname> <given-names>JI</given-names></name><name><surname>Chowdhury</surname> <given-names>RH</given-names></name><name><surname>Perich</surname> <given-names>MG</given-names></name><name><surname>Miller</surname> <given-names>LE</given-names></name><name><surname>Kording</surname> <given-names>KP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Machine learning for neural decoding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1708.00909">https://arxiv.org/abs/1708.00909</ext-link></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Góis</surname> <given-names>Z</given-names></name><name><surname>Tort</surname> <given-names>ABL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Characterizing speed cells in the rat Hippocampus</article-title><source>Cell Reports</source><volume>25</volume><fpage>1872</fpage><lpage>1884</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2018.10.054</pub-id><pub-id pub-id-type="pmid">30428354</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafting</surname> <given-names>T</given-names></name><name><surname>Fyhn</surname> <given-names>M</given-names></name><name><surname>Molden</surname> <given-names>S</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Microstructure of a spatial map in the entorhinal cortex</article-title><source>Nature</source><volume>436</volume><fpage>801</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1038/nature03721</pub-id><pub-id pub-id-type="pmid">15965463</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardcastle</surname> <given-names>K</given-names></name><name><surname>Maheswaranathan</surname> <given-names>N</given-names></name><name><surname>Ganguli</surname> <given-names>S</given-names></name><name><surname>Giocomo</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Heterogeneous, and adaptive code for navigation in medial entorhinal cortex</article-title><source>Neuron</source><volume>94</volume><fpage>375</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.03.025</pub-id><pub-id pub-id-type="pmid">28392071</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Henze</surname> <given-names>DA</given-names></name><name><surname>Csicsvari</surname> <given-names>J</given-names></name><name><surname>Hirase</surname> <given-names>H</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Accuracy of tetrode spike separation as determined by simultaneous intracellular and extracellular measurements</article-title><source>Journal of Neurophysiology</source><volume>84</volume><fpage>401</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.84.1.401</pub-id><pub-id pub-id-type="pmid">10899214</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Nastase</surname> <given-names>SA</given-names></name><name><surname>Goldstein</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Direct fit to nature: an evolutionary perspective on biological and artificial neural networks</article-title><source>Neuron</source><volume>105</volume><fpage>416</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.12.002</pub-id><pub-id pub-id-type="pmid">32027833</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname> <given-names>DH</given-names></name><name><surname>Wiesel</surname> <given-names>TN</given-names></name><name><surname>Hubel David</surname> <given-names>H</given-names></name><name><surname>Wiesel Torsten</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Receptive fields of single neurones in the cat's striate cortex</article-title><source>The Journal of Physiology</source><volume>148</volume><fpage>574</fpage><lpage>591</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1959.sp006308</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huh</surname> <given-names>CY</given-names></name><name><surname>Amilhon</surname> <given-names>B</given-names></name><name><surname>Ferguson</surname> <given-names>KA</given-names></name><name><surname>Manseau</surname> <given-names>F</given-names></name><name><surname>Torres-Platas</surname> <given-names>SG</given-names></name><name><surname>Peach</surname> <given-names>JP</given-names></name><name><surname>Scodras</surname> <given-names>S</given-names></name><name><surname>Mechawar</surname> <given-names>N</given-names></name><name><surname>Skinner</surname> <given-names>FK</given-names></name><name><surname>Williams</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Excitatory inputs determine Phase-Locking strength and Spike-Timing of CA1 stratum Oriens/Alveus parvalbumin and somatostatin interneurons during intrinsically generated hippocampal theta rhythm</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>6605</fpage><lpage>6622</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3951-13.2016</pub-id><pub-id pub-id-type="pmid">27335395</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hyung</surname> <given-names>LJ</given-names></name><name><surname>Carlson</surname> <given-names>DE</given-names></name><name><surname>Razaghi</surname> <given-names>HS</given-names></name><name><surname>Yao</surname> <given-names>W</given-names></name><name><surname>Goetz</surname> <given-names>G</given-names></name><name><surname>Hagen</surname> <given-names>E</given-names></name><name><surname>Batty</surname> <given-names>E</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Einevoll</surname> <given-names>GT</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>YASS: yet another spike sorter</article-title><conf-name>NIPS</conf-name></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeewajee</surname> <given-names>A</given-names></name><name><surname>Barry</surname> <given-names>C</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Grid cells and theta as oscillatory interference: electrophysiological data from freely-moving rats</article-title><source>Hippocampus</source><volume>18</volume><elocation-id>1175</elocation-id><pub-id pub-id-type="doi">10.1002/hipo.20510</pub-id><pub-id pub-id-type="pmid">19021251</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jercog</surname> <given-names>PE</given-names></name><name><surname>Ahmadian</surname> <given-names>Y</given-names></name><name><surname>Woodruff</surname> <given-names>C</given-names></name><name><surname>Deb-Sen</surname> <given-names>R</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name><name><surname>Kandel</surname> <given-names>ER</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Heading direction with respect to a reference point modulates place-cell activity</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2333</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10139-7</pub-id><pub-id pub-id-type="pmid">31133685</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klausberger</surname> <given-names>T</given-names></name><name><surname>Magill</surname> <given-names>PJ</given-names></name><name><surname>Márton</surname> <given-names>LF</given-names></name><name><surname>Roberts</surname> <given-names>JD</given-names></name><name><surname>Cobden</surname> <given-names>PM</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name><name><surname>Somogyi</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Brain-state- and cell-type-specific firing of hippocampal interneurons in vivo</article-title><source>Nature</source><volume>421</volume><fpage>844</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1038/nature01374</pub-id><pub-id pub-id-type="pmid">12594513</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kloosterman</surname> <given-names>F</given-names></name><name><surname>Layton</surname> <given-names>SP</given-names></name><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bayesian decoding using unsorted spikes in the rat Hippocampus</article-title><source>Journal of Neurophysiology</source><volume>111</volume><fpage>217</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1152/jn.01046.2012</pub-id><pub-id pub-id-type="pmid">24089403</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kramis</surname> <given-names>R</given-names></name><name><surname>Vanderwolf</surname> <given-names>CH</given-names></name><name><surname>Bland</surname> <given-names>BH</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Two types of hippocampal rhythmical slow activity in both the rabbit and the rat: relations to behavior and effects of atropine, diethyl ether, urethane, and pentobarbital</article-title><source>Experimental Neurology</source><volume>49</volume><fpage>58</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1016/0014-4886(75)90195-8</pub-id><pub-id pub-id-type="pmid">1183532</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname> <given-names>A</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>ImageNet classification with deep convolutional neural networks Event-Place</article-title><conf-name>Proceedings of the 25th International Conference on Neural Information Processing Systems</conf-name><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kropff</surname> <given-names>E</given-names></name><name><surname>Carmichael</surname> <given-names>JE</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Speed cells in the medial entorhinal cortex</article-title><source>Nature</source><volume>523</volume><fpage>419</fpage><lpage>424</lpage><pub-id pub-id-type="doi">10.1038/nature14622</pub-id><pub-id pub-id-type="pmid">26176924</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Hinton</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McFarland</surname> <given-names>WL</given-names></name><name><surname>Teitelbaum</surname> <given-names>H</given-names></name><name><surname>Hedges</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Relationship between hippocampal theta activity and running speed in the rat</article-title><source>Journal of Comparative and Physiological Psychology</source><volume>88</volume><fpage>324</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1037/h0076177</pub-id><pub-id pub-id-type="pmid">1120805</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mnih</surname> <given-names>V</given-names></name><name><surname>Kavukcuoglu</surname> <given-names>K</given-names></name><name><surname>Silver</surname> <given-names>D</given-names></name><name><surname>Rusu</surname> <given-names>AA</given-names></name><name><surname>Veness</surname> <given-names>J</given-names></name><name><surname>Bellemare</surname> <given-names>MG</given-names></name><name><surname>Graves</surname> <given-names>A</given-names></name><name><surname>Riedmiller</surname> <given-names>M</given-names></name><name><surname>Fidjeland</surname> <given-names>AK</given-names></name><name><surname>Ostrovski</surname> <given-names>G</given-names></name><name><surname>Petersen</surname> <given-names>S</given-names></name><name><surname>Beattie</surname> <given-names>C</given-names></name><name><surname>Sadik</surname> <given-names>A</given-names></name><name><surname>Antonoglou</surname> <given-names>I</given-names></name><name><surname>King</surname> <given-names>H</given-names></name><name><surname>Kumaran</surname> <given-names>D</given-names></name><name><surname>Wierstra</surname> <given-names>D</given-names></name><name><surname>Legg</surname> <given-names>S</given-names></name><name><surname>Hassabis</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Human-level control through deep reinforcement learning</article-title><source>Nature</source><volume>518</volume><fpage>529</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1038/nature14236</pub-id><pub-id pub-id-type="pmid">25719670</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname> <given-names>RU</given-names></name><name><surname>Kubie</surname> <given-names>JL</given-names></name><name><surname>Ranck</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Spatial firing patterns of hippocampal complex-spike cells in a fixed environment</article-title><source>The Journal of Neuroscience</source><volume>7</volume><fpage>1935</fpage><lpage>1950</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.07-07-01935.1987</pub-id><pub-id pub-id-type="pmid">3612225</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname> <given-names>RU</given-names></name><name><surname>Bostock</surname> <given-names>E</given-names></name><name><surname>Taube</surname> <given-names>JS</given-names></name><name><surname>Kubie</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>On the directional firing properties of hippocampal place cells</article-title><source>The Journal of Neuroscience</source><volume>14</volume><fpage>7235</fpage><lpage>7251</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.14-12-07235.1994</pub-id><pub-id pub-id-type="pmid">7996172</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Dostrovsky</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The Hippocampus as a spatial map preliminary evidence from unit activity in the freely-moving rat</article-title><source>Brain Research</source><volume>34</volume><fpage>171</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(71)90358-1</pub-id><pub-id pub-id-type="pmid">5124915</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Recce</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Phase relationship between hippocampal place units and the EEG theta rhythm</article-title><source>Hippocampus</source><volume>3</volume><fpage>317</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.1002/hipo.450030307</pub-id><pub-id pub-id-type="pmid">8353611</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ólafsdóttir</surname> <given-names>HF</given-names></name><name><surname>Barry</surname> <given-names>C</given-names></name><name><surname>Saleem</surname> <given-names>AB</given-names></name><name><surname>Hassabis</surname> <given-names>D</given-names></name><name><surname>Spiers</surname> <given-names>HJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hippocampal place cells construct reward related sequences through unexplored space</article-title><source>eLife</source><volume>4</volume><elocation-id>e06063</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06063</pub-id><pub-id pub-id-type="pmid">26112828</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olah</surname> <given-names>C</given-names></name><name><surname>Mordvintsev</surname> <given-names>A</given-names></name><name><surname>Schubert</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Feature visualization</article-title><source>Distill</source><volume>2</volume><elocation-id>e7</elocation-id><pub-id pub-id-type="doi">10.23915/distill.00007</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Oord</surname> <given-names>A</given-names></name><name><surname>Dieleman</surname> <given-names>S</given-names></name><name><surname>Zen</surname> <given-names>H</given-names></name><name><surname>Simonyan</surname> <given-names>K</given-names></name><name><surname>Vinyals</surname> <given-names>O</given-names></name><name><surname>Graves</surname> <given-names>A</given-names></name><name><surname>Kalchbrenner</surname> <given-names>N</given-names></name><name><surname>Senior</surname> <given-names>A</given-names></name><name><surname>Kavukcuoglu</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>WaveNet: a generative model for raw audio</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1609.03499">https://arxiv.org/abs/1609.03499</ext-link></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>O’keefe</surname> <given-names>J</given-names></name><name><surname>Nadel</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="1978">1978</year><source>The Hippocampus as a Cognitive Map</source><publisher-loc>Oxford</publisher-loc><publisher-name>Clarendon Press</publisher-name></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Steinmetz</surname> <given-names>N</given-names></name><name><surname>Kadir</surname> <given-names>S</given-names></name><name><surname>Kenneth</surname> <given-names>DH</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris kenneth</surname> <given-names>D</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Kilosort: realtime Spike-Sorting for extracellular electrophysiology with hundreds of channels</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/061481</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Stringer</surname> <given-names>C</given-names></name><name><surname>Dipoppa</surname> <given-names>M</given-names></name><name><surname>Schröder</surname> <given-names>S</given-names></name><name><surname>Federico Rossi</surname> <given-names>L</given-names></name><name><surname>Dalgleish</surname> <given-names>H</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Suite2p: beyond 10,000 neurons with standard two-photon microscopy</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/061507</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Preston-Ferrer</surname> <given-names>P</given-names></name><name><surname>Coletta</surname> <given-names>S</given-names></name><name><surname>Frey</surname> <given-names>M</given-names></name><name><surname>Burgalossi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Anatomical organization of presubicular head-direction circuits</article-title><source>eLife</source><volume>5</volume><elocation-id>e14592</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.14592</pub-id><pub-id pub-id-type="pmid">27282390</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rohit</surname> <given-names>P</given-names></name><name><surname>Rao</surname> <given-names>K</given-names></name><name><surname>Sainath</surname> <given-names>TN</given-names></name><name><surname>Li</surname> <given-names>B</given-names></name><name><surname>Johnson</surname> <given-names>L</given-names></name><name><surname>Jaitly</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title><italic>A comparison of Sequence-to-Sequence Models for Speech Recognition</italic></article-title><conf-name>Interspeech 2017</conf-name><fpage>939</fpage><lpage>943</lpage></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname> <given-names>A</given-names></name><name><surname>Yartsev</surname> <given-names>MM</given-names></name><name><surname>Ulanovsky</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Encoding of head direction by hippocampal place cells in bats</article-title><source>The Journal of Neuroscience</source><volume>34</volume><elocation-id>1067</elocation-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5393-12.2014</pub-id><pub-id pub-id-type="pmid">24431464</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sainsbury</surname> <given-names>RS</given-names></name><name><surname>Heynen</surname> <given-names>A</given-names></name><name><surname>Montoya</surname> <given-names>CP</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Behavioral correlates of hippocampal type 2 theta in the rat</article-title><source>Physiology &amp; Behavior</source><volume>39</volume><fpage>513</fpage><lpage>519</lpage><pub-id pub-id-type="doi">10.1016/0031-9384(87)90382-9</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sargolini</surname> <given-names>F</given-names></name><name><surname>Fyhn</surname> <given-names>M</given-names></name><name><surname>Hafting</surname> <given-names>T</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>Witter</surname> <given-names>MP</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Conjunctive representation of position, direction, and velocity in entorhinal cortex</article-title><source>Science</source><volume>312</volume><fpage>758</fpage><lpage>762</lpage><pub-id pub-id-type="doi">10.1126/science.1125572</pub-id><pub-id pub-id-type="pmid">16675704</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schalk</surname> <given-names>G</given-names></name><name><surname>Kubánek</surname> <given-names>J</given-names></name><name><surname>Miller</surname> <given-names>KJ</given-names></name><name><surname>Anderson</surname> <given-names>NR</given-names></name><name><surname>Leuthardt</surname> <given-names>EC</given-names></name><name><surname>Ojemann</surname> <given-names>JG</given-names></name><name><surname>Limbrick</surname> <given-names>D</given-names></name><name><surname>Moran</surname> <given-names>D</given-names></name><name><surname>Gerhardt</surname> <given-names>LA</given-names></name><name><surname>Wolpaw</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Decoding two-dimensional movement trajectories using electrocorticographic signals in humans</article-title><source>Journal of Neural Engineering</source><volume>4</volume><fpage>264</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1088/1741-2560/4/3/012</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegle</surname> <given-names>JH</given-names></name><name><surname>López</surname> <given-names>AC</given-names></name><name><surname>Patel</surname> <given-names>YA</given-names></name><name><surname>Abramov</surname> <given-names>K</given-names></name><name><surname>Ohayon</surname> <given-names>S</given-names></name><name><surname>Voigts</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Open Ephys: an open-source, plugin-based platform for multichannel electrophysiology</article-title><source>Journal of Neural Engineering</source><volume>14</volume><elocation-id>045003</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aa5eea</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Simonyan</surname> <given-names>K</given-names></name><name><surname>Vedaldi</surname> <given-names>A</given-names></name><name><surname>Zisserman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Deep inside convolutional networks: visualising image classification models and saliency maps</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1312.6034">https://arxiv.org/abs/1312.6034</ext-link></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stavisky</surname> <given-names>SD</given-names></name><name><surname>Kao</surname> <given-names>JC</given-names></name><name><surname>Nuyujukian</surname> <given-names>P</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A high performing brain-machine interface driven by low-frequency local field potentials alone and together with spikes</article-title><source>Journal of Neural Engineering</source><volume>12</volume><elocation-id>036009</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/12/3/036009</pub-id><pub-id pub-id-type="pmid">25946198</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tampuu</surname> <given-names>A</given-names></name><name><surname>Matiisen</surname> <given-names>T</given-names></name><name><surname>Ólafsdóttir</surname> <given-names>HF</given-names></name><name><surname>Barry</surname> <given-names>C</given-names></name><name><surname>Vicente</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Efficient neural decoding of self-location with a deep recurrent network</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/242867</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voigts</surname> <given-names>J</given-names></name><name><surname>Siegle</surname> <given-names>JH</given-names></name><name><surname>Pritchett</surname> <given-names>DL</given-names></name><name><surname>Moore</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The flexDrive: an ultra-light implant for optical control and highly parallel chronic recording of neuronal ensembles in freely moving mice</article-title><source>Frontiers in Systems Neuroscience</source><volume>7</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2013.00008</pub-id><pub-id pub-id-type="pmid">23717267</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname> <given-names>KMM</given-names></name><name><surname>Bizley</surname> <given-names>JK</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name><name><surname>Schnupp</surname> <given-names>JWH</given-names></name><name><surname>Andrew</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multiplexed and Robust Representations of Sound Features in Auditory Cortex</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>14565</fpage><lpage>14576</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2074-11.2011</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilent</surname> <given-names>WB</given-names></name><name><surname>Nitz</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Discrete place fields of hippocampal formation interneurons</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>4152</fpage><lpage>4161</lpage><pub-id pub-id-type="doi">10.1152/jn.01200.2006</pub-id><pub-id pub-id-type="pmid">17392415</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>William</surname> <given-names>C</given-names></name><name><surname>Jaitly</surname> <given-names>N</given-names></name><name><surname>Le</surname> <given-names>Q</given-names></name><name><surname>Vinyals</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Listen, attend and spell: a neural network for large vocabulary conversational speech recognition</article-title><conf-name>2016 IEEE International Conference on Acoustics</conf-name><pub-id pub-id-type="doi">10.1109/ICASSP.2016.7472621</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wood</surname> <given-names>F</given-names></name><name><surname>Black</surname> <given-names>MJ</given-names></name><name><surname>Vargas-Irwin</surname> <given-names>C</given-names></name><name><surname>Fellows</surname> <given-names>M</given-names></name><name><surname>Donoghue</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>On the variability of manual spike sorting</article-title><source>IEEE Transactions on Bio-Medical Engineering</source><volume>51</volume><fpage>912</fpage><lpage>918</lpage><pub-id pub-id-type="doi">10.1109/TBME.2004.826677</pub-id><pub-id pub-id-type="pmid">15188858</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoganarasimha</surname> <given-names>D</given-names></name><name><surname>Yu</surname> <given-names>X</given-names></name><name><surname>Knierim</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Head direction cell representations maintain internal coherence during conflicting proximal and distal cue rotations: comparison with hippocampal place cells</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>622</fpage><lpage>631</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3885-05.2006</pub-id><pub-id pub-id-type="pmid">16407560</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>K</given-names></name><name><surname>Ginzburg</surname> <given-names>I</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>Sejnowski</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells</article-title><source>Journal of Neurophysiology</source><volume>79</volume><fpage>1017</fpage><lpage>1044</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.79.2.1017</pub-id><pub-id pub-id-type="pmid">9463459</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.66551.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Deshmukh</surname><given-names>Sachin</given-names></name><role>Reviewing Editor</role><aff><institution>Indian Institute of Science Bangalore</institution><country>India</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/871848">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/871848v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Frey et al. describe a convolutional neural network capable of extracting behavioral correlates from wide-band LFP recordings or even lower-frequency imaging data. The analysis program described by the authors provides a rapid &quot;first pass&quot; analysis using raw, unprocessed data to generate hypotheses that can be tested later with conventional in-depth analyses. This approach is of real value to the community, particularly as it becomes more commonplace for labs to acquire multi-site in vivo recordings.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Interpreting wide-band neural activity using convolutional neural networks&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>The reviewers agree that the tools and resources described in the manuscript are a substantial contribution to the field, but have raised a number of concerns which need to be addressed. The requested changes do not require any new data, but do require new analyses to be performed. Complete reviewer recommendations are appended at the end of this message, and a summary of essential revisions follows.</p><p>Essential revisions:</p><p>1) The CNN described in the manuscript needs to be better characterized in terms of its history dependence, comparison to Bayesian decoding, and ability to decode non-local representations. Head direction decoding using CNN needs to be compared with that using a Bayesian decoder.</p><p>2) The ability of the CNN to discover underlying truth needs to be characterized using simulations.</p><p>3) Contributions of low frequency activity need to be better distinguished from the low frequency components of excitatory spikes.</p><p>4) The amount of data required for accurately recovering the frequency bands contributing to decoding needs to be characterized using simulations as well as sub-sampled data. Relative contributions of using the Wavelet coefficients as inputs and the CNN to improved accuracy also need to be characterized.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>In the current manuscript, Frey et al. describe a convolutional neural network capable of extracting behavioral correlates from wide-band LFP recordings or even lower-frequency imaging data. Other publications (referenced by the authors) have employed similar ideas previously, but to my knowledge, the current implementation is novel. In my opinion, the real value of this method, as the authors state in their final paragraph, is that it represents a rapid, &quot;first-pass&quot; analysis of large-scale electrophysiological recordings to quickly identify relevant neural features which can then become the focus of more in-depth analyses. As such, I think the analysis program described by the authors is of real value to the community, particularly as it becomes more commonplace for labs to acquire multi-site in vivo recordings.</p><p>However, to maximize its utility to the community, I have several questions/concerns that I believe need to be addressed.</p><p>(1) It is obviously important to quantify the relative accuracy of the authors' method to existing methods which correlate neural activity to behavior or sensory input. The authors attempt to do this by comparing CNN decoding to Bayesian decoding with clustered cells (Figure 1). However, I think there are several points where that comparison may be flawed.</p><p>(1a) First, while some manuscripts (including Zhang et al., 1998, referenced by the authors) do use a continuity prior in their decoding algorithms, most do not (see the vast array of papers from Matt Wilson, David Redish, Loren Frank, David Foster, and others in the field). Indeed, even Olafsdottir et al., 2015 that the authors reference in apparent support of the use of a continuity prior (Line 111 of the manuscript) explicitly state that they do not use a continuity prior in their methods. It is unclear to me in reading through the methods whether the CNN-based decoding also utilizes a continuity prior to restrict the decoded location.</p><p>To truly be a useful tool to the community, the algorithm should be capable of correlating neural activity to behaviors/sensory input in a manner that is history-independent, as it seems that a fundamental advantage of this system is for unbiased probing of such correlations. If such history-independent decoding is not possible with the CNN, this should be explicitly stated to clarify the parameters for which this method is appropriate.</p><p>Thus, I would like clarification of whether the CNN uses the animal's history to constrain its decoding output. If so, is the use of the animal's history a necessary component of the CNN-based decoding? If the CNN can be used in a history-independent manner, I would like to see it compared to history-independent Bayesian decoding.</p><p>(1b) The authors report that part of the advantage of the CNN over Bayesian decoding was that the CNN made fewer large errors, and that the median error was more similar across the two methods (Line 120). When performing the Bayesian decoding, it appears as though spikes throughout the entire experiment were used. However, it is known that during periods of immobility, population bursts during sharp-wave/ripples can encode virtual trajectories across the environment, producing non-local spatial representations. If such remote trajectories were included in the Bayesian decoding, this may account for large &quot;errors&quot; between the decoded location and the animal's actual location (even though this may not be an error at all!). Thus, I would like to see this comparison repeated using only periods of active movement, when the literature suggests that place cells are more likely to encode local spatial information.</p><p>In addition, it appears that the authors use a 500 ms window to quantify Bayesian decoding accuracy, but, from what I can tell, they use a ~33 ms window (within 2 second 'chunks') to quantify CNN decoding accuracy. This doesn't seem like a fair apples-to-apples comparison as the animal can move quite a bit over 500 ms. I would thus like to see a comparison between these two methods using similar timescales that are appropriate for both methods, perhaps a ~100-200 ms window.</p><p>(1c) Related to the previous point, a fundamental advantage of using single unit activity to examine behavioral information is that it allows the experimenter to identify when the neural population is representing information other than the immediate sensory input or behavioral output. For example, in the place cell field, one can correlate activity of single units to position during active behavior, and then study when virtual paths are encoded during hippocampal sharp-wave/ripples (a.k.a. replay) or theta sequences.</p><p>Thus, a basic question is whether such non-local representation is accessible in the authors' CNN. Can the authors train the network on behavioral data (using only periods of active movement) and then faithfully decode virtual trajectories during immobility-based ripples? Alternatively, can the authors identify the shorter theta sequences observed during active movement if the CNN operates on a finer timescale? If the position decoding is largely based on high-frequencies in the LFP representing action potentials, it seems that such finer-scale, non-local representations should also be available. Importantly, if such non-local or fine-time-scale representations cannot be identified using this method, it is important to clarify this to avoid incorrect future use.</p><p>(2) Although the majority of self-location information was present in high-frequency bands, the data in Figure 1E show that LFP frequencies below 250 Hz were also informative above chance (and very near Bayesian decoding accuracy). However, given that Figure 3B shows excitatory spikes spread well into low 200 Hz frequency ranges, it seems that using LFP below 250 Hz is likely also including some spike information. For clarification of how accurately low-frequency bands can reflect position information, I would like to see the analysis in Figure 1E performed with LFP frequencies less than 150 Hz (fast gamma and slower).</p><p>If position information can still be faithfully extracted using &lt;150 Hz frequencies, it is important to further rule out non-spatial correlates. For example, are there spatial locations where the rat is more likely to run at predictable velocities, allowing theta-band frequencies to effectively decode the animal's location? Is the LFP-based decoding more/less accurate at specific locations of the environment (near walls, near a rewarded location, etc.)? A heat-map of average decoding error per spatial bin (per animal) would be useful to visualize this analysis.</p><p>(3) When quantifying the accuracy of head direction decoding, they compare the CNN to chance levels. Although this is a valuable measure, given that head direction seems heavily driven by LFP frequencies associated with excitatory and inhibitory spiking, can the authors also compare head direction decoding between the CNN and Bayesian decoding from clustered spikes (including both exc. and inh. cells)? Is head direction information available in the clustered data or are there other elements in the high-frequency LFP that correlate to head direction? If head direction can be decoded via clustered spikes, why do the authors think this has not been observed in prior studies?</p><p>(4) In the Methods (line 356), I'm not sure what the authors mean by &quot;16 eight tetrodes&quot;. Do they mean 16 tetrodes?</p><p>(5) The x-axis of Figure 4D and 4H should be labeled, especially since the scale seems to be log rather than linear.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>– I think this method could be very useful for the EEG/ECoG communities, which care about frequency representations, and appealing to those communities would significantly expand the utility of your method for the broader neuroscience community. In my opinion, if there is no example of this type of use in the paper, it is much less likely for EEG/ECoG researchers to actually use your method in practice. I think that having an EEG or ECoG example would be much more beneficial than the calcium imaging example, since researchers are not trying to determine what frequency contents are important within a calcium imaging signal. This has a list of many open datasets for EEG: https://github.com/meagmohit/EEG-Datasets</p><p>– I think providing a general overview of your approach/method at the beginning of Results would be helpful for many readers.</p><p>– Please clarify when you are reporting test-set vs. training set predictions in your results.</p><p>– In the final paragraph of the introduction, you write &quot;Our model differs markedly from conventional decoding methods which typically use Bayesian estimators…&quot; This is overly specific to hippocampal decoding – in movement decoding Bayesian methods are not frequently used, although linear methods still commonly are.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.66551.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>In the current manuscript, Frey et al. describe a convolutional neural network capable of extracting behavioral correlates from wide-band LFP recordings or even lower-frequency imaging data. Other publications (referenced by the authors) have employed similar ideas previously, but to my knowledge, the current implementation is novel. In my opinion, the real value of this method, as the authors state in their final paragraph, is that it represents a rapid, &quot;first-pass&quot; analysis of large-scale electrophysiological recordings to quickly identify relevant neural features which can then become the focus of more in-depth analyses. As such, I think the analysis program described by the authors is of real value to the community, particularly as it becomes more commonplace for labs to acquire multi-site in vivo recordings.</p><p>However, to maximize its utility to the community, I have several questions/concerns that I believe need to be addressed.</p><p>(1) It is obviously important to quantify the relative accuracy of the authors' method to existing methods which correlate neural activity to behavior or sensory input. The authors attempt to do this by comparing CNN decoding to Bayesian decoding with clustered cells (Figure 1). However, I think there are several points where that comparison may be flawed.</p><p>(1a) First, while some manuscripts (including Zhang et al., 1998, referenced by the authors) do use a continuity prior in their decoding algorithms, most do not (see the vast array of papers from Matt Wilson, David Redish, Loren Frank, David Foster, and others in the field). Indeed, even Olafsdottir et al., 2015 that the authors reference in apparent support of the use of a continuity prior (Line 111 of the manuscript) explicitly state that they do not use a continuity prior in their methods. It is unclear to me in reading through the methods whether the CNN-based decoding also utilizes a continuity prior to restrict the decoded location.</p><p>To truly be a useful tool to the community, the algorithm should be capable of correlating neural activity to behaviors/sensory input in a manner that is history-independent, as it seems that a fundamental advantage of this system is for unbiased probing of such correlations. If such history-independent decoding is not possible with the CNN, this should be explicitly stated to clarify the parameters for which this method is appropriate.</p><p>Thus, I would like clarification of whether the CNN uses the animal's history to constrain its decoding output. If so, is the use of the animal's history a necessary component of the CNN-based decoding? If the CNN can be used in a history-independent manner, I would like to see it compared to history-independent Bayesian decoding.</p></disp-quote><p>We thank the reviewer for raising this important point about the decoding details of the Bayesian decoder and the history dependence of our model. Indeed, as stated by the reviewer, most prior work does not use a continuity prior for Bayesian decoding. The reviewer is also correct in spotting that we made a mistake in referring to our Bayesian model as a decoder with a continuity prior when in fact we did not use such a prior. We apologise for this mistake which we have now corrected in the text.</p><p>In addition we now add a further analysis comparing the CNN with Bayesian decoders with and without a continuity prior. Specifically, we implemented the continuity prior by using a Gaussian distribution centred around the previous decoded location (t-1) of the animal, adjusting the standard deviation based on the speed of the animal in the previous timesteps (last 2 seconds). As shown in Figure 1 – Supplement 4B, the difference between the median values of the Bayesian decoder with and without continuity prior is small (median decoding error with continuity, 22.51cm; without continuity, 23.23cm), although the Bayesian decoder with continuity shows higher variance and makes more ‘catastrophic’ errors which is reflected by a higher mean decoding error (mean decoding error with continuity, 33.06cm; without continuity, 23.38cm). Regardless, in both cases, the CNN yields more accurate decoding (mean decoding error of 17.31cm, Figure 1).</p><p>With regards to the history dependence of our model, the CNN is feedforward and only has access to information from across the input window, which has a size of 2.13 seconds (64 timesteps). There is no explicit continuity prior as implemented in the Bayesian decoder, however during training, the network weights will learn an implicit prior based on the behaviour of the animal in the training data. This will likely incorporate information about the statistics of the animal’s motion (e.g. distance travelled between neighbouring timesteps).</p><p>We also ran an analysis across all time windows for both the Bayesian decoder as well as the convolutional neural network (see also Question 3). As seen in Figure 1 Supplement 4A, for very small time windows, which imply a history independent manner of decoding behaviour, both models show decreased performance in comparison to longer time windows.</p><p>Page 4:</p><p>“To provide a familiar benchmark, we applied a standard Bayesian decoder without a continuity prior (Olafsdottir et al. 2015) to the spiking data from the same datasets (see methods, see also Figure 1 – Supplement 4B for comparison to Bayesian decoder with continuity prior (Zhang et al. 1998)).”</p><p>Page 17:</p><p>“We implemented a Bayesian continuity prior using a Gaussian distribution centred around the previous decoded location x<sub>t-1</sub> of the animal, adjusting the standard deviation based on the speed of the animal in the previous 4 timesteps, as implemented in (Zhang et al. 1998). […] We used the previous 4 timesteps (2 seconds) to estimate the speed of the animal and used V=1s.”</p><disp-quote content-type="editor-comment"><p>(1b) The authors report that part of the advantage of the CNN over Bayesian decoding was that the CNN made fewer large errors, and that the median error was more similar across the two methods (Line 120). When performing the Bayesian decoding, it appears as though spikes throughout the entire experiment were used. However, it is known that during periods of immobility, population bursts during sharp-wave/ripples can encode virtual trajectories across the environment, producing non-local spatial representations. If such remote trajectories were included in the Bayesian decoding, this may account for large &quot;errors&quot; between the decoded location and the animal's actual location (even though this may not be an error at all!). Thus, I would like to see this comparison repeated using only periods of active movement, when the literature suggests that place cells are more likely to encode local spatial information.</p></disp-quote><p>We thank the reviewer for the suggestion to compare decoding performance between stationary periods and periods of active movement.</p><p>For all analysis reported currently in the manuscript the Bayesian decoder is only applied to periods when animals are travelling &gt;3cm/s. No speed threshold was applied to the convolutional neural network. We have now clarified this in the text. In addition, we now include a new Supplementary Figure which shows the comparison between both models across a range of speed thresholds. As expected, the performance of both models increases as the speed threshold is increased, presumably because – as the reviewer suggested – non-locomotor neural activity is excluded. The mean decoding performance of our model improves from 17.37cm ± 3.58cm with no speed threshold to 13.40cm ± 3.59cm with a speed threshold of 25cm/s, while the Bayesian decoder improves from 22.23cm ± 3.72 to 17.17cm ± 2.78.</p><p>Importantly the CNN is more accurate than the Bayesian decoder for all thresholds.</p><p>Page 4:</p><p>“Note that for these comparisons the Bayesian decoder was only applied to periods when the animal was travelling at &gt;3cm/s, in contrast the CNN did not have a speed threshold (i.e. is trained on moving and stationary periods). […] As expected, limiting the CNN to only periods of movement actually improves its performance, accentuating the difference between it and the Bayes decoder (0cm/s speed threshold: Bayesian mean error 22.23cm ± 3.72cm; network error 17.37cm ± 3.58 cm; 25cm/s speed threshold: Bayesian mean error 17.17cm ± 2.788cm; network error 13.40cm ± 3.59 cm, Figure 1 Supplement 6).”</p><disp-quote content-type="editor-comment"><p>In addition, it appears that the authors use a 500 ms window to quantify Bayesian decoding accuracy, but, from what I can tell, they use a ~33 ms window (within 2 second 'chunks') to quantify CNN decoding accuracy. This doesn't seem like a fair apples-to-apples comparison as the animal can move quite a bit over 500 ms. I would thus like to see a comparison between these two methods using similar timescales that are appropriate for both methods, perhaps a ~100-200 ms window.</p></disp-quote><p>We thank the reviewer for this question regarding the timeframes of both the Bayesian decoder and our model. Indeed the Bayesian decoder uses a 500ms window to decode position in the environment, while our model uses a 2s window, from which 4 separate timesteps are decoded (every 500ms). However, we only analyse the last of the decoded sample – ensuring that the model is not able to decode based on future neural data. In effect, all reported performance scores for our model use a 2s window, in which the last timestep is decoded.</p><p>We now include an additional Supplementary Figure comparing the performance of both the Bayesian decoder and the convolutional neural network across different durations, ranging from 100ms up to 1.6 seconds. Both models yield more accurate decoding with longer time windows – which incorporate more data – Bayesian being most accurate with a 1s window, the CNN performing best with a 1.6s window (Figure 1 – Supplement 4A). Here again, CNN performance is better than the Bayes decoder for all time windows.</p><p>Page 17:</p><p>“The optimized Bayesian decoder uses a Gaussian smoothing kernel with σ = 1.5, a bin size of 2cm for binning the ratemaps, and uses a bin length of 0.5s (see Figure 1 Supplement 4A for Bayesian performance across different time windows).”</p><disp-quote content-type="editor-comment"><p>(1c) Related to the previous point, a fundamental advantage of using single unit activity to examine behavioral information is that it allows the experimenter to identify when the neural population is representing information other than the immediate sensory input or behavioral output. For example, in the place cell field, one can correlate activity of single units to position during active behavior, and then study when virtual paths are encoded during hippocampal sharp-wave/ripples (a.k.a. replay) or theta sequences.</p><p>Thus, a basic question is whether such non-local representation is accessible in the authors' CNN. Can the authors train the network on behavioral data (using only periods of active movement) and then faithfully decode virtual trajectories during immobility-based ripples? Alternatively, can the authors identify the shorter theta sequences observed during active movement if the CNN operates on a finer timescale? If the position decoding is largely based on high-frequencies in the LFP representing action potentials, it seems that such finer-scale, non-local representations should also be available. Importantly, if such non-local or fine-time-scale representations cannot be identified using this method, it is important to clarify this to avoid incorrect future use.</p></disp-quote><p>This is a really interesting suggestion – we thank the reviewer for making it.</p><p>In the original implementation of the convolutional model we downsample the electrophysiological signals after the wavelet transformation by a factor of 1000 to maximize the size (in seconds) of the input sample – the amount of data in a sample is constrained by GPU memory size. It’s highly likely this preprocessing would compress short replay events, making them hard to decode.</p><p>Thus to investigate if the model can detect replay events we reran the analysis using less downsampling (see Figure 1—figure supplement 7). In addition we used a new dataset in which rats were performing a navigation task and for which we had observed replay events before. We used the same preprocessing, only adjusting the downsampling factor to 60, resulting in a sampling rate of 500 Hz instead of the previously used 30 Hz. We then trained the model using 64 samples (128ms) in order to obtain a better estimate of decoded positions during replay events. We then decoded the animal position every 2 ms, resulting in over 1 million decoded positions for the experiment which lasted ~35 minutes.</p><p>To quantify if our model can detect replay we investigated model behaviour at time points where sharp-wave ripples (SWRs) had been detected using standard methods (i.e. finding LFP segments in which ripple band power (150-250Hz) was at least 5 standard deviations above the mean and expanding these regions until power dropped back to 0.5 standard deviations above the mean, only retaining segments that were longer than 60ms). Examining these SWRs we saw the CNN often decoded transient, high velocity trajectories that resembled those reported in studies of open field replay (see Figure 1 – Supplement 7A). Consistent with this interpretation the decoding error for these periods (i.e. Euclidean distance between the animal’s location and decoded location) was larger than for matched periods when the animal was stationary but in which SWRs had not been detected (n=1000, p&lt;0.001, Figure 1 Supplement 7B). We furthermore found these putative replay trajectories were longer than the average decoded trajectories during stopped periods (n=1000, p=0.003, Figure 1 – Supplement 7C) and they were as coherent as trajectories detected during movement (n=1000, p=0.257, Figure 1 – Supplement 7D).</p><p>Page 6:</p><p>“The standard decoding model downsamples the wavelet frequencies to a rate of 30Hz, potentially discarding transient non-local representations (e.g. replay events and theta sequences). […] Thus it seems plausible that non-local representations are accessible to this CNN framework.”</p><p>Page 19-20:</p><p>“To investigate if the model can detect replay events we reran the analysis using a lower downsampling factor. […] We only used periods where the speed distribution of the original replay events matched the shuffled replay events (stationary periods).”</p><disp-quote content-type="editor-comment"><p>(2) Although the majority of self-location information was present in high-frequency bands, the data in Figure 1E show that LFP frequencies below 250 Hz were also informative above chance (and very near Bayesian decoding accuracy). However, given that Figure 3B shows excitatory spikes spread well into low 200 Hz frequency ranges, it seems that using LFP below 250 Hz is likely also including some spike information. For clarification of how accurately low-frequency bands can reflect position information, I would like to see the analysis in Figure 1E performed with LFP frequencies less than 150 Hz (fast gamma and slower).</p><p>If position information can still be faithfully extracted using &lt;150 Hz frequencies, it is important to further rule out non-spatial correlates. For example, are there spatial locations where the rat is more likely to run at predictable velocities, allowing theta-band frequencies to effectively decode the animal's location? Is the LFP-based decoding more/less accurate at specific locations of the environment (near walls, near a rewarded location, etc.)? A heat-map of average decoding error per spatial bin (per animal) would be useful to visualize this analysis.</p></disp-quote><p>This was a good suggestion, it does indeed appear that the reviewer was correct – the majority of the spatial information is contained in the 125 to 250 Hz band.</p><p>We reran our LFP decoding analysis the same way as before but further segregated LFP frequencies into two bands – 0-150Hz (12 of 26 frequencies) and 150-250Hz (3 of 26 frequencies). As shown in Figure 1 – Supplement 2, spatial decoding performance was much lower when only frequencies up to 150 Hz were used, relative to the model evaluated on all frequencies up to 250 Hz. In contrast, the models trained on the 150-250Hz band and 0-250Hz band performed similarly and were sufficient to reach the same performance as the Bayesian decoder (shown in grey).</p><p>Indeed, as can be seen in Figure 3 – Supplement 1, models which were only trained on one frequency band at a time demonstrated accurate spatial decoding on frequencies from 165Hz upwards This together with the previous analysis support the reviewer’s hypothesis that excitatory spikes are picked up in frequency bands lower than the traditional 250Hz LFP cut-off. We have now revised our statements in the manuscript and point toward Figure 1 – Supplement 2 showing the difference in decoding performance for all low-frequency models.</p><p>Page 4:</p><p>“The high accuracy and efficiency of the model for these harder samples suggest that the CNN utilizes additional information from sub-threshold spikes and those that were not successfully clustered, as well as nonlinear information which is not available to the Bayesian decoder.”</p><p>Page 5: [Figure caption]</p><p>“When only local frequencies were used (&lt;250Hz, CNN-LFP), network performance dropped to the level of the Bayesian decoder (distributions show the five-fold cross validated performance across each of five animals, n=25). Note that this likely reflects excitatory spikes being picked up at frequencies between 150 and 250 Hz (Figure 1 Supplement 2).”</p><p>Page 6:</p><p>“If we retrain the model on frequency bands 0-150 Hz and 150-250 Hz we observe that spatial information is predominantly contained in higher band frequencies (Figure 1 Supplement 2, see also Figure 3 – Supplement 1), likely reflecting power from pyramidal cell waveforms reaching these frequencies.”</p><disp-quote content-type="editor-comment"><p>(3) When quantifying the accuracy of head direction decoding, they compare the CNN to chance levels. Although this is a valuable measure, given that head direction seems heavily driven by LFP frequencies associated with excitatory and inhibitory spiking, can the authors also compare head direction decoding between the CNN and Bayesian decoding from clustered spikes (including both exc. and inh. cells)? Is head direction information available in the clustered data or are there other elements in the high-frequency LFP that correlate to head direction? If head direction can be decoded via clustered spikes, why do the authors think this has not been observed in prior studies?</p></disp-quote><p>We thank the reviewer for the suggestion to run our head direction decoding also with a Bayesian decoder. We now include an additional analysis in which we quantify head direction decoding using a Bayesian decoding framework, where we calculate errors based on a circular loss, similar to the way the CNN is decoding head direction. The Bayesian decoder's accuracy across all 5 rats is 55.73 ± 8.07 degrees. In comparison, our model achieves a decoding accuracy of 34.37 ± 6.87 degrees. Thus both methods are significantly better than chance (Bayesian model, Wilcoxon signed-rank test two-sided (n=25): T=0, p=1.22e-5, CNN Wilcoxon signed-rank test two-sided (n=25): T=0, p=1.22e-5), although the CNN provides a significant benefit beyond simple Bayesian decoding (Wilcoxon signed-rank test two-sided (n=25): T=47, p=0.0018). It is not entirely surprising that it is possible to decode head direction from CA1 neurons, it has been known for some time that place cells are weakly modulated by head direction (Muller et al. 1994) and, as we report in this manuscript, hippocampal interneurons are similarly modulated. However, in the case of place cells at least, the non-linear interaction between direction and position codes likely impacts the accuracy of the Bayes decoder but is less of a problem for the CNN.</p><p>Page 7:</p><p>“Note that, a Bayesian decoder trained to decode head direction achieves a performance of 0.97rad ± 0.14rad using spike sorted neural data, significantly worse than our model (Wilcoxon signed-rank test two-sided (n=25): T=47, p=0.0018) but more accurate than would be expected by chance (Wilcoxon signed-rank test two-sided (n=25): T=0, p=1.22e-5).”</p><disp-quote content-type="editor-comment"><p>(4) In the Methods (line 356), I'm not sure what the authors mean by &quot;16 eight tetrodes&quot;. Do they mean 16 tetrodes?</p></disp-quote><p>We thank the reviewer for spotting this mistake which we have now revised. We indeed use 16 tetrodes per microdrive, resulting in 128 channels (16 tetrodes x 4 channels per tetrode x 2 microdrives).</p><disp-quote content-type="editor-comment"><p>(5) The x-axis of Figure 4D and 4H should be labeled, especially since the scale seems to be log rather than linear.</p></disp-quote><p>Indeed the frequencies are log scaled and we initially decided to only show some frequencies to not clutter the axis. We now changed the figure axis to show every frequency component. For completeness we now also report all frequencies in the methods section.</p><p>Page 16:</p><p>“The full frequency space for tetrode recordings consisted of 26 log-space frequencies with Fourier frequencies of: 2.59, 3.66, 5.18, 7.32, 10.36, 14.65, 20.72, 29.3, 41.44, 58.59, 82.88, 117.19, 165.75, 234.38, 331.5, 468.75, 663, 937.5, 1326, 1875, 2652, 3750,</p><p>5304, 7500, 15000 Hz. For calcium imaging the Fourier frequencies used are: 0.002,</p><p>0.003, 0.005 0.007, 0.01, 0.014, 0.02, 0.03, 0.04, 0.058, 0.08, 0.11, 0.16, 0.23, 0.33, 0.46, 0.66, 0.93, 1.32, 1.87, 2.65, 3.75, 5.3, 7.5, 10.6, 15 Hz.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>– I think this method could be very useful for the EEG/ECoG communities, which care about frequency representations, and appealing to those communities would significantly expand the utility of your method for the broader neuroscience community. In my opinion, if there is no example of this type of use in the paper, it is much less likely for EEG/ECoG researchers to actually use your method in practice. I think that having an EEG or ECoG example would be much more beneficial than the calcium imaging example, since researchers are not trying to determine what frequency contents are important within a calcium imaging signal. This has a list of many open datasets for EEG: https://github.com/meagmohit/EEG-Datasets</p></disp-quote><p>We thank the reviewer for the suggestion to evaluate our model on another dataset from a different neuroscience community to increase the potential impact of our framework.</p><p>We first want to point out that frequency contents might also be important for calcium imaging researchers. One or two-photon microscopes are now being developed for freely moving animals and the decrease in size, which is necessary to fit the animal, is most often accompanied by a decrease in sampling rate. Our decoding results and the underlying informative frequencies show that a sampling rate of around 1Hz is enough to capture most of the information contained in the signal. This information can give other researchers a lower bound for the sampling rate of the neural signal.</p><p>As suggested, we now evaluate our model on a publicly available ECoG dataset (Schalk et al. 2007), in which participants finger movements were recorded while simultaneously acquiring ECoG signals. As this dataset was part of a BCI competition (BBCI IV) we can directly compare our model results to the best models of the competition. We used the available training data from three subjects to train and validate our model and report performance on the provided test set. Each subject was instructed to move a given finger in response to a visual cue, which lasted around 2 seconds. We used the same model pipeline, adjusting parameters to fit the dataset. In particular, we used a downsampling factor of 50, as the original sampling rate is 1000Hz (in comparison to 30000 Hz in the CA1 recordings) which leads to an effective sampling rate of 20Hz. We trained the model with 128 timesteps (6.4s) and used a mean squared error loss function between original finger movement and decoded finger movement. As can be seen in Figure 4 – Supplement 1 we reach an average Pearson's r of 0.517 +- 0.160 across three subjects. The best result in the competition reached a performance of 0.46 (see competition <ext-link ext-link-type="uri" xlink:href="http://www.bbci.de/competition/iv/results/">results</ext-link>).</p><p>Page 12:</p><p>“To further assess the ability of our model to decode continuous behaviour from neural data, we investigated its performance on an Electrocorticography (ECoG) dataset recorded in humans (Schalk et al. 2017) made available as part of a BCI competition (BCI Competition IV, Dataset 4). […] These results together show that our model can be used on a wide variety of continuous regression problems in both rodents and humans and across a wide range of recording systems, including calcium imaging and electrocorticography datasets.”</p><disp-quote content-type="editor-comment"><p>– I think providing a general overview of your approach/method at the beginning of Results would be helpful for many readers.</p></disp-quote><p>We thank the reviewers for this suggestion. We have now added a paragraph explaining how the Results section is structured and provide a general overview of our approach in more detail.</p><p>Page 3:</p><p>“In the following section, we present our model, the results, and describe how it was applied across different datasets. […] The transformed data is then aligned with one or several decoding outputs, representing different behaviours or stimuli, which are fed through a convolutional neural network, decoding each output separately.”</p><disp-quote content-type="editor-comment"><p>– Please clarify when you are reporting test-set vs. training set predictions in your results.</p></disp-quote><p>We are sorry for the lack of clarity regarding the use of training vs. test-set. We now clarify in the manuscript that all of the reported predictions are fully cross-validated on the test set and at no point in the manuscript do we report training predictions except when explicitly stated (e.g. Figure S1).</p><p>Page 3:</p><p>“Using the wavelet coefficients as inputs, the model was trained in a supervised fashion using error backpropagation with the X and Y coordinates of the animal as regression targets. We report test-set performance for fully cross-validated models using 5 splits across the whole duration of the experiment.”</p><disp-quote content-type="editor-comment"><p>– In the final paragraph of the introduction, you write &quot;Our model differs markedly from conventional decoding methods which typically use Bayesian estimators…&quot; This is overly specific to hippocampal decoding – in movement decoding Bayesian methods are not frequently used, although linear methods still commonly are.</p></disp-quote><p>We thank the reviewer for the suggestion to open up our introduction to a wider audience. We now rewrote the manuscript in the following way.</p><p>Page 2:</p><p>“Our model differs markedly from conventional decoding methods which often use Bayesian estimators (Zhang et al. 1998) for hippocampal recordings in conjunction with highly processed neural data or linear methods for movement decoding from EEG or</p><p>ECoG signals (Antelis et al. 2013).”</p></body></sub-article></article>