<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">106827</article-id><article-id pub-id-type="doi">10.7554/eLife.106827</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106827.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Biologically informed cortical models predict optogenetic perturbations</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sourmpis</surname><given-names>Christos</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0007-0519-1116</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Petersen</surname><given-names>Carl CH</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3344-4495</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Bellec</surname><given-names>Guillaume</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7568-4994</contrib-id><email>guillaume.bellec@epfl.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>Laboratory of Computational Neuroscience, Brain Mind Institute, School of Computer and Communication Sciences and School of Life Sciences, École Polytechnique Fédérale de Lausanne (EPFL)</institution></institution-wrap><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>Laboratory of Sensory Processing, Brain Mind Institute, School of Life Sciences, École Polytechnique Fédérale de Lausanne (EPFL)</institution></institution-wrap><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04d836q62</institution-id><institution>Machine Learning Research Unit, Technical University of Vienna (TU Wien)</institution></institution-wrap><addr-line><named-content content-type="city">Vienna</named-content></addr-line><country>Austria</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>École Normale Supérieure - PSL</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Poirazi</surname><given-names>Panayiota</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution></institution-wrap><addr-line><named-content content-type="city">Heraklion</named-content></addr-line><country>Greece</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>16</day><month>01</month><year>2026</year></pub-date><volume>14</volume><elocation-id>RP106827</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2025-03-21"><day>21</day><month>03</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2025-03-15"><day>15</day><month>03</month><year>2025</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.27.615361"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-06-16"><day>16</day><month>06</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106827.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-12-18"><day>18</day><month>12</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106827.2"/></event></pub-history><permissions><copyright-statement>© 2025, Sourmpis et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Sourmpis et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-106827-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-106827-figures-v1.pdf"/><abstract><p>A recurrent neural network fitted to large electrophysiological datasets may help us understand the chain of cortical information transmission. In particular, successful network reconstruction methods should enable a model to predict the response to optogenetic perturbations. We test recurrent neural networks (RNNs) fitted to electrophysiological datasets on unseen optogenetic interventions and measure that generic RNNs used predominantly in the field generalize poorly on these perturbations. Our alternative RNN model adds biologically informed inductive biases like structured connectivity of excitatory and inhibitory neurons and spiking neuron dynamics. We measure that some biological inductive biases improve the model prediction on perturbed trials in a simulated dataset and a dataset recorded in mice in vivo. Furthermore, we show in theory and simulations that gradients of the fitted RNN can be used to target micro-perturbations in the recorded circuits and discuss the potential utility to bias an animal’s behavior and study cortical circuit mechanisms.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>systems modeling</kwd><kwd>RNN</kwd><kwd>perturbation testing</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00yjd3n13</institution-id><institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>CR-SII5_198612</award-id><principal-award-recipient><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00yjd3n13</institution-id><institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>00020_207426</award-id><principal-award-recipient><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00yjd3n13</institution-id><institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>TMAG-3_209271</award-id><principal-award-recipient><name><surname>Petersen</surname><given-names>Carl CH</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00yjd3n13</institution-id><institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>31003A_182010</award-id><principal-award-recipient><name><surname>Petersen</surname><given-names>Carl CH</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01f9mc681</institution-id><institution>Vienna Science and Technology Fund</institution></institution-wrap></funding-source><award-id>VRG24-018</award-id><principal-award-recipient><name><surname>Bellec</surname><given-names>Guillaume</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Multi area RNN models fitted to in-vivo cortical activity predict behavioral changes induced by optogenetic perturbations, if biologically informed connectivity constraints on the optogenetically targeted inhibitory neurons are applied.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A fundamental question in neuroscience is how cortical circuit mechanisms drive perception and behavior. To tackle this question, experimental neuroscientists have been collecting large-scale electrophysiology datasets under reproducible experimental settings (<xref ref-type="bibr" rid="bib50">Siegle et al., 2021</xref>; <xref ref-type="bibr" rid="bib19">Esmaeili et al., 2021</xref>; <xref ref-type="bibr" rid="bib60">Urai et al., 2022</xref>; <xref ref-type="bibr" rid="bib8">Benson et al., 2023</xref>). However, neuroscience lacks data-grounded modeling approaches to generate and test hypotheses on the causal role of neuronal and circuit-level mechanisms. To leverage the high information density of contemporary recordings, we need both (1) modeling approaches that scale well with data, and (2) metrics to quantify when the models provide a plausible mechanism for the observed phenomena.</p><p>Biophysical simulations have been crucial for our understanding of single-cell mechanisms (<xref ref-type="bibr" rid="bib25">Hodgkin, 1958</xref>), and have been used to describe interactions across cortical layers, columns, and areas (<xref ref-type="bibr" rid="bib34">Markram et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Billeh et al., 2020</xref>; <xref ref-type="bibr" rid="bib26">Isbister et al., 2023</xref>; <xref ref-type="bibr" rid="bib11">Chen et al., 2022</xref>; <xref ref-type="bibr" rid="bib46">Rimehaug et al., 2023</xref>; <xref ref-type="bibr" rid="bib20">Fraile et al., 2023</xref>; <xref ref-type="bibr" rid="bib53">Spieler et al., 2023</xref>). A promising approach to constrain models to electrophysiological data lies in the optimization of the simulation parameters by gradient descent. These methods were successful in quantitatively classifying functional cell types (<xref ref-type="bibr" rid="bib43">Pozzorini et al., 2015</xref>; <xref ref-type="bibr" rid="bib58">Teeter et al., 2018</xref>), and modeling micro-circuit interactions (<xref ref-type="bibr" rid="bib42">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="bib15">Deny et al., 2017</xref>; <xref ref-type="bibr" rid="bib32">Mahuas et al., 2020</xref>). To bridge the gap from single neurons or small retinal networks to cortical recordings in vivo, recent studies made substantial progress towards data-constrained recurrent neural network (RNN) models (<xref ref-type="bibr" rid="bib41">Perich et al., 2020</xref>; <xref ref-type="bibr" rid="bib7">Bellec et al., 2021</xref>; <xref ref-type="bibr" rid="bib3">Arthur et al., 2023</xref>; <xref ref-type="bibr" rid="bib61">Valente et al., 2022</xref>; <xref ref-type="bibr" rid="bib27">Kim et al., 2023</xref>; <xref ref-type="bibr" rid="bib49">Shai et al., 2023</xref>; <xref ref-type="bibr" rid="bib51">Sourmpis et al., 2023</xref>; <xref ref-type="bibr" rid="bib38">Pals et al., 2024</xref>). In this line of work, neurons in the RNN are mapped one-to-one to recorded cells and optimized by gradient descent to predict recorded activity at large scale.</p><p>An important question is whether these data-constrained RNNs can reveal a truthful mechanism of neuronal activity and behavior. By construction, the RNNs can generate brain-like network activity, but how can we measure whether the reconstructed network faithfully represents the biophysical mechanism? To answer this question, we submit a range of RNN reconstruction methods to a difficult <italic>perturbation test</italic>: we measure the similarity of the network response to unseen perturbations in the RNN and the recorded biological circuit.</p><p>Optogenetics is a powerful tool to induce precise causal perturbations in vivo (<xref ref-type="bibr" rid="bib19">Esmaeili et al., 2021</xref>; <xref ref-type="bibr" rid="bib22">Guo et al., 2014</xref>). It involves the expression of light-sensitive ion channels (<xref ref-type="bibr" rid="bib10">Boyden et al., 2005</xref>), such as channelrhodopsins, in specific populations of neurons (e.g. excitatory/pyramidal or inhibitory/parvalbumin-expressing). In this paper, we use datasets including both dense electrophysiological recordings and optogenetic perturbations to evaluate RNN reconstruction methods. Since the neurons in our RNNs are mapped one-to-one to the recorded cells, we can model optogenetic perturbations targeting the same cell types and areas as done in vivo. Yet, we observe that the similarity between the simulated and recorded perturbations varies greatly depending on the reconstruction methods.</p><p>Most prominently, we study two opposite types of RNN specifications. First, as a control model, we consider a traditional sigmoidal RNN (σRNN) which is arguably the most common choice for contemporary data-constrained RNNs (<xref ref-type="bibr" rid="bib41">Perich et al., 2020</xref>; <xref ref-type="bibr" rid="bib3">Arthur et al., 2023</xref>; <xref ref-type="bibr" rid="bib38">Pals et al., 2024</xref>); and second, we develop a model with biologically informed inductive biases (bioRNN): (1) neuronal dynamics follow a simplified spiking neuron model, and (2) neurons associated with fast-spiking inhibitory cells have short-distance inhibitory projections (other neurons are excitatory with both local and long-range interareal connectivity). Following <xref ref-type="bibr" rid="bib36">Neftci et al., 2019</xref>; <xref ref-type="bibr" rid="bib6">Bellec et al., 2018b</xref>; <xref ref-type="bibr" rid="bib7">Bellec et al., 2021</xref>; <xref ref-type="bibr" rid="bib51">Sourmpis et al., 2023</xref>, we adapt gradient descent techniques to optimize the bioRNN parameters of neurons and synapses to explain the recorded neural activity and behavior.</p><p>Strikingly, we find that the bioRNN is more robust to perturbations than the σRNN. This is nontrivial because it is in direct contradiction with other metrics often used in the field: the σRNN simulation achieves higher similarity with unseen recorded trials before perturbation, but lower than the bioRNN on perturbed trials. This contradiction is confirmed both on synthetic and in vivo datasets. To analyze this result, we submit a spectrum of intermediate bioRNN models to the same <italic>perturbation tests</italic> and identify two bioRNN model features that are most important to improve robustness to perturbation: (1) Dale’s law (the cell type constrains the sign of the connections; <xref ref-type="bibr" rid="bib17">Eccles, 1976</xref>), and (2) local-only inhibition (inhibitory neurons do not project to other cortical areas). In contrast, other model features are penalizing or do not improve significantly the prediction of the optogenetically perturbed response in this out-of-distribution fashion. It indicates that perturbation tests can validate biophysical modeling strategies in data-constrained deep learning models of neural mechanisms.</p><p>Beyond the optogenetic area inactivation available in the in vivo dataset, we investigate how perturbation-robust RNNs could enable targeted optogenetic protocols for the discovery of detailed neuronal circuit mechanisms in future experiments. Targeted causal interventions will become decisive in studying smaller circuit mechanisms. Acute optogenetic inactivations of genetically defined laminar subpopulations were used to characterize the causal role of specific neurons in the sensory motor pathways (<xref ref-type="bibr" rid="bib57">Tamura et al., 2025</xref>; <xref ref-type="bibr" rid="bib62">Wyart et al., 2025</xref>), and upcoming technology will make these experiments easier (<xref ref-type="bibr" rid="bib28">Lakunina et al., 2025</xref>). To illustrate how RNN reconstruction can help to target neuronal stimulation, we consider micro-perturbations (µ-perturbation) targeting dozens of neurons in a small time window. Inspired by recent read-write all-optical setups (<xref ref-type="bibr" rid="bib37">Packer et al., 2015</xref>), we imagine a model-informed µ-perturbation protocol, where neurons are targeted based on their functional rather than genetic properties. While previous work has used linear models to produce targeted stimulations (<xref ref-type="bibr" rid="bib62">Wyart et al., 2025</xref>; <xref ref-type="bibr" rid="bib35">Minai et al., 2024</xref>), we show that back-propagated gradients of perturbation-robust RNNs provide a sensitivity map to predict the effect of µ-perturbations. Concretely, in a closed-loop experimental setup in silicon, we can use RNN gradients to target a µ-perturbation and change the movement in a simulated mouse. The gradients are used to identify the few neurons having the strongest causal effect on behavior. Conceptually, it means that our RNN reconstructions enable an estimation of ‘circuit gradients’, bringing numerical and theoretical concepts from deep learning (<xref ref-type="bibr" rid="bib30">LeCun et al., 2015</xref>; <xref ref-type="bibr" rid="bib45">Richards and Kording, 2023</xref>) to study biological network computation.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Reconstructed networks: biological inductive biases strengthen robustness to perturbations</title><sec id="s2-1-1"><title>Synthetic dataset for challenging causal inference</title><p>We build a toy synthetic dataset to formalize how we intend to reverse engineer the mechanism of a recorded circuit using optogenetic perturbations and RNN reconstruction methods. It also serves as the first dataset to evaluate our network reconstruction methods. This toy example represents a simplified version of large-scale cortical recordings from multiple brain areas during a low-dimensional instructed behavior (<xref ref-type="bibr" rid="bib55">Steinmetz et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Esmaeili et al., 2021</xref>), similarly to the in vivo dataset of a GO/No-Go task <xref ref-type="bibr" rid="bib19">Esmaeili et al., 2021</xref> analyzed in the next section. Let’s consider two areas <inline-formula><alternatives><mml:math id="inf1"><mml:mi>A</mml:mi></mml:math><tex-math id="inft1">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf2"><mml:mi>B</mml:mi></mml:math><tex-math id="inft2">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> which are either transiently active together (‘hit trial’ occurring with frequency <inline-formula><alternatives><mml:math id="inf3"><mml:mi>p</mml:mi></mml:math><tex-math id="inft3">\begin{document}$p$\end{document}</tex-math></alternatives></inline-formula>) or quiescent together (‘miss trial’ occurring with probability <inline-formula><alternatives><mml:math id="inf4"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft4">\begin{document}$1-p$\end{document}</tex-math></alternatives></inline-formula>). Since the two areas are active or inactive together, it is hard to infer if they are connected in a feedforward or recurrent fashion. In Methods (Mathematical toy model of the difficult causal inference between H1 and H2), we describe a theoretical example where it is impossible to decide between opposing mechanistic hypotheses (feedforward or recurrent) when recording only the macroscopic activations of areas <inline-formula><alternatives><mml:math id="inf5"><mml:mi>A</mml:mi></mml:math><tex-math id="inft5">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf6"><mml:mi>B</mml:mi></mml:math><tex-math id="inft6">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula>. In this case, performing optogenetic inactivation of one area is decisive to distinguish between the feedforward or recurrent hypothesis.</p><p>To generate artificial spike train recordings that capture this problem, we design two reference circuits (RefCircs) from which we can record the spike trains. Each RefCirc consists of two populations of 250 spiking neurons (80% are excitatory) representing areas <inline-formula><alternatives><mml:math id="inf7"><mml:mi>A</mml:mi></mml:math><tex-math id="inft7">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf8"><mml:mi>B</mml:mi></mml:math><tex-math id="inft8">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula>. To highlight the importance of optogenetic perturbations as in the Methods (Mathematical toy model of the difficult causal inference between H1 and H2), the first circuit RefCirc1 is feedforward and the second RefCirc2 is recurrent: RefCirc1 (and not RefCirc2) has strictly zero feedback connections from <inline-formula><alternatives><mml:math id="inf9"><mml:mi>B</mml:mi></mml:math><tex-math id="inft9">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> to <inline-formula><alternatives><mml:math id="inf10"><mml:mi>A</mml:mi></mml:math><tex-math id="inft10">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula>. Yet, the two RefCircs are almost identical without optogenetic perturbations: each neuron in RefCirc1 has been constructed to have an almost identical trial-averaged activity as the corresponding neuron in RefCirc2; and in response to a stimulus, the circuits display a similar bi-modal hit-or-miss response with a hit trial frequency <inline-formula><alternatives><mml:math id="inf11"><mml:mi>p</mml:mi><mml:mo>≈</mml:mo><mml:mn>50</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:math><tex-math id="inft11">\begin{document}$p\approx 50\%$\end{document}</tex-math></alternatives></inline-formula>. We consider that a trial is a hit if area <inline-formula><alternatives><mml:math id="inf12"><mml:mi>A</mml:mi></mml:math><tex-math id="inft12">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> is active, if the averaged firing rate is above 8 Hz.Note that defining a ‘hit’ trial based on area <inline-formula><alternatives><mml:math id="inf13"><mml:mi>A</mml:mi></mml:math><tex-math id="inft13">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> is equivalent to saying that both areas need to be active during unperturbed trials with this dataset. But excluding <inline-formula><alternatives><mml:math id="inf14"><mml:mi>B</mml:mi></mml:math><tex-math id="inft14">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> in this definition avoids that the hit rate is trivially impacted when manipulating the activity of <inline-formula><alternatives><mml:math id="inf15"><mml:mi>B</mml:mi></mml:math><tex-math id="inft15">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> with optogenetic perturbations. To simulate optogenetic inactivations of an area in the RefCircs, we inject a transient current into the inhibitory neurons, modeling the opening of light-sensitive ion channels. Symmetrically, an optogenetic activation is simulated as a positive current injected into excitatory cells. <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> shows that optogenetic perturbations in area B reflect the presence or absence of feedback connections which differ in RefCirc 1 and 2. Methods section Reference circuits for hypotheses 1 and 2 provides more details on the construction of the artificial circuits. Our <italic>perturbation test</italic> will consist of the comparison of optogenetic perturbations in the reconstructed RNN and in their references RefCirc1 and 2, without retraining the RNN on these perturbations.</p></sec><sec id="s2-1-2"><title>Network reconstruction methodology (synthetic dataset)</title><p>To reconstruct the recorded circuits with an RNN, we record activity from the spiking RefCircs and optimize the parameters of an RNN to generate highly similar network activity. The whole reconstruction method is summarized graphically in panel A of <xref ref-type="fig" rid="fig1">Figure 1</xref>. In the simplest cases, the RNN is specified as a sigmoidal network model (<xref ref-type="bibr" rid="bib47">Rosenblatt, 1960</xref>; <xref ref-type="bibr" rid="bib18">Elman, 1990</xref>): σRNN1 and σRNN2 are optimized to reproduce the recording from RefCirc1 and RefCirc2, respectively. In this synthetic dataset, the reconstructed σRNNs have the same size as the RefCircs (500 neurons) and sigmoidal neurons are mapped one-to-one with RefCirc neurons (20% are mapped to inhibitory RefCirc neurons). They are initialized with all-to-all connectivity and are therefore blind to the structural difference of the RefCirc1 and 2 (feedforward or recurrent). From each of the two RefCircs, we store 2000 trials of simultaneous spike train recordings of all 500 neurons (step 1 in <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Half of the trials are used as a training set and will be the basis for our data-driven RNN optimization. The second half of the recorded trials forms the testing set and is used to evaluate the quality of the reconstruction before perturbations.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Network reconstruction and perturbation tests.</title><p>(<bold>A</bold>) The three steps to reconstruct the reference circuit (RefCirc) using a biologically informed RNN (bioRNN) or a sigmoidal RNN (σRNN) and evaluate the reconstruction based on perturbation tests. (<bold>B</bold>) Summary of the differences between a bioRNN and a σRNN. (<bold>C</bold>) Trial-averaged activity of area <italic>A</italic> of the two circuits during hit (black-dashed: RefCirc1; blue: bioRNN1; pink: σRNN1) and miss (grey-dashed: RefCirc1; light blue: bioRNN1; light pink: σRNN1) trials. All models display a hit rate of <inline-formula><alternatives><mml:math id="inf16"><mml:mi>p</mml:mi><mml:mo>≈</mml:mo><mml:mn>50</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:math><tex-math id="inft16">\begin{document}$p\approx 50\%$\end{document}</tex-math></alternatives></inline-formula>. (<bold>D</bold>) Same as <bold>C</bold> during inactivation of area <italic>B</italic>. <inline-formula><alternatives><mml:math id="inf17"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi class="mathcal" mathvariant="script">D</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft17">\begin{document}$\Delta p^\mathcal{D}=0$\end{document}</tex-math></alternatives></inline-formula> is the recorded change of hit rate for the feedforward circuit RefCirc1, so a successful reconstruction achieves <inline-formula><alternatives><mml:math id="inf18"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>≈</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:math><tex-math id="inft18">\begin{document}$\hat{p}\approx 0\%$\end{document}</tex-math></alternatives></inline-formula>. (<bold>E</bold>) Quantitative results on perturbation tests showing that σRNN achieves the lowest loss function on the unperturbed test trials, but only the bioRNN retains an accurate fit to the perturbed trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106827-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Modeling ‘optogenetic’ perturbations.</title><p>(<bold>A</bold>) Two different network hypotheses for implementing a detection task. In RefCirc1, area A projects to area B but not vice versa. In RefCirc2, the areas are recurrently connected. (<bold>B</bold>) Raster plots of all neurons in RefCirc1 during a single hit trial under normal conditions (control, left) and under optogenetic perturbation of excitatory (middle) and inhibitory (right) neurons. The duration of the light stimulus is shown with a blue shading. (<bold>C</bold>) Same for RefCirc2 (<bold>D</bold>) Trial-averaged activity of the two circuits during Hit (blue: RefCirc1; green: RefCirc2) and Miss (yellow: RefCirc 1; red: RefCirc2) trials. A trial is classified as ‘Hit’ if area <inline-formula><alternatives><mml:math id="inf19"><mml:mi>A</mml:mi></mml:math><tex-math id="inft19">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> reaches a transient firing rate above 8 Hz; and otherwise as ‘Miss’. For the control case, the maximal difference between the trial average activity of the two networks is below 0.51 Hz (zoom inset).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106827-fig1-figsupp1-v1.tif"/></fig></fig-group><p>We optimize the synaptic ‘weights’ of the σRNN to minimize the difference between its activity and that of the RefCirc (step 2 in <xref ref-type="fig" rid="fig1">Figure 1A</xref>, see Methods). The optimization combines three loss functions defined mathematically in Methods (Optimization and loss function): (i) the neuron-specific loss function <inline-formula><alternatives><mml:math id="inf20"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft20">\begin{document}$\mathcal{L}_{\mathrm{neuron}}$\end{document}</tex-math></alternatives></inline-formula> is the mean-square error of the <italic>trial-averaged</italic> neural activity (e.g. the PSTH) between σRNN and RefCirc neurons. (ii) To account for fluctuations of the single-trial network activity, we use a trial-specific loss function <inline-formula><alternatives><mml:math id="inf21"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math><tex-math id="inft21">\begin{document}$\mathcal{L}_{\mathrm{trial}}$\end{document}</tex-math></alternatives></inline-formula>, which is the distance between the distribution of single trial population-averaged activity of σRNN and RefCirc (see <xref ref-type="bibr" rid="bib51">Sourmpis et al., 2023</xref>). (iii) Finally, we add a regularization loss function <inline-formula><alternatives><mml:math id="inf22"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math><tex-math id="inft22">\begin{document}$\mathcal{L}_{\mathrm{reg}}$\end{document}</tex-math></alternatives></inline-formula> to penalize unnecessarily large weights.</p><p>We also developed a biologically informed RNN model (bioRNN) for which we have designed a successful optimization technique. The main differences between σRNNs and bioRNNs consist of the following biological inductive biases. Firstly, the bioRNN neuron model follows a simplified leaky integrate and fire dynamics (see Methods, Neuron and jaw movement model) yielding strictly binary spiking activity. Secondly, we constrain the recurrent weight matrix to describe cell-type-specific connectivity constraints: following Dale’s law, neurons have either non-negative or non-positive outgoing connections; moreover, since cortical inhibitory neurons rarely project across areas, we assume that inhibitory neurons project only locally within the same area. Thirdly, we add a term to the regularization loss <inline-formula><alternatives><mml:math id="inf23"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math><tex-math id="inft23">\begin{document}$\mathcal{L}_{\mathrm{reg}}$\end{document}</tex-math></alternatives></inline-formula> to implement the prior knowledge that cross-area connections are more sparse than within an area. Adding these biological features into the model requires an adapted gradient descent algorithm and matrix initialization strategies (Methods, Optimization and loss function). The reconstruction method with σRNNs and bioRNNs is otherwise identical: the models have the same size and are optimized on the same data for the same number of steps and using the same loss functions. The two models bioRNN1 and bioRNN2 are optimized to explain recordings from RefCirc1 and Refcirc2, respectively. Importantly, the structural difference between RefCirc1 (feedforward) and RefCirc2 (feedback) is assumed to be unknown during parameter optimization: at initialization, excitatory neurons in bioRNN1 or bioRNN2 project to any neuron in the network with transmission efficacies (aka as synaptic weights) initialized randomly.</p><p>After parameter optimization, we have four models, σRNN1, σRNN2, bioRNN1, and bioRNN2, that we call ‘reconstructed’ models. To validate the reconstructed models, we verify that the network trajectories closely match the data on the test set in terms of (i) the ‘behavioral’ hit-trial frequency, (ii) the peristimulus time histogram (PSTH) mean-square error of single neurons as evaluated by <inline-formula><alternatives><mml:math id="inf24"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math><tex-math id="inft24">\begin{document}$\mathcal{L}_{\mathrm{neuron}}$\end{document}</tex-math></alternatives></inline-formula>, and (iii) the distance between single-trial network dynamics as evaluated by <inline-formula><alternatives><mml:math id="inf25"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math><tex-math id="inft25">\begin{document}$\mathcal{L}_{\mathrm{trial}}$\end{document}</tex-math></alternatives></inline-formula> (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> and <xref ref-type="table" rid="table1">Table 1</xref>). At first sight, the σRNN displays a better data fitting when comparing with the non-perturbed trials of the testing set: <inline-formula><alternatives><mml:math id="inf26"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math><tex-math id="inft26">\begin{document}$\mathcal{L}_{\mathrm{trial}}$\end{document}</tex-math></alternatives></inline-formula> is for instance, lower with σRNN (see <xref ref-type="table" rid="table1">Table 1</xref>). This is expected considering that the optimization of bioRNNs is less flexible and numerically efficient because of the sign-constrained weight matrix and the imperfect surrogate gradient approximation through spiking activity. However, the two bioRNNs are drastically more robust when evaluating the models with <italic>perturbation tests</italic>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>BioRNN is more robust to optogenetic perturbations than σRNN.</title><p>The table reports the trial matching (TM) loss <inline-formula><alternatives><mml:math id="inf27"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math><tex-math id="inft27">\begin{document}$\mathcal{L}_{\mathrm{trial}}$\end{document}</tex-math></alternatives></inline-formula> on test trials; it measures the distance between the distributions of single trial network dynamics <xref ref-type="bibr" rid="bib51">Sourmpis et al., 2023</xref> in area A when stimulating area <inline-formula><alternatives><mml:math id="inf28"><mml:mi>B</mml:mi></mml:math><tex-math id="inft28">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula>. Column ‘no light’ indicates values on the unperturbed test trials, and ‘light’ the perturbation trials. ± indicates the 95% confidence interval, best values are shown in bold and major failure with distance above 0.5 is in red.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="2">RefCirc1 vs. RNN1</th><th align="left" valign="bottom" colspan="2">RefCirc2 vs. RNN2</th></tr></thead><tbody><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"><bold>no light</bold></td><td align="left" valign="bottom"><bold>light</bold></td><td align="left" valign="bottom"><bold>no light</bold></td><td align="left" valign="bottom"><bold>light</bold></td></tr><tr><td align="left" valign="bottom">bioRNN</td><td align="char" char="." valign="bottom">0.19 ± 0.01</td><td align="char" char="." valign="bottom">0.25 ± 0.09</td><td align="char" char="." valign="bottom">0.18 ± 0.01</td><td align="char" char="." valign="bottom"><bold>0.28</bold> ± 0.13</td></tr><tr><td align="left" valign="bottom">σRNN</td><td align="char" char="." valign="bottom"><bold>0.16</bold> ± 0.01</td><td align="char" char="." valign="bottom"><styled-content style="color: #D50000;">1.15 ± 1.07</styled-content></td><td align="char" char="." valign="bottom"><bold>0.17</bold> ± 0.01</td><td align="char" char="." valign="bottom"><styled-content style="color: #D50000;">1.22 ± 0.64</styled-content></td></tr><tr><td align="left" valign="bottom">No sparsity</td><td align="char" char="." valign="bottom">0.20 ± 0.01</td><td align="char" char="." valign="bottom"><styled-content style="color: #D50000;">1.37 ± 1.42</styled-content></td><td align="char" char="." valign="bottom">0.19 ± 0.01</td><td align="char" char="." valign="bottom">0.19 ± 0.13</td></tr><tr><td align="left" valign="bottom">Non-local inhibition</td><td align="char" char="." valign="bottom">0.20 ± 0.02</td><td align="char" char="." valign="bottom"><styled-content style="color: #D50000;">0.54 ± 0.42</styled-content></td><td align="char" char="." valign="bottom">0.18 ± 0.01</td><td align="char" char="." valign="bottom"><styled-content style="color: #D50000;">1.19 ± 0.91</styled-content></td></tr><tr><td align="left" valign="bottom">No Dale’s law</td><td align="char" char="." valign="bottom">0.18 ± 0.01</td><td align="char" char="." valign="bottom"><styled-content style="color: #D50000;">0.86 ± 0.23</styled-content></td><td align="char" char="." valign="bottom">0.18 ± 0.01</td><td align="char" char="." valign="bottom"><styled-content style="color: #D50000;">2.21 ± 1.60</styled-content></td></tr><tr><td align="left" valign="bottom">No spike</td><td align="char" char="." valign="bottom">0.17 ± 0.00</td><td align="char" char="." valign="bottom"><bold>0.19</bold> ± 0.04</td><td align="char" char="." valign="bottom">0.18 ± 0.00</td><td align="char" char="." valign="bottom">0.46 ± 0.19</td></tr><tr><td align="left" valign="bottom">No Trial Matching (TM)</td><td align="char" char="." valign="bottom">0.33 ± 0.01</td><td align="char" char="." valign="bottom">0.44 ± 0.19</td><td align="char" char="." valign="bottom">0.35 ± 0.03</td><td align="char" char="." valign="bottom">0.44 ± 0.09</td></tr></tbody></table></table-wrap></sec><sec id="s2-1-3"><title>Perturbation test</title><p>To test which of the reconstructed RNNs capture the causal mechanisms of the RefCircs, we simulate optogenetic activations and inactivations of area <inline-formula><alternatives><mml:math id="inf29"><mml:mi>B</mml:mi></mml:math><tex-math id="inft29">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> (step 3 in <xref ref-type="fig" rid="fig1">Figure 1A</xref>). We first compare the change of hit probability after perturbations in the reconstructed RNN (<inline-formula><alternatives><mml:math id="inf30"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft30">\begin{document}$\Delta\hat{p}$\end{document}</tex-math></alternatives></inline-formula>) and recorded in RefCirc (<inline-formula><alternatives><mml:math id="inf31"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math><tex-math id="inft31">\begin{document}$\Delta{p}^{\mathcal{D}}$\end{document}</tex-math></alternatives></inline-formula>) in <xref ref-type="fig" rid="fig2">Figure 2</xref>. For the σRNN, the activation or inactivation of area <inline-formula><alternatives><mml:math id="inf32"><mml:mi>B</mml:mi></mml:math><tex-math id="inft32">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> changes drastically the peak firing rate in area <inline-formula><alternatives><mml:math id="inf33"><mml:mi>A</mml:mi></mml:math><tex-math id="inft33">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula>: all trials become a hit during inactivation of area <inline-formula><alternatives><mml:math id="inf34"><mml:mi>B</mml:mi></mml:math><tex-math id="inft34">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula>. This drastic increase in hit rate is not consistent with the reference where the effect of the optogenetic inactivations is mild: the distribution of network responses remains bi-modal (hit versus miss) with only a moderate change of hit frequency for RefCirc2 <inline-formula><alternatives><mml:math id="inf35"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:math><tex-math id="inft35">\begin{document}$\Delta{p}^{\mathcal{D}}=-3\%$\end{document}</tex-math></alternatives></inline-formula>. For RefCirc1, we even expect <inline-formula><alternatives><mml:math id="inf36"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:math><tex-math id="inft36">\begin{document}$\Delta{p}^{\mathcal{D}}=0\%$\end{document}</tex-math></alternatives></inline-formula> by design because of the absence of feedback connections from <inline-formula><alternatives><mml:math id="inf37"><mml:mi>B</mml:mi></mml:math><tex-math id="inft37">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> to <inline-formula><alternatives><mml:math id="inf38"><mml:mi>A</mml:mi></mml:math><tex-math id="inft38">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula>. In contrast, the bioRNN models capture these changes more accurately (see <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref>). Quantitative results are summarized in <xref ref-type="fig" rid="fig2">Figure 2E</xref>, the error of hit probability changes <inline-formula><alternatives><mml:math id="inf39"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math><tex-math id="inft39">\begin{document}$|\Delta{p}^{\mathcal{D}}-\Delta\hat{p}|$\end{document}</tex-math></alternatives></inline-formula> is 7% with bioRNNs when averaged over all conditions (bioRNN1 and bioRNN2, with optogenetic inactivations and activations). The corresponding error is 48.5% on average for σRNNs. In this sense, we argue that the bioRNN provides a better prediction of the perturbed hit frequency than the σRNN. We also performed spike train recordings in the area that is not directly targeted by the light to compare the perturbed network dynamics in the fitted RNNs and the RefCirc. The perturbed dynamics are displayed in <xref ref-type="fig" rid="fig2">Figure 2D</xref>. The quantity <inline-formula><alternatives><mml:math id="inf40"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math><tex-math id="inft40">\begin{document}$\mathcal{L}_{\mathrm{trial}}^{\mathrm{light}}$\end{document}</tex-math></alternatives></inline-formula> is a distance between the network dynamics (RNN versus reference) and is reported in <xref ref-type="fig" rid="fig2">Figure 2D–E</xref> and <xref ref-type="table" rid="table1">Table 1</xref>. Again, the perturbed dynamics of the bioRNN are more similar to those of the reference circuits <inline-formula><alternatives><mml:math id="inf41"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.26</mml:mn></mml:math><tex-math id="inft41">\begin{document}$\mathcal{L}_{\mathrm{trial}}^{\mathrm{light}}=0.26$\end{document}</tex-math></alternatives></inline-formula>, than with the σRNN <inline-formula><alternatives><mml:math id="inf42"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1.19</mml:mn></mml:math><tex-math id="inft42">\begin{document}$\mathcal{L}_{\mathrm{trial}}^{\mathrm{light}}=1.19$\end{document}</tex-math></alternatives></inline-formula> (t-test p-value is 0.0003).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Reconstruction of network mechanisms.</title><p>(<bold>A</bold>) RefCirc1 is feedforward and RefCirc2 is recurrent. (<bold>B</bold>) The fitted RNNs are blind to the structural difference of RefCirc1 and 2 and must infer this from the spiking data. (<bold>C</bold>) Raster plot showing an example trial of the bioRNN and σRNN models, neurons in red are mapped to inhibitory neurons. (<bold>D</bold>) To study which model feature matters, bioRNN variants are defined by removing one of the features, for instance ‘No Dale’s law’ refers to a bioRNN without weight sign constraints. Trial-averaged activity in area <inline-formula><alternatives><mml:math id="inf43"><mml:mi>A</mml:mi></mml:math><tex-math id="inft43">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> under activation/inactivation of area <inline-formula><alternatives><mml:math id="inf44"><mml:mi>B</mml:mi></mml:math><tex-math id="inft44">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula>. All the RNNs are tested with the same reference circuit and training data (No spike and No Sparsity models are shown in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). (<bold>E</bold>) Error between the change of hit probability after perturbations in the RNN <inline-formula><alternatives><mml:math id="inf45"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft45">\begin{document}$\Delta\hat{p}$\end{document}</tex-math></alternatives></inline-formula> and in the RefCirc <inline-formula><alternatives><mml:math id="inf46"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math><tex-math id="inft46">\begin{document}$\Delta p^{\mathcal{D}}$\end{document}</tex-math></alternatives></inline-formula>, the whiskers indicate the 95% confidence interval. (<bold>F</bold>) The distance of network dynamics <inline-formula><alternatives><mml:math id="inf47"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math><tex-math id="inft47">\begin{document}$\mathcal{L}_{\mathrm{trial}}^{\mathrm{light}}$\end{document}</tex-math></alternatives></inline-formula> between each RNN and RefCirc (horizontal axis: light power in arbitrary units). (<bold>G</bold>) Same quantity as <inline-formula><alternatives><mml:math id="inf48"><mml:mi>D</mml:mi></mml:math><tex-math id="inft48">\begin{document}$D$\end{document}</tex-math></alternatives></inline-formula> but averaged for each RNN under the strongest light power condition (averaging activations and inactivations of area <inline-formula><alternatives><mml:math id="inf49"><mml:mi>B</mml:mi></mml:math><tex-math id="inft49">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula>), the whiskers indicate the 95% confidence interval. Statistical significance in comparison with bioRNN is computed with t-test using the mean over multiple network initializations and is indicated with 0–4 stars corresponding to p-values thresholds: 0.05, 10<sup>-2</sup>, 10<sup>-3</sup>, and 10<sup>-4</sup>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106827-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Fitting Reconstructed networks to the synthetic dataset.</title><p>(<bold>A</bold>) Schematic representation of the RefCirc1 and bioRNN1 and probability of hit trials. (<bold>B</bold>) Histogram of the firing rate distribution of the RefCirc1 and all the RNN1 versions. We observe that all RNN1 versions fit well with the RefCirc1. (<bold>C</bold>) Left: Neuron loss of the different RNN1 variants. Right: Trial-matching loss of the different RNN1 variants. We observe that the model without the trial-matching loss function behaves considerably worse. The whiskers show the 95% confidence interval of the mean across trials. (<bold>D</bold>-<bold>F</bold>) Same as <bold>A</bold>–<bold>B</bold> for RefCirc2 and RNNs2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106827-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Picking the sparsity level.</title><p>(<bold>A</bold>) Grid search for the optimal maximum regularization strength (<inline-formula><alternatives><mml:math id="inf50"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft50">\begin{document}$\lambda_{3}$\end{document}</tex-math></alternatives></inline-formula>) without a drop in performance. As a performance measure, we used the trial-matching loss, <inline-formula><alternatives><mml:math id="inf51"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math><tex-math id="inft51">\begin{document}$\mathcal{L}_{\mathrm{trial}}$\end{document}</tex-math></alternatives></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106827-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Trial-averaged traces across RNN variants.</title><p>Trial-averaged activity in area <inline-formula><alternatives><mml:math id="inf52"><mml:mi>A</mml:mi></mml:math><tex-math id="inft52">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> under activation/inactivation of area <inline-formula><alternatives><mml:math id="inf53"><mml:mi>B</mml:mi></mml:math><tex-math id="inft53">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula>. Dashed black lines indicate the activity of RefCirc1 (thick dashed) and RefCirc2 (thin dashed). All the RNNs are tested with the same reference circuit and training data, and each bioRNN model variant is shown with a different color.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106827-fig2-figsupp3-v1.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Hit frequency prediction error <inline-formula><alternatives><mml:math id="inf54"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math><tex-math id="inft54">\begin{document}$|\Delta{p}^{\mathcal{D}}-\Delta\hat{p}|$\end{document}</tex-math></alternatives></inline-formula> as in <xref ref-type="fig" rid="fig2">Figure 2E</xref>.</title><p>In contrast to <xref ref-type="fig" rid="fig2">Figure 2E</xref>, here we show separately the change of hit probability for RefCirc1 (left) and RefCirc2 (right).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106827-fig2-figsupp4-v1.tif"/></fig></fig-group><p>To analyze which features of bioRNN explain this robustness to perturbation, we then derive a family of models where only one feature of the reconstruction is omitted. Namely, the ‘No Dale’s law’ model does not have excitatory and inhibitory weight constraints, the ‘Non-local inhibition’ model allows inhibitory neurons to project outside of their areas, the ‘No Spike’ model replaces the spiking dynamics with a sigmoidal neuron model, and the ‘No Sparsity’ model omits the cross-area sparsity penalty in <inline-formula><alternatives><mml:math id="inf55"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math><tex-math id="inft55">\begin{document}$\mathcal{L}_{\mathrm{reg}}$\end{document}</tex-math></alternatives></inline-formula>. Omitting all these features in bioRNN would be equivalent to using a σRNN. The accuracy metrics on the testing sets before perturbation are reported for all RNN variants in <xref ref-type="fig" rid="fig2">Figure 2E and G</xref>. For reference, we also include the model ‘No TM’ (trial-matching), which omits the loss function <inline-formula><alternatives><mml:math id="inf56"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math><tex-math id="inft56">\begin{document}$\mathcal{L}_{\mathrm{trial}}$\end{document}</tex-math></alternatives></inline-formula> during training.</p><p>The strongest effect measured with this analysis is that Dale’s law and local inhibition explain most of the improved robustness of bioRNNs. This is visible in <xref ref-type="fig" rid="fig2">Figure 2</xref> as the perturbed trajectories of ‘No Dale’s law’ and ‘Non-local inhibition’ are most distant from the reference in <xref ref-type="fig" rid="fig2">Figure 2D</xref>. This is confirmed numerically where both the hit-rate error and the distance of network dynamics increase the most when lifting these constraints (<xref ref-type="fig" rid="fig2">Figure 2E–G</xref> and <xref ref-type="table" rid="table1">Table 1</xref>). We explain this result as follows: the mono-synaptic effect of a cell stimulated by the light is always correct in bioRNN (according to Dale’s law and inhibition locality), but often wrong in the alternative models (see <xref ref-type="fig" rid="fig2">Figure 2A</xref>). For instance, a simple explanation may justify the failure of the ‘Non-local inhibition’ model: the stimulation of inhibitory neurons in <inline-formula><alternatives><mml:math id="inf57"><mml:mi>B</mml:mi></mml:math><tex-math id="inft57">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> induces (via the erroneous mono-synaptic inhibition) a reduction in the baseline activity in area <inline-formula><alternatives><mml:math id="inf58"><mml:mi>A</mml:mi></mml:math><tex-math id="inft58">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> (see the green trace during inactivation in <xref ref-type="fig" rid="fig2">Figure 2D</xref>). More generally for <italic>perturbation testing</italic>, we speculate that these features are measured to be important here because they are central to the biophysical nature of the perturbation considered: optogenetic perturbation targets specific cell types, and these features incorporate biophysical connectivity priors that are hard to infer entirely from the unperturbed data.</p><p>Not all the biological features that we implemented in bioRNN made comparable improvements in the prediction of optogenetic perturbations. We implemented simple spiking neuron dynamics and fitted the spiking network as any other RNN using surrogate gradients (<xref ref-type="bibr" rid="bib36">Neftci et al., 2019</xref>) as in <xref ref-type="bibr" rid="bib7">Bellec et al., 2021</xref>; <xref ref-type="bibr" rid="bib51">Sourmpis et al., 2023</xref>. On perturbed data, the spiking bioRNN achieves slightly better performance than its ‘No spike’ variant, but without significant margins, t-test p-value is 0.31 for <inline-formula><alternatives><mml:math id="inf59"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math><tex-math id="inft59">\begin{document}$\mathcal{L}_{\mathrm{trial}}^{\mathrm{light}}$\end{document}</tex-math></alternatives></inline-formula> (see <xref ref-type="table" rid="table1">Table 1</xref> and <xref ref-type="fig" rid="fig2">Figure 2E–G</xref>). We speculate that simulating spikes is not advantageous here, because optogenetic perturbations are relatively broad in space and time, and it might become more relevant for other perturbation experiments where precise timing matters or at a microcircuit level. Similarly, the sparse connectivity regularization did not yield a significant improvement on the perturbation tests (see <xref ref-type="table" rid="table1">Table 1</xref>).</p><p>Besides predicting the response to optogenetic perturbations, we wondered if we could recover the connectivity structure of the recorded circuit. Our method would not be appropriate to recover individual synaptic connections, but we tested whether the fitted RNNs reflected the optogenetic signature of the structural difference between the ‘feedforward’ RefCirc1 and the ‘recurrent’ RefCirc2. Our criteria are here qualitative: the early increase in the PSTH response in area <inline-formula><alternatives><mml:math id="inf60"><mml:mi>A</mml:mi></mml:math><tex-math id="inft60">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> characteristic of mono-synaptic feedback from area <inline-formula><alternatives><mml:math id="inf61"><mml:mi>B</mml:mi></mml:math><tex-math id="inft61">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> should not exist for RefCirc1 (see <xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3A</xref>). To reveal this difference in the fitted bioRNN1 and bioRNN2 models, not only Dale’s law and local inhibition are necessary, but also spiking dynamics and sparsity appear to be helpful. For instance, the erroneous early onset on the perturbed trial in area A for the ‘No Sparsity’ model is corrected with the sparsity prior (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, red versus blue curves). Yet, these are subtle qualitative results that are likely to be less impactful and reproducible than the clear qualitative improvement obtained with perturbation testing when modeling cell-type connectivity. Indeed, we will see in the next section that we obtain consistent qualitative perturbation testing results on the larger in vivo dataset.</p></sec></sec><sec id="s2-2"><title>Predicting perturbations on in vivo electrophysiology data</title><p>To test whether our reconstruction method with biological inductive biases can predict optogenetic perturbations in large-scale recordings, we used the dataset from <xref ref-type="bibr" rid="bib19">Esmaeili et al., 2021</xref>. In this study, water-deprived mice were trained to perform a whisker tactile detection task. In 50% of the trials (Go trials), a whisker is deflected, followed by a 1 s delay, after which an auditory cue signals that the mice can lick a water spout to receive a water reward. In the other 50% of trials (No-Go trials), no whisker deflection occurs, and licking after the auditory cue results in a penalty with an extended time-out period. While the mice performed the task, experimenters recorded 6,182 units from 12 areas across 18 mice. Using this dataset, we focused on the six most relevant areas for executing this task (<xref ref-type="bibr" rid="bib19">Esmaeili et al., 2021</xref>). From each area, we randomly selected 250 neurons (200 putative excitatory and 50 putative inhibitory), which correspond to 1500 neurons in total. These areas, all shown to be causally involved in the task (<xref ref-type="bibr" rid="bib19">Esmaeili et al., 2021</xref>), include the primary and secondary whisker sensory cortex (wS1, wS2), the primary and secondary whisker motor cortex (wM1, wM2), the anterior lateral motor cortex (ALM), and the primary tongue-jaw motor cortex (tjM1). We fit the neuronal activity using the same reconstruction method as used for the synthetic dataset. In the model, we simulate the jaw movement of the mouse as a linear readout driven by the model’s neural activity. This readout is regressed with the real jaw movement extracted from video footage. The parameter optimization of the behavioral readout is performed jointly with fitting the synaptic weights to the neuronal recordings, see Methods, Optimization and loss function. After training, our reconstructed model can generate neural activity with firing rate distribution, trial-averaged activity, single-trial network dynamics, and behavioral outcome, which are all consistent with the recordings (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Before perturbations, we observe again that the σRNN model fits the testing set data better than the bioRNN model (see <xref ref-type="table" rid="table2">Table 2</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Trial-matching loss test loss <inline-formula><alternatives><mml:math id="inf62"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math><tex-math id="inft62">\begin{document}$\mathcal{L}_{\mathrm{trial}}$\end{document}</tex-math></alternatives></inline-formula> of the different reconstruction methods with the real recordings from <xref ref-type="bibr" rid="bib19">Esmaeili et al., 2021</xref> ± indicates the 95% confidence interval.</title><p>Unlike in <xref ref-type="table" rid="table1">Table 1</xref>, the same metric cannot be evaluated for the perturbation trials due to the absence of joint recordings and stimulation in this dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Method name</th><th align="left" valign="bottom">Real dataset vs reconstructed network</th></tr></thead><tbody><tr><td align="left" valign="bottom">bioRNN</td><td align="char" char="." valign="bottom">0.76 ± 0.14</td></tr><tr><td align="left" valign="bottom">σRNN</td><td align="char" char="." valign="bottom">0.62 ± 0.12</td></tr><tr><td align="left" valign="bottom">No sparsity</td><td align="char" char="." valign="bottom">0.77 ± 0.15</td></tr><tr><td align="left" valign="bottom">Non-local inhibition</td><td align="char" char="." valign="bottom">0.79 ± 0.15</td></tr><tr><td align="left" valign="bottom">No Dale’s law</td><td align="char" char="." valign="bottom">0.68 ± 0.13</td></tr><tr><td align="left" valign="bottom">No TM</td><td align="char" char="." valign="bottom">1.63 ± 0.55</td></tr><tr><td align="left" valign="bottom">No spike</td><td align="char" char="." valign="bottom">0.64 ± 0.13</td></tr></tbody></table></table-wrap><p>We then submit the reconstructed σRNNs and bioRNNs models to <italic>perturbation tests</italic>. For the sessions of the in vivo dataset with optogenetic perturbation that we considered, only the behavior of an animal is recorded during inactivation of an area at a given time window (stimulus, delay, or choice periods). For each of the six areas and time windows, we extract the averaged hit frequency under optogenetic inactivation and attempt to predict this perturbed behavior by inducing the same inactivations to the fitted RNNs. These perturbations are acute spatiotemporal optogenetic inactivations of each area during different time periods (see <xref ref-type="fig" rid="fig3">Figure 3B</xref>). As an example, we show the effect of an inactivation of wS1 during the whisker period in the model in <xref ref-type="fig" rid="fig3">Figure 3</xref>. In panel C, we display the simulated trial of a fitted bioRNN with and without perturbations side by side. The two trials are simulated with the same random seed, and this example shows that an early perturbation in wS1 can change a lick decision from hit to miss in the model (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Predicting optogenetic perturbations for in vivo electrophysiology data.</title><p>(<bold>A</bold>) During a delayed whisker detection task, the mouse reports a whisker stimulation by licking to obtain a water reward. Jaw movements are recorded by a camera. Our model simulates the jaw movements and the neural activity from six areas. (<bold>B</bold>) The experimentalists performed optogenetic inactivations of cortical areas (one area at a time) in three temporal windows. (<bold>C</bold>) Example hit trial of a reconstructed network (left). Using the same random seed, the trial turns into a miss trial if we inactivate area wS1 (right, light stimulus indicated by blue shading) during the whisker period by stimulation of inhibitory neurons (red dots). (<bold>D</bold>) Error of the change in lick frequency caused by the perturbation, <inline-formula><alternatives><mml:math id="inf63"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft63">\begin{document}$\Delta\hat{p}$\end{document}</tex-math></alternatives></inline-formula> is predicted by the model, and <inline-formula><alternatives><mml:math id="inf64"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math><tex-math id="inft64">\begin{document}$\Delta{p}^{\mathcal{D}}$\end{document}</tex-math></alternatives></inline-formula> is recorded in mice. Light-shaded circles show individual reconstructed networks with different initializations. The whiskers are the standard error of means. Statistical significance is computed with t-test using the mean change of lick frequency over different network initializations (n=3-6) and is indicated with 0-2 stars corresponding to p-values thresholds: 0.05 and 0.01. (<bold>E</bold>) Examples of <inline-formula><alternatives><mml:math id="inf65"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft65">\begin{document}$\Delta\hat{p}$\end{document}</tex-math></alternatives></inline-formula> hit rate changes under perturbation for wS1 (Top) and tjM1 (Bottom). The black circles refer to the hit rate change from the recordings, <inline-formula><alternatives><mml:math id="inf66"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math><tex-math id="inft66">\begin{document}$\Delta{p}^{\mathcal{D}}$\end{document}</tex-math></alternatives></inline-formula>. See <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> for the other areas.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106827-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Reconstruction of the real recordings.</title><p>(<bold>A</bold>) Probability of hit trials of the different variant models. (<bold>B</bold>) Histogram of the firing rate distribution from the real recordings and all the variants. (<bold>C</bold>) Top: Neuron loss of the different RNN1 variants. All RNN versions have a similar loss value. Bottom: Trial-matching loss of the different model variants. We observe that the model without the trial-matching loss function behaves considerably worse. The whiskers show the 95% confidence interval.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106827-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Hit rate changes under inactivations.</title><p>(<bold>A</bold>) Change of lick probability under inactivation of all areas in all the different temporal windows. We show the <inline-formula><alternatives><mml:math id="inf67"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>p</mml:mi></mml:math><tex-math id="inft67">\begin{document}$\Delta p$\end{document}</tex-math></alternatives></inline-formula> from the data and reconstruction model variants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106827-fig3-figsupp2-v1.tif"/></fig></fig-group><p>Consistent with the synthetic dataset, we now find with this in vivo dataset that modeling cell-type connectivity yields better prediction of the causal effect of optogenetic perturbation. We denote by <inline-formula><alternatives><mml:math id="inf68"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math><tex-math id="inft68">\begin{document}$\Delta{p}^{\mathcal{D}}$\end{document}</tex-math></alternatives></inline-formula> the in vivo change in lick probability across Go trials in response to optogenetic perturbations. The perturbations were performed in different periods for each area in <xref ref-type="bibr" rid="bib19">Esmaeili et al., 2021</xref> (stimulation, delay, or choice periods). For all areas and time windows, we measure the corresponding <inline-formula><alternatives><mml:math id="inf69"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft69">\begin{document}$\Delta\hat{p}$\end{document}</tex-math></alternatives></inline-formula> in the model. On average, the error change probability obtained with the σRNN model is <inline-formula><alternatives><mml:math id="inf70"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>21</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:math><tex-math id="inft70">\begin{document}$|\Delta{p}^{\mathcal{D}}-\Delta\hat{p}|=21\%$\end{document}</tex-math></alternatives></inline-formula> which is significantly worse than the bioRNN model’s 16% (t-test p-value is 0.014, see <xref ref-type="fig" rid="fig3">Figure 3D</xref>). As in the synthetic dataset, we find this to be consistent over multiple bioRNN model variants, and we find that imposing Dale’s law and local inhibition best explain the improvement in perturbation-robustness. We also measure that the spiking bioRNN predicts the change in lick probability slightly better than the ‘No Spike’ bioRNN model. Conversely, adding the sparsity prior does not seem to improve the perturbed hit-rate prediction on the real data as seen in the recurrent artificial dataset (RefCirc2) and not in the feedforward case (RefCirc1) as shown in <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>. In this sense, in vivo perturbation testing emerges as a hard test to evaluate modeling strategies combining deep learning and biophysical modeling.</p><p>To further analyze the consistency of the perturbations in the model, we can compare the perturbation map showing changes in lick probability obtained from acute inactivation in the data and the model. The <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> summarizes visually which area has a critical role at specific time points. The changes of lick probability in area wS1, ALM, and tjM1 are accurately predicted by the bioRNN. In contrast, our model tends to underestimate the causal effect induced by the inactivations of wS2, wM1, and wM2 (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Overall, our model is consistent with a causal chain of interaction from wS1 to ALM and continuing to tjM1.</p></sec><sec id="s2-3"><title>Applications for experimental electrophysiology</title><p>With future progress in recording technology and reconstruction methods, network reconstruction may soon predict the effect of optogenetic perturbation with even higher accuracy. In this section, we explore possible consequences and applications for experimental electrophysiology. We demonstrate in the following that (1) perturbation-robust bioRNNs enable us to estimate gradients of the recorded circuits, (2) which in turn enable us to target µ-perturbations in the recorded circuit and optimally increase (or decrease) induced movements in our simulated mouse. The so-called ‘recorded circuit’ is a bioRNN trained on the in vivo dataset that we use as a proxy experimental preparation. Its mathematical underpinnings enable us to make rigorous theoretical considerations and the design of forward-looking in silico experiments.</p><sec id="s2-3-1"><title><italic>μ</italic>-perturbations measure brain gradients</title><p>We first prove a mathematical relationship between gradients in the recorded circuit and µ-perturbations. We define the integrated movement as <inline-formula><alternatives><mml:math id="inf71"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft71">\begin{document}$Y=\sum_{t}y_{t}$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf72"><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft72">\begin{document}$y_{t}$\end{document}</tex-math></alternatives></inline-formula> is the movement of the jaw at time <inline-formula><alternatives><mml:math id="inf73"><mml:mi>t</mml:mi></mml:math><tex-math id="inft73">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> generated by the model, and we denote <inline-formula><alternatives><mml:math id="inf74"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft74">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> as the change of movement caused by the µ-perturbation. If the circuit has well-defined gradients (e.g. say a ‘No spike’ bioRNN model trained on the in vivo recordings in the previous section), using a Taylor expansion, we find that:<disp-formula id="equ1"><label>(1)</label><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mfrac><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle  \Delta Y = \sum_{i,t \in \mathcal{I}} \frac{d Y}{d {u}_i^t} \Delta u_i^t + \epsilon,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf75"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">I</mml:mi></mml:mrow></mml:math><tex-math id="inft75">\begin{document}$\mathcal{I}$\end{document}</tex-math></alternatives></inline-formula> are the neuron and time indices selected for the optogenetic intervention. The error term <inline-formula><alternatives><mml:math id="inf76"><mml:mi>ϵ</mml:mi></mml:math><tex-math id="inft76">\begin{document}$\epsilon$\end{document}</tex-math></alternatives></inline-formula> is negligible when the current <inline-formula><alternatives><mml:math id="inf77"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>u</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft77">\begin{document}$\Delta u_{i}^{t}$\end{document}</tex-math></alternatives></inline-formula> induced by the light is small. We first confirm this approximation with numerical visualization in <xref ref-type="fig" rid="fig4">Figure 4A</xref>: we display movement perturbations <inline-formula><alternatives><mml:math id="inf78"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Y</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft78">\begin{document}$\langle\Delta Y\rangle$\end{document}</tex-math></alternatives></inline-formula> in the circuit with time windows of decreasing sizes (〈⋅〉 indicates a trial average). When the time window is small, and the perturbation is only applied to excitatory or inhibitory cells in <xref ref-type="fig" rid="fig4">Figure 4A</xref>, one can appreciate visually the similarity with the binned gradient <inline-formula><alternatives><mml:math id="inf79"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>u</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:math><tex-math id="inft79">\begin{document}$\langle\sum_{i,t}\frac{dY}{du_{i}^{t}}\rangle$\end{document}</tex-math></alternatives></inline-formula> in <xref ref-type="fig" rid="fig4">Figure 4B</xref>. Proceeding to a quantitative verification of <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, we now compare the effect of small perturbations targeting only 20 neurons on a single trial. We use the gradient <inline-formula><alternatives><mml:math id="inf80"><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>u</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math><tex-math id="inft80">\begin{document}$\sum_{i,t}\frac{dY}{d{u}_{i}^{t}}$\end{document}</tex-math></alternatives></inline-formula> (see <xref ref-type="fig" rid="fig4">Figure 4C</xref>) to predict the outcome of µ-perturbation as follows: for each trial, and each 100 ms time window, we identify 20 neurons in the model with highest (or lowest) gradients <inline-formula><alternatives><mml:math id="inf81"><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>u</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math><tex-math id="inft81">\begin{document}$\sum_{i,t}\frac{dY}{d{u}_{i}^{t}}$\end{document}</tex-math></alternatives></inline-formula>. We then re-simulate the exact same trial with identical random seed, but induce a µ-perturbation on selected neurons (see rectangles in <xref ref-type="fig" rid="fig4">Figure 4</xref>). If we target neurons with strongly positive gradients, the perturbed jaw movements are strongly amplified <inline-formula><alternatives><mml:math id="inf82"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Y</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft82">\begin{document}$\Delta Y\gt 0$\end{document}</tex-math></alternatives></inline-formula>; conversely, if we target neurons with negative gradients, the jaw movements are suppressed <inline-formula><alternatives><mml:math id="inf83"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Y</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft83">\begin{document}$\Delta Y \lt 0$\end{document}</tex-math></alternatives></inline-formula>. Although <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> is only rigorously valid for models with well-defined gradients like the ‘No Spike’ model, we also confirm in <xref ref-type="fig" rid="fig4">Figure 4D</xref> that this numerical verification also holds in a spiking circuit model where the gradients are replaced with surrogate gradients (<xref ref-type="bibr" rid="bib36">Neftci et al., 2019</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Measuring circuit gradients with µ-perturbations.</title><p>(<bold>A</bold>–<bold>B</bold>) Numerical verification for <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. <bold>A</bold> shows the change of jaw movement <inline-formula><alternatives><mml:math id="inf84"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Y</mml:mi></mml:math><tex-math id="inft84">\begin{document}$\Delta Y$\end{document}</tex-math></alternatives></inline-formula> following inactivations in a “No Spike” bioRNN. From left to right, we reduce the size of the spatiotemporal window for the optogenetic stimulation. (<bold>B</bold>) Gradient values <inline-formula><alternatives><mml:math id="inf85"><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac></mml:math><tex-math id="inft85">\begin{document}$\sum_{i,t}\frac{dY}{du}$\end{document}</tex-math></alternatives></inline-formula> that approximate <inline-formula><alternatives><mml:math id="inf86"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Y</mml:mi></mml:math><tex-math id="inft86">\begin{document}$\Delta Y$\end{document}</tex-math></alternatives></inline-formula> from A using <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. (<bold>C</bold>–<bold>D</bold>) Verification that gradients predict the change of movement on single trials. In <bold>C</bold>, we display the gradients and jaw movement for three different trials, the neurons targeted by the µ-perturbation are boxed and the perturbed jaw movement is blue. Results averaged for every 100ms stimulation windows are shown in (<bold>D</bold>) positive (resp. negative) modulated means that the 20 neurons with highest (resp. lowest) gradients are targeted, random neurons are selected for the shuffled case. The whiskers show the 95% confidence interval.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106827-fig4-v1.tif"/></fig><p>An implication of <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> is that the measurements <inline-formula><alternatives><mml:math id="inf87"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Y</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft87">\begin{document}$\langle\Delta Y\rangle$\end{document}</tex-math></alternatives></inline-formula> that can be recorded in vivo are estimates of the gradients <inline-formula><alternatives><mml:math id="inf88"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>u</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:math><tex-math id="inft88">\begin{document}$\langle\sum_{i,t}\frac{dY}{d{u}_{i}^{t}}\rangle$\end{document}</tex-math></alternatives></inline-formula> in the recorded circuit. Yet, measuring detailed gradient maps (or perturbation maps) as displayed in <xref ref-type="fig" rid="fig4">Figure 4</xref> would be costly in vivo as it requires to average <inline-formula><alternatives><mml:math id="inf89"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Y</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft89">\begin{document}$\Delta Y$\end{document}</tex-math></alternatives></inline-formula> over dozens of trials for each spatio-temporal window.</p><p>Instead, gradient calculation in a bioRNN model (that was fitted to the experimental preparation) is a rapid mathematical exercise. If the extracted model is valid, then the gradients <inline-formula><alternatives><mml:math id="inf90"><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>u</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math><tex-math id="inft90">\begin{document}$\sum_{i,t}\frac{dY}{d{u}_{i}^{t}}$\end{document}</tex-math></alternatives></inline-formula> in the bioRNN approximate (1) the effect of µ-perturbations <inline-formula><alternatives><mml:math id="inf91"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Y</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft91">\begin{document}$\Delta Y$\end{document}</tex-math></alternatives></inline-formula> in the experimental preparation; (2) the gradient <inline-formula><alternatives><mml:math id="inf92"><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>u</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math><tex-math id="inft92">\begin{document}$\sum_{i,t}\frac{dY}{d{u}_{i}^{t}}$\end{document}</tex-math></alternatives></inline-formula> in the recorded circuit.</p></sec><sec id="s2-3-2"><title>Targeting in vivo µ-perturbations with bioRNN gradients</title><p>Building on this theoretical finding, we build a speculative experimental setup where the bioRNN gradients are used to target a µ-perturbation and increase (or decrease) the movements <inline-formula><alternatives><mml:math id="inf93"><mml:mi>Y</mml:mi></mml:math><tex-math id="inft93">\begin{document}$Y$\end{document}</tex-math></alternatives></inline-formula> in the experimental preparation in real time. We show a schematic of this speculative closed-loop experiment in <xref ref-type="fig" rid="fig5">Figure 5C</xref> extending contemporary read-write electrophysiology setups (<xref ref-type="bibr" rid="bib37">Packer et al., 2015</xref>; <xref ref-type="bibr" rid="bib1">Adesnik and Abdeladim, 2021</xref>; <xref ref-type="bibr" rid="bib21">Grosenick et al., 2015</xref>; <xref ref-type="bibr" rid="bib40">Papagiakoumou et al., 2020</xref>). We demonstrate in silico in <xref ref-type="fig" rid="fig5">Figure 5A–B</xref> how this experiment could use bioRNN gradients to bias the simulated mouse movement <inline-formula><alternatives><mml:math id="inf94"><mml:mi>Y</mml:mi></mml:math><tex-math id="inft94">\begin{document}$Y$\end{document}</tex-math></alternatives></inline-formula>. As a preparation step, and before applying perturbations, we assume that the bioRNN is well fitted to the recorded circuit and we collect a large databank <inline-formula><alternatives><mml:math id="inf95"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mi class="mathcal" mathvariant="script">B</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft95">\begin{document}$\mathcal{B}$\end{document}</tex-math></alternatives></inline-formula> of simulated trials from the fitted bioRNN. Then in real-time, we record the activity from the experimental preparation until the time <inline-formula><alternatives><mml:math id="inf96"><mml:msup><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="inft96">\begin{document}$t^{*}$\end{document}</tex-math></alternatives></inline-formula> at which the stimulation will be delivered (Step 1 in <xref ref-type="fig" rid="fig5">Figure 5A</xref>, <inline-formula><alternatives><mml:math id="inf97"><mml:msup><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="inft97">\begin{document}$t^{*}$\end{document}</tex-math></alternatives></inline-formula> is 100 ms before the decision period). Rapidly, we find the trial with the closest spike trains in the databank of simulated trials (Step 2) and use the corresponding gradient maps to target neurons with the highest gradient <inline-formula><alternatives><mml:math id="inf98"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac></mml:math><tex-math id="inft98">\begin{document}$\frac{dY}{du}$\end{document}</tex-math></alternatives></inline-formula> in the model (Step 3). The targeted stimulation is then delivered immediately at <inline-formula><alternatives><mml:math id="inf99"><mml:msup><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="inft99">\begin{document}$t^{*}$\end{document}</tex-math></alternatives></inline-formula> to the experimental preparation (Step 4). When testing this in silico on our artificial experimental preparation, we show in <xref ref-type="fig" rid="fig5">Figure 5C</xref> that this approach can bias the quantity of jaw movement <inline-formula><alternatives><mml:math id="inf100"><mml:mi>Y</mml:mi></mml:math><tex-math id="inft100">\begin{document}$Y$\end{document}</tex-math></alternatives></inline-formula> driven by the circuit in a predictable way. The amount of movement is consistently augmented if we target neurons with the highest <inline-formula><alternatives><mml:math id="inf101"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac></mml:math><tex-math id="inft101">\begin{document}$\frac{dY}{du}$\end{document}</tex-math></alternatives></inline-formula> (or diminished if we target neurons with the lowest <inline-formula><alternatives><mml:math id="inf102"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac></mml:math><tex-math id="inft102">\begin{document}$\frac{dY}{du}$\end{document}</tex-math></alternatives></inline-formula>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Gradient-targeted µ-perturbations could precisely bias an animal behavior.</title><p>(<bold>A</bold>) Protocol to deliver an optimal µ-perturbation on the experimental preparation based on jaw gradients. (Step 1) The circuit is recorded until stimulation time <inline-formula><alternatives><mml:math id="inf103"><mml:msup><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="inft103">\begin{document}$t^{*}$\end{document}</tex-math></alternatives></inline-formula>. (Step 2) The closest bioRNN trial to the ongoing recorded trial is retrieved from the databank <inline-formula><alternatives><mml:math id="inf104"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">B</mml:mi></mml:mrow></mml:math><tex-math id="inft104">\begin{document}$\mathcal{B}$\end{document}</tex-math></alternatives></inline-formula>. (Step 3) We select the neurons with the highest (or lowest) gradient value for the µ-perturbation. (Step 4) The µ-perturbation is delivered at <inline-formula><alternatives><mml:math id="inf105"><mml:msup><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="inft105">\begin{document}$t^{*}$\end{document}</tex-math></alternatives></inline-formula>. (<bold>B</bold>) Effect of the µ-perturbation using the artificial setup (<bold>A</bold>) under different light protocols. Practically, for ‘High gradient’, we keep step 3 as it is, for ‘Low gradient’, we change the sign of the gradient, and for ‘Zero gradient’, we pick the 40 neurons with lowest gradient norm. The whisker indicate the 95% confidence interval. (<bold>C</bold>) Speculative schematic of a close-up setup implementing the protocol (<bold>A</bold>) inspired by the all optical ‘read-write’ setup from <xref ref-type="bibr" rid="bib2">Aravanis et al., 2007</xref>; <xref ref-type="bibr" rid="bib37">Packer et al., 2015</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106827-fig5-v1.tif"/></fig></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Finding the right level of detail to model recorded phenomena has sparked intensive debates in computational neuroscience. When the goal is to achieve the strongest predictive power, generalist deep learning models have proven successful across many scientific disciplines, questioning how biophysical modeling plays a role in this context. Our results show that <italic>perturbation testing</italic> is a new approach to evaluate the implementation of biophysical features in a deep learning system. Our key finding about <italic>perturbation testing</italic> relies on the difficulty for deep learning models to predict the effect of optogenetic perturbations out-of-distribution (meaning, the perturbed trials are not available in the training set of the data-constrained model). We see that standard deep learning RNNs generalize poorly to perturbed trials, even when they achieved the best fit on the unperturbed test set. In contrast, this is alleviated with our bioRNN, which implements biophysical constraints that are relevant to the nature of the perturbation. In our case, modeling cell type connectivity is crucial because the optogenetic perturbations are targeted to these genetically encoded cell types. In this sense, we believe that these features were successful on the perturbation tests because they are central to modeling the perturbation of the deep learning system. Perturbation testing emerges as a quantitative tool to search for data-constrained models beyond two standard types of incomplete brain models in computational neuroscience: (1) physiologically detailed models intended to explain brain mechanisms but do not enable powerful quantitative predictions; (2) deep learning models with high predictive power but capturing a wrong biophysical mechanism, causing erroneous generalizations. We view our work as a simple and reasonable way to combine deep learning and biophysical modeling, while rigorously evaluating the combined models.</p><p>Our reconstruction method and modeling choices when building the data-constrained bioRNN are innovative and are validated on <italic>perturbation tests</italic>. We achieve a partial reconstruction of the sensory-motor pathway in the mouse cortex during a sensory detection task from electrophysiology data. The model is optimized to explain electrophysiological recordings and generalizes better than standard models to in vivo optogenetic interventions. We found unambiguously that anatomically informed sign and connectivity constraints for dominant excitatory and inhibitory cell types improve the model robustness to optogenetic perturbations. We also find that assuming that inhibitory connections are short and do not project to other areas is crucial to pass our optogenetic <italic>Perturbation test</italic>. Modeling spiking neuron dynamics and adding a sparsity prior yielded more nuanced results and was not decisive, showing that making a difference on <italic>Perturbation testing</italic> is challenging. In hindsight, we conclude that adding biological constraints becomes beneficial when (1) they model the interaction between the circuit and the perturbation mechanism; (2) their implementation should not impair the efficiency of the optimization process.</p><p>Broadly speaking, this hindsight is also supported by other results elsewhere in neuroscience. For instance, biologically inspired topological networks having higher correlation for neighboring neurons are more consistent with comparable causal interventions in the Monkey’s visual system (<xref ref-type="bibr" rid="bib48">Schrimpf et al., 2024</xref>), and detailed cell-type distribution and connectome improve models of vision in the fly brain (<xref ref-type="bibr" rid="bib29">Lappalainen et al., 2023</xref>; <xref ref-type="bibr" rid="bib13">Cowley et al., 2024</xref>). For future work, there is a dense knowledge of unexploited physiological data at the connectivity, laminar, or cell-type level that could be added to improve a cortical model like ours (<xref ref-type="bibr" rid="bib23">Harris et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Liu et al., 2022</xref>; <xref ref-type="bibr" rid="bib59">Udvary et al., 2022</xref>; <xref ref-type="bibr" rid="bib54">Staiger and Petersen, 2021</xref>; <xref ref-type="bibr" rid="bib46">Rimehaug et al., 2023</xref>). By submitting the extended models to the relevant <italic>perturbation tests</italic>, it becomes possible to measure quantitatively the goodness of their biological mechanism implementations. We do not rule out that significant improvements on <italic>perturbation tests</italic> can also be achieved with other means (e.g. by training deep learning architectures <xref ref-type="bibr" rid="bib4">Azabou et al., 2024</xref>; <xref ref-type="bibr" rid="bib39">Pandarinath et al., 2018</xref>; <xref ref-type="bibr" rid="bib64">Ye et al., 2023</xref> on larger datasets to enable generalization, or with generic regularization techniques like low-rank connectivity <xref ref-type="bibr" rid="bib16">Dubreuil et al., 2022</xref>; <xref ref-type="bibr" rid="bib61">Valente et al., 2022</xref>). However, in a similar way as the σRNN was a priori an excellent predictor on our initial test set, any powerful brain model will likely have failure modes that can be well characterized and measured with an appropriate perturbation test. So <italic>perturbation tests</italic> could become a central component of an iterative loop to identify needed data collection or model improvements towards robust brain models.</p><p>To highlight the importance of perturbation-robust circuit models, we have discussed possible implications for experimental neuroscience in section 2.3. We build the RNN twin of the biological circuit from unperturbed electrode recordings. By implementing the correct biophysical constraints, the RNN becomes perturbation robust (i.e. it predicts the effect of causal perturbation) even without including perturbation data in the RNN training. We then demonstrated in silico that gradients of this RNN produce sensitivity maps to target micro-stimulation of the biological circuit. As a result, we could design a hypothetical closed-loop setup combining read-write electrophysiology with a brain model to influence the brain activity or behavior, having potentially important practical and ethical consequences. More conceptually, we have shown theoretically that the gradients of a perturbation-robust RNN are also consistent with the gradients of the recorded biological circuits. In perspective with the foundational role of gradients in machine learning theory (<xref ref-type="bibr" rid="bib30">LeCun et al., 2015</xref>; <xref ref-type="bibr" rid="bib45">Richards and Kording, 2023</xref>), it enables the measurement of ‘brain gradients’ and lays a computational link between in vivo experimental research and decades of theoretical results on artificial learning and cognition.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Mathematical toy model of the difficult causal inference between H1 and H2</title><p>Let’s consider two simplistic mathematical models that both depend on two binary random variables <inline-formula><alternatives><mml:math id="inf106"><mml:mi>A</mml:mi></mml:math><tex-math id="inft106">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf107"><mml:mi>B</mml:mi></mml:math><tex-math id="inft107">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> which represent that putative area A is active as <inline-formula><alternatives><mml:math id="inf108"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft108">\begin{document}$A=1$\end{document}</tex-math></alternatives></inline-formula> and area B as <inline-formula><alternatives><mml:math id="inf109"><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft109">\begin{document}$B=1$\end{document}</tex-math></alternatives></inline-formula>. With this notation, we can construct two hypothetical causal mechanisms <inline-formula><alternatives><mml:math id="inf110"><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:math><tex-math id="inft110">\begin{document}$H1$\end{document}</tex-math></alternatives></inline-formula> (‘a feedforward hypothesis’) and <inline-formula><alternatives><mml:math id="inf111"><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:math><tex-math id="inft111">\begin{document}$H2$\end{document}</tex-math></alternatives></inline-formula> (‘a recurrent hypothesis’), which are radically different. The empirical frequency <inline-formula><alternatives><mml:math id="inf112"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft112">\begin{document}$p(A,B)$\end{document}</tex-math></alternatives></inline-formula> of the outcome does not allow us to differentiate whether the system was generated by a feedforward mechanism <inline-formula><alternatives><mml:math id="inf113"><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:math><tex-math id="inft113">\begin{document}$H1$\end{document}</tex-math></alternatives></inline-formula> or a recurrent mechanism <inline-formula><alternatives><mml:math id="inf114"><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:math><tex-math id="inft114">\begin{document}$H2$\end{document}</tex-math></alternatives></inline-formula>. Schematically, we can represent the two mechanism hypotheses as follows:<disp-formula id="equ2"><label>(2)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle p\left(A,B\right) $$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ3"><label>(3)</label><alternatives><mml:math id="m3"><mml:mi>B</mml:mi></mml:math><tex-math id="t3">\begin{document}$$\displaystyle B$$\end{document}</tex-math></alternatives></disp-formula></p><p>For hypothesis <inline-formula><alternatives><mml:math id="inf115"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft115">\begin{document}$H1$\end{document}</tex-math></alternatives></inline-formula>: we assume that external inputs are driving the activity of area <inline-formula><alternatives><mml:math id="inf116"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="normal">A</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft116">\begin{document}$\rm A$\end{document}</tex-math></alternatives></inline-formula> such that <inline-formula><alternatives><mml:math id="inf117"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft117">\begin{document}$A=1$\end{document}</tex-math></alternatives></inline-formula> is active with probability <inline-formula><alternatives><mml:math id="inf118"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft118">\begin{document}$p_0$\end{document}</tex-math></alternatives></inline-formula>, and there are strong feed-forward connections from <inline-formula><alternatives><mml:math id="inf119"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft119">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> to <inline-formula><alternatives><mml:math id="inf120"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>B</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft120">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> causing systemically <inline-formula><alternatives><mml:math id="inf121"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft121">\begin{document}$B=1$\end{document}</tex-math></alternatives></inline-formula> as soon as <inline-formula><alternatives><mml:math id="inf122"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft122">\begin{document}$A=1$\end{document}</tex-math></alternatives></inline-formula>. Alternatively, in <inline-formula><alternatives><mml:math id="inf123"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft123">\begin{document}$H2$\end{document}</tex-math></alternatives></inline-formula>, we assume that areas <inline-formula><alternatives><mml:math id="inf124"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="normal">A</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft124">\begin{document}$\rm A$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf125"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="normal">B</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft125">\begin{document}$\rm B$\end{document}</tex-math></alternatives></inline-formula> receive independent external inputs with probability <inline-formula><alternatives><mml:math id="inf126"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:msqrt></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft126">\begin{document}$p_1=1- \sqrt{1-p_0} $\end{document}</tex-math></alternatives></inline-formula>. Each of these two inputs is sufficient to cause <inline-formula><alternatives><mml:math id="inf127"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft127">\begin{document}$A=1$\end{document}</tex-math></alternatives></inline-formula> or <inline-formula><alternatives><mml:math id="inf128"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft128">\begin{document}$B=1$\end{document}</tex-math></alternatives></inline-formula>, and the two areas are also strongly connected, so <inline-formula><alternatives><mml:math id="inf129"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft129">\begin{document}$A=1$\end{document}</tex-math></alternatives></inline-formula> always causes <inline-formula><alternatives><mml:math id="inf130"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft130">\begin{document}$B=1$\end{document}</tex-math></alternatives></inline-formula> and vice versa. Under these hypothetical mechanisms <inline-formula><alternatives><mml:math id="inf131"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft131">\begin{document}$H1$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf132"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft132">\begin{document}$H2$\end{document}</tex-math></alternatives></inline-formula>, one finds that the empirical probability table <inline-formula><alternatives><mml:math id="inf133"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft133">\begin{document}$p\left(A,B\right) $\end{document}</tex-math></alternatives></inline-formula> is identical as seen in the proof at the end of subsection: <inline-formula><alternatives><mml:math id="inf134"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft134">\begin{document}$p_{H2}\left(A=1,B=1\right) =2p_1 - p_1^2 = p_0$\end{document}</tex-math></alternatives></inline-formula> (‘Hit trial’, both areas are active), <inline-formula><alternatives><mml:math id="inf135"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft135">\begin{document}$p\left(A=0,B=0\right) =1-p_0$\end{document}</tex-math></alternatives></inline-formula> (‘Miss trial’, the areas are quiescent). In both cases, the possibility that only one area is active is excluded by construction. So for any <inline-formula><alternatives><mml:math id="inf136"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft136">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf137"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>B</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft137">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> <inline-formula><alternatives><mml:math id="inf138"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft138">\begin{document}$p_{H1} \left(A,B\right) = p_{H2}\left(A,B\right) $\end{document}</tex-math></alternatives></inline-formula> and in other words, even if we observe an infinite number of trials and compute any statistics of the binary activations <inline-formula><alternatives><mml:math id="inf139"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft139">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf140"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>B</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft140">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula>, discriminating the two possible causal interactions (<inline-formula><alternatives><mml:math id="inf141"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="normal">H</mml:mi><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft141">\begin{document}$\rm H1$\end{document}</tex-math></alternatives></inline-formula> versus <inline-formula><alternatives><mml:math id="inf142"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="normal">H</mml:mi><mml:mn>2</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft142">\begin{document}$\rm H2$\end{document}</tex-math></alternatives></inline-formula>) is impossible.</p><p>A solution to discriminate between hypotheses <inline-formula><alternatives><mml:math id="inf143"><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:math><tex-math id="inft143">\begin{document}$H1$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf144"><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:math><tex-math id="inft144">\begin{document}$H2$\end{document}</tex-math></alternatives></inline-formula> is to induce a causal perturbation. We can discriminate between our two hypotheses if we can impose a perturbation that forces the inactivation of area <inline-formula><alternatives><mml:math id="inf145"><mml:mi>B</mml:mi></mml:math><tex-math id="inft145">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> in both mathematical models. In mathematical terms, we refer to the <inline-formula><alternatives><mml:math id="inf146"><mml:mi>d</mml:mi><mml:mi>o</mml:mi></mml:math><tex-math id="inft146">\begin{document}$do$\end{document}</tex-math></alternatives></inline-formula> operator from causality theory. Under the feedforward mechanism <inline-formula><alternatives><mml:math id="inf147"><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:math><tex-math id="inft147">\begin{document}$H1$\end{document}</tex-math></alternatives></inline-formula> and inactivation of <inline-formula><alternatives><mml:math id="inf148"><mml:mi>B</mml:mi></mml:math><tex-math id="inft148">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf149"><mml:mi>A</mml:mi></mml:math><tex-math id="inft149">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> is not affected <inline-formula><alternatives><mml:math id="inf150"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft150">\begin{document}$p_{H1}\left(A=1|\,do\,(B=0)\right)= p_{0}$\end{document}</tex-math></alternatives></inline-formula>. Under the recurrent hypothesis, <inline-formula><alternatives><mml:math id="inf151"><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:math><tex-math id="inft151">\begin{document}$H2$\end{document}</tex-math></alternatives></inline-formula>, and inactivation of <inline-formula><alternatives><mml:math id="inf152"><mml:mi>B</mml:mi></mml:math><tex-math id="inft152">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf153"><mml:mi>A</mml:mi></mml:math><tex-math id="inft153">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> is activated only by its external input such that <inline-formula><alternatives><mml:math id="inf154"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mn>1</mml:mn><mml:mo>≠</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft154">\begin{document}$p_{H2}\left(A=1\,|\,do\,(B=0)\right)=p1\neq p_{0}$\end{document}</tex-math></alternatives></inline-formula>. So the measurement of the frequency of activation of area <inline-formula><alternatives><mml:math id="inf155"><mml:mi>A</mml:mi></mml:math><tex-math id="inft155">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> under inactivation of <inline-formula><alternatives><mml:math id="inf156"><mml:mi>B</mml:mi></mml:math><tex-math id="inft156">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> can discriminate between <inline-formula><alternatives><mml:math id="inf157"><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:math><tex-math id="inft157">\begin{document}$H1$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf158"><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:math><tex-math id="inft158">\begin{document}$H2$\end{document}</tex-math></alternatives></inline-formula> which illustrates mathematically how a causal perturbation can be decisive to discriminate between those two hypothetical mechanisms.</p><p>Proof: Let <inline-formula><alternatives><mml:math id="inf159"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>a</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft159">\begin{document}$a$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf160"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>b</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft160">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula> denote the binary exteral inputs into A and B, we have:<inline-formula><alternatives><mml:math id="inf161"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mstyle><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em"><mml:mlabeledtr><mml:mtd id="mjx-eqn-2"><mml:mtext>(2)</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft161">\begin{document}$ \begin{align} p_{H2}(A=1, B=1)= \sum_{a,b}p(A=1, B=1|a,b) p(a,b) = p(a=1,b=1) + p(b=0, a=1) + p(b=1,a=0) \end{align}$\end{document}</tex-math></alternatives></inline-formula> then if we have that <inline-formula><alternatives><mml:math id="inf162"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft162">\begin{document}$p\left(A=1,B=1\mid a,b\right)$\end{document}</tex-math></alternatives></inline-formula>  is 0 or 1, and <inline-formula><alternatives><mml:math id="inf163"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft163">\begin{document}$p\left(a=1\right)= \left(b=1\right) =p1$\end{document}</tex-math></alternatives></inline-formula> . With the independence between <inline-formula><alternatives><mml:math id="inf164"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft164">\begin{document}$a$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf165"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>b</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft165">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula> we find: <inline-formula><alternatives><mml:math id="inf166"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft166">\begin{document}$p\left(A=1,B=1\right) =2p_1 - p_1^2 = p_0$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec id="s4-2"><title>Neuron and jaw movement model</title><p>We model neurons as leaky-integrate and fire (LIF) neurons. The output of every neuron <inline-formula><alternatives><mml:math id="inf167"><mml:mi>j</mml:mi></mml:math><tex-math id="inft167">\begin{document}$j$\end{document}</tex-math></alternatives></inline-formula> at time <inline-formula><alternatives><mml:math id="inf168"><mml:mi>t</mml:mi></mml:math><tex-math id="inft168">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> is a binary outcome <inline-formula><alternatives><mml:math id="inf169"><mml:msubsup><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft169">\begin{document}$z_{j}^{t}$\end{document}</tex-math></alternatives></inline-formula> (spike if <inline-formula><alternatives><mml:math id="inf170"><mml:msubsup><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft170">\begin{document}$z_{j}^{t}=1$\end{document}</tex-math></alternatives></inline-formula>, no spike if <inline-formula><alternatives><mml:math id="inf171"><mml:msubsup><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft171">\begin{document}$z_{j}^{t}=0$\end{document}</tex-math></alternatives></inline-formula>) generated from its membrane voltage <inline-formula><alternatives><mml:math id="inf172"><mml:msubsup><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft172">\begin{document}$v_{j}^{t}$\end{document}</tex-math></alternatives></inline-formula>. The following equations give the dynamics of the membrane voltage <inline-formula><alternatives><mml:math id="inf173"><mml:msubsup><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft173">\begin{document}$v_{j}^{t}$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ4"><label>(4)</label><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>v</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>z</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle v_j^{t} = \alpha_j v_j^{t-1} + (1-\alpha_j) u_j^t - v_{\mathrm{thr},j} z_j^{t-1} + \xi^t_j$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ5"><label>(5)</label><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>u</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mfrac><mml:msubsup><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mfrac><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle  u_j^t = \sum_{d,i} W_{ij}^{rec, d} \frac{z_i^{t-d}}{\delta t} + \sum_{i} W_{ij}^{\mathrm{in}} \frac{x_i^{t}}{\delta t}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf174"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft174">\begin{document}$W_{ij}^{d}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf175"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft175">\begin{document}$W_{ij}^{in,d}$\end{document}</tex-math></alternatives></inline-formula> are the recurrent and input weight matrices. The timestep of the simulation <inline-formula><alternatives><mml:math id="inf176"><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:math><tex-math id="inft176">\begin{document}$\delta t$\end{document}</tex-math></alternatives></inline-formula> is 2ms when we simulate the real dataset and 1ms otherwise. The superscript <inline-formula><alternatives><mml:math id="inf177"><mml:mi>d</mml:mi></mml:math><tex-math id="inft177">\begin{document}$d$\end{document}</tex-math></alternatives></inline-formula> denotes the synaptic delay; every synapse has one synaptic delay of either 2 or 3ms. With <inline-formula><alternatives><mml:math id="inf178"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:math><tex-math id="inft178">\begin{document}$\alpha_{j}=\exp\left(-\frac{\delta t}{\tau_{m,j}}\right)$\end{document}</tex-math></alternatives></inline-formula>, we define the integration speed of the membrane voltage, where <inline-formula><alternatives><mml:math id="inf179"><mml:msub><mml:mi>τ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:math><tex-math id="inft179">\begin{document}$\tau_{m}=30$\end{document}</tex-math></alternatives></inline-formula> ms for excitatory and <inline-formula><alternatives><mml:math id="inf180"><mml:msub><mml:mi>τ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math><tex-math id="inft180">\begin{document}$\tau_{m}=10$\end{document}</tex-math></alternatives></inline-formula> ms for inhibitory neurons. The noise source <inline-formula><alternatives><mml:math id="inf181"><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft181">\begin{document}$\xi_{j}^{t}$\end{document}</tex-math></alternatives></inline-formula> is a Gaussian random variable with zero mean and standard deviation <inline-formula><alternatives><mml:math id="inf182"><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:msqrt></mml:math><tex-math id="inft182">\begin{document}$\beta_{j}v_{\mathrm{thr},j}\sqrt{\delta t}$\end{document}</tex-math></alternatives></inline-formula> (<inline-formula><alternatives><mml:math id="inf183"><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft183">\begin{document}$\beta_{j}$\end{document}</tex-math></alternatives></inline-formula> is typically initialized at 0.14). The input <inline-formula><alternatives><mml:math id="inf184"><mml:msubsup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft184">\begin{document}$x_{i}^{t}$\end{document}</tex-math></alternatives></inline-formula> is a binary pulse signal with a duration of 10ms. For the real dataset, we have two binary pulse input signals, one for the whisker deflection and one for the auditory cue. The spikes are sampled with a Bernoulli distribution <inline-formula><alternatives><mml:math id="inf185"><mml:msubsup><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">B</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft185">\begin{document}$z_{j}^{t}\sim\mathcal{B}(\exp(\frac{v_{j}^{t}-v_{\mathrm{thr},j}}{v_{0}}))$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf186"><mml:msub><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft186">\begin{document}$v_{0}$\end{document}</tex-math></alternatives></inline-formula> is the temperature of the exponential function and <inline-formula><alternatives><mml:math id="inf187"><mml:msub><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft187">\begin{document}$v_{thr,j}$\end{document}</tex-math></alternatives></inline-formula> is the effective membrane threshold. After each spike, the neuron receives a reset current with an amplitude of <inline-formula><alternatives><mml:math id="inf188"><mml:msub><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft188">\begin{document}$v_{trh,j}$\end{document}</tex-math></alternatives></inline-formula> and enters an absolute refractory period of 4ms, during which it cannot fire.</p><p>For networks fitted to the real dataset, we also simulate the jaw movement. The jaw movement trace <inline-formula><alternatives><mml:math id="inf189"><mml:mi>y</mml:mi></mml:math><tex-math id="inft189">\begin{document}$y$\end{document}</tex-math></alternatives></inline-formula> is controlled by a linear readout from the spiking activity of all excitatory neurons. Specifically, <inline-formula><alternatives><mml:math id="inf190"><mml:mi>y</mml:mi></mml:math><tex-math id="inft190">\begin{document}$y$\end{document}</tex-math></alternatives></inline-formula> is computed as <inline-formula><alternatives><mml:math id="inf191"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:math><tex-math id="inft191">\begin{document}$y=\exp(\tilde{y})+b$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf192"><mml:mi>b</mml:mi></mml:math><tex-math id="inft192">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula> is a scaling parameter and <inline-formula><alternatives><mml:math id="inf193"><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft193">\begin{document}$\tilde{y}^{t}$\end{document}</tex-math></alternatives></inline-formula> is given by <inline-formula><alternatives><mml:math id="inf194"><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft194">\begin{document}$\tilde{y}^{t}=\alpha_{jaw}\tilde{y}^{t-1}+(1-\alpha_{jaw})\sum_{d,j}W_{j}^{jaw }z_{j}^{t-d}$\end{document}</tex-math></alternatives></inline-formula>. Here, <inline-formula><alternatives><mml:math id="inf195"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft195">\begin{document}$W_{j}^{jaw}$\end{document}</tex-math></alternatives></inline-formula> is the output weight matrix (linear readout) for the jaw, and <inline-formula><alternatives><mml:math id="inf196"><mml:msub><mml:mi>τ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math><tex-math id="inft196">\begin{document}$\tau_{jaw}=5$\end{document}</tex-math></alternatives></inline-formula> ms defines <inline-formula><alternatives><mml:math id="inf197"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft197">\begin{document}$\alpha_{jaw}=\exp(-\frac{\delta t}{\tau_{jaw}})$\end{document}</tex-math></alternatives></inline-formula>, which controls the integration velocity of the jaw trace.</p></sec><sec id="s4-3"><title>Session-stitching and network structure</title><p>As in <xref ref-type="bibr" rid="bib51">Sourmpis et al., 2023</xref>, we simulate multi-area cortical neuronal activity fitted to electrophysiology neural recordings. Before we start the optimization, we define and fix each neuron’s area and cell type in the model by uniquely assigning them to a neuron from the recordings. For the real dataset from <xref ref-type="bibr" rid="bib19">Esmaeili et al., 2021</xref>, the cell type is inferred from the cell’s action potential waveform (with fast-spiking neurons classified as inhibitory and regular-spiking neurons as excitatory). Most electrophysiology datasets include recordings from multiple sessions, and our method would typically require simultaneous recordings of all neurons. To address this challenge, similarly to <xref ref-type="bibr" rid="bib51">Sourmpis et al., 2023</xref> we use the technique called ‘session-stitching’ which allows neighboring modeled neurons to be mapped with neurons recorded across multiple sessions. This effectively creates a ‘collage’ that integrates data from multiple sessions within our model. This approach has practical implications for our optimization process. Specifically, the trial-matching loss includes a term for each session, with the overall loss calculated as the average across all sessions (see, Optimization and loss function).</p><p>For both the real and the synthetic datasets, we simulate each area with 250 LIF neurons and impose that each area has 200 excitatory neurons and 50 inhibitory. Respecting the observation that inhibitory neurons mostly project in the area that they belong to <xref ref-type="bibr" rid="bib56">Tamamaki and Tomioka, 2010</xref>; <xref ref-type="bibr" rid="bib33">Markram et al., 2004</xref>, we don’t allow for across-area inhibitory connections. The ‘thalamic’ input is available to every neuron of the circuit, and the ‘motor’ output for the real dataset, that is jaw movement, is extracted with a trained linear readout from all the excitatory neurons of the network, see (Neuron and jaw movement model).</p></sec><sec id="s4-4"><title>Reference circuits for hypotheses 1 and 2</title><p>To build a synthetic dataset that illustrates the difficulty of separating the feedforward (H1) and recurrent hypotheses (H2), we construct two reference spiking circuit models RefCirc1 and RefCirc2. The two networks consist of two areas A and B, and their activity follows the hard causal inference problem from method (Mathematical toy model of the difficult causal inference between H1 and H2), making it hard to distinguish A1 and A2 when recording the co-activation of A and B. Moreover, to make the problem even harder, the two networks are constructed to make it almost impossible to distinguish between H1 and H2 with dense recordings: the two circuits are designed to have the same PSTH and single-trial network dynamics despite their structural difference; one is feedforward and the other is recurrent.</p><p>To do so, RefCirc1 and 2 are circuit models that start from random network initializations following the specifications described in Methods sections Neuron and jaw movement model and Session-stitching and network structure. The only difference is that we do not allow feedback connections from A to B in RefCirc1; the construction below is otherwise identical. The synaptic weights of the two circuits are optimized with the losses described in Methods (Optimization and loss function) to fit the identical target statistics in all areas: the same PSTH activity for each neuron and the same distribution of single-trial network dynamics. The target statistics are chosen so the activity in RefCirc1 and 2 resembles kinematics and statistics from a primary and a secondary sensory area. The baseline firing rates of the neurons are dictated by the target PSTH distribution and it follows a log-normal distribution, with excitatory neurons having a mean of 2.9 Hz and a standard deviation of 1.25 Hz and inhibitory neurons having a mean of 4.47 Hz and a standard deviation of 1.31 Hz. The distribution of single-trial activity is given by the targeted single-trial dynamics: in RefCirc1 and 2, the areas A and B respond to input 50% of the time with a transient population average response following a double exponential kernel characterized by  <inline-formula><alternatives><mml:math id="inf198"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft198">\begin{document}$\tau_{rise}$\end{document}</tex-math></alternatives></inline-formula> = 5ms and  <inline-formula><alternatives><mml:math id="inf199"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft199">\begin{document}$\tau_{fall}$\end{document}</tex-math></alternatives></inline-formula> = 20ms. Mimicking a short signal propagation between areas, these transients have a 4ms delay in area A and 12ms delay in B (relative to the onset time of the stimulus). To impose a ‘behavioral’ hit versus miss distribution that could emerge from a feedforward and recurrent hypothesis (see Methods, Mathematical toy model of the difficult causal inference between H1 and H2), the targeted population-averaged response of each trial is either a double-exponential transient in both area A and B (‘Hit trials’), or remains at a baseline level in both areas (‘Miss trials’) in the remaining trials. At the end of the training, we verified that RefCirc1 and RefCirc2 generate very similar network activity in the absence of perturbation (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The circuits are then frozen and used to generate the synthetic dataset. We generate 2000 trials from these RefCircs, 1000 of which are used for the training set and 1000 for the testing set.</p></sec><sec id="s4-5"><title>Optimization and loss function</title><p>The optimization method we use to fit our models is back-propagation through time (BPTT). To overcome the non-differentiability of the spiking function, we use surrogate gradients (<xref ref-type="bibr" rid="bib36">Neftci et al., 2019</xref>). In particular, we use the piece-wise linear surrogate derivative from <xref ref-type="bibr" rid="bib6">Bellec et al., 2018b</xref>. For the derivative calculations, we use <inline-formula><alternatives><mml:math id="inf200"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mfrac></mml:math><tex-math id="inft200">\begin{document}$\frac{v_{j}^{t}-v_{\mathrm{thr},j}}{v_{0}}$\end{document}</tex-math></alternatives></inline-formula> and not <inline-formula><alternatives><mml:math id="inf201"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft201">\begin{document}$\mathrm{exp}(\frac{v_{j}^{t}-v_{\mathrm{thr},j}}{v_{0}})$\end{document}</tex-math></alternatives></inline-formula>. We use sample-and-measure loss functions that rely on summary statistics, as in <xref ref-type="bibr" rid="bib7">Bellec et al., 2021</xref>; <xref ref-type="bibr" rid="bib51">Sourmpis et al., 2023</xref>, to fit the networks to the data. Our loss function has two main terms: one to fit the trial-averaged activity of every neuron (<inline-formula><alternatives><mml:math id="inf202"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft202">\begin{document}$\mathcal{L}_{\mathrm{neuron}}$\end{document}</tex-math></alternatives></inline-formula>), and one to fit the single trial population average activity (<inline-formula><alternatives><mml:math id="inf203"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math><tex-math id="inft203">\begin{document}$\mathcal{L}_{\mathrm{trial}}$\end{document}</tex-math></alternatives></inline-formula>), <inline-formula><alternatives><mml:math id="inf204"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math><tex-math id="inft204">\begin{document}$\mathcal{L}=\mathcal{L}_{\mathrm{neuron}}+\mathcal{L}_{\mathrm{trial}}$\end{document}</tex-math></alternatives></inline-formula>. The two terms of the loss function are reweighted with a parameter-free multi-task method (<xref ref-type="bibr" rid="bib14">Défossez et al., 2023</xref>) that enables the gradients to have comparable scales.</p><p>As in <xref ref-type="bibr" rid="bib51">Sourmpis et al., 2023</xref>: (1) To calculate the trial-averaged loss, we first filter the trial-averaged spiking activity <inline-formula><alternatives><mml:math id="inf205"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft205">\begin{document}$\mathcal{T}^{t}_{\mathrm{neuron},j}(\boldsymbol{z})=\frac{1}{K}\sum_{k} \boldsymbol{z}^{t}_{j,k}\ast f$\end{document}</tex-math></alternatives></inline-formula> using a rolling average window (<inline-formula><alternatives><mml:math id="inf206"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>f</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft206">\begin{document}$f$\end{document}</tex-math></alternatives></inline-formula>) of 8 ms. We then normalize it by the trial-averaged filtered data activity, (<inline-formula><alternatives><mml:math id="inf207"><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math><tex-math id="inft207">\begin{document}$\boldsymbol{z}^{\mathcal{D}}$\end{document}</tex-math></alternatives></inline-formula> are recorded spike trains)<disp-formula id="equ6"><label>(6)</label><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle \mathcal{T}^{\prime t}_{\mathrm{neuron},j}(\boldsymbol{z}) = (\mathcal{T}^t_{\mathrm{neuron},j}(\boldsymbol{z}) - \langle \mathcal{T}^t_{\mathrm{neuron},j}(\boldsymbol{z}^{\mathcal{D}}) \rangle_t)/(\sigma_t(\mathcal{T}^t_{\mathrm{neuron},j}(\boldsymbol{z}^{\mathcal{D}}))),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf208"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mo>.</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft208">\begin{document}$\langle.\rangle_{t}$\end{document}</tex-math></alternatives></inline-formula> is the time average, and <inline-formula><alternatives><mml:math id="inf209"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft209">\begin{document}$\sigma_{t}$\end{document}</tex-math></alternatives></inline-formula> the standard deviation over time. The trial-averaged loss function is defined as:<disp-formula id="equ7"><label>(7)</label><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:munderover><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mtext> </mml:mtext><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle  \mathcal{L}_{\mathrm{neuron}} = \sum_{j}^N \sum_t^T \| \mathcal{T}^{\prime t}_{\mathrm{neuron},j}(\boldsymbol{z}) - \mathcal{T}^{\prime t}_{\mathrm{neuron},j}(\boldsymbol{z}^{\mathcal{D}}) \| ^2~,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf210"><mml:mi>T</mml:mi></mml:math><tex-math id="inft210">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> is the number of time points in a trial and <inline-formula><alternatives><mml:math id="inf211"><mml:mi>N</mml:mi></mml:math><tex-math id="inft211">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula> is the number of neurons. For the real dataset, where we want to fit also the jaw movement, we have an additional term for the trial-averaged filtered and normalized jaw, <inline-formula><alternatives><mml:math id="inf212"><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><tex-math id="inft212">\begin{document}$\|\sum_{t}^{T}\mathcal{T}^{\prime t}_{\mathrm{neuron}}(\boldsymbol{y})- \mathcal{T}^{\prime t}_{\mathrm{neuron}}(\boldsymbol{y}^{\mathcal{D}})\|^{2}$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf213"><mml:mi>y</mml:mi></mml:math><tex-math id="inft213">\begin{document}$y$\end{document}</tex-math></alternatives></inline-formula> is the simulated jaw movement and <inline-formula><alternatives><mml:math id="inf214"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math><tex-math id="inft214">\begin{document}$y^{\mathcal{D}}$\end{document}</tex-math></alternatives></inline-formula> the recorded jaw movement.</p><p>To calculate the trial-matching loss, we first filter the population-average activity of each area <inline-formula><alternatives><mml:math id="inf215"><mml:mi>A</mml:mi></mml:math><tex-math id="inft215">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf216"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>A</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:math><tex-math id="inft216">\begin{document}$\mathcal{T}^{t}_{A,k}(\boldsymbol{z})=\frac{1}{|A|}\sum_{j\in A}\boldsymbol{z} ^{t}_{j,k}\ast f$\end{document}</tex-math></alternatives></inline-formula>, using a rolling average window of 32ms. We then normalize it by the population-averaged filtered activity of the same area from the recordings, <inline-formula><alternatives><mml:math id="inf217"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft217">\begin{document}$\mathcal{T}^{\prime t}_{A,k}(\boldsymbol{z})=(\mathcal{T}^{t}_{A,k}(\boldsymbol{z})-\langle\mathcal{T}^{t}_{A,k}(\boldsymbol{z}^{\mathcal{D}}) \rangle_{k})/\sigma_{k}(\mathcal{T}^{t}_{A,k}(\boldsymbol{z}^{\mathcal{D}}))$\end{document}</tex-math></alternatives></inline-formula>, and concatenate all the areas that were simultaneously recorded, <inline-formula><alternatives><mml:math id="inf218"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft218">\begin{document}$\mathcal{T}^{\prime t}_{\mathrm{trial},k}(\boldsymbol{z})=(\mathcal{T}^{\prime t }_{A1,k},\mathcal{T}^{\prime t}_{A2,k})$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf219"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mo>.</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft219">\begin{document}$\langle.\rangle_{k}$\end{document}</tex-math></alternatives></inline-formula> is the trial average, and <inline-formula><alternatives><mml:math id="inf220"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft220">\begin{document}$\sigma_{k}$\end{document}</tex-math></alternatives></inline-formula> the standard deviation over trials. The trial-matching loss is defined as:<disp-formula id="equ8"><label>(8)</label><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mi>k</mml:mi><mml:mi>K</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:munderover><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mtext> </mml:mtext><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle  \mathcal{L}_{\mathrm{trial}} = \min_{\pi} \sum_k^K \sum_t^T \| \mathcal{T}^{\prime t}_{\mathrm{trial}, k} \left(\boldsymbol{z} \right) - \mathcal{T}^{\prime t}_{\mathrm{trial},\pi(k)} \left(\boldsymbol{z}^{\mathcal{D}} \right) \|^2~,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf221"><mml:mi>π</mml:mi></mml:math><tex-math id="inft221">\begin{document}$\pi$\end{document}</tex-math></alternatives></inline-formula> is an assignment between pairs of <inline-formula><alternatives><mml:math id="inf222"><mml:mi>K</mml:mi></mml:math><tex-math id="inft222">\begin{document}$K$\end{document}</tex-math></alternatives></inline-formula> recorded and generated trials <inline-formula><alternatives><mml:math id="inf223"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>π</mml:mi><mml:mo>:</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>K</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo stretchy="false">→</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>K</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft223">\begin{document}$\pi:\{1,\dots K\}\rightarrow\{1,\dots K\}$\end{document}</tex-math></alternatives></inline-formula>. Note that the minimum over <inline-formula><alternatives><mml:math id="inf224"><mml:mi>π</mml:mi></mml:math><tex-math id="inft224">\begin{document}$\pi$\end{document}</tex-math></alternatives></inline-formula> is a combinatorial optimization that needs to be calculated for every evaluation of the loss function. For the real dataset, we consider the jaw movement as an additional area, and we concatenate it to the <inline-formula><alternatives><mml:math id="inf225"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft225">\begin{document}$\mathcal{T}^{\prime t}_{\mathrm{trial},k}=(\mathcal{T}^{\prime t}_{A1,k}, \mathcal{T}^{\prime t}_{A2,k},\mathcal{T}^{\prime t}_{jaw,k})$\end{document}</tex-math></alternatives></inline-formula>.</p><p>Based on this loss function, we optimize the following parameters: <inline-formula><alternatives><mml:math id="inf226"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft226">\begin{document}$W_{ij}^{rec,d}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf227"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft227">\begin{document}$W_{ij}^{in,d}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf228"><mml:msub><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft228">\begin{document}$v_{thr,j}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf229"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>β</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft229">\begin{document}$\beta$\end{document}</tex-math></alternatives></inline-formula> for the RefCircs. For the RNNs, we optimize only the recurrent connectivity <inline-formula><alternatives><mml:math id="inf230"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft230">\begin{document}$W_{ij}^{rec,d}$\end{document}</tex-math></alternatives></inline-formula>, and the rest are fixed from the RefCircs. For the real dataset, in addition to the parameters optimized in the RefCircs, we also optimize the jaw’s linear readout <inline-formula><alternatives><mml:math id="inf231"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft231">\begin{document}$W_{j}^{jaw}$\end{document}</tex-math></alternatives></inline-formula> and its scaling parameter <inline-formula><alternatives><mml:math id="inf232"><mml:mi>b</mml:mi></mml:math><tex-math id="inft232">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula>.</p><sec id="s4-5-1"><title>Implementing Dale’s law and local inhibition</title><p>In our network, the recurrent weights <inline-formula><alternatives><mml:math id="inf233"><mml:msup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft233">\begin{document}$W^{rec}$\end{document}</tex-math></alternatives></inline-formula> are computed as the elementwise product of two matrices: <inline-formula><alternatives><mml:math id="inf234"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msup><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft234">\begin{document}$\tilde{W}^{rec}$\end{document}</tex-math></alternatives></inline-formula>, which encodes the strength of synaptic efficacies and is always positive, and <inline-formula><alternatives><mml:math id="inf235"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft235">\begin{document}$W^{rec}_{sign}$\end{document}</tex-math></alternatives></inline-formula>, which has a fixed sign determined by the neurotransmitter type of the presynaptic neuron and <inline-formula><alternatives><mml:math id="inf236"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft236">\begin{document}$|W^{rec}_{sign}|=1$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ9"><label>(9)</label><alternatives><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>∘</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t9">\begin{document}$$\displaystyle  W^{rec} = \tilde{W}^{rec} \circ W^{rec}_{sign}$$\end{document}</tex-math></alternatives></disp-formula></p><p>To enforce Dale’s law during optimization, we set any negative values of <inline-formula><alternatives><mml:math id="inf237"><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft237">\begin{document}$\tilde{W}^{rec}$\end{document}</tex-math></alternatives></inline-formula> to zero at each iteration as in <xref ref-type="bibr" rid="bib5">Bellec et al., 2018a</xref>. Similarly, to constrain only local inhibitory connections during optimization, we zero out any changes in the synaptic efficacies of across-areas inhibitory connections at each iteration. In simplified models, Dale’s law or the local inhibition constraint can be disrupted by omitting this correction step.</p><p>The success of the network optimization highly depends on the initialization of the recurrent weight matrices. To initialize signed matrices, we follow the theoretical (<xref ref-type="bibr" rid="bib44">Rajan and Abbott, 2006</xref>) and practical insights (<xref ref-type="bibr" rid="bib6">Bellec et al., 2018b</xref>; <xref ref-type="bibr" rid="bib12">Cornford et al., 2020</xref>) developed previously. After defining the constraints on the weight signs <inline-formula><alternatives><mml:math id="inf238"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft238">\begin{document}$W^{rec}_{sign}$\end{document}</tex-math></alternatives></inline-formula>, the initialization amplitude <inline-formula><alternatives><mml:math id="inf239"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msup><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft239">\begin{document}$\tilde{W}^{rec}$\end{document}</tex-math></alternatives></inline-formula> for each target neuron is adjusted to a zero-summed input weights (the sum of incoming excitatory inputs is equal to the sum of inhibitory inputs). Then the weight amplitude is re-normalized by the modulus of its largest eigenvalue of <inline-formula><alternatives><mml:math id="inf240"><mml:msup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft240">\begin{document}$W^{rec}$\end{document}</tex-math></alternatives></inline-formula>, so all the eigenvalues of this matrix <inline-formula><alternatives><mml:math id="inf241"><mml:msup><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft241">\begin{document}$W^{rec}$\end{document}</tex-math></alternatives></inline-formula> have modulus 1 or smaller.</p></sec><sec id="s4-5-2"><title>Stopping criterion for the optimization</title><p>For the synthetic dataset, we train the models for 4000 gradient descent steps. For the real dataset, due to limited data and a noisy test set, we select the final model based on the optimization step that yields the best trial-type accuracy (closest to the trial-type accuracy from the data), derived from the jaw trace and whisker stimulus, along with the highest trial-matched Pearson correlation between the model and the recordings.</p></sec><sec id="s4-5-3"><title>Sparsity regularization</title><p>There is a plethora of ways to enforce sparsity. In this work, we use weight regularization. In particular, we use the p- norm with p=1/2 of the recurrent and input weights that promote a high level of sparsity (<xref ref-type="bibr" rid="bib63">Xu et al., 2012</xref>). To avoid numerical instabilities, we apply this regularization only for synaptic weights above <inline-formula><alternatives><mml:math id="inf242"><mml:mi>α</mml:mi></mml:math><tex-math id="inft242">\begin{document}$\alpha$\end{document}</tex-math></alternatives></inline-formula> and prune all synapses below <inline-formula><alternatives><mml:math id="inf243"><mml:mi>α</mml:mi></mml:math><tex-math id="inft243">\begin{document}$\alpha$\end{document}</tex-math></alternatives></inline-formula>. (we set <inline-formula><alternatives><mml:math id="inf244"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft244">\begin{document}$\alpha=1e^{-7}$\end{document}</tex-math></alternatives></inline-formula>). The regularized loss function becomes:<disp-formula id="equ10"><label>(10)</label><alternatives><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t10">\begin{document}$$\displaystyle  \mathcal{L}_{all} = \mathcal{L} + \lambda_1 \left\|W^{in}\right\|_{\frac{1}{2}}^{\frac{1}{2}} + \lambda_2 \left\|W^{rec, d}\right\|_{\frac{1}{2}}^{\frac{1}{2}} + \lambda_3 \left\|W^{\mathrm{across}, d}\right\|_{\frac{1}{2}}^{\frac{1}{2}} ,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf245"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft245">\begin{document}$W^{\mathrm{across},d}$\end{document}</tex-math></alternatives></inline-formula> are the connections from one area to the other.</p><p>For the synthetic dataset, we choose the level of across-area sparsity by performing a small grid search for <inline-formula><alternatives><mml:math id="inf246"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft246">\begin{document}$\lambda_{3}$\end{document}</tex-math></alternatives></inline-formula>. In particular, the sparsity level <inline-formula><alternatives><mml:math id="inf247"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft247">\begin{document}$\lambda_{3}$\end{document}</tex-math></alternatives></inline-formula> is the maximum value <inline-formula><alternatives><mml:math id="inf248"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft248">\begin{document}$\lambda_{3}$\end{document}</tex-math></alternatives></inline-formula> where the performance remains as good as without sparsity, see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>. For the real dataset, we use the same value <inline-formula><alternatives><mml:math id="inf249"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft249">\begin{document}$\lambda_{3}$\end{document}</tex-math></alternatives></inline-formula> as the one we found for the full reconstruction method of bioRNN1.</p></sec></sec><sec id="s4-6"><title>Perturbation test of in silico optogenetics</title><p>In systems neuroscience, a method to test causal interactions between brain regions uses spatially and temporally precise optogenetic activations or inactivations (<xref ref-type="bibr" rid="bib19">Esmaeili et al., 2021</xref>; <xref ref-type="bibr" rid="bib22">Guo et al., 2014</xref>). Usually, inactivations refer to the strong activation of inhibitory neurons for cortical areas. These inhibitory neurons have strong intra-area connections that effectively ‘silence’ their local neighborhood (<xref ref-type="bibr" rid="bib24">Helmstaedter et al., 2009</xref>).</p><p>Our model can simulate these perturbations and allow us to compare the causal mechanisms of two networks based on their responses to optogenetic perturbations. We implement activations and inactivations as a strong input current to all the neurons in one area’s excitatory or inhibitory population. For the RefCircs and reconstructed RNNs, we use a transient current that lasts 40ms, from 20ms before to 20ms after the input stimulus. The strength of the current (light power) varies until there is an effect in the full reconstruction method bioRNN1. For the synthetic dataset in <xref ref-type="fig" rid="fig2">Figure 2</xref> (except for panel D), we inject a current of <inline-formula><alternatives><mml:math id="inf250"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>u</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.08</mml:mn></mml:math><tex-math id="inft250">\begin{document}$\Delta u_{i}^{t}=0.08$\end{document}</tex-math></alternatives></inline-formula> into excitatory neurons for activations and <inline-formula><alternatives><mml:math id="inf251"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>u</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft251">\begin{document}$\Delta u_{i}^{t}=1$\end{document}</tex-math></alternatives></inline-formula> into inhibitory neurons for inactivations. For the real dataset, we perform optogenetics inactivations in three different periods. As in <xref ref-type="bibr" rid="bib19">Esmaeili et al., 2021</xref>, we silence the cortical circuit during the whisker presentation, the time between the whisker and auditory stimulus, or when the animal was licking for the reward. In particular, we use transient currents to the inhibitory neurons during (i.) 100 ms before and after the whisker presentation, (ii.) 100 ms after the whisker presentation till 100ms before the onset of the auditory cue, and (iii.) after the auditory cue till the end of our simulation. For cases (i.) and (ii.), we linearly decreased the strength of the current to avoid rebound excitation. The light power is chosen so that our model has the best results in reproducing the lick probability of the recordings. It is important to mention that the perturbation data are not used to optimize the network but to test whether the resulting network has the same causal interactions with the recordings.</p><p>For the RefCircs and bioRNNs, we evaluate the effect of the perturbations directly from the neural activity. We use the distance of network dynamics <inline-formula><alternatives><mml:math id="inf252"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math><tex-math id="inft252">\begin{document}$\mathcal{L}_{\mathrm{trial}}$\end{document}</tex-math></alternatives></inline-formula> to compare the two perturbed networks. For the real dataset, we compare the effect of the inactivations on the behavior; as behavior here, we mean whether the mouse/model licked. We classify the licking action using a multilayer perceptron with two hidden layers with 128 neurons each. The classifier is trained with the jaw movement of the real dataset, which was extracted from video filming using custom software, to predict the lick action, which was extracted from a piezo sensor placed in the spout. This classifier predicted lick correctly 94% of the time. We then used the same classifier on the jaw movement from the model to determine whether there was a ‘lick’ or not. For the comparisons in both the artificial and real datasets, we trained multiple models with different random seeds for each variant and aggregated the results. The different random seeds affect both the weight initialization and the noise of our model. In particular, we used from three to six different random seeds for each different model variant.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Supervision, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-106827-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The code for this project is open-sourced and published at <ext-link ext-link-type="uri" xlink:href="https://github.com/Sourmpis/BiologicallyInformed">https://github.com/Sourmpis/BiologicallyInformed</ext-link> (copy archived at <xref ref-type="bibr" rid="bib52">Sourmpis, 2026</xref>). The dataset for the artificial dataset can be downloaded/generated on our code repository. The in vivo dataset was published openly for the previous publication <xref ref-type="bibr" rid="bib19">Esmaeili et al., 2021</xref>. The dataset is accessible at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.4720013">https://doi.org/10.5281/zenodo.4720013</ext-link>.</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Esmaeili</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Data set for &quot;Rapid suppression and sustained activation of distinct cortical regions for a delayed sensory-triggered motor response&quot;</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.4720013</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Alireza Modirshanechi, Shuqi Wang, and Tâm Nguyen for their valuable feedback on the manuscript. We are grateful to Vahid Esmaeili for collecting the dataset and ongoing support throughout this project. This research is supported by the Sinergia project CRSII5_198612, the Swiss National Science Foundation (SNSF) project 200020_207426 awarded to WG, SNSF projects TMAG-3_209271 and 31003 A_182010 awarded to CP, and the Vienna Science and Technology Fund (WWTF) project VRG24-018 awarded to GB.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adesnik</surname><given-names>H</given-names></name><name><surname>Abdeladim</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Probing neural codes with two-photon holographic optogenetics</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1356</fpage><lpage>1366</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00902-9</pub-id><pub-id pub-id-type="pmid">34400843</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aravanis</surname><given-names>AM</given-names></name><name><surname>Wang</surname><given-names>LP</given-names></name><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>Meltzer</surname><given-names>LA</given-names></name><name><surname>Mogri</surname><given-names>MZ</given-names></name><name><surname>Schneider</surname><given-names>MB</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>An optical neural interface: in vivo control of rodent motor cortex with integrated fiberoptic and optogenetic technology</article-title><source>Journal of Neural Engineering</source><volume>4</volume><fpage>S143</fpage><lpage>S156</lpage><pub-id pub-id-type="doi">10.1088/1741-2560/4/3/S02</pub-id><pub-id pub-id-type="pmid">17873414</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arthur</surname><given-names>BJ</given-names></name><name><surname>Kim</surname><given-names>CM</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Preibisch</surname><given-names>S</given-names></name><name><surname>Darshan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A scalable implementation of the recursive least-squares algorithm for training spiking neural networks</article-title><source>Frontiers in Neuroinformatics</source><volume>17</volume><elocation-id>1099510</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2023.1099510</pub-id><pub-id pub-id-type="pmid">37441157</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Azabou</surname><given-names>M</given-names></name><name><surname>Arora</surname><given-names>V</given-names></name><name><surname>Ganesh</surname><given-names>V</given-names></name><name><surname>Mao</surname><given-names>X</given-names></name><name><surname>Nachimuthu</surname><given-names>S</given-names></name><name><surname>Mendelson</surname><given-names>M</given-names></name><name><surname>Richards</surname><given-names>B</given-names></name><name><surname>Perich</surname><given-names>M</given-names></name><name><surname>Lajoie</surname><given-names>G</given-names></name><name><surname>Dyer</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>A unified, scalable framework for neural population decoding</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2310.16046</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bellec</surname><given-names>G</given-names></name><name><surname>Kappel</surname><given-names>D</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Legenstein</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Deep rewiring: training very sparse deep networks</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1711.05136</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bellec</surname><given-names>G</given-names></name><name><surname>Salaj</surname><given-names>D</given-names></name><name><surname>Subramoney</surname><given-names>A</given-names></name><name><surname>Legenstein</surname><given-names>R</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Long short-term memory and learning-to-learn in networks of spiking neurons</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1803.09574</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bellec</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Modirshanechi</surname><given-names>A</given-names></name><name><surname>Brea</surname><given-names>J</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Fitting summary statistics of neural data with a differentiable spiking network simulator</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2106.10064</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname><given-names>B</given-names></name><name><surname>Benson</surname><given-names>J</given-names></name><name><surname>Birman</surname><given-names>D</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Catarino</surname><given-names>JA</given-names></name><name><surname>Chapuis</surname><given-names>GA</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name><collab>International Brain Laboratory</collab></person-group><year iso-8601-date="2023">2023</year><article-title>A brain-wide map of neural activity during complex behaviour</article-title><source>Nature</source><volume>1</volume><elocation-id>092350</elocation-id><pub-id pub-id-type="doi">10.1038/s41586-025-09235-0</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Billeh</surname><given-names>YN</given-names></name><name><surname>Cai</surname><given-names>B</given-names></name><name><surname>Gratiy</surname><given-names>SL</given-names></name><name><surname>Dai</surname><given-names>K</given-names></name><name><surname>Iyer</surname><given-names>R</given-names></name><name><surname>Gouwens</surname><given-names>NW</given-names></name><name><surname>Abbasi-Asl</surname><given-names>R</given-names></name><name><surname>Jia</surname><given-names>X</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Olsen</surname><given-names>SR</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Arkhipov</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Systematic integration of structural and functional data into multi-scale models of mouse primary visual cortex</article-title><source>Neuron</source><volume>106</volume><fpage>388</fpage><lpage>403</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.01.040</pub-id><pub-id pub-id-type="pmid">32142648</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyden</surname><given-names>ES</given-names></name><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>Bamberg</surname><given-names>E</given-names></name><name><surname>Nagel</surname><given-names>G</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Millisecond-timescale, genetically targeted optical control of neural activity</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1263</fpage><lpage>1268</lpage><pub-id pub-id-type="doi">10.1038/nn1525</pub-id><pub-id pub-id-type="pmid">16116447</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Scherr</surname><given-names>F</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A data-based large-scale model for primary visual cortex enables brain-like robust and versatile visual processing</article-title><source>Science Advances</source><volume>8</volume><elocation-id>7592</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abq7592</pub-id><pub-id pub-id-type="pmid">36322646</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cornford</surname><given-names>J</given-names></name><name><surname>Kalajdzievski</surname><given-names>D</given-names></name><name><surname>Leite</surname><given-names>M</given-names></name><name><surname>Lamarquette</surname><given-names>A</given-names></name><name><surname>Kullmann</surname><given-names>DM</given-names></name><name><surname>Richards</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Learning to live with Dale’s principle: ANNs with separate excitatory and inhibitory units</article-title><source>Neuroscience</source><volume>1</volume><elocation-id>364968</elocation-id><pub-id pub-id-type="doi">10.1101/2020.11.02.364968</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowley</surname><given-names>BR</given-names></name><name><surname>Calhoun</surname><given-names>AJ</given-names></name><name><surname>Rangarajan</surname><given-names>N</given-names></name><name><surname>Ireland</surname><given-names>E</given-names></name><name><surname>Turner</surname><given-names>MH</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Mapping model units to visual neurons reveals population code for social behaviour</article-title><source>Nature</source><volume>629</volume><fpage>1100</fpage><lpage>1108</lpage><pub-id pub-id-type="doi">10.1038/s41586-024-07451-8</pub-id><pub-id pub-id-type="pmid">38778103</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Défossez</surname><given-names>A</given-names></name><name><surname>Copet</surname><given-names>J</given-names></name><name><surname>Synnaeve</surname><given-names>G</given-names></name><name><surname>Adi</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>High fidelity neural audio compression</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2210.13438</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deny</surname><given-names>S</given-names></name><name><surname>Ferrari</surname><given-names>U</given-names></name><name><surname>Macé</surname><given-names>E</given-names></name><name><surname>Yger</surname><given-names>P</given-names></name><name><surname>Caplette</surname><given-names>R</given-names></name><name><surname>Picaud</surname><given-names>S</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Multiplexed computations in retinal ganglion cells of a single type</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>1964</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-02159-y</pub-id><pub-id pub-id-type="pmid">29213097</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dubreuil</surname><given-names>A</given-names></name><name><surname>Valente</surname><given-names>A</given-names></name><name><surname>Beiran</surname><given-names>M</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The role of population structure in computations through neural dynamics</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>783</fpage><lpage>794</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01088-4</pub-id><pub-id pub-id-type="pmid">35668174</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eccles</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>From electrical to chemical transmission in the central nervous system: the closing address of the sir henry dale centennial symposium</article-title><source>Notes and Records of the Royal Society of London</source><volume>1</volume><elocation-id>0015</elocation-id><pub-id pub-id-type="doi">10.1098/rsnr.1976.0015</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elman</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Finding structure in time</article-title><source>Cognitive Science</source><volume>14</volume><fpage>179</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1207/s15516709cog1402_1</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esmaeili</surname><given-names>V</given-names></name><name><surname>Tamura</surname><given-names>K</given-names></name><name><surname>Muscinelli</surname><given-names>SP</given-names></name><name><surname>Modirshanechi</surname><given-names>A</given-names></name><name><surname>Boscaglia</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>AB</given-names></name><name><surname>Oryshchuk</surname><given-names>A</given-names></name><name><surname>Foustoukos</surname><given-names>G</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Crochet</surname><given-names>S</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Petersen</surname><given-names>CCH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Rapid suppression and sustained activation of distinct cortical regions for a delayed sensory-triggered motor response</article-title><source>Neuron</source><volume>109</volume><fpage>2183</fpage><lpage>2201</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.05.005</pub-id><pub-id pub-id-type="pmid">34077741</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Fraile</surname><given-names>JG</given-names></name><name><surname>Scherr</surname><given-names>F</given-names></name><name><surname>Ramasco</surname><given-names>JJ</given-names></name><name><surname>Arkhipov</surname><given-names>A</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Mirasso</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Competition between bottom-up visual input and internal inhibition generates error neurons in a model of the mouse primary visual cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.01.27.525984</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grosenick</surname><given-names>L</given-names></name><name><surname>Marshel</surname><given-names>JH</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Closed-loop and activity-guided optogenetic control</article-title><source>Neuron</source><volume>86</volume><fpage>106</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.034</pub-id><pub-id pub-id-type="pmid">25856490</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>ZV</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Ophir</surname><given-names>E</given-names></name><name><surname>Gutnisky</surname><given-names>D</given-names></name><name><surname>Ting</surname><given-names>JT</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Flow of cortical activity underlying a tactile decision in mice</article-title><source>Neuron</source><volume>81</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.020</pub-id><pub-id pub-id-type="pmid">24361077</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>JA</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Hirokawa</surname><given-names>KE</given-names></name><name><surname>Whitesell</surname><given-names>JD</given-names></name><name><surname>Choi</surname><given-names>H</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Bohn</surname><given-names>P</given-names></name><name><surname>Caldejon</surname><given-names>S</given-names></name><name><surname>Casal</surname><given-names>L</given-names></name><name><surname>Cho</surname><given-names>A</given-names></name><name><surname>Feiner</surname><given-names>A</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Gaudreault</surname><given-names>N</given-names></name><name><surname>Gerfen</surname><given-names>CR</given-names></name><name><surname>Graddis</surname><given-names>N</given-names></name><name><surname>Groblewski</surname><given-names>PA</given-names></name><name><surname>Henry</surname><given-names>AM</given-names></name><name><surname>Ho</surname><given-names>A</given-names></name><name><surname>Howard</surname><given-names>R</given-names></name><name><surname>Knox</surname><given-names>JE</given-names></name><name><surname>Kuan</surname><given-names>L</given-names></name><name><surname>Kuang</surname><given-names>X</given-names></name><name><surname>Lecoq</surname><given-names>J</given-names></name><name><surname>Lesnar</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Luviano</surname><given-names>J</given-names></name><name><surname>McConoughey</surname><given-names>S</given-names></name><name><surname>Mortrud</surname><given-names>MT</given-names></name><name><surname>Naeemi</surname><given-names>M</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Oh</surname><given-names>SW</given-names></name><name><surname>Ouellette</surname><given-names>B</given-names></name><name><surname>Shen</surname><given-names>E</given-names></name><name><surname>Sorensen</surname><given-names>SA</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Williford</surname><given-names>A</given-names></name><name><surname>Phillips</surname><given-names>JW</given-names></name><name><surname>Jones</surname><given-names>AR</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hierarchical organization of cortical and thalamic connectivity</article-title><source>Nature</source><volume>575</volume><fpage>195</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1716-z</pub-id><pub-id pub-id-type="pmid">31666704</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helmstaedter</surname><given-names>M</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name><name><surname>Feldmeyer</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>L2/3 interneuron groups defined by multiparameter analysis of axonal projection, dendritic geometry, and electrical excitability</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>951</fpage><lpage>962</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn130</pub-id><pub-id pub-id-type="pmid">18802122</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hodgkin</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="1958">1958</year><article-title>Ionic movements and electrical activity in giant nerve fibres</article-title><source>Proceedings of the Royal Society of London. Series B, Biological Sciences</source><volume>148</volume><fpage>1</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1098/rspb.1958.0001</pub-id><pub-id pub-id-type="pmid">13494473</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Isbister</surname><given-names>JB</given-names></name><name><surname>Ecker</surname><given-names>A</given-names></name><name><surname>Pokorny</surname><given-names>C</given-names></name><name><surname>Bolaños-Puchet</surname><given-names>S</given-names></name><name><surname>Egas Santander</surname><given-names>D</given-names></name><name><surname>Arnaudon</surname><given-names>A</given-names></name><name><surname>Awile</surname><given-names>O</given-names></name><name><surname>Barros-Zulaica</surname><given-names>N</given-names></name><name><surname>Blanco Alonso</surname><given-names>J</given-names></name><name><surname>Boci</surname><given-names>E</given-names></name><name><surname>Chindemi</surname><given-names>G</given-names></name><name><surname>Courcol</surname><given-names>JD</given-names></name><name><surname>Damart</surname><given-names>T</given-names></name><name><surname>Delemontex</surname><given-names>T</given-names></name><name><surname>Dietz</surname><given-names>A</given-names></name><name><surname>Ficarelli</surname><given-names>G</given-names></name><name><surname>Gevaert</surname><given-names>M</given-names></name><name><surname>Herttuainen</surname><given-names>J</given-names></name><name><surname>Ivaska</surname><given-names>G</given-names></name><name><surname>Ji</surname><given-names>W</given-names></name><name><surname>Keller</surname><given-names>D</given-names></name><name><surname>King</surname><given-names>J</given-names></name><name><surname>Kumbhar</surname><given-names>P</given-names></name><name><surname>Lapere</surname><given-names>S</given-names></name><name><surname>Litvak</surname><given-names>P</given-names></name><name><surname>Mandge</surname><given-names>D</given-names></name><name><surname>Muller</surname><given-names>EB</given-names></name><name><surname>Pereira</surname><given-names>F</given-names></name><name><surname>Planas</surname><given-names>J</given-names></name><name><surname>Ranjan</surname><given-names>R</given-names></name><name><surname>Reva</surname><given-names>M</given-names></name><name><surname>Romani</surname><given-names>A</given-names></name><name><surname>Rössert</surname><given-names>C</given-names></name><name><surname>Schürmann</surname><given-names>F</given-names></name><name><surname>Sood</surname><given-names>V</given-names></name><name><surname>Teska</surname><given-names>A</given-names></name><name><surname>Tuncel</surname><given-names>A</given-names></name><name><surname>Van Geit</surname><given-names>W</given-names></name><name><surname>Wolf</surname><given-names>M</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Ramaswamy</surname><given-names>S</given-names></name><name><surname>Reimann</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Modeling and Simulation of Neocortical Micro- and Mesocircuitry. Part II: Physiology and Experimentation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.05.17.541168</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>CM</given-names></name><name><surname>Finkelstein</surname><given-names>A</given-names></name><name><surname>Chow</surname><given-names>CC</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Darshan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Distributing task-related neural activity across a cortical network through task-independent connections</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>2851</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-38529-y</pub-id><pub-id pub-id-type="pmid">37202424</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lakunina</surname><given-names>A</given-names></name><name><surname>Socha</surname><given-names>KZ</given-names></name><name><surname>Ladd</surname><given-names>A</given-names></name><name><surname>Bowen</surname><given-names>AJ</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Colonell</surname><given-names>J</given-names></name><name><surname>Doshi</surname><given-names>A</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Krumin</surname><given-names>M</given-names></name><name><surname>Kulik</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>A</given-names></name><name><surname>Neutens</surname><given-names>P</given-names></name><name><surname>O’Callaghan</surname><given-names>J</given-names></name><name><surname>Olsen</surname><given-names>M</given-names></name><name><surname>Putzeys</surname><given-names>J</given-names></name><name><surname>Tilmans</surname><given-names>HA</given-names></name><name><surname>Ye</surname><given-names>Z</given-names></name><name><surname>Welkenhuysen</surname><given-names>M</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Ting</surname><given-names>JT</given-names></name><name><surname>Dutta</surname><given-names>B</given-names></name><name><surname>Harris</surname><given-names>TD</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><collab>Neuropixels Opto Consortium</collab></person-group><year iso-8601-date="2025">2025</year><article-title>Neuropixels opto: combining high-resolution electrophysiology and optogenetics</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2025.02.04.636286</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lappalainen</surname><given-names>JK</given-names></name><name><surname>Tschopp</surname><given-names>FD</given-names></name><name><surname>Prakhya</surname><given-names>S</given-names></name><name><surname>McGill</surname><given-names>M</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Shinomiya</surname><given-names>K</given-names></name><name><surname>Takemura</surname><given-names>SY</given-names></name><name><surname>Gruntman</surname><given-names>E</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Turaga</surname><given-names>SC</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Connectome-constrained deep mechanistic networks predict neural responses across the fly visual system at single-neuron resolution</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.11.532232</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Foustoukos</surname><given-names>G</given-names></name><name><surname>Crochet</surname><given-names>S</given-names></name><name><surname>Petersen</surname><given-names>CCH</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Axonal and dendritic morphology of excitatory neurons in layer 2/3 mouse barrel cortex imaged through whole-brain two-photon tomography and registered to a digital brain atlas</article-title><source>Frontiers in Neuroanatomy</source><volume>15</volume><elocation-id>791015</elocation-id><pub-id pub-id-type="doi">10.3389/fnana.2021.791015</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mahuas</surname><given-names>G</given-names></name><name><surname>Isacchini</surname><given-names>G</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Ferrari</surname><given-names>U</given-names></name><name><surname>Mora</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A New Inference Approach for Training Shallow and Deep Generalized Linear Models of Noisy Interacting Neurons</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.06.11.145904</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Toledo-Rodriguez</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Silberberg</surname><given-names>G</given-names></name><name><surname>Wu</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Interneurons of the neocortical inhibitory system</article-title><source>Nature Reviews. Neuroscience</source><volume>5</volume><fpage>793</fpage><lpage>807</lpage><pub-id pub-id-type="doi">10.1038/nrn1519</pub-id><pub-id pub-id-type="pmid">15378039</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Muller</surname><given-names>E</given-names></name><name><surname>Ramaswamy</surname><given-names>S</given-names></name><name><surname>Reimann</surname><given-names>MW</given-names></name><name><surname>Abdellah</surname><given-names>M</given-names></name><name><surname>Sanchez</surname><given-names>CA</given-names></name><name><surname>Ailamaki</surname><given-names>A</given-names></name><name><surname>Alonso-Nanclares</surname><given-names>L</given-names></name><name><surname>Antille</surname><given-names>N</given-names></name><name><surname>Arsever</surname><given-names>S</given-names></name><name><surname>Kahou</surname><given-names>GAA</given-names></name><name><surname>Berger</surname><given-names>TK</given-names></name><name><surname>Bilgili</surname><given-names>A</given-names></name><name><surname>Buncic</surname><given-names>N</given-names></name><name><surname>Chalimourda</surname><given-names>A</given-names></name><name><surname>Chindemi</surname><given-names>G</given-names></name><name><surname>Courcol</surname><given-names>J-D</given-names></name><name><surname>Delalondre</surname><given-names>F</given-names></name><name><surname>Delattre</surname><given-names>V</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name><name><surname>Dumusc</surname><given-names>R</given-names></name><name><surname>Dynes</surname><given-names>J</given-names></name><name><surname>Eilemann</surname><given-names>S</given-names></name><name><surname>Gal</surname><given-names>E</given-names></name><name><surname>Gevaert</surname><given-names>ME</given-names></name><name><surname>Ghobril</surname><given-names>J-P</given-names></name><name><surname>Gidon</surname><given-names>A</given-names></name><name><surname>Graham</surname><given-names>JW</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Haenel</surname><given-names>V</given-names></name><name><surname>Hay</surname><given-names>E</given-names></name><name><surname>Heinis</surname><given-names>T</given-names></name><name><surname>Hernando</surname><given-names>JB</given-names></name><name><surname>Hines</surname><given-names>M</given-names></name><name><surname>Kanari</surname><given-names>L</given-names></name><name><surname>Keller</surname><given-names>D</given-names></name><name><surname>Kenyon</surname><given-names>J</given-names></name><name><surname>Khazen</surname><given-names>G</given-names></name><name><surname>Kim</surname><given-names>Y</given-names></name><name><surname>King</surname><given-names>JG</given-names></name><name><surname>Kisvarday</surname><given-names>Z</given-names></name><name><surname>Kumbhar</surname><given-names>P</given-names></name><name><surname>Lasserre</surname><given-names>S</given-names></name><name><surname>Le Bé</surname><given-names>J-V</given-names></name><name><surname>Magalhães</surname><given-names>BRC</given-names></name><name><surname>Merchán-Pérez</surname><given-names>A</given-names></name><name><surname>Meystre</surname><given-names>J</given-names></name><name><surname>Morrice</surname><given-names>BR</given-names></name><name><surname>Muller</surname><given-names>J</given-names></name><name><surname>Muñoz-Céspedes</surname><given-names>A</given-names></name><name><surname>Muralidhar</surname><given-names>S</given-names></name><name><surname>Muthurasa</surname><given-names>K</given-names></name><name><surname>Nachbaur</surname><given-names>D</given-names></name><name><surname>Newton</surname><given-names>TH</given-names></name><name><surname>Nolte</surname><given-names>M</given-names></name><name><surname>Ovcharenko</surname><given-names>A</given-names></name><name><surname>Palacios</surname><given-names>J</given-names></name><name><surname>Pastor</surname><given-names>L</given-names></name><name><surname>Perin</surname><given-names>R</given-names></name><name><surname>Ranjan</surname><given-names>R</given-names></name><name><surname>Riachi</surname><given-names>I</given-names></name><name><surname>Rodríguez</surname><given-names>J-R</given-names></name><name><surname>Riquelme</surname><given-names>JL</given-names></name><name><surname>Rössert</surname><given-names>C</given-names></name><name><surname>Sfyrakis</surname><given-names>K</given-names></name><name><surname>Shi</surname><given-names>Y</given-names></name><name><surname>Shillcock</surname><given-names>JC</given-names></name><name><surname>Silberberg</surname><given-names>G</given-names></name><name><surname>Silva</surname><given-names>R</given-names></name><name><surname>Tauheed</surname><given-names>F</given-names></name><name><surname>Telefont</surname><given-names>M</given-names></name><name><surname>Toledo-Rodriguez</surname><given-names>M</given-names></name><name><surname>Tränkler</surname><given-names>T</given-names></name><name><surname>Van Geit</surname><given-names>W</given-names></name><name><surname>Díaz</surname><given-names>JV</given-names></name><name><surname>Walker</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Zaninetta</surname><given-names>SM</given-names></name><name><surname>DeFelipe</surname><given-names>J</given-names></name><name><surname>Hill</surname><given-names>SL</given-names></name><name><surname>Segev</surname><given-names>I</given-names></name><name><surname>Schürmann</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reconstruction and simulation of neocortical microcircuitry</article-title><source>Cell</source><volume>163</volume><fpage>456</fpage><lpage>492</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.09.029</pub-id><pub-id pub-id-type="pmid">26451489</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Minai</surname><given-names>Y</given-names></name><name><surname>Smith</surname><given-names>M</given-names></name><name><surname>Soldado-Magraner</surname><given-names>J</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>MiSO: Optimizing brain stimulation to create neural activity states</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>24126</fpage><lpage>24149</lpage><pub-id pub-id-type="doi">10.52202/079017-0760</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neftci</surname><given-names>EO</given-names></name><name><surname>Mostafa</surname><given-names>H</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Surrogate gradient learning in spiking neural networks: bringing the power of gradient-based optimization to spiking neural networks</article-title><source>IEEE Signal Processing Magazine</source><volume>36</volume><fpage>51</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1109/MSP.2019.2931595</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Packer</surname><given-names>AM</given-names></name><name><surname>Russell</surname><given-names>LE</given-names></name><name><surname>Dalgleish</surname><given-names>HWP</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Simultaneous all-optical manipulation and recording of neural circuit activity with cellular resolution in vivo</article-title><source>Nature Methods</source><volume>12</volume><fpage>140</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1038/nmeth.3217</pub-id><pub-id pub-id-type="pmid">25532138</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pals</surname><given-names>M</given-names></name><name><surname>Sağtekin</surname><given-names>AE</given-names></name><name><surname>Pei</surname><given-names>F</given-names></name><name><surname>Gloeckler</surname><given-names>M</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Inferring Stochastic Low-Rank Recurrent Neural Networks from Neural Data</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2406.16749</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandarinath</surname><given-names>C</given-names></name><name><surname>O’Shea</surname><given-names>DJ</given-names></name><name><surname>Collins</surname><given-names>J</given-names></name><name><surname>Jozefowicz</surname><given-names>R</given-names></name><name><surname>Stavisky</surname><given-names>SD</given-names></name><name><surname>Kao</surname><given-names>JC</given-names></name><name><surname>Trautmann</surname><given-names>EM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title><source>Nature Methods</source><volume>15</volume><fpage>805</fpage><lpage>815</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0109-9</pub-id><pub-id pub-id-type="pmid">30224673</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papagiakoumou</surname><given-names>E</given-names></name><name><surname>Ronzitti</surname><given-names>E</given-names></name><name><surname>Emiliani</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Scanless two-photon excitation with temporal focusing</article-title><source>Nature Methods</source><volume>17</volume><fpage>571</fpage><lpage>581</lpage><pub-id pub-id-type="doi">10.1038/s41592-020-0795-y</pub-id><pub-id pub-id-type="pmid">32284609</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Arlt</surname><given-names>C</given-names></name><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Young</surname><given-names>ME</given-names></name><name><surname>Mosher</surname><given-names>CP</given-names></name><name><surname>Minxha</surname><given-names>J</given-names></name><name><surname>Carter</surname><given-names>E</given-names></name><name><surname>Rutishauser</surname><given-names>U</given-names></name><name><surname>Rudebeck</surname><given-names>PH</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name><name><surname>Rajan</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Inferring brain-wide interactions using data-constrained recurrent neural network models</article-title><source>Neuroscience</source><volume>1</volume><elocation-id>423348</elocation-id><pub-id pub-id-type="doi">10.1101/2020.12.18.423348</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title><source>Nature</source><volume>454</volume><fpage>995</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1038/nature07140</pub-id><pub-id pub-id-type="pmid">18650810</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pozzorini</surname><given-names>C</given-names></name><name><surname>Mensi</surname><given-names>S</given-names></name><name><surname>Hagens</surname><given-names>O</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Automated high-throughput characterization of single neurons by means of simplified spiking models</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004275</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004275</pub-id><pub-id pub-id-type="pmid">26083597</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname><given-names>K</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Eigenvalue spectra of random matrices for neural networks</article-title><source>Physical Review Letters</source><volume>97</volume><elocation-id>188104</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.97.188104</pub-id><pub-id pub-id-type="pmid">17155583</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The study of plasticity has always been about gradients</article-title><source>The Journal of Physiology</source><volume>601</volume><fpage>3141</fpage><lpage>3149</lpage><pub-id pub-id-type="doi">10.1113/JP282747</pub-id><pub-id pub-id-type="pmid">37078235</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rimehaug</surname><given-names>AE</given-names></name><name><surname>Stasik</surname><given-names>AJ</given-names></name><name><surname>Hagen</surname><given-names>E</given-names></name><name><surname>Billeh</surname><given-names>YN</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Dai</surname><given-names>K</given-names></name><name><surname>Olsen</surname><given-names>SR</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Einevoll</surname><given-names>GT</given-names></name><name><surname>Arkhipov</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Uncovering circuit mechanisms of current sinks and sources with biophysical simulations of primary visual cortex</article-title><source>eLife</source><volume>12</volume><elocation-id>e87169</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.87169</pub-id><pub-id pub-id-type="pmid">37486105</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rosenblatt</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1960">1960</year><chapter-title>Perceptual generalization over transformation groups</chapter-title><person-group person-group-type="editor"><name><surname>Rosenblatt</surname><given-names>F</given-names></name></person-group><source>Self Organizing Systems</source><publisher-name>Pergamon Press</publisher-name><fpage>63</fpage><lpage>96</lpage></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>McGrath</surname><given-names>P</given-names></name><name><surname>Margalit</surname><given-names>E</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Do topographic deep ANN models of the primate ventral stream predict the perceptual effects of direct IT cortical interventions?</article-title><source>Neuroscience</source><volume>1</volume><elocation-id>572970</elocation-id><pub-id pub-id-type="doi">10.1101/2024.01.09.572970</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Shai</surname><given-names>A</given-names></name><name><surname>Schnitzer</surname><given-names>M</given-names></name><name><surname>Tanaka</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Cornn: convex optimization of recurrent neural networks for rapid inference of neural dynamics</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2311.10200</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Jia</surname><given-names>X</given-names></name><name><surname>Durand</surname><given-names>S</given-names></name><name><surname>Gale</surname><given-names>S</given-names></name><name><surname>Bennett</surname><given-names>C</given-names></name><name><surname>Graddis</surname><given-names>N</given-names></name><name><surname>Heller</surname><given-names>G</given-names></name><name><surname>Ramirez</surname><given-names>TK</given-names></name><name><surname>Choi</surname><given-names>H</given-names></name><name><surname>Luviano</surname><given-names>JA</given-names></name><name><surname>Groblewski</surname><given-names>PA</given-names></name><name><surname>Ahmed</surname><given-names>R</given-names></name><name><surname>Arkhipov</surname><given-names>A</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Billeh</surname><given-names>YN</given-names></name><name><surname>Brown</surname><given-names>D</given-names></name><name><surname>Buice</surname><given-names>MA</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Caldejon</surname><given-names>S</given-names></name><name><surname>Casal</surname><given-names>L</given-names></name><name><surname>Cho</surname><given-names>A</given-names></name><name><surname>Chvilicek</surname><given-names>M</given-names></name><name><surname>Cox</surname><given-names>TC</given-names></name><name><surname>Dai</surname><given-names>K</given-names></name><name><surname>Denman</surname><given-names>DJ</given-names></name><name><surname>de Vries</surname><given-names>SEJ</given-names></name><name><surname>Dietzman</surname><given-names>R</given-names></name><name><surname>Esposito</surname><given-names>L</given-names></name><name><surname>Farrell</surname><given-names>C</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Galbraith</surname><given-names>J</given-names></name><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Gelfand</surname><given-names>EC</given-names></name><name><surname>Hancock</surname><given-names>N</given-names></name><name><surname>Harris</surname><given-names>JA</given-names></name><name><surname>Howard</surname><given-names>R</given-names></name><name><surname>Hu</surname><given-names>B</given-names></name><name><surname>Hytnen</surname><given-names>R</given-names></name><name><surname>Iyer</surname><given-names>R</given-names></name><name><surname>Jessett</surname><given-names>E</given-names></name><name><surname>Johnson</surname><given-names>K</given-names></name><name><surname>Kato</surname><given-names>I</given-names></name><name><surname>Kiggins</surname><given-names>J</given-names></name><name><surname>Lambert</surname><given-names>S</given-names></name><name><surname>Lecoq</surname><given-names>J</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Lee</surname><given-names>JH</given-names></name><name><surname>Leon</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Liang</surname><given-names>E</given-names></name><name><surname>Long</surname><given-names>F</given-names></name><name><surname>Mace</surname><given-names>K</given-names></name><name><surname>Melchior</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>D</given-names></name><name><surname>Mollenkopf</surname><given-names>T</given-names></name><name><surname>Nayan</surname><given-names>C</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Ngo</surname><given-names>K</given-names></name><name><surname>Nguyen</surname><given-names>T</given-names></name><name><surname>Nicovich</surname><given-names>PR</given-names></name><name><surname>North</surname><given-names>K</given-names></name><name><surname>Ocker</surname><given-names>GK</given-names></name><name><surname>Ollerenshaw</surname><given-names>D</given-names></name><name><surname>Oliver</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Perkins</surname><given-names>J</given-names></name><name><surname>Reding</surname><given-names>M</given-names></name><name><surname>Reid</surname><given-names>D</given-names></name><name><surname>Robertson</surname><given-names>M</given-names></name><name><surname>Ronellenfitch</surname><given-names>K</given-names></name><name><surname>Seid</surname><given-names>S</given-names></name><name><surname>Slaughterbeck</surname><given-names>C</given-names></name><name><surname>Stoecklin</surname><given-names>M</given-names></name><name><surname>Sullivan</surname><given-names>D</given-names></name><name><surname>Sutton</surname><given-names>B</given-names></name><name><surname>Swapp</surname><given-names>J</given-names></name><name><surname>Thompson</surname><given-names>C</given-names></name><name><surname>Turner</surname><given-names>K</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Whitesell</surname><given-names>JD</given-names></name><name><surname>Williams</surname><given-names>D</given-names></name><name><surname>Williford</surname><given-names>A</given-names></name><name><surname>Young</surname><given-names>R</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Naylor</surname><given-names>S</given-names></name><name><surname>Phillips</surname><given-names>JW</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Olsen</surname><given-names>SR</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Survey of spiking in the mouse visual system reveals functional hierarchy</article-title><source>Nature</source><volume>592</volume><fpage>86</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-03171-x</pub-id><pub-id pub-id-type="pmid">33473216</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sourmpis</surname><given-names>C</given-names></name><name><surname>Petersen</surname><given-names>C</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Bellec</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Trial matching: capturing variability with data-constrained spiking neural networks</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2306.03603</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sourmpis</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2026">2026</year><data-title>BiologicallyInformed</data-title><version designator="swh:1:rev:e6f5984a57369dfbc42b47c26de6bbab2385f293">swh:1:rev:e6f5984a57369dfbc42b47c26de6bbab2385f293</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:bf95c3d68669519d06f3b2f8d564494a8a02fcfb;origin=https://github.com/Sourmpis/BiologicallyInformed;visit=swh:1:snp:f9dc7d12963183fa54c89df3e4cf7ac888f78ce8;anchor=swh:1:rev:e6f5984a57369dfbc42b47c26de6bbab2385f293">https://archive.softwareheritage.org/swh:1:dir:bf95c3d68669519d06f3b2f8d564494a8a02fcfb;origin=https://github.com/Sourmpis/BiologicallyInformed;visit=swh:1:snp:f9dc7d12963183fa54c89df3e4cf7ac888f78ce8;anchor=swh:1:rev:e6f5984a57369dfbc42b47c26de6bbab2385f293</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Spieler</surname><given-names>A</given-names></name><name><surname>Rahaman</surname><given-names>N</given-names></name><name><surname>Martius</surname><given-names>G</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Levina</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The expressive leaky memory neuron: an efficient and expressive phenomenological neuron model can solve long-horizon tasks</article-title><conf-name>In The Twelfth International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Staiger</surname><given-names>JF</given-names></name><name><surname>Petersen</surname><given-names>CCH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neuronal circuits in barrel cortex for whisker sensory perception</article-title><source>Physiological Reviews</source><volume>101</volume><fpage>353</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1152/physrev.00019.2019</pub-id><pub-id pub-id-type="pmid">32816652</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Zatka-Haas</surname><given-names>P</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Distributed coding of choice, action and engagement across the mouse brain</article-title><source>Nature</source><volume>576</volume><fpage>266</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1787-x</pub-id><pub-id pub-id-type="pmid">31776518</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tamamaki</surname><given-names>N</given-names></name><name><surname>Tomioka</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Long-Range GABAergic connections distributed throughout the neocortex and their possible function</article-title><source>Frontiers in Neuroscience</source><volume>4</volume><elocation-id>202</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2010.00202</pub-id><pub-id pub-id-type="pmid">21151790</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tamura</surname><given-names>K</given-names></name><name><surname>Bech</surname><given-names>P</given-names></name><name><surname>Mizuno</surname><given-names>H</given-names></name><name><surname>Veaute</surname><given-names>L</given-names></name><name><surname>Crochet</surname><given-names>S</given-names></name><name><surname>Petersen</surname><given-names>CCH</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Cell-class-specific orofacial motor maps in mouse neocortex</article-title><source>Current Biology</source><volume>35</volume><fpage>1382</fpage><lpage>1390</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2025.01.056</pub-id><pub-id pub-id-type="pmid">40015267</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teeter</surname><given-names>C</given-names></name><name><surname>Iyer</surname><given-names>R</given-names></name><name><surname>Menon</surname><given-names>V</given-names></name><name><surname>Gouwens</surname><given-names>N</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Berg</surname><given-names>J</given-names></name><name><surname>Szafer</surname><given-names>A</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Hawrylycz</surname><given-names>M</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Generalized leaky integrate-and-fire models classify multiple neuron types</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>709</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-02717-4</pub-id><pub-id pub-id-type="pmid">29459723</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Udvary</surname><given-names>D</given-names></name><name><surname>Harth</surname><given-names>P</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Hege</surname><given-names>H-C</given-names></name><name><surname>de Kock</surname><given-names>CPJ</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name><name><surname>Oberlaender</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The impact of neuron morphology on cortical network architecture</article-title><source>Cell Reports</source><volume>39</volume><elocation-id>110677</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2022.110677</pub-id><pub-id pub-id-type="pmid">35417720</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name><name><surname>Leifer</surname><given-names>AM</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Large-scale neural recordings call for new insights to link brain and behavior</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>11</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00980-9</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Valente</surname><given-names>A</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Extracting computational mechanisms from neural data using low-rank RNNs</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wyart</surname><given-names>C</given-names></name><name><surname>Jia</surname><given-names>X</given-names></name><name><surname>Kumar</surname><given-names>A</given-names></name><name><surname>Peysson</surname><given-names>N</given-names></name><name><surname>Carbo-Tano</surname><given-names>M</given-names></name><name><surname>Fidelin</surname><given-names>K</given-names></name><name><surname>Didelot</surname><given-names>M</given-names></name><name><surname>Lejeune</surname><given-names>C</given-names></name><name><surname>Grosse-Wentrup</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>All-Optical Investigation Reveals a Hierarchical Organization of Vsx2+ Reticulospinal Neurons Coordinating Steering and Forward Locomotion</article-title><source>Research Square</source><pub-id pub-id-type="doi">10.21203/rs.3.rs-7216842/v1</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Chang</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>F</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>L1/2 regularization: a thresholding representation theory and a fast solver</article-title><source>IEEE Transactions on Neural Networks and Learning Systems</source><volume>23</volume><fpage>1013</fpage><lpage>1027</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2012.2197412</pub-id><pub-id pub-id-type="pmid">24807129</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>J</given-names></name><name><surname>Collinger</surname><given-names>JL</given-names></name><name><surname>Wehbe</surname><given-names>L</given-names></name><name><surname>Gaunt</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neural Data Transformer 2: Multi-Context Pretraining for Neural Spiking Activity</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.09.18.558113</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106827.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>École Normale Supérieure - PSL</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study demonstrates the significance of incorporating biological constraints in training neural networks to develop models that make accurate predictions under novel conditions. By comparing standard sigmoid recurrent neural networks (RNNs) with biologically constrained RNNs, the manuscript offers <bold>compelling</bold> evidence that biologically grounded inductive biases enhance generalization to perturbed conditions. This manuscript will appeal to a wide audience in systems and computational neuroscience.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106827.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This manuscript introduces a biologically informed RNN (bioRNN) that predicts the effects of optogenetic perturbations in both synthetic and in vivo datasets. By comparing standard sigmoid RNNs (σRNNs) and bioRNNs, the authors make a compelling case that biologically grounded inductive biases improve generalization to perturbed conditions. This work is innovative, technically strong, and grounded in relevant neuroscience, particularly the pressing need for data-constrained models that generalize causally.</p><p>Comments on revisions:</p><p>The authors have addressed all my concerns.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106827.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Sourmpis et al. present a study in which the importance of including certain inductive biases in the fitting of recurrent networks is evaluated with respect to the generalization ability of the networks when exposed to untrained perturbations.</p><p>The work proceeds in three stages:</p><p>(i) a simple illustration of the problem is made. Two reference (ground-truth) networks with qualitatively different connectivity, but similar observable network dynamics, are constructed, and recurrent networks with varying aspects of design similarity to the reference networks are trained to reproduce the reference dynamics. The activity of these trained networks during untrained perturbations is then compared to the activity of the perturbed reference networks. It is shown that, of the design characteristics that were varied, the enforced sign (Dale's law) and locality (spatial extent) of efference were especially important.</p><p>(ii) The intuition from the constructed example is then extended to networks that have been trained to reproduce certain aspects of multi-region neural activity recorded from mice during a detection task with a working-memory component. A similar pattern is demonstrated, in which enforcing the sign and locality of efference in the fitted networks has an influence on the ability of the trained networks to predict aspects of neural activity during unseen (untrained) perturbations.</p><p>(iii) The authors then illustrate the relationship between the gradient of the motor readout of trained networks with respect to the net inputs to the network units, and the sensitivity of the motor readout to small perturbations of the input currents to the units, which (in vivo) could be controlled optogenetically. The paper is concluded with a proposed use for trained networks, in which the models could be analyzed to determine the most sensitive directions of the network and, during online monitoring, inform a targeted optogenetic perturbation to bias behavior.</p><p>The authors do not overstate their claims, and in general, I find that I agree with their conclusions.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106827.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sourmpis</surname><given-names>Christos</given-names></name><role specific-use="author">Author</role><aff><institution>École Polytechnique Fédérale de Lausanne (EPFL)</institution><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Petersen</surname><given-names>Carl CH</given-names></name><role specific-use="author">Author</role><aff><institution>École Polytechnique Fédérale de Lausanne (EPFL)</institution><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name><role specific-use="author">Author</role><aff><institution>École Polytechnique Fédérale de Lausanne (EPFL)</institution><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Bellec</surname><given-names>Guillaume</given-names></name><role specific-use="author">Author</role><aff><institution>TU Wien</institution><addr-line><named-content content-type="city">Vienna</named-content></addr-line><country>Austria</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public review)</bold></p><p>Major:</p><p>(1) In line 76, the authors make a very powerful statement: 'σRNN simulation achieves higher similarity with unseen recorded trials before perturbation, but lower than the bioRNN on perturbed trials.' I couldn't find a figure showing this. This might be buried somewhere and, in my opinion, deserves some spotlight - maybe a figure or even inclusion in the abstract.</p></disp-quote><p>We agree with the reviewer that these results are important. The failure of σRNN on perturbed data could be inferred from the former Figures 1E, 2C-E, and 3D. Following the reviewers' comments, we have tried to make this the most prominent message of Figure 1, in particular with the addition of the new panel E. We also moved Table 1 from the Supplementary to the main text to highlight this quantitatively.</p><disp-quote content-type="editor-comment"><p>(2) It's mentioned in the introduction (line 84) and elsewhere (e.g., line 259) that spiking has some advantage, but I don't see any figure supporting this claim. In fact, spiking seems not to matter (Figure 2C, E). Please clarify how spiking improves performance, and if it does not, acknowledge that. Relatedly, in line 246, the authors state that 'spiking is a better metric but not significant' when discussing simulations. Either remove this statement and assume spiking is not relevant, or increase the number of simulations.</p></disp-quote><p>We could not find the exact quote from the reviewer, and we believe that he intended to quote “spiking is better on all metrics, but without significant margins”. Indeed, spiking did not improve the fit significantly on perturbed trials, this is particularly true in comparison with the benefits of Dale’s law and local inhibition. As suggested by the reviewer, we rephrased the sentence from this quote and more generally the corresponding paragraphs in the intro (lines 83-87) and in the results (lines 245-271). Our corrections in the results sections are also intended to address the minor point (4) raised by the same reviewer.</p><disp-quote content-type="editor-comment"><p>(3) The authors prefer the metric of predicting hits over MSE, especially when looking at real data (Figure 3). I would bring the supplementary results into the main figures, as both metrics are very nicely complementary. Relatedly, why not add Pearson correlation or R2, and not just focus on MSE Loss?</p></disp-quote><p>In Figure 3 for the in-vivo data, we do not have simultaneous electrophysiological recordings and optogenetic stimulation in this dataset. The two are performed on different recording sessions. Therefore, we can only compare the effect of optogenetics on the behavior, and we cannot compute Pearson correlation or R2 of the perturbed network activity. To avoid ambiguity, we wrote “For the sessions of the in vivo dataset with optogenetic perturbation that we considered, only the behavior of an animal is recorded” on line 294.</p><disp-quote content-type="editor-comment"><p>(4) I really like the 'forward-looking' experiment in closed loop! But I felt that the relevance of micro perturbations is very unclear in the intro and results. This could be better motivated: why should an experimentalist care about this forward-looking experiment? Why exactly do we care about micro perturbation (e.g., in contrast to non-micro perturbation)? Relatedly, I would try to explain this in the intro without resorting to technical jargon like 'gradients'.</p></disp-quote><p>As suggested, we updated the last paragraph of the introduction (lines 88 - 95) to give better motivation for why algorithmically targeted acute spatio-temporal perturbations can be important to dissect the function of neural circuits. We also added citations to recent studies with targeted in vivo optogenetic stimulation. As far as we know the existing previous work targeted network stimulation mostly using linear models, while we used non-linear RNNs and their gradients.</p><disp-quote content-type="editor-comment"><p>Minor:</p><p>(1) In the intro, the authors refer to 'the field' twice. Personally, I find this term odd. I would opt for something like 'in neuroscience'.</p></disp-quote><p>We implemented the suggested change: l.27 and l.30</p><disp-quote content-type="editor-comment"><p>(2) Line 45: When referring to previous work using data-constrained RNN models, Valente et al. is missing (though it is well cited later when discussing regularization through low-rank constraints)</p></disp-quote><p>We added the citation: l.45</p><disp-quote content-type="editor-comment"><p>(3) Line 11: Method should be methods (missing an 's').</p></disp-quote><p>We fixed the typo.</p><disp-quote content-type="editor-comment"><p>(4) In line 250, starting with 'So far', is a strange choice of presentation order. After interpreting the results for other biological ingredients, the authors introduce a new one. I would first introduce all ingredients and then interpret. It's telling that the authors jump back to 2B after discussing 2C.</p></disp-quote><p>We restructured the last two paragraphs of section 2.1, and we hope that the presentation order is now more logical.</p><disp-quote content-type="editor-comment"><p>(5) The black dots in Figure 3E are not explained, or at least I couldn't find an explanation.</p></disp-quote><p>We added an explanation in the caption of Figure 3E.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>(1) Some aspects of the methods are unclear. For comparisons between recurrent networks trained from randomly initialized weights, I would expect that many initializations were made for each model variant to be compared, and that the performance characteristics are constructed by aggregating over networks trained from multiple random initializations. I could not tell from the methods whether this was done or how many models were aggregated.</p></disp-quote><p>The expectation of the reviewer is correct, we trained multiple models with different random seeds (affecting both the weight initialization and the noise of our model) for each variant and aggregated the results. We have now clarified this in Methods 4.6. lines 658-662.</p><disp-quote content-type="editor-comment"><p>(2) It is possible that including perturbation trials in the training sets would improve model performance across conditions, including held-out (untrained) perturbations (for instance, to units that had not been perturbed during training). It could be noted that if perturbations are available, their use may alleviate some of the design decisions that are evaluated here.</p></disp-quote><p>In general, we agree with the reviewer that including perturbation trials in the training set would likely improve model performance across conditions. One practical limitation explaining partially why we did not do it with our dataset is the small quantity of perturbed trials for each targeted cortical area: the number of trials with light perturbations is too scarce to robustly train and test our models.</p><p>More profoundly, to test hard generalizations to perturbations (aka perturbation testing), it will always be necessary that the perturbations are not trivially represented in the training data. Including perturbation trials during training would compromise our main finding: some biological model constraints improve the generalization to perturbation. To test this claim, it was necessary to keep the perturbations out of the training data.</p><p>We agree that including all available data of perturbed and non-perturbed recordings would be useful to build the best generalist predictive system. It could help, for instance, for closed-loop circuit control as we studied in Figure 5. Yet, there too, it will be important for the scientific validation process to always keep some causal perturbations of interest out of the training set. This is necessary to fairly measure the real generalization capability of any model. Importantly, this is why we think out-of-distribution “perturbation testing” is likely to have a recurring impact in the years to come, even beyond the case of optogenetic inactivation studied in detail in our paper.</p><disp-quote content-type="editor-comment"><p><bold>Recommendation for the authors:</bold></p><p><bold>Reviewer #1 (Recommendation for the authors):</bold></p><p>The code is not very easy to follow. I know this is a lot to ask, but maybe make clear where the code is to train the different models, which I think is a great contribution of this work? I predict that many readers will want to use the code and so this will improve the impact of this work.</p></disp-quote><p>We updated the code to make it easier to train a model from scratch.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendation for the authors):</bold></p><p>The figures are really tough to read. Some of that small font should be sized up, and it's tough to tell in the posted paper what's happening in Figure 2B.</p></disp-quote><p>We updated Figures 1 and 2 significantly, in part to increase their readability. We also implemented the &quot;Superficialities&quot; suggestions.</p></body></sub-article></article>