<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">97765</article-id><article-id pub-id-type="doi">10.7554/eLife.97765</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.97765.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Precision-based causal inference modulates audiovisual temporal recalibration</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Li</surname><given-names>Luhe</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0182-3952</contrib-id><email>luhe.li@nyu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Hong</surname><given-names>Fangfang</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1890-1977</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Badde</surname><given-names>Stephanie</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4005-5503</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Landy</surname><given-names>Michael S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2079-4552</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Department of Psychology, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>Department of Psychology, University of Pennsylvania</institution></institution-wrap><addr-line><named-content content-type="city">Philadelphia</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05wvpxv85</institution-id><institution>Department of Psychology, Tufts University</institution></institution-wrap><addr-line><named-content content-type="city">Medford</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Center for Neural Science, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Chait</surname><given-names>Maria</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>25</day><month>02</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP97765</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-04-15"><day>15</day><month>04</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-03-11"><day>11</day><month>03</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.03.08.584189"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-06-24"><day>24</day><month>06</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.97765.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-15"><day>15</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.97765.2"/></event></pub-history><permissions><copyright-statement>© 2024, Li et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Li et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-97765-v1.pdf"/><abstract><p>Cross-modal temporal recalibration guarantees stable temporal perception across ever-changing environments. Yet, the mechanisms of cross-modal temporal recalibration remain unknown. Here, we conducted an experiment to measure how participants’ temporal perception was affected by exposure to audiovisual stimuli with constant temporal delays that we varied across sessions. Consistent with previous findings, recalibration effects plateaued with increasing audiovisual asynchrony (nonlinearity) and varied by which modality led during the exposure phase (asymmetry). We compared six observer models that differed in how they update the audiovisual temporal bias during the exposure phase and in whether they assume a modality-specific or modality-independent precision of arrival latency. The causal-inference observer shifts the audiovisual temporal bias to compensate for perceived asynchrony, which is inferred by considering two causal scenarios: when the audiovisual stimuli have a common cause or separate causes. The asynchrony-contingent observer updates the bias to achieve simultaneity of auditory and visual measurements, modulating the update rate by the likelihood of the audiovisual stimuli originating from a simultaneous event. In the asynchrony-correction model, the observer first assesses whether the sensory measurement is asynchronous; if so, she adjusts the bias proportionally to the magnitude of the measured asynchrony. Each model was paired with either modality-specific or modality-independent precision of arrival latency. A Bayesian model comparison revealed that both the causal-inference process and modality-specific precision in arrival latency are required to capture the nonlinearity and asymmetry observed in audiovisual temporal recalibration. Our findings support the hypothesis that audiovisual temporal recalibration relies on the same causal-inference processes that govern cross-modal perception.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>audiovisual temporal recalibration</kwd><kwd>causal inference</kwd><kwd>Bayesian modeling</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>EY08266</award-id><principal-award-recipient><name><surname>Landy</surname><given-names>Michael S</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Audiovisual temporal recalibration relies on the same causal-inference processes that govern cross-modal perception.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Perception is not rigid but rather can adapt to the environment. In a multimodal environment, misalignment across the senses can occur because signals in different modalities may arrive with different physical and neural delays in the relevant brain areas (<xref ref-type="bibr" rid="bib12">Fain, 2019</xref>; <xref ref-type="bibr" rid="bib4">Badde et al., 2020a</xref>; <xref ref-type="bibr" rid="bib38">Pöppel, 1988</xref>; <xref ref-type="bibr" rid="bib52">Spence and Squire, 2003</xref>). Perceptual misalignment can also arise from changes in the environment, such as when wearing a virtual-reality headset or adapting to hearing aids. Cross-modal temporal recalibration serves as a critical mechanism to maintain perceptual synchrony despite changes in the perceptual systems and the environment (reviewed in <xref ref-type="bibr" rid="bib27">King, 2005</xref>; <xref ref-type="bibr" rid="bib59">Vroomen and Keetels, 2010</xref>). This phenomenon is exemplified in audiovisual temporal recalibration, where exposure to a consistent audiovisual stimulus-onset asynchrony (SOA) shifts the point of subjective simultaneity (PSS) between auditory and visual stimuli; as a result, stimuli at first perceived as temporally discrepant are gradually perceived as more synchronous (<xref ref-type="bibr" rid="bib11">Di Luca et al., 2009</xref>; <xref ref-type="bibr" rid="bib13">Fujisaki et al., 2004</xref>; <xref ref-type="bibr" rid="bib16">Hanson et al., 2008</xref>; <xref ref-type="bibr" rid="bib18">Harrar and Harris, 2008</xref>; <xref ref-type="bibr" rid="bib19">Heron et al., 2007</xref>; <xref ref-type="bibr" rid="bib26">Keetels and Vroomen, 2007</xref>; <xref ref-type="bibr" rid="bib34">Navarra et al., 2005</xref>; <xref ref-type="bibr" rid="bib39">Roach et al., 2011</xref>; <xref ref-type="bibr" rid="bib54">Tanaka et al., 2011</xref>; <xref ref-type="bibr" rid="bib56">Vatakis et al., 2007</xref>; <xref ref-type="bibr" rid="bib57">Vatakis et al., 2008</xref>; <xref ref-type="bibr" rid="bib58">Vroomen et al., 2004</xref>; <xref ref-type="bibr" rid="bib59">Vroomen and Keetels, 2010</xref>).</p><p>However, the mechanisms of cross-modal temporal recalibration remain unknown. Current models of audiovisual temporal recalibration either did not specify the recalibration process (<xref ref-type="bibr" rid="bib11">Di Luca et al., 2009</xref> ; <xref ref-type="bibr" rid="bib35">Navarra et al., 2009</xref>; <xref ref-type="bibr" rid="bib64">Yarrow et al., 2015</xref>), or did not capture all characteristics of recalibration effects (<xref ref-type="bibr" rid="bib39">Roach et al., 2011</xref>; <xref ref-type="bibr" rid="bib47">Sato and Aihara, 2011</xref>; <xref ref-type="bibr" rid="bib64">Yarrow et al., 2015</xref>). Specifically, audiovisual temporal recalibration shows two distinct characteristics: the amount of recalibration is nonlinear and asymmetric as a function of the SOA participants are adapted to (adapter SOA). The amount of recalibration is not a linear function of the adapter SOA, but instead plateaus at an SOA of approximately 100–300 ms (<xref ref-type="bibr" rid="bib13">Fujisaki et al., 2004</xref>; <xref ref-type="bibr" rid="bib58">Vroomen et al., 2004</xref>). Recalibration can also be asymmetrical: its magnitude differs depending on whether the visual or auditory stimulus leads during the exposure phase (<xref ref-type="bibr" rid="bib13">Fujisaki et al., 2004</xref>; <xref ref-type="bibr" rid="bib36">O’Donohue et al., 2022</xref>; <xref ref-type="bibr" rid="bib55">Van der Burg et al., 2013</xref>).</p><p>Here, we propose a causal-inference model to explain the mechanism of audiovisual temporal recalibration. Causal inference is the process by which the observer determines whether multisensory signals originate from a common source and adjust their perceptual estimates accordingly (<xref ref-type="bibr" rid="bib46">Sato et al., 2007</xref>; <xref ref-type="bibr" rid="bib51">Shams and Beierholm, 2010</xref>; <xref ref-type="bibr" rid="bib60">Wei and Körding, 2009</xref>). Bayesian models based on causal inference have been proposed to explain multisensory integration effects (<xref ref-type="bibr" rid="bib28">Körding et al., 2007</xref>; <xref ref-type="bibr" rid="bib46">Sato et al., 2007</xref>), and these models have been empirically validated in studies of spatial audiovisual and visual-tactile integration (<xref ref-type="bibr" rid="bib5">Badde et al., 2020b</xref>; <xref ref-type="bibr" rid="bib7">Beierholm et al., 2009</xref>; <xref ref-type="bibr" rid="bib42">Rohe and Noppeney, 2015</xref>; <xref ref-type="bibr" rid="bib61">Wozny et al., 2010</xref>). In the temporal domain, some studies have successfully used causal inference to model the integration of cross-modal relative timing, accurately predicting simultaneity judgments in audiovisual speech (<xref ref-type="bibr" rid="bib31">Magnotti et al., 2013</xref>) and more complex scenarios involving one auditory and two visual stimuli (<xref ref-type="bibr" rid="bib48">Sato, 2021</xref>).</p><p>In the context of cross-modal recalibration, causal inference is expected to play a role based on the intuition that the amount of recalibration should be reduced when the multisensory signals are not perceived as causally related (<xref ref-type="bibr" rid="bib13">Fujisaki et al., 2004</xref>; <xref ref-type="bibr" rid="bib24">Hsiao et al., 2022</xref>; <xref ref-type="bibr" rid="bib58">Vroomen et al., 2004</xref>). Supporting this, causal-inference models successfully predicted cross-modal spatial recalibration of visual-auditory (<xref ref-type="bibr" rid="bib23">Hong, 2023</xref>; <xref ref-type="bibr" rid="bib22">Hong et al., 2021</xref>; <xref ref-type="bibr" rid="bib46">Sato et al., 2007</xref>) and visuo-tactile (<xref ref-type="bibr" rid="bib5">Badde et al., 2020b</xref>) signals. Building on this framework, here, we propose a causal-inference model for cross-modal temporal recalibration that derives the multisensory percept based on inferences about the shared origin of the signals and updates the cross-modal temporal biases such that subsequent measurements are shifted toward the percept.</p><p>The first aim of this study is to test whether causal inference is necessary to explain the nonlinearity of audiovisual temporal recalibration across different adapter SOAs. To this aim, we compared the causal-inference model with two alternatives: an asynchrony-contingent model and an asynchrony-correction model. The asynchrony-contingent model scales the amount of recalibration by the likelihood that the sensory measurement of SOA was caused by a synchronous audiovisual stimulus pair. The model predicts a nonlinear recalibration effect across adapter SOAs without requiring observers to perform full Bayesian inference. The asynchrony-correction model assumes that recalibration only occurs when asynchronous onsets between the cross-modal stimuli are registered, followed by the update of the cross-modal temporal bias to compensate for this SOA measurement. This account is based on the intuitive rationale that repeated measurements of asynchrony can prompt the perceptual system to restore coherence. Unlike the causal-inference and the asynchrony-contingent models, this model predicts minimal recalibration when the adapter SOA falls within the range of measured asynchronies that can arise with simultaneously presented stimuli due to sensory noise. This model serves as the baseline for model comparison.</p><p>The second aim was to examine factors that had the potential to drive the asymmetry of recalibration across visual-leading and auditory-leading adapter SOAs. It has been suggested that the asymmetry may be explained by physical and neural latency differences between signals (<xref ref-type="bibr" rid="bib36">O’Donohue et al., 2022</xref>; <xref ref-type="bibr" rid="bib55">Van der Burg et al., 2013</xref>). These latency differences can vary significantly based on the physical distance between the stimulus and the sensors, as well as the neural transmission time required for the signal to reach the relevant sensory region (<xref ref-type="bibr" rid="bib21">Hirsh and Sherrick, 1961</xref>; <xref ref-type="bibr" rid="bib27">King, 2005</xref>). Yet–speaking against a merely physical origin–these latency differences vary with sensory experience in infancy (<xref ref-type="bibr" rid="bib4">Badde et al., 2020a</xref>). While these latency differences can explain the audiovisual temporal bias observed in most humans, they would affect recalibration to different adapter SOAs equally, making it unlikely for any asymmetry to arise. In contrast to latency differences, sensory uncertainty has been shown to affect the degree of cross-modal recalibration in a complex fashion (<xref ref-type="bibr" rid="bib5">Badde et al., 2020b</xref>; <xref ref-type="bibr" rid="bib22">Hong et al., 2021</xref>). We hypothesized that the difference across modalities in the precision of the arrival times, the time it takes visual and auditory signals to arrive in the relevant brain areas, plays a critical role in the asymmetry of cross-modal temporal recalibration.</p><p>To examine the mechanism underlying audiovisual temporal recalibration, we manipulated the adapter SOA across sessions, introducing asynchronies up to 0.7 seconds of either auditory or visual lead. Before and after the exposure phase in each session, we measured participants’ perception of audiovisual relative timing using a ternary temporal-order judgment (TOJ) task. To preview the empirical results, we confirmed the nonlinearity of the recalibration effect: recalibration magnitude increased linearly for short adapter SOAs, but then reached an asymptote or even decreased with increasing adapter SOAs. Furthermore, participants showed idiosyncratic asymmetries of the recalibration effect across modalities; for most participants, the amount of recalibration was larger when the auditory stimulus led than when it lagged, but the opposite was found for other participants. To scrutinize the factors that might drive the nonlinearity and asymmetry of audiovisual temporal recalibration, we fitted six models to the TOJ data. These models determined the amount of recalibration based on either perceptual causal-inference processes, a heuristic evaluation of the common cause of the audiovisual stimuli, or a fixed criterion for the need to correct asynchrony. For each of these three models, we implemented either modality-specific or modality-independent precision of the arrival times. The model comparison revealed that the assumptions of Bayesian causal inference combined with modality-specific precision are essential to accurately capture the nonlinearity and idiosyncratic asymmetry of audiovisual temporal recalibration.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavioral results</title><p>We adopted a classical three-phase recalibration paradigm in which participants completed a pre-test, an exposure phase, and a post-test in each session. In pre- and post-tests, we measured participants’ perception of audiovisual relative timing using a ternary TOJ task: participants reported the perceived order (“visual first”, “auditory first”, or “simultaneous”) of audiovisual stimulus pairs with varying SOA (range: from –0.5 to 0.5 s with 15 levels; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). In the exposure phase, we induced temporal recalibration. Specifically, participants were exposed to a series of audiovisual stimuli with a consistent SOA (250 trials; <xref ref-type="fig" rid="fig1">Figure 1B</xref>). To ensure that participants were attentive to the stimuli, we inserted oddball stimuli with greater intensity in either one or both modalities (5% of the total trials independently sampled for each modality). Participants were instructed to press a key corresponding to the auditory, visual, or both oddballs whenever an oddball stimulus appeared. The high <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> of oddball-detection performance (auditory <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> = 3.34 ± 0.54, visual <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> = 2.44 ± 0.72) indicates that participants paid attention to both modalities. The post-test was almost identical to the pre-test, except that before every TOJ trial, there were three top-up oddball-detection trials to maintain the recalibration effect. In total, participants completed nine sessions on separate days. The adapter SOA (range: –0.7 to 0.7 s) was fixed within each session, but varied randomly across sessions and in random order across participants.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Task timing.</title><p>(<bold>A</bold>) Temporal-order judgment task administered in the pre- and post-tests. In each trial, participants made a temporal-order judgment in response to an audiovisual stimulus pair with a varying stimulus-onset asynchrony (SOA). Negative values: auditory lead; positive values: visual lead. The contrast of the visual stimulus was enhanced for illustration purposes. (<bold>B</bold>) Oddball-detection task performed in the exposure phase and top-up trials during the post-test phase. Participants were repeatedly presented with an audiovisual stimulus pair with an SOA that was fixed within each session but varied across sessions. Occasionally, the intensity of either one or both of the stimuli was increased. Participants were instructed to, whenever an oddball stimulus appeared, press a key indicating the modality of the oddball (visual, auditory, or both) .</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-fig1-v1.tif"/></fig><p>We compared the TOJs between the pre- and post-tests to examine the amount of audiovisual temporal recalibration induced by the audiovisual stimuli during the exposure phase. Specifically, we fitted the data from the pre- and post-tests jointly, allowing the curves to shift between the two tests while assuming the same shape for the psychometric functions that is determined by the relative arrival latencies, their precision, and fixed response criteria (<xref ref-type="fig" rid="fig2">Figure 2A</xref>; see Appendix 1 for the formalization of the atheoretical model and an alternative model assuming a shift in the response criteria due to recalibration). The point of subjective simultaneity (PSS) is the physical SOA that corresponds to the maximum probability of reporting simultaneity (<xref ref-type="bibr" rid="bib53">Sternberg and Knoll, 1973</xref>). The amount of audiovisual temporal recalibration was defined as the difference between the pre- and post-test PSSs. At the group level, we observed a nonlinear pattern of recalibration as a function of the adapter SOA: the amount of recalibration in the direction of the adapter SOA first increased but then plateaued with increasing magnitude of the adapter SOA (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Additionally, we observed an asymmetry in the amount of recalibration between auditory-leading and visual-leading adapter SOAs, with auditory-leading adapter SOAs inducing a greater amount of recalibration (<xref ref-type="fig" rid="fig2">Figure 2B</xref>; see <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1A</xref> for individual participants’ data). To quantify this asymmetry for each participant, we calculated an asymmetry index, defined as the sum of the recalibration effects across all adapter SOAs (zero: no evidence for asymmetry; positive values: greater recalibration given visual-lead adapter SOAs; negative: greater recalibration given auditory-lead adapter SOAs). For each participant, we bootstrapped the TOJ responses to obtain a 95% confidence interval for the asymmetry index. Eight out of nine participants showed an asymmetry index significantly different from zero, with the majority showing greater recalibration for auditory-leading adapter SOAs, suggesting a general asymmetry in recalibration (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1B</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Behavioral results.</title><p>(<bold>A</bold>) The probability of reporting that the auditory stimulus came first (blue), the two arrived at the same time (green), or the visual stimulus came first (red) as a function of test stimulus-onset asynchrony (SOA) for a representative participant in a single session. The adapter SOA was –0.3 s for this session. Curves: best-fitting psychometric functions estimated jointly using the data from the pre-test (dashed) and post-test (solid). Shaded areas: 95% bootstrapped confidence intervals. (<bold>B</bold>) Mean recalibration effects averaged across all participants as a function of adapter SOA. The recalibration effects are defined as the shifts in the point of subjective simultaneity (PSS) from the pre- to the post-test, where the PSS is the physical SOA at which the probability of reporting simultaneity is maximized. Error bars: ± SEM.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-fig2-v1.tif"/></fig></sec><sec id="s2-2"><title>Modeling results</title><p>In the following sections, we describe our models for cross-modal temporal recalibration by first laying out the general assumptions of these models, and then elaborating on the differences between them. Then, we compare the models’ ability to capture the observed data.</p><sec id="s2-2-1"><title>General model assumptions</title><p>We formulated six process models of cross-modal temporal recalibration (<xref ref-type="fig" rid="fig3">Figure 3</xref>). These models share several assumptions about audiovisual temporal perception and recalibration that we selected based on a comparison of atheoretical, descriptive models of TOJ data (Appendix 1). First, when an auditory and a visual signal are presented, the corresponding neural signals arrive in the relevant brain areas with a variable latency due to internal and external noise. We assume arrival times for the two modalities are independent and that the arrival latencies are exponentially distributed (<xref ref-type="bibr" rid="bib14">García-Pérez and Alcalá-Quintana, 2012</xref>; <xref ref-type="fig" rid="fig3">Figure 3A</xref>, left panel). Moreover, we assume a constant offset between auditory and visual arrival times, reflecting an audiovisual temporal bias. A simple derivation shows that the resulting measurement of SOA has a double-exponential distribution (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, right panel; see derivation in Appendix 3). The probability density function peaks at an SOA that is the physical SOA of the stimuli plus the observer’s audiovisual temporal bias. The slopes of the measurement distribution reflect the precision of the arrival times; the steeper the slope, the more precise the measured latency. When the precision differs between modalities, the measurement distribution of the SOA between the auditory and visual stimuli is asymmetrical (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Illustration of the six observer models of cross-modal temporal recalibration.</title><p>(<bold>A</bold>) Left: Arrival-latency distributions for auditory (cyan) and visual (pink) sensory signals. When the precision of arrival latency is modality-independent, these two exponential distributions have identical shape. Right: The resulting symmetrical double-exponential measurement distribution of the stimulus-onset asynchrony (SOA) of the stimuli. (<bold>B</bold>) When the precision of the arrival latencies is modality-specific, the arrival-latency distributions for auditory and visual signals have different shapes, and the resulting measurement distribution of the SOA is asymmetrical. (<bold>C</bold>) Bias update rules and predicted recalibration effects for the three contrasted recalibration models: The causal-inference model updates the audiovisual bias based on the difference between the estimated and measured SOA. The asynchrony-contingent model updates the audiovisual bias by a proportion of the measured SOA and modulates the update rate by the likelihood that the measured sensory signals originated from a simultaneous audiovisual pair. The asynchrony-correction model adjusts the audiovisual bias by a proportion of the measured SOA when this measurement exceeds fixed criteria for simultaneity.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-fig3-v1.tif"/></fig><p>Second, these models define temporal recalibration as the accumulation of updates to the audiovisual temporal bias after each encounter with an SOA. The accumulated shift in the audiovisual bias at the end of the exposure phase is then carried over to the post-test phase and persists throughout. Lastly, the bias is assumed to be reset to the same initial value in the pre-test across all nine sessions, reflecting the stability of the audiovisual temporal bias over time (<xref ref-type="bibr" rid="bib15">Grabot and van Wassenhove, 2017</xref>).</p></sec><sec id="s2-2-2"><title>Models of cross-modal temporal recalibration</title><p>The six models we tested differed in the mechanisms governing the updates of the audiovisual bias during the exposure phase as well as the modality specificity of arrival time precision.</p><p>We formulated a temporal variant of the spatial Bayesian causal-inference model of recalibration (<xref ref-type="bibr" rid="bib5">Badde et al., 2020b</xref>; <xref ref-type="bibr" rid="bib23">Hong, 2023</xref>; <xref ref-type="bibr" rid="bib22">Hong et al., 2021</xref>; <xref ref-type="bibr" rid="bib46">Sato et al., 2007</xref>) to describe the recalibration of the relative timing between cross-modal stimuli (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, left panel). In this model, when an observer is presented with an audiovisual stimulus pair during the exposure phase, they compute two intermediate estimates of the SOA between the stimuli, one for the common-cause scenario and the other for the separate-causes scenario. In the common-cause scenario, the estimated SOA of the stimuli is smaller than the measured SOA as it is combined with a prior distribution over SOA that reflects simultaneity. In the separate-causes scenario, the estimated SOA is approximately equal to the measured SOA. The two estimates are then averaged with each one weighted by the posterior probability of the corresponding causal scenario. The audiovisual bias is then updated to reduce the difference between the measured SOA and the combined estimate of the SOA. In other words, causal inference regulates the recalibration process by shifting the measured SOA to more closely match the percept, which in turn is computed based on the inferred causal structure.</p><p>The asynchrony-contingent model assumes that the observer estimates the likelihood that the sensory signals originated from a simultaneous audiovisual pair and updates the audiovisual bias to compensate for the measured SOA but the degree of compensation is scaled by this likelihood (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, middle panel). There is a key distinction between the likelihood of simultaneity and the posterior probability of a common cause. The posterior probability of a common cause incorporates the prior distribution of SOAs when signals originate from the same source, including non-zero probabilities for SOAs ≠ 0. In contrast, the likelihood of simultaneity exclusively considers the case when SOA = 0. We assume that asynchrony-contingent observer computes the likelihood of simultaneity based on the knowledge of the double-exponential measurement distribution, instead of assuming a Gaussian measurement distribution as was done previously (<xref ref-type="bibr" rid="bib32">Maij et al., 2009</xref>). The update rate of the audiovisual bias is proportional to this likelihood. For a stimulus pair with a large SOA, the average likelihood of the stimuli being physically simultaneous decreases, leading to reduced recalibration effects compared to stimulus pairs with smaller SOAs. Thus, this asynchrony-contingent model is capable of replicating the nonlinearity of recalibration across adapter SOAs without requiring the observer to perform full Bayesian inference.</p><p>The asynchrony-correction model assumes that the observer first compares the sensory measurement of SOA to their criteria for audiovisual simultaneity to decide whether to recalibrate in a given trial. If the measured SOA falls within the range perceived as simultaneous according to the fixed criteria, the observer might attribute a non-zero measurement of SOA to sensory noise and omit recalibration. On the other hand, if the measured SOA exceeds this range, the observer perceives the stimuli as asynchronous, and shifts the audiovisual bias by a proportion of the measurement of SOA (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, right panel). This model serves as a direct contrast to the causal-inference model, as it predicts an opposite pattern: a nonlinear but monotonic increase in the amount of temporal recalibration, with minimal recalibration when the measured SOA falls within the simultaneity range and increasing recalibration as the measured SOA moves further outside of this range.</p><p>We additionally assumed either modality-specific or modality-independent precision of the arrival times. Each choice suggests a different origin of the variability. The variability in arrival times is either modality specific, limited by neural-latency noise in each sensory channel (<xref ref-type="bibr" rid="bib65">Yarrow et al., 2022</xref>), or modality independent, resulting from variabilities in a central timing mechanism (<xref ref-type="bibr" rid="bib21">Hirsh and Sherrick, 1961</xref>).</p></sec><sec id="s2-2-3"><title>Model fitting and model comparison</title><p>We fit six models to each participant’s data. Each model was constrained jointly by the temporal-order judgments from the pre- and post-tests of all nine sessions. To quantify model performance, we calculated model evidence, i.e., the likelihood of each model given the data marginalized over all possible parameters, which revealed that the causal-inference model had the strongest model evidence at the group level and best fit the data of most participants, followed by the asynchrony-contingent model and then the asynchrony-correction model. To quantify the differences between model performance, we performed a Bayesian model comparison by computing the Bayes factor for each model relative to the worst-performing model, the asynchrony-correction model with modality-independent arrival-latency precision (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, see <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref> for individual-level model comparison). Within each of these three model categories, the version incorporating modality-specific precision consistently outperformed the modality-independent version.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Model comparison and predictions.</title><p>(<bold>A</bold>) Model comparison based on model evidence. Each bar represents the group-averaged log Bayes factor of each model relative to the asynchrony-correction, modality-independent-precision model, which had the weakest model evidence. (<bold>B</bold>) Empirical data (points) and model predictions (lines and shaded regions) for the recalibration effect as a function of adapter stimulus-onset asynchrony (SOA).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-fig4-v1.tif"/></fig></sec><sec id="s2-2-4"><title>Model prediction</title><p>To inspect the quality of the model fit, for every model, we used the best-fitting parameter estimates for each participant to predict the group-average recalibration effect as a function of adapter SOA (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). The nonlinearity of audiovisual temporal recalibration across adapter SOAs was captured by both the asynchrony-contingent and causal-inference models. Nonetheless, the causal-inference model outperformed the asynchrony-contingent model by accurately predicting a non-zero average recalibration effect at adapter SOAs of 0.7 and –0.7 s, where the asynchrony-contingent model predicted no recalibration. Additionally, incorporating modality-specific precision enabled both the asynchrony-contingent and causal-inference models to more accurately predict increased recalibration when the adapter SOA was auditory-leading. Overall, the model that relies on causal inference during the exposure phase and assumes modality-specific precision of arrival times most accurately captured both the nonlinearity and asymmetry of the recalibration effect. This model could also account for individual participants’ idiosyncratic asymmetry in temporal recalibration to auditory- and visual-leading adapter SOAs (see <xref ref-type="fig" rid="app5fig1">Appendix 5—figures 1</xref>–<xref ref-type="fig" rid="app5fig2">3</xref> for predictions of individual participants’ recalibration effects of all models; see <xref ref-type="fig" rid="app6fig1">Appendix 6—figures 1</xref>–<xref ref-type="fig" rid="app6fig3">3</xref> for predictions of individual participants’ TOJ responses using the causal-inference models with modality-specific precision).</p></sec><sec id="s2-2-5"><title>Model simulation</title><p>Simulations with the causal-inference model revealed which factors of the modeled recalibration process determine the degree of nonlinearity and asymmetry of cross-modal temporal recalibration to different adapter SOAs. The prior belief that the auditory and visual stimuli share a common cause plays a crucial role in adjudicating the relative influence of the two causal scenarios (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). When the observer has a prior belief that audiovisual stimuli always originate from the same source, they recalibrate by a proportion of the perceived SOA no matter how large the measured SOA is, mirroring the behavior of the asynchrony-correction model when its criteria for simultaneity are such that no stimuli are treated as simultaneous. On the other hand, when the observer believes that the audiovisual stimuli always have separate causes, they treat the audiovisual stimuli as independent of each other and do not recalibrate. Estimates of the common-cause prior for our participants fall between the two extreme beliefs, resulting in the nonlinear pattern of recalibration that lies between the extremes of no recalibration and the proportional recalibration effects as a function of the adapter SOA (see <xref ref-type="table" rid="app6table1">Appendix 6—table 1</xref> for parameter estimates for individual participants).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Simulation of temporal recalibration using the causal-inference model.</title><p>(<bold>A</bold>) The influence of the observer's prior belief of a common cause: the stronger the prior, the larger the recalibration effects. (<bold>B</bold>) The influence of latency noise: recalibration effects increase with decreasing sensory precision (i.e. increasing latency noise captured by the exponential time constant) of both modalities. (<bold>C</bold>) The influence of auditory/visual latency noise: recalibration effects are asymmetric between auditory-leading and visual-leading adapter stimulus-onset asynchronies (SOAs) due to differences in the precision of auditory and visual arrival latencies. Left panel: Increasing auditory latency precision (i.e. reducing auditory latency noise) reduces recalibration in response to visual-leading adapter SOAs. Right panel: Increasing visual precision (i.e. reducing visual latency noise) reduces recalibration in response to auditory-leading adapter SOAs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-fig5-v1.tif"/></fig><p>Simulations also identified key model elements of the causal-inference model that predict a non-zero recalibration effect even at large SOAs, a feature that distinguishes the causal-inference from the asynchrony-contingent model. This non-zero recalibration effect for large adapter SOAs can be replicated by either assuming a strong prior for a common cause (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) or by assuming low sensory precision of arrival times (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Both relationships are intuitive: observers with a stronger prior belief in a common cause and ideal observers with lower sensory precision are more likely to assign a higher posterior probability to the common-cause scenario, leading to greater recalibration. A decrease of the spread of the prior distribution over SOA conditioned on a common cause increases the recalibration magnitude, but only over a small range of SOAs for which there is a higher probability of the common-cause scenario (<xref ref-type="fig" rid="app7fig1">Appendix 7—figure 1A</xref>), and thus cannot account for non-zero recalibration for large SOAs.</p><p>Differences in arrival-time precision between audition and vision result in an asymmetry of audiovisual temporal recalibration across adapter SOAs (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). The amount of recalibration is attenuated when the modality with the higher precision lags the less precise one during the exposure phase. When the more recent stimulus component in a cross-modal pair is more precise, the observer is more likely to attribute the asynchrony to separate causes and thus recalibrate less. In addition, the fixed audiovisual bias does not affect asymmetry, but shifts the recalibration function laterally and determines the adapter SOA for which no recalibration occurs (<xref ref-type="fig" rid="app7fig1">Appendix 7—figure 1B</xref>).</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>This study scrutinized the mechanism underlying audiovisual temporal recalibration. We measured the effects of exposure to audiovisual stimulus pairs with a constant temporal offset (adapter SOA) on audiovisual temporal-order perception across a wide range of adapter SOAs. Recalibration effects changed nonlinearly with the magnitude of adapter SOAs and were asymmetric across auditory-leading and visual-leading adapter SOAs. We then compared the predictions of different observer models for the amount of recalibration as a function of adapter SOA. A Bayesian causal-inference model with modality-specific precision of the arrival latencies fit the observed data best. These findings suggest that human observers rely on causal-inference-based percepts to recalibrate cross-modal temporal perception. These results align closely with studies that have demonstrated the role of causal inference in audiovisual (<xref ref-type="bibr" rid="bib22">Hong et al., 2021</xref>) and visual-tactile spatial recalibration (<xref ref-type="bibr" rid="bib5">Badde et al., 2020b</xref>). Our results are also consistent with previous recalibration models that assumed a strong relation between perception and recalibration (<xref ref-type="bibr" rid="bib47">Sato and Aihara, 2011</xref>; <xref ref-type="bibr" rid="bib46">Sato et al., 2007</xref>). Hence, we suggest that the same mechanisms underlie cross-modal perception and recalibration across different sensory features.</p><p>The observed recalibration results could not be predicted by the asynchrony-contingent model that employed a heuristic approximation of the causal-inference process. Even though this model was capable of predicting a nonlinear relationship between the recalibration effect and the adapter SOA, it failed to capture a non-zero recalibration effect at large adapter SOAs, as observed in several participants. This is becausethis model uses the likelihood of a synchronous audiovisual stimulus pair given the measured SOA to modulate the update rate of audiovisual bias, which will be very small on average for large SOAs. Therefore, the model predicts little to no recalibration at large adapter SOAs. In contrast, the causal-inference model can capture the non-zero recalibration effect because the common-cause scenario always influences the amount of recalibration even when the adapter SOA is too large to be perceived as synchronous. Simulation (<xref ref-type="fig" rid="fig5">Figure 5A and B</xref>) shows that a strong prior belief in a common cause or less precision of arrival times can result in non-zero recalibration effects following exposure to clearly asynchronous stimulus pairs. Notably, even though it might at first seem counter-intuitive that cross-modal temporal recalibration can be elicited by clearly asynchronous streams of sensory information, many of us have experienced this effect during laggy, long video conferences.</p><p>The asynchrony-correction model assumes that observers recalibrate to restore temporal synchrony whenever the SOA measurement indicates a temporal discrepancy, but this model predicts recalibration effects across adapter SOAs that are contrary to our observations. This suggests that cross-modal temporal recalibration is not merely triggered by an asynchronous sensory measurement of SOA and an attempt to correct it. In contrast, the causal-inference model accurately captured the plateau of the recalibration effects as adapter SOA increased, because the probability that the auditory and visual stimuli have separate causes also increased. This resulted in a smaller discrepancy between the sensory measurement and the final percept of the SOA, leading to less recalibration.</p><p>We found that most of our participants exhibited larger recalibration effects in response to exposure to audiovisual stimuli with a consistent auditory lead compared to exposure to a visual lead. This result is consistent with a previous study that reported greater cumulative recalibration in response to audiovisual stimuli with an auditory lead at the group level (<xref ref-type="bibr" rid="bib36">O’Donohue et al., 2022</xref>). Our simulation results further suggested that this asymmetry in recalibration effects might be due to higher precision of auditory compared to visual arrival latencies. A few participants showed the opposite pattern: stronger recalibration effects following exposure to visual-leading audiovisual stimuli. This is not surprising, as causal-inference models often reveal substantial individual differences in sensory noise (<xref ref-type="bibr" rid="bib22">Hong et al., 2021</xref>; <xref ref-type="bibr" rid="bib31">Magnotti et al., 2013</xref>). A recent EEG study further provided neural correlates for individual sensory noise by identifying correlations between neural-latency noise and behavioral sensory noise measured from simultaneity-judgment tasks for audiovisual, visuo-tactile, and audio-tactile pairs (<xref ref-type="bibr" rid="bib65">Yarrow et al., 2022</xref>). Therefore, our model explains how individual differences in precision of arrival latency could contribute to the asymmetry in cross-modal temporal recalibration observed in previous studies. For example, <xref ref-type="bibr" rid="bib13">Fujisaki et al., 2004</xref>, found a slightly larger recalibration in response to audiovisual stimuli with a visual lead compared to an auditory lead, while their pilot results with the same design but a wider range of adapter SOAs showed the opposite pattern.</p><p>In order to incorporate causal inference into our recalibration models, we modeled recalibration as a shift of audiovisual bias. Building on previous latency-shift models (<xref ref-type="bibr" rid="bib11">Di Luca et al., 2009</xref>; <xref ref-type="bibr" rid="bib35">Navarra et al., 2009</xref>), we specified a mechanism for how the audiovisual bias is updated during the exposure to an audiovisual SOA. Our model is not mutually exclusive with other models that implement recalibration as a shift of simultaneity criteria (<xref ref-type="bibr" rid="bib62">Yarrow et al., 2011a</xref>; <xref ref-type="bibr" rid="bib64">Yarrow et al., 2015</xref>), or a change of sensitivity to discriminate SOA (<xref ref-type="bibr" rid="bib45">Roseboom et al., 2015</xref>). A possible implementation of recalibration at the circuitry level is given by models assuming that audiovisual offsets are encoded by populations of neurons tuned to different SOAs. In these models, recalibration is the consequence of selective gain reduction of neurons tuned to SOAs similar to the adapter SOA (<xref ref-type="bibr" rid="bib9">Cai et al., 2012</xref>; <xref ref-type="bibr" rid="bib39">Roach et al., 2011</xref>; <xref ref-type="bibr" rid="bib64">Yarrow et al., 2015</xref>). Simulations show that this model can predict nonlinear recalibration effects as a function of adapter SOA depending on the number of neurons and the range of preferred SOAs (Appendix 8). However, to capture the asymmetric recalibration effects depending on which modality leads, one needs to incorporate inhomogeneous neuronal selectivity, i.e., unequal tuning curves, for auditory-leading and visual-leading SOAs.</p><p>Causal inference may effectively function as a credit-assignment mechanism to enhance perceptual accuracy during recalibration. In sensorimotor adaptation, humans correct motor errors that are more likely attributed to their own motor system rather than to the environment (<xref ref-type="bibr" rid="bib8">Berniker and Körding, 2008</xref>; <xref ref-type="bibr" rid="bib60">Wei and Körding, 2009</xref>). In visuomotor adaptation, substantial temporal recalibration occurs in response to exposure to movement-leading SOAs but less so to visual-leading SOAs (<xref ref-type="bibr" rid="bib40">Rohde and Ernst, 2012</xref>; <xref ref-type="bibr" rid="bib41">Rohde et al., 2014</xref>), because only movement-leading SOAs can be interpreted as causally linked sensory feedback from a preceding movement.</p><p>Causal-inference-based recalibration can further solve the conundrum that humans, despite the ability for cross-modal temporal recalibration, show persistent temporal biases (<xref ref-type="bibr" rid="bib15">Grabot and van Wassenhove, 2017</xref>). These audiovisual and visual-tactile temporal biases appear to be shaped by early sensory experience (<xref ref-type="bibr" rid="bib4">Badde et al., 2020a</xref>) and seem to be resistant to recalibration. The persistence of these biases contradicts recalibration models that correct for the measured cross-modal asynchrony. Instead, our causal-inference-based models of recalibration include an assumption that recalibration eliminates the discrepancy between measured and inferred asynchrony, both of which are influenced by cross-modal biases.</p><p>Previous studies have probed the role of causal inference for temporal recalibration and perception by experimentally varying task-irrelevant cues to a shared origin of the cross-modal stimuli, with mixed results. Earlier studies found no significant change in temporal recalibration when altering the sound presentation method (headphones vs. a speaker) or switching the presentation ear (<xref ref-type="bibr" rid="bib13">Fujisaki et al., 2004</xref>), nor did recalibration effects vary with the spatial alignment of the audiovisual stimulus pair (<xref ref-type="bibr" rid="bib26">Keetels and Vroomen, 2007</xref>). However, subsequent studies provide evidence that spatial grouping influences temporal recalibration, with the PSS shifting toward the temporal relationship suggested by spatially co-located stimuli (<xref ref-type="bibr" rid="bib20">Heron et al., 2012</xref>; <xref ref-type="bibr" rid="bib63">Yarrow et al., 2011b</xref>). Others found that spatial cues (<xref ref-type="bibr" rid="bib19">Heron et al., 2007</xref>; <xref ref-type="bibr" rid="bib67">Yuan et al., 2012</xref>) and featural content cues (e.g., male or female audiovisual speech and high-pitch sounds) (<xref ref-type="bibr" rid="bib43">Roseboom and Arnold, 2011</xref>; <xref ref-type="bibr" rid="bib44">Roseboom et al., 2013</xref>; <xref ref-type="bibr" rid="bib67">Yuan et al., 2012</xref>) are both determinants of cross-modal temporal recalibration. Recent studies on audiovisual integration have extended causal-inference models to account for both the spatial position and temporal discrepancy of audiovisual signals (<xref ref-type="bibr" rid="bib23">Hong, 2023</xref>; <xref ref-type="bibr" rid="bib33">McGovern et al., 2016</xref>). Conversely, perceived conflicts in task-irrelevant features of visual-haptic stimuli do not influence the integration of task-relevant features, suggesting that causal inference is feature-specific rather than pertaining to whole objects (<xref ref-type="bibr" rid="bib6">Badde et al., 2023</xref>). These studies indicate that task-relevant spatial and temporal information is taken into account for causal inference and might play a critical role in cross-modal recalibration.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>Ten students from New York University (three males; age: 24.4 ± 1.77; all right-handed) participated in the experiment. They all reported normal or corrected-to-normal vision. All participants provided written informed consent before the experiment and received $15 per hour as monetary compensation. The study was conducted in accordance with the guidelines laid down in the Declaration of Helsinki and approved by the New York University Institutional Review Board (reference number: IRB-2016-595). One out of 10 participants was identified as an outlier and therefore excluded from further data analysis (Appendix 9).</p></sec><sec id="s4-2"><title>Apparatus and stimuli</title><p>Participants completed the experiments in a dark and semi-sound-attenuated room. They were seated 1 m from an acoustically transparent, white screen (1.36 × 1.02 m, 68 × 52° visual angle) and placed their head on a chin rest. An LCD projector (Hitachi CP-X3010N, 1024 × 768 pixels, 60 Hz) was mounted above and behind participants to project visual stimuli on the screen. The visual and auditory stimulus durations were 33.33 ms. The visual stimulus was a high-contrast (36.1 cd/m<sup>2</sup>) Gaussian blob (SD: 3.6°) on a gray background (10.2 cd/m<sup>2</sup>) projected onto the screen. The auditory stimulus was a 500 Hz beep (50 dB SPL) without a temporal hamming window due to its short duration, which was played by a loudspeaker located behind the center of the screen. Some visual and auditory stimuli were of higher intensity, the parameters of these stimuli were determined individually (see Intensity-discrimination task). We adjusted the timing of audiovisual stimulus presentation and verified the timing using an oscilloscope (PICOSCOPE 2204A).</p></sec><sec id="s4-3"><title>Procedure</title><p>The experiment consisted of nine sessions, which took place on nine separate days. In each session, participants completed a pre-test, an exposure, and a post-test phase in sequence. The adapter SOA was fixed within each session, but varied across sessions (±700, ±300, ±200, ±100, 0 ms). The order of the adapter SOA was randomized across participants, with sessions separated by at least 1 day. The intensities of the oddball stimuli were determined prior to the experiment for each participant using an intensity-discrimination task to equate the difficulty of detecting oddball stimuli between participants and across modalities.</p><sec id="s4-3-1"><title>Pre-test phase</title><p>Participants completed a ternary TOJ task during the pre-test phase. Each trial started with a fixation cross (0.1–0.2 s, uniform distribution; <xref ref-type="fig" rid="fig1">Figure 1</xref>), followed by a blank screen (0.4–0.6 s, uniform distribution). Then, an auditory and a visual stimulus (0.033 s) were presented with a variable SOA. There were a total of 15 possible test SOAs (±0.5 s and from –0.3 to 0.3 s in steps of 0.05 s), with positive values representing visual lead and negative values representing auditory lead. Following stimulus presentation there was another blank screen (0.4–0.6 s, uniform distribution), and then a response probe appeared on the screen. Participants indicated by button press whether the auditory stimulus occurred before or after the visual stimulus, or whether the two were simultaneous. There was no time limit for the response, and response feedback was not provided. The inter-trial interval was 0.2–0.4 s (uniform distribution). Each test SOA was presented 20 times in pseudo-randomized order, resulting in 300 trials in total, divided into five blocks. Participants usually took around 15 minutes to finish the pre-test phase.</p></sec><sec id="s4-3-2"><title>Exposure phase</title><p>Participants completed an oddball-detection task during the exposure phase. In each trial, participants were presented with an audiovisual stimulus pair with a fixed SOA (adapter SOA). In 10% of trials, the intensity of either the visual or the auditory component (or both) was greater than in the other trials. Participants were instructed to press the corresponding button as soon as possible to indicate whether there was an auditory oddball, a visual oddball, or both stimuli were oddballs. The task timing (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) was almost identical to the ternary TOJ task, except that there was a response time limit of 1.4 s. Prior to the exposure phase, participants practiced the task for as long as needed to familiarize themselves with the task. During this practice, they were presented with bimodal stimuli with the same adapter SOA used in the exposure phase. There were a total of 250 trials, divided into five blocks. At the end of each block, we presented a performance summary with the hit rate and false-alarm rate of each modality. Participants usually completed the exposure phase in about 15 minutes.</p></sec><sec id="s4-3-3"><title>Post-test phase</title><p>Participants completed the ternary TOJ task as well as the oddball-detection task during the post-test phase. Specifically, each TOJ was preceded by three top-up (oddball-detection) trials. The adapter SOA in the top-up trials was the same as that in the exposure phase to prevent dissipation of temporal recalibration (<xref ref-type="bibr" rid="bib29">Machulla et al., 2012</xref>). Both visual and auditory <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> remained consistent from the exposure to post-test phases, indicating similar performance in the top-up trials to performance during the exposure phase (<xref ref-type="fig" rid="app10fig1">Appendix 10—figure 1</xref>). To facilitate task switching, the ITI between the last top-up trial and the following TOJ trial was longer (with the additional time jittered around 1 s). Additionally, the fixation cross was displayed in red to signal the start of a TOJ trial. As in the pre-test phase, there were 300 TOJ trials (15 test SOAs × 20 repetitions) with the addition of 900 top-up trials, grouped into six blocks. At the end of each block, we provided a summary of the oddball-detection performance. Participants usually completed the post-test phase in around one hour.</p></sec><sec id="s4-3-4"><title>Intensity-discrimination task</title><p>This task was conducted to estimate the just-noticeable-difference (JND) in intensity for a standard visual stimulus with a luminance of 36.1 cd/m<sup>2</sup> and a standard auditory stimulus with a volume of 40 dB SPL. The task was two-interval, forced choice. The trial started with a fixation (0.1–0.2 s) and a blank screen (0.4–0.6 s). Participants were presented with a standard stimulus (0.033 s) in one randomly selected interval and a comparison stimulus (0.033 s) in the other interval, temporally separated by an inter-stimulus interval (0.6–0.8 s). They indicated which interval contained the brighter/louder stimulus without time constraint. Seven test stimulus levels (luminance range: 5–195% relative to the standard visual stimulus intensity; volume range: 50–150% relative to the standard auditory stimulus’ amplitude) were repeated 20 times, resulting in 140 trials for each task. We fit a cumulative Gaussian distribution function to these data and defined the oddball as an auditory or visual stimulus with an intensity judged as more intense than the standard 90% of the time. A higher probability than the standard JND of 75% was selected because the pilot studies showed that the harder oddball-detection task became too demanding during the one-hour post-test.</p></sec></sec><sec id="s4-4"><title>Modeling</title><p>In this section, we first outline general assumptions, shared across all candidate models, regarding sensory noise, measurements, and bias. Then, we formalize three process models of recalibration that differ in the implementation of recalibration. In each recalibration model, we also provide a formalization of the ternary TOJ task administered in the pre- and the post-test phases, data from which were used to constrain the model parameters. Finally, we describe how the models were fit to the data.</p><sec id="s4-4-1"><title>General modal assumptions regarding sensory noise, measurements, and bias</title><p>When an audiovisual stimulus pair with an SOA, <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, is presented, it triggers auditory and visual signals that are registered in the relevant region of cortex where audiovisual temporal-order comparisons are made. This leads to two internal measurements of the arrival time for each signal in an observer’s brain. These arrival times are subject to noise and thus vary across presentations of the same physical stimulus pair. As in previous work (<xref ref-type="bibr" rid="bib14">García-Pérez and Alcalá-Quintana, 2012</xref>), we model the probability distribution of the arrival time as shifted exponential distributions (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). The arrival time of the auditory signal relative to onset <inline-formula><mml:math id="inf6"><mml:msub><mml:mi>t</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> is the sum of the fixed delay of internal signal, <inline-formula><mml:math id="inf7"><mml:msub><mml:mi>β</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula>, and an additional random delay that is exponentially distributed with time constant <inline-formula><mml:math id="inf8"><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula>; analogous for the visual latency (with delay <inline-formula><mml:math id="inf9"><mml:msub><mml:mi>β</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula> and time constant <inline-formula><mml:math id="inf10"><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula>).</p><p>The measured SOA of the audiovisual stimulus pair is modeled as the difference of the arrival times of the two stimuli. Thus, the sensory measurement of SOA, <inline-formula><mml:math id="inf11"><mml:mi>m</mml:mi></mml:math></inline-formula>, reflects the sum of three components: the physical SOA, <inline-formula><mml:math id="inf12"><mml:mi>s</mml:mi></mml:math></inline-formula>; a fixed latency that is the difference between the auditory and visual fixed delay, <inline-formula><mml:math id="inf13"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mtext>pre</mml:mtext></mml:msub><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>; and the difference between two exponentially distributed random delays. A negative value of <inline-formula><mml:math id="inf14"><mml:msub><mml:mi>β</mml:mi><mml:mtext>pre</mml:mtext></mml:msub></mml:math></inline-formula> indicates faster auditory processing. We assume that the audiovisual fixed latency corresponds to the observer’s persistent default audiovisual temporal bias (<xref ref-type="bibr" rid="bib4">Badde et al., 2020a</xref>; <xref ref-type="bibr" rid="bib15">Grabot and van Wassenhove, 2017</xref>). Thus, we assume that after leaving the experimental room, the default bias is restored and thus consistent across pre-tests.</p><p>We model the recalibration process as a shift of the audiovisual temporal bias at the end of every exposure trial <inline-formula><mml:math id="inf15"><mml:mi>i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf16"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mtext>pre</mml:mtext></mml:msub><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the current audiovisual bias, and <inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the cumulative shift of audiovisual temporal bias. After the 250 exposure trials the updated biases can be expressed as <inline-formula><mml:math id="inf19"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mtext>post</mml:mtext></mml:msub><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mtext>pre</mml:mtext></mml:msub><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo separator="true">,</mml:mo><mml:mn>250</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. We also assume that the amounts of auditory and visual latency noise, <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf21"><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula>, remain constant across phases and sessions.</p><p>Given that both latency distributions are shifted exponential distributions, the probability density function of the sensory measurements of SOA, <inline-formula><mml:math id="inf22"><mml:mi>m</mml:mi></mml:math></inline-formula>, given physical SOA, <inline-formula><mml:math id="inf23"><mml:mi>s</mml:mi></mml:math></inline-formula>, is a double-exponential function (see derivation in Appendix 3; <xref ref-type="fig" rid="fig6">Figure 6A</xref>):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="0.6em 0.2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mtext>if</mml:mtext><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mtext>if</mml:mtext><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The probability density function of measured SOA peaks at the physical SOA of the stimuli plus the participant’s audiovisual temporal bias, <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The left and right spread of this measurement distribution depends on the amount of the latency noise for the visual, <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula>, and auditory, <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula>, signals. In models with modality-independent arrival-time precision, <inline-formula><mml:math id="inf27"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the measurement distribution is symmetrical. This symmetrical measurement distribution is often approximated by a Gaussian distribution to fit TOJ responses in previous temporal-recalibration studies (<xref ref-type="bibr" rid="bib11">Di Luca et al., 2009</xref>; <xref ref-type="bibr" rid="bib13">Fujisaki et al., 2004</xref>; <xref ref-type="bibr" rid="bib17">Harrar and Harris, 2005</xref>; <xref ref-type="bibr" rid="bib26">Keetels and Vroomen, 2007</xref>; <xref ref-type="bibr" rid="bib34">Navarra et al., 2005</xref>; <xref ref-type="bibr" rid="bib54">Tanaka et al., 2011</xref>; <xref ref-type="bibr" rid="bib56">Vatakis et al., 2007</xref>; <xref ref-type="bibr" rid="bib57">Vatakis et al., 2008</xref>; <xref ref-type="bibr" rid="bib58">Vroomen et al., 2004</xref>). Note that we assume the observer has perfect knowledge of the visual and auditory latency noise. Thus, the density of the measurement distribution corresponds to the likelihood function during the inference process when the observer only has the noisy measurement, <inline-formula><mml:math id="inf28"><mml:mi>m</mml:mi></mml:math></inline-formula>, and needs to infer the physical SOA, <inline-formula><mml:math id="inf29"><mml:mi>s</mml:mi></mml:math></inline-formula>.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Simulating responses of the temporal-order-judgment (TOJ) task with a causal-inference perceptual process.</title><p>(<bold>A</bold>) An example probability density for the measurement of a zero stimulus-onset asynchrony (SOA). (<bold>B</bold>) The probability density of estimates resulting from a zero-SOA stimulus based on simulation using the causal-inference process. The symmetrical criteria around zero partition the distribution of estimated SOA into three regions, coded by different colors. The area under each segment of the estimate distribution corresponds to the probabilities of the three possible intended responses for a zero SOA. (<bold>C</bold>) The simulated psychometric function computed by repeatedly calculating the probabilities of the three response types across all test SOAs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-fig6-v1.tif"/></fig></sec><sec id="s4-4-2"><title>The causal-inference model</title><sec id="s4-4-2-1"><title>Formalization of recalibration in the exposure phase</title><p>The causal-inference model assumes that, at the end of every exposure trial <inline-formula><mml:math id="inf30"><mml:mi>i</mml:mi></mml:math></inline-formula>, a discrepancy between the measured SOA, <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, and the final estimate of the stimulus SOA, <inline-formula><mml:math id="inf32"><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, signals the need for recalibration. The cumulative shift of audiovisual temporal bias <inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> after exposure trial <inline-formula><mml:math id="inf34"><mml:mi>i</mml:mi></mml:math></inline-formula> is<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf35"><mml:mi>α</mml:mi></mml:math></inline-formula> is the learning rate.</p><p>The ideal observer infers intermediate location estimates for two causal scenarios: the auditory and visual stimuli can arise from a single cause <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> or two independent causes <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The posterior distribution of the SOA, <inline-formula><mml:math id="inf38"><mml:mi>s</mml:mi></mml:math></inline-formula>, conditioned on each causal scenario is computed by multiplying the likelihood function (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) with the corresponding prior over SOA. In the case of a common cause <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, the prior distribution of the SOA between sound and light is a Gaussian distribution (<xref ref-type="bibr" rid="bib31">Magnotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib33">McGovern et al., 2016</xref>)  <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. To maintain consistency with previous studies, we used an unbiased prior which assigns the highest probability to a physically synchronous stimulus pair <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Similarly, the prior distribution conditioned on separate causes <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is also a Gaussian distribution, <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, with a much larger spread compared to the common-cause scenario. The intermediate estimates <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> conditioned on the common-cause scenario and <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> conditioned on separate-cause scenario are the maximum-a-posteriori estimates of conditional posteriors, which are approximated numerically as there is no closed-form solution.</p><p>The final estimate of the stimulus SOA, <inline-formula><mml:math id="inf46"><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula>, depends on the posterior probability of each causal scenario. According to Bayes rule, the posterior probability that an audiovisual stimulus pair with the measured SOA, <inline-formula><mml:math id="inf47"><mml:mi>m</mml:mi></mml:math></inline-formula>, shares a common cause is<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The likelihood of a common source/separate sources for a fixed SOA measurement was approximated by numerically integrating the scenario-specific protoposterior (i.e. the un-normalized posterior),<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="0.7em 0.3em" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∫</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∫</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The posterior probability of a common cause additionally depends on the observer’s prior belief of a common cause for auditory and visual stimuli, <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>P</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext/><mml:mi>common</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>The final estimate of SOA was derived by model averaging, i.e., the average of the scenario-specific SOA estimates, <inline-formula><mml:math id="inf49"><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf50"><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> each weighted by the posterior probability of the corresponding causal scenario,<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-4-2-2"><title>Formalization of the ternary TOJ task with a causal-inference perceptual process</title><p>In the ternary TOJ task administered in the pre- and post-test phases, the observer is presented with an audiovisual stimulus pair and has to decide whether the auditory stimulus was presented first, the visual stimulus was presented first, or both of them were presented at the same time. The observer makes this perceptual judgment by comparing the final estimate of the SOA, <inline-formula><mml:math id="inf51"><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula>, to two internal criteria (<xref ref-type="bibr" rid="bib10">Cary et al., 2024</xref>; <xref ref-type="bibr" rid="bib14">García-Pérez and Alcalá-Quintana, 2012</xref>). We assume that the observer has a symmetric pair of criteria, ±<italic>c</italic>, centered on the stimulus SOA corresponding to perceptual simultaneity (<inline-formula><mml:math id="inf52"><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). In addition, the observer may lapse or make an error when responding by a lapse rate, <inline-formula><mml:math id="inf53"><mml:mi>λ</mml:mi></mml:math></inline-formula>. The probabilities of reporting visual lead, <inline-formula><mml:math id="inf54"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula>, auditory lead, <inline-formula><mml:math id="inf55"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula>, or that the two stimuli were simultaneous, <inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula>, are thus<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="0.7em 0.7em 0.3em" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>3</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>&gt;</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>3</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>&lt;</mml:mo><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mtext>and</mml:mtext></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The probability distribution of causal-inference-based SOA estimates <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> has no closed form distribution function and thus was approximated using simulations, resulting in <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. <xref ref-type="fig" rid="fig6">Figure 6</xref> illustrates the process of simulating the psychometric functions, using a zero test SOA as an example. First, we sampled 10,000 SOA measurements from the double-exponential probability distribution corresponding to the test SOA of zero (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Second, for each sampled measurement, we simulated the process by which the observer carries out causal inference and by doing so produces an estimate of the stimulus SOA, while keeping the causal-inference model parameters fixed. This process resulted in a Monte Carlo approximation of the probability density distribution of the causal-inference-based SOA estimates (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Third, we calculated the probability of the three types of responses (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>) for this specific test SOA. This process was repeated for each test SOA to generate three psychometric functions (<xref ref-type="fig" rid="fig6">Figure 6C</xref>).</p></sec></sec></sec><sec id="s4-5"><title>The asynchrony-contingent model</title><p>In the asynchrony-contingent model, the observer measures the audiovisual SOA, <inline-formula><mml:math id="inf59"><mml:mi>s</mml:mi></mml:math></inline-formula>, by comparing the arrival latency of the auditory and visual signals. The observer uses the likelihood that the audiovisual stimuli occurred simultaneously <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext>SOA</mml:mtext><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> to update the temporal bias during recalibration, instead of performing causal inference. We again assume that the observer has perfect knowledge about the precision and fixed delays of the arrival times and thus assume the likelihood corresponds to the measurement distribution (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). The observer uses this probability of simultaneity to scale the update rate of the audiovisual bias,<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We assume the observer’s estimate of the stimulus SOA, <inline-formula><mml:math id="inf61"><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula>, is identical to the measured SOA, <inline-formula><mml:math id="inf62"><mml:mi>m</mml:mi></mml:math></inline-formula>. Thus, from the experimenter’s perspective, the probability of the three different responses in the TOJ task can be obtained by replacing the SOA estimate, <inline-formula><mml:math id="inf63"><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula>, with the SOA measurement, <inline-formula><mml:math id="inf64"><mml:mi>m</mml:mi></mml:math></inline-formula>, in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>. As we know the probability distribution of <inline-formula><mml:math id="inf65"><mml:mi>m</mml:mi></mml:math></inline-formula>, the psychometric functions have a closed form (<xref ref-type="bibr" rid="bib14">García-Pérez and Alcalá-Quintana, 2012</xref>).</p></sec><sec id="s4-6"><title>The asynchrony-correction model</title><p>In the asynchrony-correction model, the observer begins by evaluating if the sensory measurement of SOA, <inline-formula><mml:math id="inf66"><mml:mi>m</mml:mi></mml:math></inline-formula>, falls outside the criterion range for reporting that the two stimuli were presented simultaneously ±<italic>c</italic>. If the measurement does exceed this criterion, the observer adjusts the audiovisual bias by shifting it against the measurement, i.e., shifting it so that the measured SOA of a pair would be closer to zero and is more likely to perceived as simultaneous. This adjustment is proportional to the sensory measurement of the SOA, <inline-formula><mml:math id="inf67"><mml:mi>m</mml:mi></mml:math></inline-formula>, at a fixed rate determined by the learning rate <italic>α</italic>. The update rule of the audiovisual bias in trial <italic>i</italic> is thus<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="0.6em 0.2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mtext>if </mml:mtext><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mi>c</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mtext>otherwise.</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The derivation of the psychometric functions is identical to the asynchrony-contingent model.</p></sec><sec id="s4-7"><title>Model fitting</title><sec id="s4-7-1"><title>Model log-likelihood</title><p>The model was fitted by optimizing the lower bound on the marginal log-likelihood. We fit the model to the joint ternary TOJ data collected during the pre- and post-test phases of all sessions. We did not collect TOJs in the exposure phase. But, to model the post-test data, we need to estimate the distribution of shifts of audiovisual bias resulting from the exposure phase (<inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). We do this using Monte Carlo simulation of the 250 exposure trials to estimate the probability distribution of the cumulative shifts.</p><p>The set of model parameters <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi></mml:mrow></mml:math></inline-formula> is listed in <xref ref-type="table" rid="table1">Table 1</xref>. There are <inline-formula><mml:math id="inf70"><mml:mi>J</mml:mi></mml:math></inline-formula> sessions, each including <inline-formula><mml:math id="inf71"><mml:mi>K</mml:mi></mml:math></inline-formula> trials in the pre-test phase and <inline-formula><mml:math id="inf72"><mml:mi>K</mml:mi></mml:math></inline-formula> trials in the post-test phase. We denote the full dataset of pre-test data as <inline-formula><mml:math id="inf73"><mml:msub><mml:mi>X</mml:mi><mml:mtext>pre</mml:mtext></mml:msub></mml:math></inline-formula> and for the post-test data as <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. We fit the pre- and post-test data jointly by summing their log-likelihood, <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Model parameters.</title><p>Check marks signify that the parameter is used for determining the likelihood of the data from the temporal-order judgment task in the pre- and post-test phase and/or for the Monte Carlo simulation of recalibration in the exposure phase.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Notation</th><th align="left" valign="bottom">Specification</th><th align="left" valign="bottom">Temporal-order- judgment task</th><th align="left" valign="bottom">Recalibration in the exposure phase</th></tr></thead><tbody><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf76"><mml:msub><mml:mi>β</mml:mi><mml:mtext>pre</mml:mtext></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">The fixed relative delay between visual and auditory processing, i.e., the audiovisual bias prior to the exposure phase</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf77"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf78"><mml:mi>✓</mml:mi></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf79"><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">Amount of auditory latency noise, the exponential time constant of the auditory arrival-latency distribution</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf80"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf81"><mml:mi>✓</mml:mi></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf82"><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">Amount of visual latency noise, the exponential time constant of the visual -latency distribution</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf83"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf84"><mml:mi>✓</mml:mi></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf85"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">The spread of the Gaussian prior for the common-cause scenario</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf86"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf87"><mml:mi>✓</mml:mi></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf88"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">The spread of the Gaussian prior for the separate-causes scenario</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf89"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf90"><mml:mi>✓</mml:mi></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf91"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext/><mml:mi>common</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">The prior probability of a common cause</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf92"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf93"><mml:mi>✓</mml:mi></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf94"><mml:mi>c</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">Simultaneity criterion</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf95"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><italic>λ</italic></td><td align="left" valign="bottom">Lapse rate</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf96"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><italic>α</italic></td><td align="left" valign="bottom">Learning rate for shifting audiovisual bias</td><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf97"><mml:mi>✓</mml:mi></mml:math></inline-formula></td></tr></tbody></table></table-wrap><p>In a given trial, the observer responded either auditory-first (A), visual-first (V), or simultaneous (S). We denote a single response using indicator variables that are equal to 1 if that was the response in that trial and 0 otherwise. These variables for trial <inline-formula><mml:math id="inf98"><mml:mi>k</mml:mi></mml:math></inline-formula> in session <inline-formula><mml:math id="inf99"><mml:mi>j</mml:mi></mml:math></inline-formula> are <inline-formula><mml:math id="inf100"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext><mml:mo separator="true">,</mml:mo><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf101"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext><mml:mo separator="true">,</mml:mo><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>V</mml:mi></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf102"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext><mml:mo separator="true">,</mml:mo><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msubsup></mml:math></inline-formula> for the pre-test trials, and <inline-formula><mml:math id="inf103"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>post</mml:mtext><mml:mo separator="true">,</mml:mo><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup></mml:math></inline-formula>, etc., for the post-test trials. The log-likelihood of all pre-test responses <inline-formula><mml:math id="inf104"><mml:msub><mml:mi>X</mml:mi><mml:mtext>pre</mml:mtext></mml:msub></mml:math></inline-formula> given the model parameters is<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="0.7em 0.3em" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">(</mml:mo></mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The psychometric functions for the pre-test (e.g., <inline-formula><mml:math id="inf105"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo separator="true">,</mml:mo><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>) are defined in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>, and are the same across all sessions as we assumed that the audiovisual bias <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was the same before recalibration in every session.</p><p>The log-likelihood of responses in the post-test depends on the audiovisual bias after recalibration <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mtext>post</mml:mtext><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for session <inline-formula><mml:math id="inf108"><mml:mi>j</mml:mi></mml:math></inline-formula>. To determine the log-likelihood of the post-test data requires us to integrate out the unknown value of the cumulative shift <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. We approximated this integral in two steps based on our previous work (<xref ref-type="bibr" rid="bib22">Hong et al., 2021</xref>). First, we simulated the 250 exposure trials 1000 times for a given set of parameters <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi></mml:mrow></mml:math></inline-formula> and session <inline-formula><mml:math id="inf111"><mml:mi>j</mml:mi></mml:math></inline-formula>. This resulted in 1000 values of <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The distribution of these values was well fit by a Gaussian whose parameters were determined by the empirical mean and standard deviation of the sample distribution, resulting in the distribution <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Second, we approximated the integral of the log-likelihood of the data over possible values of <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> by numerical integration. We discretized the approximated distribution <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> into 100 equally spaced bins centered on values <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). The range of the bins was triple the range of the values from the Monte Carlo sample, so that the lower bound was <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mtext>max</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and the upper bound was <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mtext>max</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mtext>max</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The log-likelihood of the post-test data was approximated as<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="0.7em 0.7em 0.7em 0.7em 0.3em" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>∫</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>≈</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">(</mml:mo></mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>≈</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">(</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi>u</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mn>100</mml:mn></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true" maxsize="2.470em" minsize="2.470em"/></mml:mrow><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="0.7em 0.3em" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">(</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mtext>post</mml:mtext><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>post</mml:mtext><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup></mml:mrow></mml:msup><mml:mo>×</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mtext>post</mml:mtext><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>post</mml:mtext><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>V</mml:mi></mml:msubsup></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mtext>post</mml:mtext><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>post</mml:mtext><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The psychometric functions in the post-test (e.g., <inline-formula><mml:math id="inf120"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo separator="true">,</mml:mo><mml:mtext>post</mml:mtext><mml:mo separator="true">,</mml:mo><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) differed across sessions and bins because the simulated audiovisual bias after the exposure phase <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mtext>post</mml:mtext><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> depends on the adapter SOA fixed in session <inline-formula><mml:math id="inf122"><mml:mi>j</mml:mi></mml:math></inline-formula> and the simulation bin <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-7-2"><title>Parameter estimation and model comparison</title><p>We approximated the lower bounds to the model evidence (i.e. the marginal likelihood) of each model for each participant’s data using variational Bayesian Monte Carlo (<xref ref-type="bibr" rid="bib2">Acerbi, 2018</xref>; <xref ref-type="bibr" rid="bib3">Acerbi, 2020</xref>). We set the prior distribution of parameters based on the results of maximum-likelihood estimation using Bayesian Adaptive Direct Search to ensure that the parameter ranges were plausible (<xref ref-type="bibr" rid="bib1">Acerbi and Ma, 2017</xref>). We repeated each search 20 times with a different and random starting point to address the possibility of reporting a local minimum. For each model, the fit with the maximum lower bounds of the model evidence across the repeated searches was chosen for the maximum model evidence and best parameter estimates.</p><p>We then conducted a Bayesian model comparison based on model evidence. The model with the strongest evidence was considered the best-fitting model (<xref ref-type="bibr" rid="bib30">MacKay, 2003</xref>). To quantify the support of model selection, we computed the Bayes factor, the ratio of the model evidence between each model and the asynchrony-correction, modality-independent-precision model, which had the weakest model evidence. To compare any two models, one can simply calculate the difference in their log Bayes factors as both are relative to the same weakest model.</p></sec><sec id="s4-7-3"><title>Model recovery and parameter recovery</title><p>We conducted a model-recovery analysis for the six models and confirmed that they are identifiable (Appendix 11). In addition, we considered an alternative causal-inference model in which the bias update is proportional to the posterior probability of a common cause, instead of driven by the difference between the measured and estimated SOA. A separate model recovery analysis on variations of the causal-inference model was unable to distinguish between them (Appendix 12). For the causal-inference, modality-specific-precision model, we also carried out a parameter recovery analysis and confirmed that all the parameters are recoverable (Appendix 13).</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Supervision, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All participants provided informed written consent before the experiment and received monetary compensation. The study was conducted in accordance with the guidelines laid down in the Declaration of Helsinki and approved by the New York University Institutional Review Board.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-97765-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data and code are available via the <ext-link ext-link-type="uri" xlink:href="https://osf.io/8s7qv/">Open Science Framework</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Hong</surname><given-names>F</given-names></name><name><surname>Badde</surname><given-names>S</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Precision-based causal inference modulates audiovisual temporal recalibration</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/8S7QV</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank the NYU High-Performance Computing (NYU HPC) for providing computational resources and support. We thank the anonymous reviewers’ advice that helped us improve the manuscript. Funding: This research was funded by NIH EY08266.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Acerbi</surname><given-names>L</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Practical Bayesian optimization for model fitting with Bayesian adaptive direct search</article-title><conf-name>Proceedings of the 31st International Conference on Neural Information Processing Systems</conf-name><fpage>1834</fpage><lpage>1844</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Acerbi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Variational Bayesian Monte Carlo</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1810.05558">https://arxiv.org/abs/1810.05558</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Acerbi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Variational Bayesian Monte Carlo with Noisy Likelihoods</article-title><conf-name>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</conf-name><fpage>8211</fpage><lpage>8222</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badde</surname><given-names>S</given-names></name><name><surname>Ley</surname><given-names>P</given-names></name><name><surname>Rajendran</surname><given-names>SS</given-names></name><name><surname>Shareef</surname><given-names>I</given-names></name><name><surname>Kekunnaya</surname><given-names>R</given-names></name><name><surname>Röder</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Sensory experience during early sensitive periods shapes cross-modal temporal biases</article-title><source>eLife</source><volume>9</volume><elocation-id>e61238</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.61238</pub-id><pub-id pub-id-type="pmid">32840213</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badde</surname><given-names>S</given-names></name><name><surname>Navarro</surname><given-names>KT</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Modality-specific attention attenuates visual-tactile integration and recalibration effects by reducing prior expectations of a common source for vision and touch</article-title><source>Cognition</source><volume>197</volume><elocation-id>104170</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2019.104170</pub-id><pub-id pub-id-type="pmid">32036027</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badde</surname><given-names>S</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name><name><surname>Adams</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Multisensory causal inference is feature-specific, not object-based</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>378</volume><elocation-id>20220345</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2022.0345</pub-id><pub-id pub-id-type="pmid">37545302</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beierholm</surname><given-names>UR</given-names></name><name><surname>Quartz</surname><given-names>SR</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Bayesian priors are encoded independently from likelihoods in human multisensory perception</article-title><source>Journal of Vision</source><volume>9(5)</volume><elocation-id>23</elocation-id><pub-id pub-id-type="doi">10.1167/9.5.23</pub-id><pub-id pub-id-type="pmid">19757901</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berniker</surname><given-names>M</given-names></name><name><surname>Körding</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Estimating the sources of motor errors for adaptation and generalization</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1454</fpage><lpage>1461</lpage><pub-id pub-id-type="doi">10.1038/nn.2229</pub-id><pub-id pub-id-type="pmid">19011624</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>M</given-names></name><name><surname>Stetson</surname><given-names>C</given-names></name><name><surname>Eagleman</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A neural model for temporal order judgments and their active recalibration: A common mechanism for space and time?</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>470</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00470</pub-id><pub-id pub-id-type="pmid">23130010</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cary</surname><given-names>E</given-names></name><name><surname>Lahdesmaki</surname><given-names>I</given-names></name><name><surname>Badde</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Audiovisual simultaneity windows reflect temporal sensory uncertainty</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>31</volume><fpage>2170</fpage><lpage>2179</lpage><pub-id pub-id-type="doi">10.3758/s13423-024-02478-4</pub-id><pub-id pub-id-type="pmid">38388825</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Luca</surname><given-names>M</given-names></name><name><surname>Machulla</surname><given-names>TK</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Recalibration of multisensory simultaneity: Cross-modal transfer coincides with a change in perceptual latency</article-title><source>Journal of Vision</source><volume>9(12)</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.1167/9.12.7</pub-id><pub-id pub-id-type="pmid">20053098</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fain</surname><given-names>GL</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>Sensory Transduction</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/oso/9780198835028.001.0001</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujisaki</surname><given-names>W</given-names></name><name><surname>Shimojo</surname><given-names>S</given-names></name><name><surname>Kashino</surname><given-names>M</given-names></name><name><surname>Nishida</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Recalibration of audiovisual simultaneity</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>773</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1038/nn1268</pub-id><pub-id pub-id-type="pmid">15195098</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>García-Pérez</surname><given-names>MA</given-names></name><name><surname>Alcalá-Quintana</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>On the discrepant results in synchrony judgment and temporal-order judgment tasks: A quantitative model</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>19</volume><fpage>820</fpage><lpage>846</lpage><pub-id pub-id-type="doi">10.3758/s13423-012-0278-y</pub-id><pub-id pub-id-type="pmid">22829342</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grabot</surname><given-names>L</given-names></name><name><surname>van Wassenhove</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Time order as psychological bias</article-title><source>Psychological Science</source><volume>28</volume><fpage>670</fpage><lpage>678</lpage><pub-id pub-id-type="doi">10.1177/0956797616689369</pub-id><pub-id pub-id-type="pmid">28485701</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanson</surname><given-names>JVM</given-names></name><name><surname>Heron</surname><given-names>J</given-names></name><name><surname>Whitaker</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Recalibration of perceived time across sensory modalities</article-title><source>Experimental Brain Research</source><volume>185</volume><fpage>347</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1007/s00221-008-1282-3</pub-id><pub-id pub-id-type="pmid">18236035</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrar</surname><given-names>V</given-names></name><name><surname>Harris</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Simultaneity constancy: Detecting events with touch and vision</article-title><source>Experimental Brain Research</source><volume>166</volume><fpage>465</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1007/s00221-005-2386-7</pub-id><pub-id pub-id-type="pmid">16028031</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrar</surname><given-names>V</given-names></name><name><surname>Harris</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The effect of exposure to asynchronous audio, visual, and tactile stimulus combinations on the perception of simultaneity</article-title><source>Experimental Brain Research</source><volume>186</volume><fpage>517</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1007/s00221-007-1253-0</pub-id><pub-id pub-id-type="pmid">18183377</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heron</surname><given-names>J</given-names></name><name><surname>Whitaker</surname><given-names>D</given-names></name><name><surname>McGraw</surname><given-names>PV</given-names></name><name><surname>Horoshenkov</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Adaptation minimizes distance-related audiovisual delays</article-title><source>Journal of Vision</source><volume>7(13)</volume><elocation-id>5</elocation-id><pub-id pub-id-type="doi">10.1167/7.13.5</pub-id><pub-id pub-id-type="pmid">17997633</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heron</surname><given-names>J</given-names></name><name><surname>Roach</surname><given-names>NW</given-names></name><name><surname>Hanson</surname><given-names>JVM</given-names></name><name><surname>McGraw</surname><given-names>PV</given-names></name><name><surname>Whitaker</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Audiovisual time perception is spatially specific</article-title><source>Experimental Brain Research</source><volume>218</volume><fpage>477</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1007/s00221-012-3038-3</pub-id><pub-id pub-id-type="pmid">22367399</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirsh</surname><given-names>IJ</given-names></name><name><surname>Sherrick</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>Perceived order in different sense modalities</article-title><source>Journal of Experimental Psychology</source><volume>62</volume><fpage>423</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1037/h0045283</pub-id><pub-id pub-id-type="pmid">13907740</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>F</given-names></name><name><surname>Badde</surname><given-names>S</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Causal inference regulates audiovisual spatial recalibration via its influence on audiovisual perception</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008877</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008877</pub-id><pub-id pub-id-type="pmid">34780469</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2023">2023</year><source>The Role of Causal Inference in Multisensory Integration and Recalibration</source><publisher-name>New York University</publisher-name></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsiao</surname><given-names>A</given-names></name><name><surname>Lee-Miller</surname><given-names>T</given-names></name><name><surname>Block</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Conscious awareness of a visuo-proprioceptive mismatch: Effect on cross-sensory recalibration</article-title><source>Frontiers in Neuroscience</source><volume>16</volume><elocation-id>958513</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2022.958513</pub-id><pub-id pub-id-type="pmid">36117619</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jazayeri</surname><given-names>M</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Optimal representation of sensory information by neural populations</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>690</fpage><lpage>696</lpage><pub-id pub-id-type="doi">10.1038/nn1691</pub-id><pub-id pub-id-type="pmid">16617339</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keetels</surname><given-names>M</given-names></name><name><surname>Vroomen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>No effect of auditory-visual spatial disparity on temporal recalibration</article-title><source>Experimental Brain Research</source><volume>182</volume><fpage>559</fpage><lpage>565</lpage><pub-id pub-id-type="doi">10.1007/s00221-007-1012-2</pub-id><pub-id pub-id-type="pmid">17598092</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Multisensory integration: Strategies for synchronization</article-title><source>Current Biology</source><volume>15</volume><fpage>R339</fpage><lpage>R341</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2005.04.022</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname><given-names>KP</given-names></name><name><surname>Beierholm</surname><given-names>U</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Quartz</surname><given-names>S</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Causal inference in multisensory perception</article-title><source>PLOS ONE</source><volume>2</volume><elocation-id>e943</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0000943</pub-id><pub-id pub-id-type="pmid">17895984</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Machulla</surname><given-names>TK</given-names></name><name><surname>Di Luca</surname><given-names>M</given-names></name><name><surname>Froehlich</surname><given-names>E</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Multisensory simultaneity recalibration: Storage of the aftereffect in the absence of counterevidence</article-title><source>Experimental Brain Research</source><volume>217</volume><fpage>89</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1007/s00221-011-2976-5</pub-id><pub-id pub-id-type="pmid">22207361</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>MacKay</surname><given-names>DJC</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Information Theory, Inference and Learning Algorithms</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnotti</surname><given-names>JF</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Causal inference of asynchronous audiovisual speech</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>798</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00798</pub-id><pub-id pub-id-type="pmid">24294207</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maij</surname><given-names>F</given-names></name><name><surname>Brenner</surname><given-names>E</given-names></name><name><surname>Smeets</surname><given-names>JBJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Temporal information can influence spatial localization</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>490</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1152/jn.91253.2008</pub-id><pub-id pub-id-type="pmid">19439670</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGovern</surname><given-names>DP</given-names></name><name><surname>Roudaia</surname><given-names>E</given-names></name><name><surname>Newell</surname><given-names>FN</given-names></name><name><surname>Roach</surname><given-names>NW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perceptual learning shapes multisensory causal inference via two distinct mechanisms</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>24673</elocation-id><pub-id pub-id-type="doi">10.1038/srep24673</pub-id><pub-id pub-id-type="pmid">27091411</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navarra</surname><given-names>J</given-names></name><name><surname>Vatakis</surname><given-names>A</given-names></name><name><surname>Zampini</surname><given-names>M</given-names></name><name><surname>Soto-Faraco</surname><given-names>S</given-names></name><name><surname>Humphreys</surname><given-names>W</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Exposure to asynchronous audiovisual speech extends the temporal window for audiovisual integration</article-title><source>Brain Research. Cognitive Brain Research</source><volume>25</volume><fpage>499</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1016/j.cogbrainres.2005.07.009</pub-id><pub-id pub-id-type="pmid">16137867</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navarra</surname><given-names>J</given-names></name><name><surname>Hartcher-O’Brien</surname><given-names>J</given-names></name><name><surname>Piazza</surname><given-names>E</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Adaptation to audiovisual asynchrony modulates the speeded detection of sound</article-title><source>PNAS</source><volume>106</volume><fpage>9169</fpage><lpage>9173</lpage><pub-id pub-id-type="doi">10.1073/pnas.0810486106</pub-id><pub-id pub-id-type="pmid">19458252</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Donohue</surname><given-names>M</given-names></name><name><surname>Lacherez</surname><given-names>P</given-names></name><name><surname>Yamamoto</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Musical training refines audiovisual integration but does not influence temporal recalibration</article-title><source>Scientific Reports</source><volume>12</volume><elocation-id>15292</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-022-19665-9</pub-id><pub-id pub-id-type="pmid">36097277</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petrini</surname><given-names>K</given-names></name><name><surname>Denis</surname><given-names>G</given-names></name><name><surname>Love</surname><given-names>SA</given-names></name><name><surname>Nardini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Combining the senses: The role of experience- and task-dependent mechanisms in the development of audiovisual simultaneity perception</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>46</volume><fpage>1105</fpage><lpage>1117</lpage><pub-id pub-id-type="doi">10.1037/xhp0000827</pub-id><pub-id pub-id-type="pmid">32718153</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pöppel</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1988">1988</year><source>Mindworks: Time and Conscious Experience</source><publisher-name>Harcourt Brace Jo-vanovich</publisher-name></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roach</surname><given-names>NW</given-names></name><name><surname>Heron</surname><given-names>J</given-names></name><name><surname>Whitaker</surname><given-names>D</given-names></name><name><surname>McGraw</surname><given-names>PV</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Asynchrony adaptation reveals neural population code for audio-visual timing</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>278</volume><fpage>1314</fpage><lpage>1322</lpage><pub-id pub-id-type="doi">10.1098/rspb.2010.1737</pub-id><pub-id pub-id-type="pmid">20961905</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohde</surname><given-names>M</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>To lead and to lag - forward and backward recalibration of perceived visuo-motor simultaneity</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>599</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00599</pub-id><pub-id pub-id-type="pmid">23346063</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohde</surname><given-names>M</given-names></name><name><surname>Greiner</surname><given-names>L</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Asymmetries in visuomotor recalibration of time perception: Does causal binding distort the window of integration?</article-title><source>Acta Psychologica</source><volume>147</volume><fpage>127</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1016/j.actpsy.2013.07.011</pub-id><pub-id pub-id-type="pmid">23928564</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname><given-names>T</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sensory reliability shapes perceptual inference via two mechanisms</article-title><source>Journal of Vision</source><volume>15(5)</volume><elocation-id>22</elocation-id><pub-id pub-id-type="doi">10.1167/15.5.22</pub-id><pub-id pub-id-type="pmid">26067540</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roseboom</surname><given-names>W</given-names></name><name><surname>Arnold</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Twice upon a time: multiple concurrent temporal recalibrations of audiovisual speech</article-title><source>Psychological Science</source><volume>22</volume><fpage>872</fpage><lpage>877</lpage><pub-id pub-id-type="doi">10.1177/0956797611413293</pub-id><pub-id pub-id-type="pmid">21690312</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roseboom</surname><given-names>W</given-names></name><name><surname>Kawabe</surname><given-names>T</given-names></name><name><surname>Nishida</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Audio-visual temporal recalibration can be constrained by content cues regardless of spatial overlap</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>189</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00189</pub-id><pub-id pub-id-type="pmid">23658549</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roseboom</surname><given-names>W</given-names></name><name><surname>Linares</surname><given-names>D</given-names></name><name><surname>Nishida</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sensory adaptation for timing perception</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>282</volume><elocation-id>20142833</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2014.2833</pub-id><pub-id pub-id-type="pmid">25788590</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>Y</given-names></name><name><surname>Toyoizumi</surname><given-names>T</given-names></name><name><surname>Aihara</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Bayesian inference explains perception of unity and ventriloquism aftereffect: Identification of common sources of audiovisual stimuli</article-title><source>Neural Computation</source><volume>19</volume><fpage>3335</fpage><lpage>3355</lpage><pub-id pub-id-type="doi">10.1162/neco.2007.19.12.3335</pub-id><pub-id pub-id-type="pmid">17970656</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>Y</given-names></name><name><surname>Aihara</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A Bayesian model of sensory adaptation</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e19377</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0019377</pub-id><pub-id pub-id-type="pmid">21541346</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Comparing Bayesian models for simultaneity judgement with different causal assumptions</article-title><source>Journal of Mathematical Psychology</source><volume>102</volume><elocation-id>102521</elocation-id><pub-id pub-id-type="doi">10.1016/j.jmp.2021.102521</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>KA</given-names></name><name><surname>Bavelier</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Components of visual prior entry</article-title><source>Cognitive Psychology</source><volume>47</volume><fpage>333</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1016/s0010-0285(03)00035-5</pub-id><pub-id pub-id-type="pmid">14642288</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seriès</surname><given-names>P</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Is the homunculus “aware” of sensory adaptation?</article-title><source>Neural Computation</source><volume>21</volume><fpage>3271</fpage><lpage>3304</lpage><pub-id pub-id-type="doi">10.1162/neco.2009.09-08-869</pub-id><pub-id pub-id-type="pmid">19686064</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shams</surname><given-names>L</given-names></name><name><surname>Beierholm</surname><given-names>UR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Causal inference in perception</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>425</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.07.001</pub-id><pub-id pub-id-type="pmid">20705502</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spence</surname><given-names>C</given-names></name><name><surname>Squire</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Multisensory integration: Maintaining the perception of synchrony</article-title><source>Current Biology</source><volume>13</volume><fpage>R519</fpage><lpage>R521</lpage><pub-id pub-id-type="doi">10.1016/s0960-9822(03)00445-7</pub-id><pub-id pub-id-type="pmid">12842029</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sternberg</surname><given-names>S</given-names></name><name><surname>Knoll</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="1973">1973</year><chapter-title>The perception of temporal order: fundamental issues and a general model</chapter-title><person-group person-group-type="editor"><name><surname>Kornblum</surname><given-names>S</given-names></name></person-group><source>Attention and Performance IV</source><publisher-name>Academic Press</publisher-name><fpage>625</fpage><lpage>685</lpage></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>A</given-names></name><name><surname>Asakawa</surname><given-names>K</given-names></name><name><surname>Imai</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The change in perceptual synchrony between auditory and visual speech after exposure to asynchronous speech</article-title><source>Neuroreport</source><volume>22</volume><fpage>684</fpage><lpage>688</lpage><pub-id pub-id-type="doi">10.1097/WNR.0b013e32834a2724</pub-id><pub-id pub-id-type="pmid">21817926</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Burg</surname><given-names>E</given-names></name><name><surname>Alais</surname><given-names>D</given-names></name><name><surname>Cass</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rapid recalibration to audiovisual asynchrony</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>14633</fpage><lpage>14637</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1182-13.2013</pub-id><pub-id pub-id-type="pmid">24027264</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vatakis</surname><given-names>A</given-names></name><name><surname>Navarra</surname><given-names>J</given-names></name><name><surname>Soto-Faraco</surname><given-names>S</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Temporal recalibration during asynchronous audiovisual speech perception</article-title><source>Experimental Brain Research</source><volume>181</volume><fpage>173</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1007/s00221-007-0918-z</pub-id><pub-id pub-id-type="pmid">17431598</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vatakis</surname><given-names>A</given-names></name><name><surname>Navarra</surname><given-names>J</given-names></name><name><surname>Soto-Faraco</surname><given-names>S</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Audiovisual temporal adaptation of speech: Temporal order versus simultaneity judgments</article-title><source>Experimental Brain Research</source><volume>185</volume><fpage>521</fpage><lpage>529</lpage><pub-id pub-id-type="doi">10.1007/s00221-007-1168-9</pub-id><pub-id pub-id-type="pmid">17962929</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vroomen</surname><given-names>J</given-names></name><name><surname>Keetels</surname><given-names>M</given-names></name><name><surname>de Gelder</surname><given-names>B</given-names></name><name><surname>Bertelson</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Recalibration of temporal order perception by exposure to audio-visual asynchrony</article-title><source>Brain Research. Cognitive Brain Research</source><volume>22</volume><fpage>32</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1016/j.cogbrainres.2004.07.003</pub-id><pub-id pub-id-type="pmid">15561498</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vroomen</surname><given-names>J</given-names></name><name><surname>Keetels</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Perception of intersensory synchrony: A tutorial review</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>72</volume><fpage>871</fpage><lpage>884</lpage><pub-id pub-id-type="doi">10.3758/APP.72.4.871</pub-id><pub-id pub-id-type="pmid">20436185</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>K</given-names></name><name><surname>Körding</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Relevance of error: what drives motor adaptation?</article-title><source>Journal of Neurophysiology</source><volume>101</volume><fpage>655</fpage><lpage>664</lpage><pub-id pub-id-type="doi">10.1152/jn.90545.2008</pub-id><pub-id pub-id-type="pmid">19019979</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wozny</surname><given-names>DR</given-names></name><name><surname>Beierholm</surname><given-names>UR</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Probability matching as a computational strategy used in perception</article-title><source>PLOS Computational Biology</source><volume>6</volume><elocation-id>e1000871</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000871</pub-id><pub-id pub-id-type="pmid">20700493</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarrow</surname><given-names>K</given-names></name><name><surname>Jahn</surname><given-names>N</given-names></name><name><surname>Durant</surname><given-names>S</given-names></name><name><surname>Arnold</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>Shifts of criteria or neural timing? The assumptions underlying timing perception studies</article-title><source>Consciousness and Cognition</source><volume>20</volume><fpage>1518</fpage><lpage>1531</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2011.07.003</pub-id><pub-id pub-id-type="pmid">21807537</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarrow</surname><given-names>K</given-names></name><name><surname>Roseboom</surname><given-names>W</given-names></name><name><surname>Arnold</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>Spatial grouping resolves ambiguity to drive temporal recalibration</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>37</volume><fpage>1657</fpage><lpage>1661</lpage><pub-id pub-id-type="doi">10.1037/a0024235</pub-id><pub-id pub-id-type="pmid">21688937</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarrow</surname><given-names>K</given-names></name><name><surname>Minaei</surname><given-names>S</given-names></name><name><surname>Arnold</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A model-based comparison of three theories of audiovisual temporal recalibration</article-title><source>Cognitive Psychology</source><volume>83</volume><fpage>54</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2015.10.002</pub-id><pub-id pub-id-type="pmid">26545105</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarrow</surname><given-names>K</given-names></name><name><surname>Kohl</surname><given-names>C</given-names></name><name><surname>Segasby</surname><given-names>T</given-names></name><name><surname>Kaur Bansal</surname><given-names>R</given-names></name><name><surname>Rowe</surname><given-names>P</given-names></name><name><surname>Arnold</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Neural-latency noise places limits on human sensitivity to the timing of events</article-title><source>Cognition</source><volume>222</volume><elocation-id>105012</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2021.105012</pub-id><pub-id pub-id-type="pmid">34998243</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarrow</surname><given-names>K</given-names></name><name><surname>Solomon</surname><given-names>JA</given-names></name><name><surname>Arnold</surname><given-names>DH</given-names></name><name><surname>Roseboom</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The best fitting of three contemporary observer models reveals how participants’ strategy influences the window of subjective synchrony</article-title><source>Journal of Experimental Psychology</source><volume>49</volume><fpage>1534</fpage><lpage>1563</lpage><pub-id pub-id-type="doi">10.1037/xhp0001154</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>B</given-names></name><name><surname>Bi</surname><given-names>C</given-names></name><name><surname>Yin</surname><given-names>H</given-names></name><name><surname>Huang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Audiovisual temporal recalibration: space-based versus context-based</article-title><source>Perception</source><volume>41</volume><fpage>1218</fpage><lpage>1233</lpage><pub-id pub-id-type="doi">10.1068/p7243</pub-id><pub-id pub-id-type="pmid">23469702</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Comparing atheoretical models of audiovisual temporal recalibration</title><p><xref ref-type="fig" rid="fig2">Figure 2</xref> in the main text shows raw psychometric data with fit psychometric functions (panel A) and estimates of PSS shift across sessions (panel B). These estimates are from fitting an <italic>atheoretical</italic> model that estimates PSS shift without reference to an underlying model of how recalibration comes about. Rather, it aims to show the pattern of recalibration from the data alone. Here, we describe how those estimates were computed and compare four different models of recalibration for this purpose. These four models vary based on two factors: the use of either a Gaussian or an exponential measurement distribution, and the assumption of either a bias shift or a criterion shift. Next, we describe these models followed by a model comparison.</p><sec sec-type="appendix" id="s8-1"><title>Gaussian vs. exponential measurement distribution</title><p>Models of TOJs typically assume independent sensory channels for each modality, with the measurement error of arrival latency for each channel being either Gaussian (<xref ref-type="bibr" rid="bib49">Schneider and Bavelier, 2003</xref>; <xref ref-type="bibr" rid="bib53">Sternberg and Knoll, 1973</xref>) or exponentially distributed (<xref ref-type="bibr" rid="bib14">García-Pérez and Alcalá-Quintana, 2012</xref>; <xref ref-type="bibr" rid="bib37">Petrini et al., 2020</xref>).</p><p>If arrival latencies are corrupted by Gaussian-distributed noise, the measurements of difference between auditory and visual arrival latencies <inline-formula><mml:math id="inf124"><mml:mi>m</mml:mi></mml:math></inline-formula> also have a Gaussian distribution,<disp-formula id="equ12"><label>(S1)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf125"><mml:mi>s</mml:mi></mml:math></inline-formula> is the physical SOA, <italic>β</italic> is the audiovisual bias, and <inline-formula><mml:math id="inf126"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is the sum of the variabilities of auditory and visual latency. This model has a conceptual problem: the continuous Gaussian distribution allows for a non-zero probability of negative arrival latencies, but the auditory or visual signal cannot be registered in the brain before the physical stimulus occurs.</p><p>The exponential distribution of arrival latencies avoids this problem. As in the main text, the measurement of audiovisual latency <inline-formula><mml:math id="inf127"><mml:mi>m</mml:mi></mml:math></inline-formula> has a double-exponential distribution (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). For both models, the order judgment is then made by comparing the arrival-latency difference and decision criteria.</p></sec><sec sec-type="appendix" id="s8-2"><title>Bias-shift vs. criterion-shift model</title><p>Audiovisual temporal recalibration can result from a change in the audiovisual bias or decision criteria (<xref ref-type="bibr" rid="bib62">Yarrow et al., 2011a</xref>; <xref ref-type="bibr" rid="bib66">Yarrow et al., 2023</xref>). The asymmetry observed in audiovisual temporal recalibration can also be modeled by allowing for asymmetric decision criteria by expanding the simultaneity window on one side (<xref ref-type="bibr" rid="bib36">O’Donohue et al., 2022</xref>; <xref ref-type="bibr" rid="bib41">Rohde et al., 2014</xref>; <xref ref-type="bibr" rid="bib62">Yarrow et al., 2011a</xref>). For each model, we assume that either the bias or criterion parameter changes after the recalibration phase, and is restored to a default value before the pre-test of a subsequent session. We fit the TOJ responses jointly across pre- and post-tests and all nine sessions. These TOJ models include a bias or a criterion shift as a session-dependent free parameter and keep the other parameters constant across sessions.</p><sec sec-type="appendix" id="s8-2-1"><title>Specification of the bias-shift model</title><p>In the decision stage, the observer compares the measured SOA with a simultaneity temporal window bounded by symmetric criteria ±<italic>c</italic>. Including a lapse rate <italic>λ</italic>, the psychometric functions are<disp-formula id="equ13"><label>(S2)</label><mml:math id="m13"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="0.7em 0.7em 0.3em" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>3</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>3</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>&lt;</mml:mo><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is a Gaussian or double-exponential distribution.</p><p>In the pre-test, the parameters and the resulting predicted psychometric functions are the same across sessions. In the post-test, there is a free parameter for the shift of audiovisual bias <italic>β</italic> after exposure to the adapter SOA within that session. Therefore, there are 9 free bias parameters, <inline-formula><mml:math id="inf129"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mtext>post</mml:mtext><mml:mo separator="true">,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, one for each of the 9 sessions <inline-formula><mml:math id="inf130"><mml:mi>j</mml:mi></mml:math></inline-formula>. In sum, the parameter set <inline-formula><mml:math id="inf131"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi></mml:mrow></mml:math></inline-formula> includes either 14 free parameters, <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mtext>post</mml:mtext><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, using a double-exponential distribution, or 13 free parameters, <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mtext>post</mml:mtext><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, for the Gaussian measurement distribution.</p></sec><sec sec-type="appendix" id="s8-2-2"><title>Specification of the criterion-shift model</title><p>In the criterion-shift model, the simultaneity window is defined by a lower criterion <inline-formula><mml:math id="inf134"><mml:msub><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula> and an upper criterion <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>c</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math></inline-formula>, which can be asymmetric. The measurement distribution is compared with the simultaneity temporal window to obtain the psychometric functions for the three responses. The psychometric functions are the same as <xref ref-type="disp-formula" rid="equ13">Equation S2</xref> except that we allow for an expansion of the simultaneity window in the post-test phase, thus replacing <inline-formula><mml:math id="inf136"><mml:mi>c</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="inf137"><mml:msub><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula> in the definition of <inline-formula><mml:math id="inf138"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf139"><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf140"><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in the definition of <inline-formula><mml:math id="inf141"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>In the exposure phase, if the adapter SOA is inside the simultaneity window, no recalibration occurs. On the other hand, if it is outside the simultaneity window, recalibration occurs by shifting outward the criterion that is closer to the adapter SOA, i.e., expanding the range considered simultaneous on that side. The other side of the criterion remains unchanged. We assume that in a given session only the criterion on the same side as the adapter is ever changed. Thus, each session is characterized by a free parameter for how much that criterion is shifted (<inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for session <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), so that either <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is updated to <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is updated to <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. All pre-test sessions share all parameters (and predicted psychometric functions). Taken together, the parameter set <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> includes either 14 free parameters, <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, for the double-exponential model, or 13 free parameters for the Gaussian model: <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec></sec><sec sec-type="appendix" id="s8-3"><title>Model log-likelihood</title><p>We fit each model <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> separately using the same variational Bayesian Monte Carlo procedure. The TOJ data from the pre- and post-test phases of all sessions were fit jointly. Note that each model does not simulate recalibration, but fits session-dependent free parameters instead.</p><p>The parameter set <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is specified above for each model, and <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the dataset containing each type of response from <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> phases, <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> sessions, and <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> trials <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>300</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. We maximized the log-likelihood of model parameters given the data,<disp-formula id="equ14"><label>(S3)</label><mml:math id="m14"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt 0.7em 0.3em" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mtext>pre</mml:mtext><mml:mo>,</mml:mo><mml:mtext>post</mml:mtext><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where the <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are indicator variables that are equal to one when the corresponding key was pressed, and zero otherwise. The psychometric functions (e.g., <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) for the pre-test are the same across sessions and differ for the post-test across sessions due to recalibration.</p></sec><sec sec-type="appendix" id="s8-4"><title>Model comparison</title><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Model comparison results of the atheoretical models.</title><p>The model with the largest log model evidence is determined to be the best-fitting model for each participant. The table displays the log Bayes factor of each model relative to the best-fitting model for each participant. Thus, the best-fitting model has a relative log Bayes factor of zero. Larger values for other models indicate stronger support for the best-fitting model. The vertical histogram shows the number of participants for which each model was the best fit, based on the criterion that a model is rejected if its relative log Bayes factor is greater than log(10) = 2.3.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app1-fig1-v1.tif"/></fig><p>Model comparison based on model evidence found that the bias-shift model with exponential measurement distribution captured the data of most participants. We continued to use this model as the atheoretical model as the baseline to compare with other recalibration models in the main text.</p></sec></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>Individual-level empirical data of the recalibration effect</title><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>Individual-level recalibration effect.</title><p>(<bold>A</bold>) Individual recalibration effect estimated from the atheoretical model that assumes a bias shift and double-exponential measurement distribution. Error bars: 95% confidence interval of recalibration amount from 1000 bootstrapped datasets for that participant. (<bold>B</bold>) The 95% confidence interval of the asymmetry index (i.e. summed recalibration amount across adapter SOAs) from 1000 bootstrapped datasets for each participant. The confidence interval for all except the last participant excluded zero (CI of S9: [–0.093, 0.006]), suggesting a general asymmetry in recalibration.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app2-fig1-v1.tif"/></fig></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s10"><title>Derivation of the double-exponential distribution of arrival-latency difference</title><p>In this section, we derive the asymmetric double-exponential distribution of measured SOA that results from exponential arrival-time distributions (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref> in the main text). The stimulus onsets occur at times <inline-formula><mml:math id="inf160"><mml:msub><mml:mi>s</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf161"><mml:msub><mml:mi>s</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula>. Each modality has a fixed delay until detection (<inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). The probability distribution of further delays is an exponential distribution with time constant <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, the arrival time for auditory detection is <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where the random delay <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is distributed as <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, and similarly for the visual stimulus. We are interested in measurements of SOA, i.e., the distribution of <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. We now derive the distribution for the random variable <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>We assume that visual and auditory delays (<inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) are independent. Thus, to compute the density <inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for a particular relative delay <inline-formula><mml:math id="inf174"><mml:mi>d</mml:mi></mml:math></inline-formula>, we integrate over all pairs of delays <inline-formula><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that correspond to that relative delay such that both individual delays are possible (i.e. both are non-negative). We treat the cases <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> separately.</p><p><bold>Case 1</bold>: <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi>d</mml:mi><mml:mo>≥</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula><disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, the lower limit on the integral is zero so that both delays are non-negative. Substituting the PDFs:<disp-formula id="equ16"><label>(S4)</label><mml:math id="m16"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="0.7em 0.7em 0.7em 0.7em 0.3em" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p><bold>Case 2</bold>: <inline-formula><mml:math id="inf180"><mml:mrow><mml:mi>d</mml:mi><mml:mo>≤</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula><disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>V</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>A</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, the lower limit on the integral is <inline-formula><mml:math id="inf181"><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> (which is a positive value) so that both delays are non-negative. Substituting the PDFs:<disp-formula id="equ18"><label>(S5)</label><mml:math id="m18"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="0.7em 0.7em 0.7em 0.7em 0.7em 0.3em" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Combining the two cases, the PDF of <inline-formula><mml:math id="inf182"><mml:mi>D</mml:mi></mml:math></inline-formula> is<disp-formula id="equ19"><label>(S6)</label><mml:math id="m19"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="0.6em 0.2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mtext>if </mml:mtext><mml:mi>d</mml:mi><mml:mo>≤</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mtext>if </mml:mtext><mml:mi>d</mml:mi><mml:mo>≥</mml:mo><mml:mn>0.</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The difference of the random delays follows a double exponential distribution centered at zero, with different decay rates on either side of zero determined by <inline-formula><mml:math id="inf183"><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf184"><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula>. Adding in the fixed relative delays (<inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>), we arrive at <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> in the main text.</p></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s11"><title>Individual-level model comparison of the recalibration models</title><fig id="app4fig1" position="float"><label>Appendix 4—figure 1.</label><caption><title>Model comparison results of the recalibration models.</title><p>The table displays log Bayes factor relative to the best-fitting model for each participant. Thus, the best-fitting model has a relative log Bayes factor of 0. Larger values for other models indicate stronger support for the best-fitting model. The vertical histogram shows the number of participants for which each model was the best fit, based on the criterion that a model is rejected if the log Bayes factor relative to the best-fitting model is greater than log(10) = 2.3.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app4-fig1-v1.tif"/></fig></sec></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s12"><title>Individual-level model predictions of recalibration effects</title><fig id="app5fig1" position="float"><label>Appendix 5—figure 1.</label><caption><title>Empirical data for the recalibration effect for each participant and model predictions of the causal-inference recalibration model with modality-specific precision.</title><p>Each panel represents a single participant. Black dots: empirical estimates from the best atheoretical model (the bias-shift model with exponential likelihood). Error bars: the 95% bootstrapped confidence intervals. Red lines: predictions of the recalibration model from the best-fitting parameter estimates for that participant.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app5-fig1-v1.tif"/></fig><fig id="app5fig2" position="float"><label>Appendix 5—figure 2.</label><caption><title>Empirical data of the recalibration effect for each participant and model predictions of the asynchrony-contingent recalibration model with modality-specific precision.</title><p>Each panel represents a single participant. Black dots: empirical estimates from the best atheoretical model (the bias-shift model with exponential likelihood). Error bars: the 95% bootstrapped confidence intervals. Red lines: predictions of the recalibration model from the best-fitting parameter estimates for that participant.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app5-fig2-v1.tif"/></fig><fig id="app5fig3" position="float"><label>Appendix 5—figure 3.</label><caption><title>Empirical data of the recalibration effect for each participant and model predictions of the asynchrony-correction recalibration model with modality-specific precision.</title><p>Each panel represents a single participant. Black dots: empirical estimates from the best atheoretical model (the bias-shift model with exponential likelihood). Error bars: the 95% bootstrapped confidence intervals. Red lines: predictions of the recalibration model from the best-fitting parameter estimates for that participant.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app5-fig3-v1.tif"/></fig></sec></app><app id="appendix-6"><title>Appendix 6</title><sec sec-type="appendix" id="s13"><title>Scrutinizing the causal-inference model with modality-specific precision</title><table-wrap id="app6table1" position="float"><label>Appendix 6—table 1.</label><caption><title>Parameter estimates of individual participants.</title><p>All parameters except <inline-formula><mml:math id="inf186"><mml:mrow><mml:mi>λ</mml:mi><mml:mo separator="true">,</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext/><mml:mi>common</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and <italic>α</italic> are in milliseconds.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom"><inline-formula><mml:math id="inf187"><mml:msub><mml:mi>β</mml:mi><mml:mtext>pre</mml:mtext></mml:msub></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf188"><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf189"><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula></th><th align="left" valign="bottom">Criterion</th><th align="left" valign="bottom"><italic>λ</italic></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf190"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="left" valign="bottom"><italic>α</italic></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf191"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf192"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="left" valign="bottom">S1</td><td align="char" char="." valign="bottom">–66.78</td><td align="char" char="." valign="bottom">49.76</td><td align="char" char="." valign="bottom">67.89</td><td align="char" char="." valign="bottom">91.74</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">0.671</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">69.51</td><td align="char" char="." valign="bottom">214.24</td></tr><tr><td align="left" valign="bottom">S2</td><td align="char" char="." valign="bottom">45.86</td><td align="char" char="." valign="bottom">90.41</td><td align="char" char="." valign="bottom">144.48</td><td align="char" char="." valign="bottom">185.86</td><td align="char" char="." valign="bottom">0.047</td><td align="char" char="." valign="bottom">0.498</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">70.66</td><td align="char" char="." valign="bottom">390.54</td></tr><tr><td align="left" valign="bottom">S3</td><td align="char" char="." valign="bottom">10.94</td><td align="char" char="." valign="bottom">94.48</td><td align="char" char="." valign="bottom">86.64</td><td align="char" char="." valign="bottom">190.74</td><td align="char" char="." valign="bottom">0.008</td><td align="char" char="." valign="bottom">0.504</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">40.05</td><td align="char" char="." valign="bottom">260.65</td></tr><tr><td align="left" valign="bottom">S4</td><td align="char" char="." valign="bottom">–4.42</td><td align="char" char="." valign="bottom">80.78</td><td align="char" char="." valign="bottom">113.14</td><td align="char" char="." valign="bottom">105.57</td><td align="char" char="." valign="bottom">0.054</td><td align="char" char="." valign="bottom">0.221</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">17.99</td><td align="char" char="." valign="bottom">267.67</td></tr><tr><td align="left" valign="bottom">S5</td><td align="char" char="." valign="bottom">30.15</td><td align="char" char="." valign="bottom">89.35</td><td align="char" char="." valign="bottom">66.38</td><td align="char" char="." valign="bottom">109.35</td><td align="char" char="." valign="bottom">0.049</td><td align="char" char="." valign="bottom">0.571</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">69.53</td><td align="char" char="." valign="bottom">263.09</td></tr><tr><td align="left" valign="bottom">S6</td><td align="char" char="." valign="bottom">–9.79</td><td align="char" char="." valign="bottom">41.69</td><td align="char" char="." valign="bottom">62.88</td><td align="char" char="." valign="bottom">85.54</td><td align="char" char="." valign="bottom">0.029</td><td align="char" char="." valign="bottom">0.345</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">69.71</td><td align="char" char="." valign="bottom">528.98</td></tr><tr><td align="left" valign="bottom">S7</td><td align="char" char="." valign="bottom">–31.37</td><td align="char" char="." valign="bottom">68.79</td><td align="char" char="." valign="bottom">60.78</td><td align="char" char="." valign="bottom">59.69</td><td align="char" char="." valign="bottom">0.051</td><td align="char" char="." valign="bottom">0.864</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">18.57</td><td align="char" char="." valign="bottom">218.73</td></tr><tr><td align="left" valign="bottom">S8</td><td align="char" char="." valign="bottom">0.16</td><td align="char" char="." valign="bottom">66.37</td><td align="char" char="." valign="bottom">76.78</td><td align="char" char="." valign="bottom">63.25</td><td align="char" char="." valign="bottom">0.017</td><td align="char" char="." valign="bottom">0.378</td><td align="char" char="." valign="bottom">0.008</td><td align="char" char="." valign="bottom">26.48</td><td align="char" char="." valign="bottom">247.67</td></tr><tr><td align="left" valign="bottom">S9</td><td align="char" char="." valign="bottom">14.03</td><td align="char" char="." valign="bottom">33.22</td><td align="char" char="." valign="bottom">41.23</td><td align="char" char="." valign="bottom">47.12</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">0.632</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">73.70</td><td align="char" char="." valign="bottom">575.44</td></tr><tr><td align="left" valign="bottom">Mean</td><td align="char" char="." valign="bottom">–1.25</td><td align="char" char="." valign="bottom">68.32</td><td align="char" char="." valign="bottom">80.02</td><td align="char" char="." valign="bottom">104.32</td><td align="char" char="." valign="bottom">0.031</td><td align="char" char="." valign="bottom">0.520</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">50.69</td><td align="char" char="." valign="bottom">329.67</td></tr><tr><td align="left" valign="bottom">SEM</td><td align="char" char="." valign="bottom">11.10</td><td align="char" char="." valign="bottom">7.51</td><td align="char" char="." valign="bottom">10.41</td><td align="char" char="." valign="bottom">17.32</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">0.064</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">8.16</td><td align="char" char="." valign="bottom">45.53</td></tr><tr><td align="left" valign="bottom">Lower bound</td><td align="char" char="." valign="bottom">–200</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">100</td></tr><tr><td align="left" valign="bottom">Upper bound</td><td align="char" char="." valign="bottom">200</td><td align="char" char="." valign="bottom">200</td><td align="char" char="." valign="bottom">200</td><td align="char" char="." valign="bottom">350</td><td align="char" char="." valign="bottom">0.06</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">0.02</td><td align="char" char="." valign="bottom">300</td><td align="char" char="." valign="bottom">1000</td></tr></tbody></table></table-wrap><fig id="app6fig1" position="float"><label>Appendix 6—figure 1.</label><caption><title>Data and predictions of the causal-inference, modality-specific-precision model for the temporal-order-judgment responses for each participant (S1–S3).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app6-fig1-v1.tif"/></fig><fig id="app6fig2" position="float"><label>Appendix 6—figure 2.</label><caption><title>Data and predictions of the causal-inference, modality-specific-precision model for the temporal-order-judgment responses for each participant (S4–S6).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app6-fig2-v1.tif"/></fig><fig id="app6fig3" position="float"><label>Appendix 6—figure 3.</label><caption><title>Data and predictions of the causal-inference, modality-specific-precision model for the temporal-order-judgment responses for each participant (S7–S9).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app6-fig3-v1.tif"/></fig></sec></app><app id="appendix-7"><title>Appendix 7</title><sec sec-type="appendix" id="s14"><title>Simulation of the causal-inference model</title><fig id="app7fig1" position="float"><label>Appendix 7—figure 1.</label><caption><title>The effect of the spread of the prior over SOA given a common cause on recalibration.</title><p>(<bold>A</bold>) A smaller spread increases the recalibration magnitude, but only for a small SOA range where the probability of a common cause is higher. (<bold>B</bold>) The effect of the audiovisual temporal bias on recalibration. Audiovisual temporal bias affects where recalibration starts in the direction of the other modality, but its effects are reduced for large adapter SOAs. Overall, temporal bias shifts the recalibration function laterally, with minimal impact on the asymmetry of recalibration effect.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app7-fig1-v1.tif"/></fig></sec></app><app id="appendix-8"><title>Appendix 8</title><sec sec-type="appendix" id="s15"><title>Simulation of a population-code recalibration model</title><fig id="app8fig1" position="float"><label>Appendix 8—figure 1.</label><caption><title>Simulation of a population-code model with varying adapter SOA.</title><p>(<bold>A</bold>) Left panel: Tuning curves of a neural population adapted to a 175 ms SOA, with 29 neurons having preferred SOAs that range from –500 to 500 ms. Middle panel: Neural responses to a stimulus with 0 ms SOA before (solid) and after (dashed) adaptation to a 175 ms SOA. Green: physical 0 ms SOA. Purple: estimated SOA based on an adaptation-unaware maximum-likelihood readout of the population response after adaptation. Right panel: Bias (i.e. estimated SOA minus physical SOA) across different stimulus SOAs after adaptation to a 175 ms SOA. (<bold>B</bold>) The same as in (<bold>A</bold>) after adaptation to a 525 ms SOA. (<bold>C</bold>) Bias as a function of stimulus SOA for several adapter SOAs. (<bold>D</bold>) Recalibration effect (i.e. the points of subjective simultaneity [PSS]: the stimulus SOA leading to an estimate of a 0 ms SOA) assuming an unbiased PSS before adaptation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app8-fig1-v1.tif"/></fig><p>We simulated the population-code model of audiovisual temporal recalibration (<xref ref-type="bibr" rid="bib39">Roach et al., 2011</xref>; <xref ref-type="bibr" rid="bib64">Yarrow et al., 2015</xref>) to examine its prediction of the recalibration effect (i.e. the shift of the PSS) for different adapter SOAs. This model proposes that a population of neurons is tuned to different SOAs, and recalibration occurs through a selective gain reduction around the adapter SOA (<xref ref-type="fig" rid="app8fig1">Appendix 8—figure 1A</xref>, left panel). We assume a maximum-likelihood decoder that is unaware of the adaptation, resulting in a bias: estimates are shifted away from the adapted SOA (<xref ref-type="fig" rid="app8fig1">Appendix 8—figure 1A</xref>, middle panel; <xref ref-type="bibr" rid="bib25">Jazayeri and Movshon, 2006</xref>; <xref ref-type="bibr" rid="bib50">Seriès et al., 2009</xref>). The bias (i.e. the difference between estimated and physical SOA) is nonuniform as a function of the test SOA (<xref ref-type="fig" rid="app8fig1">Appendix 8—figure 1A</xref>, right panel).</p><p>When the adapter SOA is close to or exceeds the range of preferred SOAs across the neural population, it elicits less adaptation across neurons (<xref ref-type="fig" rid="app8fig1">Appendix 8—figure 1B</xref>, left panel) and results in a smaller change in population response (<xref ref-type="fig" rid="app8fig1">Appendix 8—figure 1B</xref>, middle panel). Consequently, the bias is significantly reduced as the adapter SOA increases (<xref ref-type="fig" rid="app8fig1">Appendix 8—figure 1B</xref>, right panel).</p><p>We simulated the bias in estimation as a function of stimulus SOA for this neural population when adapted to a range of different adapter SOAs (<xref ref-type="fig" rid="app8fig1">Appendix 8—figure 1C</xref>). These adapter SOAs produce a consistent nonuniform bias across stimulus SOAs. The overall magnitude of the bias is largest for small adapter SOAs.</p><p>Finally, we determined the recalibration effect by calculating the PSS after adaptation for each adapter SOA, assuming an unbiased PSS before adaptation (<xref ref-type="fig" rid="app8fig1">Appendix 8—figure 1D</xref>). For each adapter SOA we determined the stimulus SOA that, after adaptation, resulted in an SOA estimate of 0 ms, i.e., the stimulus SOA plus the bias is zero. The results show that the population-code model can predict nonlinearity in recalibration because the strength of adaptation decreases as the adapter SOA approaches the bounds of the range of preferred SOAs across the neural population. However, it cannot account for the asymmetry of recalibration observed in the empirical data, under the assumption that neurons representing relative timing, a supramodal attribute, have similar selectivity in this model regardless of which modality leads.</p></sec></app><app id="appendix-9"><title>Appendix 9</title><sec sec-type="appendix" id="s16"><title>Exclusion of an outlier participant</title><fig id="app9fig1" position="float"><label>Appendix 9—figure 1.</label><caption><title>Temporal-order judgments from the outlier participant.</title><p>We excluded this participant's data because the probabilities of the three responses were similar across test stimulus-onset asynchronies (SOAs). These responses were too noisy to constrain the psychometric function.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app9-fig1-v1.tif"/></fig></sec></app><app id="appendix-10"><title>Appendix 10</title><sec sec-type="appendix" id="s17"><title>Performance in the oddball-detection task</title><fig id="app10fig1" position="float"><label>Appendix 10—figure 1.</label><caption><title>Oddball-detection performance in the exposure and post-test phases.</title><p>Each dot represents a participant. In each phase, we combined responses across sessions and computed hit rates and false-alarm rates for auditory and visual oddballs. For each modality, hit and false-alarm rates were the probability of reporting an oddball when an oddball was presented and when it was absent, respectively. The resulting <inline-formula><mml:math id="inf193"><mml:msup><mml:mi>d</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup></mml:math></inline-formula> values for each participant confirm that they engaged in the task and attended to both modalities during the exposure phase. In general, participants showed a higher <inline-formula><mml:math id="inf194"><mml:msup><mml:mi>d</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup></mml:math></inline-formula> for auditory oddballs compared to visual ones, despite our attempt to equate performance levels. Furthermore, the <inline-formula><mml:math id="inf195"><mml:msup><mml:mi>d</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup></mml:math></inline-formula> remained consistent from the exposure to post-test phases, indicating similar performance in the top-up trials to performance during the exposure phase.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app10-fig1-v1.tif"/></fig></sec></app><app id="appendix-11"><title>Appendix 11</title><sec sec-type="appendix" id="s18"><title>Model recovery of recalibration models</title><fig id="app11fig1" position="float"><label>Appendix 11—figure 1.</label><caption><title>Confusion matrix resulting from the model-recovery simulations.</title><p>Each cell indicates the proportion of the 100 simulations for each model generating the data that were best fit by each of the six models (i.e. the values in each row sum to one).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app11-fig1-v1.tif"/></fig><p>We conducted a model-recovery analysis for the six models described in the main text. In the main text, each model was fit to the data of each participant multiple times and the best-fitting parameter set for that model was selected. This yielded one parameter set for each participant and model. For each model, we randomly selected 100 parameter sets. Each parameter was randomly drawn from either a Gaussian (for unbounded parameters) or log-Gaussian (for parameters restricted to be positive) with mean and SD equal to that of the best-fit parameter sets across participants for that model. We then simulated datasets for each model based on each of the 100 parameter sets. All six models were fit to each simulated dataset.</p><p>We used the same model-comparison procedure as in the main text. All asynchrony-contingent and causal-inference models have satisfactory recovery results, suggesting that these models are identifiable. In addition, the data generated by the asynchrony-correction model were sometimes best fit by causal-inference models but not by the asynchrony-contingent models. This is reasonable since the asynchrony-correction model is similar to the causal-inference model with the value of <inline-formula><mml:math id="inf196"><mml:msub><mml:mi>p</mml:mi><mml:mtext>common</mml:mtext></mml:msub></mml:math></inline-formula> set to one.</p></sec></app><app id="appendix-12"><title>Appendix 12</title><sec sec-type="appendix" id="s19"><title>Model recovery of variants of the causal-inference recalibration models</title><fig id="app12fig1" position="float"><label>Appendix 12—figure 1.</label><caption><title>Confusion matrix resulting from the model-recovery simulations.</title><p>Update model: the alternative model assuming that causal inference modulates recalibration at the bias-update phase. Percept model: the model described in the main text that assumes that causal inference modulates recalibration at the perceptual stage.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app12-fig1-v1.tif"/></fig><p>We compared the causal-inference recalibration model in the main text to a variant in which causal inference modulates recalibration in the update phase. Specifically, the learning rate is modulated by the posterior probability of a common cause, i.e., the probability that the audiovisual measurements arose from the same source. The bias update rule (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) becomes<disp-formula id="equ20"><label>(S7)</label><mml:math id="m20"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We performed a model-recovery analysis comparing these two models. 120 datasets were simulated for each model using the best-fitting parameters from the fit to the raw data and we fit both models to each simulated dataset. The confusion matrix summarizes the probability that the generating model is the best-fitting model. Clearly, the results indicate that these two models are not distinguishable given the amount of data we collected in the main experiment.</p></sec></app><app id="appendix-13"><title>Appendix 13</title><sec sec-type="appendix" id="s20"><title>Parameter recovery of the causal-inference, modality-specific-precision model</title><p>We conducted a parameter recovery analysis of the winning model by simulating 100 datasets. Parameter values were sampled using the group mean and standard deviation across participants of the best-fitting estimates from model fits of the data. Each dataset was fit with the same causal-inference model with modality-specific precision. Key parameters, including audiovisual bias <inline-formula><mml:math id="inf197"><mml:msub><mml:mi>β</mml:mi><mml:mtext>pre</mml:mtext></mml:msub></mml:math></inline-formula>, amount of auditory latency noise <inline-formula><mml:math id="inf198"><mml:msub><mml:mi>τ</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula>, amount of visual latency noise <inline-formula><mml:math id="inf199"><mml:msub><mml:mi>τ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula>, criterion, and lapse rate <italic>λ</italic> showed satisfactory recovery performance. The less accurate recovery of <inline-formula><mml:math id="inf200"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is likely due to a tradeoff with the learning rate <italic>α</italic>, as both parameters influence the update of the audiovisual bias. The less accurate recovery of the widths of the SOA for a common cause (<inline-formula><mml:math id="inf201"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>) and separate causes (<inline-formula><mml:math id="inf202"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>) is acceptable, as their function can still be fulfilled within the respective range.</p><fig id="app13fig1" position="float"><label>Appendix 13—figure 1.</label><caption><title>Predicted value vs. simulated value of each parameter.</title><p>Each dot represents 1 out of 100 simulated datasets. The identity line marks perfect recovery performance.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97765-app13-fig1-v1.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97765.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chait</surname><given-names>Maria</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>In this <bold>valuable</bold> study, Li et al., set out to understand the mechanisms of audiovisual temporal recalibration - the brain's ability to adjust to the latency differences that emerge due to different (distance-dependent) transduction latencies of auditory and visual signals - through psychophysical measurements and modeling. The analysis and specification of a formal model for this process provide <bold>convincing</bold> evidence to support a role for causal inference in recalibration.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97765.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This study asks whether the phenomenon of crossmodal temporal recalibration, i.e. the adjustment of time perception by consistent temporal mismatches across the senses, can be explained by the concept of multisensory causal inference. In particular they ask whether the explanation offered by causal inference better explains temporal recalibration better than a model assuming that crossmodal stimuli are always integrated, regardless of how discrepant they are.</p><p>The study is motivated by previous work in the spatial domain, where it has been shown consistently across studies that the use of crossmodal spatial information is explained by the concept of multisensory causal inference. It is also motivated by the observation that the behavioral data showcasing temporal recalibration feature nonlinearities that, by their nature, cannot be explained by a fixed integration model (sometimes also called mandatory fusion).</p><p>To probe this the authors implemented a sophisticated experiment that probed temporal recalibration in several sessions. They then fit the data using the two classes of candidate models and rely model criteria to provide evidence for their conclusion. The study is sophisticated, conceptually and technically state-of-the-art and theoretically grounded. The data clearly support the authors conclusions.</p><p>I find the conceptual advance somewhat limited. First, by design the fixed integration model cannot explain data with a nonlinear dependency on multisensory discrepancy, as already explained in many studies on spatial multisensory perception. Hence, it is not surprising that the causal inference model better fits the data. Second, and again similar to studies on spatial paradigms, the causal inference model fails to predict the behavioral data for large discrepancies. The model predictions in Figure 5 show the (expected) vanishing recalibration for large delta, while the behavioral data don't' decay to zero. Either the range of tested SOAs is too small to show that both the model and data converge to the same vanishing effect at large SOAs, or the model's formula is not the best for explaining the data. Again, the studies using spatial paradigms have the same problem, but in my view this poses the most interesting question here.</p><p>In my view there is nothing generally wrong with the study, it does extend the 'known' to another type of paradigm. However, it covers little new ground on the conceptual side.</p><p>On that note, the small sample size of n=10 is likely not an issue, but still it is on the very low end for this type of study.</p><p>Comments on revision:</p><p>The revision has addressed most of these points and makes for a much stronger contribution. The issue of sample size remains.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97765.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Li et al.'s goal is to understand the mechanisms of audiovisual temporal recalibration. This is an interesting challenge that the brain readily solves in order to compensate for real-world latency differences in the time of arrival of audio/visual signals. To do this they perform a 3-phase recalibration experiment on 9 observers that involves a temporal order judgment (TOJ) pretest and posttest (in which observers are required to judge whether an auditory and visual stimulus were coincident, auditory leading or visual leading) and a conditioning phase in which participants are exposed to a sequence of AV stimuli with a particular temporal disparity. Participants are required to monitor both streams of information for infrequent oddballs, before being tested again in the TOJ, although this time there are 3 conditioning trials for every 1 TOJ trial. Like many previous studies, they demonstrate that conditioning stimuli shift the point of subjective simultaneity (pss) in the direction of the exposure sequence.</p><p>These shifts are modest - maxing out at around -50 ms for auditory leading sequences and slightly less than that for visual leading sequences. Similar effects are observed even for the longest offsets where it seems unlikely listeners would perceive the stimuli as synchronous (and therefore under a causal inference model you might intuitively expect no recalibration, and indeed simulations in Figure 5 seem to predict exactly that which isn't what most of their human observers did). Overall I think their data contribute evidence that a causal inference step is likely included within the process of recalibration.</p><p>Strengths:</p><p>The manuscript performs comprehensive testing over 9 days and 100s of trials and accompanies this with mathematical models to explain the data. The paper is reasonably clearly written and the data appear to support the conclusions.</p><p>Comments on revision:</p><p>In the revised manuscript the authors incorporate an alternative model (the asynchrony contingent model), and demonstrate that the causal inference model still out performs this. They provide additional analysis with Bayes factors to perform model comparisons, and provide significant individual subject data in the supplementary materials. Overall they have addressed most of the key points that my original review raised, including a demonstration of the conditions under which recalibration effects do not delay to zero over long delays. The number of subjects remains rather low, but at least we can now appreciate the heterogeneity within them. I still have some reservations about the magnitude of the conceptual advance that this study makes.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97765.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Li et al. describe an audiovisual temporal recalibration experiment in which participants perform baseline sessions of ternary order judgments about audiovisual stimulus pairs with various stimulus-onset asynchronies (SOAs). These are followed by adaptation at several adapting SOAs (each on a different day), followed by post-adaptation sessions to assess changes in psychometric functions. The key novelty is the formal specification and application/fit of a causal-inference model for the perception of relative timing, providing simulated predictions for the complete set of psychometric functions both pre and post adaptation.</p><p>Strengths:</p><p>(1) Formal models are preferable to vague theoretical statements about a process, and prior to this work, certain accounts of temporal recalibration (specifically those that do not rely on a population code) had only qualitative theoretical statements to explain how/why the magnitude of recalibration changes non-linearly with the stimulus-onset asynchrony of the adaptor.</p><p>(2) The experiment is appropriate, the methods are well described, and the average model prediction is a good match to the average data (Figure 4). Conclusions are supported by the data and modelling.</p><p>(3) The work should be impactful. There seems a good chance that this will become the go-to modelling framework for those exploring non population-code accounts of temporal recalibration (or comparing them with population-code accounts).</p><p>(4) Key issues for the generality of the model, such as recalibration asymmetries reported by other authors that are inconsistent with those reported here, are thoughtfully discussed.</p><p>Weaknesses:</p><p>(1) Models are not compared using a gold-standard measure such as leave-one-out cross validation. However, this is legitimate given lengthy model fitting times, and a sensible approximation is presented.</p><p>(2) The model misses in a systematic way for the psychometric functions of some participants/conditions. In addition to misses relating to occasional failures to estimate the magnitude of recalibration, some of the misses are because all functions are only permitted to shift in central tendency (whereas some participants show changes better characterized at one or both decision criteria). Given the fact that the modelling in general embraces individual differences, it might have been worth allowing different kinds of change for different participants. However, this is not really critical for the central concern (changes in the magnitude of recalibration for different adaptors) and there is a limit to how much can be done along these lines without making the model too flexible to test.</p><p>(3) As a minor point, the model relies on simulation, which may limit its take-up/application by others in the field (although open access code will be provided).</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97765.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Li</surname><given-names>Luhe</given-names></name><role specific-use="author">Author</role><aff><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Hong</surname><given-names>Fangfang</given-names></name><role specific-use="author">Author</role><aff><institution>University of Pennsylvania</institution><addr-line><named-content content-type="city">Philadelphia</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Badde</surname><given-names>Stephanie</given-names></name><role specific-use="author">Author</role><aff><institution>Tufts University</institution><addr-line><named-content content-type="city">Medford</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Landy</surname><given-names>Michael S</given-names></name><role specific-use="author">Author</role><aff><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>This study asks whether the phenomenon of crossmodal temporal recalibration, i.e. the adjustment of time perception by consistent temporal mismatches across the senses, can be explained by the concept of multisensory causal inference. In particular, they ask whether the explanation offered by causal inference better explains temporal recalibration better than a model assuming that crossmodal stimuli are always integrated, regardless of how discrepant they are.</p><p>The study is motivated by previous work in the spatial domain, where it has been shown consistently across studies that the use of crossmodal spatial information is explained by the concept of multisensory causal inference. It is also motivated by the observation that the behavioral data showcasing temporal recalibration feature nonlinearities that, by their nature, cannot be explained by a fixed integration model (sometimes also called mandatory fusion).</p><p>To probe this the authors implemented a sophisticated experiment that probed temporal recalibration in several sessions. They then fit the data using the two classes of candidate models and rely on model criteria to provide evidence for their conclusion. The study is sophisticated, conceptually and technically state-of-the-art, and theoretically grounded. The data clearly support the authors’ conclusions.</p><p>I find the conceptual advance somewhat limited. First, by design, the fixed integration model cannot explain data with a nonlinear dependency on multisensory discrepancy, as already explained in many studies on spatial multisensory perception. Hence, it is not surprising that the causal inference model better fits the data.</p></disp-quote><p>We have addressed this comment by including an asynchrony-contingent model, which is capable of predicting the nonlinearity of recalibration effects by employing a heuristic approximation of the causal-inference process (Fig. 3). We also updated the previous competitor model with a more reasonable asynchrony-correction model as the baseline of model comparison, which assumes recalibration aims to restore synchrony whenever the sensory measurement of SOA indicates an asynchrony. The causal-inference model outperformed both models, as indicated by model evidence (Fig. 4A). Furthermore, model predictions show that the causal-inference model more accurately captures recalibration at large SOAs at both the group (Fig. 4B) and the individual levels (Fig. S4).</p><disp-quote content-type="editor-comment"><p>Second, and again similar to studies on spatial paradigms, the causal inference model fails to predict the behavioral data for large discrepancies. The model predictions in Figure 5 show the (expected) vanishing recalibration for large delta, while the behavioral data don’t decay to zero. Either the range of tested SOAs is too small to show that both the model and data converge to the same vanishing effect at large SOAs, or the model's formula is not the best for explaining the data. Again, the studies using spatial paradigms have the same problem, but in my view, this poses the most interesting question here.</p></disp-quote><p>We included an additional simulation (Fig. 5B) to show that the causal-inference model can predict non-zero recalibration for long adapter SOAs, especially in observers with a high common-cause prior and low sensory precision. This ability to predict a non-zero recalibration effect even at large SOA, such as 0.7 s, is one key feature of the causal-inference model that distinguishes it from the asynchrony-contingent model.</p><disp-quote content-type="editor-comment"><p>In my view there is nothing generally wrong with the study, it does extend the 'known' to another type of paradigm. However, it covers little new ground on the conceptual side.</p><p>On that note, the small sample size of n=10 is likely not an issue, but still, it is on the very low end for this type of study.</p></disp-quote><p>This study used a within-subject design, which included 3 phases each repeated in 9 sessions, totaling 13.5 hours per participant. This extensive data collection allows us to better constrain the model for each participant. Our conclusions are based on the different models’ ability to fit individual data.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>Li et al.’s goal is to understand the mechanisms of audiovisual temporal recalibration. This is an interesting challenge that the brain readily solves in order to compensate for real-world latency differences in the time of arrival of audio/visual signals. To do this they perform a 3-phase recalibration experiment on 9 observers that involves a temporal order judgment (TOJ) pretest and posttest (in which observers are required to judge whether an auditory and visual stimulus were coincident, auditory leading or visual leading) and a conditioning phase in which participants are exposed to a sequence of AV stimuli with a particular temporal disparity. Participants are required to monitor both streams of information for infrequent oddballs, before being tested again in the TOJ, although this time there are 3 conditioning trials for every 1 TOJ trial. Like many previous studies, they demonstrate that conditioning stimuli shift the point of subjective simultaneity (pss) in the direction of the exposure sequence.</p><p>These shifts are modest - maxing out at around -50 ms for auditory leading sequences and slightly less than that for visual leading sequences. Similar effects are observed even for the longest offsets where it seems unlikely listeners would perceive the stimuli as synchronous (and therefore under a causal inference model you might intuitively expect no recalibration, and indeed simulations in Figure 5 seem to predict exactly that which isn't what most of their human observers did). Overall I think their data contribute evidence that a causal inference step is likely included within the process of recalibration.</p><p>Strengths:</p><p>The manuscript performs comprehensive testing over 9 days and 100s of trials and accompanies this with mathematical models to explain the data. The paper is reasonably clearly written and the data appear to support the conclusions.</p><p>Weaknesses:</p><p>While I believe the data contribute evidence that a causal inference step is likely included within the process of recalibration, this to my mind is not a mechanism but might be seen more as a logical checkpoint to determine whether whatever underlying neuronal mechanism actually instantiates the recalibration should be triggered.</p></disp-quote><p>We have addressed this comment by replacing the fixed-update model with an asynchrony-correction model, which assumes that the system first evaluates whether the measurement of SOA is asynchronous, thus indicating a need for recalibration (Fig. 3). If it does, it shifts the audiovisual bias by a proportion of the measured SOA. We additionally included an asynchrony-contingent model, which is capable of replicating the nonlinearity of recalibration effects by a heuristic approximation of the causal-inference process.</p><p>Model comparisons indicate that the causal-inference model of temporal recalibration outperforms both alternative models (Fig. 4A). Furthermore, the model predictions demonstrate that the causal-inference model more accurately captures recalibration at large SOAs at both the group level (Fig. 4B) and individual level (Fig. S4).</p><disp-quote content-type="editor-comment"><p>The authors’ causal inference model strongly predicts that there should be no recalibration for stimuli at 0.7 ms offset, yet only 3/9 participants appear to show this effect. They note that a significant difference in their design and that of others is the inclusion of longer lags, which are unlikely to originate from the same source, but don’t offer any explanation for this key difference between their data and the predictions of a causal inference model.</p></disp-quote><p>We added further simulations to show that the causal-inference model can predict non-zero recalibration also for longer adapter SOAs, especially in observers with a large common-cause prior (Fig. 5A) and low sensory precision (Fig. 5B). This ability to predict a non-zero recalibration effect even at longer adapter SOAs, such as 0.7 s, is a key feature of the causal-inference model that distinguishes it from the asynchrony-contingent model.</p><disp-quote content-type="editor-comment"><p>I’m also not completely convinced that the causal inference model isn’t ‘best’ simply because it has sufficient free parameters to capture the noise in the data. The tested models do not (I think) have equivalent complexity - the causal inference model fits best, but has more parameters with which to fit the data. Moreover, while it fits ‘best’, is it a good model? Figure S6 is useful in this regard but is not completely clear - are the red dots the actual data or the causal inference prediction? This suggests that it does fit the data very well, but is this based on predicting held-out data, or is it just that by having more parameters it can better capture the noise? Similarly, S7 is a potentially useful figure but it's not clear what is data and what are model predictions (what are the differences between each row for each participant; are they two different models or pre-test post-test or data and model prediction?!).</p><p>I'm not an expert on the implementation of such models but my reading of the supplemental methods is that the model is fit using all the data rather than fit and tested on held-out data. This seems problematic.</p><p>We recognize the risk of overfitting with the causal-inference model. We now rely on Bayesian model comparisons, which use model evidence for model selection. This method automatically incorporates a penalty for model complexity through the marginalization over the parameter space (MacKay, 2003).</p><p>Our design is not suitable for cross-validation because the model-fitting process is computationally intensive and time-consuming. Each fit of the causal-inference model takes approximately 30 hours, and multiple fits with different initial starting points are required to rule out that the parameter estimates correspond to local minima.</p></disp-quote><p>I would have liked to have seen more individual participant data (which is currently in the supplemental materials, albeit in a not very clear manner as discussed above).</p><disp-quote content-type="editor-comment"><p>We have revised Supplementary Figures S4-S6 to show additional model predictions of the recalibration effect for individual participants, and participants’ temporal-order judgments are now shown in Supplement Figure S7. These figures confirm the better performance of the causal-inference model.</p></disp-quote><p>The way that S3 is described in the text (line 141) makes it sound like everyone was in the same direction, however, it is clear that 2 /9 listeners show the opposite pattern, and 2 have confidence intervals close to zero (albeit on the -ve side).</p><p>We have revised the text to clarify that the asymmetry occurs in both directions and is idiosyncratic (lines 168-171). We summarized the distribution of the individual asymmetries of the recalibration effect across visual-leading and auditory-leading adapter SOAs in Supplementary Figure S2.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary:</p><p>Li et al. describe an audiovisual temporal recalibration experiment in which participants perform baseline sessions of ternary order judgments about audiovisual stimulus pairs with various stimulus-onset asynchronies (SOAs). These are followed by adaptation at several adapting SOAs (each on a different day), followed by post-adaptation sessions to assess changes in psychometric functions. The key novelty is the formal specification and application/fit of a causal-inference model for the perception of relative timing, providing simulated predictions for the complete set of psychometric functions both pre and post-adaptation.</p><p>Strengths:</p><p>(1) Formal models are preferable to vague theoretical statements about a process, and prior to this work, certain accounts of temporal recalibration (specifically those that do not rely on a population code) had only qualitative theoretical statements to explain how/why the magnitude of recalibration changes non-linearly with the stimulus-onset asynchrony of the adapter.</p><p>(2) The experiment is appropriate, the methods are well described, and the average model prediction is a fairly good match to the average data (Figure 4). Conclusions may be overstated slightly, but seem to be essentially supported by the data and modelling.</p><p>(3) The work should be impactful. There seems a good chance that this will become the go-to modelling framework for those exploring non-population-code accounts of temporal recalibration (or comparing them with population-code accounts).</p><p>(4) A key issue for the generality of the model, specifically in terms of recalibration asymmetries reported by other authors that are inconsistent with those reported here, is properly acknowledged in the discussion.</p><p>Weaknesses:</p><p>(1) The evidence for the model comes in two forms. First, two trends in the data (non-linearity and asymmetry) are illustrated, and the model is shown to be capable of delivering patterns like these. Second, the model is compared, via AIC, to three other models. However, the main comparison models are clearly not going to fit the data very well, so the fact that the new model fits better does not seem all that compelling. I would suggest that the authors consider a comparison with the atheoretical model they use to first illustrate the data (in Figure 2). This model fits all sessions but with complete freedom to move the bias around (whereas the new model constrains the way bias changes via a principled account). The atheoretical model will obviously fit better, but will have many more free parameters, so a comparison via AIC/BIC or similar should be informative</p></disp-quote><p>In the revised manuscript, we switched from AIC to Bayesian model selection, which approximates and compares model evidence. This method incorporates a strong penalty for model complexity through marginalization over the parameter space (MacKay, 2003).</p><p>We have addressed this comment by updating the former competitor model into a more reasonable version that induces recalibration only for some measured SOAs and by including another (asynchrony-contingent) model that is capable of predicting the nonlinearity and asymmetry of recalibration (Fig. 3) while heuristically approximating the causal inference computations. The causal-inference model outperformed the asynchrony-contingent model, as indicated by model evidence (Fig. 4A). Furthermore, model predictions show that the causal-inference model more accurately captures recalibration at large SOAs at both the group (Fig. 4B) and the individual level (Fig. S4).</p><disp-quote content-type="editor-comment"><p>(2) It does not appear that some key comparisons have been subjected to appropriate inferential statistical tests. Specifically, lines 196-207 - presumably this is the mean (and SD or SE) change in AIC between models across the group of 9 observers. So are these differences actually significant, for example via t-test?</p></disp-quote><p>We statistically compared the models using Bayes factors (Fig. 4A). The model evidence for each model was approximated using Variational Bayesian Monte Carlo. Bayes factors provided strong evidence in support of the causal-inference model relative to the other models.</p><disp-quote content-type="editor-comment"><p>(3) The manuscript tends to gloss over the population-code account of temporal recalibration, which can already provide a quantitative account of how the magnitude of recalibration varies with adapter SOA. This could be better acknowledged, and the features a population code may struggle with (asymmetry?) are considered.</p></disp-quote><p>We simulated a population-code model to examine its prediction of the recalibration effect for different adapter SOAs (lines 380–388, Supplement Section 8). The population-code model can predict the nonlinearity of recalibration, i.e., a decreasing recalibration effect as the adapter SOA increases. However, to capture the asymmetry of recalibration effects across auditory-leading and visual-leading adapter stimuli, we would need to assume that the auditory-leading and visual-leading SOAs are represented by neural populations with unequal tuning curves.</p><disp-quote content-type="editor-comment"><p>(4) The engagement with relevant past literature seems a little thin. Firstly, papers that have applied causal inference modeling to judgments of relative timing are overlooked (see references below). There should be greater clarity regarding how the modelling here builds on or differs from these previous papers (most obviously in terms of additionally modelling the recalibration process, but other details may vary too). Secondly, there is no discussion of previous findings like that in Fujisaki et al.’s seminal work on recalibration, where the spatial overlap of the audio and visual events didn’t seem to matter (although admittedly this was an N = 2 control experiment). This kind of finding would seem relevant to a causal inference account.</p><p>References:</p><p>Magnotti JF, Ma WJ and Beauchamp MS (2013) Causal inference of asynchronous audiovisual speech. Front. Psychol. 4:798. doi: 10.3389/fpsyg.2013.00798</p><p>Sato, Y. (2021). Comparing Bayesian models for simultaneity judgement with different causal assumptions. J. Math. Psychol., 102, 102521.</p></disp-quote><p>We have revised the Introduction and Discussion to better situate our study within the existing literature. Specifically, we have incorporated the suggested references (lines 66–69) and provided clearer distinctions on how our modeling approach builds on or differs from previous work on causal-inference models, particularly in terms of modeling the recalibration process (lines 75–79). Additionally, we have discussed findings that might contradict the assumptions of the causal-inference model (lines 405–424).</p><disp-quote content-type="editor-comment"><p>(5) As a minor point, the model relies on simulation, which may limit its take-up/application by others in the field.</p></disp-quote><p>Upon acceptance, we will publicly share the code for all models (simulation and parameter fitting) to enable researchers to adapt and apply these models to their own data.</p><disp-quote content-type="editor-comment"><p>(6) There is little in the way of reassurance regarding the model’s identifiability and recoverability. The authors might for example consider some parameter recovery simulations or similar.</p></disp-quote><p>We conducted a model recovery for each of the six models described in the main text and confirmed that the asynchrony-contingent and causal-inference models are identifiable (Supplement Section 11). Simulations of the asynchrony-correction model were sometimes best fit by causal-inference models, because the latter behaves similarly when the prior of a common cause is set to one.</p><p>We also conducted a parameter recovery for the winning model, the causal-inference model with modality-specific precision (Supplement Section 13).</p><p>Key parameters, including audiovisual bias , amount of auditory latency noise , amount of visual latency noise , criterion, lapse rate showed satisfactory recovery performance. The less accurate recovery of is likely due to a tradeoff with learning rate .</p><disp-quote content-type="editor-comment"><p>(7) I don't recall any statements about open science and the availability of code and data.</p></disp-quote><p>Upon acceptance of the manuscript, all code (simulation and parameter fitting) and data will be made available on OSF and publicly available.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewing Editor (Recommendations For The Authors):</bold></p><p>In addition to the comments below, we would like to offer the following summary based on the discussion between reviewers:</p><p>The major shortcoming of the work is that there should ideally be a bit more evidence to support the model, over and above a demonstration that it captures important trends and beats an account that was already known to be wrong. We suggest you:</p><p>(1) Revise the figure legends (Figure 5 and Figure 6E).</p></disp-quote><p>We revised all figures and figure legends.</p><disp-quote content-type="editor-comment"><p>(2) Additionally report model differences in terms of BIC (which will favour the preferred model less under the current analysis);</p></disp-quote><p>We now base the model comparison on Bayesian model selection, which approximates and compares model evidence. This method incorporates a strong penalty for model complexity through marginalization over the parameter space (MacKay, 2003).</p><disp-quote content-type="editor-comment"><p>(3) Move to instead fitting the models multiple times in order to get leave-one-out estimates of best-fitting loglikelihood for each left-out data point (and then sum those for the comparison metric).</p></disp-quote><p>Unfortunately, our design is not suitable for cross-validation methods because the model-fitting process is computationally intensive and time-consuming. Each fit of the causal-inference model takes approximately 30 hours, and multiple fits with different initial starting points are required to rule out local minima.</p><disp-quote content-type="editor-comment"><p>(4) Offering a comparison with a more convincing model for example an atheoretical fit with free parameters for all adapters, e.g., as suggested by Reviewer 3.</p></disp-quote><p>We updated the previous competitor model and included an asynchrony-contingent model, which is capable of predicting the nonlinearity of recalibration (Fig. 3). The causal-inference model still outperformed the asynchrony-contingent model (Fig. 4A). Furthermore, model predictions show that only the causal-inference model captures non-zero recalibration effects for long adapter SOAs at both the group level (Fig. 4B) and individual level (Figure S4).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>A larger sample size would be better.</p></disp-quote><p>This study used a within-subject design, which included 9 sessions, totaling 13.5 hours per participant. This extensive data collection allows us to better constrain the model for each participant. Our conclusions are based on the different models’ ability to fit individual data rather than on group statistics.</p><disp-quote content-type="editor-comment"><p>It would be good to better put the study in the context of spatial ventriloquism, where similar model comparisons have been done over the last ten years and there is a large body of work to connect to.</p></disp-quote><p>We now discuss our model in relation to models of cross-modal spatial recalibration in the Introduction (lines 70–78) and Discussion (lines 324–330).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>Previous authors (e.g., Yarrow et al.,) have described latency shift and criterion change models as providing a good fit of experimental data. Did the authors attempt a criterion shift model in addition to a shift model?</p></disp-quote><p>We have considered criterion-shift variants of our atheoretical recalibration models in Supplement Section 1. To summarize the results, we varied two model assumptions: (1) the use of either a Gaussian or an exponential measurement distribution, and (2) recalibration being implemented either as a shift of bias or a criterion. We fit each model variant separately to the ternary TOJ responses of all sessions. Bayesian model comparisons indicated that the bias-shift model with exponential measurement distributions best captured the data of most participants.</p><disp-quote content-type="editor-comment"><p>Figure 4B - I'm not convinced that the modality-independent uncertainty is anything but a straw man. Models not allowed to be asymmetric do not show asymmetry? (the asymmetry index is irrelevant in the fixed update model as I understand it so it is not surprising the model is identical?).</p></disp-quote><p>We included the assumption that temporal uncertainty might be modality-independent for several reasons. First, there is evidence suggesting that a central mechanism governs the precision of temporal-order judgments (Hirsh &amp; Sherrick, 1961), indicating that precision is primarily limited by a central mechanism rather than the sensory channels themselves. Second, from a modeling perspective, it was necessary to test whether an audio-visual temporal bias alone, i.e., assuming modality-independent uncertainty, could introduce asymmetry across adapter SOAs. Additionally, most previous studies implicitly assumed symmetric likelihoods, i.e., modality-independent latency noise, by fitting cumulative Gaussians to the psychometric curves derived from 2AFC-TOJ tasks (Di Luca et al., 2009; Fujisaki et al., 2004; Harrar &amp; Harris, 2005; Keetels &amp; Vroomen, 2007; Navarra et al., 2005; Tanaka et al., 2011; Vatakis et al., 2007, 2008; Vroomen et al., 2004).</p><disp-quote content-type="editor-comment"><p>Why does a zero SOA adapter shift the pss towards auditory leading? Is this a consequence of the previous day’s conditioning - it’s not clear from the methods whether all listeners had the same SOA conditioning sequence across days.</p></disp-quote><p>The auditory-leading recalibration effect for an adapter SOA of zero has been consistently reported in previous studies (e.g., Fujisaki et al., 2004; Vroomen et al., 2004). This effect symbolizes the asymmetry in recalibration. This asymmetry can be explained by differences across modalities in the noisiness of the latencies (Figure 5C) in combination with audiovisual temporal bias (Figure S8).</p><p>We added details about the order of testing to the Methods section (lines 456–457).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>Abstract</p><p>“Our results indicate that human observers employ causal-inference-based percepts to recalibrate cross-modal temporal perception” Your results indicate this is plausible. However, this statement (basically repeated at the end of the intro and again in the discussion) is - in my opinion - too strong.</p></disp-quote><p>We have revised the statement as suggested.</p><disp-quote content-type="editor-comment"><p>Intro and later</p><p>Within the wider literature on relative timing perception, the temporal order judgement (TOJ) task refers to a task with just two response options. Tasks with three response options, as employed here, are typically referred to as ternary judgments. I would suggest language consistent with the existing literature (or if not, the contrast to standard usage could be clarified).</p><p>Ref: Ulrich, R. (1987). Threshold models of temporal-order judgments evaluated by a ternary response task. Percept. Psychophys., 42, 224-239.</p></disp-quote><p>We revised the term for the task as suggested throughout the manuscript.</p><disp-quote content-type="editor-comment"><p>Results, 2.2.2</p><p>“However, temporal precision might not be due to the variability of arrival latency.” Indeed, although there is some recent evidence that it might be.</p><p>Ref: Yarrow, K., Kohl, C, Segasby, T., Kaur Bansal, R., Rowe, P., &amp; Arnold, D.H. Neural-latency noise places limits on human sensitivity to the timing of events. Cognition, 222, 105012 (2022).</p></disp-quote><p>We included the reference as suggested (lines 245–248).</p><disp-quote content-type="editor-comment"><p>Methods, 4.3.</p><p>Should there be some information here about the order of adaptation sessions (e.g. random for each observer)?</p></disp-quote><p>We added details about the order of testing to the Methods section (lines 456–457).</p><disp-quote content-type="editor-comment"><p>Supplemental material section 1.</p><p>Here, you test whether the changes resulting from recalibration look more like a shift of the entire psychometric function or an expansion of the psychometric function on one side (most straightforwardly compatible with a change of one decision criterion). Fine, but the way you have done this is odd, because you have introduced a further difference in the models (Gaussian vs. exponential latency noise) so that you cannot actually conclude that the trend towards a win for the bias-shift model is simply down to the bias vs. criterion difference. It could just as easily be down to the different shapes of psychometric functions that the two models can predict (with the exponential noise model permitting asymmetry in slopes). There seems to be no reason that this comparison cannot be made entirely within the exponential noise framework (by a very simple reparameterization that focuses on the two boundaries rather than the midpoint and extent of the decision window). Then, you would be focusing entirely on the question of interest. It would also equate model parameters, removing any reliance on asymptotic assumptions being met for AIC.</p></disp-quote><p>We revised our exploration of atheoretical recalibration models. To summarize the results, we varied two model assumptions: (1) the use of either a Gaussian or an exponential measurement distribution, and (2) recalibration being implemented either as a shift of the cross-modal temporal bias or as a shift of the criterion. We fit each model separately to the ternary TOJ responses of all sessions. Bayesian model comparisons indicated that the bias-shift model with exponential measurement distributions best described the data of most participants.</p><p>References</p><p>Di Luca, M., Machulla, T.-K., &amp; Ernst, M. O. (2009). Recalibration of multisensory simultaneity: cross-modal transfer coincides with a change in perceptual latency. Journal of Vision, 9(12), Article 7.</p><p>Fujisaki, W., Shimojo, S., Kashino, M., &amp; Nishida, S. ’ya. (2004). Recalibration of audiovisual simultaneity. Nature Neuroscience, 7(7), 773–778.</p><p>Harrar, V., &amp; Harris, L. R. (2005). Simultaneity constancy: detecting events with touch and vision. Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale, 166(3-4), 465–473.</p><p>Hirsh, I. J., &amp; Sherrick, C. E., Jr. (1961). Perceived order in different sense modalities. Journal of Experimental Psychology, 62(5), 423–432.</p><p>Keetels, M., &amp; Vroomen, J. (2007). No effect of auditory-visual spatial disparity on temporal recalibration. Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale, 182(4), 559–565.</p><p>MacKay, D. J. (2003). Information theory, inference and learning algorithms.</p><p>Navarra, J., Vatakis, A., Zampini, M., Soto-Faraco, S., Humphreys, W., &amp; Spence, C. (2005). Exposure to asynchronous audiovisual speech extends the temporal window for audiovisual integration. Brain Research. Cognitive Brain Research, 25(2), 499–507.</p><p>Tanaka, A., Asakawa, K., &amp; Imai, H. (2011). The change in perceptual synchrony between auditory and visual speech after exposure to asynchronous speech. Neuroreport, 22(14), 684–688.</p><p>Vatakis, A., Navarra, J., Soto-Faraco, S., &amp; Spence, C. (2007). Temporal recalibration during asynchronous audiovisual speech perception. Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale, 181(1), 173–181.</p><p>Vatakis, A., Navarra, J., Soto-Faraco, S., &amp; Spence, C. (2008). Audiovisual temporal adaptation of speech: temporal order versus simultaneity judgments. Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale, 185(3), 521–529.</p><p>Vroomen, J., Keetels, M., de Gelder, B., &amp; Bertelson, P. (2004). Recalibration of temporal order perception by exposure to audio-visual asynchrony. Brain Research. Cognitive Brain Research, 22(1), 32–35.</p></body></sub-article></article>