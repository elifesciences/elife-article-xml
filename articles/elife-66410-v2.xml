<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">66410</article-id><article-id pub-id-type="doi">10.7554/eLife.66410</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="heading"><subject>Physics of Living Systems</subject></subj-group></article-categories><title-group><article-title>Fast deep neural correspondence for tracking and identifying neurons in <italic>C. elegans</italic> using semi-synthetic training</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-221863"><name><surname>Yu</surname><given-names>Xinwei</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8699-3546</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-222731"><name><surname>Creamer</surname><given-names>Matthew S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9458-0629</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-221867"><name><surname>Randi</surname><given-names>Francesco</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6200-7254</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-108809"><name><surname>Sharma</surname><given-names>Anuj K</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5061-9731</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-140581"><name><surname>Linderman</surname><given-names>Scott W</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3878-9073</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-30795"><name><surname>Leifer</surname><given-names>Andrew M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5362-5093</contrib-id><email>leifer@princeton.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Physics, Princeton University</institution><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Princeton Neuroscience Institute, Princeton University</institution><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Department of Statistics, Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Wu Tsai Neurosciences Institute, Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Senior Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>14</day><month>07</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e66410</elocation-id><history><date date-type="received" iso-8601-date="2021-01-09"><day>09</day><month>01</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-07-13"><day>13</day><month>07</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Yu et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Yu et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-66410-v2.pdf"/><abstract><p>We present an automated method to track and identify neurons in <italic>C. elegans</italic>, called ‘fast Deep Neural Correspondence’ or fDNC, based on the transformer network architecture. The model is trained once on empirically derived semi-synthetic data and then predicts neural correspondence across held-out real animals. The same pre-trained model both tracks neurons across time and identifies corresponding neurons across individuals. Performance is evaluated against hand-annotated datasets, including NeuroPAL (Yemini et al., 2021). Using only position information, the method achieves 79.1% accuracy at tracking neurons within an individual and 64.1% accuracy at identifying neurons across individuals. Accuracy at identifying neurons across individuals is even higher (78.2%) when the model is applied to a dataset published by another group (Chaudhary et al., 2021). Accuracy reaches 74.7% on our dataset when using color information from NeuroPAL. Unlike previous methods, fDNC does not require straightening or transforming the animal into a canonical coordinate system. The method is fast and predicts correspondence in 10 ms making it suitable for future real-time applications.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Understanding the intricacies of the brain often requires spotting and tracking specific neurons over time and across different individuals. For instance, scientists may need to precisely monitor the activity of one neuron even as the brain moves and deforms; or they may want to find universal patterns by comparing signals from the same neuron across different individuals.</p><p>Both tasks require matching which neuron is which in different images and amongst a constellation of cells. This is theoretically possible in certain ‘model’ animals where every single neuron is known and carefully mapped out. Still, it remains challenging: neurons move relative to one another as the animal changes posture, and the position of a cell is also slightly different between individuals. Sophisticated computer algorithms are increasingly used to tackle this problem, but they are far too slow to track neural signals as real-time experiments unfold.</p><p>To address this issue, Yu et al. designed a new algorithm based on the Transformer, an artificial neural network originally used to spot relationships between words in sentences. To learn relationships between neurons, the algorithm was fed hundreds of thousands of ‘semi-synthetic’ examples of constellations of neurons. Instead of painfully collated actual experimental data, these datasets were created by a simulator based on a few simple measurements. Testing the new algorithm on the tiny worm <italic>Caenorhabditis elegans</italic> revealed that it was faster and more accurate, finding corresponding neurons in about 10ms.</p><p>The work by Yu et al. demonstrates the power of using simulations rather than experimental data to train artificial networks. The resulting algorithm can be used immediately to help study how the brain of <italic>C. elegans</italic> makes decisions or controls movements. Ultimately, this research could allow brain-machine interfaces to be developed.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>computer vision</kwd><kwd>deep learning</kwd><kwd>artificial neural network</kwd><kwd>tracking</kwd><kwd>registration</kwd><kwd>transformer</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>C. elegans</italic></kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>543003</award-id><principal-award-recipient><name><surname>Leifer</surname><given-names>Andrew M</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>697092</award-id><principal-award-recipient><name><surname>Linderman</surname><given-names>Scott W</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>IOS-184537</award-id><principal-award-recipient><name><surname>Leifer</surname><given-names>Andrew M</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>PHY-1734030</award-id><principal-award-recipient><name><surname>Leifer</surname><given-names>Andrew M</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R21NS101629</award-id><principal-award-recipient><name><surname>Leifer</surname><given-names>Andrew M</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1R01NS113119</award-id><principal-award-recipient><name><surname>Linderman</surname><given-names>Scott W</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>P40 OD010440</award-id><principal-award-recipient><name><surname>Creamer</surname><given-names>Matthew S</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution>Swartz Foundation</institution></institution-wrap></funding-source><award-id>Swartz Fellowship for Theoretical Neuroscience</award-id><principal-award-recipient><name><surname>Randi</surname><given-names>Francesco</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A deep neural network trained on semi-synthetic data learns to quickly track and identify neurons in <italic>Caenorhabditis elegans</italic>.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The nervous system of the nematode <italic>C. elegans</italic> is well characterized, such that each of the 302 neurons is named and has stereotyped locations across animals (<xref ref-type="bibr" rid="bib40">White et al., 1986</xref>; <xref ref-type="bibr" rid="bib33">Sulston, 1976</xref>; <xref ref-type="bibr" rid="bib41">Witvliet et al., 2020</xref>). The capability to find corresponding neurons across animals is essential to investigate neural coding and neural dynamics across animals. Despite the worm’s overall stereotypy, the variability in neurons’ spatial arrangement is sufficient to make predicting neural correspondence a challenge. For whole-brain calcium imaging (<xref ref-type="bibr" rid="bib30">Schrödel et al., 2013</xref>; <xref ref-type="bibr" rid="bib38">Venkatachalam et al., 2016</xref>; <xref ref-type="bibr" rid="bib24">Nguyen et al., 2016</xref>), identifying neurons across animals is additionally challenging because the nuclear localized markers that are used tend to obscure morphological features that would otherwise assist in neural identification.</p><p>An ideal method for finding neural correspondence in <italic>C. elegans</italic> should accommodate two major use cases. The first is tracking neurons within an individual across time as the animal’s head moves and deforms. Here, the goal is to be able to say with confidence that a neuron imaged in a volume taken at time <italic>t</italic><sub>1</sub> is the same as another neuron taken from a volume imaged at time <italic>t</italic><sub>2</sub>. Tracking across time is needed to extract calcium dynamics from neurons during freely moving population calcium imaging (<xref ref-type="bibr" rid="bib38">Venkatachalam et al., 2016</xref>; <xref ref-type="bibr" rid="bib24">Nguyen et al., 2016</xref>; <xref ref-type="bibr" rid="bib15">Lagache et al., 2020</xref>). Additionally, very fast real-time tracking will be needed to bring closed-loop techniques such as brain-machine interfaces (<xref ref-type="bibr" rid="bib5">Clancy et al., 2014</xref>), and optical patch clamping (<xref ref-type="bibr" rid="bib9">Hochbaum et al., 2014</xref>) to moving animals.</p><p>The second and more general use case is finding neural correspondence across individuals. Often this is to identify the name of a neuron with respect to the connectome (<xref ref-type="bibr" rid="bib40">White et al., 1986</xref>) or a gene expression atlas (<xref ref-type="bibr" rid="bib7">Hammarlund et al., 2018</xref>). Even when a neuron’s name cannot be ascertained, being able to identify which neurons are the same across recordings allows researchers to study neural population codes common across individuals.</p><p>For both use cases, a method to find neural correspondence is desired that is accurate, fast, requires minimal experimental training data and that generalizes across animal pose, orientation, imaging hardware, and conditions. Furthermore, an ideal method should not only perform well when restricted to neural positioning information but, should also be flexible enough to leverage genetically encoded color labeling information or other features for improved accuracy when available. Multicolor strains are powerful new tools that use multiple genetically encoded fluorescent labels to aid neural identification (<xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref>; <xref ref-type="bibr" rid="bib35">Toyoshima et al., 2019</xref>) (we use one of those strains, NeuroPAL (<xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref>), for validating our model). However, some applications, like whole-brain imaging in moving worms, are not yet easily compatible with the multicolor imaging required by these new strains, so there remains a need for improved methods that use position information alone.</p><p>A variety of automated methods for <italic>C. elegans</italic> have been developed that address some, but not all these needs. Most methods developed so far focus on finding the extrinsic similarity (<xref ref-type="bibr" rid="bib2">Bronstein, 2007</xref>) between one neuron configuration, called a test, and another neuron configuration called a template. Methods like these deform space to minimize distances between neurons in the template and neurons in the test and then attempt to solve an assignment problem (<xref ref-type="bibr" rid="bib14">Lagache et al., 2018</xref>). For example, a simple implementation would be to use a non-rigid registration model, like Coherent Point Drift (CPD) (<xref ref-type="bibr" rid="bib21">Myronenko and Song, 2010</xref>) to optimize a warping function between neuron positions in the test and template. More recent non-rigid registration algorithms like PR-GLS (<xref ref-type="bibr" rid="bib19">Ma et al., 2016</xref>) also incorporate relative spatial arrangement of the neurons (<xref ref-type="bibr" rid="bib39">Wen et al., 2018</xref>).</p><p>Models can also do better by incorporating the statistics of neural variability. NeRVE registration and clustering (<xref ref-type="bibr" rid="bib25">Nguyen et al., 2017</xref>), for example, also uses a non-rigid point set registration algorithm (<xref ref-type="bibr" rid="bib10">Jian and Vemuri, 2011</xref>) to find a warping function that minimizes the difference between a configuration of neurons at one time point and another. But NeRVE further registers the test neurons onto multiple templates to define a feature vector and then finds neural correspondence by clustering those feature vectors. By using multiple templates, the method implicitly incorporates more information about the range and statistics of that individual animal’s poses to improve accuracy.</p><p>A related line of work uses generative models to capture the statistics of variability across many individual worms. These generative models specify a joint probability distribution over neural labels and the locations, shapes, sizes, or appearance of neurons identified in the imaging data of multiple individuals (<xref ref-type="bibr" rid="bib3">Bubnis et al., 2019</xref>; <xref ref-type="bibr" rid="bib36">Varol et al., 2020</xref>; <xref ref-type="bibr" rid="bib22">Nejatbakhsh et al., 2020</xref>; <xref ref-type="bibr" rid="bib23">Nejatbakhsh and Varol, 2021</xref>). These approaches are based on assumptions about the likelihood of observing a test neural configuration, given an underlying configuration of labeled neurons. For example, these generative models often begin with a Gaussian distribution over neuron positions in a canonical coordinate system and then assume a distribution over potentially non-rigid transformations of the worm’s pose for each test configuration. Then, under these assumptions, the most likely neural correspondence is estimated via approximate Bayesian inference.</p><p>The success of generative modeling hinges upon the accuracy of its underlying assumptions, and these are challenging to make for high-dimensional data. An alternative is to take a discriminative modeling approach (<xref ref-type="bibr" rid="bib1">Bishop, 2006</xref>). For example, recent work (<xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref>) has used conditional random fields (CRF) to directly parameterize a conditional distribution over neuron labels, rather than assuming a model for the high-dimensional and complex image data. CRF allows for a wide range of informative features to be incorporated in the model, such as the angles between neurons, or their relative anterior-posterior positions, which are known to be useful for identifying neurons (<xref ref-type="bibr" rid="bib18">Long et al., 2009</xref>). Ultimately, however, it is up to the modeler to select and hand curate a set of features to input into the CRF.</p><p>The next logical step is to allow for much richer features to be learned from the data. Artificial neural networks are ideal for tackling this problem, but they require immensely large training sets. Until now, their use for neuron identification has been limited. For example, in one tracking algorithm, artificial neural networks provide only the initialization, or first guess, for non-rigid registration (<xref ref-type="bibr" rid="bib39">Wen et al., 2018</xref>).</p><p>Our approach is based on a simple insight: it is straightforward to generate very large semi-synthetic datasets of test and template worms that nonetheless are derived from measurements. We use neural positions extracted from existing imaging datasets, and then apply known, nonlinear transformations to warp those positions into new shapes for other body postures, or other individuals. Furthermore, we simulate the types of noise that appear in real datasets, such as missing or spurious neurons. Using these large-scale semi-synthetic datasets, we train an artificial neural network to map the simulated neural positions back to the ground truth. Given sufficient training data (which we can generate at will), the network learns the most informative features of the neural configurations, rather than requiring the user to specify them by hand.</p><p>Importantly, using semi-synthetic data also allows us to train our model even when we completely lack experimentally acquired ground truth data. And indeed, in this work, semi-synthetic data is derived exclusively from measurements that lack any ground truth correspondence either within-, or across animals. All ground truth for training comes only from simulation. Realistic synthetic, semi-synthetic or augmented datasets have been key to cracking other challenging problems in neurosicence (<xref ref-type="bibr" rid="bib26">Parthasarathy et al., 2017</xref>; <xref ref-type="bibr" rid="bib43">Yoon et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Sun et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Lee et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Mathis and Mathis, 2020</xref>; <xref ref-type="bibr" rid="bib29">Pereira et al., 2020</xref>) and have already shown promising potential for tracking neurons (<xref ref-type="bibr" rid="bib39">Wen et al., 2018</xref>).</p><p>In this work, we use semi-synthetic data to train a Transformer network, an artificial neural network architecture that has shown great success in natural language processing tasks (<xref ref-type="bibr" rid="bib37">Vaswani et al., 2017</xref>). Transformers incorporate an attention mechanism that can leverage similarities between pairs of inputs to build a rich representation of the input sequence for downstream tasks like machine translation and sentiment prediction. We reasoned this same architecture would be well-suited to extract spatial relationships between neurons in order to build a representation that facilitates finding correspondence to neurons in a template worm.</p><p>Not only is the Transformer well-suited to learning features for the neural correspondence problem, it also obviates the need to straighten (<xref ref-type="bibr" rid="bib28">Peng et al., 2008</xref>) the worm in advance. Until now, existing methods have either required the worm to be straightened in preprocessing (<xref ref-type="bibr" rid="bib3">Bubnis et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref>) or explicitly transformed them during inference (<xref ref-type="bibr" rid="bib36">Varol et al., 2020</xref>; <xref ref-type="bibr" rid="bib22">Nejatbakhsh et al., 2020</xref>). Straightening the worm is a non-trivial task, and it is especially error-prone for complicated poses such as when the worm rolls along its centerline.</p><p>Finally, one of the main advantages of the Transformer architecture is that it permits parallel processing of the neural positions using modern GPU hardware. In contrast to existing methods, which have not been optimized for speed, the Transformer can make real-time predictions once it has been trained. This speed is a necessary step toward bringing real-time applications (<xref ref-type="bibr" rid="bib5">Clancy et al., 2014</xref>; <xref ref-type="bibr" rid="bib9">Hochbaum et al., 2014</xref>) to freely moving animals.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Fast deep neural correspondence accurately matches neurons across semi-synthetic individuals</title><p>We developed a fast deep neural correspondence (fDNC) model that seeks to find the correspondence between configurations of <italic>C. elegans</italic> neurons in different individuals or in the same individual across time (<xref ref-type="fig" rid="fig1">Figure 1</xref>). We used a deep learning artificial neural network architecture, called the transformer architecture (<xref ref-type="bibr" rid="bib37">Vaswani et al., 2017</xref>), that specializes at finding pairs of relations in datasets, <xref ref-type="fig" rid="fig1">Figure 1F</xref>. The transformer architecture identified similarities across spatial relations of neurons in a test and a template to identify correspondences between the neurons.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Fast deep neural correspondence model.</title><p>(<bold>A–D</bold>) Schematic of training and analysis pipeline for using the fast Deep Neural Correspondence (fDNC) model to predict correspondence between neurons across individuals. (<bold>A</bold>) Volumetric images of fluorescent labeled neuronal nuclei are segmented to extract neuron positions. (Scale bar, 10 µm). (<bold>B</bold>) Semi-synthetic training data is generated with a simulator. The simulator transforms the neural positions of a real worm and introduces noise to generate new semi-synthetic individuals. Approximately <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> neuron configurations without labels from 12 moving worms were used to generate 2.304 × 10<sup>5</sup> labeled semi-synthetic worms for training. (<bold>C</bold>) During training, the fDNC model finds optimal internal parameters to minimize the difference between predicted neural correspondence and true correspondence in pairs of semi-synthetic worms. (<bold>D</bold>) Given positions for neurons in real worm A and positions for neurons in real worm B, the trained model predicts correspondences between them. Furthermore,if labels for neurons in A are known, the model can then assign corresponding labels to neurons in worm B. (<bold>E</bold>) Detailed schematic of the simulator from panel B. (<bold>F</bold>) Transformer architecture of the fDNC model. The position features of a template worm with <inline-formula><mml:math id="inf2"><mml:mi>n</mml:mi></mml:math></inline-formula> neurons and a test worm with <inline-formula><mml:math id="inf3"><mml:mi>m</mml:mi></mml:math></inline-formula> neurons are taken as input. The features are computed via a multi-head attention mechanism. ‘Add and Norm’ refers to an addition and layer normalization step. <inline-formula><mml:math id="inf4"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:mi>b</mml:mi></mml:math></inline-formula> are neuron positions and <inline-formula><mml:math id="inf6"><mml:mi>u</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf7"><mml:mi>v</mml:mi></mml:math></inline-formula> are embeddings for the template and test, respectively. We choose the number of layers <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula> and the embedding dimension <inline-formula><mml:math id="inf9"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math></inline-formula> by evaluating the performance on a held-out validation set.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66410-fig1-v2.tif"/></fig><p>Within a single individual, neural positions vary as the worm moves, deforms, and changes its orientation and pose. Across isogenic individuals, there is an additional source of variability that arises from the animal’s development. In practice, further variability also arises from experimental measurements: neuron positions must first be extracted from fluorescent images (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), and slight differences in label expression, imaging artifacts, and optical scattering all contribute to errors in segmenting individual neurons.</p><p>We created a simulator to model these different sources of variability and used it to generate realistic pairs of empirically derived semi-synthetic animals with known correspondence between their neurons for training our model (<xref ref-type="fig" rid="fig1">Figure 1B,E</xref>). The simulator took configurations of neuron positions that lacked ground truth from real worms as inputs and then scaled and deformed them, forced them to adopt different poses sampled from real worms, and then introduced additional sources of noise to generate many new semi-synthetic individuals. We then trained our fDNC model on these experimentally derived semi-synthetic individuals of different sizes and poses.</p><p>Training our model on the empirically derived semi-synthetic data offered advantages compared to experimentally acquired data. First, it allowed us to train on larger datasets than would otherwise be practical. We trained on 2.304 × 10<sup>5</sup> semi-synthetic individuals, but only seeded our simulator with unlabeled neural configurations from experimentally acquired recordings of 12 individuals (4 × 10<sup>3</sup> volumes spread across the 12 individuals, all of which lacked ground-truth correspondence). Second, we did not need to provide ground truth correspondence because the simulator instead generates its own ground truth correspondence between semi-synthetic individuals, thereby avoiding a tedious and error prone manual step. Consequently, no experimentally acquired ground truth correspondence was used to train the model. Later in the work, we use ground truth information from human annotated NeuroPAL (<xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref>) strains to evaluate the performance of our model, but no NeuroPAL strains were used for training. Importantly, the amount of test data with ground truth correspondence needed for evaluating performance is much smaller than the amount of training data that would be needed for training. Third, by using large and varied semi-synthetic data, we force the model to generalize its learning to a wide range of variabilities in neural positions and we avoid the risks of overtraining on idiosyncrasies specific to our imaging conditions or segmentation. Overall, we reasoned that training with semi-synthetic data should make the model more robust and more accurate across a wider range of conditions, orientations and animal poses than would be practical with experimentally acquired ground-truth datasets.</p><p>We trained our fDNC model on 2.304 × 10<sup>5</sup> semi-synthetic individuals (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) and then, after training, evaluated its performance on 2000 additional held-out semi-synthetic pairs of individuals which had not been accessible to the model during training, <xref ref-type="fig" rid="fig1">Figure 1D</xref> and <xref ref-type="fig" rid="fig2">Figure 2</xref>. Model performance was evaluated by calculating the accuracy of the models’ predicted correspondence with respect to the ground truth in pairs of semi-synthetic individuals. One individual is called the ‘test’ and the other is the ‘template’. Every neuron in the test or template, whichever has fewer is assigned a match. Accuracy is reported as the number of correctly predicted matches between test and template, divided by the total number of ground truth matches in the test and template pair. Our fDNC model achieved 96.5% mean accuracy on the 2000 pairs of held-out semi-synthetic individuals. We compared this performance to that of Coherent Point Drift (CPD) (<xref ref-type="bibr" rid="bib21">Myronenko and Song, 2010</xref>), a classic registration method used for automatic cell annotation. CPD achieved 31.1% mean accuracy on the same held-out semi-synthetic individuals. Our measurements show that the fDNC model significantly outperforms CPD at finding correspondence in semi-synthetic data. For the rest of the work, we use experimentally acquired human annotated data to evaluate performance.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>fDNC accurately predicts matches between neuron from semi-synthetic worms (<bold>A</bold>) Schematic of evaluation pipeline.</title><p>fDNC model performance is evaluated on pairs of semi-synthetic worms with known correspondence that had been held out from training. Given neural positions in worms A and B, the model predicts matches between A and B. Accuracy is the number of correctly predicted matches divided by the total number of ground truth matches for the A-B pair. (<bold>B</bold>) Model performance of a Coherent Point Drift Registration (CPD) is compared to the fDNC model on 2000 randomly selected pairs of held-out semi-synthetic individuals, without replacement. (<inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, Wilcoxon signed rank test).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66410-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>fDNC model training curve.</title><p>The loss was evaluated on a held-out test set consisting of 12,800 semi-synthetic worms for every 500 iterations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66410-fig2-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-2"><title>fDNC accurately tracks neurons within an individual across time</title><p>We next evaluated the fDNC model’s performance at tracking neurons within an individual over time, as is needed, for example, to measure calcium activity in moving animals (<xref ref-type="bibr" rid="bib38">Venkatachalam et al., 2016</xref>; <xref ref-type="bibr" rid="bib24">Nguyen et al., 2016</xref>). We evaluated model performance on an experimentally acquired calcium imaging recording of a freely moving <italic>C. elegans</italic> from <xref ref-type="bibr" rid="bib25">Nguyen et al., 2017</xref> in which a team of human experts had manually tracked and annotated neuron positions over time (strain AML32, 1514 volumes, six volumes per second, additional details are describeed in the 'Datasets' section of the 'Materials and methods.'). The recording has sufficiently large animal movement that the average distance a neuron travels between volumes (4.8 µm) is of similar scale to the average distance between nearest neuron neighbors (5.3 µm). The recording was excluded from training and from the set of recordings used by the simulator. We collected neuron configurations from all <inline-formula><mml:math id="inf11"><mml:mi>n</mml:mi></mml:math></inline-formula> time points during this recording to form <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> pairs of configurations upon which to evaluate the fDNC model. Each pair consisted of a test and template. The template was always from the same time point <inline-formula><mml:math id="inf13"><mml:mi>t</mml:mi></mml:math></inline-formula>, while the test was taken to be the volume at any of the other time points. We applied the pre-trained fDNC model to the pairs of neuron configurations and compared the model’s predicted correspondence to the ground truth from manual human tracking (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Across the pairs, the fDNC model showed a mean accuracy of 79.1%. We emphasize that the fDNC model achieved this high accuracy on tracking a real worm using only neuron position information even though it is trained exclusively on semi-synthetic data.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Tracking neurons within an individual across time.</title><p>(<bold>A</bold>) Schematic shows how the pose and orientation of a freely moving animal change with time. Black dot indicates head. (<bold>B</bold>) Pipeline to evaluate the fDNC model at tracking neurons within an individual across time. The fDNC model takes in positional features of a template neuron configuration from one time <italic>t</italic><sub>1</sub> of a freely moving worm, and predicts the correspondence at another time <italic>t</italic><sub>2</sub>, called the test. Recording is of a moving animal undergoing calcium imaging from <xref ref-type="bibr" rid="bib25">Nguyen et al., 2017</xref>. Ground truth neuron correspondence are provided by manual human annotation. The same time point is used as the template for all 1513 template-test pairs. (<bold>C</bold>) Performance of fDNC and alternative models at tracking neurons within an individual are displayed in order of mean performance. CPD refers to Coherent Point Drift. NeRVE(1) refers to the restricted NeRVE model that has access to only the same template as CPD and fDNC. NeRVE(100) refers to the full NeRVE model which uses 100 templates from the same individual to make a single prediction. A Wilcoxon signed rank significance test of fDNC’s performance compared to CPD, NeRVE(1) and NeRVE(100) yields <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>2.5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>223</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>1.3</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>140</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf15"><mml:mrow><mml:mn>1.5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>102</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, respectively. Boxplots show median and interquartile range. (<bold>D</bold>) fDNC tracking performance by neuron. Cumulative fraction of neurons is shown as a function of the acceptable error rate. (<bold>E</bold>) Detailed comparison of fDNC tracking to human annotation of a moving GCaMP recording from <xref ref-type="bibr" rid="bib25">Nguyen et al., 2017</xref>. Color at each time point indicates the neuron label manually annotated by a human. White gaps indicate that the neuron is missing at that time point. In the case of perfect agreement between human and fDNC, each row will have only a single color or white.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66410-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Example of fDNC used to track neurons during free moving calcium imaging.</title><p>fDNC applied to a published calcium imaging dataset of a freely moving worm with an additional label in neurons AVAL and AVAR (<xref ref-type="bibr" rid="bib6">Hallinen et al., 2021</xref>). (<bold>a</bold>) Calcium activity extracted using fDNC. (<bold>b</bold>) Neurons AVAL and AVAR exhibit expected calcium transients when the worm exhibits negative velocity. AVAL and AVAR’s identity were confirmed by cell-specific BFP expression. The fDNC model was blinded to the BFP channel.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66410-fig3-figsupp1-v2.tif"/></fig></fig-group><p>We compared the performance of our fDNC model to that of CPD Registration, and to Neuron Registration Vector Encoding and clustering (NeRVE), a classical computer vision model that we had previously developed specifically for tracking neurons within a moving animal over time (<xref ref-type="bibr" rid="bib25">Nguyen et al., 2017</xref>; <xref ref-type="fig" rid="fig3">Figure 3C</xref>). fDNC clearly outperformed CPD achieving 79.1% accuracy compared to CPD’s 62.7%.</p><p>Both CPD and fDNC predict neural correspondence of a test configuration by comparing only to a single template. In contrast, the NeRVE method takes 100 templates, where each one is a different neuron configuration from the same individual, and uses them all to inform its prediction. The additional templates give the NeRVE method extra information about the range of possible neural configurations made by the specific individual whose neurons are being tracked. We therefore compared the fDNC model both to the full NeRVE method and also to a restricted version of the NeRVE method in which NeRVE had access only to the same single template as the CPD or fDNC models. (Under this restriction, the NeRVE method no longer clusters and the method collapses to a series of gaussian mixture model registrations [<xref ref-type="bibr" rid="bib10">Jian and Vemuri, 2011</xref>]). In this way, we could compare the two methods when given the same information. fDNC’s mean performance of 79.1% was statistically significantly more accurate than the restricted NeRVE model (mean 73.1%, <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1.3</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>140</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, Wilcoxon signed rank test). The full NeRVE model that had access to additional templates outperformed the fDNC model slightly (82.9% <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1.5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>102</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, Wilcoxon signed rank test).</p><p>Because CPD, NeRVE, and fDNC are all time-independent algorithms, their performance on a given volume is the same, even if nearby volumes are omitted or shuffled in time. One benefit of this approach is that errors from prior volumes do not accumulate over the duration of the recording. To visualize performance over time, we show a volume-by-volume comparison of fDNC’s tracking to that of a human (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). We also characterize model performance on a per neuron basis (<xref ref-type="fig" rid="fig3">Figure 3D</xref>).</p><p>Finally, we used fDNC to extract whole brain calcium activity from a previously published recording of a moving animal in which two well-characterized neurons AVAL and AVAR were unambiguously labeled with an additional colored fluorophore (<xref ref-type="bibr" rid="bib6">Hallinen et al., 2021</xref>; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>, <xref ref-type="video" rid="video1">Video 1</xref>). Calcium activity extracted from neurons AVAL and AVAR exhibited calcium activity transients when the animal underwent prolonged backward locomotion, as expected (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). We conclude that the fDNC model is suitable for tracking neurons across time and performs similarly to the NeRVE method.</p><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-66410-video1.mp4"><label>Video 1.</label><caption><title>Video of neuron tracking during calcium imaging in moving animal.</title><p>fDNC algorithm is applied to a calcium imaging recording from <xref ref-type="bibr" rid="bib6">Hallinen et al., 2021</xref> (six volumes per second, 200 planes per second). Same recording as in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. Images are shown from the RFP channel and show nuclear localized tagRFP in each neuron. For each volume, a single optical plane is shown that contains neuron AVAR (labeled in pink). Labels assigned by fDNC are shown. Color indicates whether the neuron resides in the displayed optical plane (green), or up to two planes above or below (white). The time of the video corresponding to <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> is shown on the left top corner.</p></caption></media><p>In the following sections, we further show that the fDNC method is orders of magnitude faster than NeRVE. Moreover, unlike NeRVE which can only be used within an individual, fDNC is also able to predict the much more challenging neural correspondence across individuals.</p></sec><sec id="s2-3"><title>fDNC is fast enough for future real-time applications</title><p>Because it relies on an artificial neural network, the fDNC model finds correspondence for a set of neurons faster than traditional methods (<xref ref-type="table" rid="table1">Table 1</xref>). From the time that a configuration of segmented neurons is loaded onto a GPU, it takes only an average of 10 ms for the fDNC model to predict correspondence for all neurons on a 2.4 GHz Intel machine with an NVIDIA Tesla P100 GPU. If not using a GPU, the model predicts correspondence for all neurons in 50 ms. In contrast, on the same hardware it takes CPD 930 ms and it takes NeRVE on average over 10 s. The fDNC model may be a good candidate for potential closed-loop tracking applications because its speed of 100 volumes per second is an order of magnitude faster than the 6–10 volumes per second recording rate typically used in whole-brain imaging of freely moving <italic>C. elegans</italic> (<xref ref-type="bibr" rid="bib24">Nguyen et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Venkatachalam et al., 2016</xref>). We note that for a complete closed-loop tracking system, fast segmentation algorithms will also be needed in addition to the fast registration and labeling algorithms presented here. The fDNC model is agnostic to the details of the segmentation algorithm so it is well suited to take advantage of fast segmentation algorithms when they are developed.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Time required to predict neural correspondence.</title><p>Table shows the measured time per volume required for different models to predict neural correspondence of a single volume. Time required is measured after neuron segmentation is complete and a configuration of neural positions has been loaded into memory. The same hardware is used for all models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Time (s/Volume)</th></tr></thead><tbody><tr><td>CPD (<xref ref-type="bibr" rid="bib21">Myronenko and Song, 2010</xref>)</td><td>0.93</td></tr><tr><td>NeRVE(1) (<xref ref-type="bibr" rid="bib25">Nguyen et al., 2017</xref>)</td><td>10</td></tr><tr><td>NeRVE(100) (<xref ref-type="bibr" rid="bib25">Nguyen et al., 2017</xref>)</td><td>&gt;10</td></tr><tr><td>fDNC [this work]</td><td>0.01</td></tr></tbody></table></table-wrap><p>The fDNC model uses built-in libraries to parallelize the computations for labeling a single volume, and this contributes to its speed. In particular, each layer of the neural network contains thousands of artificial neurons performing the same computation. Computations for each neuron in a layer can all be performed in parallel and modern GPUs have as many as 3500 CUDA cores.</p><p>In practice, the method is even faster for post-processing applications (not-realtime) because it is also parallelizable at the level of each volume. Labeling one volume has no dependencies on any previous volumes and therefore each volume can be processed simultaneously. The number of volumes to be processed in parallel is limited only by the number of volumes that can be loaded onto the memory of a GPU. When tracking during post-processing in this work, we used 32 volumes simultaneously.</p></sec><sec id="s2-4"><title>fDNC accurately finds neural correspondence across individuals</title><p>Having shown that fDNC performs well at identifying neurons within the same individual, we wanted to assess its capability to identify neurons across different animals, using neural position information alone, as before. Identifying corresponding neurons across individuals is crucial for studying the nervous system. However, finding neural correspondence across individuals is more challenging than within an individual because there is variability in neuronal position from both the animal’s movement as well as from development. To evaluate the fDNC model’s performance at finding neural correspondence across individuals using only position information, we applied the same semi-synthetically-trained fDNC model to a set of 11 NeuroPAL worms. NeuroPAL worms contain extra color information that allows a human to assign ground truth labels to evaluate the model’s performance. Crucially, the fDNC model was blinded to this additional color information. In these experiments, NeuroPAL color information was only used to evaluate performance after the fact, not to find correspondence.</p><p>NeuroPAL worms have multicolor neurons labeled with genetically encoded fluorescent proteins (<xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref>). Only a single volume was recorded for each worm since immobilization is required to capture multicolor information from the NeuroPAL strain. For each of the 11 Neuropal recording, neurons were automatically segmented and manually annotated based on the neuron’s position and color features as described in <xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref> (see <xref ref-type="fig" rid="fig4">Figure 4A,B</xref>). Across the 11 animals, a human assigned a ground-truth label to a mean of 43% of segmented head neurons, providing approximately 58 labeled neurons per animal (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, additional details in 'Datasets' section of 'Materials and Methods'). The remaining segmented neurons were not confidently identifiable by the human and thus were left without ground truth labels. We selected as template the recording that contained the largest number of confidently labeled ground turth human annotated neurons. We evaluated our model by comparing its predicted correspondence between neurons in the other 10 test datasets and this template, using only position information (no color information). All 11 ground-truth recordings were held-out in that they were not involved in the generation of the semi-synthetic data that had been used to train the model.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>fDNC model finds neural correspondence across individuals.</title><p>(<bold>A</bold>) Fluorescence image shows neuronal nuclei of a NeuroPAL worm. A single optical slice is shown from an optical stack. (Scale bar, 10 µm). Genetically encoded color labels in NeuroPAL animals aid ground truth manual neural identification (<xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref>) and are used here to evaluate performance. Black dots indicate neurons found via automatic segmentation. (<bold>B</bold>) Locations of all segmented neurons from A. Neurons that additionally have a human annotated label are shown in green. Those that a human was unable to label are red. (<bold>C</bold>) Number of segmented neurons (mean 133.6) and subset of those that were given human annotations (mean 57.5) is shown for 11 NeuroPAL individuals. Box plot shows median and interquartile range. (<bold>D</bold>) Pipeline to evaluate fDNC model performance across NeuroPAL individual is shown. Predicted labels are compared with human annotated labels to compute accuracy. (<bold>E</bold>) Performance of the fDNC model and CPD is shown evaluated on NeuroPAL recordings using position information alone. Accuracy is the fraction of labeled neurons present in both test and template that are correctly matched. Performance is evaluated on 10 pairs of 11 recordings, where the template is always the same (Worm A). (<inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.005</mml:mn></mml:mrow></mml:math></inline-formula>, Wilcoxon signed-rank test). (<bold>F</bold>) Performance evaluated on a separate publicly accessible dataset of nine NeuroPAL individuals from <xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref> (<inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.018</mml:mn></mml:mrow></mml:math></inline-formula>, Wilcoxon signed-rank test).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66410-fig4-v2.tif"/></fig><p>We applied the synthetically trained fDNC model to each pair of held-out NeuroPAL test and template recordings and calculated the accuracy as the number of correctly predicted matches divided by the total number of ground truth matches in the pair. Across the 10 pairs of NeuroPAL recordings using position information alone, the fDNC model had a mean accuracy of 64.1%, significantly higher than the CPD method’s accuracy of 53.1% (<inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.005</mml:mn></mml:mrow></mml:math></inline-formula>, Wilcoxon signed-rank test).</p><p>We wondered whether we could better use the likelihood information about potential matches generated by the algorithm. For each neuron <inline-formula><mml:math id="inf21"><mml:mi>i</mml:mi></mml:math></inline-formula> in the test recording, the fDNC model computes a relative confidence with which that neuron corresponds to each possible neuron <inline-formula><mml:math id="inf22"><mml:mi>j</mml:mi></mml:math></inline-formula> in the template, <inline-formula><mml:math id="inf23"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. A Hungarian algorithm finds the most probable match by considering all <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>s for all neurons in the test. By default we use this best match in evaluating performance. The <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>s also provide the user with a list of alternative matches ranked by the model’s estimate of their respective likelihood. We therefore also assessed the accuracy for the top three most likely matches.</p><p>Given <inline-formula><mml:math id="inf26"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf27"><mml:mi>j</mml:mi></mml:math></inline-formula> are ground truth matches, we asked whether the value <inline-formula><mml:math id="inf28"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is among the top three values of the set <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> where <inline-formula><mml:math id="inf30"><mml:mi>k</mml:mi></mml:math></inline-formula> can be chosen from all the neurons in the template. We defined accuracy as the number of instances in which this criteria was met, divided by the number of ground truth matches in the test template pair. When considering the top three neurons, the fDNC model achieves an accuracy of 76.6% using only position information.</p></sec><sec id="s2-5"><title>Validating on an alternative dataset</title><p>Data quality, selection criteria, human annotation, hardware, segmentation, and preprocessing can all vary from lab to lab making it challenging to directly compare methods. To validate our model against different measurement conditions and to allow for a direct comparison with another recent method, we applied our fDNC model to a previously published dataset of 9 NeuroPAL individuals (<xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref>). This public dataset used different imaging hardware and conditions and was annotated by human experts from a different group. On this public dataset, using position information alone, our method achieved 78.2% accuracy while CPD achieved 58.9%, <xref ref-type="fig" rid="fig4">Figure 4F</xref>. When assessing the top three candidate accuracy, the fDNC model performance was 91.3%. The fDNC model performance was overall higher on the published dataset than on our newly collected dataset presented here. This suggests that our method performs well when applied to real-world datasets in the literature.</p><p>We further sought to compare the fDNC model to the reported accuracy of a recent model called Conditional Random Fields (CRF) from <xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref> by comparing their performance on the same published dataset from that work. There are fundamental differences between the two methods that make a direct comparison of their performance challenging. CRF assigns labels to a test worm. In contrast, fDNC assigns matches between two worms or two configurations, the test and template. To evaluate whether a match is correct using fDNC, we require a ground truth label in both test and template. Consequently, our denominator for accuracy is the intersection of neurons with ground truth labels in test and template. In contrast, the denominator for evaluating accuracy of the CRF model is all neurons with ground truth labels in the test.</p><p>When applied to the same dataset in <xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref>, fDNC had an accuracy of 78%. But for the purposes of comparison with CRF this could, in principle, correspond to an accuracy of 61.2–82.5%, depending on how well those neurons in the test that lack ground truth labels in the template were matched. These bounds are calculated for the extreme cases in which neurons with ground truth labels in the test but not in the template are either all matched incorrectly (61.2%) or all matched perfectly (82.5%). Seventy-eight percent is the accuracy under the assumption that those neurons with ground truth labels in the test but not in the template are correctly matched at the same rate as those neurons with ground truth labels in both. In other words, we assume the neurons we have ground truth information about are representative of the ones we don’t. For the sake of comparison, we use this assumption to compare fDNC to the published values of CRF (<xref ref-type="table" rid="table2">Table 2</xref>).</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Comparison of across-animal model performance on additional dataset.</title><p>Table lists reported mean accuracy of different models evaluated on the same publicly accessible dataset from <xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref>. We note in the text an assumption needed to compare these methods. <inline-formula><mml:math id="inf31"><mml:mi>N</mml:mi></mml:math></inline-formula> indicates the number of template-test pairs used to calculate accuracy. (CRF method uses an atlas as the template, whereas we randomly take one of the nine individuals and designate that as the template). CPD and fDNC performance on this dataset are also shown in <xref ref-type="fig" rid="fig4">Figure 4F</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Accuracy</th><th><inline-formula><mml:math id="inf32"><mml:mi>N</mml:mi></mml:math></inline-formula></th><th>Reported in</th></tr></thead><tbody><tr><td>CPD</td><td>59%</td><td>8</td><td>This work</td></tr><tr><td>CRF (open atlas)</td><td><inline-formula><mml:math id="inf33"><mml:mrow><mml:mi/><mml:mo>≈</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math></inline-formula>%</td><td>9</td><td><xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref></td></tr><tr><td>CRF (data driven atlas)</td><td>74%</td><td>9</td><td><xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref></td></tr><tr><td>fDNC</td><td>78%</td><td>8</td><td>This work</td></tr></tbody></table></table-wrap><p>fDNC accuracy is higher than the reported performance for the open atlas variant of CRF. Under the specific assumption described above, it is also higher than the data driven atlas variant, although we note that this could change with different assumptions, and we are unable to test for statistical significance. The fDNC method also offers other advantages compared to the CRF approach in that the fDNC method is optimized for speed and avoids the need to transform the worm into a canonical coordinate system. Importantly, compared to the data-driven atlas variant of the CRF, the fDNC model has an advantage in that it does not require assembling a data-driven atlas from representative recordings with known ground-truth labels. Taken together, we conclude that the fDNC model’s accuracy is comparable to that of the CRF model while also providing other advantages.</p></sec><sec id="s2-6"><title>Incorporating color information</title><p>Our method only takes positional information as input to predict neural correspondence. However, when additional features are available, the position-based predictions from the fDNC model can be combined with predictions based on other features to improve overall performance. As demonstrated in <xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref>, adding color features from a NeuroPAL strain can reduce the ambiguity of predicting neural correspondence. We applied a very simple color model to calculate the similarity of color features between neuron <inline-formula><mml:math id="inf34"><mml:mi>i</mml:mi></mml:math></inline-formula> in the test recording to every possible neuron <inline-formula><mml:math id="inf35"><mml:mi>j</mml:mi></mml:math></inline-formula> in the template. The color model returns matching probabilities, <inline-formula><mml:math id="inf36"><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mtext>c</mml:mtext></mml:msubsup></mml:math></inline-formula> based on the Kullback-Liebler divergence of the normalized color spectra in a pair of candidate neurons (details described in Materials and methods). The color model is run in parallel to the fDNC model (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Overall matching probabilities <inline-formula><mml:math id="inf37"><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mtext>all</mml:mtext></mml:msubsup></mml:math></inline-formula> that incorporate both color and position information are calculated by combining the color matching probabilities <inline-formula><mml:math id="inf38"><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mtext>c</mml:mtext></mml:msubsup></mml:math></inline-formula> with the position probabilities <inline-formula><mml:math id="inf39"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The Hungarian algorithm is run on the combined matching algorithm to predict the best matches.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>fDNC performance when incorporating color features.</title><p>(<bold>A</bold>) Pipeline to evaluate fDNC performance across animals with additional color features. A simple color model is added in parallel to the fDNC model to use both color and position information from 11 NeuroPAL recordings. Accuracy is calculated from ground truth human annotation and is the fraction of labeled neurons present in both test and template that are correctly matched. Matching probabilities from the color and fDNC models are combined to form the final matching probabilities. (<bold>B</bold>) Accuracy of the position-only fDNC model and the combined fDNC and color model are evaluated on 11 NeuroPAL recordings (same recordings as in <xref ref-type="fig" rid="fig4">Figure 4</xref>). <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>5.0</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, Wilcoxon signed rank test.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66410-fig5-v2.tif"/></fig><p>Adding color information increased the fDNC model’s average accuracy from 64.1% to 74.7% (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) when evaluated on our dataset, and improved the accuracy in every recording evaluated. The top three candidate labels attained 92.4% accuracy. Accuracy was calculated from a comparison to human ground truth labeling, as before.</p><p>We chose a trivially simple color model in part to demonstrate the flexibility with which the fDNC model framework can integrate information about other features. Since our simple color model utilized no prior knowledge about the distributions of colors in the worm, we would expect a more sophisticated color model, for example, the statistical model used in <xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref>, to do better. And indeed that model evaluated on a different dataset is reported to have a higher performance with color than our model on our dataset (86% reported accuracy in <xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref> compared to 75% for the fDNC evaluated here). But that model also performs much worse than fDNC when both are restricted to use only neural position information (50% reported accuracy for <xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref> compared to 64% for the fDNC). Together, this suggests the fDNC model framework can take advantage of additional feature information like color and still perform relatively well when such information is missing.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Identifying correspondence between constellations of neurons is important for resolving two classes of problems: The first is tracking the identities of neurons across time in a moving animal. The second is mapping neurons from one individual animal onto another, and in particular onto a reference atlas, such as one obtained from electron microscopy (<xref ref-type="bibr" rid="bib41">Witvliet et al., 2020</xref>). Mapping onto an atlas allows recordings of neurons in the laboratory to be related to known connectomic, gene expression, or other measurements in the literature.</p><p>The fDNC model finds neural correspondence within and across individuals with an accuracy that is comparable or compares favorable to other methods. The model focuses primarily on identifying neural correspondence using position information alone. For tracking neurons within an individual using only position, fDNC achieves a high accuracy of 79%, while for across individuals using only position it achieves 64% accuracy on our dataset and 78% on a published dataset from another group.</p><p>We expect that an upper bound may exist, set by variability introduced during the animal’s development, that ultimately limits the accuracy with which any human or algorithm can find correspondence across individuals via only position information. For example, pairs of neurons in one individual that perfectly switch position with respect to another individual will never be unambiguously identified by position alone. It is unclear how close fDNC’s performance of 64% on our dataset or 78% on the dataset in <xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref> comes to this hypothetical upper bound, but there is reason to think that at least some room for improvement remains.</p><p>Specifically, we do not expect accuracy at tracking within an individual to be fundamentally limited, in part because we do not expect two neurons to perfectly switch position on the timescale of a single recording. Therefore fDNC’s 79% accuracy within-individuals suggests room for improving within-individual correspondence, and by extension, across-individual correspondence because the latter necessarily includes all of the variability of the former. One avenue for achieving higher performance could be to improve the simulator’s ability to better capture variability of a real testset, for example by using different choices of parameters in the simulator.</p><p>Even at the current level of accuracy, the ability to find correspondence across animals using position information alone remains useful. For example, we are interested in studying neural population coding of locomotion in <italic>C. elegans</italic> (<xref ref-type="bibr" rid="bib6">Hallinen et al., 2021</xref>), and neural correspondence at 64% accuracy will allow us to reject null hypotheses about the extent to which neural coding of locomotion is stereotyped across individuals.</p><p>The fDNC model framework also makes it easy to integrate other features which further improve accuracy. We demonstrated that color information could be added by integrating the fDNC model with a simple color model to increase overall accuracy. We expect that performance would improve further with a more sophisticated color model that takes into account the statistics of the colors in a NeuroPAL worm (<xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref>).</p><p>The fDNC model framework offers a number of additional advantages beyond accuracy. First, it is versatile and general. The same pre-trained model performed well at both tracking neurons within a freely moving individual across time and at finding neural correspondence across different individuals. Without any additional training, it achieved even higher accuracy on a publicly accessible dataset acquired on different hardware with different imaging conditions from a different group. This suggests that the framework should be applicable to many real-world datasets. The model provides probability estimates of all possible matches for each neuron. This allows an experimenter to consider a collection of possible matches such as the top three most likely.</p><p>In contrast to previous methods, an advantage of the fDNC method is that it does not require the worm to be straightened, axis aligned, or otherwise transformed into a canonical coordinate system. This eliminates an error-prone and often manual step. Instead, the fDNC model finds neural correspondence directly from neural position information even in worms that are in different poses or orientations.</p><p>Importantly, the model is trained entirely on semi-synthetic data, which avoids the need for large experimentally acquired ground truth datasets to train the artificial neural network. Acquiring ground truth neural correspondence in <italic>C. elegans</italic> is time consuming, error prone, and often requires manual hand annotation. The ability to train the fDNC model with semi-synthetic data derived from measurements alleviates this bottleneck and makes the model attractive for use with other organisms with stereotyped nervous systems where ground truth datasets are similarly challenging to acquire.</p><p>The model is also fast and finds neural correspondence of a new neural configuration in 10 ms. The development of fast algorithms for tracking neurons are an important step for bringing real-time closed loop applications such as optical brain-machine interfaces (<xref ref-type="bibr" rid="bib5">Clancy et al., 2014</xref>) and optical patch clamping (<xref ref-type="bibr" rid="bib9">Hochbaum et al., 2014</xref>) to whole-brain imaging in freely moving animals. By contrast, existing real-time methods for <italic>C. elegans</italic> in moving animals are restricted to small subsets of neurons, are limited to two-dimensions, and work only at low spatial resolution (<xref ref-type="bibr" rid="bib17">Leifer et al., 2011</xref>; <xref ref-type="bibr" rid="bib32">Stirman et al., 2011</xref>; <xref ref-type="bibr" rid="bib12">Kocabas et al., 2012</xref>; <xref ref-type="bibr" rid="bib31">Shipley et al., 2014</xref>). We note that to be used in a real-time closed loop application, our fDNC model would need to be combined with faster segmentation algorithms because current segmentation algorithms are too slow for real-time use. Because segmentation can be easily paralellized, we expect that faster segmentation algorithms will be developed soon.</p><p>Many of the advantages listed here stem from the fDNC model’s use of the transformer architecture (<xref ref-type="bibr" rid="bib37">Vaswani et al., 2017</xref>) in combination with supervised learning. The transformer architecture, with its origins in natural language processing, is well suited to find spatial relationships within a configuration of neurons. By using supervised learning on empirically derived semi-synthetic training data of animals in a variety of different poses and orientations, the model is forced to learn relative spatial features within the neurons that are informative for finding neural correspondence across many postures and conditions. Finally, the transformer architecture leverages recent advances in GPU parallel processing for speed and efficiency, which is an important step toward future real-time applications.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th>Reagent type (species) or resource</th><th>Designation</th><th>Source or reference</th><th>Identifiers</th><th>Additional information</th></tr></thead><tbody><tr><td>Strain, strain background (<italic>C. elegans</italic>)</td><td>AML320</td><td>this work</td><td/><td>See <xref ref-type="table" rid="table4">Table 4</xref></td></tr><tr><td>Strain, strain background (<italic>C. elegans</italic>)</td><td>OH15262</td><td><xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref></td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/WB-STRAIN:WBStrain00047397">WB-STRAIN:WBStrain00047397</ext-link></td><td/></tr></tbody></table></table-wrap><sec id="s4-1"><title>Datasets</title><sec id="s4-1-1"><title>Recordings used by simulator to generate semi-synthetic data</title><p>Our model was trained on a semi-synthetic training dataset that was simulated from 4000 volumes spread across recordings of 12 freely moving animals of strain AML32 acquired during calcium imaging. The recordings fed to the simulator had no ground truth correspondence either within or across animals. Each recording had originally contained approximately 3000 volumes recorded at six volumes/s. The recordings fed to the simulator were set aside after use by the simulator and were never re-used for evaluating model performance.</p></sec><sec id="s4-1-2"><title>Datasets used to evaluate performance</title><p>The model’s performance was evaluated on various types of datasets with ground truth correspondence, as shown in <xref ref-type="table" rid="table3">Table 3</xref>. All these recordings were held-out in the sense that they were never used for training. Some of these recordings had ground truth correspondence within an individual over time, while others had ground truth correspondence across individuals. One of the NeuroPAL (<xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref>) datasets is a published dataset from an independent research group (<xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref>). One of the calcium imaging datasets, from <xref ref-type="bibr" rid="bib6">Hallinen et al., 2021</xref>, had no ground truth correspondence and served to demonstrate the model’s ability to extract calcium activity.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Ground truth content, by dataset.</title><p>Table lists ground truth properties for each dataset used in this work to evaluate the model. None of the datasets listed here were used for training. ‘Vol’ refers to volume and ‘indiv’ refers to individuals. Ground truth ‘matches pair<sup>−1</sup>’ indicates the average number of ground truth matches for random pairs of test and template, which is a property of the ground truth dataset, and does not depend on the model tested.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Held-out semi-synthetic testset</th><th>Ca<sup>2+</sup> imaging</th><th>NeuroPAL</th><th>NeuroPAL</th><th>Ca<sup>2+</sup> imaging</th></tr></thead><tbody><tr><td>Figure</td><td><xref ref-type="fig" rid="fig2">Figure 2</xref></td><td><xref ref-type="fig" rid="fig3">Figure 3</xref></td><td><xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref></td><td><xref ref-type="fig" rid="fig4">Figure 4F</xref></td><td><xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref></td></tr><tr><td>Type</td><td>-</td><td>moving</td><td>immobile</td><td>immobile</td><td>moving</td></tr><tr><td>Correspondence</td><td>across indiv</td><td>within indiv</td><td>across indiv</td><td>across indiv</td><td>within indiv</td></tr><tr><td>Ground Truth</td><td>simulator</td><td>human</td><td>human</td><td>human</td><td>-</td></tr><tr><td>Ground truth matches pair<sup>−1</sup></td><td>85.7</td><td>64.4</td><td>50.1</td><td>50.5</td><td>-</td></tr><tr><td>Ground truth labels vol<sup>−1</sup></td><td>102.1</td><td>69.2</td><td>57.5</td><td>64.3</td><td>-</td></tr><tr><td>Segmented neurons vol<sup>−1</sup></td><td>114.1</td><td>118.4</td><td>133.6</td><td>118.8</td><td>131.1</td></tr><tr><td>Total Vols</td><td>2000</td><td>1514</td><td>11</td><td>9</td><td>1400</td></tr><tr><td>Individuals</td><td>2000</td><td>1</td><td>11</td><td>9</td><td>1</td></tr><tr><td>Vols indiv<sup>−1</sup></td><td>1</td><td>1514</td><td>1</td><td>1</td><td>1400</td></tr><tr><td>Vols s<sup>−1</sup></td><td>-</td><td>6</td><td>-</td><td>-</td><td>6</td></tr><tr><td>Strain</td><td>-</td><td>AML32</td><td>AML320 (via OH15262)</td><td>OH15495</td><td>AML310</td></tr><tr><td>Reference</td><td>this work</td><td><xref ref-type="bibr" rid="bib25">Nguyen et al., 2017</xref></td><td>this work</td><td><xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref></td><td><xref ref-type="bibr" rid="bib6">Hallinen et al., 2021</xref></td></tr></tbody></table></table-wrap></sec></sec><sec id="s4-2"><title>Data availability</title><p>Neural configurations acquired as part of this study have been posted in an Open Science Foundation repository with DOI:<ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.17605/OSF.IO/T7DZU">10.17605/OSF.IO/T7DZU</ext-link> available at <ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.17605/OSF.IO/T7DZU">https://dx.doi.org/10.17605/OSF.IO/T7DZU</ext-link>. The publicly accessible dataset from <xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref> is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/shiveshc/CRF_Cell_ID">https://github.com/shiveshc/CRF_Cell_ID</ext-link>, commit 74fb2feeb50afb4b840e8ec1b8ee7b7aaa77a426. Datasets from <xref ref-type="bibr" rid="bib25">Nguyen et al., 2017</xref> and <xref ref-type="bibr" rid="bib6">Hallinen et al., 2021</xref> are publicly available in repositories associated with their respective publications.</p></sec><sec id="s4-3"><title>Strains</title><p>Those strains used to create new datasets presented in this work are listed in Key Resources. All strains mentioned in this study, including those involved in previously published datasets, are listed in <xref ref-type="table" rid="table4">Table 4</xref>. All strains express a nuclear localized red fluorescent protein in all neurons. All but strains OH15495 and OH15262 also express nuclear localized GCaMP6s in all neurons. NeuroPAL (<xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref>) strains further express many additional fluorophores.</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>List of all strains mentioned in this work.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Strain</th><th>RRID</th><th>Genotype</th><th>Notes</th><th>Ref</th></tr></thead><tbody><tr><td>AML32</td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/WB-STRAIN:WBStrain00000192">WB-STRAIN:WBStrain00000192</ext-link></td><td>wtfIs5[P<italic>rab-3</italic>::NLS::GCaMP6s; P<italic>rab-3</italic>::NLS::tagRFP]</td><td/><td><xref ref-type="bibr" rid="bib25">Nguyen et al., 2017</xref></td></tr><tr><td>AML310</td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/WB-STRAIN:WBStrain00048356">WB-STRAIN:WBStrain00048356</ext-link></td><td>wtfIs5[P<italic>rab-3</italic>::NLS::GCaMP6s; P<italic>rab-3</italic>::NLS::tagRFP]; wtfEx258 [P<italic>rig-3</italic>::tagBFP::unc-54]</td><td/><td><xref ref-type="bibr" rid="bib6">Hallinen et al., 2021</xref></td></tr><tr><td>AML320</td><td/><td>(otIs669[NeuroPAL] V 14x; wtfIs145 [pBX + rab-3::his-24::GCaMP6::unc-54])</td><td>derived from OH15262</td><td>this work</td></tr><tr><td>OH15262</td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/WB-STRAIN:WBStrain00047397">WB-STRAIN:WBStrain00047397</ext-link></td><td>otIs669[NeuroPAL]</td><td/><td><xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref></td></tr><tr><td>OH15495</td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/WB-STRAIN:WBStrain00047403">WB-STRAIN:WBStrain00047403</ext-link></td><td>otIs696[NeuroPAL]</td><td/><td><xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref>; <xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref></td></tr></tbody></table></table-wrap></sec><sec id="s4-4"><title>Imaging</title><p>To image neurons in the head of freely moving worms, we used a dual-objective spinning-disk based tracking system (<xref ref-type="bibr" rid="bib24">Nguyen et al., 2016</xref>) (Yokogawa CSU-X1 mounted on a Nikon Eclipse TE2000-S). Fluorescent images of the head of a worm were recorded through a 40x objective with both 488- and 561 nm excitation laser light as the animal crawled. The 40x objective translated up and down along the imaging axis to acquire 3D image stacks at a rate of 6 head volumes/s.</p><p>To image neurons in the immobile multi-color NeuroPAL worms (<xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref>), we modified our setup by adding emission filters in a motorized filter wheel (Prior ProScan-II), and adding a Stanford Research Systems SR474 shutter controller (with SR475 shutters) to programmatically illuminate the worm with different wavelength laser light. We use three lasers of different wavelengths: 405 nm (Coherent OBIS-LX 405 nm 100 mW), 488 nm (Coherent SAPPHIRE 488 nm 200 mW), and 561 nm (Coherent SAPPHIRE 561 nm 200 mW). Only one laser at a time reached the sample, through a 40x oil-immersion objective (1.3 NA, Nikon S Fluor). The powers measured at the sample, after spinning disk and objective, were 0.14 mW (405 nm), 0.35 mW (488 nm), and 0.36 mW (561 nm). In the spinning disk unit, a dichroic mirror (Chroma ZT405/488/561tpc) separated the excitation from the emission light. The latter was relayed to a cooled sCMOS camera (Hamamatsu ORCA-Flash 4.0 C11440-22CU), passing through the filters mounted on the filter wheel (<xref ref-type="table" rid="table5">Table 5</xref>). Fluorescent images were acquired in different ‘channels’, that is, different combinations of excitation wavelength, emission filter, and camera exposure time (<xref ref-type="table" rid="table6">Table 6</xref>). The acquisition was performed using a custom software written in LabVIEW that specifies the sequence of channels to be imaged, and controls shutters, filter wheel, piezo translator, and camera. After setting the z position, the software acquires a sequence of images in the specified channels.</p><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>List of emission filters for multicolor imaging.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Filter label</th><th>Filters (Semrock part n.)</th></tr></thead><tbody><tr><td>F1</td><td>FF01-440/40</td></tr><tr><td>F2</td><td>FF01-607/36</td></tr><tr><td>F3</td><td>FF02-675/67 + FF01-692/LP</td></tr></tbody></table></table-wrap><table-wrap id="table6" position="float"><label>Table 6.</label><caption><title>Imaging channels used.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Channel</th><th>Excitation λ (nm)</th><th>Emission window (nm) [filter]</th><th>Primary fluorophore</th></tr></thead><tbody><tr><td>ch0</td><td>405</td><td>420–460 [F1]</td><td>mtagBFP</td></tr><tr><td>ch1</td><td>488</td><td>589–625 [F2]</td><td>CyOFP</td></tr><tr><td>ch2</td><td>561</td><td>589–625 [F2]</td><td>tagRFP-t</td></tr><tr><td>ch3</td><td>561</td><td>692–708 [F3]</td><td>mNeptune</td></tr></tbody></table></table-wrap></sec><sec id="s4-5"><title>Preprocessing and segmentation</title><p>We extracted the position of individual neurons from 3D fluorescent images to generate a 3D point cloud (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). This process is called segmentation and the fDNC model is agnostic to the specific choice of the segmentation algorithm. Segmentation was always performed on tagRFP, never on GCaMP.</p><p>For recordings of strains AML32, we used a segmentation algorithm adopted from <xref ref-type="bibr" rid="bib25">Nguyen et al., 2017</xref>. We first applied a threshold to find pixels where the intensities are significantly larger than the background. Then, we computed the 3D Hessian matrix and its eigenvalues of the intensity image. Candidate neurons were regions where the maximal eigenvalue was negative. Next, we searched for the local intensity peaks in the region and spatially disambiguated peaks in the same region with a watershed separation based on pixel intensity.</p><p>For recordings of NeuroPAL strains, we used the same segmentation algorithm as in <xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref>. The publicly accessible dataset from <xref ref-type="bibr" rid="bib4">Chaudhary et al., 2021</xref> used in <xref ref-type="fig" rid="fig4">Figure 4</xref> had already been segmented prior to our use.</p></sec><sec id="s4-6"><title>Generating semi-synthetic point clouds with correspondence for training</title><p>We developed a simulator to generate a large training set of semi-synthetic animals with known neural correspondence. The simulator takes as its input the point clouds collected from approximately 4000 volumes spread across 12 recordings of freely moving animals. Each recording contains roughly 3000 volumes. For each volume, the simulator performs a series of stochastic deformations and transformations to generate 64 new semi-synthetic individuals where the ground truth correspondence between neurons in the individuals and the original point cloud is known. A total of 2.304 × 10<sup>5</sup> semi-synthetic point clouds were used for training.</p><p>The simulator introduces a variety of different sources of variability and real-world deformations to create each semi-synthetic point cloud (<xref ref-type="fig" rid="fig1">Figure 1B,E</xref>). The simulator starts by straightening the worm in the XY plane using its centerline so that it now lies in a canonical worm coordinate system. Before straightening, Z is along the optical axis and XY are defined to be perpendicular to the optical axis and are arbitrarily set by the orientation of the camera. After straightening, the animal’s posterior-anterior axis lies along the X axis. To introduce animal-to-animal variability in relative neural position, a non-rigid transformation is applied to the neuron point cloud against a template randomly selected from recordings of the real observed worms using coherent point drift (CPD) (<xref ref-type="bibr" rid="bib21">Myronenko and Song, 2010</xref>). To add variability associated with rotation and distortion of the worm’s head in the transverse plane, we apply a random affine transformation to the transverse plane. To simulate missing neurons and segmentation errors, spurious neurons are randomly added, and some true neurons are randomly removed, for up to 20% of the observed neurons. To introduce variability associated with animal pose, we randomly deform the centerline of the head. Lastly, to account for variability in animals’ size and orientation, a random affine transformation in the XY plane is applied that rescaled the animal’s size by up to 5%. With those steps, the simulator deforms a sampled worm and generates a new semi-synthetic worm with different orientation and posture while maintaining known correspondence.</p><p>Centerlines generated by the simulator were directly sampled from recordings of real individuals. The magnitude of added Gaussian noise was arbitrarily set to have a standard deviation of 0.42 µm.</p></sec><sec id="s4-7"><title>Deep neural correspondence model</title><sec id="s4-7-1"><title>Overview and input</title><p>The deep neural correspondence model (fDNC) is an artificial neural network based on the Transformer (<xref ref-type="bibr" rid="bib37">Vaswani et al., 2017</xref>) architecture (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) and is implemented in the automatic differentiation framework PyTorch (<xref ref-type="bibr" rid="bib27">Paszke et al., 2017</xref>). The fDNC model takes as input the positional coordinates of a pair of worms, a template worm <inline-formula><mml:math id="inf41"><mml:mi>a</mml:mi></mml:math></inline-formula>, and test worm, <inline-formula><mml:math id="inf42"><mml:mi>b</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). For each worm, approximately 120 neurons are segmented and passed to the fDNC model. The input neuron sequences are randomly shuffled for both template worm and test worm. This eliminates the possibility that the information from the original sequence order is used.</p></sec><sec id="s4-7-2"><title>Architecture</title><p>The model works as an encoder, which maps the input neuron coordinates <inline-formula><mml:math id="inf43"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> to continuous embeddings <inline-formula><mml:math id="inf44"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The model is composed of a stack of <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula> identical layers. Each layer consists of two sub-layers: a multi-head self-attention mechanism (<xref ref-type="bibr" rid="bib37">Vaswani et al., 2017</xref>) and a fully connected feed-forward network. The multi-head attention mechanism is the defining feature of the transformer architecture and makes the architecture well-suited for finding relations in sequences of data, such as words in a sentence or, in our case, spatial locations of neurons in a worm. Each head contains a one-to-one mapping between the nodes in the artificial network and the <italic>C. elegans</italic> neurons. In the transformer architecture, features of a previous layer are mapped via a linear layer into three attributes of each node, called the query, the key and the value pairs. These attributes of each node contain high dimensional feature vectors which, in our context, represent information about the neuron’s relative position. The multi-head attention mechanism computes a weight for each pair of nodes (corresponding to each pair of <italic>C. elegans</italic> neurons). The weights are calculated by performing a set computation on the query and key. The output is calculated by multiplying this resultant weight by the value. In our implementation, we set the number of heads in the multi-head attention module to be eight and we set the dimension of our feature vectors to be 128. We chose the best set of the hyperparameters (details in Training section) by evaluating on a validation set, which is distinct from the training set and also from any data used for evaluation. A residual connection (<xref ref-type="bibr" rid="bib8">He et al., 2016</xref>) and layer normalization (<xref ref-type="bibr" rid="bib11">Jl et al., 2016</xref>) are employed for each sub-layer, as is widely used in artificial neural networks.</p></sec><sec id="s4-7-3"><title>Calculating probabilities for potential matches</title><p>The fDNC model generates a high dimensional (<inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math></inline-formula>) embedding <italic>u</italic><sub><italic>i</italic></sub> for neuron <inline-formula><mml:math id="inf47"><mml:mi>i</mml:mi></mml:math></inline-formula> from the template worm and <italic>v</italic><sub><italic>j</italic></sub> for the neuron <inline-formula><mml:math id="inf48"><mml:mi>j</mml:mi></mml:math></inline-formula> from the test worm. The similarity of a pair of embeddings, as measured by the inner product <inline-formula><mml:math id="inf49"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>, determines the probability that the pair is a match. Specifically, we define the probability that neuron <inline-formula><mml:math id="inf50"><mml:mi>i</mml:mi></mml:math></inline-formula> in the template worm matches neuron <inline-formula><mml:math id="inf51"><mml:mi>j</mml:mi></mml:math></inline-formula> in the test worm as <inline-formula><mml:math id="inf52"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Equivalently, the vector <inline-formula><mml:math id="inf53"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is modeled as the ‘softmax’ function of the inner products between the embedding of neuron <inline-formula><mml:math id="inf54"><mml:mi>i</mml:mi></mml:math></inline-formula> and the embeddings of all candidate neurons <inline-formula><mml:math id="inf55"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula>. The softmax output is non-negative and sums to one so that <italic>p</italic><sub><italic>i</italic></sub> can be interpreted as a discrete probability distribution over assignments of neuron <inline-formula><mml:math id="inf56"><mml:mi>i</mml:mi></mml:math></inline-formula>.</p><p>We also find the most probable correspondence between the two sets of neurons by solving a maximum weight bipartite matching problem where the weights are given by the inner products between test and template worm embeddings. This is a classic combinatorial optimization problem, and it can be solved in polynomial time using the Hungarian algorithm (<xref ref-type="bibr" rid="bib13">Kuhn, 1955</xref>).</p></sec><sec id="s4-7-4"><title>End-user output</title><p>The fDNC model returns two sets of outputs to the end user. One is the algorithm’s estimate of the most probable matches for each neuron in the test worm; that is, the solution to the maximum weight bipartite matching problem described above. The other is an ordered list of alternative candidate matches for each individual neuron in the test worm and their probabilities ranked from most to least probable.</p></sec><sec id="s4-7-5"><title>Training</title><p>The model was trained on 2.304 × 10<sup>5</sup> semi-synthetic animals derived from recordings of 12 individuals. The model was trained only once and the same trained model was used throughout this work.</p><p>Training is as follows. We performed supervised learning with ground truth matches provided by the semi-synthetically generated data. A cross-entropy loss function was used. If neuron <inline-formula><mml:math id="inf57"><mml:mi>i</mml:mi></mml:math></inline-formula> and neuron <inline-formula><mml:math id="inf58"><mml:mi>j</mml:mi></mml:math></inline-formula> were matched by human, the cross-entropy loss function favors the model to output <inline-formula><mml:math id="inf59"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. If neuron <inline-formula><mml:math id="inf60"><mml:mi>i</mml:mi></mml:math></inline-formula> and neuron <inline-formula><mml:math id="inf61"><mml:mi>j</mml:mi></mml:math></inline-formula> were not matched, the loss function favors the model to output <inline-formula><mml:math id="inf62"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. The model was trained for 12 hr on a 2.40 GHz Intel machine with NVIDIA Tesla P100 GPU.</p><p>We trained different models with different hyperparameters and chose the one with best performance. The training curve for each model we trained is shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. All the models converged after 12 hr of training. We show the performance of trained models on a held-out validation set consisting of 12,800 semi-synthetic worms in <xref ref-type="table" rid="table7">Table 7</xref>. We chose the model with 6 layers and 128 dimensional embedding space since it reaches the highest performance and increasing the complexity of the model did not appear to increase the performance dramatically.</p><table-wrap id="table7" position="float"><label>Table 7.</label><caption><title>Model validation for hyperparameters selection.</title><p>Table lists losses of models with different hyperparameter values. <inline-formula><mml:math id="inf63"><mml:mi>N</mml:mi></mml:math></inline-formula> represents the number of layers for the transformer architecture. <inline-formula><mml:math id="inf64"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the dimension of the embedding space. The loss shown is the average cross entropy loss evaluated on a held out validation set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><inline-graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66410-inf1-v2.tif"/><break/></th><th>32</th><th>64</th><th>128</th></tr></thead><tbody><tr><td>4</td><td>83.1%</td><td>88.4%</td><td>90.7%</td></tr><tr><td>6</td><td>86.3%</td><td>94.6%</td><td>96.8%</td></tr><tr><td>8</td><td>90.5%</td><td>96.4%</td><td>96.8%</td></tr></tbody></table></table-wrap></sec></sec><sec id="s4-8"><title>Evaluating model performance and comparing against other models</title><p>To evaluate performance, putative matches are found between template and test, and compared to ground truth. Every segmented neuron in the test or template (whichever has fewer) is assigned a match. Accuracy is defined as the number of proposed matches that agree with ground truth, divided by the total number of ground truth matches. The number of ground truth matches is a property of the dataset used to evaluate our model, and is listed in <xref ref-type="table" rid="table3">Table 3</xref>.</p><sec id="s4-8-1"><title>Coherent Point Drift</title><p>We use Coherent Point Drift (CPD) Registration (<xref ref-type="bibr" rid="bib21">Myronenko and Song, 2010</xref>) as a baseline with which to compare our model’s performance. In our implementation, CPD is used to find the optimal non-rigid transformation to align the test worm with respect to the template worm. We then calculated the distance for each pair of the neurons from the transformed test worm and the template worm. We used the Hungarian algorithm (<xref ref-type="bibr" rid="bib13">Kuhn, 1955</xref>) to find the optimal correspondence that minimizes the total squared distance for all matches.</p></sec></sec><sec id="s4-9"><title>Color model</title><p>The recently developed NeuroPAL strain (<xref ref-type="bibr" rid="bib42">Yemini et al., 2021</xref>) expresses four different genetically encoded fluorescent proteins in specific expression patterns to better identify neurons across animals. Manual human annotation based on these expression patterns serves as ground truth in evaluating our model’s performance at finding across-animal correspondence. In <xref ref-type="fig" rid="fig5">Figure 5B</xref>, we also explored combining color information with our fDNC model. To do so, we developed a simple color matching model that operated in parallel to our position-based fDNC model. Outputs of both models were then combined to predict the final correspondence between neurons.</p><p>Our color matching model consists of two steps: First, the intensity of each of the color channels is normalized by the total intensity. Then the similarity of color for each pair of neurons is measured as the inverse of the Kullback–Leibler divergence between their normalized color features.</p><p>To calculate the final combined matching matrix, we add the color similarity matrix to the position matching log probability matrix from our fDNC model. The similarity matrix of color is multiplied by a factor λ. We chose <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula> so that the amplitude of values in the similarity matrix of color is comparable to our fDNC output. We note the matching results are not particularly sensitive to the choice of λ. The most probable matches are obtained by applying Hungarian algorithm on the combined matching matrix.</p></sec><sec id="s4-10"><title>Code</title><p>Source code in Python is provided for the model, for the simulator, and for training and evaluation. A jupyter notebook with a simple example is also provided. Code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/XinweiYu/fDNC_Neuron_ID">https://github.com/XinweiYu/fDNC_Neuron_ID</ext-link> (<xref ref-type="bibr" rid="bib44">Yu, 2021</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:1482a11f2196272fced7e3ef9ea8ec867d9527f3;origin=https://github.com/XinweiYu/fDNC_Neuron_ID;visit=swh:1:snp:e5e96d9309b26413616ec249ec37d7abdf7e9f97;anchor=swh:1:rev:19c678781cd11a17866af7b6348ac0096a168c06">swh:1:rev:19c678781cd11a17866af7b6348ac0096a168c06</ext-link>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Eviatar Yemini and Oliver Hobert of Columbia University for strain OH15262. We acknowledge productive discussions with John Murray of University of Pennsylvania. This work used computing resources from the Princeton Institute for Computational Science and Engineering. Research reported in this work was supported by the Simons Foundation under awards SCGB #543003 to AML and SCGB #697092 to SWL; by the National Science Foundation, through an NSF CAREER Award to AML (IOS-1845137) and through the Center for the Physics of Biological Function (PHY-1734030); by the National Institute of Neurological Disorders and Stroke of the National Institutes of Health under award numbers R21NS101629 to AML and R01NS113119 to SWL; and by the Swartz Foundation through the Swartz Fellowship for Theoretical Neuroscience to FR. Some strains are being distributed by the CGC, which is funded by NIH Office of Research Infrastructure Programs (P40 OD010440).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Investigation, Writing - review and editing, Collected data</p></fn><fn fn-type="con" id="con3"><p>Resources, Designed optics and related software libraries</p></fn><fn fn-type="con" id="con4"><p>Resources, Writing - review and editing, Performed all transgenics</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Funding acquisition, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Supervision, Funding acquisition, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-66410-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All datasets generated as part of this work have been deposited in a public Open Science Foundation repository DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/T7DZU">https://doi.org/10.17605/OSF.IO/T7DZU</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>X</given-names></name><name><surname>Cramer</surname><given-names>M</given-names></name><name><surname>Randi</surname><given-names>F</given-names></name><name><surname>Sharma</surname><given-names>A</given-names></name><name><surname>Linderman</surname><given-names>S</given-names></name><name><surname>Leifer</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>fDLC_Neuron_ID_C.elegans</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="doi">10.17605/OSF.IO/T7DZU</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation id="dataset2" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>JP</given-names></name><name><surname>Linder</surname><given-names>AN</given-names></name><name><surname>Plummer</surname><given-names>GS</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Leifer</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Tracking Neurons in a Moving and Deforming Brain Dataset</data-title><source>IEEE DataPorts</source><pub-id assigning-authority="other" pub-id-type="doi">10.21227/H2901H</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bishop</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Pattern Recognition and Machine Learning</source><publisher-loc>New York</publisher-loc><publisher-name>Springer-Verlag</publisher-name></element-citation></ref><ref id="bib2"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bronstein</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Rock, paper, and scissors: extrinsic vs. intrinsic similarity of non-rigid shapes</article-title><conf-name>Proceedings / IEEE International Conference on Computer Vision. IEEE International Conference on Computer Vision</conf-name><pub-id pub-id-type="doi">10.1109/ICCV.1995.466933</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bubnis</surname> <given-names>G</given-names></name><name><surname>Ban</surname> <given-names>S</given-names></name><name><surname>DiFranco</surname> <given-names>MD</given-names></name><name><surname>Kato</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A probabilistic atlas for cell identification</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1903.09227">https://arxiv.org/abs/1903.09227</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaudhary</surname> <given-names>S</given-names></name><name><surname>Lee</surname> <given-names>SA</given-names></name><name><surname>Li</surname> <given-names>Y</given-names></name><name><surname>Patel</surname> <given-names>DS</given-names></name><name><surname>Lu</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Graphical-model framework for automated annotation of cell identities in dense cellular images</article-title><source>eLife</source><volume>10</volume><elocation-id>e60321</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.60321</pub-id><pub-id pub-id-type="pmid">33625357</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clancy</surname> <given-names>KB</given-names></name><name><surname>Koralek</surname> <given-names>AC</given-names></name><name><surname>Costa</surname> <given-names>RM</given-names></name><name><surname>Feldman</surname> <given-names>DE</given-names></name><name><surname>Carmena</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Volitional modulation of optically recorded calcium signals during neuroprosthetic learning</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>807</fpage><lpage>809</lpage><pub-id pub-id-type="doi">10.1038/nn.3712</pub-id><pub-id pub-id-type="pmid">24728268</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hallinen</surname> <given-names>KM</given-names></name><name><surname>Dempsey</surname> <given-names>R</given-names></name><name><surname>Scholz</surname> <given-names>M</given-names></name><name><surname>Yu</surname> <given-names>X</given-names></name><name><surname>Linder</surname> <given-names>A</given-names></name><name><surname>Randi</surname> <given-names>F</given-names></name><name><surname>Sharma</surname> <given-names>AK</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name><name><surname>Leifer</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Decoding locomotion from population neural activity in moving <italic>C. elegans</italic></article-title><source>eLife</source><volume>10</volume><elocation-id>e66135</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.66135</pub-id><pub-id pub-id-type="pmid">34323218</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hammarlund</surname> <given-names>M</given-names></name><name><surname>Hobert</surname> <given-names>O</given-names></name><name><surname>Miller</surname> <given-names>DM</given-names></name><name><surname>Sestan</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The CeNGEN Project: The Complete Gene Expression Map of an Entire Nervous System</article-title><source>Neuron</source><volume>99</volume><fpage>430</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.07.042</pub-id><pub-id pub-id-type="pmid">30092212</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Ren</surname> <given-names>S</given-names></name><name><surname>Sun</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep residual learning for image recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR33180.2016</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochbaum</surname> <given-names>DR</given-names></name><name><surname>Zhao</surname> <given-names>Y</given-names></name><name><surname>Farhi</surname> <given-names>SL</given-names></name><name><surname>Klapoetke</surname> <given-names>N</given-names></name><name><surname>Werley</surname> <given-names>CA</given-names></name><name><surname>Kapoor</surname> <given-names>V</given-names></name><name><surname>Zou</surname> <given-names>P</given-names></name><name><surname>Kralj</surname> <given-names>JM</given-names></name><name><surname>Maclaurin</surname> <given-names>D</given-names></name><name><surname>Smedemark-Margulies</surname> <given-names>N</given-names></name><name><surname>Saulnier</surname> <given-names>JL</given-names></name><name><surname>Boulting</surname> <given-names>GL</given-names></name><name><surname>Straub</surname> <given-names>C</given-names></name><name><surname>Cho</surname> <given-names>YK</given-names></name><name><surname>Melkonian</surname> <given-names>M</given-names></name><name><surname>Wong</surname> <given-names>GK</given-names></name><name><surname>Harrison</surname> <given-names>DJ</given-names></name><name><surname>Murthy</surname> <given-names>VN</given-names></name><name><surname>Sabatini</surname> <given-names>BL</given-names></name><name><surname>Boyden</surname> <given-names>ES</given-names></name><name><surname>Campbell</surname> <given-names>RE</given-names></name><name><surname>Cohen</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>All-optical electrophysiology in mammalian neurons using engineered microbial rhodopsins</article-title><source>Nature Methods</source><volume>11</volume><fpage>825</fpage><lpage>833</lpage><pub-id pub-id-type="doi">10.1038/nmeth.3000</pub-id><pub-id pub-id-type="pmid">24952910</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jian</surname> <given-names>B</given-names></name><name><surname>Vemuri</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Robust Point Set Registration Using Gaussian Mixture Models</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>33</volume><fpage>1633</fpage><lpage>1645</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2010.223</pub-id><pub-id pub-id-type="pmid">21173443</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jl</surname> <given-names>B</given-names></name><name><surname>Kiros</surname> <given-names>JR</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Layer normalization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</ext-link></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kocabas</surname> <given-names>A</given-names></name><name><surname>Shen</surname> <given-names>CH</given-names></name><name><surname>Guo</surname> <given-names>ZV</given-names></name><name><surname>Ramanathan</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Controlling interneuron activity in <italic>Caenorhabditis elegans</italic> to evoke chemotactic behaviour</article-title><source>Nature</source><volume>490</volume><fpage>273</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.1038/nature11431</pub-id><pub-id pub-id-type="pmid">23000898</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhn</surname> <given-names>HW</given-names></name></person-group><year iso-8601-date="1955">1955</year><article-title>The hungarian method for the assignment problem</article-title><source>Naval Research Logistics Quarterly</source><volume>2</volume><fpage>83</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1002/nav.3800020109</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lagache</surname> <given-names>T</given-names></name><name><surname>Lansdell</surname> <given-names>B</given-names></name><name><surname>Tang</surname> <given-names>J</given-names></name><name><surname>Yuste</surname> <given-names>R</given-names></name><name><surname>Fairhall</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Tracking activity in a deformable nervous system with motion correction and point-set registration</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/373035</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lagache</surname> <given-names>T</given-names></name><name><surname>Hanson</surname> <given-names>A</given-names></name><name><surname>Fairhall</surname> <given-names>A</given-names></name><name><surname>Yuste</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Robust single neuron tracking of calcium imaging in behaving Hydra</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.06.22.165696</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>J</given-names></name><name><surname>Mitelut</surname> <given-names>C</given-names></name><name><surname>Shokri</surname> <given-names>H</given-names></name><name><surname>Kinsella</surname> <given-names>I</given-names></name><name><surname>Dethe</surname> <given-names>N</given-names></name><name><surname>Wu</surname> <given-names>S</given-names></name><name><surname>Li</surname> <given-names>K</given-names></name><name><surname>Reyes</surname> <given-names>EB</given-names></name><name><surname>Turcu</surname> <given-names>D</given-names></name><name><surname>Batty</surname> <given-names>E</given-names></name><name><surname>Kim</surname> <given-names>YJ</given-names></name><name><surname>Brackbill</surname> <given-names>N</given-names></name><name><surname>Kling</surname> <given-names>A</given-names></name><name><surname>Goetz</surname> <given-names>G</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Carlson</surname> <given-names>D</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>YASS: yet another spike sorter applied to large-scale multi-electrode array recordings in primate retina</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.03.18.997924</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leifer</surname> <given-names>AM</given-names></name><name><surname>Fang-Yen</surname> <given-names>C</given-names></name><name><surname>Gershow</surname> <given-names>M</given-names></name><name><surname>Alkema</surname> <given-names>MJ</given-names></name><name><surname>Samuel</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Optogenetic manipulation of neural activity in freely moving <italic>Caenorhabditis elegans</italic></article-title><source>Nature Methods</source><volume>8</volume><fpage>147</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1554</pub-id><pub-id pub-id-type="pmid">21240279</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname> <given-names>F</given-names></name><name><surname>Peng</surname> <given-names>H</given-names></name><name><surname>Liu</surname> <given-names>X</given-names></name><name><surname>Kim</surname> <given-names>SK</given-names></name><name><surname>Myers</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A 3D digital atlas of <italic>C. elegans</italic> and its application to single-cell analyses</article-title><source>Nature Methods</source><volume>6</volume><fpage>667</fpage><lpage>672</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1366</pub-id><pub-id pub-id-type="pmid">19684595</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname> <given-names>J</given-names></name><name><surname>Zhao</surname> <given-names>J</given-names></name><name><surname>Yuille</surname> <given-names>AL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Non-Rigid Point Set Registration by Preserving Global and Local Structures</article-title><source>IEEE Transactions on Image Processing : a Publication of the IEEE Signal Processing Society</source><volume>25</volume><fpage>53</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1109/TIP.2015.2467217</pub-id><pub-id pub-id-type="pmid">26276991</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>MW</given-names></name><name><surname>Mathis</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep learning tools for the measurement of animal behavior in neuroscience</article-title><source>Current Opinion in Neurobiology</source><volume>60</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.10.008</pub-id><pub-id pub-id-type="pmid">31791006</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myronenko</surname> <given-names>A</given-names></name><name><surname>Song</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Point set registration: coherent point drift</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>32</volume><fpage>2262</fpage><lpage>2275</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2010.46</pub-id><pub-id pub-id-type="pmid">20975122</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Nejatbakhsh</surname> <given-names>A</given-names></name><name><surname>Varol</surname> <given-names>E</given-names></name><name><surname>Yemini</surname> <given-names>E</given-names></name><name><surname>Venkatachalam</surname> <given-names>V</given-names></name><name><surname>Lin</surname> <given-names>A</given-names></name><name><surname>Samuel</surname> <given-names>ADT</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Extracting neural signals from semi-immobilized animals with deformable non-negative matrix factorization</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.07.07.192120</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nejatbakhsh</surname> <given-names>A</given-names></name><name><surname>Varol</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neuron matching in <italic>C. elegans</italic> With Robust Approximate Linear Regression Without Correspondence</article-title><conf-name>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</conf-name><fpage>2837</fpage><lpage>2846</lpage><pub-id pub-id-type="doi">10.1109/WACV48630.2021.00288</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname> <given-names>JP</given-names></name><name><surname>Shipley</surname> <given-names>FB</given-names></name><name><surname>Linder</surname> <given-names>AN</given-names></name><name><surname>Plummer</surname> <given-names>GS</given-names></name><name><surname>Liu</surname> <given-names>M</given-names></name><name><surname>Setru</surname> <given-names>SU</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name><name><surname>Leifer</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Whole-brain calcium imaging with cellular resolution in freely behaving <italic>Caenorhabditis elegans</italic></article-title><source>PNAS</source><volume>113</volume><fpage>E1074</fpage><lpage>E1081</lpage><pub-id pub-id-type="doi">10.1073/pnas.1507110112</pub-id><pub-id pub-id-type="pmid">26712014</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname> <given-names>JP</given-names></name><name><surname>Linder</surname> <given-names>AN</given-names></name><name><surname>Plummer</surname> <given-names>GS</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name><name><surname>Leifer</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Automatically tracking neurons in a moving and deforming brain</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005517</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005517</pub-id><pub-id pub-id-type="pmid">28545068</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Parthasarathy</surname> <given-names>N</given-names></name><name><surname>Batty</surname> <given-names>E</given-names></name><name><surname>Falcon</surname> <given-names>W</given-names></name><name><surname>Rutten</surname> <given-names>T</given-names></name><name><surname>Rajpal</surname> <given-names>M</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons</chapter-title><person-group person-group-type="editor"><name><surname>Guyon</surname> <given-names>I</given-names></name><name><surname>Luxburg</surname> <given-names>U. V</given-names></name><name><surname>Bengio</surname> <given-names>S</given-names></name><name><surname>Wallach</surname> <given-names>H</given-names></name><name><surname>Fergus</surname> <given-names>R</given-names></name><name><surname>Vishwanathan</surname> <given-names>S</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><volume>30</volume><publisher-name>Curran Associates, Inc</publisher-name><fpage>6434</fpage><lpage>6445</lpage><pub-id pub-id-type="doi">10.1101/153759</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Paszke</surname> <given-names>A</given-names></name><name><surname>Gross</surname> <given-names>S</given-names></name><name><surname>Chintala</surname> <given-names>S</given-names></name><name><surname>Chanan</surname> <given-names>G</given-names></name><name><surname>Yang</surname> <given-names>E</given-names></name><name><surname>Devito</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Automatic differentiation in PyTorch</article-title><conf-name>31st Conference on Neural Information Processing Systems (NIPS 2017)</conf-name></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname> <given-names>H</given-names></name><name><surname>Long</surname> <given-names>F</given-names></name><name><surname>Liu</surname> <given-names>X</given-names></name><name><surname>Kim</surname> <given-names>SK</given-names></name><name><surname>Myers</surname> <given-names>EW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Straightening <italic>Caenorhabditis elegans</italic> images</article-title><source>Bioinformatics</source><volume>24</volume><elocation-id>234</elocation-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btm569</pub-id><pub-id pub-id-type="pmid">18025002</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pereira</surname> <given-names>TD</given-names></name><name><surname>Tabris</surname> <given-names>N</given-names></name><name><surname>Li</surname> <given-names>J</given-names></name><name><surname>Ravindranath</surname> <given-names>S</given-names></name><name><surname>Papadoyannis</surname> <given-names>ES</given-names></name><name><surname>Wang</surname> <given-names>ZY</given-names></name><name><surname>Turner</surname> <given-names>DM</given-names></name><name><surname>McKenzie-Smith</surname> <given-names>G</given-names></name><name><surname>Kocher</surname> <given-names>SD</given-names></name><name><surname>Falkner</surname> <given-names>AL</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>SLEAP: multi-animal pose tracking</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.08.31.276246</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrödel</surname> <given-names>T</given-names></name><name><surname>Prevedel</surname> <given-names>R</given-names></name><name><surname>Aumayr</surname> <given-names>K</given-names></name><name><surname>Zimmer</surname> <given-names>M</given-names></name><name><surname>Vaziri</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brain-wide 3D imaging of neuronal activity in <italic>Caenorhabditis elegans</italic> with sculpted light</article-title><source>Nature Methods</source><volume>10</volume><fpage>1013</fpage><lpage>1020</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2637</pub-id><pub-id pub-id-type="pmid">24013820</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shipley</surname> <given-names>FB</given-names></name><name><surname>Clark</surname> <given-names>CM</given-names></name><name><surname>Alkema</surname> <given-names>MJ</given-names></name><name><surname>Leifer</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Simultaneous optogenetic manipulation and calcium imaging in freely moving <italic>C. elegans</italic></article-title><source>Frontiers in Neural Circuits</source><volume>8</volume><elocation-id>28</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2014.00028</pub-id><pub-id pub-id-type="pmid">24715856</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stirman</surname> <given-names>JN</given-names></name><name><surname>Crane</surname> <given-names>MM</given-names></name><name><surname>Husson</surname> <given-names>SJ</given-names></name><name><surname>Wabnig</surname> <given-names>S</given-names></name><name><surname>Schultheis</surname> <given-names>C</given-names></name><name><surname>Gottschalk</surname> <given-names>A</given-names></name><name><surname>Lu</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Real-time multimodal optical control of neurons and muscles in freely behaving <italic>Caenorhabditis elegans</italic></article-title><source>Nature Methods</source><volume>8</volume><fpage>153</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1555</pub-id><pub-id pub-id-type="pmid">21240278</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sulston</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Post-embryonic development in the ventral cord of <italic>Caenorhabditis elegans</italic></article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>275</volume><fpage>287</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1098/rstb.1976.0084</pub-id><pub-id pub-id-type="pmid">8804</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sun</surname> <given-names>R</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Dy</surname> <given-names>J</given-names></name><name><surname>Krause</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Scalable approximate bayesian inference for particle tracking data</article-title><conf-name>Proceedings of the 35th International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Toyoshima</surname> <given-names>Y</given-names></name><name><surname>Wu</surname> <given-names>S</given-names></name><name><surname>Kanamori</surname> <given-names>M</given-names></name><name><surname>Sato</surname> <given-names>H</given-names></name><name><surname>Jang</surname> <given-names>MS</given-names></name><name><surname>Oe</surname> <given-names>S</given-names></name><name><surname>Murakami</surname> <given-names>Y</given-names></name><name><surname>Teramoto</surname> <given-names>T</given-names></name><name><surname>Park</surname> <given-names>CH</given-names></name><name><surname>Yuishi Iwasaki</surname> <given-names>Y</given-names></name><name><surname>Ishihara</surname> <given-names>T</given-names></name><name><surname>Yoshida</surname> <given-names>R</given-names></name><name><surname>Iino</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An annotation dataset facilitates automatic annotation of whole-brain activity imaging of <italic>C. elegans</italic></article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/698241</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Varol</surname> <given-names>E</given-names></name><name><surname>Nejatbakhsh</surname> <given-names>A</given-names></name><name><surname>Sun</surname> <given-names>R</given-names></name><name><surname>Mena</surname> <given-names>G</given-names></name><name><surname>Yemini</surname> <given-names>E</given-names></name><name><surname>Hobert</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2020">2020</year><chapter-title>Statistical Atlas of <italic>C. elegans</italic> Neurons</chapter-title><source>Medical Image Computing and Computer Assisted Intervention – MICCAI 2020</source><publisher-name>Springer International Publishing</publisher-name><fpage>119</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-59722-1_12</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vaswani</surname> <given-names>A</given-names></name><name><surname>Shazeer</surname> <given-names>N</given-names></name><name><surname>Parmar</surname> <given-names>N</given-names></name><name><surname>Uszkoreit</surname> <given-names>J</given-names></name><name><surname>Jones</surname> <given-names>L</given-names></name><name><surname>Gomez</surname> <given-names>AN</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention is all you need</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</ext-link></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venkatachalam</surname> <given-names>V</given-names></name><name><surname>Ji</surname> <given-names>N</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Clark</surname> <given-names>C</given-names></name><name><surname>Mitchell</surname> <given-names>JK</given-names></name><name><surname>Klein</surname> <given-names>M</given-names></name><name><surname>Tabone</surname> <given-names>CJ</given-names></name><name><surname>Florman</surname> <given-names>J</given-names></name><name><surname>Ji</surname> <given-names>H</given-names></name><name><surname>Greenwood</surname> <given-names>J</given-names></name><name><surname>Chisholm</surname> <given-names>AD</given-names></name><name><surname>Srinivasan</surname> <given-names>J</given-names></name><name><surname>Alkema</surname> <given-names>M</given-names></name><name><surname>Zhen</surname> <given-names>M</given-names></name><name><surname>Samuel</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Pan-neuronal imaging in roaming <italic>Caenorhabditis elegans</italic></article-title><source>PNAS</source><volume>113</volume><fpage>E1082</fpage><lpage>E1088</lpage><pub-id pub-id-type="doi">10.1073/pnas.1507109113</pub-id><pub-id pub-id-type="pmid">26711989</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wen</surname> <given-names>C</given-names></name><name><surname>Miura</surname> <given-names>T</given-names></name><name><surname>Fujie</surname> <given-names>Y</given-names></name><name><surname>Teramoto</surname> <given-names>T</given-names></name><name><surname>Ishihara</surname> <given-names>T</given-names></name><name><surname>Kimura</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep-learning-based flexible pipeline for segmenting and tracking cells in 3D image time series for whole brain imaging</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/385567</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname> <given-names>JG</given-names></name><name><surname>Southgate</surname> <given-names>E</given-names></name><name><surname>Thomson</surname> <given-names>JN</given-names></name><name><surname>Brenner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>The structure of the nervous system of the nematode <italic>Caenorhabditis elegans</italic></article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>314</volume><fpage>1</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1098/rstb.1986.0056</pub-id><pub-id pub-id-type="pmid">22462104</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Witvliet</surname> <given-names>D</given-names></name><name><surname>Mulcahy</surname> <given-names>B</given-names></name><name><surname>Mitchell</surname> <given-names>JK</given-names></name><name><surname>Meirovitch</surname> <given-names>Y</given-names></name><name><surname>Berger</surname> <given-names>DR</given-names></name><name><surname>Wu</surname> <given-names>Y</given-names></name><name><surname>Liu</surname> <given-names>Y</given-names></name><name><surname>Koh</surname> <given-names>WX</given-names></name><name><surname>Parvathala</surname> <given-names>R</given-names></name><name><surname>Holmyard</surname> <given-names>D</given-names></name><name><surname>Schalek</surname> <given-names>RL</given-names></name><name><surname>Shavit</surname> <given-names>N</given-names></name><name><surname>Chisholm</surname> <given-names>AD</given-names></name><name><surname>Lichtman</surname> <given-names>JW</given-names></name><name><surname>Samuel</surname> <given-names>ADT</given-names></name><name><surname>Zhen</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Connectomes across development reveal principles of brain maturation in <italic>C. elegans</italic></article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.04.30.066209</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yemini</surname> <given-names>E</given-names></name><name><surname>Lin</surname> <given-names>A</given-names></name><name><surname>Nejatbakhsh</surname> <given-names>A</given-names></name><name><surname>Varol</surname> <given-names>E</given-names></name><name><surname>Sun</surname> <given-names>R</given-names></name><name><surname>Mena</surname> <given-names>GE</given-names></name><name><surname>Samuel</surname> <given-names>ADT</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Venkatachalam</surname> <given-names>V</given-names></name><name><surname>Hobert</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>NeuroPAL: a multicolor atlas for Whole-Brain neuronal identification in <italic>C. elegans</italic></article-title><source>Cell</source><volume>184</volume><fpage>272</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.12.012</pub-id><pub-id pub-id-type="pmid">33378642</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoon</surname> <given-names>YG</given-names></name><name><surname>Dai</surname> <given-names>P</given-names></name><name><surname>Wohlwend</surname> <given-names>J</given-names></name><name><surname>Chang</surname> <given-names>JB</given-names></name><name><surname>Marblestone</surname> <given-names>AH</given-names></name><name><surname>Boyden</surname> <given-names>ES</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Feasibility of 3D Reconstruction of Neural Morphology Using Expansion Microscopy and Barcode-Guided Agglomeration</article-title><source>Frontiers in Computational Neuroscience</source><volume>11</volume><elocation-id>97</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2017.00097</pub-id><pub-id pub-id-type="pmid">29114215</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Yu</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>fDNC: fast Deep Neural Correspondence</data-title><source>Software Heritage</source><version designator="swh:1:rev:19c678781cd11a17866af7b6348ac0096a168c06">swh:1:rev:19c678781cd11a17866af7b6348ac0096a168c06</version><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:1482a11f2196272fced7e3ef9ea8ec867d9527f3;origin=https://github.com/XinweiYu/fDNC_Neuron_ID;visit=swh:1:snp:e5e96d9309b26413616ec249ec37d7abdf7e9f97;anchor=swh:1:rev:19c678781cd11a17866af7b6348ac0096a168c06">https://archive.softwareheritage.org/swh:1:dir:1482a11f2196272fced7e3ef9ea8ec867d9527f3;origin=https://github.com/XinweiYu/fDNC_Neuron_ID;visit=swh:1:snp:e5e96d9309b26413616ec249ec37d7abdf7e9f97;anchor=swh:1:rev:19c678781cd11a17866af7b6348ac0096a168c06</ext-link></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.66410.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewer</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This manuscript will be of interest to <italic>C. elegans</italic> neuroscientists and also biologists interested in methodological innovations in live imaging. The method described in the paper is clever and elegant, and the solution to the neuron correspondence problem is significant because it is another step toward closed-loop neural perturbation experiments in mobile worms.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Fast deep learning correspondence for neuron tracking and identification in <italic>C. elegans</italic> using synthetic training&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Gordon J Berman as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Ronald Calabrese as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Many details about the training and characterization are missing. There should be more supplemental info accompanying the manuscript on training the network, characterizations of robustness against noise and errors, verifications – including quantifications like scrambling the data sequence, etc. Not having the information creates a big uncertainty on how to evaluate how close the work is to being usable in a real scenario (i.e., one where segmentation is also available). For whole-brain imaging, there are so many perturbations that can throw off tracking algorithms – segmentation error, cells sometimes &quot;show up&quot; and sometimes &quot;disappear&quot; (the newer version of GCaMPs are really low in baseline), etc. Not seeing these explored systematically concerned the reviews as they didn't have a good sense of whether they have attempted to look at these issues. Thus, we ask the authors to add additional figures and quantifications along these lines to buttress the manuscript's claims.</p><p>2) There also was a paucity of details on the network training itself. The methods section mentions, in passing, that some hyper-parameter choices were made on a validation set. Which hyper-parameters were selected in this way, and what ranges of parameters were tried? In this exploration, did the authors observe that network performance was sensitive to some hyper-parameters?</p><p>3) Since speed was a crucial criterion for model design, was there any trade-off between network size and running speed, such that future hardware may possibly achieve higher accuracy without further conceptual advances? Similarly, the results all report the performance of one trained network, which the authors report taking half a day to train. This is both a long time and not so long. Were other networks trained and not described, or did they fail to converge? If so, how often do these networks train successfully if using different initial conditions, and do their performances differ?</p><p>4) Relatedly, the reviewers also thought that the clarity of the algorithm performance is lacking. For instance, there are no training curves shown for the algorithm. Adding details on the network performance/training into supplementary materials would be beneficial.</p><p>5) The majority of the paper uses the authors' own data, which has unique features and structures, leading the reviewers to wonder if the presented results are as generalizable as the authors claim. For example, the authors only accounted for cells that are present in all data sets. What would the numbers look like if they divided by all cells that might show up in any of the animals (a significantly larger number)? The authors do bring up the issue of coverage and accuracy trade-off, but did not really explore this issue at all – the reviewers thought that this point is critical to whole-brain imaging, as the data are rather noisy. So this will have to be addressed using other whole-brain data (maybe published data from another lab, e.g. the Zimmer lab), re-do analysis on the neuron identification part, and more characterization on the coverage-accuracy trade-off. If using the Zimmer data, for example, they should be able to show that they can get the same temporal PCs. If the algorithm is very generalizable and extremely fast and quite accurate as the authors claim, it should be fairly simple to use it on a real whole-brain experiment data set and show that meaningful conclusions can come of it. Without this, one should not make such claims that &quot;The method is fast and predicts correspondence in 10 ms making it suitable for future real-time applications.&quot;</p><p>6) Related to this point, the reviewers thought that the tracking having an ~80% accuracy is not a meaningful goal. First, this accuracy is an average, and it has no bearing on whether a cell can be *continuously* tracked. The traces may be broken, and worse, wrong cells are linked together. This 80% does not guarantee anything at this point. Having an accuracy on a per-frame basis is not the main goal, and there are existing data from the authors themselves and others where this point could be validated. One would have to show that the traces are similar, and better yet, the temporal PCs are similar. The tracking having 80% accuracy cannot be used for optogenetics at all. It is not meaningful to fire the laser at cells with 20% uncertainty in their identities and carry out any meaningful experiments. Thus, either additional validation on the continuity of the tracked accuracy needs to be provided, or the text on optogenetics needs to be significantly toned down or removed.</p><p>7) What is the practical use-case for this cross-individual correspondence, since 65.8% means there are still a lot of errors. Perhaps the authors can discuss (even with some back-of-the-envelope estimates) how much this means for an experiment that compares neural activity between two different worms? What is an experiment that may require doing this fast correspondence estimation between two worms in real-time? Practically speaking, how often would one need to compute correspondences between pairs of frames between two worms? Would the overall correspondence be better if more volumes from each animal were used to find a consensus, or would the authors recommend using NeRVE in that case?</p><p>8) &quot;Recording&quot; was used multiple times in the text and it's not clear whether they are time series or single volumes. For instance, it is not clear what exactly are the &quot;12 individual animals&quot; used for generating the training data. Are they single time frames or are they video? If videos, how many frames? It is not clear the NeuroPAL data sets are videos or single volumes.</p><p>9) Relatedly, if many time points of 12 individual animals are used to generate training data, this is not fully synthetic. The basis of the training data from many worm heads holds a lot of information. The question is also whether all (any) of the augmentation components are necessary or useful. There should be a full characterization of the differential benefit of the different augmentations from not augmenting at all. Calling it synthetic data (e.g., line 566) may be somewhat of a misnomer.</p><p><italic>Reviewer #1:</italic></p><p>In this submission, the authors introduce a new methodology for tracking neural correspondences in calcium imaging of freely moving <italic>C. elegans</italic> using the transformer architecture. The method presents produces state-of-the-art assignment accuracies in a manner that is significantly faster and more robust than existing approaches, potentially allowing for real-time tracking applications once other aspects of the computer vision pipeline become faster. The authors demonstrate the ability of their method on data within and between individuals (using the NeuroPALworm lines), as well as on synthetic control data.</p><p><italic>Reviewer #2:</italic></p><p>Yu et al. developed a deep neural network model with the goal of solving two challenging problems in live imaging of neural activity in mobile <italic>C. elegans</italic>. They call their method fast Deep Learning Correspondence (fDLC), and it simultaneously achieves (1) identification of the same neurons within one animal in a movie, and (2) identification of corresponding neurons between two animals. These problems are difficult because worms change poses as they move, including wiggles within a horizontal plane as well as rolls, and there may be developmental variability between individual worms. Many past approaches have relied on computationally `straightening` the worm into a canonical coordinate system, or on generative models that rely on manually curated features. Instead, fDLC takes a neural network approach; the model builds on the transformer architecture, popularized by its success in natural language process (NLP). To circumvent the prohibitively large quantities of data typically required to train such models, the authors used a relatively modest experimental dataset and synthetically augmented the training data, simulating worm-like movements and imaging conditions to generate arbitrarily large training sets. They then tested the trained model on a variety of data not used on the training, reporting good accuracy. The fDLC approach described here is particularly impressive because of its speed -- it computes correspondances among ~100 neurons in 10 msec per volume on relatively standard GPU hardware.</p><p>Strengths</p><p>The paper is generally clearly written, the methods and results are well presented, and the figures are concise summaries of the results. The introduction gives a thorough and thoughtful review of the related literature and how this work relates to previous methods. I particularly appreciate the authors have made their code publicly available. I believe the paper describes a valuable contribution that will be of significant impact in the study of <italic>C. elegans</italic> neuroscience, as it solves a series of related technical challenges whose solution will open the door for more bold experiments.</p><p>The method described is well suited for the problem, and the performance described is impressive when compared to the closest methods available in the literature. The results are all well demonstrated and justify the conclusions.</p><p>Weaknesses</p><p>The strengths of the fDLC method are to enable real-time neural perturbations and to allow direct comparisons between different worms. However, as the authors point out, the real-time experiments are currently still intractable because cell segmentation remains slow. Further, direct comparisons between different worms remain to be demonstrated as an application of fDLC.</p><p>On the first application, the true impact of fDLC may have to wait for further development of real-time cell segmentation. This seems like an imminently achievable technology. On the second application, it remains to be shown whether the 65.8% accuracy -- while quite impressive -- is sufficient to allow novel analyses and insights to be gained. For instance, if one were analyzing a dataset of 10 separately imaged worms, the overall accuracy of identifying an individual corresponding neuron among these 10 animals may be significantly lower.</p><p>– Perhaps I'm being a bit nit-picky on terminology, but the use of the phrase `transfer learning` in the abstract (also in Figure 1) seems a bit of a stretch. Am I interpreting correctly that the `transfer` is between the train and test sets, without any further refinement? In what way is this `transfer learning` beyond the standard machine learning use of the test/train split?</p><p>– The methods sections mentions in passing, that some hyper-parameter choices were made on a validation set. Which hyper-parameters were selected in this way, and what ranges of parameters were tried? In this exploration, did the authors observe that network performance was sensitive to some hyper-parameters?</p><p>– In the tracking results of the same worm across time, the fDLC approach treats each set of coordinates as independent measurements and does not explicitly use any temporal information. Nevertheless, I would imagine that segmented neuron positions from adjacent frames of the same movie, when the worm has not moved its pose by much, may be easier to track than pairs of frames picked at random. Is this true? What about frames that are 2, 3, etc. samples apart?</p><p>– The acronym `fDLC` may be easily confused with some modification of DeepLabCut. While this work is also a deep learning based tracking software, I think mistaking this method for DeepLabCut may be not desirable.</p><p><italic>Reviewer #3:</italic></p><p>This manuscript describes a deep learning model for tracking neurons in <italic>C. elegans</italic> worms; a side utility of the algorithm is described to be for neuron identification. The problems it is trying to address are significant as there is a need for fast neuron tracking in moving <italic>C. elegans</italic> whole brain imaging; the premise of the work of using synthetic data for training is interesting. The manuscript has several significant deficiencies, including claims not fully supported by evidence and overreaching conclusions.</p><p>Major strengths:</p><p>1. The idea of using augmentation to real data to generate training sets for ML model is interesting, particularly in situations where data are hard to come by.</p><p>2. fDLC's speed is attractive for the use cases.</p><p>Major weaknesses:</p><p>1. For tracking to have ~80% accuracy is not meaningful. First, this accuracy is an average, and it has no bearing on whether a cell can be *continuously* tracked. The traces may be broken, and worse, wrong cells are linked together. This 80% does not guarantee anything at this point. Having an accuracy on per-frame basis is not useful at all. To actually have an impact on tracking, traces have to be shown, and these traces need to be verified. There are existing data from the authors themselves and others. One would have to show that the traces are similar, and better yet, the temporal PCs are similar. The tracking having 80% accuracy cannot be used for optogenetics at all. It is not meaningful to fire the laser at cells with 20% uncertainty in their identities and carry out any meaningful experiments. This claim does not make sense. The text on optogenetics needs to be significantly toned down, or better yet, removed.</p><p>2. What the Transformer network learned with the data is unclear. The paper does not show exactly what the Transformer network has learned – what features of the data are important? This is critical, as it is possible that another form of information is actually being learned from the data. For instance, in training where the hand-curated cells are used, the cells may be entered in a particular order. It is therefore possible that the Transformer network is learning the order of which the cells are entered, rather than the actual spatial relationships. To show that the Transformer network is really learning something meaningful in the data, one would have to scramble the order of the data and show that the results are not different.</p><p>3. The authors stressed that the learning does not require users to prescribe what to look for, but the warping, transformation, noises added are in essence adding information in user-defined way. This claim does not make sense. In the text, the authors also use language such as &quot;roughly matched (their) estimate of variability observed by eye&quot;. This is not rigorous and seems dangerous. Exact details and rationales of choices for the warping, transformation, noises added, etc need to be included and fully justified.</p><p>4. Clarity of algorithm performance is lacking. For instance, there are no training curves shown for the algorithm.</p><p>5. Related, importantly, the accuracy of the algorithm must be very much data-dependent. Sources that can perturb a perfect scenario need to be examined. For instance, how would cells' activities in GCaMP recordings affect accuracy? How would segmentation error affect accuracy? It is not possible to evaluate the real-world utility if these issues are not explored. For all we know, it could be the best data that are fed to the algorithm that is used to calculate the accuracies here.</p><p>6. The authors stated that there is a trade-off between accuracy and coverage. This is an important point, but the authors did not fully characterize such trade-off (related to the accuracy comment above); nor was the coverage assumption/definition that went into each part of the work clearly stated. In the tracking part, what would the coverage be? How is it defined? Comparisons to literature algorithm for neuron identification is should not be done when the coverage is also not well defined, i.e. the denominators for the percentages in table 4 are ill-defined.</p><p>7. Clarity of the experimental data is lacking. &quot;Recording&quot; was used multiple times in the text and it's not clear whether they are time series or single volumes. For instance, it is not clear what exactly are the &quot;12 individual animals&quot; used for generating the training data. Are they single time frames or are they video? If videos, how many frames? It is not clear the NeuroPAL data sets are videos or single volumes.</p><p>8. Related to the issue above, if many time points of 12 individual animals are used to generate training data, this is not at all synthetic. The basis of the training data from many worm heads holds a lot of information. The question is also whether all (any) of the augmentation components are necessary or useful. There should be a full characterization of the differential benefit of the different augmentations from not augmenting at all. Calling it synthetic data in my opinion is a misnomer (e.g. line 566).</p><p>9. Generally speaking, if the algorithm is very generalizable and extremely fast, and quite accurate as the authors claim, it should be fairly simple to use it on a real whole-brain experiment data set and show that meaningful conclusions can come of it. Without this, one should not make such claims that &quot;The method is fast and predicts correspondence in 10 ms making it suitable for future real-time applications.&quot;</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Fast deep neural correspondence for tracking and identifying neurons in <italic>C. elegans</italic> using semi-synthetic training&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Ronald Calabrese (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Essential Revisions:</p><p>1. The data are from the authors themselves, not peer-reviewed, and not independently validated. The authors did not use, for instance, the Zimmer lab's data; the reason why was unclear to the reviewers. Also, the authors themselves have at least one volume of sensible data from their own previous work (NeRVE, Nguyen et al., 2017) in which they actually performed PCA on the GCaMP data. Applying fDNC to that set of data and showing that PCAs are comparable would make their claim much stronger.</p><p>2. The accuracy is a central claim in the paper. It is good that the authors now define what accuracy is in the text, but it is still confusing. A match between the template and the test does not assign a name necessarily -- unless the template neurons already have labels/identities from the ground truth information. From the text, it seems that the template is used as the reference with identities already assigned and that only the neurons common to both the test and template are considered since the denominator of the accuracy is defined as &quot;the total number of ground truth matches&quot;. (Another interpretation of the definition would suggest that neurons that both the template and the test got wrong but matches each other would have been counted as accurate?!)</p><p>There are two issues – the definition is not applicable for some other methods and that this definition is artificially favorable for fDNC.</p><p>a. In Table 2, the authors compare the accuracies of fDNC to that of CPD and CRF (ref 3). This is not appropriate. fDNC and CPD both use template matching, while CRF does not. This is to say that the accuracy definition is not the same for these methods.</p><p>b. The accuracy of fDNC is artificially more favorable. NeuroPAL datasets do not reliably identify the same neurons. When using one NeuroPAL dataset as template, and another as the test set, the matches are on the order of 70-80%. The definition of accuracy the authors use, therefore, is artificially high (by some significant percentage). The errors associated in neurons not common to the test and the template are discounted.</p><p>c. The coverage and the accuracy discussion should be restored.</p><p>3. Implying that fDNC is not &quot;data-privileged&quot; is false (page 13). fDNC is not naive – information from 4000 volumes from 12 animals is there, and fDNC must use a known annotated NeuroPAL dataset as a template, and therefore there is information again (e.g. variability of positions etc). Revising the discussion around this point is important.</p><p><italic>Reviewer #1:</italic></p><p>I thank the authors for their thoughtful revisions and especially for providing additional methodological details and caveats. I think that it would make a good addition to the literature.</p><p><italic>Reviewer #2:</italic></p><p>I thank the authors for their careful and detailed responses to comments and concerns. I especially appreciate the additional methodological details on the training of their model and the clarified definition of how performance is evaluated. I think this work is an interesting and valuable contribution to the literature, and another substantial step in achieving real-time manipulations in this popular experimental organism.</p><p><italic>Reviewer #3:</italic></p><p>The revised manuscript is improved for many of the details, including the data used and how the model was constructed, which are good for reproducibility.</p><p>The responses are unsatisfying in a few major places:</p><p>1. One of the central concerns from the previous round of review is on whether the algorithm performs well enough for tracking. The revision is unsatisfactory.</p><p>a. The authors were asked to apply their tracking to real data to show that the tracking results can generate meaningful data. The authors misunderstood the request as asking for biological insights. The intention is to VALIDATE, not to generate new insights. In fact, that's precisely the reason to apply the algorithm/model to well-curated data that are already peer-reviewed and published.</p><p>b. Figure 3 and the supplemental data were a step in the right direction, but are still unsatisfactory. Figure 3E now shows the tracking accuracy; as expected, the errors are sporadic. Some neurons appear to be ok while others not. This is THE reason PCA was asked in the last round. Figure 3 supplement showing AVAL/R traces are not enough to demonstrate the traces are sensible. It is anecdotal. AVA are among the most &quot;obvious&quot; neurons; peaks correlating to reversal behavior made the cells easy to identify and the tracking errors very easily ignored. The question is whether the rest of the neurons (&gt;99% of them) can give sensible traces.</p><p>c. The data are from the authors themselves, not peer reviewed and not independently validated. The authors dodged the request to use, for instance, the Zimmer lab's data; the reason is unclear to me. Also, if anything, the authors themselves have at least one volume of sensible data from their own previous work (NeRVE, Nguyen et al., 2017) in which they actually did PCA on the GCaMP data. The least they can do is to apply fDNC to that set of data and show that PCAs are comparable.</p><p>d. Tracking is tracking, and should not be confounded with the discussion on neuron identification. The accuracy for tracking purposes should be discussed separately.</p><p>2. The authors cited a biorxiv paper (ref 22) and glossed over its contribution. This paper is now published (https://elifesciences.org/articles/59187). The major contribution of 3DeeCellTracker is also a deep learning algorithm for tracking cells, and the paper also dealt with the sort of data this manuscript addresses. There is no discussion and no comparison.</p><p>a. In fact, Wen et al. directly compared 3DeeCellTracker performance with other algorithms, including even on the dataset from Nguyen et al. (2017). The accuracies reported in Wen et al., are quite favorable (&gt;90%). This data set should be directly compared.</p><p>b. fDNC may be faster, which suggests a trade-off between speed and accuracy. It seems pertinent to include this discussion.</p><p>3. The accuracy is a central claim in the paper. It is good that the authors now define what accuracy is in the text, but it is still confusing. A match between the template and the test does not assign a name necessarily, UNLESS the template neurons already have labels/identities from the ground truth information. From the text, it seems that the template IS used as the reference with identities already assigned, and that only the neurons common to both the test and template are considered since the denominator of the accuracy is defined as &quot;the total number of ground truth matches&quot;. (Another interpretation of the definition would suggest that neurons that both the template and the test got wrong but matches each other would have been counted as accurate?!)</p><p>There are two issues – the definition is not applicable for some other methods and that this definition is artificially favorable for fDNC.</p><p>a. In Table 2, the authors compare the accuracies of fDNC to that of CPD and CRF (ref 3). This is not appropriate. fDNC and CPD both use template matching, while CRF does not. This is to say that the accuracy definition is not the same for these methods.</p><p>b. The accuracy of fDNC is artificially more favorable. NeuroPAL datasets do not reliably identify the same neurons. When using one NeuroPAL dataset as template, and another as the test set, the matches is on the order of 70-80%. The definition of accuracy the authors use, therefore, is artificially high (by some significant percentage). The errors associated in neurons NOT common to the test and the template are discounted.</p><p>c. It seems that the coverage and the accuracy discussion should be restored.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.66410.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Many details about the training and characterization are missing. There should be more supplemental info accompanying the manuscript on training the network, characterizations of robustness against noise and errors, verifications – including quantifications like scrambling the data sequence, etc. Not having the information creates a big uncertainty on how to evaluate how close the work is to being usable in a real scenario (i.e., one where segmentation is also available). For whole-brain imaging, there are so many perturbations that can throw off tracking algorithms – segmentation error, cells sometimes &quot;show up&quot; and sometimes &quot;disappear&quot; (the newer version of GCaMPs are really low in baseline), etc. Not seeing these explored systematically concerned the reviews as they didn't have a good sense of whether they have attempted to look at these issues. Thus, we ask the authors to add additional figures and quantifications along these lines to buttress the manuscript's claims.</p></disp-quote><p>Thank you for these suggestions:</p><p>– To characterize training we have added Figure 2 – Supplementary Figure 1 showing training curves and Table 7 showing final performance for all hyperparameters that we explored.</p><p>– We had added the following new figures and a video to further demonstrate performance on real-world calcium imaging datasets of moving <italic>C. elegans</italic>:</p><p>– Figure 3E shows a volume-by-volume comparison of the model’s assigned neural identities to those that were manually annotated in a freely moving calcium imaging dataset.</p><p>– Figure 3D shows a breakdown of model performance by neuron for that dataset.</p><p>– Figure 3 – Supplementary Figure 1 shows calcium activity extracted from a recently published recording of whole-brain activity of a moving worm from (Hallinen et al., 2021). In that recording an additional fluorophore unambiguously labels AVAL and AVAR. We show that AVAL and AVAR’s calcium activity show expected transients.</p><p>– Video 1 shows labeled neurons over time from the same dataset in Figure 3 – Supplementary Figure 1.</p><p>– Regarding concerns related to GCaMP’s baseline activity: We note that all animals in this study expressed both RFP and GCaMP. Segmentation is performed only on RFP, thus we do not anticipate GCaMP activity to have an impact. We now clarify this in the text: “Segmentation was always performed on tagRFP, never on GCaMP.”</p><p>– Regarding the scrambling of data sequences: all of the input neuron sequences used in this work have been randomly shuffled both for the training set and the test set, thereby preventing the model from learning any information from original sequence order. We now clarify this in the text. “The input neuron sequences have been randomly shuffled for both template worm and test worm. This eliminates the possibility that the information from the original sequence order is used.”</p><p>We also note that, by design, the transformer model can’t extract information from the order of input sequence without additional position embeddings due to its permutation invariance property.</p><p>– Regarding challenging our model with realistic noise: For semi-synthetic worms, up to 20% of the total neurons are randomly added or abandoned. As mentioned above, we have also now added an additional real-world recording and show that calcium activity of a well characterized neuron pair, AVAL and AVAR exhibit expected transients.</p><disp-quote content-type="editor-comment"><p>2) There also was a paucity of details on the network training itself. The methods section mentions, in passing, that some hyper-parameter choices were made on a validation set. Which hyper-parameters were selected in this way, and what ranges of parameters were tried? In this exploration, did the authors observe that network performance was sensitive to some hyper-parameters?</p></disp-quote><p>In Table 7, we now report the hyperparameters that we tried. The hyper-parameters include the dimensionality of hidden space (32, 64, 128) and the number of layers (4, 6, 8) for the transformer architecture. All the models we tried converged ( see new Figure 2 – Supplementary Figure 1). We have now added a paragraph of text: “We trained different models with different hyperparameters and chose the one with best performance. The training curve for each model we trained is shown in Figure 2 Supplementary Figure 1. All the models converged after 12 hours of training. We show the performance of trained models on a held-out validation set consisting of 12,800 semi-synthetic worms in Table 7. We chose the model with 6 layers and 128 dimensional embedding space since it reaches the highest performance and increasing the complexity of the model did not appear to increase the performance dramatically. “</p><disp-quote content-type="editor-comment"><p>3) Since speed was a crucial criterion for model design, was there any trade-off between network size and running speed, such that future hardware may possibly achieve higher accuracy without further conceptual advances? Similarly, the results all report the performance of one trained network, which the authors report taking half a day to train. This is both a long time and not so long. Were other networks trained and not described, or did they fail to converge? If so, how often do these networks train successfully if using different initial conditions, and do their performances differ?</p></disp-quote><p>The model already achieves a very high accuracy over our semi-synthetic data ( 96.5%). This suggests that the current bottleneck for improving performance on real data likely has less to do with speed and more to do with the semi-synthetic data’s ability to capture the full variability of real measurements. We now mention this in the discussion:</p><p>“Therefore fDNC's 79% accuracy within-individuals suggests room for improving within-individual correspondence, and by extension, across-individual correspondence because the latter necessarily includes all of the variability of the former. One avenue for achieving higher performance could be to improve the simulator's ability to better capture variability of a real dataset, for example by using different choices of parameters in the simulator. ”</p><p>All the models with different hyperparameters converged. We have added text to describe convergence and convergence time.</p><p>“We trained different models with different hyperparameters and chose the one with best performance. The training curve for each model we trained is shown in Figure 2 Supplementary Figure 1. All the models converged after 12 hours of training. We show the performance of trained models on a held-out validation set consisting of 12,800 semi-synthetic worms in Table 7. We chose the model with 6 layers and 128 dimensional embedding space since it reaches the highest performance and increasing the complexity of the model did not appear to increase the performance dramatically.”</p><disp-quote content-type="editor-comment"><p>4) Relatedly, the reviewers also thought that the clarity of the algorithm performance is lacking. For instance, there are no training curves shown for the algorithm. Adding details on the network performance/training into supplementary materials would be beneficial.</p></disp-quote><p>Training curves have been added, see Figure 2 – Supplementary Figure 1.</p><disp-quote content-type="editor-comment"><p>5) The majority of the paper uses the authors' own data, which has unique features and structures, leading the reviewers to wonder if the presented results are as generalizable as the authors claim. For example, the authors only accounted for cells that are present in all data sets. What would the numbers look like if they divided by all cells that might show up in any of the animals (a significantly larger number)? The authors do bring up the issue of coverage and accuracy trade-off, but did not really explore this issue at all – the reviewers thought that this point is critical to whole-brain imaging, as the data are rather noisy. So this will have to be addressed using other whole-brain data (maybe published data from another lab, e.g. the Zimmer lab), re-do analysis on the neuron identification part, and more characterization on the coverage-accuracy trade-off. If using the Zimmer data, for example, they should be able to show that they can get the same temporal PCs. If the algorithm is very generalizable and extremely fast and quite accurate as the authors claim, it should be fairly simple to use it on a real whole-brain experiment data set and show that meaningful conclusions can come of it. Without this, one should not make such claims that &quot;The method is fast and predicts correspondence in 10 ms making it suitable for future real-time applications.&quot;</p></disp-quote><p>We demonstrate generalizability by showing that the model performs well on all recordings in a published dataset from another group (Chaudhary et al., 2021) and on a previously published GCaMP dataset (Figure 3). We have now added an additional calcium imaging dataset, Figure 3 – Supplementary Figure 1 and corresponding video, Video 1. Extracted calcium dynamics of neurons AVAL and AVAR from this dataset exhibit expected transients.</p><p>We thank the reviewers for pointing out that our definition of accuracy may have unnecessarily caused confusion. We have made changes that should remove ambiguity.</p><p>– We now use a more straightforward definition of accuracy consistently across the entire manuscript. And we note that we do account for all neurons: “To evaluate performance, putative matches are found between template and test, and compared to ground truth. Every segmented neuron in the test or template (whichever has fewer) is assigned a match. Accuracy is defined as the number of proposed matches that agree with ground truth, divided by the total number of ground truth matches.”</p><p>– In Table 3, we now list the average number of ground truth matches for pairs of test and templates sampled from each dataset:</p><p>“The number of ground truth matches is a property of the dataset used to evaluate our model, and is listed in Table 3.”</p><p>– We regenerated all figures using this definition of accuracy. Numerical values are all now slightly different (e.g. in the worst case 65.8% became 64.1%), but our conclusions remain the same.</p><p>– We removed all discussion of what we previously had termed “coverage” because it no longer applies to this definition of accuracy.</p><p>We support our claim that the method is “fast and predicts correspondence in 10 ms” by providing evidence of the model’s speed (Table 1) and accuracy (Figure 3).</p><p>We disagree with the comment that we “should not make such claims” without additional “meaningful conclusions.” We have followed <italic>eLife</italic>’s author guidelines for Tools and Resources submissions: “Tools and Resources articles do not have to report major new biological insights or mechanisms, but it must be clear that they will enable such advances to take place, for example, through exploratory or proof-of-concept experiments.” Our experiments demonstrate the potential of this method for new discovery, and we are excited to use this method in all of our future scientific investigations.</p><disp-quote content-type="editor-comment"><p>6) Related to this point, the reviewers thought that the tracking having an ~80% accuracy is not a meaningful goal. First, this accuracy is an average, and it has no bearing on whether a cell can be *continuously* tracked. The traces may be broken, and worse, wrong cells are linked together. This 80% does not guarantee anything at this point. Having an accuracy on a per-frame basis is not the main goal, and there are existing data from the authors themselves and others where this point could be validated. One would have to show that the traces are similar, and better yet, the temporal PCs are similar. The tracking having 80% accuracy cannot be used for optogenetics at all. It is not meaningful to fire the laser at cells with 20% uncertainty in their identities and carry out any meaningful experiments. Thus, either additional validation on the continuity of the tracked accuracy needs to be provided, or the text on optogenetics needs to be significantly toned down or removed.</p></disp-quote><p>– Figure 3E now demonstrates the extent to which neurons are tracked continuously.</p><p>– Figure 3D now shows a breakdown of accuracy per-neuron.</p><p>– To further demonstrate that the model works with real-world data, in Figure 3 Supplementary Figure 1 we now apply the method to an additional previously published real-world recording of a moving animal during calcium imaging recording and show that well-characterized neurons AVAL and AVAR exhibit expected calcium transients.</p><p>– We have added text to note that the model tracks neurons without regard to time- or history-dependence: “Because CPD, NeRVE and fDNC are all time-independent algorithms, their performance on a given volume is the same, even if nearby volumes are omitted or shuffled in time.” We argue that, in this context, the average per-frame accuracy is relevant.</p><p>We have added a paragraph describing these additional accuracy results:</p><p>“… To visualize performance over time, we show a volume-by-volume comparison of fDNC's tracking to that of a human (Figure 3E). We also characterize model performance on a per neuron basis (Figure 3D). Finally, we used fDNC to extract whole brain calcium activity from a previously published recording of a moving animal in which two well-characterized neurons AVAL and AVAR were unambiguously labeled with an additional colored fluorophore (Hallinen et al., 2021), (Figure 3 – Supplementary Figure 1A). Calcium activity extracted from neurons AVAL and AVAR exhibited calcium activity transients when the animal underwent prolonged backward locomotion, as expected (Figure 3 – Supplementary Figure 1B).”</p><p>We have now revised language to deemphasize optogenetics and also to give more specific examples of real-time applications:</p><p>“The development of fast algorithms for tracking neurons are an important step for bringing real-time closed loop applications such as optical brain-machine interfaces (Clancy et al., 2014) and optical patch clamping (Hochbaum et al., 2014) to whole-brain imaging in freely moving animals.”</p><p>We also have added more context by comparing to the existing state of real-time methods in <italic>C. elegans</italic>, including for optogenetics:</p><p>“By contrast, existing real-time methods for <italic>C. elegans</italic> in moving animals are restricted to small subsets of neurons, are limited to two-dimensions, and work at low spatial resolution (Leifer et al., 2011; Stirman et al., 2011; Kocabas et al., 2012; Shipley et al., 2014).”</p><p>Obviously, we strive for 100% accuracy, but any automated method entails some amount of uncertainty. Our method is an important step toward improving labeling accuracy, and we believe ~80% is sufficient for many optogenetics experiments.</p><p>Nonetheless, we have broadened the discussion of real-time applications to focus less on optogenetics and to include BMI which relies only on calcium imaging.</p><disp-quote content-type="editor-comment"><p>7) What is the practical use-case for this cross-individual correspondence, since 65.8% means there are still a lot of errors. Perhaps the authors can discuss (even with some back-of-the-envelope estimates) how much this means for an experiment that compares neural activity between two different worms? What is an experiment that may require doing this fast correspondence estimation between two worms in real-time? Practically speaking, how often would one need to compute correspondences between pairs of frames between two worms? Would the overall correspondence be better if more volumes from each animal were used to find a consensus, or would the authors recommend using NeRVE in that case?</p></disp-quote><p>We have now added a paragraph to the discussion to clarify that correspondence is needed in two classes of use cases:</p><p>“Identifying correspondence between constellation of neurons is important for resolving two classes of problems: The first is tracking the identities of neurons across time in a moving animal. The second is mapping neurons from one individual animal onto another, and in particular onto a reference atlas, such as one obtained from electron microscopy (Witvliet et al., 2020). Mapping onto an atlas allows recordings of neurons in the laboratory to be related to known connectomic, gene expression, or other measurements in the literature”.</p><p>We have also now added four paragraphs to the discussion that put performance into a broader context, discuss potential fundamental limits, and provide one specific use case from our own work. We also now remind the reader that our model achieves 78% accuracy on the Chaudhury et al. dataset.</p><p>“The fDNC model finds neural correspondence within and across individuals with an accuracy that compares favorably to other methods. The model focuses primarily on identifying neural correspondence using position information alone. For tracking neurons within an individual using only position, fDNC achieves a high accuracy of 79%, while for across individuals using only position it achieves 64% accuracy on our dataset, and 78% on a published dataset from another group.</p><p>We expect that an upper bound may exist, set by variability introduced during the animal's development, that ultimately limits the accuracy with which any human or algorithm can find correspondence across individuals via only position information. For example, pairs of neurons in one individual that perfectly switch position with respect to another individual will never be unambiguously identified by position alone. It is unclear how close fDNC's performance of 64% on our dataset or 78% on the dataset in [2] comes to this hypothetical upper bound, but there is reason to think that at least some room for improvement remains.</p><p>Specifically, we do not expect accuracy at tracking within an individual to be fundamentally limited, in part because we do not expect two neurons to perfectly switch position on the timescale of a single recording. Therefore fDNC's 79% accuracy within-individuals suggests room for improving within-individual correspondence, and by extension, across-individual correspondence because the latter necessarily includes all of the variability of the former. One avenue for achieving higher performance could be to improve the simulator's ability to better capture variability of a real dataset, for example by using different choices of parameters in the simulator.</p><p>Even at the current level of accuracy, the ability to find correspondence across animals using position information alone remains useful. For example, we are interested in studying neural population coding of locomotion in <italic>C. elegans</italic> [34] , and neural correspondence at 64% accuracy will allow us to reject null hypothesis about the extent to which neural coding of locomotion is stereotyped across individuals.”</p><p>We are unable to evaluate the effect of using more volumes from each individual because we lack across-animal datasets that also have within-animal multi-volume ground truth correspondence, see Table 3.</p><disp-quote content-type="editor-comment"><p>8) &quot;Recording&quot; was used multiple times in the text and it's not clear whether they are time series or single volumes. For instance, it is not clear what exactly are the &quot;12 individual animals&quot; used for generating the training data. Are they single time frames or are they video? If videos, how many frames? It is not clear the NeuroPAL data sets are videos or single volumes.</p></disp-quote><p>We have now added Table 3, which lists information about the number of individuals, volumes, volume rate and other properties for each dataset used.</p><disp-quote content-type="editor-comment"><p>9) Relatedly, if many time points of 12 individual animals are used to generate training data, this is not fully synthetic. The basis of the training data from many worm heads holds a lot of information. The question is also whether all (any) of the augmentation components are necessary or useful. There should be a full characterization of the differential benefit of the different augmentations from not augmenting at all. Calling it synthetic data (e.g., line 566) may be somewhat of a misnomer.</p></disp-quote><p>Thank you for pointing out that the term synthetic could be confusing. To avoid ambiguity, we now use the term “semi-synthetic” throughout.</p><p>Note, however, that the 12 individual animals used by the simulator lack any ground truth correspondence within or between animals (only positions and postures are derived from measurements). We now emphasize:</p><p>“Importantly, using semi-synthetic data also allows us to train our model even when we completely lack experimentally acquired ground truth data. And indeed, in this work, semi-synthetic data is derived exclusively from measurements that lack any ground truth correspondence either within-, or across animals. All ground truth for training comes only from simulation.”</p><p>Implicit in the reviewer’s question, is another: Even if we had large numbers of ground truth datasets of multiple volumes from within a single animal, would that be sufficient to achieve good performance across animals? This is an interesting hypothetical. It is worth noting that the variability across animals is necessarily greater than the variability within animals, so it is possible that it would not be sufficient. We now mention this in the text:</p><p>“…suggests room for improving within-individual correspondence, and by extension, across-individual correspondence because the latter necessarily includes all of the variability of the former.”</p><p>One might further ask, why not collect more ground truth data? Here the transformer required O(10^5) semi-synthetic volumes to reach peak performance. It took the whole lab two weeks of dedicated effort to manually generate the ground-truth dataset with O(10^3) volumes, as described in (Nguyen et al., 2017). Based on these estimates, it would take two years to generate comparable ground truth data to train the transformer.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>[…]</p><p>On the first application, the true impact of fDLC may have to wait for further development of real-time cell segmentation. This seems like an imminently achievable technology. On the second application, it remains to be shown whether the 65.8% accuracy -- while quite impressive -- is sufficient to allow novel analyses and insights to be gained. For instance, if one were analyzing a dataset of 10 separately imaged worms, the overall accuracy of identifying an individual corresponding neuron among these 10 animals may be significantly lower.</p><p>– Perhaps I'm being a bit nit-picky on terminology, but the use of the phrase `transfer learning` in the abstract (also in Figure 1) seems a bit of a stretch. Am I interpreting correctly that the `transfer` is between the train and test sets, without any further refinement? In what way is this `transfer learning` beyond the standard machine learning use of the test/train split?</p></disp-quote><p>We had sought to highlight that our test set evaluates within- and across-animal correspondence, while our semi-synthetic training set is derived from individual volumes that lack any correspondence information at all. We agree that the term transfer learning is at best confusing or at worst incorrect and have therefore removed `transfer learning’ from the text. Thank you for pointing this out.</p><disp-quote content-type="editor-comment"><p>– The methods sections mentions, in passing, that some hyper-parameter choices were made on a validation set. Which hyper-parameters were selected in this way, and what ranges of parameters were tried? In this exploration, did the authors observe that network performance was sensitive to some hyper-parameters?</p></disp-quote><p>As discussed in response to “Essential Revisions #2” we have added Table 7 and accompanying text describing the choice and performance of hyper-parameters.</p><disp-quote content-type="editor-comment"><p>– In the tracking results of the same worm across time, the fDLC approach treats each set of coordinates as independent measurements and does not explicitly use any temporal information. Nevertheless, I would imagine that segmented neuron positions from adjacent frames of the same movie, when the worm has not moved its pose by much, may be easier to track than pairs of frames picked at random. Is this true? What about frames that are 2, 3, etc. samples apart?</p></disp-quote><p>As discussed in response to “Essential Revisions #1,” we now include Figure 3E, which shows a volume by volume comparison of fDNC predictions to that of a human for each neuron over time. The fDNC algorithm does not use temporal correlations and in fact its performance on a volume is the same, even if surrounding volumes are omitted or shuffled in time.</p><p>It is interesting to ask, under what conditions would temporal information be useful? Certainly, as the review suggests, in the regime where neuron motion between frames is small compared to the mean distance between neurons, we would expect temporal information to be valuable. Any benefit of temporal information must be weighed against the potential drawback that time dependent algorithms can accumulate errors over time. In our recordings, neuron motion between frames is of similar length scale to the mean distance between closest neuron neighbors, and this may hint at why this and previous work (Nguyen et al., 2017) have been successful with time-independent strategies. We now mention this in the text:</p><p>“The recording has sufficiently large animal movement that the average distance a neuron travels between volumes (31 um) is of similar scale to the average distance between nearest neuron neighbors (35 um).“</p><disp-quote content-type="editor-comment"><p>– The acronym `fDLC` may be easily confused with some modification of DeepLabCut. While this work is also a deep learning based tracking software, I think mistaking this method for DeepLabCut may be not desirable.</p></disp-quote><p>We thank the reviewer for pointing this out. We have adjusted the acronym. We now use `fDNC’ for fast Deep Neural Correspondence.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>This manuscript describes a deep learning model for tracking neurons in <italic>C. elegans</italic> worms; a side utility of the algorithm is described to be for neuron identification. The problems it is trying to address are significant as there is a need for fast neuron tracking in moving <italic>C. elegans</italic> whole brain imaging; the premise of the work of using synthetic data for training is interesting. The manuscript has several significant deficiencies, including claims not fully supported by evidence and overreaching conclusions.</p><p>Major strengths:</p><p>1. The idea of using augmentation to real data to generate training sets for ML model is interesting, particularly in situations where data are hard to come by.</p><p>2. fDLC's speed is attractive for the use cases.</p><p>Major weaknesses:</p><p>1. For tracking to have ~80% accuracy is not meaningful. First, this accuracy is an average, and it has no bearing on whether a cell can be *continuously* tracked. The traces may be broken, and worse, wrong cells are linked together. This 80% does not guarantee anything at this point. Having an accuracy on per-frame basis is not useful at all. To actually have an impact on tracking, traces have to be shown, and these traces need to be verified. There are existing data from the authors themselves and others. One would have to show that the traces are similar, and better yet, the temporal PCs are similar. The tracking having 80% accuracy cannot be used for optogenetics at all. It is not meaningful to fire the laser at cells with 20% uncertainty in their identities and carry out any meaningful experiments. This claim does not make sense. The text on optogenetics needs to be significantly toned down, or better yet, removed.</p></disp-quote><p>Please see detailed response to “Essential Revisions: #6”.</p><disp-quote content-type="editor-comment"><p>2. What the Transformer network learned with the data is unclear. The paper does not show exactly what the Transformer network has learned – what features of the data are important? This is critical, as it is possible that another form of information is actually being learned from the data. For instance, in training where the hand-curated cells are used, the cells may be entered in a particular order. It is therefore possible that the Transformer network is learning the order of which the cells are entered, rather than the actual spatial relationships. To show that the Transformer network is really learning something meaningful in the data, one would have to scramble the order of the data and show that the results are not different.</p></disp-quote><p>The order of data is indeed scrambled in both training and test sets and we have clarified this in the text. Therefore the model is not learning the order. Please see detailed response to “Essential Revisions: #1”.</p><disp-quote content-type="editor-comment"><p>3. The authors stressed that the learning does not require users to prescribe what to look for, but the warping, transformation, noises added are in essence adding information in user-defined way. This claim does not make sense. In the text, the authors also use language such as &quot;roughly matched (their) estimate of variability observed by eye&quot;. This is not rigorous and seems dangerous. Exact details and rationales of choices for the warping, transformation, noises added, etc need to be included and fully justified.</p></disp-quote><p>We have now removed that text and now specify in the discussion that one avenue for future improvement is to better tune the simulator to capture variability. “One avenue for achieving higher performance could be to improve the simulator's ability to better capture variability of a real dataset, for example by using different choices of parameters in the simulator.”</p><disp-quote content-type="editor-comment"><p>4. Clarity of algorithm performance is lacking. For instance, there are no training curves shown for the algorithm.</p></disp-quote><p>Training curves have been added in Figure 2 – Supplementary Figure 1.</p><disp-quote content-type="editor-comment"><p>5. Related, importantly, the accuracy of the algorithm must be very much data-dependent. Sources that can perturb a perfect scenario need to be examined. For instance, how would cells' activities in GCaMP recordings affect accuracy? How would segmentation error affect accuracy? It is not possible to evaluate the real-world utility if these issues are not explored. For all we know, it could be the best data that are fed to the algorithm that is used to calculate the accuracies here.</p></disp-quote><p>To account for differences in data, and to provide fair comparison against other methods, we evaluate performance on multiple recordings from our own group including those that we have published previously, and on all recordings in a published dataset from a different group (Chaudhary, 2021). Performance on individual recordings in each dataset are visible in Figure 4, and they span a wide range. Also our model performs better on the Chaudhary dataset (78.2%) than our own (64.1%). That we use a wide range of datasets, and that our model performs even better on another group’s dataset is evidence that we are not using only “the best data to calculate accuracies.”</p><p>Regarding GCaMP, we note that accuracy reported in Figure 3 is evaluated on a recording that contains GCaMP activity (originally from Nguyen et al., 2017). Moreover, we also have now added a new example where we apply our method to a recording we recently published (Hallinen et al., 2021) and in this case we also show that GCaMP activity behaves as expected, Figure 3 – Supplementary Figure 1.</p><disp-quote content-type="editor-comment"><p>6. The authors stated that there is a trade-off between accuracy and coverage. This is an important point, but the authors did not fully characterize such trade-off (related to the accuracy comment above); nor was the coverage assumption/definition that went into each part of the work clearly stated. In the tracking part, what would the coverage be? How is it defined? Comparisons to literature algorithm for neuron identification is should not be done when the coverage is also not well defined, i.e. the denominators for the percentages in table 4 are ill-defined.</p></disp-quote><p>Regarding coverage, it is important to note that the algorithm assigns every segmented neuron in the test or template (whichever has fewer) a match. We now reiterate this point more often in the text:</p><p>“Every segmented neuron in the test or template (whichever has fewer) is assigned a match. Accuracy is defined as the number of proposed matches that agree with ground truth, divided by the total number of ground truth matches. The number of ground truth matches is a property of the dataset used to evaluate our model and is listed in Table 3.”</p><p>The numerator and denominator are now well defined for all calculations of the accuracy of our model, NeRVE and CPD, including in Table 4(Table 2 in new version): “number of proposed matches that agree with ground truth, divided by the total number of ground truth matches”.</p><p>We now list information about the denominator explicitly in Table 3 by showing the ground truth matches from test and template pairs sampled from each dataset. We note that this is a property of the dataset and not of the model. Now that we have a more simplified definition of accuracy, a discussion of “coverage” is no longer relevant and has been removed.</p><p>There remains the question of how best to compare our model’s accuracy to that of the CRF model from (Chaudhary et al) because that model reports accuracy using templates that are privileged (see below). We have added two paragraphs describing the specific assumption under which our two models can be directly compared:</p><p>“The CRF model compares test and template like we do, but their template is privileged in the sense that it is derived from either the literature (“open atlas”) or aggregated from their other recordings (``data-driven''). Moreover, the data driven atlas incorporates statistics about variability from across their recordings. By contrast our template is simply one of the other recordings in the dataset. Pairing a test with a privileged template provides slightly more ground truth matches on which to evaluate performance, because the privileged template contains more ground truth labels. We expect the difference is modest, however, because the number of ground truth matches is still limited by the number of neurons with ground truth labels in the test.</p><p>Nonetheless, we must make an assumption to directly compare reported accuracy of the CRF model on the dataset in (Chaudhary et al) to the fDNC model's performance on the same dataset. We must assume that on average there is nothing particularly special about those neurons that have ground truth labels present in the intersection of test and privileged template, but that lack a ground truth label in a non-privileged template sampled from the recordings. Under this assumption, we compared the reported performance of the CRF model on the published dataset in (Chaudhary et al) to the performance of the fDNC model evaluated on the same dataset (Table 2).”</p><disp-quote content-type="editor-comment"><p>7. Clarity of the experimental data is lacking. &quot;Recording&quot; was used multiple times in the text and it's not clear whether they are time series or single volumes. For instance, it is not clear what exactly are the &quot;12 individual animals&quot; used for generating the training data. Are they single time frames or are they video? If videos, how many frames? It is not clear the NeuroPAL data sets are videos or single volumes.</p></disp-quote><p>See response to “Essential Revisions #8”.</p><disp-quote content-type="editor-comment"><p>8. Related to the issue above, if many time points of 12 individual animals are used to generate training data, this is not at all synthetic. The basis of the training data from many worm heads holds a lot of information. The question is also whether all (any) of the augmentation components are necessary or useful. There should be a full characterization of the differential benefit of the different augmentations from not augmenting at all. Calling it synthetic data in my opinion is a misnomer (e.g. line 566).</p></disp-quote><p>See response to “Essential Revisions #9”.</p><disp-quote content-type="editor-comment"><p>9. Generally speaking, if the algorithm is very generalizable and extremely fast, and quite accurate as the authors claim, it should be fairly simple to use it on a real whole-brain experiment data set and show that meaningful conclusions can come of it. Without this, one should not make such claims that &quot;The method is fast and predicts correspondence in 10 ms making it suitable for future real-time applications.&quot;</p></disp-quote><p>See response to “Essential Revisions #5”.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Essential Revisions:</p><p>1. The data are from the authors themselves, not peer-reviewed, and not independently validated. The authors did not use, for instance, the Zimmer lab's data; the reason why was unclear to the reviewers. Also, the authors themselves have at least one volume of sensible data from their own previous work (NeRVE, Nguyen et al., 2017) in which they actually performed PCA on the GCaMP data. Applying fDNC to that set of data and showing that PCAs are comparable would make their claim much stronger.</p></disp-quote><p>– The published population recordings that we know of from the Zimmer group are for immobilized animals. Tracking an immobile recording would not be a good demonstration of the fDNC method. Neuron configurations do not change over time in immobilized animals, so tracking during immobilization is relatively trivial. fDNC would only be used for tracking neurons in moving animals.</p><p>– Figure 3e shows fDNC applied to the requested GCaMP recording from Nguyen et al. 2017.</p><p>We have further clarified the caption to make this clear.</p><p>“Detailed comparison of fdNC tracking to human annotation of a moving GCaMP recording from Nguyen et al. (2017) [18]”.</p><p>– The most stringent comparison we can perform is to compare fDNC tracking to human ground truth tracking, as we have done in Figure 3d and e with the GCaMP recordings from Nguyen 2017. Comparing calcium activity, as suggested, is one step further removed, and is a less informative comparison. If the reviewer’s goal is to assess whether fDNC-tracked neurons can result in plausible calcium traces, Figure 3—figure supplement 1 (p. 27) shows that it does.</p><p>– We disagree that PCA analysis of neural activity is informative or relevant to demonstrate the method and including the analysis risks dragging the paper in a confusing new direction. But to facilitate peer review, we have included the requested analysis below. Left shows neural activity from Ngyuen et al. 2017 using human tracking projected into its first two principal components. Right shows activity from the same recording tracked via fDNC and projected into <italic>its</italic> first two principal components.</p><fig id="respfig1"><label>Author response image 1.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66410-resp-fig1-v2.tif"/></fig><p>Comparing tracking via calcium activity in this way is less stringent and less informative than comparing tracking directly as in Figure 3d and e. We also find these plots difficult to interpret and potentially confusing. Finally, far from being a standard analysis, in published work low dimensional neural state space trajectories have only previously been applied to immobile <italic>C. elegans</italic>, not to moving animals. So including these plots would also broach new scientific ground that is beyond the scope of this methods paper.</p><disp-quote content-type="editor-comment"><p>2. The accuracy is a central claim in the paper. It is good that the authors now define what accuracy is in the text, but it is still confusing. A match between the template and the test does not assign a name necessarily -- unless the template neurons already have labels/identities from the ground truth information. From the text, it seems that the template is used as the reference with identities already assigned and that only the neurons common to both the test and template are considered since the denominator of the accuracy is defined as &quot;the total number of ground truth matches&quot;. (Another interpretation of the definition would suggest that neurons that both the template and the test got wrong but matches each other would have been counted as accurate?!)</p></disp-quote><p>There may be a misunderstanding. Ground truth labels are only used for evaluating performance after the fact, they are not part of the model. See response to Essential feedback #3. All neurons get matched, no label is required. As stated in the text,</p><p>“Every segmented neuron in the test or template (whichever has fewer) is assigned a match.”</p><disp-quote content-type="editor-comment"><p>There are two issues – the definition is not applicable for some other methods and that this definition is artificially favorable for fDNC.</p><p>a. In Table 2, the authors compare the accuracies of fDNC to that of CPD and CRF (ref 3). This is not appropriate. fDNC and CPD both use template matching, while CRF does not. This is to say that the accuracy definition is not the same for these methods.</p></disp-quote><p>We have rewritten the section where we compare fDNC to CPD to highlight the reviewer’s point about template matching and the differences in the models between fDNC and CRF. We now make explicit the assumptions under which we compare fDNC and CRF, despite their differences, and provide quantitative bounds on the range of possible assumptions. And, out of an abundance of caution, we have tempered our conclusions about relative accuracy. We say that fDNC’s accuracy is “comparable” to CRF in addition to having other advantages.</p><p>We hope the reviewers and editors will recognize the value in comparing methods on the same published datasets, and understand that in this case an assumption is necessary to make the comparison.</p><p>“We further sought to compare the fDNC model to the reported accuracy of a recent model called Conditional Random Fields (CRF) from (Chaudhary et. al, 2021) by evaluating fDNC on the same published dataset from that work. […] Taken together, we conclude that the fDNC model's accuracy is comparable to that of the CRF model while also providing other advantages.”</p><disp-quote content-type="editor-comment"><p>b. The accuracy of fDNC is artificially more favorable. NeuroPAL datasets do not reliably identify the same neurons. When using one NeuroPAL dataset as template, and another as the test set, the matches are on the order of 70-80%. The definition of accuracy the authors use, therefore, is artificially high (by some significant percentage). The errors associated in neurons not common to the test and the template are discounted.</p></disp-quote><p>– For neurons that are not part of the set of ground truth labels that intersect test and template, we neither catch errors nor catch correct matches. It is not obvious to us whether this undercounts or overcounts our accuracy compared to the hypothetical in which a human had a complete set of ground truth labels at their disposal.</p><p>– We have carefully considered alternative definitions of accuracy and of all of them, this definition best reflects the information we have. We note that (Chaudhury et al.) faces the same challenge in their framework with respect to neurons that lack ground truth in their test worms and they approach this similarly. They evaluate performance on only those neurons with ground truth labels in the test and ignore segmented neurons that lack ground truth labels for the purposes of reporting accuracy.</p><p>– In Table 3 we provide quantitative details about the number of segmented neurons per individual, the number of ground truth labels per individual, and the number of ground truth matches per pair, so that a reader has all of the information they need to understand the ramifications of our choice of accuracy.</p><disp-quote content-type="editor-comment"><p>c. The coverage and the accuracy discussion should be restored.</p></disp-quote><p>The key points about how we define accuracy and how we think about the denominator are present and clearer than in the initial submission. Table 3 in particular, precisely quantifies how many neurons are segmented and how many have ground truth labels. The previous round of reviewer feedback made clear that the “coverage and accuracy” framing was causing confusion. We hesitate to revive it.</p><disp-quote content-type="editor-comment"><p>3. Implying that fDNC is not &quot;data-privileged&quot; is false (page 13). fDNC is not naive – information from 4000 volumes from 12 animals is there, and fDNC must use a known annotated NeuroPAL dataset as a template, and therefore there is information again (e.g. variability of positions etc). Revising the discussion around this point is important.</p></disp-quote><p>– We rewrote the section (pasted above, in response to Reviewer #3 feedback 2b.) and removed the word “privileged” as it is imprecise and may be causing confusion. Thank you for pointing this out.</p><p>– There may be a misunderstanding. fDNC finds matches between two configurations. It does not require a known annotated NeuroPAL dataset as a template (and does not use NeuroPAL for training). We added new text to clarify:</p><p>“Later in the work we use ground truth information from human annotated NeuroPAL (Yemini et al., 2021) strains to evaluate the performance of our model, but no NeuroPAL strains were used for training.”</p><p>– For example, in Figure 3 fDNC finds correspondence between two worms even though it is blind to any NeuroPAL color information. NeuroPAL is then used only to evaluate the performance of the matches. We added text to clarify:</p><p>“NeuroPAL worms contain extra color information that allows a human to assign ground truth labels to evaluate the model's performance. Crucially, the fDNC model was blinded to this additional color information. In these experiments, NeuroPAL color information was only used to evaluate performance after the fact, not to find correspondence.”</p></body></sub-article></article>