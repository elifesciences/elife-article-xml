<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">36018</article-id>
<article-id pub-id-type="doi">10.7554/eLife.36018</article-id>
<article-categories>
<subj-group subj-group-type="display-channel">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Ongoing, rational calibration of reward-driven perceptual biases</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" id="author-93941">
<name>
<surname>Fan</surname>
<given-names>Yunshu</given-names>
</name>
<contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0003-2597-5173</contrib-id>
<email>yunshuf@pennmedicine.upenn.edu</email>
<xref ref-type="aff" rid="aff1">1</xref>
<xref ref-type="other" rid="fund3"/>
<xref ref-type="fn" rid="con1"/>
<xref ref-type="fn" rid="conf1"/>
</contrib>
<contrib contrib-type="author" id="author-17965">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0002-6018-0483</contrib-id>
<email>jigold@pennmedicine.upenn.edu</email>
<xref ref-type="aff" rid="aff1">1</xref>
<xref ref-type="other" rid="fund1"/>
<xref ref-type="fn" rid="con2"/>
<xref ref-type="fn" rid="conf2"/>
</contrib>
<contrib contrib-type="author" corresp="yes" id="author-28145">
<name>
<surname>Ding</surname>
<given-names>Long</given-names>
</name>
<contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0002-1716-3848</contrib-id>
<email>lding@pennmedicine.upenn.edu</email>
<xref ref-type="aff" rid="aff1">1</xref>
<xref ref-type="other" rid="fund1"/>
<xref ref-type="other" rid="fund2"/>
<xref ref-type="fn" rid="con3"/>
<xref ref-type="fn" rid="conf1"/>
</contrib>
<aff id="aff1">
<label>1</label>
<institution content-type="dept">Neuroscience Graduate Group, Department of Neuroscience</institution>
<institution>University of Pennsylvania</institution>
<addr-line>
<named-content content-type="city">Philadelphia</named-content>
</addr-line>
<country>United States</country>
</aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Latham</surname>
<given-names>Peter</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution>University College London</institution>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Ivry</surname>
<given-names>Richard B</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution>University of California, Berkeley</institution>
<country>United States</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="publication" publication-format="electronic">
<day>10</day>
<month>10</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<year>2018</year>
</pub-date>
<volume>7</volume>
<elocation-id>e36018</elocation-id>
<history>
<date date-type="received" iso-8601-date="2018-02-19">
<day>19</day>
<month>02</month>
<year>2018</year>
</date>
<date date-type="accepted" iso-8601-date="2018-10-07">
<day>07</day>
<month>10</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>© 2018, Fan et al</copyright-statement>
<copyright-year>2018</copyright-year>
<copyright-holder>Fan et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-36018-v3.pdf"/>
<abstract>
<object-id pub-id-type="doi">10.7554/eLife.36018.001</object-id>
<p>Decision-making is often interpreted in terms of normative computations that maximize a particular reward function for stable, average behaviors. Aberrations from the reward-maximizing solutions, either across subjects or across different sessions for the same subject, are often interpreted as reflecting poor learning or physical limitations. Here we show that such aberrations may instead reflect the involvement of additional satisficing and heuristic principles. For an asymmetric-reward perceptual decision-making task, three monkeys produced adaptive biases in response to changes in reward asymmetries and perceptual sensitivity. Their choices and response times were consistent with a normative accumulate-to-bound process. However, their context-dependent adjustments to this process deviated slightly but systematically from the reward-maximizing solutions. These adjustments were instead consistent with a rational process to find satisficing solutions based on the gradient of each monkey’s reward-rate function. These results suggest new dimensions for assessing the rational and idiosyncratic aspects of flexible decision-making.</p>
</abstract>
<kwd-group kwd-group-type="author-keywords">
<kwd>perceptual decision</kwd>
<kwd>saccade</kwd>
<kwd>motion discrimination</kwd>
<kwd>reward bias</kwd>
<kwd>non-human primate</kwd>
<kwd>drift diffusion</kwd>
</kwd-group>
<kwd-group kwd-group-type="research-organism">
<title>Research organism</title>
<kwd>Rhesus macaque</kwd>
</kwd-group>
<funding-group>
<award-group id="fund1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id>
<institution>National Eye Institute</institution>
</institution-wrap>
</funding-source>
<award-id>R01-EY022411</award-id>
<principal-award-recipient>
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<name>
<surname>Ding</surname>
<given-names>Long</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="fund2">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006920</institution-id>
<institution>University of Pennsylvania</institution>
</institution-wrap>
</funding-source>
<award-id>University Research Foundation Pilot Award</award-id>
<principal-award-recipient>
<name>
<surname>Ding</surname>
<given-names>Long</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="fund3">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000933</institution-id>
<institution>Hearst Foundations</institution>
</institution-wrap>
</funding-source>
<award-id>Graduate student fellowship</award-id>
<principal-award-recipient>
<name>
<surname>Fan</surname>
<given-names>Yunshu</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>Author impact statement</meta-name>
<meta-value>The rational application of heuristic learning strategies and satisficing goals accounted for near-optimal decisions that combined reward and noisy visual information by well-trained monkeys.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec sec-type="intro" id="s1">
<title>Introduction</title>
<p>Normative theory has played an important role in our understanding of how the brain forms decisions. For example, many perceptual, memory, and reward-based decisions show inherent trade-offs between speed and accuracy. These trade-offs are parsimoniously captured by a class of sequential-sampling models, such as the drift-diffusion model (DDM), that are based on the accumulation of noisy evidence over time to a pre-defined threshold value, or bound (<xref ref-type="bibr" rid="bib54">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib25">Gold and Shadlen, 2002</xref>; <xref ref-type="bibr" rid="bib6">Bogacz et al., 2006</xref>; <xref ref-type="bibr" rid="bib37">Krajbich et al., 2010</xref>). These models have close ties to statistical decision theory, particularly the sequential probability ratio test that can, under certain assumptions, maximize expected accuracy for a given number of samples or minimize the number of samples needed for a given level of accuracy (<xref ref-type="bibr" rid="bib3">Barnard, 1946</xref>; <xref ref-type="bibr" rid="bib80">Wald, 1947</xref>; <xref ref-type="bibr" rid="bib81">Wald and Wolfowitz, 1948</xref>). However, even when these models provide good descriptions of the average behavior of groups of subjects, they may not capture the substantial variability under different conditions and/or across individual subjects. The goal of this study was to better understand the principles that govern this variability and how these principles relate to normative theory.</p>
<p>We focused on a perceptual decision-making task with asymmetric rewards. For this task, both human and animal subjects tend to make decisions that are biased towards the percept associated with the larger payoff (e.g., <xref ref-type="bibr" rid="bib43">Maddox and Bohil, 1998</xref>; <xref ref-type="bibr" rid="bib79">Voss et al., 2004</xref>; <xref ref-type="bibr" rid="bib10">Diederich and Busemeyer, 2006</xref>; <xref ref-type="bibr" rid="bib40">Liston and Stone, 2008</xref>; <xref ref-type="bibr" rid="bib63">Serences, 2008</xref>; <xref ref-type="bibr" rid="bib17">Feng et al., 2009</xref>; <xref ref-type="bibr" rid="bib66">Simen et al., 2009</xref>; <xref ref-type="bibr" rid="bib49">Nomoto et al., 2010</xref>; <xref ref-type="bibr" rid="bib74">Summerfield and Koechlin, 2010</xref>; <xref ref-type="bibr" rid="bib76">Teichert and Ferrera, 2010</xref>; <xref ref-type="bibr" rid="bib19">Gao et al., 2011</xref>; <xref ref-type="bibr" rid="bib39">Leite and Ratcliff, 2011</xref>; <xref ref-type="bibr" rid="bib46">Mulder et al., 2012</xref>; <xref ref-type="bibr" rid="bib82">Wang et al., 2013</xref>; <xref ref-type="bibr" rid="bib84">White and Poldrack, 2014</xref>). These biases are roughly consistent with a rational strategy to maximize a particular reward function that depends on both the speed and accuracy of the decision process, such as the reward rate per trial or per unit time (<xref ref-type="bibr" rid="bib25">Gold and Shadlen, 2002</xref>; <xref ref-type="bibr" rid="bib6">Bogacz et al., 2006</xref>). This strategy can be accomplished via context-dependent adjustments in a DDM-like decision process along two primary dimensions (<xref ref-type="fig" rid="fig1">Figure 1A</xref>): (1) the momentary sensory evidence, via the drift rate; and (2) the decision rule, via the relative bound heights that govern how much evidence is needed for each alternative (<xref ref-type="bibr" rid="bib55">Ratcliff, 1985</xref>). Subjects tend to make adjustments along one or both of these dimensions to produce overall biases that are consistent with normative theory, but with substantial individual variability (<xref ref-type="bibr" rid="bib79">Voss et al., 2004</xref>; <xref ref-type="bibr" rid="bib8">Cicmil et al., 2015</xref>; <xref ref-type="bibr" rid="bib6">Bogacz et al., 2006</xref>; <xref ref-type="bibr" rid="bib66">Simen et al., 2009</xref>; <xref ref-type="bibr" rid="bib74">Summerfield and Koechlin, 2010</xref>; <xref ref-type="bibr" rid="bib39">Leite and Ratcliff, 2011</xref>; <xref ref-type="bibr" rid="bib46">Mulder et al., 2012</xref>; <xref ref-type="bibr" rid="bib27">Goldfarb et al., 2014</xref>).</p>
<fig id="fig1" position="float">
<object-id pub-id-type="doi">10.7554/eLife.36018.002</object-id>
<label>Figure 1.</label>
<caption>
<title>Theoretical framework and task design.</title>
<p>(<bold>A</bold>) Schematics of the drift-diffusion model (DDM). Motion evidence is modeled as samples from a unit-variance Gaussian distribution (mean: signed coherence, <italic>Coh</italic>). Effective evidence is modeled as the sum of motion evidence and an internal momentary-evidence bias (<italic>me</italic>). The decision variable starts at value a × <italic>z</italic>, where <italic>z</italic> governs decision-rule bias and accumulates effective evidence over time with a proportional scaling factor (<italic>k</italic>). A decision is made when the decision variable reaches either bound. Response time (RT) is assumed to be the sum of the decision time and a saccade-specific non-decision time. (<bold>B</bold>) Response-time (RT) random-dot visual motion direction discrimination task with asymmetric rewards. A monkey makes a saccade decision based on the perceived global motion of a random-dot kinematogram. Reward is delivered on correct trials and with a magnitude that depends on reward context. Two reward contexts (LR-Left and LR-Right) were alternated in blocks of trials with signaled block changes. Motion directions and strengths were randomly interleaved within blocks.</p>
</caption>
<graphic xlink:href="elife-36018-fig1-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<p>To better understand the principles that govern these kinds of idiosyncratic behavioral patterns, we trained three monkeys to perform a response-time (RT), asymmetric-reward decision task with mixed perceptual uncertainty (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Like human subjects, the monkeys showed robust decision biases toward the large-reward option. These biases were sensitive not just to the reward asymmetry, as has been shown previously, but also to experience-dependent changes in perceptual sensitivity. These biases were consistent with adjustments to both the momentary evidence and decision rule in the DDM. However, these two adjustments favored the large- and small-reward choice, respectively, leading to nearly, but not exactly, maximal reward rates. We accounted for these adjustments in terms of a satisficing, gradient-based learning model that calibrated biases to balance the relative influence of perceptual and reward-based information on the decision process. Together, the results imply complementary roles of normative and heuristic principles to understand how the brain combines uncertain sensory input and internal preferences to form decisions that can vary considerably across individuals and task conditions.</p>
</sec>
<sec sec-type="results" id="s2">
<title>Results</title>
<p>We trained three monkeys to perform the asymmetric-reward random-dot motion discrimination (‘dots’) task (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). All three monkeys were initially trained on a symmetric-reward version of the task for which they were required to make fast eye movements (saccades) in the direction congruent with the global motion of a random-dot kinematogram to receive juice reward. They then performed the asymmetric-reward versions that were the focus of this study. Specifically, in blocks of 30 – 50 trials, we alternated direction-reward associations between a ‘LR-Right’ reward context (the large reward was paired with a correct rightward saccade and the small reward was paired with a correct leftward saccade) and the opposite ‘LR-Left’ reward context. We also varied the ratio of large versus small reward magnitudes (‘reward ratio’) across sessions for each monkey. Within a block, we randomly interleaved motion stimuli with different directions and motion strengths (expressed as coherence, the fraction of dots moving in the same direction). We monitored the monkey’s choice (which saccade to make) and RT (when to make the saccade) on each trial.</p>
<fig-group>
<fig id="fig2" position="float">
<object-id pub-id-type="doi">10.7554/eLife.36018.003</object-id>
<label>Figure 2.</label>
<caption>
<title>Relationships between sensitivity and bias from logistic fits to choice data.</title>
<p>(<bold>A</bold>) For each monkey, the probability of making a rightward choice is plotted as a function of signed coherence (–/+indicate left/right motion) from all sessions, separately for the two reward contexts, as indicated. Lines are logistic fits. (<bold>B</bold>) Top row: Motion sensitivity (steepness of the logistic function) in each context as a function of session index (colors as in A). Solid lines indicate significant positive partial Spearman correlation after accounting for changes in reward ratio across sessions (p&lt;0.05). Black dashed lines indicate each monkey’s motion sensitivity for the task with equal rewards before training on this asymmetric reward task. Middle row: ΔBias (horizontal shift between the two psychometric functions for the two reward contexts at chance level) as a function of session index. Solid line indicates significant negative partial Spearman correlation after accounting for changes in reward ratio across sessions (p&lt;0.05). Bottom row: Lapse rate as a function of session index (median = 0 for all three monkeys). (<bold>C</bold>) ΔBias as a function of motion sensitivity for each reward context (colors as in A). Solid line indicates a significant negative partial Spearman correlation after accounting for changes in reward ratio across sessions (p&lt;0.05). (<bold>D</bold>) Optimal versus fitted Δbias. Optimal Δbias was computed as the difference in the horizontal shift in the psychometric functions in each reward context that would have resulted in the maximum reward per trial, given each monkey’s fitted motion sensitivity and experienced values of reward ratio and coherences from each session (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Solid lines indicate significant positive Spearman correlations (p&lt;0.01). Partial Spearman correlation after accounting for changes in reward ratio across sessions are also significant for moneys F and C (p&lt;0.05).</p>
<p>
<supplementary-material id="fig2sdata1">
<object-id pub-id-type="doi">10.7554/eLife.36018.006</object-id>
<label>Figure 2—source data 1.</label>
<caption>
<title>Task parameters and the monkeys’ performance for each trial and each session.</title>
<p>The same data are also used in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>
</p>
</caption>
<media xlink:href="elife-36018-fig2-data1-v3.zip" mimetype="application" mime-subtype="zip"/>
</supplementary-material>
</p>
<p>
<supplementary-material id="fig2sdata2">
<object-id pub-id-type="doi">10.7554/eLife.36018.007</object-id>
<label>Figure 2—source data 2.</label>
<caption>
<title>Source data for <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</title>
<p>Log-likelihood of the logistic regressions with and without sequential choice bias terms.</p>
</caption>
<media xlink:href="elife-36018-fig2-data2-v3.zip" mimetype="application" mime-subtype="zip"/>
</supplementary-material>
</p>
</caption>
<graphic xlink:href="elife-36018-fig2-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<fig id="fig2s1" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.005</object-id>
<label>Figure 2—figure supplement 1.</label>
<caption>
<title>Relationship between bias and sensitivity. </title>
<p>(<bold>A</bold>), Identification of the optimal Δbias for an example session using logistic fits.</p>
<p>For each reward context (blue for LR-Left and red for LR-Right), RTrial was computed as a function of bias values sampled uniformly over a broad range, given the session-specific sensitivities, lapse rate, coherences, and large:small reward ratio. The optimal Δbias was defined as the difference between the bias values with the maximal RTrial for the two reward contexts. The fitted Δbias was defined as the difference between the fitted bias values for the two reward contexts. (<bold>B</bold>) The optimal bias decreases with increasing sensitivity. The example heatmap shows normalized RTrial as a function of sensitivity and bias values in the LR-Right blocks, assuming the same coherence levels as used for the monkeys and a large:small reward ratio of 2.3. The black curve indicates the optimal bias values for a given sensitivity value. (<bold>C</bold>) Scatterplots of optimal Δbiases obtained via the procedure described above as a function of sensitivity for each of the two reward contexts. Same format as <xref ref-type="fig" rid="fig2">Figure 2C</xref> Solid lines indicate significant partial Spearman correlation after accounting for changes in reward ratio across sessions (p&lt;0.05). Note that the scatterplots of the monkeys’ Δbiases and sensitivities in <xref ref-type="fig" rid="fig2">Figure 2C</xref> also show negative correlations, similar to this pattern.</p>
</caption>
<graphic xlink:href="elife-36018-fig2-figsupp1-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<fig id="fig2s2" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.004</object-id>
<label>Figure 2—figure supplement 2.</label>
<caption>
<title>Monkeys showed minimal sequential choice biases.</title>
<p>Histogram of the fraction of sessions with 0, 1 or 2 types of sequential choice biases. Colors indicate the sequential bias types with respect to the previous reward (Large or Small) and outcome (Correct or Error), as indicated. Significant sequential bias effects were identified by a likelihood-ratio test for <italic>H<sub>0</sub>
</italic>: the sequential term in the logistic regression = 0, p&lt;0.05.</p>
</caption>
<graphic xlink:href="elife-36018-fig2-figsupp2-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
</fig-group>
<sec id="s2-1">
<title>The monkeys’ biases reflected changes in reward context and perceptual sensitivity</title>
<p>For the asymmetric-reward task, all three monkeys tended to make more choices towards the large-reward option, particularly when the sensory evidence was weak. These choice biases corresponded to horizontal shifts in the psychometric function describing the probability of making a rightward choice as a function of signed motion coherence (negative for leftward motion, positive for rightward motion; <xref ref-type="fig" rid="fig2">Figure 2A</xref>). These functions showed somewhat similar patterns of behavior but some differences in detail for the three monkeys. For example, each monkey showed steady increases in perceptual sensitivity (steepness of the psychometric function), which initially dropped relative to values from the symmetric-reward task then tended to increase with more experience with asymmetric rewards (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, top; <italic>H<sub>0</sub>
</italic>: partial Spearman’s <italic>ρ</italic> of sensitivity versus session index after accounting for session-specific reward ratios = 0, p&lt;0.01 in all cases, except LR-Left for monkey C, for which p = 0.56). Moreover, lapse rates were near zero across sessions (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, bottom), implying that the monkeys knew how to perform the task. The monkeys differed in terms of overall bias, which was the smallest in monkey F. Nevertheless, for all three monkeys bias magnitude tended to decrease over sessions, although this tendency was statistically significant only for monkey C after accounting for co-variations with reward rate (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, middle). There was often a negative correlation between choice bias and sensitivity, consistent with a general strategy of adjusting bias to obtain more reward (<xref ref-type="fig" rid="fig2">Figure 2C</xref>; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>). Monkeys F and C used suboptimal biases that were larger than the optimal values, whereas monkey A showed greater variations (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). The monkeys showed only negligible or inconsistent sequential choice biases (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), and adding sequential terms did not substantially affect the best-fitting values of the non-sequential terms in the logistic regression (spearman’s <italic>ρ</italic> &gt;0.8 comparing session-by-session best-fitting values of the terms in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> with and without additional sequential terms from <xref ref-type="disp-formula" rid="equ3">Equation 2)</xref>. Therefore, all subsequent analyses did not include sequential choice effects.</p>
<p>To better understand the computational principles that governed these idiosyncratic biases, while also taking into account systematic relationships between the choice and RT data, we fit single-trial RT data (i.e., we modeled full RT distributions, not just mean RTs) from individual sessions to a DDM. We used a hierarchical-DDM (HDDM) method that assumes that parameters from individual sessions of the same monkey are samples from a group distribution (<xref ref-type="bibr" rid="bib85">Wiecki et al., 2013</xref>). The HDDM was fit to data from each monkey separately. The HDDM had six parameters for each reward context. Four were from a basic DDM (<xref ref-type="fig" rid="fig1">Figure 1A</xref>): <italic>a,</italic> the total bound height, representing the distance between the two choice bounds; <italic>k</italic>, a scaling factor that converts sensory evidence (motion strength and direction) to the drift rate; and <italic>t<sub>0</sub>
</italic> and <italic>t<sub>1</sub>
</italic>, non-decision times for leftward and rightward choices, respectively. The additional two parameters provided biases that differed in terms of their effects on the full RT distributions (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>): <italic>me</italic>, which is additional momentary evidence that is added to the motion evidence at each accumulating step and has asymmetric effects on the two choices and on correct versus error trials (positive values favor the rightward choice); and <italic>z</italic>, which determines the decision rules for the two choices and tends to have asymmetric effects on the two choices but not on correct versus error trials (values &gt; 0.5 favor the rightward choice). The HDDM fitting results are shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, and summaries of best-fitting parameters and goodness-of-fit metrics are provided in <xref ref-type="table" rid="table1">Table 1</xref>. A DDM variant with collapsing bounds provided qualitatively similar results as the HDDM (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Thus, subsequent analyses use the model with fixed bounds, unless otherwise noted.</p>
<fig-group>
<fig id="fig3" position="float">
<object-id pub-id-type="doi">10.7554/eLife.36018.008</object-id>
<label>Figure 3.</label>
<caption>
<title>Comparison of choice and RT data to HDDM fits with both momentary-evidence (<italic>me</italic>) and decision-rule (<italic>z</italic>) biases.</title>
<p>(<bold>A</bold>) Psychometric data (points as in <xref ref-type="fig" rid="fig2">Figure 2A</xref>) shown with predictions based on HDDM fits to both choice and RT data. B, RT data (circles) and HDDM-predicted RT distributions (lines). Both sets of RT data were plotted as the session-averaged values corresponding to the 20<sup>th</sup>, 40 <sup>th</sup>, 60<sup>th</sup>, and 80<sup>th</sup> percentiles of the full distribution for the five most frequently used coherence levels (we only show data when &gt; 40% of the total sessions contain &gt;4 trials for that combination of motion direction, coherence, and reward context). Top row: Trials in which monkey chose the left target. Bottom row: Trials in which monkeys chose the right target. Columns correspond to each monkey (as in A), divided into choices in the large- (left column) or small- (right column) reward direction (correct/error choices are as indicated in the left-most columns; note that no reward was given on error trials). The HDDM-predicted RT distributions were generated with 50 runs of simulations, each run using the number of trials per condition (motion direction × coherence × reward context × session) matched to experimental data and using the best-fitting HDDM parameters for that monkey.</p>
<p>
<supplementary-material id="fig3sdata1">
<object-id pub-id-type="doi">10.7554/eLife.36018.011</object-id>
<label>Figure 3—source data 1.</label>
<caption>
<title>Source data for <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>.</title>
<p>Collapsing-bound model fitting parameters and goodness of fits for each session.</p>
</caption>
<media xlink:href="elife-36018-fig3-data1-v3.zip" mimetype="application" mime-subtype="zip"/>
</supplementary-material>
</p>
</caption>
<graphic xlink:href="elife-36018-fig3-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<fig id="fig3s1" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.009</object-id>
<label>Figure 3—figure supplement 1.</label>
<caption>
<title>Qualitative comparison between the monkeys’ RT distribution and DDM predictions.</title>
<p>(<bold>A</bold>) RT distributions as predicted by a DDM with no bias in decision rule (<italic>z</italic>) or momentary evidence (<italic>me</italic>; left), with <italic>me</italic> &gt; 0 (middle), and with <italic>z</italic> &gt; 0.5 (right). RT distributions are shown separately for correct (red) and error (black) trials and using values corresponding to 20th, 40th, 60th, and 80th percentiles. The predictions assumed zero non-decision time to demonstrate effects on RT by only <italic>me</italic> or <italic>z</italic> biases. Positive/negative coh values indicate rightward/leftward saccades. The values of <italic>me</italic> and <italic>z</italic> were chosen to induce similar choice biases (~0.075 in coherence units). Note that the <italic>me</italic> bias induces large asymmetries in RT both between the two choices and between correct and error trials, whereas the <italic>z</italic> bias induces a large asymmetry in RT for the two choices, but with little asymmetry between correct and error trials. (<bold>B</bold>) The monkeys’ mean RTs for four quantiles for the LR-Right (<italic>top</italic>) and LR-Left (<italic>bottom</italic>) reward contexts, respectively (same convention as in A). Note the presence of substantial asymmetries between correct and error trials for all three monkeys.</p>
</caption>
<graphic xlink:href="elife-36018-fig3-figsupp1-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<fig id="fig3s2" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.010</object-id>
<label>Figure 3—figure supplement 2.</label>
<caption>
<title>Fits to a DDM with collapsing bounds.</title>
<p>(<bold>A, B</bold>) A DDM with collapsing bounds and both momentary evidence (<italic>me</italic>) and decision rule (<italic>z</italic>) biases fit to each monkey’s RT data. Same format as <xref ref-type="fig" rid="fig3">Figure 3</xref>. (<bold>C</bold>) The model that included both <italic>me</italic> and <italic>z</italic> adjustments (‘full’) had smaller Akaike Information Criterion (AIC) values than reduced models (‘<italic>me</italic>’ or ‘<italic>z</italic>’ only) across sessions. Note also the different ranges of ΔAIC for the full–<italic>me</italic> and full–<italic>z</italic> comparisons. The mean ΔAIC (full-<italic>me</italic>) and ΔAIC (full-<italic>z</italic>) values are significantly different from zero (Wilcoxon signed rank test, p=0.0007 for Monkey F’s full–<italic>me</italic> comparison and p&lt;0.0001 for all others). (<bold>D</bold>), RT distributions as predicted by the DDM with collapsing bounds, using no bias in <italic>z</italic> or <italic>me</italic> (<italic>left</italic>), <italic>me</italic> &gt; 0 (<italic>middle</italic>), or <italic>z</italic> &gt; 0.5 (<italic>right</italic>). Same format as <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>.</p>
</caption>
<graphic xlink:href="elife-36018-fig3-figsupp2-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
</fig-group>
<table-wrap id="table1" position="float">
<object-id pub-id-type="doi">10.7554/eLife.36018.012</object-id>
<label>Table 1.</label>
<caption>
<title>Best-fitting parameters of HDDM.</title>
<p>
<supplementary-material id="table1sdata1">
<object-id pub-id-type="doi">10.7554/eLife.36018.013</object-id>
<label>Table 1—source data 1.</label>
<caption>
<title>HDDM model fitting parameters for each session.</title>
<p>The same data are also used in <xref ref-type="fig" rid="fig3">Figures 3</xref>, <xref ref-type="fig" rid="fig4">4</xref>, <xref ref-type="fig" rid="fig5">5</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, <xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplements 1</xref>, <xref ref-type="fig" rid="fig6s2">2</xref>, <xref ref-type="fig" rid="fig6s3">3</xref>, <xref ref-type="fig" rid="fig8">Figure 8</xref> and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>)</p>
</caption>
<media xlink:href="elife-36018-table1-data1-v3.zip" mimetype="application" mime-subtype="zip"/>
</supplementary-material>
</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th rowspan="3" valign="bottom"/>
<th colspan="4" valign="bottom">Monkey F (26079 trials)</th>
<th colspan="4" valign="bottom">Monkey C (37161 trials)</th>
<th colspan="4" valign="bottom">Monkey A (21089 trials)</th>
</tr>
<tr>
<th colspan="2" valign="bottom">LR-Left</th>
<th colspan="2" valign="bottom">LR-Right</th>
<th colspan="2" valign="bottom">LR-Left</th>
<th colspan="2" valign="bottom">LR-Right</th>
<th colspan="2" valign="bottom">LR-Left</th>
<th colspan="2" valign="bottom">LR-Right</th>
</tr>
<tr>
<th>Mean</th>
<th>Std</th>
<th>Mean</th>
<th>Std</th>
<th>Mean</th>
<th>Std</th>
<th>Mean</th>
<th>Std</th>
<th>Mean</th>
<th>Std</th>
<th>Mean</th>
<th>Std</th>
</tr>
</thead>
<tbody>
<tr>
<td valign="bottom">
<italic>a</italic>
</td>
<td>1.67</td>
<td>0.16</td>
<td>1.43</td>
<td>0.12</td>
<td>1.77</td>
<td>0.09</td>
<td>1.53</td>
<td>0.13</td>
<td>1.33</td>
<td>0.13</td>
<td>1.36</td>
<td>0.09</td>
</tr>
<tr>
<td valign="bottom">
<italic>k</italic>
</td>
<td>10.22</td>
<td>1.87</td>
<td>9.91</td>
<td>2.11</td>
<td>6.58</td>
<td>0.51</td>
<td>5.08</td>
<td>0.92</td>
<td>4.04</td>
<td>0.33</td>
<td>3.45</td>
<td>0.46</td>
</tr>
<tr>
<td valign="bottom">
<italic>t<sub>1</sub>
</italic>
</td>
<td>0.31</td>
<td>0.03</td>
<td>0.29</td>
<td>0.03</td>
<td>0.35</td>
<td>0.04</td>
<td>0.33</td>
<td>0.05</td>
<td>0.29</td>
<td>0.04</td>
<td>0.27</td>
<td>0.04</td>
</tr>
<tr>
<td valign="bottom">
<italic>t<sub>0</sub>
</italic>
</td>
<td>0.28</td>
<td>0.04</td>
<td>0.31</td>
<td>0.05</td>
<td>0.33</td>
<td>0.04</td>
<td>0.31</td>
<td>0.03</td>
<td>0.21</td>
<td>0.08</td>
<td>0.26</td>
<td>0.04</td>
</tr>
<tr>
<td valign="bottom">
<italic>z</italic>
</td>
<td>0.60</td>
<td>0.03</td>
<td>0.57</td>
<td>0.04</td>
<td>0.62</td>
<td>0.03</td>
<td>0.40</td>
<td>0.04</td>
<td>0.57</td>
<td>0.06</td>
<td>0.39</td>
<td>0.04</td>
</tr>
<tr>
<td valign="bottom">
<italic>me</italic>
</td>
<td>−0.06</td>
<td>0.04</td>
<td>0.08</td>
<td>0.05</td>
<td>−0.14</td>
<td>0.04</td>
<td>0.21</td>
<td>0.06</td>
<td>−0.22</td>
<td>0.05</td>
<td>0.27</td>
<td>0.09</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The DDM fits provided a parsimonious account of both the choice and RT data. Consistent with the results from the logistic analyses, the HDDM analyses showed that the monkeys made systematic improvements in psychometric sensitivity (<italic>H<sub>0</sub>
</italic>: partial Spearman’s <italic>ρ</italic> of sensitivity versus session index after accounting for session-specific reward ratios = 0, p&lt;0.01 in all cases except p=0.06 for LR-Left for monkey A). Moreover, there was a negative correlation between psychometric sensitivity and choice bias (<italic>H<sub>0</sub>
</italic>: partial Spearman’s <italic>ρ</italic> of sensitivity versus total bias after accounting for session-specific reward ratios = 0, p&lt;0.001 in all cases). These fits ascribed the choice biases to changes in both the momentary evidence (<italic>me</italic>) and the decision rule (<italic>z</italic>) of the decision process, as opposed to either parameter alone (<xref ref-type="table" rid="table2">Table 2</xref>). These fits also indicated context-dependent differences in non-decision times, which were smaller for all large-reward choices for all three monkeys except in the LR-Right context for monkeys C and A (<italic>t-</italic>test, p&lt;0.05). However, the differences in non-decision times were relatively small across reward contexts, suggesting that the observed reward biases were driven primarily by effects on decision-related processes.</p>
<table-wrap id="table2" position="float">
<object-id pub-id-type="doi">10.7554/eLife.36018.014</object-id>
<label>Table 2.</label>
<caption>
<title>The difference in deviance information criterion (ΔDIC) between the full model (i.e., the model that includes both <italic>me</italic> and <italic>z</italic>) and either reduced model (<italic>me-</italic>only or <italic>z-</italic>only), for experimental data and data simulated using each reduced model.</title>
<p>Negative/positive values favor the full/reduced model. Note that the ΔDIC values for the experimental data were all strongly negative, favoring the full model. In contrast, the ΔDIC values for the simulated data were all positive, implying that this procedure did not simply prefer the more complex model.</p>
<p>
<supplementary-material id="table2sdata1">
<object-id pub-id-type="doi">10.7554/eLife.36018.015</object-id>
<label>Table 2—source data 1.</label>
<caption>
<title>DIC for model fitting to the monkeys’ data and to the simulated data.</title>
</caption>
<media xlink:href="elife-36018-table2-data1-v3.zip" mimetype="application" mime-subtype="zip"/>
</supplementary-material>
</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th rowspan="3" valign="bottom"/>
<th colspan="4" valign="bottom">Experimental data</th>
<th colspan="2" valign="bottom">Simu: <italic>me</italic> model</th>
<th colspan="2" valign="bottom">Simu: <italic>z</italic> model</th>
</tr>
<tr>
<th colspan="2" valign="bottom">∆DIC: full - me</th>
<th colspan="2" valign="bottom">∆DIC: full - z</th>
<th colspan="2" valign="bottom">∆DIC: full - me</th>
<th colspan="2" valign="bottom">∆DIC: full - z</th>
</tr>
<tr>
<th valign="bottom">Mean</th>
<th valign="bottom">Std</th>
<th valign="bottom">Mean</th>
<th valign="bottom">Std</th>
<th valign="bottom">Mean</th>
<th valign="bottom">Std</th>
<th valign="bottom">Mean</th>
<th valign="bottom">Std</th>
</tr>
</thead>
<tbody>
<tr>
<td valign="bottom">Monkey F</td>
<td valign="bottom">−124.6</td>
<td valign="bottom">2.3</td>
<td valign="bottom">−2560.4</td>
<td valign="bottom">5.2</td>
<td valign="bottom">3.1</td>
<td valign="bottom">9.8</td>
<td valign="bottom">0.2</td>
<td valign="bottom">11.8</td>
</tr>
<tr>
<td valign="bottom">Monkey C</td>
<td valign="bottom">−1700.4</td>
<td valign="bottom">2.1</td>
<td valign="bottom">−6937.9</td>
<td valign="bottom">1.3</td>
<td valign="bottom">17.5</td>
<td valign="bottom">11.3</td>
<td valign="bottom">1.8</td>
<td valign="bottom">1.3</td>
</tr>
<tr>
<td valign="bottom">Monkey A</td>
<td valign="bottom">−793.6</td>
<td valign="bottom">3.4</td>
<td valign="bottom">−2225.7</td>
<td valign="bottom">4.0</td>
<td valign="bottom">25.4</td>
<td valign="bottom">9.0</td>
<td valign="bottom">1.2</td>
<td valign="bottom">3.4</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="s2-2">
<title>The monkeys’ bias adjustments were adaptive with respect to optimal reward-rate functions</title>
<p>To try to identify common principles that governed these monkey- and context-dependent decision biases, we analyzed behavior with respect to optimal benchmarks based on certain reward-rate functions. We focused on reward per unit time (RR) and per trial (RTrial), which for this task are optimized in a DDM framework by adjusting momentary-evidence (<italic>me</italic>) and decision-rule (<italic>z</italic>) biases, such that both favor the large-reward choice. However, the magnitudes of these optimal adjustments depend on other task parameters (<italic>a, k</italic>, <italic>t<sub>0</sub>,</italic> and <italic>t<sub>1</sub>
</italic>, non-bias parameters from the DDM, plus the ratio of the two reward sizes and inter-trial intervals) that can vary from session to session. Thus, to determine the optimal adjustments, we performed DDM simulations with the fitted HDDM parameters from each session, using different combinations of <italic>me</italic> and <italic>z</italic> values (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). As reported previously (<xref ref-type="bibr" rid="bib6">Bogacz et al., 2006</xref>; <xref ref-type="bibr" rid="bib66">Simen et al., 2009</xref>), when the large reward was paired with the leftward choice, the optimal strategy used <italic>z</italic> &lt; 0.5 and <italic>me</italic> &lt; 0 (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, top panels, purple and orange circles for RR and RTrial, respectively). Conversely, when the larger reward was paired with the rightward choice, the optimal strategy used <italic>z</italic> &gt; 0.5 and <italic>me</italic> &gt; 0 (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, bottom panels).</p>
<fig-group>
<fig id="fig4" position="float">
<object-id pub-id-type="doi">10.7554/eLife.36018.016</object-id>
<label>Figure 4.</label>
<caption>
<title>Actual versus optimal adjustments of momentary-evidence (<italic>me</italic>) and decision-rule (<italic>z</italic>) biases.</title>
<p>(<bold>A</bold>) Schematic of the comparison procedure. Choice and RT data from the two reward contexts in a given session were fitted separately using the HDDM. These context- and session-specific best-fitting <italic>me</italic> and <italic>z</italic> values are plotted as the monkey’s data (black circles in B and C). Optimal values were determined by fixing parameters <italic>a</italic>, <italic>k</italic>, and non-decision times at best-fitting values from the HDDM and searching in the <italic>me</italic>/<italic>z</italic> grid space for combinations of <italic>me</italic> and <italic>z</italic> that produced maximal reward function values. For each <italic>me</italic> and <italic>z</italic> combination, the predicted probability of left/right choice and RTs were used with the actual task information (inter-trial interval, error timeout, and reward sizes) to calculate the expected reward rate (RR) and average reward per trial (RTrial). Optimal <italic>me/z</italic> adjustments were then identified to maximize RR (purple) or RTrial (orange). (<bold>B</bold>) Scatterplots of the monkeys’ <italic>me/z</italic> adjustments (black), predicted optimal adjustments for maximal RR (purple), and predicted optimal adjustments for maximal RTrial (orange), for the two reward contexts in all sessions (each data point was from a single session). Values of <italic>me</italic> &gt; 0 or <italic>z</italic> &gt; 0.5 produce biases favoring rightward choices. (<bold>C</bold>) Scatterplots of the differences in <italic>me</italic> (abscissa) and <italic>z</italic> (ordinate) between the two reward contexts for monkeys (black), for maximizing RR (purple), and for maximizing RTrial (orange). Positive Δme and Δz values produce biases favoring large-reward choices.</p>
<p>
<supplementary-material id="fig4sdata1">
<object-id pub-id-type="doi">10.7554/eLife.36018.019</object-id>
<label>Figure 4—source data 1.</label>
<caption>
<title>RTrial and RR function for each session and reward context.</title>
<p>The same data are used for <xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, <xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, <xref ref-type="fig" rid="fig7">Figure 7</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>, <xref ref-type="fig" rid="fig8">Figure 8</xref> and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>
</p>
</caption>
<media xlink:href="elife-36018-fig4-data1-v3.zip" mimetype="application" mime-subtype="zip"/>
</supplementary-material>
</p>
</caption>
<graphic xlink:href="elife-36018-fig4-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<fig id="fig4s1" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.017</object-id>
<label>Figure 4—figure supplement 1.</label>
<caption>
<title>Estimates of momentary-evidence (<italic>me</italic>) and decision-rule (<bold>z</bold>) biases using the collapsing-bound DDM fits.</title>
<p>Same format as <xref ref-type="fig" rid="fig4">Figure 4B and C</xref>, except here only showing fits to the monkeys’ data. As with the model without collapsing bounds, the adjustments in <italic>me</italic> tended to favor the large reward but the adjustments in <italic>z</italic> tended to favor the small reward.</p>
</caption>
<graphic xlink:href="elife-36018-fig4-figsupp1-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<fig id="fig4s2" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.018</object-id>
<label>Figure 4—figure supplement 2.</label>
<caption>
<title>Hypothetical neural activity encoding a reward-biased perceptual decision variable.</title>
<p>The blue and red curves depict rise-to-threshold dynamics in favor of a particular (say, rightward) choice under the two reward contexts, as indicated. Note that when the rightward choice is paired with larger reward: 1) the slope of the ramping process, which corresponds to an adjustment in momentary evidence (<italic>me</italic>), is steeper; and 2) the baseline activity, which corresponds to the decision-rule (<italic>z</italic>) adjustment, is lower.</p>
</caption>
<graphic xlink:href="elife-36018-fig4-figsupp2-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
</fig-group>
<p>The monkeys’ adjustments of momentary-evidence (<italic>me</italic>) and decision-rule (<italic>z</italic>) biases showed both differences and similarities with respect to these optimal predictions (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, black circles; similar results were obtained using fits from a model with collapsing bounds, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). In the next section, we consider the differences, in particular the apparent use of shifts in <italic>me</italic> in the adaptive direction (i.e., favoring the large-reward choice) but of a magnitude that was larger than predicted, along with shifts in <italic>z</italic> that tended to be in the non-adaptive direction (i.e., favoring the small-reward choice). Here we focus on the similarities and show that the monkeys’ decision biases were adaptive with respect to the reward-rate function in four ways (RTrial provided slightly better predictions of the data and thus are presented in the main figures; results based on RR are presented in the Supplementary Figures).</p>
<p>First, the best-fitting <italic>me</italic> and <italic>z</italic> values from each monkey corresponded to near-maximal reward rates (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). We compared the optimal values of reward per trial (RTrial<sub>max</sub>) to the values predicted from the monkeys’ best-fitting <italic>me</italic> and <italic>z</italic> adjustments (RTrial<sub>predict</sub>). Both RTrial<sub>predict</sub> and RTrial<sub>max</sub> depended on the same non-bias parameters in the HDDM fits that were determined per session (<italic>a, k</italic>, <italic>t<sub>0</sub>,</italic> and <italic>t<sub>1</sub>
</italic>) and thus are directly comparable. Their ratios tended to be nearly, but slightly less than, one (mean ratio: 0.977, 0.984, and 0.983 for monkeys F, C, and A, respectively) and remained relatively constant across sessions (<italic>H<sub>0</sub>
</italic>: slopes of linear regressions of these ratios versus session number = 0, p&gt;0.05 for all three monkeys). Similar results were also obtained using the monkeys’ realized rewards, which closely matched RTrial<sub>predict</sub> (mean ratio: 0.963, 0.980, and 0.974; across-session Spearman’s <italic>ρ</italic> = 0.976, 0.995, and 0.961, for monkeys F, C, and A, respectively, p&lt;0.0001 in all three cases). These results reflected the shallow plateau in the RTrial function near its peak (<xref ref-type="fig" rid="fig5">Figure 5B</xref>), such that the monkeys’ actual adjustments of <italic>me</italic> and <italic>z</italic> were within the contours for 97% RTrial<sub>max</sub> in most sessions (<xref ref-type="fig" rid="fig5">Figure 5C</xref>; see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> for results using RR). Thus, the monkeys’ overall choice biases were consistent with strategies that lead to nearly optimal reward outcomes.</p>
<fig-group>
<fig id="fig5" position="float">
<object-id pub-id-type="doi">10.7554/eLife.36018.020</object-id>
<label>Figure 5.</label>
<caption>
<title>Predicted versus optimal reward per trial (RTrial).</title>
<p>(<bold>A</bold>) Scatterplots of RTrial<sub>predict</sub>:RTrial<sub>max</sub> ratio as a function of session index. Each session was represented by two ratios, one for each reward context. (<bold>B</bold>) 97% RTrial<sub>max</sub> contours for all sessions, computed using the best-fitting HDDM parameters and experienced coherences and reward ratios from each session. Light grey: LR-Left blocks; Dark grey: LR-Right blocks. (<bold>C</bold>) The monkeys’ adjustments (blue: LR-Left blocks, red: LR-Right blocks) were largely within the 97% RTrial<sub>max</sub> contours for all sessions and tended to cluster in the <italic>me</italic> over-biased, <italic>z</italic> under-biased quadrants (except Monkey F in the LR-Right blocks). The contours and monkeys’ adjustments are centered at the optimal adjustments for each session.</p>
</caption>
<graphic xlink:href="elife-36018-fig5-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<fig id="fig5s1" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.021</object-id>
<label>Figure 5—figure supplement 1.</label>
<caption>
<title>Predicted versus optimal reward rate (RR).</title>
<p>Same format as <xref ref-type="fig" rid="fig5">Figure 5</xref>. Mean RR<sub>predict</sub>:RR<sub>max</sub> ratio across sessions = 0.971 for monkey F, 0.980 for monkey C, and 0.980 for monkey A.</p>
</caption>
<graphic xlink:href="elife-36018-fig5-figsupp1-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
</fig-group>
<p>Second, the across-session variability of each monkey’s decision biases was predicted by idiosyncratic features of the reward functions. The reward functions were, on average, different for the two reward contexts and each of the three monkeys (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). These differences included the size of the near-maximal plateau (red patch), which determined the level of tolerance in RTrial for deviations from optimal adjustments in <italic>me</italic> and <italic>z</italic>. This tolerance corresponded to the session-by-session variability in each monkey’s <italic>me</italic> and <italic>z</italic> adjustments (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). In general, monkey F had the smallest plateaus and tended to use the narrowest range of <italic>me</italic> and <italic>z</italic> adjustments across sessions. In contrast, monkey A had the largest plateaus and tended to use the widest range of <italic>me</italic> and <italic>z</italic> adjustments (Pearson’s <italic>ρ</italic> between the size of the 97% RTrial contour, in pixels, and the sum of the across-session variances in each monkeys’ <italic>me</italic> and <italic>z a</italic>djustments = 0.83, p=0.041). Analyses using the RR function produced qualitatively similar results (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p>
<fig-group>
<fig id="fig6" position="float">
<object-id pub-id-type="doi">10.7554/eLife.36018.022</object-id>
<label>Figure 6.</label>
<caption>
<title>Relationships between adjustments of momentary-evidence (<italic>me</italic>) and decision-rule (<bold>z</bold>) biases and RTrial function properties.</title>
<p>(<bold>A</bold>) Mean RTrial as a function of <italic>me</italic> and <italic>z</italic> adjustments for the LR-Left (top) and LR-Right (bottom) blocks. Hotter colors represent larger RTrial values (see legend to the right). RTrial was normalized to RTrial<sub>max</sub> for each session and then averaged across sessions. (<bold>B</bold>) Scatterplot of the total variance in <italic>me</italic> and <italic>z</italic> adjustments across sessions (ordinate) and the area of &gt;97% max of the average RTrial patch (abscissa). Variance and patch areas were measured separately for the two reward blocks (circles for LR-Left blocks, squares for LR-Right blocks). (<bold>C, D</bold>) The monkeys’ session- and context-specific values of <italic>me</italic> (<bold>C</bold>) and <italic>z</italic> (<bold>D</bold>) co-varied with the orientation of the &gt;97% heatmap patch (same as the contours in <xref ref-type="fig" rid="fig5">Figure 5B</xref>). Orientation is measured as the angle of the tilt from vertical. Circles: data from LR-Left block; squares: data from LR-Right block; lines: significant correlation between <italic>me</italic> (or <italic>z</italic>) and patch orientations across monkeys (p&lt;0.05). Colors indicate different monkeys (see legend in B). E, Scatterplots of conditionally optimal versus fitted Δ<italic>me</italic> (top row) and Δ<italic>z</italic> (bottom row). For each reward context, the conditionally optimal <italic>me (z)</italic> value was identified given the monkey’s best-fitting z (<italic>me</italic>) values. The conditionally optimal Δ<italic>me (</italic>Δ<italic>z)</italic> was the difference between the two conditional optimal <italic>me (z)</italic> values for the two reward contexts. Grey lines indicate the range of conditional Δ<italic>me</italic> (Δ<italic>z</italic>) values corresponding to the 97% maximal RTrial given the monkeys’ fitted z (<italic>me</italic>) values.</p>
</caption>
<graphic xlink:href="elife-36018-fig6-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<fig id="fig6s1" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.023</object-id>
<label>Figure 6—figure supplement 1.</label>
<caption>
<title>The monkeys’ momentary-evidence (<italic>me</italic>) and decision-rule (z) adjustments reflected RR function properties.</title>
<p>Same format as <xref ref-type="fig" rid="fig6">Figure 6</xref>, but using RR instead of RTrial.</p>
</caption>
<graphic xlink:href="elife-36018-fig6-figsupp1-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<fig id="fig6s2" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.025</object-id>
<label>Figure 6—figure supplement 2.</label>
<caption>
<title>The HDDM model fitting procedure does not introduce spurious correlations between patch orientation and <italic>me</italic> value.</title>
<p>Artificial sessions were simulated with fixed <italic>me</italic> values (±0.1 for the two reward contexts) and different <italic>k</italic> values. (<bold>A</bold>) Recovered <italic>k</italic> values from HDDM fitting closely matched <italic>k</italic> values used for the simulations. (<bold>B</bold>) Recovered <italic>me</italic> values from HDDM fitting closely matched <italic>me</italic> values used for simulation and did not correlate with RTrial patch orientation.</p>
</caption>
<graphic xlink:href="elife-36018-fig6-figsupp2-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<fig id="fig6s3" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.024</object-id>
<label>Figure 6—figure supplement 3.</label>
<caption>
<title>The correlation between fitted and conditionally optimal adjustments was stronger for the real, session-by-session data (red lines) than for unmatched (shuffled) sessions (bars).</title>
<p>(<bold>A, C</bold>) Momentary-evidence (Δ<italic>me</italic>) adjustments. (<bold>B, D</bold>) Decision-rule (Δ<italic>z</italic>) adjustments. (<bold>A, B</bold>) optimal values obtained with the RTrial function. (<bold>C, D</bold>) optimal values obtained with the RR function. Red lines indicate the partial Spearman correlation coefficients between the fitted and optimal Δ<italic>me</italic> or Δ<italic>z</italic> (obtained in the same way as the data in <xref ref-type="fig" rid="fig6">Figure 6E</xref>) for matched sessions. Bars represent the histograms of partial correlation for unmatched sessions, which were obtained by 100 random shuffles of the sessions (i.e., comparing the optimal and best-fitting values from different sessions). Note that the histograms for the unmatched sessions are centered at positive values, reflecting the non-session-specific tendency of reward surfaces to skew towards overly biased <italic>me</italic> and <italic>z</italic> values. The correlation values for matched sessions (red lines) are at even more positive values (Wilcoxson rank-sum test, p&lt;0.001 for all three monkeys and both Δ<italic>me</italic> and Δ<italic>z</italic>), suggesting additional session-specific tuning of the <italic>me</italic> and <italic>z</italic> parameters.</p>
</caption>
<graphic xlink:href="elife-36018-fig6-figsupp3-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
</fig-group>
<p>Third, the session-by-session adjustments in both <italic>me</italic> and <italic>z</italic> corresponded to particular features of each monkey’s context-specific reward function. The shape of this function, including the orientation of the plateau with respect to <italic>z</italic> and <italic>me</italic>, depended on the monkey’s perceptual sensitivity and the reward ratio for the given session. The monkeys’ <italic>me</italic> and <italic>z</italic> adjustments varied systematically with this orientation (<xref ref-type="fig" rid="fig6">Figure 6C and D</xref> for RTrial, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1C and D</xref> for RR). This result was not an artifact of the fitting procedure, which was able to recover appropriate, simulated bias parameter values regardless of the values of non-bias parameters that determine the shape of the reward function (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>).</p>
<p>Fourth, the monkeys’ <italic>me</italic> and <italic>z</italic> adjustments were correlated with the values that would maximize RTrial, given the value of the other parameter for the given session and reward context (<xref ref-type="fig" rid="fig6">Figure 6E</xref> for RTrial, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1E</xref> for RR). These correlations were substantially weakened by shuffling the session-by-session reward functions (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>). Together, these results suggest that all three monkeys used biases that were adaptively calibrated with respect to the reward information and perceptual sensitivity of each session.</p>
</sec>
<sec id="s2-3">
<title>The monkeys’ adaptive adjustments were consistent with a satisficing, gradient-based learning process</title>
<p>Thus far, we showed that all three monkeys adjusted their decision strategies in a manner that matched many features of the optimal predictions based on their idiosyncratic, context-specific reward-rate functions. However, their biases did not match the optimal predictions exactly. Specifically, all three monkeys used shifts in <italic>me</italic> favoring the large-reward choice (adaptive direction) but of a magnitude that was larger than predicted, along with shifts in <italic>z</italic> favoring the small-reward choice (non-adaptive direction). We next show that these shifts can be explained by a model in which the monkeys are initially over-biased, then adjust their model parameters to increase reward and stop learning when the reward is high enough, but not at its maximum possible value.</p>
<p>The intuition for this gradient-based satisficing model is shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>. The lines on the RTrial heatmap represent the trajectories of a gradient-tracking procedure that adjusts <italic>me</italic> and <italic>z</italic> values to increase RTrial until a termination point (for illustration, here we used 97% of the maximum possible value). For example, consider adjusting <italic>me</italic> and <italic>z</italic> by following all of the magenta gradient lines until their end-points. The lines are color-coded by <italic>me/z</italic> being adaptive vs. non-adaptive, regardless of their relative magnitudes to the optimal values. In other words, as long as the initial <italic>me</italic> and <italic>z</italic> values fall within the area covered by the magenta lines, the positive gradient-tracking procedure would lead to a good-enough solution with adaptive <italic>me</italic> and non-adaptive <italic>z</italic> values similar to what we found in the monkeys’ data. <xref ref-type="fig" rid="fig7">Figure 7</xref> also illustrates why assumptions about the starting point of this adaptive process are important: randomly selected starting points would result in learned <italic>me</italic> and <italic>z</italic> values distributed around the peak of the reward function, whereas the data (e.g., <xref ref-type="fig" rid="fig5">Figure 5C</xref>) show distinct clustering that implies particular patterns of starting points.</p>
<fig-group>
<fig id="fig7" position="float">
<object-id pub-id-type="doi">10.7554/eLife.36018.026</object-id>
<label>Figure 7.</label>
<caption>
<title>Relationships between starting and ending values of the satisficing, reward function gradient-based updating process.</title>
<p>Example gradient lines of the average RTrial maps for the three monkeys are color coded based on the end point of gradient-based <italic>me</italic> and <italic>z</italic> adjustments in the following ways: (1) <italic>me</italic> biases to large reward whereas <italic>z</italic> biases to small reward (magenta); (2) <italic>z</italic> biases to large reward whereas <italic>me</italic> biases to small reward (blue); (3) <italic>me</italic> and <italic>z</italic> both bias to large reward (green), and (4) <italic>me</italic> and <italic>z</italic> both bias to small reward (yellow). The gradient lines ended on the 97% RTrial<sub>max</sub> contours. Top row: LR-Left block; bottom row: LR-Right block.</p>
</caption>
<graphic xlink:href="elife-36018-fig7-v3" mimetype="image" mime-subtype="x-tiff"/>
</fig>
<fig id="fig7s1" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.027</object-id>
<label>Figure 7—figure supplement 1.</label>
<caption>
<title>RR gradient trajectories color-coded by the end points of the <italic>me/z</italic> patterns.</title>
<p>Same format as <xref ref-type="fig" rid="fig7">Figure 7</xref> but using gradients based on RR instead of RTrial.</p>
</caption>
<graphic xlink:href="elife-36018-fig7-figsupp1-v3" mimetype="image" mime-subtype="x-tiff"/>
</fig>
</fig-group>
<p>We simulated this process using: (1) different starting points; (2) gradients defined by the reward function derived separately for each reward context, session, and monkey; and (3) a termination rule corresponding to achieving each monkey’s average reward in that session (RTrial<sub>predict</sub>) estimated from the corresponding best-fitting model parameters and task conditions. This process is illustrated for LR-Left blocks in an example session from monkey C (<xref ref-type="fig" rid="fig8">Figure 8A</xref>). We estimated the unbiased <italic>me</italic> and <italic>z</italic> values as the midpoints between their values for LR-Left and LR-Right blocks (square). At this point, the RTrial gradient is larger along the <italic>me</italic> dimension than the <italic>z</italic> dimension, reflecting the tilt of the reward function. We set the initial point at baseline <italic>z</italic> and a very negative value of <italic>me</italic> (90% of the highest coherence used in the session; overshoot in the adaptive direction) and referred to this setting as the ‘over-<italic>me</italic>’ model. The <italic>me</italic> and <italic>z</italic> values were then updated according to the RTrial gradient (see cartoon insert in <xref ref-type="fig" rid="fig8">Figure 8A</xref>), until the monkey’s RTrial<sub>predict</sub> or better was achieved (magenta trace and circle). The endpoint of this updating process was very close to monkey C’s actual adjustment (gray circle). For comparison, three alternative models are illustrated. The ‘over-<italic>z</italic>’ model assumes updating from the baseline <italic>me</italic> and over-adjusted <italic>z</italic> values (blue, initial <italic>z</italic> set as 0.1 for the LR-Left context and 0.9 for the LR-Right context). The ‘over-both’ model assumes updating from the over-adjusted <italic>me</italic> and <italic>z</italic> values (green). The ‘neutral’ model assumes the same updating process but from the baseline <italic>me</italic> and baseline <italic>z</italic> (black). The endpoints from these alternative models deviated considerably from the monkey’s actual adjustment.</p>
<fig-group>
<fig id="fig8" position="float">
<object-id pub-id-type="doi">10.7554/eLife.36018.028</object-id>
<label>Figure 8.</label>
<caption>
<title>The satisficing reward function gradient-based model.</title>
<p>(<bold>A</bold>) Illustration of the procedure for predicting a monkey’s <italic>me</italic> and <italic>z</italic> values for a given RTrial function. For better visibility, RTrial for the LR-Left reward context in an example session is shown as a heatmap in greyscale. Gradient lines are shown as black lines. The square indicates the unbiased <italic>me</italic> and <italic>z</italic> combination (average values across the two reward contexts). The four trajectories represent gradient-based searches based on four alternative assumptions of initial values (see table on the right). All four searches stopped when the reward exceeded the average reward the monkey received in that session (RTrial<sub>predict</sub>), estimated from the corresponding best-fitting model parameters and task conditions. Open circles indicate the end values. Grey filled circle indicates the monkey’s actual <italic>me</italic> and z. Note that the end points differ among the four assumptions, with the magenta circle being the closest to the monkey’s fitted <italic>me</italic> and <italic>z</italic> of that session. (<bold>B</bold>) Scatterplots of the predicted and actual Δ<italic>me</italic> and Δz between reward contexts. Grey circles here are the same as the black circles in <xref ref-type="fig" rid="fig4">Figure 4C</xref>. Colors indicate model identity, as in (<bold>A</bold>). (<bold>C</bold>) Average regression coefficients between each monkey’s Δ<italic>me</italic> (left four bars) and Δ<italic>z</italic> (right four bars) values and predicted values for each of the four models. Filled bars: <italic>t-</italic>test, p&lt;0.05. (<bold>D</bold>) Covariation of <italic>me</italic> (top) and <italic>z</italic> (bottom) with the orientation of the &gt;97% maximal RTrial heatmap patch for monkeys and predictions of the four models. Blue: data from LR-Left blocks, red: data from LR-Right blocks. Data in the ‘Monkey’ column are the same as in <xref ref-type="fig" rid="fig6">Figure 6C and D</xref>. Note that predictions of the ‘over-<italic>me</italic>’ model best matched the monkey data than the other models.</p>
</caption>
<graphic xlink:href="elife-36018-fig8-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<fig id="fig8s1" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.029</object-id>
<label>Figure 8—figure supplement 1.</label>
<caption>
<title>Predictions of a RR gradient-based model.</title>
<p>Same format as <xref ref-type="fig" rid="fig8">Figure 8</xref> but using gradients based on RR instead of RTrial. The overly-biased starting <italic>me</italic> and <italic>z</italic> values were set as 90% of highest coherence level, and 0.1, respectively, except for the <italic>over-both</italic> model for one monkey C session (<italic>me</italic> = 88% * max(coh), <italic>z</italic> = 0.11) to avoid a local peak in the RR surface. Such local peaks at overly biased <italic>me</italic> and <italic>z</italic> values can divert the gradient-based updating process to even more biased values without ever reaching the monkey's final RR (e.g., the green trace at the bottom left corner in monkey C's LR-Left data in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>).</p>
</caption>
<graphic xlink:href="elife-36018-fig8-figsupp1-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<fig id="fig8s2" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.030</object-id>
<label>Figure 8—figure supplement 2.</label>
<caption>
<title>Dependence of the orientation and area of the near-optimal RTrial patch on parameters reflecting internal decision process and external task specifications.</title>
<p>The top two rows show the RTrial heatmaps with two values of a single parameter indicated above, while keeping the other parameters fixed at the baseline values. The third and fourth rows show the estimated orientation (the amount of tilt from vertical, in degrees) and area (in pixels), respectively, of the image patches corresponding to ≥97% of RTrial<sub>max</sub>. The baseline values of the parameters are: <italic>a</italic> = 1.5, <italic>k</italic> = 6, non-decision times = 0.3 s for both choices, <italic>ITI</italic> = 4 s, <italic>Timeout</italic> = 8 s, <italic>large-reward (LR): small-reward (SR) ratio</italic> = 2.</p>
</caption>
<graphic xlink:href="elife-36018-fig8-figsupp2-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<fig id="fig8s3" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.031</object-id>
<label>Figure 8—figure supplement 3.</label>
<caption>
<title>The joint effect of DDM model parameters <italic>a</italic> (governing the speed-accuracy trade-off) and <italic>k</italic> (governing perceptual sensitivity) on the shape of the reward function.</title>
<p>(<bold>A and B</bold>) Example RTrial functions corresponding to steeper gradients along the <italic>z</italic> (panel <bold>A</bold>, corresponding to the red points in panels <bold>C</bold> and <bold>D</bold>) or <italic>me</italic> (panel <bold>B</bold>, corresponding to the orange points in panels <bold>C</bold> and <bold>D</bold>) dimension. The gradient lines (black) stop when RTrial &gt;0.97 of the maximum value. A: <italic>a</italic> = 1, <italic>k</italic> = 5. B: <italic>a</italic> = 1, <italic>k</italic> = 40. <italic>Large-reward:small-reward ratio</italic> = 2. (<bold>C</bold>), Orientation of the patch corresponding to &gt;0.97 maximal RTrial as a function of the product of <italic>a</italic> and <italic>k</italic>. (<bold>D</bold>) The ratio of the mean gradients along the <italic>me</italic> and <italic>z</italic> dimensions as a function of the product of <italic>a</italic> and <italic>k</italic>. Our model assumes that the initial bias is along the dimension with the steeper gradient according to each monkey’s idiosyncratic RTrial function. Note that because <italic>me</italic> and <italic>z</italic> have different units, the boundary between initial-<italic>me</italic> and initial-<italic>z</italic> conditions may not correspond to a gradient ratio of 1.</p>
</caption>
<graphic xlink:href="elife-36018-fig8-figsupp3-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
<fig id="fig8s4" position="float" specific-use="child-fig">
<object-id pub-id-type="doi">10.7554/eLife.36018.032</object-id>
<label>Figure 8—figure supplement 4.</label>
<caption>
<title>Effects of the shape of the reward function on deviations from optimality.</title>
<p>(<bold>A</bold>) Illustration of our heuristic updating model and measurement of deviation of the end point from optimal. Yellow dot: optimal solution. Gray lines: trajectory for gradient ascent, ending at 0.97 maximal RTrial. Black line: trajectory for updating from the starting point (black dot, <italic>me</italic> = 0.54, <italic>z</italic> = 0.5), which ended at 0.97 maximal RTrial (blue dot). The deviation of the end point from optimal is measured as the distance from the yellow dot to the blue dot (yellow dashed line). The same starting point and ending criterion were used for data shown in (<bold>B</bold>) and (<bold>C</bold>). (<bold>B</bold>) The area corresponding to &gt;0.97 maximal RTrial plateau and end-point deviation from optimal increase with reward ratio. The product of <italic>a</italic> and <italic>k</italic> is fixed as 30. (<bold>C</bold>) The area corresponding to &gt;0.97 maximal RTrial plateau and end-point deviation from optimal decrease with the product of <italic>a</italic> and <italic>k</italic>. Reward ratio is fixed as 3.</p>
</caption>
<graphic xlink:href="elife-36018-fig8-figsupp4-v3" mimetype="application" mime-subtype="postscript"/>
</fig>
</fig-group>
<p>The ‘over-<italic>me</italic>’ model produced better predictions than the other three alternative models for all three monkeys. Of the four models, only the ‘over-<italic>me</italic>’ model captured the monkeys’ tendency to bias <italic>me</italic> toward the large-reward choice (positive Δ<italic>me</italic>) and bias <italic>z</italic> toward the small-reward choice (negative Δ<italic>z</italic>; <xref ref-type="fig" rid="fig8">Figure 8B</xref>). In contrast, the ‘over-<italic>z</italic>’ model predicted small adjustments in <italic>me</italic> and large adjustments in <italic>z</italic> favoring the large-reward choice; the ‘over-both’ model predicted relatively large, symmetric <italic>me</italic> and z adjustments favoring the large-reward choice; and the ‘neutral’ model predicted relatively small, symmetric adjustments in both <italic>me</italic> and <italic>z</italic> favoring the large-reward choice. Accordingly, for each monkey, the predicted and actual values of both Δ<italic>me</italic> and Δ<italic>z</italic> were most strongly positively correlated for predictions from the ‘over-<italic>me</italic>’ model compared to the other models (<xref ref-type="fig" rid="fig8">Figure 8C</xref>). The ‘over-<italic>me’</italic> model was also the only one of the models we tested that recapitulated the measured relationships between both <italic>me</italic>- and <italic>z</italic>-dependent biases and session-by-session changes in the orientation of the RTrial function (<xref ref-type="fig" rid="fig8">Figure 8D</xref>). Similar results were observed using RR function (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref> and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). We also examined whether the shape of the reward surface alone can explain the monkeys' bias patterns. We repeated the simulations using randomized starting points, with or without additional noise in each updating step. These simulations could not reproduce the monkeys' bias patterns (data not shown), suggesting that using ‘over-<italic>me</italic>’ starting points is critical for accounting for the monkeys' suboptimal behavior.</p>
</sec>
</sec>
<sec sec-type="discussion" id="s3">
<title>Discussion</title>
<p>We analyzed the behavior of three monkeys performing a decision task that encouraged the use of both uncertain visual motion evidence and the reward context. All three monkeys made choices that were sensitive to the strength of the sensory evidence and were biased toward the larger-reward choice, which is roughly consistent with results from previous studies of humans and monkeys performing similar tasks (<xref ref-type="bibr" rid="bib43">Maddox and Bohil, 1998</xref>; <xref ref-type="bibr" rid="bib79">Voss et al., 2004</xref>; <xref ref-type="bibr" rid="bib10">Diederich and Busemeyer, 2006</xref>; <xref ref-type="bibr" rid="bib40">Liston and Stone, 2008</xref>; <xref ref-type="bibr" rid="bib63">Serences, 2008</xref>; <xref ref-type="bibr" rid="bib17">Feng et al., 2009</xref>; <xref ref-type="bibr" rid="bib66">Simen et al., 2009</xref>; <xref ref-type="bibr" rid="bib49">Nomoto et al., 2010</xref>; <xref ref-type="bibr" rid="bib74">Summerfield and Koechlin, 2010</xref>; <xref ref-type="bibr" rid="bib76">Teichert and Ferrera, 2010</xref>; <xref ref-type="bibr" rid="bib19">Gao et al., 2011</xref>; <xref ref-type="bibr" rid="bib39">Leite and Ratcliff, 2011</xref>; <xref ref-type="bibr" rid="bib46">Mulder et al., 2012</xref>; <xref ref-type="bibr" rid="bib82">Wang et al., 2013</xref>; <xref ref-type="bibr" rid="bib84">White and Poldrack, 2014</xref>). However, we also found that these adjustments differed considerably in detail for the three monkeys, in terms of overall magnitude, dependence on perceptual sensitivity and offered rewards, and relationship to RTs. We quantified these effects with a logistic analysis and a commonly used model of decision-making, the drift-diffusion model (DDM), which allowed us to compare the underlying decision-related computations to hypothetical benchmarks that would maximize reward. We found that all three monkeys made reward context-dependent adjustments with two basic components: (1) an over-adjustment of the momentary evidence provided by the sensory stimulus (<italic>me</italic>) in favor of the large-reward option; and (2) an adjustment to the decision rule that governs the total evidence needed for each choice (<italic>z</italic>), but in the opposite direction (i.e., towards the small-reward option). Similar to some earlier reports of human and monkey performance on somewhat similar tasks, our monkeys did not optimize reward rate (<xref ref-type="bibr" rid="bib71">Starns and Ratcliff, 2010</xref>
<xref ref-type="bibr" rid="bib72">Starns and Ratcliff, 2012</xref>; <xref ref-type="bibr" rid="bib76">Teichert and Ferrera, 2010</xref>). Instead, their adjustments tended to provide nearly, but not exactly, maximal reward intake. We proposed a common heuristic strategy based on the monkeys’ individual reward functions to account for the idiosyncratic adjustments across monkeys and across sessions within the same monkey.</p>
<sec id="s3-1">
<title>Considerations for assessing optimality and rationality</title>
<p>Assessing decision optimality requires a model of the underlying computations. In this study, we chose the DDM for several reasons. First, it provided a parsimonious account of both the choice and RT data (<xref ref-type="bibr" rid="bib52">Palmer et al., 2005</xref>; <xref ref-type="bibr" rid="bib56">Ratcliff et al., 1999</xref>). Second, as discussed in more detail below, the DDM and related accumulate-to-bound models have provided useful guidance for identifying neural substrates of the decision process (<xref ref-type="bibr" rid="bib60">Roitman and Shadlen, 2002</xref>; <xref ref-type="bibr" rid="bib11">Ding and Gold, 2010</xref>; <xref ref-type="bibr" rid="bib12">Ding and Gold, 2012a</xref>; <xref ref-type="bibr" rid="bib31">Hanks et al., 2011</xref>; <xref ref-type="bibr" rid="bib58">Ratcliff et al., 2003</xref>; <xref ref-type="bibr" rid="bib61">Rorie et al., 2010</xref>; <xref ref-type="bibr" rid="bib46">Mulder et al., 2012</xref>; <xref ref-type="bibr" rid="bib74">Summerfield and Koechlin, 2010</xref>; <xref ref-type="bibr" rid="bib18">Frank et al., 2015</xref>). Third, these models are closely linked to normative theory, including under certain assumptions matching the statistical procedure known as the sequential probability ratio test that can optimally balance the speed and accuracy of uncertain decisions (<xref ref-type="bibr" rid="bib3">Barnard, 1946</xref>; <xref ref-type="bibr" rid="bib80">Wald, 1947</xref>; <xref ref-type="bibr" rid="bib81">Wald and Wolfowitz, 1948</xref>; <xref ref-type="bibr" rid="bib16">Edwards, 1965</xref>). These normative links were central to our ability to use the DDM to relate the monkeys’ behavior to different forms of reward optimization. The particular form of DDM that we used produced reasonably good, but not perfect, fits to the monkeys’ data. These results support the utility of the DDM framework but also underscore the fact that we do not yet know the true model, which could impact our optimality assessment.</p>
<p>Assessing optimality also requires an appropriate definition of the optimization goal. In our study, we focused primarily on the goal of maximizing reward rate (per trial or per unit of time). Based on this definition, the monkeys showed suboptimal reward-context-dependent adjustments. It is possible that the monkeys’ were optimizing for a different goal, such as accuracy or a competition between reward and accuracy (‘COBRA,’ <xref ref-type="bibr" rid="bib43">Maddox and Bohil, 1998</xref>). However, the monkeys’ behavior was not consistent with optimizing for these goals, either. Specifically, none of these goals would predict optimal <italic>z</italic> adjustment that favors the small reward choice: accuracy maximization would require unbiased decisions (<italic>me</italic> = 0 and <italic>z</italic> = 0.5), whereas COBRA would require <italic>z</italic> values with smaller magnitude (between 0.5 and those predicted for reward maximization alone), but still in the adaptive direction. Therefore, the monkeys’ strategies were not consistent with simply maximizing commonly considered reward functions.</p>
<p>Deviations from optimal behavior are often ascribed to a lack of effort or poor learning. However, these explanations seem unlikely to be primary sources of suboptimality in our study. For example, lapse rates, representing the overall ability to attend to and perform the task, were consistently near zero for all three monkeys. Moreover, the monkeys’ reward outcomes (RTrial or RR with respect to optimal values) did not change systematically with experience but instead stayed close to the optimal values. These results imply that the monkeys understood the task demands and performed consistently well over the course of our study. Suboptimal performance has also been observed in human subjects, even with explicit instructions about the optimality criteria (<xref ref-type="bibr" rid="bib71">Starns and Ratcliff, 2010</xref>; <xref ref-type="bibr" rid="bib72">Starns and Ratcliff, 2012</xref>), suggesting that additional factors need to be considered to understand apparent suboptimality in general forms of decision-making. In our study, the monkeys made adjustments that were adapted to changes in their idiosyncratic, context-dependent reward functions, which reflected session-specific reward ratios and motion coherences and the monkeys’ daily variations of perceptual sensitivity and speed-accuracy trade-offs (<xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Based on these observations, we reasoned that the seemingly sub-optimal behaviors may instead reflect a common, adaptive, rational strategy that aimed to attain good-enough (satisficing) outcomes.</p>
<p>The gradient-based, satisficing model we proposed was based on the considerations discussed below to account for our results. We do not yet know how well this model generalizes to other tasks and conditions, but it exemplifies an additional set of general principles for assessing the rationality of decision-making behavior: goals that are not necessarily optimal but good enough, potential heuristic strategies based on the properties of the utility function, and flexible adaptation to changes in the external and internal conditions.</p>
</sec>
<sec id="s3-2">
<title>Assumptions and experimental predictions of the proposed learning strategy</title>
<p>In general, finding rational solutions through trial-and-error or stepwise updates requires a sufficient gradient in the utility function to drive learning (<xref ref-type="bibr" rid="bib75">Sutton and Barto, 1998</xref>). Our proposed scheme couples a standard gradient-following algorithm with principles that have been used to explain and facilitate decisions with high uncertainties, time pressures, and/or complexity to achieve a satisficing solution (<xref ref-type="bibr" rid="bib68">Simon, 1966</xref>; <xref ref-type="bibr" rid="bib86">Wierzbicki, 1982</xref>; <xref ref-type="bibr" rid="bib22">Gigerenzer and Goldstein, 1996</xref>; <xref ref-type="bibr" rid="bib50">Nosofsky and Palmeri, 1997</xref>; <xref ref-type="bibr" rid="bib29">Goodrich et al., 1998</xref>; <xref ref-type="bibr" rid="bib62">Sakawa and Yauchi, 2001</xref>; <xref ref-type="bibr" rid="bib28">Goldstein and Gigerenzer, 2002</xref>; <xref ref-type="bibr" rid="bib73">Stirling, 2003</xref>; <xref ref-type="bibr" rid="bib23">Gigerenzer, 2010</xref>; <xref ref-type="bibr" rid="bib51">Oh et al., 2016</xref>). This scheme complements but differs from a previously proposed satisficing strategy to account for human subjects’ suboptimal calibration of the speed-accuracy trade-off via adjustments of the decision bounds of a DDM that favor robust solutions given uncertainties about the inter-trial interval (<xref ref-type="bibr" rid="bib87">Zacksenhouse et al., 2010</xref>). In contrast, our proposed strategy focuses on reward-biased behaviors for a given speed-accuracy tradeoff and operates on reward per trial, which is, by definition, independent of inter-trial-interval.</p>
<p>Our scheme was based on four key assumptions, as follows. Our first key assumption was that the starting point for gradient following was not the unbiased state (i.e., <italic>me</italic> = 0 and <italic>z</italic> = 0.5) but an over-biased state. Notably, in many cases the monkeys could have performed as well or better than they did, in terms of optimizing reward rate, by making unbiased decisions. The fact that none did so prompted our assumption that their session-by-session adjustments tended to reduce, not inflate, biases. Specifically, we assumed that the initial experience of the asymmetric reward prompted an over-reaction to bias choices towards the large-reward alternative. In general, such an initial over-reaction is not uncommon, as other studies have shown excessive, initial biases that are reduced or eliminated with training (<xref ref-type="bibr" rid="bib24">Gold et al., 2008</xref>; <xref ref-type="bibr" rid="bib33">Jones et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">Nikolaev et al., 2016</xref>). The over-reaction is also rational because the penalty is larger for an under-reaction than for an over-reaction. For example, in the average RTrial heatmaps for our task (<xref ref-type="fig" rid="fig6">Figure 6A</xref>), the gradient dropped faster in the under-biased side than in the over-biased side. This pattern is generally true for tasks with sigmoid-like psychometric functions (for example, the curves in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Our model further suggests that the nature of this initial reaction, which may be driven by individually tuned features of the reward function that can remain largely consistent even for equal-reward tasks (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>) and then constrain the end-points of a gradient-based adjustment process (<xref ref-type="fig" rid="fig8">Figure 8</xref>), may help account for the extensive individual variability in biases that has been reported for reward-biased perceptual tasks (<xref ref-type="bibr" rid="bib79">Voss et al., 2004</xref>; <xref ref-type="bibr" rid="bib74">Summerfield and Koechlin, 2010</xref>; <xref ref-type="bibr" rid="bib39">Leite and Ratcliff, 2011</xref>; <xref ref-type="bibr" rid="bib8">Cicmil et al., 2015</xref>).</p>
<p>The specific form of initial over-reaction in our model, which was based on the gradient asymmetry of the reward function, makes testable predictions. Specifically, our data were most consistent with an initial bias in momentary evidence (<italic>me</italic>), which caused the biggest change in the reward function. However, this gradient asymmetry can change dramatically under different conditions. For example, changes in the subject’s cautiousness (i.e., the total bound height parameter, <italic>a</italic>) and perceptual sensitivity (<italic>k</italic>) would result in a steeper gradient in the other dimension (the decision rule, or <italic>z</italic>) of the reward function (<xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref>). Our model predicts that such a subject would be more prone to an initial bias along that dimension. This prediction can be tested by using speed-accuracy instructions to affect the bound height and different stimulus parameters to change perceptual sensitivity (<xref ref-type="bibr" rid="bib52">Palmer et al., 2005</xref>; <xref ref-type="bibr" rid="bib20">Gegenfurtner and Hawken, 1996</xref>).</p>
<p>Our second key assumption was that from this initial, over-biased state, the monkeys made adjustments to both the momentary evidence (<italic>me</italic>) and decision rule (<italic>z</italic>) that generally followed the gradient of the reward function. The proposed step-wise adjustments occurred too quickly to be evident in behavior; for example the estimated biases were similar for the early and late halves in a block (data not shown). Instead, our primary support for this scheme was that the steady-state biases measured in each session were tightly coupled to the shape of the reward function for that session. It would be interesting to design tasks that might allow for more direct measurements of the updating process itself, for example, by manipulating both the initial biases and relevant reward gradient that might promote a longer adjustment process.</p>
<p>Our third key assumption was that the shallowness of the utility of the function around the peak supported satisficing solutions. Specifically, gradient-based adjustments, particularly those that use rapid updates based on implicit knowledge of the utility function, may be sensitive only to relatively large gradients. For our task, the gradients were much smaller around the peak, implying that there were large ranges of parameter values that provided such similar outcomes that further adjustments were not used. In principle, it is possible to change the task conditions to test if and how subjects might optimize with respect to steeper functions around the peak. For example, for RTrial, the most effective way to increase the gradient magnitude near the peak (i.e., reducing the area of the dark red patch) is to increase sensory sensitivity (<italic>k</italic>) or cautiousness (<italic>a</italic>; i.e., emphasizing accuracy over speed; <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>). For RR, the gradient can also be enhanced by increasing the time-out penalty. Despite some practical concerns about these manipulations (e.g., increasing time-out penalties can decrease motivation), it would be interesting to study their effects on performance in more detail to understand the conditions under which satisficing or ‘good enough’ strategies are used (<xref ref-type="bibr" rid="bib67">Simon, 1956</xref>; <xref ref-type="bibr" rid="bib69">Simon, 1982</xref>).</p>
<p>Our fourth key assumption was that the monkeys terminated adjustments as soon as they reached a good-enough reward outcome. This termination rule produced end points that approximated the monkeys’ behavior reasonably well. Other termination rules are likely to produce similar end points. For example, the learning rate for synaptic weights might decrease as the presynaptic and postsynaptic activities become less variable (<xref ref-type="bibr" rid="bib1">Aitchison et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Kirkpatrick et al., 2017</xref>). In this scheme, learning gradually slows down as the monkey approaches the plateau on the reward surface, which might account for our results.</p>
<p>The satisficing reward gradient-based scheme we propose may further inform appropriate task designs for future studies. For example, our scheme implies that the shape of the reward function near the peak, particularly the steepness of the gradient, can have a strong impact on how closely a subject comes to the optimal solution for a given set of conditions. Thus, task manipulations that affect the shape of the reward-function peak could, in principle, be used to control whether a study focuses on more- or less-optimal behaviors (<xref ref-type="fig" rid="fig8s4">Figure 8—figure supplement 4</xref>). For example, increasing perceptual sensitivity (e.g., via training) and/or decisions that emphasize accuracy over speed (e.g., via instructions) tends to sharpen the peak of the reward function. According to our scheme, this sharpening should promote increasingly optimal decision-making, above and beyond the performance gains associated with increasing accuracy, because the gradient can be followed closer to the peak of the reward function. The shape of the peak is also affected by the reward ratio, such that higher ratios lead to larger plateaus, i.e. shallower gradient, near the peak. This relationship leads to the idea that, all else being equal, a smaller reward ratio may be more suitable for investigating principles of near-optimal behavior, whereas a larger reward ratio may be more suitable for investigating the source and principles of sub-optimal behaviors.</p>
</sec>
<sec id="s3-3">
<title>Possible neural mechanisms</title>
<p>The DDM framework has been used effectively to identify and interpret neural substrates of key computational components of the decision process for symmetric-reward versions of the motion-discrimination task. Our study benefitted from an RT task design that provided a richer set of constraints for inferring characteristics of the underlying decision process than choice data alone (<xref ref-type="bibr" rid="bib17">Feng et al., 2009</xref>; <xref ref-type="bibr" rid="bib49">Nomoto et al., 2010</xref>; <xref ref-type="bibr" rid="bib76">Teichert and Ferrera, 2010</xref>). The monkeys’ strategy further provides valuable anchors for future studies of the neural mechanisms underlying decisions that are biased by reward asymmetry, stimulus probability asymmetry, and other task contexts.</p>
<p>For neural correlates of bias terms in the DDM, it is commonly hypothesized that <italic>me</italic> adjustments may be implemented as modulation of MT output and/or synaptic weights for the connections between different MT subpopulations and decision areas (<xref ref-type="bibr" rid="bib8">Cicmil et al., 2015</xref>). In contrast, <italic>z</italic> adjustments may be implemented as context-dependent baseline changes in neural representations of the decision variable and/or context-dependent changes in the rule that determines the final choice (<xref ref-type="bibr" rid="bib42">Lo and Wang, 2006</xref>; <xref ref-type="bibr" rid="bib53">Rao, 2010</xref>; <xref ref-type="bibr" rid="bib41">Lo et al., 2015</xref>; <xref ref-type="bibr" rid="bib83">Wei et al., 2015</xref>). The manifestation of these adjustments in neural activity that encodes a decision variable may thus differ in its temporal characteristics: a <italic>me</italic> adjustment is assumed to modulate the rate of change in neural activity, whereas a <italic>z</italic> adjustment does not. However, such a theoretical difference can be challenging to observe, because of the stochasticity in spike generation and, given such stochasticity, practical difficulties in obtaining sufficient data with long decision deliberation times. By adjusting <italic>me</italic> and <italic>z</italic> in opposite directions, our monkeys’ strategies may allow a simpler test to disambiguate neural correlates of <italic>me</italic> and <italic>z</italic>. Specifically, a neuron or neuronal population that encodes <italic>me</italic> may show reward modulation congruent with its choice preference, whereas a neuron or neuronal population that encodes <italic>z</italic> may show reward modulation opposite to its choice preference (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). These predictions further suggest that, although it is important to understand if and how human or animal subjects can perform a certain task optimally, for certain systems-level questions, there may be benefits to tailoring task designs to promote sub-optimal strategies in otherwise well-trained subjects.</p>
</sec>
</sec>
<sec sec-type="materials|methods" id="s4">
<title>Materials and methods</title>
<sec id="s4-1">
<title>Subjects</title>
<p>We used three rhesus macaques (<italic>Macaca mulatta</italic>), two male and one female, to study behavior on an asymmetric-reward response-time random-dot motion discrimination task (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, see below). Prior to this study, monkeys F and C had been trained extensively on the equal-reward RT version of the task (<xref ref-type="bibr" rid="bib11">Ding and Gold, 2010</xref>; <xref ref-type="bibr" rid="bib13">Ding and Gold, 2012b</xref>; <xref ref-type="bibr" rid="bib12">Ding and Gold, 2012a</xref>). Monkey A had been trained extensively on non-RT dots tasks (<xref ref-type="bibr" rid="bib9">Connolly et al., 2009</xref>; <xref ref-type="bibr" rid="bib4">Bennur and Gold, 2011</xref>), followed by &gt;130 sessions of training on the equal-reward RT dots task. All training and experimental procedures were in accordance with the National Institutes of Health Guide for the Care and Use of Laboratory Animals and were approved by the University of Pennsylvania Institutional Animal Care and Use Committee (#804726).</p>
</sec>
<sec id="s4-2">
<title>Behavioral task</title>
<p>Our task (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) was based on the widely used random-dot motion discrimination task that typically has symmetric rewards (<xref ref-type="bibr" rid="bib60">Roitman and Shadlen, 2002</xref>; <xref ref-type="bibr" rid="bib11">Ding and Gold, 2010</xref>). Briefly, a trial started with presentation of a fixation point at the center of a computer screen in front of a monkey. Two choice targets appeared 0.5 s after the monkey acquired fixation. After a delay, the fixation point was dimmed and a random-dot kinematogram (speed: 6 °/s) was shown in a 5° aperture centered on the fixation point. For monkeys F and C, the delay duration was drawn from a truncated exponential distribution with mean = 0.7 s, max = 2.5 s, min = 0.4 s. For monkey A, the delay was set as 0.75 s. The monkey was required to report the perceived global motion direction by making a saccade to the corresponding choice target at a self-determined time (a 50 ms minimum latency was imposed to discourage fast guesses). The stimulus was immediately turned off when the monkeys’ gaze left the fixation window (4°, 4°, and 3° square windows for monkey F, C, and A, respectively). Correct choices (i.e., saccades to the target congruent with actual motion direction) were rewarded with juice. Error choices were not rewarded and instead penalized with a timeout before the next trial began (timeout duration: 3 s, 0.5–2 s, and 2.5 s, for monkeys F, C, and A, respectively).</p>
<p>On each trial, the motion direction was randomly selected toward one of the choice targets along the horizontal axis. The motion strength of the kinematogram was controlled as the fraction of dots moving coherently to one direction (coherence). On each trial, coherence was randomly selected from 0.032, 0.064, 0.128, 0.256, and 0.512 for monkeys F and C, and from 0.128, 0.256, 0.512, and 0.75 for monkey A. In a subset of sessions, coherence levels of 0.064, 0.09, 0.35, and/or 0.6 were also used for monkey A.</p>
<p>We imposed two types of reward context on the basic task. For the ‘LR-Left’ reward context, correct leftward saccades were rewarded with a larger amount of juice than correct rightward saccades. For the ‘LR-Right’ reward context, correct leftward saccades were rewarded with a smaller amount of juice than correct rightward saccades. The large:small reward ratio was on average 1.34, 1.91, and 2.45 for monkeys F, C, and A, respectively. Reward context was alternated between blocks and constant within a block. Block changes were signaled to the monkey with an inter-block interval of 5 s. The reward context for the current block was signaled to the monkey in two ways: 1) in the first trial after a block change, the two choice targets were presented in blue and green colors, for small and large rewards, respectively (this trial was not included for analysis); and 2) only the highest coherence level (near 100% accuracy) was used for the first two trials after a block change to ensure that the monkey physically experienced the difference in reward outcome for the two choices. For the rest of the block, choice targets were presented in the same color and motion directions and coherence levels were randomly interleaved.</p>
<p>We only included sessions in which there are more than 200 trials, more than eight coherences and more than eight trials for each coherence, motion direction and reward context (61, 37, and 43 sessions for monkey F, C, and A, respectively).</p>
</sec>
<sec id="s4-3">
<title>Basic characterization of behavioral performance</title>
<p>Eye position was monitored using a video-based system (ASL) sampled at 240 Hz. RT was measured as the time from stimulus onset to saccade onset, the latter identified offline with respect to velocity (&gt;40°/s) and acceleration (&gt;8000°/s<sup>2</sup>). Performance was quantified with psychometric and chronometric functions (<xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="fig" rid="fig3">Figure 3</xref>), which describe the relationship of motion strength (signed coherence, <italic>Coh</italic>, which was the proportion of the dots moving in the same direction, positive for rightward motion, negative for leftward motion) with choice and RT, respectively. Psychometric functions were fitted to a logistic function (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>), in which <inline-formula>
<mml:math id="inf1">
<mml:mstyle displaystyle="true" scriptlevel="0">
<mml:mrow>
<mml:mrow>
<mml:mi>λ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mstyle>
</mml:math>
</inline-formula> is the error rate, or lapse rate, independent of the motion information; 𝞪<sub>0</sub> and (𝞪<sub>0 </sub>+ 𝞪<sub>rew</sub>)are the bias terms, which measures the coherence at which the performance was at chance level, in the LR-Right and LR-Left reward contexts, respectively. 𝞫<sub>0</sub> and (𝞫<sub>0</sub> + 𝞫<sub>rew</sub>) are the perceptual sensitivities in the LR-Right and LR-Left reward contexts, respectively.<disp-formula id="equ1">
<label>(1)</label>
<mml:math id="m1">
<mml:msub>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>g</mml:mi>
<mml:mi>h</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>w</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>r</mml:mi>
<mml:mi>d</mml:mi> <mml:mi/>
<mml:mi>c</mml:mi>
<mml:mi>h</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>c</mml:mi>
<mml:mi>e</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi>λ</mml:mi>
<mml:mo>+</mml:mo>
<mml:mfenced separators="|">
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo>-</mml:mo>
<mml:mn>2</mml:mn> <mml:mi/>
<mml:mi>λ</mml:mi>
</mml:mrow>
</mml:mfenced> <mml:mi/>
<mml:mo>×</mml:mo> <mml:mi/>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo>+</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>e</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mi>S</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>n</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>v</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>y</mml:mi>
<mml:mo>(</mml:mo>
<mml:mi>C</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>h</mml:mi>
<mml:mo>-</mml:mo>
<mml:mi>B</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>s</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:mfrac>
</mml:math>
</disp-formula>
</p>
</sec>
<sec id="s4-4">
<title>Reward-biased drift-diffusion model</title>
<p>To infer the computational strategies employed by the monkeys, we adopted the widely used accumulation-to-bound framework, the drift-diffusion model (DDM; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). In the standard DDM, motion evidence is modeled as a random variable following a Gaussian distribution with a mean linearly proportional to the signed coherence and a fixed variance. The decision variable (DV) is modeled as temporal accumulation (integral) of the evidence, drifting between two decision bounds. Once the DV crosses a bound, evidence accumulation is terminated, the identity of the decision is determined by which bound is crossed, and the decision time is determined by the accumulation time. RT is modeled as the sum of decision time and saccade-specific non-decision times, the latter accounting for the contributions of evidence-independent sensory and motor processes.</p>
<p>To model the observed influences of motion stimulus and reward context on monkeys’ choice and RT behavior, we introduced two reward context-dependent terms: <italic>z</italic> specifies the relative bound heights for the two choices and <italic>me</italic> specifies the equivalent momentary evidence that is added to the motion evidence at each accumulating step. Thus, for each reward context, six parameters were used to specify the decision performance: <italic>a:</italic> total bound height; <italic>k</italic>: proportional scaling factor converting evidence to the drift rate; <italic>t<sub>0</sub>
</italic> and <italic>t<sub>1</sub>
</italic>: non-decision times for leftward and rightward choices, respectively; and <italic>z</italic> and <italic>me.</italic> Similar approaches have been used in studies of human and animal decision making under unequal payoff structure and/or prior probabilities (<xref ref-type="bibr" rid="bib79">Voss et al., 2004</xref>; <xref ref-type="bibr" rid="bib6">Bogacz et al., 2006</xref>; <xref ref-type="bibr" rid="bib10">Diederich and Busemeyer, 2006</xref>; <xref ref-type="bibr" rid="bib74">Summerfield and Koechlin, 2010</xref>; <xref ref-type="bibr" rid="bib31">Hanks et al., 2011</xref>; <xref ref-type="bibr" rid="bib46">Mulder et al., 2012</xref>).</p>
<p>To fit the monkeys’ data, we implemented hierarchical DDM fitting using an open-source package in Python, which performs Bayesian estimates of DDM parameters based on single-trial RTs (<xref ref-type="bibr" rid="bib85">Wiecki et al., 2013</xref>). This method assumes that parameters from individual sessions are samples from a group distribution. The initial prior distribution of a given parameter is determined from previous reports of human perceptual performance and is generally consistent with monkey performance on equal reward motion discrimination tasks (<xref ref-type="bibr" rid="bib11">Ding and Gold, 2010</xref>; <xref ref-type="bibr" rid="bib44">Matzke and Wagenmakers, 2009</xref>). The posterior distributions of the session- and group-level parameters are estimated with Markov chain Monte Carlo sampling. The HDDM was fit to each monkey separately.</p>
<p>For each dataset, we performed 5 chains of sampling with a minimum of 10000 total samples (range: 10000 – 20000; burn-in: 5000 samples) and inspected the trace, autocorrelation and marginal posterior histogram of the group-level parameters to detect signs of poor convergence. To ensure similar level of convergence across models, we computed the Gelman-Rubin statistic (R-hat) and only accepted fits with R-hat &lt;1.01.</p>
<p>To assess whether reward context modulation of both <italic>z</italic> and <italic>me</italic> was necessary to account for monkeys’ behavioral data, we compared fitting performance between the model with both terms (‘full’) and reduced models with only one term (‘z-only’ and ‘me-only’). Model selection was based on the deviance information criterion (DIC), with a smaller DIC value indicating a preferred model. Because DIC tends to favor more complex models, we bootstrapped the expected ΔDIC values, assuming the reduced models were the ground truth, using trial-matched simulations. For each session, we generated simulated data using the DDM, with single-session parameters fitted by <italic>me</italic>-only or <italic>z</italic>-only HDDM models and with the number of trials for each direction × coherence × reward context combination matched to the monkey’s data for that session. These simulated data were then re-fitted by all three models to estimate the predicted ΔDIC, assuming the reduced model as the generative model.</p>
<p>To test an alternative model, we also fitted monkeys’ data to a DDM with collapsing bounds(<xref ref-type="bibr" rid="bib88">Zylberberg et al., 2016</xref>). This DDM was constructed as the expected first-stopping-time distribution given a set of parameters, using the PyMC module (version 2.3.6) in Python (version 3.5.2). The three model variants, 'full', <italic>'me</italic>-only' and <italic>'z</italic>-only', and their associated parameters were the same as in HDDM, except that the total bound distance decreases with time. The distance between the two choice bounds was set as<disp-formula id="equ2">
<mml:math id="m2">
<mml:mi>a</mml:mi>
<mml:mo>/</mml:mo>
<mml:mo>(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>+</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>e</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>β</mml:mi>
<mml:mfenced separators="|">
<mml:mrow>
<mml:mi>t</mml:mi>
<mml:mo>-</mml:mo>
<mml:mi>d</mml:mi>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
</mml:msup>
<mml:mo>)</mml:mo>
</mml:math>
</disp-formula>where <inline-formula>
<mml:math id="inf2">
<mml:mi>a</mml:mi>
</mml:math>
</inline-formula> is the initial bound distance, <inline-formula>
<mml:math id="inf3">
<mml:mi>β</mml:mi>
</mml:math>
</inline-formula> determines the rate of collapsing, and <inline-formula>
<mml:math id="inf4">
<mml:mi>d</mml:mi>
</mml:math>
</inline-formula> determines the onset of the collapse. Fitting was performed by computing the maximum <italic>a posteriori</italic> estimates, followed by Markov chain Monte Carlo sampling, of DDM parameters given the experimental RT data.</p>
</sec>
<sec id="s4-5">
<title>Sequential analysis</title>
<p>To examine possible sequential choice effects, for each monkey and session we fitted the choice data to three logistic functions. Each function was in the same form as <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> but with one of four possible additional terms describing a sequential effect based on whether the previous trial was correct or not, and whether the previous trial was to the large or small reward target. The sequential effect was assessed via a likelihood-ratio test for <italic>H<sub>0 </sub>
</italic>: the sequential term in <xref ref-type="disp-formula" rid="equ3">Equation 2</xref> = 0, p&lt;0.05<disp-formula id="equ3">
<label>(2)</label>
<mml:math id="m3">
<mml:msub>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>g</mml:mi>
<mml:mi>h</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>w</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>r</mml:mi>
<mml:mi>d</mml:mi> <mml:mi/>
<mml:mi>c</mml:mi>
<mml:mi>h</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>c</mml:mi>
<mml:mi>e</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi>λ</mml:mi>
<mml:mo>+</mml:mo>
<mml:mfenced separators="|">
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo>-</mml:mo>
<mml:mn>2</mml:mn> <mml:mi/>
<mml:mi>λ</mml:mi>
</mml:mrow>
</mml:mfenced> <mml:mi/>
<mml:mo>×</mml:mo> <mml:mi/>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo>+</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>e</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mi>S</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>n</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>v</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>y</mml:mi>
<mml:mo>(</mml:mo>
<mml:mi>C</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>h</mml:mi>
<mml:mo>-</mml:mo>
<mml:mo>(</mml:mo>
<mml:mi>B</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>s</mml:mi>
<mml:mo>+</mml:mo>
<mml:mi>B</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>a</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>s</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>q</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:mfrac>
</mml:math>
</disp-formula>
</p>
<p>
<italic>Bias<sub>seq</sub>
</italic> was determined using indicator variables for the given sequential effect and the reward context (e.g., LR-Right context, previous correct LR choice): <italic>I<sub>seq</sub>
</italic> <inline-formula>
<mml:math id="inf5">
<mml:mo>×</mml:mo>
</mml:math>
</inline-formula> <italic>I<sub>rew </sub>
</italic> <inline-formula>
<mml:math id="inf6">
<mml:mo>×</mml:mo>
</mml:math>
</inline-formula> 𝞪<sub>seq</sub>, where</p>
<p>
<italic>I<sub>rew</sub>
</italic> =±1 for LR-Right/LR Left reward contexts.</p>
<p>
<italic>I<sub>seq</sub>
</italic> = <italic>I<sub>prevLR-prevCorrect</sub>, I<sub>prevLR-prevError</sub>, I<sub>prevSR-prevCorrect</sub>,</italic> and <italic> I<sub>prevSR-prevError</sub>
</italic> for the 4 types of sequential effects (note that there were not enough trials to compute previous error SR choice).</p>
</sec>
<sec id="s4-6">
<title>Optimality analysis</title>
<p>To examine the level of optimality of the monkeys’ performance, we focused on two reward functions: reward rate (RR, defined as the average reward per second) and reward per trial (RTrial, defined as the average reward per trial) for a given reward context for each session. To estimate the reward functions in relation to <italic>me</italic> and <italic>z</italic> adjustments for a given reward context, we numerically obtained choice and RT values for different combinations of <italic>z</italic> (ranging from 0 to 1) and <italic>me</italic> (ranging from −0.6 to 0.6 coherence unless otherwise specified), given <italic>a, k,</italic> and non-decision time values fitted by the full model. We then calculated RR and RTrial, using trial-matched parameters, including the actual ITI, timeout, and large:small reward ratio. RR<sub>max</sub> and RTrial<sub>max</sub> were identified as the maximal values given the sampled <italic>me-z</italic> combinations, using 1000 trials for each coherence ×direction condition. Optimal <italic>me</italic> and <italic>z</italic> adjustments were defined as the <italic>me</italic> and <italic>z</italic> values corresponding to RR<sub>max</sub> or RTrial<sub>max</sub>. RR<sub>predict</sub> and RTrial<sub>predict</sub> were calculated with the fitted <italic>me</italic> and <italic>z</italic> values in the full model.</p>
</sec>
</sec>
</body>
<back>
<ack id="ack">
<title>Acknowledgments</title>
<p>We thank Takahiro Doi for helpful comments, Javier Caballero and Rachel Gates for animal training, Jean Zweigle for animal care, and Michael Yoder for data entry.</p>
</ack>
<sec sec-type="additional-information" id="s5">
<title>Additional information</title>
<fn-group content-type="competing-interest">
<title>Competing interests</title>
<fn fn-type="COI-statement" id="conf2">
<p>Reviewing editor, <italic>eLife</italic>
</p>
</fn>
<fn fn-type="COI-statement" id="conf1">
<p>No competing interests declared</p>
</fn>
</fn-group>
<fn-group content-type="author-contribution">
<title>Author contributions</title>
<fn fn-type="con" id="con1">
<p>Data curation, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p>
</fn>
<fn fn-type="con" id="con2">
<p>Conceptualization, Resources, Software, Formal analysis, Supervision, Funding acquisition, Visualization, Methodology, Writing—review and editing</p>
</fn>
<fn fn-type="con" id="con3">
<p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p>
</fn>
</fn-group>
<fn-group content-type="ethics-information">
<title>Ethics</title>
<fn fn-type="other">
<p>Animal experimentation: All training and experimental procedures were in accordance with the National Institutes of Health Guide for the Care and Use of Laboratory Animals and were approved by the University of Pennsylvania Institutional Animal Care and Use Committee (#804726).</p>
</fn>
</fn-group>
</sec>
<sec sec-type="supplementary-material" id="s6">
<title>Additional files</title>
<supplementary-material id="transrepform">
<object-id pub-id-type="doi">10.7554/eLife.36018.033</object-id>
<label>Transparent reporting form</label>
<media xlink:href="elife-36018-transrepform-v3.docx" mimetype="application" mime-subtype="docx"/>
</supplementary-material>
<sec sec-type="data-availability" id="s7">
<title>Data availability</title>
<p>Raw data used during this study are included as the supporting files.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="bib1">
<element-citation publication-type="preprint">
<person-group person-group-type="author">
<name>
<surname>Aitchison</surname> <given-names>L</given-names>
</name>
<name>
<surname>Pouget</surname> <given-names>A</given-names>
</name>
<name>
<surname>Latham</surname> <given-names>P</given-names>
</name>
</person-group>
<year iso-8601-date="2017">2017</year>
<article-title>Probabilistic synapses</article-title>
<source>arXiv</source>
<ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1410.1029">https://arxiv.org/abs/1410.1029</ext-link>
</element-citation>
</ref>
<ref id="bib2">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ashby</surname> <given-names>FG</given-names>
</name>
</person-group>
<year iso-8601-date="1983">1983</year>
<article-title>A biased random walk model for two choice reaction times</article-title>
<source>Journal of Mathematical Psychology</source>
<volume>27</volume>
<fpage>277</fpage>
<lpage>297</lpage>
<pub-id pub-id-type="doi">10.1016/0022-2496(83)90011-1</pub-id>
</element-citation>
</ref>
<ref id="bib3">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Barnard</surname> <given-names>GA</given-names>
</name>
</person-group>
<year iso-8601-date="1946">1946</year>
<article-title>Sequential tests in industrial statistics</article-title>
<source>Supplement to the Journal of the Royal Statistical Society</source>
<volume>8</volume>
<fpage>1</fpage>
<lpage>26</lpage>
<pub-id pub-id-type="doi">10.2307/2983610</pub-id>
</element-citation>
</ref>
<ref id="bib4">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bennur</surname> <given-names>S</given-names>
</name>
<name>
<surname>Gold</surname> <given-names>JI</given-names>
</name>
</person-group>
<year iso-8601-date="2011">2011</year>
<article-title>Distinct representations of a perceptual decision and the associated oculomotor plan in the monkey lateral intraparietal area</article-title>
<source>Journal of Neuroscience</source>
<volume>31</volume>
<fpage>913</fpage>
<lpage>921</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.4417-10.2011</pub-id>
<pub-id pub-id-type="pmid">21248116</pub-id>
</element-citation>
</ref>
<ref id="bib5">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Blank</surname> <given-names>H</given-names>
</name>
<name>
<surname>Biele</surname> <given-names>G</given-names>
</name>
<name>
<surname>Heekeren</surname> <given-names>HR</given-names>
</name>
<name>
<surname>Philiastides</surname> <given-names>MG</given-names>
</name>
</person-group>
<year iso-8601-date="2013">2013</year>
<article-title>Temporal characteristics of the influence of punishment on perceptual decision making in the human brain</article-title>
<source>Journal of Neuroscience</source>
<volume>33</volume>
<fpage>3939</fpage>
<lpage>3952</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.4151-12.2013</pub-id>
<pub-id pub-id-type="pmid">23447604</pub-id>
</element-citation>
</ref>
<ref id="bib6">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bogacz</surname> <given-names>R</given-names>
</name>
<name>
<surname>Brown</surname> <given-names>E</given-names>
</name>
<name>
<surname>Moehlis</surname> <given-names>J</given-names>
</name>
<name>
<surname>Holmes</surname> <given-names>P</given-names>
</name>
<name>
<surname>Cohen</surname> <given-names>JD</given-names>
</name>
</person-group>
<year iso-8601-date="2006">2006</year>
<article-title>The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks. psychol rev 113:700-765.churchland AK, kiani R, shadlen MN (2008) Decision-making with multiple alternatives</article-title>
<source>Nature Neuroscience</source>
<volume>11</volume>
<fpage>693</fpage>
<lpage>702</lpage>
<pub-id pub-id-type="doi">10.1037/0033-295X.113.4.700</pub-id>
</element-citation>
</ref>
<ref id="bib7">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Churchland</surname> <given-names>AK</given-names>
</name>
<name>
<surname>Kiani</surname> <given-names>R</given-names>
</name>
<name>
<surname>Chaudhuri</surname> <given-names>R</given-names>
</name>
<name>
<surname>Wang</surname> <given-names>XJ</given-names>
</name>
<name>
<surname>Pouget</surname> <given-names>A</given-names>
</name>
<name>
<surname>Shadlen</surname> <given-names>MN</given-names>
</name>
</person-group>
<year iso-8601-date="2011">2011</year>
<article-title>Variance as a signature of neural computations during decision making</article-title>
<source>Neuron</source>
<volume>69</volume>
<fpage>818</fpage>
<lpage>831</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuron.2010.12.037</pub-id>
<pub-id pub-id-type="pmid">21338889</pub-id>
</element-citation>
</ref>
<ref id="bib8">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cicmil</surname> <given-names>N</given-names>
</name>
<name>
<surname>Cumming</surname> <given-names>BG</given-names>
</name>
<name>
<surname>Parker</surname> <given-names>AJ</given-names>
</name>
<name>
<surname>Krug</surname> <given-names>K</given-names>
</name>
</person-group>
<year iso-8601-date="2015">2015</year>
<article-title>Reward modulates the effect of visual cortical microstimulation on perceptual decisions</article-title>
<source>eLife</source>
<volume>4</volume>
<elocation-id>e07832</elocation-id>
<pub-id pub-id-type="doi">10.7554/eLife.07832</pub-id>
<pub-id pub-id-type="pmid">26402458</pub-id>
</element-citation>
</ref>
<ref id="bib9">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Connolly</surname> <given-names>PM</given-names>
</name>
<name>
<surname>Bennur</surname> <given-names>S</given-names>
</name>
<name>
<surname>Gold</surname> <given-names>JI</given-names>
</name>
</person-group>
<year iso-8601-date="2009">2009</year>
<article-title>Correlates of perceptual learning in an oculomotor decision variable</article-title>
<source>Journal of Neuroscience</source>
<volume>29</volume>
<fpage>2136</fpage>
<lpage>2150</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3962-08.2009</pub-id>
<pub-id pub-id-type="pmid">19228966</pub-id>
</element-citation>
</ref>
<ref id="bib10">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Diederich</surname> <given-names>A</given-names>
</name>
<name>
<surname>Busemeyer</surname> <given-names>JR</given-names>
</name>
</person-group>
<year iso-8601-date="2006">2006</year>
<article-title>Modeling the effects of payoff on response bias in a perceptual discrimination task: bound-change, drift-rate-change, or two-stage-processing hypothesis</article-title>
<source>Perception &amp; Psychophysics</source>
<volume>68</volume>
<fpage>194</fpage>
<lpage>207</lpage>
<pub-id pub-id-type="doi">10.3758/BF03193669</pub-id>
<pub-id pub-id-type="pmid">16773893</pub-id>
</element-citation>
</ref>
<ref id="bib11">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ding</surname> <given-names>L</given-names>
</name>
<name>
<surname>Gold</surname> <given-names>JI</given-names>
</name>
</person-group>
<year iso-8601-date="2010">2010</year>
<article-title>Caudate encodes multiple computations for perceptual decisions</article-title>
<source>Journal of Neuroscience</source>
<volume>30</volume>
<fpage>15747</fpage>
<lpage>15759</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.2894-10.2010</pub-id>
<pub-id pub-id-type="pmid">21106814</pub-id>
</element-citation>
</ref>
<ref id="bib12">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ding</surname> <given-names>L</given-names>
</name>
<name>
<surname>Gold</surname> <given-names>JI</given-names>
</name>
</person-group>
<year iso-8601-date="2012">2012a</year>
<article-title>Neural correlates of perceptual decision making before, during, and after decision commitment in monkey frontal eye field</article-title>
<source>Cerebral Cortex</source>
<volume>22</volume>
<fpage>1052</fpage>
<lpage>1067</lpage>
<pub-id pub-id-type="doi">10.1093/cercor/bhr178</pub-id>
<pub-id pub-id-type="pmid">21765183</pub-id>
</element-citation>
</ref>
<ref id="bib13">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ding</surname> <given-names>L</given-names>
</name>
<name>
<surname>Gold</surname> <given-names>JI</given-names>
</name>
</person-group>
<year iso-8601-date="2012">2012b</year>
<article-title>Separate, causal roles of the caudate in Saccadic choice and execution in a perceptual decision task</article-title>
<source>Neuron</source>
<volume>75</volume>
<fpage>865</fpage>
<lpage>874</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuron.2012.07.021</pub-id>
<pub-id pub-id-type="pmid">22958826</pub-id>
</element-citation>
</ref>
<ref id="bib14">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ding</surname> <given-names>L</given-names>
</name>
</person-group>
<year iso-8601-date="2015">2015</year>
<article-title>Distinct dynamics of ramping activity in the frontal cortex and caudate nucleus in monkeys</article-title>
<source>Journal of Neurophysiology</source>
<volume>114</volume>
<fpage>1850</fpage>
<lpage>1861</lpage>
<pub-id pub-id-type="doi">10.1152/jn.00395.2015</pub-id>
<pub-id pub-id-type="pmid">26224774</pub-id>
</element-citation>
</ref>
<ref id="bib15">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Drugowitsch</surname> <given-names>J</given-names>
</name>
<name>
<surname>Moreno-Bote</surname> <given-names>R</given-names>
</name>
<name>
<surname>Churchland</surname> <given-names>AK</given-names>
</name>
<name>
<surname>Shadlen</surname> <given-names>MN</given-names>
</name>
<name>
<surname>Pouget</surname> <given-names>A</given-names>
</name>
</person-group>
<year iso-8601-date="2012">2012</year>
<article-title>The cost of accumulating evidence in perceptual decision making</article-title>
<source>Journal of Neuroscience</source>
<volume>32</volume>
<fpage>3612</fpage>
<lpage>3628</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.4010-11.2012</pub-id>
<pub-id pub-id-type="pmid">22423085</pub-id>
</element-citation>
</ref>
<ref id="bib16">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Edwards</surname> <given-names>W</given-names>
</name>
</person-group>
<year iso-8601-date="1965">1965</year>
<article-title>Optimal strategies for seeking information: Models for statistics, choice reaction times, and human information processing</article-title>
<source>Journal of Mathematical Psychology</source>
<volume>2</volume>
<fpage>312</fpage>
<lpage>329</lpage>
<pub-id pub-id-type="doi">10.1016/0022-2496(65)90007-6</pub-id>
</element-citation>
</ref>
<ref id="bib17">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Feng</surname> <given-names>S</given-names>
</name>
<name>
<surname>Holmes</surname> <given-names>P</given-names>
</name>
<name>
<surname>Rorie</surname> <given-names>A</given-names>
</name>
<name>
<surname>Newsome</surname> <given-names>WT</given-names>
</name>
</person-group>
<year iso-8601-date="2009">2009</year>
<article-title>Can monkeys choose optimally when faced with noisy stimuli and unequal rewards?</article-title>
<source>PLoS Computational Biology</source>
<volume>5</volume>
<elocation-id>e1000284</elocation-id>
<pub-id pub-id-type="doi">10.1371/journal.pcbi.1000284</pub-id>
<pub-id pub-id-type="pmid">19214201</pub-id>
</element-citation>
</ref>
<ref id="bib18">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Frank</surname> <given-names>MJ</given-names>
</name>
<name>
<surname>Gagne</surname> <given-names>C</given-names>
</name>
<name>
<surname>Nyhus</surname> <given-names>E</given-names>
</name>
<name>
<surname>Masters</surname> <given-names>S</given-names>
</name>
<name>
<surname>Wiecki</surname> <given-names>TV</given-names>
</name>
<name>
<surname>Cavanagh</surname> <given-names>JF</given-names>
</name>
<name>
<surname>Badre</surname> <given-names>D</given-names>
</name>
</person-group>
<year iso-8601-date="2015">2015</year>
<article-title>fMRI and EEG predictors of dynamic decision parameters during human reinforcement learning</article-title>
<source>Journal of Neuroscience</source>
<volume>35</volume>
<fpage>485</fpage>
<lpage>494</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.2036-14.2015</pub-id>
<pub-id pub-id-type="pmid">25589744</pub-id>
</element-citation>
</ref>
<ref id="bib19">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gao</surname> <given-names>J</given-names>
</name>
<name>
<surname>Tortell</surname> <given-names>R</given-names>
</name>
<name>
<surname>McClelland</surname> <given-names>JL</given-names>
</name>
</person-group>
<year iso-8601-date="2011">2011</year>
<article-title>Dynamic integration of reward and stimulus information in perceptual decision-making</article-title>
<source>PLoS ONE</source>
<volume>6</volume>
<elocation-id>e16749</elocation-id>
<pub-id pub-id-type="doi">10.1371/journal.pone.0016749</pub-id>
<pub-id pub-id-type="pmid">21390225</pub-id>
</element-citation>
</ref>
<ref id="bib20">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gegenfurtner</surname> <given-names>KR</given-names>
</name>
<name>
<surname>Hawken</surname> <given-names>MJ</given-names>
</name>
</person-group>
<year iso-8601-date="1996">1996</year>
<article-title>Interaction of motion and color in the visual pathways</article-title>
<source>Trends in Neurosciences</source>
<volume>19</volume>
<fpage>394</fpage>
<lpage>401</lpage>
<pub-id pub-id-type="doi">10.1016/S0166-2236(96)10036-9</pub-id>
<pub-id pub-id-type="pmid">8873357</pub-id>
</element-citation>
</ref>
<ref id="bib21">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gigerenzer</surname> <given-names>G</given-names>
</name>
<name>
<surname>Gaissmaier</surname> <given-names>W</given-names>
</name>
</person-group>
<year iso-8601-date="2011">2011</year>
<article-title>Heuristic decision making</article-title>
<source>Annual Review of Psychology</source>
<volume>62</volume>
<fpage>451</fpage>
<lpage>482</lpage>
<pub-id pub-id-type="doi">10.1146/annurev-psych-120709-145346</pub-id>
<pub-id pub-id-type="pmid">21126183</pub-id>
</element-citation>
</ref>
<ref id="bib22">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gigerenzer</surname> <given-names>G</given-names>
</name>
<name>
<surname>Goldstein</surname> <given-names>DG</given-names>
</name>
</person-group>
<year iso-8601-date="1996">1996</year>
<article-title>Reasoning the fast and frugal way: models of bounded rationality</article-title>
<source>Psychological Review</source>
<volume>103</volume>
<fpage>650</fpage>
<lpage>669</lpage>
<pub-id pub-id-type="doi">10.1037/0033-295X.103.4.650</pub-id>
<pub-id pub-id-type="pmid">8888650</pub-id>
</element-citation>
</ref>
<ref id="bib23">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gigerenzer</surname> <given-names>G</given-names>
</name>
</person-group>
<year iso-8601-date="2010">2010</year>
<article-title>Moral satisficing: rethinking moral behavior as bounded rationality</article-title>
<source>Topics in Cognitive Science</source>
<volume>2</volume>
<fpage>528</fpage>
<lpage>554</lpage>
<pub-id pub-id-type="doi">10.1111/j.1756-8765.2010.01094.x</pub-id>
<pub-id pub-id-type="pmid">25163875</pub-id>
</element-citation>
</ref>
<ref id="bib24">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gold</surname> <given-names>JI</given-names>
</name>
<name>
<surname>Law</surname> <given-names>CT</given-names>
</name>
<name>
<surname>Connolly</surname> <given-names>P</given-names>
</name>
<name>
<surname>Bennur</surname> <given-names>S</given-names>
</name>
</person-group>
<year iso-8601-date="2008">2008</year>
<article-title>The relative influences of priors and sensory evidence on an oculomotor decision variable during perceptual learning</article-title>
<source>Journal of Neurophysiology</source>
<volume>100</volume>
<fpage>2653</fpage>
<lpage>2668</lpage>
<pub-id pub-id-type="doi">10.1152/jn.90629.2008</pub-id>
<pub-id pub-id-type="pmid">18753326</pub-id>
</element-citation>
</ref>
<ref id="bib25">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gold</surname> <given-names>JI</given-names>
</name>
<name>
<surname>Shadlen</surname> <given-names>MN</given-names>
</name>
</person-group>
<year iso-8601-date="2002">2002</year>
<article-title>Banburismus and the brain: decoding the relationship between sensory stimuli, decisions, and reward</article-title>
<source>Neuron</source>
<volume>36</volume>
<fpage>299</fpage>
<lpage>308</lpage>
<pub-id pub-id-type="doi">10.1016/S0896-6273(02)00971-6</pub-id>
<pub-id pub-id-type="pmid">12383783</pub-id>
</element-citation>
</ref>
<ref id="bib26">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gold</surname> <given-names>JI</given-names>
</name>
<name>
<surname>Shadlen</surname> <given-names>MN</given-names>
</name>
</person-group>
<year iso-8601-date="2007">2007</year>
<article-title>The neural basis of decision making</article-title>
<source>Annual Review of Neuroscience</source>
<volume>30</volume>
<fpage>535</fpage>
<lpage>574</lpage>
<pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id>
<pub-id pub-id-type="pmid">17600525</pub-id>
</element-citation>
</ref>
<ref id="bib27">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Goldfarb</surname> <given-names>S</given-names>
</name>
<name>
<surname>Leonard</surname> <given-names>NE</given-names>
</name>
<name>
<surname>Simen</surname> <given-names>P</given-names>
</name>
<name>
<surname>Caicedo-Núñez</surname> <given-names>CH</given-names>
</name>
<name>
<surname>Holmes</surname> <given-names>P</given-names>
</name>
</person-group>
<year iso-8601-date="2014">2014</year>
<article-title>A comparative study of drift diffusion and linear ballistic accumulator models in a reward maximization perceptual choice task</article-title>
<source>Frontiers in Neuroscience</source>
<volume>8</volume>
<elocation-id>148</elocation-id>
<pub-id pub-id-type="doi">10.3389/fnins.2014.00148</pub-id>
<pub-id pub-id-type="pmid">25140124</pub-id>
</element-citation>
</ref>
<ref id="bib28">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Goldstein</surname> <given-names>DG</given-names>
</name>
<name>
<surname>Gigerenzer</surname> <given-names>G</given-names>
</name>
</person-group>
<year iso-8601-date="2002">2002</year>
<article-title>Models of ecological rationality: the recognition heuristic</article-title>
<source>Psychological Review</source>
<volume>109</volume>
<fpage>75</fpage>
<lpage>90</lpage>
<pub-id pub-id-type="doi">10.1037/0033-295X.109.1.75</pub-id>
<pub-id pub-id-type="pmid">11863042</pub-id>
</element-citation>
</ref>
<ref id="bib29">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Goodrich</surname> <given-names>MA</given-names>
</name>
<name>
<surname>Stirling</surname> <given-names>WC</given-names>
</name>
<name>
<surname>Frost</surname> <given-names>RL</given-names>
</name>
</person-group>
<year iso-8601-date="1998">1998</year>
<article-title>A theory of satisficing decisions and control</article-title>
<source>IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans</source>
<volume>28</volume>
<fpage>763</fpage>
<lpage>779</lpage>
<pub-id pub-id-type="doi">10.1109/3468.725348</pub-id>
</element-citation>
</ref>
<ref id="bib30">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hanks</surname> <given-names>TD</given-names>
</name>
<name>
<surname>Ditterich</surname> <given-names>J</given-names>
</name>
<name>
<surname>Shadlen</surname> <given-names>MN</given-names>
</name>
</person-group>
<year iso-8601-date="2006">2006</year>
<article-title>Microstimulation of macaque area LIP affects decision-making in a motion discrimination task</article-title>
<source>Nature Neuroscience</source>
<volume>9</volume>
<fpage>682</fpage>
<lpage>689</lpage>
<pub-id pub-id-type="doi">10.1038/nn1683</pub-id>
<pub-id pub-id-type="pmid">16604069</pub-id>
</element-citation>
</ref>
<ref id="bib31">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hanks</surname> <given-names>TD</given-names>
</name>
<name>
<surname>Mazurek</surname> <given-names>ME</given-names>
</name>
<name>
<surname>Kiani</surname> <given-names>R</given-names>
</name>
<name>
<surname>Hopp</surname> <given-names>E</given-names>
</name>
<name>
<surname>Shadlen</surname> <given-names>MN</given-names>
</name>
</person-group>
<year iso-8601-date="2011">2011</year>
<article-title>Elapsed decision time affects the weighting of prior probability in a perceptual decision task</article-title>
<source>Journal of Neuroscience</source>
<volume>31</volume>
<fpage>6339</fpage>
<lpage>6352</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.5613-10.2011</pub-id>
<pub-id pub-id-type="pmid">21525274</pub-id>
</element-citation>
</ref>
<ref id="bib32">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Horwitz</surname> <given-names>GD</given-names>
</name>
<name>
<surname>Newsome</surname> <given-names>WT</given-names>
</name>
</person-group>
<year iso-8601-date="2001">2001</year>
<article-title>Target selection for saccadic eye movements: direction-selective visual responses in the superior colliculus</article-title>
<source>Journal of Neurophysiology</source>
<volume>86</volume>
<fpage>2527</fpage>
<lpage>2542</lpage>
<pub-id pub-id-type="doi">10.1152/jn.2001.86.5.2527</pub-id>
<pub-id pub-id-type="pmid">11698540</pub-id>
</element-citation>
</ref>
<ref id="bib33">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jones</surname> <given-names>PR</given-names>
</name>
<name>
<surname>Moore</surname> <given-names>DR</given-names>
</name>
<name>
<surname>Shub</surname> <given-names>DE</given-names>
</name>
<name>
<surname>Amitay</surname> <given-names>S</given-names>
</name>
</person-group>
<year iso-8601-date="2015">2015</year>
<article-title>The role of response bias in perceptual learning</article-title>
<source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>
<volume>41</volume>
<fpage>1456</fpage>
<lpage>1470</lpage>
<pub-id pub-id-type="doi">10.1037/xlm0000111</pub-id>
<pub-id pub-id-type="pmid">25867609</pub-id>
</element-citation>
</ref>
<ref id="bib34">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kiani</surname> <given-names>R</given-names>
</name>
<name>
<surname>Shadlen</surname> <given-names>MN</given-names>
</name>
</person-group>
<year iso-8601-date="2009">2009</year>
<article-title>Representation of confidence associated with a decision by neurons in the parietal cortex</article-title>
<source>Science</source>
<volume>324</volume>
<fpage>759</fpage>
<lpage>764</lpage>
<pub-id pub-id-type="doi">10.1126/science.1169405</pub-id>
<pub-id pub-id-type="pmid">19423820</pub-id>
</element-citation>
</ref>
<ref id="bib35">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kirkpatrick</surname> <given-names>J</given-names>
</name>
<name>
<surname>Pascanu</surname> <given-names>R</given-names>
</name>
<name>
<surname>Rabinowitz</surname> <given-names>N</given-names>
</name>
<name>
<surname>Veness</surname> <given-names>J</given-names>
</name>
<name>
<surname>Desjardins</surname> <given-names>G</given-names>
</name>
<name>
<surname>Rusu</surname> <given-names>AA</given-names>
</name>
<name>
<surname>Milan</surname> <given-names>K</given-names>
</name>
<name>
<surname>Quan</surname> <given-names>J</given-names>
</name>
<name>
<surname>Ramalho</surname> <given-names>T</given-names>
</name>
<name>
<surname>Grabska-Barwinska</surname> <given-names>A</given-names>
</name>
<name>
<surname>Hassabis</surname> <given-names>D</given-names>
</name>
<name>
<surname>Clopath</surname> <given-names>C</given-names>
</name>
<name>
<surname>Kumaran</surname> <given-names>D</given-names>
</name>
<name>
<surname>Hadsell</surname> <given-names>R</given-names>
</name>
</person-group>
<year iso-8601-date="2017">2017</year>
<article-title>Overcoming catastrophic forgetting in neural networks</article-title>
<source>PNAS</source>
<volume>114</volume>
<fpage>3521</fpage>
<lpage>3526</lpage>
<pub-id pub-id-type="doi">10.1073/pnas.1611835114</pub-id>
<pub-id pub-id-type="pmid">28292907</pub-id>
</element-citation>
</ref>
<ref id="bib36">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Klein</surname> <given-names>SA</given-names>
</name>
</person-group>
<year iso-8601-date="2001">2001</year>
<article-title>Measuring, estimating, and understanding the psychometric function: a commentary</article-title>
<source>Perception &amp; Psychophysics</source>
<volume>63</volume>
<fpage>1421</fpage>
<lpage>1455</lpage>
<pub-id pub-id-type="doi">10.3758/BF03194552</pub-id>
<pub-id pub-id-type="pmid">11800466</pub-id>
</element-citation>
</ref>
<ref id="bib37">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Krajbich</surname> <given-names>I</given-names>
</name>
<name>
<surname>Armel</surname> <given-names>C</given-names>
</name>
<name>
<surname>Rangel</surname> <given-names>A</given-names>
</name>
</person-group>
<year iso-8601-date="2010">2010</year>
<article-title>Visual fixations and the computation and comparison of value in simple choice</article-title>
<source>Nature Neuroscience</source>
<volume>13</volume>
<fpage>1292</fpage>
<lpage>1298</lpage>
<pub-id pub-id-type="doi">10.1038/nn.2635</pub-id>
<pub-id pub-id-type="pmid">20835253</pub-id>
</element-citation>
</ref>
<ref id="bib38">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Latimer</surname> <given-names>KW</given-names>
</name>
<name>
<surname>Yates</surname> <given-names>JL</given-names>
</name>
<name>
<surname>Meister</surname> <given-names>ML</given-names>
</name>
<name>
<surname>Huk</surname> <given-names>AC</given-names>
</name>
<name>
<surname>Pillow</surname> <given-names>JW</given-names>
</name>
</person-group>
<year iso-8601-date="2015">2015</year>
<article-title>NEURONAL MODELING. Single-trial spike trains in parietal cortex reveal discrete steps during decision-making</article-title>
<source>Science</source>
<volume>349</volume>
<fpage>184</fpage>
<lpage>187</lpage>
<pub-id pub-id-type="doi">10.1126/science.aaa4056</pub-id>
<pub-id pub-id-type="pmid">26160947</pub-id>
</element-citation>
</ref>
<ref id="bib39">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Leite</surname> <given-names>FP</given-names>
</name>
<name>
<surname>Ratcliff</surname> <given-names>R</given-names>
</name>
</person-group>
<year iso-8601-date="2011">2011</year>
<article-title>What cognitive processes drive response biases? A diffusion model analysis</article-title>
<source>Judgment and Decision Making</source>
<volume>6</volume>
<fpage>651</fpage>
<lpage>687</lpage>
<pub-id pub-id-type="doi">10.1037/t42052-000</pub-id>
</element-citation>
</ref>
<ref id="bib40">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Liston</surname> <given-names>DB</given-names>
</name>
<name>
<surname>Stone</surname> <given-names>LS</given-names>
</name>
</person-group>
<year iso-8601-date="2008">2008</year>
<article-title>Effects of prior information and reward on oculomotor and perceptual choices</article-title>
<source>Journal of Neuroscience</source>
<volume>28</volume>
<fpage>13866</fpage>
<lpage>13875</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3120-08.2008</pub-id>
<pub-id pub-id-type="pmid">19091976</pub-id>
</element-citation>
</ref>
<ref id="bib41">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lo</surname> <given-names>CC</given-names>
</name>
<name>
<surname>Wang</surname> <given-names>CT</given-names>
</name>
<name>
<surname>Wang</surname> <given-names>XJ</given-names>
</name>
</person-group>
<year iso-8601-date="2015">2015</year>
<article-title>Speed-accuracy tradeoff by a control signal with balanced excitation and inhibition</article-title>
<source>Journal of Neurophysiology</source>
<volume>114</volume>
<fpage>650</fpage>
<lpage>661</lpage>
<pub-id pub-id-type="doi">10.1152/jn.00845.2013</pub-id>
<pub-id pub-id-type="pmid">25995354</pub-id>
</element-citation>
</ref>
<ref id="bib42">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lo</surname> <given-names>CC</given-names>
</name>
<name>
<surname>Wang</surname> <given-names>XJ</given-names>
</name>
</person-group>
<year iso-8601-date="2006">2006</year>
<article-title>Cortico-basal ganglia circuit mechanism for a decision threshold in reaction time tasks</article-title>
<source>Nature Neuroscience</source>
<volume>9</volume>
<fpage>956</fpage>
<lpage>963</lpage>
<pub-id pub-id-type="doi">10.1038/nn1722</pub-id>
<pub-id pub-id-type="pmid">16767089</pub-id>
</element-citation>
</ref>
<ref id="bib43">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Maddox</surname> <given-names>WT</given-names>
</name>
<name>
<surname>Bohil</surname> <given-names>CJ</given-names>
</name>
</person-group>
<year iso-8601-date="1998">1998</year>
<article-title>Base-rate and payoff effects in multidimensional perceptual categorization</article-title>
<source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>
<volume>24</volume>
<fpage>1459</fpage>
<lpage>1482</lpage>
<pub-id pub-id-type="doi">10.1037/0278-7393.24.6.1459</pub-id>
<pub-id pub-id-type="pmid">9835061</pub-id>
</element-citation>
</ref>
<ref id="bib44">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Matzke</surname> <given-names>D</given-names>
</name>
<name>
<surname>Wagenmakers</surname> <given-names>EJ</given-names>
</name>
</person-group>
<year iso-8601-date="2009">2009</year>
<article-title>Psychological interpretation of the ex-Gaussian and shifted Wald parameters: a diffusion model analysis</article-title>
<source>Psychonomic Bulletin &amp; Review</source>
<volume>16</volume>
<fpage>798</fpage>
<lpage>817</lpage>
<pub-id pub-id-type="doi">10.3758/PBR.16.5.798</pub-id>
<pub-id pub-id-type="pmid">19815782</pub-id>
</element-citation>
</ref>
<ref id="bib45">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Milosavljevic</surname> <given-names>M</given-names>
</name>
<name>
<surname>Malmaud</surname> <given-names>J</given-names>
</name>
<name>
<surname>Huth</surname> <given-names>A</given-names>
</name>
<name>
<surname>Koch</surname> <given-names>C</given-names>
</name>
<name>
<surname>Rangel</surname> <given-names>A</given-names>
</name>
</person-group>
<year iso-8601-date="2001">2001</year>
<article-title>The drift diffusion model can account for the accuracy and reaction times of value-based choice under high and low time pressure</article-title>
<source>Judgment and Decision Making</source>
<volume>5</volume>
<fpage>437</fpage>
<lpage>449</lpage>
<pub-id pub-id-type="doi">10.2139/ssrn.1901533</pub-id>
</element-citation>
</ref>
<ref id="bib46">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mulder</surname> <given-names>MJ</given-names>
</name>
<name>
<surname>Wagenmakers</surname> <given-names>EJ</given-names>
</name>
<name>
<surname>Ratcliff</surname> <given-names>R</given-names>
</name>
<name>
<surname>Boekel</surname> <given-names>W</given-names>
</name>
<name>
<surname>Forstmann</surname> <given-names>BU</given-names>
</name>
</person-group>
<year iso-8601-date="2012">2012</year>
<article-title>Bias in the brain: a diffusion model analysis of prior probability and potential payoff</article-title>
<source>Journal of Neuroscience</source>
<volume>32</volume>
<fpage>2335</fpage>
<lpage>2343</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.4156-11.2012</pub-id>
<pub-id pub-id-type="pmid">22396408</pub-id>
</element-citation>
</ref>
<ref id="bib47">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Newsome</surname> <given-names>WT</given-names>
</name>
<name>
<surname>Britten</surname> <given-names>KH</given-names>
</name>
<name>
<surname>Movshon</surname> <given-names>JA</given-names>
</name>
</person-group>
<year iso-8601-date="1989">1989</year>
<article-title>Neuronal correlates of a perceptual decision</article-title>
<source>Nature</source>
<volume>341</volume>
<fpage>52</fpage>
<lpage>54</lpage>
<pub-id pub-id-type="doi">10.1038/341052a0</pub-id>
<pub-id pub-id-type="pmid">2770878</pub-id>
</element-citation>
</ref>
<ref id="bib48">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nikolaev</surname> <given-names>AR</given-names>
</name>
<name>
<surname>Gepshtein</surname> <given-names>S</given-names>
</name>
<name>
<surname>van Leeuwen</surname> <given-names>C</given-names>
</name>
</person-group>
<year iso-8601-date="2016">2016</year>
<article-title>Intermittent regime of brain activity at the early, bias-guided stage of perceptual learning</article-title>
<source>Journal of Vision</source>
<volume>16</volume>
<elocation-id>11</elocation-id>
<pub-id pub-id-type="doi">10.1167/16.14.11</pub-id>
<pub-id pub-id-type="pmid">27846639</pub-id>
</element-citation>
</ref>
<ref id="bib49">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nomoto</surname> <given-names>K</given-names>
</name>
<name>
<surname>Schultz</surname> <given-names>W</given-names>
</name>
<name>
<surname>Watanabe</surname> <given-names>T</given-names>
</name>
<name>
<surname>Sakagami</surname> <given-names>M</given-names>
</name>
</person-group>
<year iso-8601-date="2010">2010</year>
<article-title>Temporally extended dopamine responses to perceptually demanding reward-predictive stimuli</article-title>
<source>Journal of Neuroscience</source>
<volume>30</volume>
<fpage>10692</fpage>
<lpage>10702</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.4828-09.2010</pub-id>
<pub-id pub-id-type="pmid">20702700</pub-id>
</element-citation>
</ref>
<ref id="bib50">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nosofsky</surname> <given-names>RM</given-names>
</name>
<name>
<surname>Palmeri</surname> <given-names>TJ</given-names>
</name>
</person-group>
<year iso-8601-date="1997">1997</year>
<article-title>An exemplar-based random walk model of speeded classification</article-title>
<source>Psychological Review</source>
<volume>104</volume>
<fpage>266</fpage>
<lpage>300</lpage>
<pub-id pub-id-type="doi">10.1037/0033-295X.104.2.266</pub-id>
<pub-id pub-id-type="pmid">9127583</pub-id>
</element-citation>
</ref>
<ref id="bib51">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Oh</surname> <given-names>H</given-names>
</name>
<name>
<surname>Beck</surname> <given-names>JM</given-names>
</name>
<name>
<surname>Zhu</surname> <given-names>P</given-names>
</name>
<name>
<surname>Sommer</surname> <given-names>MA</given-names>
</name>
<name>
<surname>Ferrari</surname> <given-names>S</given-names>
</name>
<name>
<surname>Egner</surname> <given-names>T</given-names>
</name>
</person-group>
<year iso-8601-date="2016">2016</year>
<article-title>Satisficing in split-second decision making is characterized by strategic cue discounting</article-title>
<source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>
<volume>42</volume>
<fpage>1937</fpage>
<lpage>1956</lpage>
<pub-id pub-id-type="doi">10.1037/xlm0000284</pub-id>
<pub-id pub-id-type="pmid">27253846</pub-id>
</element-citation>
</ref>
<ref id="bib52">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Palmer</surname> <given-names>J</given-names>
</name>
<name>
<surname>Huk</surname> <given-names>AC</given-names>
</name>
<name>
<surname>Shadlen</surname> <given-names>MN</given-names>
</name>
</person-group>
<year iso-8601-date="2005">2005</year>
<article-title>The effect of stimulus strength on the speed and accuracy of a perceptual decision</article-title>
<source>Journal of Vision</source>
<volume>5</volume>
<fpage>1</fpage>
<lpage>404</lpage>
<pub-id pub-id-type="doi">10.1167/5.5.1</pub-id>
<pub-id pub-id-type="pmid">16097871</pub-id>
</element-citation>
</ref>
<ref id="bib53">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rao</surname> <given-names>RP</given-names>
</name>
</person-group>
<year iso-8601-date="2010">2010</year>
<article-title>Decision making under uncertainty: a neural model based on partially observable markov decision processes</article-title>
<source>Frontiers in Computational Neuroscience</source>
<volume>4</volume>
<elocation-id>146</elocation-id>
<pub-id pub-id-type="doi">10.3389/fncom.2010.00146</pub-id>
<pub-id pub-id-type="pmid">21152255</pub-id>
</element-citation>
</ref>
<ref id="bib54">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ratcliff</surname> <given-names>R</given-names>
</name>
</person-group>
<year iso-8601-date="1978">1978</year>
<article-title>A theory of memory retrieval</article-title>
<source>Psychological Review</source>
<volume>85</volume>
<fpage>59</fpage>
<lpage>108</lpage>
<pub-id pub-id-type="doi">10.1037/0033-295X.85.2.59</pub-id>
</element-citation>
</ref>
<ref id="bib55">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ratcliff</surname> <given-names>R</given-names>
</name>
</person-group>
<year iso-8601-date="1985">1985</year>
<article-title>Theoretical interpretations of the speed and accuracy of positive and negative responses</article-title>
<source>Psychological Review</source>
<volume>92</volume>
<fpage>212</fpage>
<lpage>225</lpage>
<pub-id pub-id-type="doi">10.1037/0033-295X.92.2.212</pub-id>
<pub-id pub-id-type="pmid">3991839</pub-id>
</element-citation>
</ref>
<ref id="bib56">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ratcliff</surname> <given-names>R</given-names>
</name>
<name>
<surname>Van Zandt</surname> <given-names>T</given-names>
</name>
<name>
<surname>McKoon</surname> <given-names>G</given-names>
</name>
</person-group>
<year iso-8601-date="1999">1999</year>
<article-title>Connectionist and diffusion models of reaction time</article-title>
<source>Psychological Review</source>
<volume>106</volume>
<fpage>261</fpage>
<lpage>300</lpage>
<pub-id pub-id-type="doi">10.1037/0033-295X.106.2.261</pub-id>
<pub-id pub-id-type="pmid">10378014</pub-id>
</element-citation>
</ref>
<ref id="bib57">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ratcliff</surname> <given-names>R</given-names>
</name>
<name>
<surname>Tuerlinckx</surname> <given-names>F</given-names>
</name>
</person-group>
<year iso-8601-date="2002">2002</year>
<article-title>Estimating parameters of the diffusion model: approaches to dealing with contaminant reaction times and parameter variability</article-title>
<source>Psychonomic Bulletin &amp; Review</source>
<volume>9</volume>
<fpage>438</fpage>
<lpage>481</lpage>
<pub-id pub-id-type="doi">10.3758/BF03196302</pub-id>
<pub-id pub-id-type="pmid">12412886</pub-id>
</element-citation>
</ref>
<ref id="bib58">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ratcliff</surname> <given-names>R</given-names>
</name>
<name>
<surname>Cherian</surname> <given-names>A</given-names>
</name>
<name>
<surname>Segraves</surname> <given-names>M</given-names>
</name>
</person-group>
<year iso-8601-date="2003">2003</year>
<article-title>A comparison of macaque behavior and superior colliculus neuronal activity to predictions from models of two-choice decisions</article-title>
<source>Journal of Neurophysiology</source>
<volume>90</volume>
<fpage>1392</fpage>
<lpage>1407</lpage>
<pub-id pub-id-type="doi">10.1152/jn.01049.2002</pub-id>
<pub-id pub-id-type="pmid">12761282</pub-id>
</element-citation>
</ref>
<ref id="bib59">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ratcliff</surname> <given-names>R</given-names>
</name>
<name>
<surname>Smith</surname> <given-names>PL</given-names>
</name>
</person-group>
<year iso-8601-date="2004">2004</year>
<article-title>A comparison of sequential sampling models for two-choice reaction time</article-title>
<source>Psychological Review</source>
<volume>111</volume>
<fpage>333</fpage>
<lpage>367</lpage>
<pub-id pub-id-type="doi">10.1037/0033-295X.111.2.333</pub-id>
<pub-id pub-id-type="pmid">15065913</pub-id>
</element-citation>
</ref>
<ref id="bib60">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Roitman</surname> <given-names>JD</given-names>
</name>
<name>
<surname>Shadlen</surname> <given-names>MN</given-names>
</name>
</person-group>
<year iso-8601-date="2002">2002</year>
<article-title>Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction time task</article-title>
<source>The Journal of Neuroscience</source>
<volume>22</volume>
<fpage>9475</fpage>
<lpage>9489</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-21-09475.2002</pub-id>
<pub-id pub-id-type="pmid">12417672</pub-id>
</element-citation>
</ref>
<ref id="bib61">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rorie</surname> <given-names>AE</given-names>
</name>
<name>
<surname>Gao</surname> <given-names>J</given-names>
</name>
<name>
<surname>McClelland</surname> <given-names>JL</given-names>
</name>
<name>
<surname>Newsome</surname> <given-names>WT</given-names>
</name>
</person-group>
<year iso-8601-date="2010">2010</year>
<article-title>Integration of sensory and reward information during perceptual decision-making in lateral intraparietal cortex (LIP) of the macaque monkey</article-title>
<source>PLoS One</source>
<volume>5</volume>
<elocation-id>e9308</elocation-id>
<pub-id pub-id-type="doi">10.1371/journal.pone.0009308</pub-id>
<pub-id pub-id-type="pmid">20174574</pub-id>
</element-citation>
</ref>
<ref id="bib62">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sakawa</surname> <given-names>M</given-names>
</name>
<name>
<surname>Yauchi</surname> <given-names>K</given-names>
</name>
</person-group>
<year iso-8601-date="2001">2001</year>
<article-title>An interactive fuzzy satisficing method for multiobjective nonconvex programming problems with fuzzy numbers through coevolutionary genetic algorithms</article-title>
<source>IEEE Transactions on Systems, Man and Cybernetics, Part B</source>
<volume>31</volume>
<fpage>459</fpage>
<lpage>467</lpage>
<pub-id pub-id-type="doi">10.1109/3477.931546</pub-id>
</element-citation>
</ref>
<ref id="bib63">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Serences</surname> <given-names>JT</given-names>
</name>
</person-group>
<year iso-8601-date="2008">2008</year>
<article-title>Value-based modulations in human visual cortex</article-title>
<source>Neuron</source>
<volume>60</volume>
<fpage>1169</fpage>
<lpage>1181</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.051</pub-id>
<pub-id pub-id-type="pmid">19109919</pub-id>
</element-citation>
</ref>
<ref id="bib64">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shadlen</surname> <given-names>MN</given-names>
</name>
<name>
<surname>Newsome</surname> <given-names>WT</given-names>
</name>
</person-group>
<year iso-8601-date="2001">2001</year>
<article-title>Neural basis of a perceptual decision in the parietal cortex (area LIP) of the rhesus monkey</article-title>
<source>Journal of Neurophysiology</source>
<volume>86</volume>
<fpage>1916</fpage>
<lpage>1936</lpage>
<pub-id pub-id-type="doi">10.1152/jn.2001.86.4.1916</pub-id>
<pub-id pub-id-type="pmid">11600651</pub-id>
</element-citation>
</ref>
<ref id="bib65">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shadlen</surname> <given-names>MN</given-names>
</name>
<name>
<surname>Shohamy</surname> <given-names>D</given-names>
</name>
</person-group>
<year iso-8601-date="2016">2016</year>
<article-title>Decision making and sequential sampling from memory</article-title>
<source>Neuron</source>
<volume>90</volume>
<fpage>927</fpage>
<lpage>939</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuron.2016.04.036</pub-id>
<pub-id pub-id-type="pmid">27253447</pub-id>
</element-citation>
</ref>
<ref id="bib66">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Simen</surname> <given-names>P</given-names>
</name>
<name>
<surname>Contreras</surname> <given-names>D</given-names>
</name>
<name>
<surname>Buck</surname> <given-names>C</given-names>
</name>
<name>
<surname>Hu</surname> <given-names>P</given-names>
</name>
<name>
<surname>Holmes</surname> <given-names>P</given-names>
</name>
<name>
<surname>Cohen</surname> <given-names>JD</given-names>
</name>
</person-group>
<year iso-8601-date="2009">2009</year>
<article-title>Reward rate optimization in two-alternative decision making: empirical tests of theoretical predictions</article-title>
<source>Journal of Experimental Psychology: Human Perception and Performance</source>
<volume>35</volume>
<fpage>1865</fpage>
<lpage>1897</lpage>
<pub-id pub-id-type="doi">10.1037/a0016926</pub-id>
<pub-id pub-id-type="pmid">19968441</pub-id>
</element-citation>
</ref>
<ref id="bib67">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Simon</surname> <given-names>HA</given-names>
</name>
</person-group>
<year iso-8601-date="1956">1956</year>
<article-title>Rational choice and the structure of the environment</article-title>
<source>Psychological Review</source>
<volume>63</volume>
<fpage>129</fpage>
<lpage>138</lpage>
<pub-id pub-id-type="doi">10.1037/h0042769</pub-id>
<pub-id pub-id-type="pmid">13310708</pub-id>
</element-citation>
</ref>
<ref id="bib68">
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Simon</surname> <given-names>HA</given-names>
</name>
</person-group>
<year iso-8601-date="1966">1966</year>
<chapter-title>Theories of Decision-Making in Economics and Behavioural Science</chapter-title>
<source>Surveys of Economic Theory: Resource Allocation</source>
<publisher-loc>London</publisher-loc>
<publisher-name>Palgrave Macmillan UK</publisher-name>
<fpage>1</fpage>
<lpage>28</lpage>
<pub-id pub-id-type="doi">10.1007/978-1-349-00210-8_1</pub-id>
</element-citation>
</ref>
<ref id="bib69">
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Simon</surname> <given-names>HA</given-names>
</name>
</person-group>
<year iso-8601-date="1982">1982</year>
<source>Models of Bounded Rationality</source>
<publisher-loc>Cambridge, Mass.</publisher-loc>
<publisher-name>MIT Press</publisher-name>
</element-citation>
</ref>
<ref id="bib70">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Smith</surname> <given-names>PL</given-names>
</name>
<name>
<surname>Ratcliff</surname> <given-names>R</given-names>
</name>
</person-group>
<year iso-8601-date="2004">2004</year>
<article-title>Psychology and neurobiology of simple decisions</article-title>
<source>Trends in Neurosciences</source>
<volume>27</volume>
<fpage>161</fpage>
<lpage>168</lpage>
<pub-id pub-id-type="doi">10.1016/j.tins.2004.01.006</pub-id>
<pub-id pub-id-type="pmid">15036882</pub-id>
</element-citation>
</ref>
<ref id="bib71">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Starns</surname> <given-names>JJ</given-names>
</name>
<name>
<surname>Ratcliff</surname> <given-names>R</given-names>
</name>
</person-group>
<year iso-8601-date="2010">2010</year>
<article-title>The effects of aging on the speed-accuracy compromise: boundary optimality in the diffusion model</article-title>
<source>Psychology and Aging</source>
<volume>25</volume>
<fpage>377</fpage>
<lpage>390</lpage>
<pub-id pub-id-type="doi">10.1037/a0018022</pub-id>
<pub-id pub-id-type="pmid">20545422</pub-id>
</element-citation>
</ref>
<ref id="bib72">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Starns</surname> <given-names>JJ</given-names>
</name>
<name>
<surname>Ratcliff</surname> <given-names>R</given-names>
</name>
</person-group>
<year iso-8601-date="2012">2012</year>
<article-title>Age-related differences in diffusion model boundary optimality with both trial-limited and time-limited tasks</article-title>
<source>Psychonomic Bulletin &amp; Review</source>
<volume>19</volume>
<fpage>139</fpage>
<lpage>145</lpage>
<pub-id pub-id-type="doi">10.3758/s13423-011-0189-3</pub-id>
<pub-id pub-id-type="pmid">22144142</pub-id>
</element-citation>
</ref>
<ref id="bib73">
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Stirling</surname> <given-names>WC</given-names>
</name>
</person-group>
<year iso-8601-date="2003">2003</year>
<source>Satisficing Games and Decision Making: With Applications to Engineering and Computer Science</source>
<publisher-loc>Cambridge, England; New York</publisher-loc>
<publisher-name>Cambridge University Press</publisher-name>
<pub-id pub-id-type="doi">10.1017/CBO9780511543456</pub-id>
</element-citation>
</ref>
<ref id="bib74">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Summerfield</surname> <given-names>C</given-names>
</name>
<name>
<surname>Koechlin</surname> <given-names>E</given-names>
</name>
</person-group>
<year iso-8601-date="2010">2010</year>
<article-title>Economic value biases uncertain perceptual choices in the parietal and prefrontal cortices</article-title>
<source>Frontiers in Human Neuroscience</source>
<volume>4</volume>
<elocation-id>208</elocation-id>
<pub-id pub-id-type="doi">10.3389/fnhum.2010.00208</pub-id>
<pub-id pub-id-type="pmid">21267421</pub-id>
</element-citation>
</ref>
<ref id="bib75">
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Sutton</surname> <given-names>RS</given-names>
</name>
<name>
<surname>Barto</surname> <given-names>A</given-names>
</name>
</person-group>
<year iso-8601-date="1998">1998</year>
<source>Reinforcement Learning: An Introduction</source>
<publisher-loc>Cambridge, Massachusetts, USA; </publisher-loc>
<publisher-name>MIT Press</publisher-name>
</element-citation>
</ref>
<ref id="bib76">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Teichert</surname> <given-names>T</given-names>
</name>
<name>
<surname>Ferrera</surname> <given-names>VP</given-names>
</name>
</person-group>
<year iso-8601-date="2010">2010</year>
<article-title>Suboptimal integration of reward magnitude and prior reward likelihood in categorical decisions by monkeys</article-title>
<source>Frontiers in Neuroscience</source>
<volume>4</volume>
<elocation-id>186</elocation-id>
<pub-id pub-id-type="doi">10.3389/fnins.2010.00186</pub-id>
<pub-id pub-id-type="pmid">21151367</pub-id>
</element-citation>
</ref>
<ref id="bib77">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Thura</surname> <given-names>D</given-names>
</name>
<name>
<surname>Beauregard-Racine</surname> <given-names>J</given-names>
</name>
<name>
<surname>Fradet</surname> <given-names>CW</given-names>
</name>
<name>
<surname>Cisek</surname> <given-names>P</given-names>
</name>
</person-group>
<year iso-8601-date="2012">2012</year>
<article-title>Decision making by urgency gating: theory and experimental support</article-title>
<source>Journal of Neurophysiology</source>
<volume>108</volume>
<fpage>2912</fpage>
<lpage>2930</lpage>
<pub-id pub-id-type="doi">10.1152/jn.01071.2011</pub-id>
<pub-id pub-id-type="pmid">22993260</pub-id>
</element-citation>
</ref>
<ref id="bib78">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vandekerckhove</surname> <given-names>J</given-names>
</name>
<name>
<surname>Tuerlinckx</surname> <given-names>F</given-names>
</name>
</person-group>
<year iso-8601-date="2007">2007</year>
<article-title>Fitting the Ratcliff diffusion model to experimental data</article-title>
<source>Psychonomic Bulletin &amp; Review</source>
<volume>14</volume>
<fpage>1011</fpage>
<lpage>1026</lpage>
<pub-id pub-id-type="doi">10.3758/BF03193087</pub-id>
<pub-id pub-id-type="pmid">18229471</pub-id>
</element-citation>
</ref>
<ref id="bib79">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Voss</surname> <given-names>A</given-names>
</name>
<name>
<surname>Rothermund</surname> <given-names>K</given-names>
</name>
<name>
<surname>Voss</surname> <given-names>J</given-names>
</name>
</person-group>
<year iso-8601-date="2004">2004</year>
<article-title>Interpreting the parameters of the diffusion model: an empirical validation</article-title>
<source>Memory &amp; Cognition</source>
<volume>32</volume>
<fpage>1206</fpage>
<lpage>1220</lpage>
<pub-id pub-id-type="doi">10.3758/BF03196893</pub-id>
<pub-id pub-id-type="pmid">15813501</pub-id>
</element-citation>
</ref>
<ref id="bib80">
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Wald</surname> <given-names>A</given-names>
</name>
</person-group>
<year iso-8601-date="1947">1947</year>
<source>Sequential Analysis</source>
<publisher-loc>New York</publisher-loc>
<publisher-name>Wiley</publisher-name>
</element-citation>
</ref>
<ref id="bib81">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wald</surname> <given-names>A</given-names>
</name>
<name>
<surname>Wolfowitz</surname> <given-names>J</given-names>
</name>
</person-group>
<year iso-8601-date="1948">1948</year>
<article-title>Optimum character of the sequential probability ratio test</article-title>
<source>The Annals of Mathematical Statistics</source>
<volume>19</volume>
<fpage>326</fpage>
<lpage>339</lpage>
<pub-id pub-id-type="doi">10.1214/aoms/1177730197</pub-id>
</element-citation>
</ref>
<ref id="bib82">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname> <given-names>AY</given-names>
</name>
<name>
<surname>Miura</surname> <given-names>K</given-names>
</name>
<name>
<surname>Uchida</surname> <given-names>N</given-names>
</name>
</person-group>
<year iso-8601-date="2013">2013</year>
<article-title>The dorsomedial striatum encodes net expected return, critical for energizing performance vigor</article-title>
<source>Nature Neuroscience</source>
<volume>16</volume>
<fpage>639</fpage>
<lpage>647</lpage>
<pub-id pub-id-type="doi">10.1038/nn.3377</pub-id>
<pub-id pub-id-type="pmid">23584742</pub-id>
</element-citation>
</ref>
<ref id="bib83">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wei</surname> <given-names>W</given-names>
</name>
<name>
<surname>Rubin</surname> <given-names>JE</given-names>
</name>
<name>
<surname>Wang</surname> <given-names>XJ</given-names>
</name>
</person-group>
<year iso-8601-date="2015">2015</year>
<article-title>Role of the indirect pathway of the basal ganglia in perceptual decision making</article-title>
<source>Journal of Neuroscience</source>
<volume>35</volume>
<fpage>4052</fpage>
<lpage>4064</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3611-14.2015</pub-id>
<pub-id pub-id-type="pmid">25740532</pub-id>
</element-citation>
</ref>
<ref id="bib84">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>White</surname> <given-names>CN</given-names>
</name>
<name>
<surname>Poldrack</surname> <given-names>RA</given-names>
</name>
</person-group>
<year iso-8601-date="2014">2014</year>
<article-title>Decomposing bias in different types of simple decisions</article-title>
<source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>
<volume>40</volume>
<fpage>385</fpage>
<lpage>398</lpage>
<pub-id pub-id-type="doi">10.1037/a0034851</pub-id>
</element-citation>
</ref>
<ref id="bib85">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wiecki</surname> <given-names>TV</given-names>
</name>
<name>
<surname>Sofer</surname> <given-names>I</given-names>
</name>
<name>
<surname>Frank</surname> <given-names>MJ</given-names>
</name>
</person-group>
<year iso-8601-date="2013">2013</year>
<article-title>HDDM: hierarchical bayesian estimation of the Drift-Diffusion model in Python</article-title>
<source>Frontiers in Neuroinformatics</source>
<volume>7</volume>
<elocation-id>14</elocation-id>
<pub-id pub-id-type="doi">10.3389/fninf.2013.00014</pub-id>
<pub-id pub-id-type="pmid">23935581</pub-id>
</element-citation>
</ref>
<ref id="bib86">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wierzbicki</surname> <given-names>AP</given-names>
</name>
</person-group>
<year iso-8601-date="1982">1982</year>
<article-title>A mathematical basis for satisficing decision making</article-title>
<source>Mathematical Modelling</source>
<volume>3</volume>
<fpage>391</fpage>
<lpage>405</lpage>
<pub-id pub-id-type="doi">10.1016/0270-0255(82)90038-0</pub-id>
</element-citation>
</ref>
<ref id="bib87">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zacksenhouse</surname> <given-names>M</given-names>
</name>
<name>
<surname>Bogacz</surname> <given-names>R</given-names>
</name>
<name>
<surname>Holmes</surname> <given-names>P</given-names>
</name>
</person-group>
<year iso-8601-date="2010">2010</year>
<article-title>Robust versus optimal strategies for two-alternative forced choice tasks</article-title>
<source>Journal of Mathematical Psychology</source>
<volume>54</volume>
<fpage>230</fpage>
<lpage>246</lpage>
<pub-id pub-id-type="doi">10.1016/j.jmp.2009.12.004</pub-id>
<pub-id pub-id-type="pmid">23180885</pub-id>
</element-citation>
</ref>
<ref id="bib88">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zylberberg</surname> <given-names>A</given-names>
</name>
<name>
<surname>Fetsch</surname> <given-names>CR</given-names>
</name>
<name>
<surname>Shadlen</surname> <given-names>MN</given-names>
</name>
</person-group>
<year iso-8601-date="2016">2016</year>
<article-title>The influence of evidence volatility on choice, reaction time and confidence in a perceptual decision</article-title>
<source>eLife</source>
<volume>5</volume>
<elocation-id>e17688</elocation-id>
<pub-id pub-id-type="doi">10.7554/eLife.17688</pub-id>
<pub-id pub-id-type="pmid">27787198</pub-id>
</element-citation>
</ref>
</ref-list>
</back>
<sub-article article-type="decision-letter" id="SA1">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.36018.037</article-id>
<title-group>
<article-title>Decision letter</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="editor">
<name>
<surname>Latham</surname>
<given-names>Peter</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution>University College London</institution>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
</front-stub>
<body>
<boxed-text>
<p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p>
</boxed-text>
<p>Thank you for submitting your article &quot;Ongoing, rational calibration of reward-driven perceptual biases&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Peter Latham as the Reviewing Editor, and the evaluation has been overseen by Richard Ivry as the Senior Editor. The following individual involved in review of your submission has agreed to reveal his identity: Roger Ratcliff (Reviewer #3).</p>
<p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p>
<p>Summary:</p>
<p>The authors consider a relatively standard 2AFC task in which monkeys view a random dot kinematogram and have to decide whether the dots are moving to the right or left. To make things slightly more interesting than usual, the rewards are asymmetric: in blocks of 30-50 trials, one saccade directions receives higher reward than the other.</p>
<p>The monkey learned to take the asymmetry into account: the more uncertain they were, the more they favored the direction that had higher reward. However, they were slightly suboptimal: they received about 97% of the reward they could have received if they had used an optimal policy.</p>
<p>To explain the suboptimality, the authors used a &quot;satisficing&quot; gradient-based learning rule. Essentially, the monkeys follow the gradient in trial averaged reward rate until they achieve 97% performance, and then stop learning. For such a model, the final values of the parameters depend on initial conditions. The authors found, however, that, under their model, all the animals used the same initial condition; so-called &quot;over-me&quot;.</p>
<p>Essential revisions:</p>
<p>There were a lot of things we liked about this paper (more on that below). However, all of us had problems with the &quot;satisficing&quot; learning rule. We know that's an essential feature of the paper, but it seems like a near untenable hypothesis. How, for instance, can the animal know when it reaches 97%? Presumably there's nothing special about 97%, but that still leaves the problem: how can the animal know when it's near the optimum, and so turn off learning? For the satisficing learning rule to be a viable explanation, these questions need to be answered.</p>
<p>Note that there are reasons to turn down the learning rate; see, for example, Aitchison et al., 2017, and Kirkpatrick et al., PNAS (2016), 106:10296--10301. But those approaches – which essentially turn down the learning rate when the synapses become more sure of their true values – would put a slightly different spin on the paper. In addition, the authors need to try other explanations. For instance, because learning is stochastic, the animal never reaches an optimum. And if the energy surface has a non-quadratic maximum (as it appears the energy surface does in this case), there will be bias. If the bias matches the observed bias, that would be a strong contender for a viable model. It's also possible that the model class used by the monkeys does not contain the true model, which could happen if z and me are tied in some way. That seems like an unlikely model, but probably not less likely than a model that turns down the learning rate. It is, at the very least, worth mentioning.</p>
<p>In addition, you should look for sequential effects: does behavior on one trial depend on the outcome of the previous trial? If so, can you link these to changes in me and z? If so, that could shed light on which model, if any, is correct. And it would be a nice addition to the analysis so far, which is primarily steady state.</p>
<p>On the plus side, the paper is short, well-written, and to the point, and the findings are novel and interesting. We're highly sympathetic to the idea that animals adopt satisficing solutions as opposed to optimal ones in many settings. We also think the ability to account for idiosyncratic differences in performance of different animals is a very nice result.</p>
<p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p>
<p>Thank you for resubmitting your work entitled &quot;Ongoing, rational calibration of reward-driven perceptual biases&quot; for further consideration at <italic>eLife</italic>. Your revised article has been reviewed by three reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Richard Ivry as the Senior Editor.</p>
<p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p>
<p>We see no fundamental obstacles to acceptance. However, there are still some things that are not clear. And in one case it appears that we were not sufficiently clear, so there is still a small amount of work to be done. Following are our suggestions; hopefully they will be crystal clear.</p>
<p>1) At least two of the reviewers were somewhat confused by the stopping rule. We think we eventually understood things, and the rule is in fact simple: stop learning when the reward reaches a certain (good enough) value. However, this is surprisingly hard to extract. We have a couple of suggestions to fix this:</p>
<p>a) You say:</p>
<p>&quot;We next consider if and how a consistent, adaptive process used by all three monkeys could lead to these idiosyncratic and not-quite optimal patterns of adjustments.&quot;</p>
<p>This is not all that informative – basically, you're saying you have a model, but you're not going to tell the reader what it is. Why not say:</p>
<p>&quot;We show that this can be explained by a model in which the monkeys are initially over-biased, adjust their model parameters to increase reward, but stop learning when the reward is high enough, but not maximum.&quot;</p>
<p>b) In Figure 8 legend, you say &quot;All four searches stopped when the reward exceeded the monkeys' RTrial<sub>predict</sub> in that session.&quot; This is hard to make sense of, for two reasons. First, we had to go back and figure out what RTrial<sub>predict</sub> is. Second, the implication is that there's something special about RTrial<sub>predict</sub>. In fact, the point is that you stop integrating when the reward reaches the reward the monkeys got on that trial. It would be a lot easier to understand if you just said that, without mentioning RTrial<sub>predict</sub>.</p>
<p>c) Identical comments apply to the third paragraph of the subsection “The monkeys’ adaptive adjustments were consistent with a satisficing, gradient based learning process”.</p>
<p>2) The added paragraph was not so clear:</p>
<p>“Our last assumption was that the monkeys terminated adjustments as soon as they reached a good-enough reward outcome. […] Moreover, the updating process could use step sizes that are fixed or adaptive to the gradient of a reward-related cost function (Aitchison et al., 2017 and Kirkpatrick et al., 2017).”</p>
<p>The first two sentences are fine, but after that one would have to know those papers inside and out to understand what's going on. I think all you need to say is something like:</p>
<p>(Aitchison et al., 2017 and Kirkpatrick et al., 2017) proposed a model in which learning rates decreased as synapses become more certain. In this scheme, learning can become very slow near an optimum, and might account for our results.</p>
<p>3) In our previous review, we said</p>
<p>In addition, the authors need to try other explanations. For instance, because learning is stochastic, the animal never reaches an optimum. And if the energy surface has a non-quadratic maximum (as it appears the energy surface does in this case), there will be bias. If the bias matches the observed bias, that would be a strong contender for a viable model.</p>
<p>In your response, you seemed to be able to guess what would happen in this case. We admire you if you are indeed correct, but we think it will require simulations. In particular, you need to run a model of the form</p>
<p>Delta z = eta dR/dz + noise</p>
<p>Delta me = eta dR/dme + noise</p>
<p>where R is reward. The average reward value of z and me under this model will, for a non-quadratic surface, but slightly biased. We think it's important to determine, numerically, what that bias is. Given that you're set up to solve an ODE, it shouldn't be much work to check what happens to the above equations.</p>
<p>4) You should cite Ratcliff, 1985, – as far as we know, that was the first paper to have bias in drift rates and starting point as 2 possibilities.</p>
<p>5) Take out this quote:</p>
<p>&quot;… they may not capture the substantial variability under different conditions and/or across individual subjects&quot;.</p>
<p>6) Add in the Green and Swets reference near Figure 2 and note that signal detection theory does not have a model of criterion setting and does not achieve optimal performance where that has been studied.</p>
</body>
</sub-article>
<sub-article article-type="reply" id="SA2">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.36018.038</article-id>
<title-group>
<article-title>Author response</article-title>
</title-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p>Essential revisions:</p>
<p>There were a lot of things we liked about this paper (more on that below). However, all of us had problems with the &quot;satisficing&quot; learning rule. We know that's an essential feature of the paper, but it seems like a near untenable hypothesis. How, for instance, can the animal know when it reaches 97%? Presumably there's nothing special about 97%, but that still leaves the problem: how can the animal know when it's near the optimum, and so turn off learning? For the satisficing learning rule to be a viable explanation, these questions need to be answered.</p>
</disp-quote>
<p>We apologize for the confusion. We agree with the reviewers that the monkeys likely did not know when they reached 97% of the maximum reward per trial or reward rate, which would imply that the monkeys also knew the location of the peak of the reward surface. In fact, the model simulations that we used to compare to the monkeys’ data did not use that stopping criterion. Instead, we assumed that the monkeys used an absolute reward criterion, computed as the expected reward based on the session-specific task and model parameters (using RTrial for Figure 8 and RR for Figure 8—figure supplement 1). For example, they might be “satisfied” with 3 ml juice per 10 trials for reward per trial, or 3 ml juice per minute for reward rate, to turn off learning.</p>
<p>We used a 97% maximum reward per trial or reward rate as the stopping point only for the illustrations of gradient trajectories in Figure 7, Figure 7—figure supplement 1, Figure 8—figure supplement 3 and 4. Note that in these simulations, the distinction between a “97% of maximum” versus an “absolute reward” criterion is arbitrary and depends only on our choice of model parameters.</p>
<p>We have revised the text to highlight these points, as follows:</p>
<p>Figure 8 legend: “All four searches stopped when the reward exceeded the monkeys’ RTrial<sub>predict</sub> in that session.”</p>
<p>Results: “The lines on the RTrial heatmap represent the trajectories of a gradient-tracking procedure that adjusts <italic>me</italic> and <italic>z</italic> values to increase RTrial until a termination point (for illustration, here we used 97% of the maximum possible value).”</p>
<disp-quote content-type="editor-comment">
<p>Note that there are reasons to turn down the learning rate; see, for example, Aitchison, Pouget, and Latham, 2017, and Kirkpatrick et al., PNAS (2016), 106:10296--10301. But those approaches – which essentially turn down the learning rate when the synapses become more sure of their true values – would put a slightly different spin on the paper.</p>
</disp-quote>
<p>Thank you for pointing us to these relevant studies. We have added a paragraph in the Discussion to speculate on different mechanisms of stopping:</p>
<p>“Our last assumption was that the monkeys terminated adjustments as soon as they reached a good-enough reward outcome. […] Moreover, the updating process could use step sizes that are fixed or adaptive to the gradient of a reward-related cost function (Aitchison et al., 2017 and Kirkpatrick et al., 2017)”</p>
<p>To explore these points further, we simulated an updating process with a stopping rule that was based on the gradient of the reward function. As shown in <xref ref-type="fig" rid="respfig1">Author response image 1A</xref>, in the same example session as in Figure 8, the reward and gradient-based updating and termination rules landed at similar end points. However, in general the reward-based updating process that we used in the manuscript approximated the monkeys’ data better (<xref ref-type="fig" rid="respfig1">Author response image 1B</xref>). We have not included this analysis in the revised manuscript but would be happy to do so at the reviewers’ discretion.</p>
<fig id="respfig1">
<object-id pub-id-type="doi">10.7554/eLife.36018.035</object-id>
<label>Author response image 1.</label>
<caption>
<title>Comparison between reward and gradient-based updating process.</title>
<p>(<bold>A</bold>) example over-<italic>me</italic> reward (magenta) and gradient (cyan)-based updating process The same example as shown in Figure 8A. Gray circle indicates monkeys’ <italic>me</italic> and <italic>z</italic>. (<bold>B</bold>) Same format as Figure 8B. Scatterplot of end points for the two updating processes.</p>
</caption>
<graphic xlink:href="elife-36018-resp-fig1-v3" mimetype="image" mime-subtype="jpeg"/>
</fig>
<disp-quote content-type="editor-comment">
<p>In addition, the authors need to try other explanations. For instance, because learning is stochastic, the animal never reaches an optimum. And if the energy surface has a non-quadratic maximum (as it appears the energy surface does in this case), there will be bias. If the bias matches the observed bias, that would be a strong contender for a viable model.</p>
</disp-quote>
<p>The reviewers are correct that the energy surface has a non-quadratic maximum and the gradient trajectories across the <italic>me/z</italic> space show certain biases (Figure 7 and Figure 7—figure supplement 1). For example, the area covered by the magenta or blue gradient lines is larger than the area covered by the green lines, indicating that if a monkey randomly picks a starting point within the <italic>me/z</italic> space, it is more likely to end up with one of two combinations: either an overly biased <italic>me</italic> and a non-adaptive <italic>z</italic> or an overly biased <italic>z</italic> and a non-adaptive <italic>me</italic>. However, these two combinations were not equally represented in our data. Instead, all three monkeys we tested showed a strong bias toward the first. Therefore, we do not think that the inherent bias in the energy surface alone can explain the observed patterns of behavior. As we now note in the text: “Figure 7 also illustrates why assumptions about the starting point of this adaptive process are important: randomly selected starting points would result in learned <italic>me</italic> and <italic>z</italic> values distributed around the peak of the reward function, whereas the data (e.g., Figure 5C) show distinct clustering that implies particular patterns of starting points.”</p>
<disp-quote content-type="editor-comment">
<p>It's also possible that the model class used by the monkeys does not contain the true model, which could happen if z and me are tied in some way. That seems like an unlikely model, but probably not less likely than a model that turns down the learning rate. It is, at the very least, worth mentioning.</p>
</disp-quote>
<p>Thank you for this suggestion. We agree that this possibility is important and should be acknowledged. We have added a sentence in Discussion (subsection “Considerations for assessing optimality and rationality”): “The particular form of DDM that we used produced reasonably good, but not perfect, fits to the monkeys’ data. These results support the utility of the DDM framework but also underscore the fact that we do not yet know the true model, which could impact our optimality assessment.”</p>
<disp-quote content-type="editor-comment">
<p>In addition, you should look for sequential effects: does behavior on one trial depend on the outcome of the previous trial? If so, can you link these to changes in me and z? If so, that could shed light on which model, if any, is correct. And it would be a nice addition to the analysis so far, which is primarily steady state.</p>
</disp-quote>
<p>Thank you for this suggestion. We did these analyses and found no evidence for consistent or substantial sequential choice effects for the three monkeys. We therefore conclude that these effects do not substantially impact our results. We now describe these findings in Results (subsection “The monkeys’ biases reflected changes in reward context and perceptual sensitivity”, first paragraph and Figure 2—figure supplement 1).</p>
<p>We also added a paragraph in the Materials and methods section describing the sequential analysis (subsection “Sequential analysis”).</p>
<disp-quote content-type="editor-comment">
<p>On the plus side, the paper is short, well-written, and to the point, and the findings are novel and interesting. We're highly sympathetic to the idea that animals adopt satisficing solutions as opposed to optimal ones in many settings. We also think the ability to account for idiosyncratic differences in performance of different animals is a very nice result.</p>
</disp-quote>
<p>Thank you for these encouraging comments!</p>
<p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p>
<disp-quote content-type="editor-comment">
<p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p>
<p>We see no fundamental obstacles to acceptance. However, there are still some things that are not clear. And in one case it appears that we were not sufficiently clear, so there is still a small amount of work to be done. Following are our suggestions; hopefully they will be crystal clear.</p>
<p>1) At least two of the reviewers were somewhat confused by the stopping rule. We think we eventually understood things, and the rule is in fact simple: stop learning when the reward reaches a certain (good enough) value. However, this is surprisingly hard to extract. We have a couple of suggestions to fix this:</p>
<p>a) You say:</p>
<p>&quot;We next consider if and how a consistent, adaptive process used by all three monkeys could lead to these idiosyncratic and not-quite optimal patterns of adjustments.&quot;</p>
<p>This is not all that informative – basically, you're saying you have a model, but you're not going to tell the reader what it is. Why not say:</p>
<p>&quot;We show that this can be explained by a model in which the monkeys are initially over-biased, adjust their model parameters to increase reward, but stop learning when the reward is high enough, but not maximum.&quot;</p>
</disp-quote>
<p>Thank you for the suggestion. We have incorporated this way of phrasing. The new text is: “We next show that these shifts can be explained by a model in which the monkeys are initially over-biased, then adjust their model parameters to increase reward and stop learning when the reward is high enough, but not at its maximum possible value.”</p>
<disp-quote content-type="editor-comment">
<p>b) In Figure 8 legend, you say &quot;All four searches stopped when the reward exceeded the monkeys' RTrial<sub>predict</sub> in that session.&quot; This is hard to make sense of, for two reasons. First, we had to go back and figure out what RTrial<sub>predict</sub> is. Second, the implication is that there's something special about RTrial<sub>predict</sub>. In fact, the point is that you stop integrating when the reward reaches the reward the monkeys got on that trial. It would be a lot easier to understand if you just said that, without mentioning RTrial<sub>predict</sub>.</p>
</disp-quote>
<p>Thank you for pointing this out. However, the stopping rule we used was not when the reward reaches the reward the monkeys got on that trial, but rather when the reward reaches the average reward the monkeys got on in that session. To make the sentence easier to understand, we changed it into “All four searches stopped when the reward exceeded the average reward the monkey received in that session (RTrial<sub>predict</sub>), estimated from the corresponding best-fitting model parameters and task conditions”.</p>
<disp-quote content-type="editor-comment">
<p>c) Identical comments apply to the third paragraph of the subsection “The monkeys’ adaptive adjustments were consistent with a satisficing, gradient based learning process”.</p>
</disp-quote>
<p>We changed “RTrial<sub>predict</sub>” in that sentence into “average reward in that session (RTrial<sub>predict</sub>)”.</p>
<disp-quote content-type="editor-comment">
<p>2) The added paragraph was not so clear:</p>
<p>“Our last assumption was that the monkeys terminated adjustments as soon as they reached a good-enough reward outcome. […] Moreover, the updating process could use step sizes that are fixed or adaptive to the gradient of a reward-related cost function (Aitchison et al., 2017 and Kirkpatrick et al., 2017).”</p>
<p>The first two sentences are fine, but after that one would have to know those papers inside and out to understand what's going on. I think all you need to say is something like:</p>
<p>(Aitchison et al., 2017 and Kirkpatrick et al., 2017) proposed a model in which learning rates decreased as synapses become more certain. In this scheme, learning can become very slow near an optimum, and might account for our results.</p>
</disp-quote>
<p>Thank you for the suggestion. We have changed the last two sentences to: “For example, the learning rate for synaptic weights might decrease as the presynaptic and postsynaptic activities become less variable (Aitchison et al., 2017; Kirkpatrick et al., 2017). In this scheme, learning gradually slows down as the monkey approaches the plateau on the reward surface, which might account for our results.”</p>
<disp-quote content-type="editor-comment">
<p>3) In our previous review, we said</p>
<p>In addition, the authors need to try other explanations. For instance, because learning is stochastic, the animal never reaches an optimum. And if the energy surface has a non-quadratic maximum (as it appears the energy surface does in this case), there will be bias. If the bias matches the observed bias, that would be a strong contender for a viable model.</p>
<p>In your response, you seemed to be able to guess what would happen in this case. We admire you if you are indeed correct, but we think it will require simulations. In particular, you need to run a model of the form</p>
<p>Delta z = eta dR/dz + noise</p>
<p>Delta me = eta dR/dme + noise</p>
<p>where R is reward. The average reward value of z and me under this model will, for a non-quadratic surface, but slightly biased. We think it's important to determine, numerically, what that bias is. Given that you're set up to solve an ODE, it shouldn't be much work to check what happens to the above equations.</p>
</disp-quote>
<p>Thank you for the suggestion. We simulated this model at random starting points with different scaling factors (eta) and different noise levels. As shown in <xref ref-type="fig" rid="respfig2">Author response image 2</xref>, when the reward-gradient updating processes start from random locations on the reward function, the end points tend to cover all directions relative to the peak, with majority of the end points clustered in the over-me, under-z region and the over-z, under-me region. This result is inconsistent with our monkeys’ behaviors, which deviated from the peak only in the over-<italic>me</italic>, under-<italic>z</italic> region. Therefore we do not think this model can explain our data. We have not included this analysis in the revised manuscript but would be happy to do so at your discretion.</p>
<fig id="respfig2">
<object-id pub-id-type="doi">10.7554/eLife.36018.036</object-id>
<label>Author response image 2.</label>
<caption>
<title>Gradient updating with randomness in the starting location and each updating step does not generate the biased end-point pattern seen in the data.</title>
<p>(<bold>A</bold>) Simulation of reward-gradient updating trajectories starting from random locations (white circles) on the reward function. Each updating step = scaling factor x gradient + noise (noise along the <italic>me</italic> and <italic>z</italic> dimensions were generated independently from the same Gaussian distribution). The updating process stopped when reward exceeded 97% of the maximum (red circles indicate end points). Note that the end-points of the updating process were located all around the reward-function plateau. The reward function was from the LR-R blocks in an example session of monkey C. (<bold>B</bold>) Polar histograms of the angle of the end-points relative to the peak of the reward function, showing end-points scattered all around the peak with clusters in two locations.</p>
</caption>
<graphic xlink:href="elife-36018-resp-fig2-v3" mimetype="image" mime-subtype="jpeg"/>
</fig>
<disp-quote content-type="editor-comment">
<p>4) You should cite Ratcliff, 1985, – as far as we know, that was the first paper to have bias in drift rates and starting point as 2 possibilities.</p>
</disp-quote>
<p>Thank you for pointing out this reference. We have added this citation in the second paragraph of the Introduction.</p>
<disp-quote content-type="editor-comment">
<p>5) Take out this quote:</p>
<p>&quot;… they may not capture the substantial variability under different conditions and/or across individual subjects&quot;.</p>
</disp-quote>
<p>Thank you for the suggestion. However, we have opted to keep this sentence, which we think makes a useful and accurate point.</p>
<disp-quote content-type="editor-comment">
<p>6) Add in the Green and Swets reference near Figure 2 and note that signal detection theory does not have a model of criterion setting and does not achieve optimal performance where that has been studied.</p>
</disp-quote>
<p>Given that Figure 2 is about logistic regression, not signal detection theory, we have decided not to include this reference here.</p>
</body>
</sub-article>
</article>