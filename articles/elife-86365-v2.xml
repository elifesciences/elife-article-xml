<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">86365</article-id><article-id pub-id-type="doi">10.7554/eLife.86365</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>BrainPy, a flexible, integrative, efficient, and extensible framework for general-purpose brain dynamics programming</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-304257"><name><surname>Wang</surname><given-names>Chaoming</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7986-3890</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-307856"><name><surname>Zhang</surname><given-names>Tianqiu</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-307857"><name><surname>Chen</surname><given-names>Xiaoyu</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-348088"><name><surname>He</surname><given-names>Sichao</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-307858"><name><surname>Li</surname><given-names>Shangyang</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-126841"><name><surname>Wu</surname><given-names>Si</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9650-6935</contrib-id><email>siwu@pku.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>School of Psychological and Cognitive Sciences, IDG/McGovern Institute for Brain Research, Peking-Tsinghua Center for Life Sciences, Center of Quantitative Biology, Academy for Advanced Interdisciplinary Studies, Bejing Key Laboratory of Behavior and Mental Health, Peking University</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution>Guangdong Institute of Intelligence Science and Technology</institution><addr-line><named-content content-type="city">Guangdong</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01yj56c84</institution-id><institution>Beijing Jiaotong University</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Stimberg</surname><given-names>Marcel</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000zhpw23</institution-id><institution>Institut de la Vision</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Poirazi</surname><given-names>Panayiota</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution></institution-wrap><country>Greece</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>22</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e86365</elocation-id><history><date date-type="received" iso-8601-date="2023-01-21"><day>21</day><month>01</month><year>2023</year></date><date date-type="accepted" iso-8601-date="2023-12-20"><day>20</day><month>12</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-10-28"><day>28</day><month>10</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.10.28.514024"/></event></pub-history><permissions><copyright-statement>Â© 2023, Wang et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Wang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-86365-v2.pdf"/><abstract><p>Elucidating the intricate neural mechanisms underlying brain functions requires integrative brain dynamics modeling. To facilitate this process, it is crucial to develop a general-purpose programming framework that allows users to freely define neural models across multiple scales, efficiently simulate, train, and analyze model dynamics, and conveniently incorporate new modeling approaches. In response to this need, we present BrainPy. BrainPy leverages the advanced just-in-time (JIT) compilation capabilities of JAX and XLA to provide a powerful infrastructure tailored for brain dynamics programming. It offers an integrated platform for building, simulating, training, and analyzing brain dynamics models. Models defined in BrainPy can be JIT compiled into binary instructions for various devices, including Central Processing Unit, Graphics Processing Unit, and Tensor Processing Unit, which ensures high-running performance comparable to native C or CUDA. Additionally, BrainPy features an extensible architecture that allows for easy expansion of new infrastructure, utilities, and machine-learning approaches. This flexibility enables researchers to incorporate cutting-edge techniques and adapt the framework to their specific needs.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>brain simulation</kwd><kwd>brain simulator</kwd><kwd>brain modeling</kwd><kwd>just-in-time compilation</kwd><kwd>computational neuroscience</kwd><kwd>spiking neural networks</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002855</institution-id><institution>Ministry of Science &amp; Technology, People Republic of China</institution></institution-wrap></funding-source><award-id>2021ZD0200204</award-id><principal-award-recipient><name><surname>Wu</surname><given-names>Si</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Peking-Tsinghua Center for Life Sciences</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Wu</surname><given-names>Si</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>BrainPy presents a general-purpose programming framework that enables efficient implementation, simulation, training, and analysis of brain dynamics models across multiple organization scales, ultimately facilitating the understanding of the complex neural mechanisms underlying brain functions.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Brain dynamics modeling, which uses computational models to simulate and elucidate brain functions, is receiving increasing attention from researchers across different disciplines. Recently, gigantic projects in brain science have been initiated worldwide, including the BRAIN Initiative (<xref ref-type="bibr" rid="bib45">Jorgenson et al., 2015</xref>), Human Brain Project (<xref ref-type="bibr" rid="bib5">Amunts et al., 2016</xref>), and China Brain Project (<xref ref-type="bibr" rid="bib71">Poo et al., 2016</xref>), which are continuously producing new data about the structures and activity patterns of neural systems. Computational modeling is a fundamental and indispensable tool for interpreting this vast amount of data. However, to date, we still lack a general-purpose programming framework for brain dynamics modeling. By general purpose, we mean that such a programming framework can implement most brain dynamics models, integrate diverse modeling demands (e.g., simulation, training, and analysis), and accommodate new modeling approaches constantly emerging in the field while maintaining high-running performance. General-purpose programming frameworks are exemplified by TensorFlow (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>) and PyTorch (<xref ref-type="bibr" rid="bib66">Paszke et al., 2019</xref>) in the field of Deep Learning, which provides convenient interfaces for researchers to define various AI models flexibly and efficiently. These frameworks have become essential infrastructure in AI research, and play an indispensable role in this round of the AI revolution (<xref ref-type="bibr" rid="bib23">Dean, 2022</xref>). Brain dynamics modeling also needs such a general-purpose programming framework urgently (<xref ref-type="bibr" rid="bib21">DâAngelo and Jirsa, 2022</xref>).</p><p>To develop a general-purpose programming framework for brain dynamics modeling, we face several challenges.</p><list list-type="bullet"><list-item><p>The first challenge comes from the complexity of the brain. The brain is organized modularly, hierarchically, and across multi-scales (<xref ref-type="bibr" rid="bib62">Meunier et al., 2010</xref>), implying that the framework must support model construction at different levels (e.g., channel, neuron, network) and model composition across multiple scales (e.g., neurons to networks, networks to circuits). Current brain simulators typically focus on only one or two scales, for example, spiking networks (<xref ref-type="bibr" rid="bib31">Gewaltig and Diesmann, 2007</xref>; <xref ref-type="bibr" rid="bib22">Davison et al., 2008</xref>; <xref ref-type="bibr" rid="bib9">Beyeler et al., 2015</xref>; <xref ref-type="bibr" rid="bib82">Stimberg et al., 2019</xref>) or firing rate models (<xref ref-type="bibr" rid="bib77">Sanz Leon et al., 2013</xref>; <xref ref-type="bibr" rid="bib16">Cakan et al., 2021</xref>). Recently, NetPyNE (<xref ref-type="bibr" rid="bib25">Dura-Bernal et al., 2019</xref>) and BMTK (<xref ref-type="bibr" rid="bib19">Dai et al., 2020a</xref>) have adopted descriptive languages to expand the modeling scales from channels to neurons and networks, but their modeling interfaces are still limited to predefined scales.</p></list-item><list-item><p>The second challenge is the integration of different modeling needs (<xref ref-type="bibr" rid="bib73">Ramezanian-Panahi et al., 2022</xref>; <xref ref-type="bibr" rid="bib21">DâAngelo and Jirsa, 2022</xref>). To elucidate brain functions comprehensively with computational models, we need to not only simulate neural activities, but also analyze the underlying mechanisms, and sometimes, we need to train models from data or tasks, implying that a general-purpose programming framework needs to be a platform to integrate multiple modeling demands. Current brain simulators mainly focus on simulation (<xref ref-type="bibr" rid="bib13">Brette et al., 2007</xref>; <xref ref-type="bibr" rid="bib89">Tikidji-Hamburyan et al., 2017</xref>; <xref ref-type="bibr" rid="bib11">Blundell et al., 2018</xref>), and largely ignore training and analysis.</p></list-item><list-item><p>The third challenge is achieving high-running performance while maintaining programming convenience (<xref ref-type="bibr" rid="bib89">Tikidji-Hamburyan et al., 2017</xref>; <xref ref-type="bibr" rid="bib11">Blundell et al., 2018</xref>), which is particularly true for brain dynamics modeling, as its unique characteristics make it difficult to run efficiently within a convenient Python interface. The current popular approach for solving this challenge is code generation based on descriptive languages (<xref ref-type="bibr" rid="bib35">Goodman, 2010</xref>; <xref ref-type="bibr" rid="bib11">Blundell et al., 2018</xref>). However, this approach has intrinsic limitations regarding transparency, flexibility, and extensibility (<xref ref-type="bibr" rid="bib89">Tikidji-Hamburyan et al., 2017</xref>; <xref ref-type="bibr" rid="bib11">Blundell et al., 2018</xref>) (Appendix 1).</p></list-item><list-item><p>The fourth challenge comes from the rapid development of the field. Brain dynamics modeling is relatively new and developing rapidly. New concepts, models, and mathematical approaches are constantly emerging, implying that a general-purpose programming framework needs to be extensible to take up new advances in the field conveniently.</p></list-item></list><p>In this paper, we propose BrainPy (âBrain Dynamics Programming in Pythonâ, <xref ref-type="fig" rid="fig1">Figure 1</xref>) as a solution to address all the above challenges. BrainPy provides infrastructure tailored for brain dynamics programming, including mathematical operators, differential equation solvers, universal model-building formats, and object-oriented JIT compilation. Such infrastructure provides the flexibility for users to define brain dynamics models freely and lays the foundation for BrainPy to build an integrative framework for brain dynamics modeling. First, BrainPy introduces a <monospace>brainpy.DynamicalSystem</monospace> interface to unify diverse brain dynamics models. Models at any level of resolution can be defined as <monospace>DynamicalSystem</monospace> classes, which further can be hierarchically composed to create higher-level models. Second, BrainPy builds an integrated platform for studying brain dynamics models, where the same BrainPy model can be used for simulation, training (e.g., offline learning, online learning, or backpropagation training), and analysis (e.g., low-dimensional bifurcation analysis or high-dimensional slow point analysis). Third, through JIT compilation and dedicated operators, BrainPy achieves impressive performance for its code execution. The same models can be deployed into different devices (such as Central Processing Unit [CPU], Graphics Processing Unit [GPU], and Tensor Processing Unit [TPU]) without additional code modification. Fourth, BrainPy is highly extensible. New extensions can be easily implemented as plug-in modules. Even the low-level primitive operators in the kernel system can be extended in the user-level Python interface. BrainPy is implemented in a robust continuous integration pipeline and is equipped with an automatic documentation building environment (Appendix 3). It is open sourced at <ext-link ext-link-type="uri" xlink:href="https://github.com/brainpy/BrainPy">https://github.com/brainpy/BrainPy</ext-link>. Rich tutorials and extensive examples are available at <ext-link ext-link-type="uri" xlink:href="https://brainpy.readthedocs.io">https://brainpy.readthedocs.io</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://brainpy-examples.readthedocs.io">https://brainpy-examples.readthedocs.io</ext-link>, respectively.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>BrainPy is an integrative framework targeting general-purpose brain dynamics programming.</title><p>(<bold>A</bold>) Infrastructure: BrainPy provides infrastructure tailored for brain dynamics programming, including NumPy-like operators for computations based on dense matrices, sparse and event-based operators for event-driven computations, numerical integrators for solving diverse differential equations, the modular and composable programming interface for universal model building, and a toolbox useful for brain dynamics modeling. (<bold>B</bold>) Function: BrainPy provides an integrated platform for studying brain dynamics, including model building, simulation, training, and analysis. Models defined in BrainPy can be used for simulation, training, and analysis jointly. (<bold>C</bold>) Compilation: Based on JAX (<xref ref-type="bibr" rid="bib29">Frostig et al., 2018</xref>) and XLA (<xref ref-type="bibr" rid="bib76">Sabne, 2020</xref>), BrainPy provides just-in-time (JIT) compilation for Python class objects. All models defined in BrainPy can be JIT compiled into machine codes to achieve high-running performance. (<bold>D</bold>) Device: The same BrainPy model can run on different devices including Central Processing Unit (CPU), Graphics Processing Unit (GPU), or Tensor Processing Unit (TPU), without additional code modification.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-fig1-v2.tif"/></fig></sec><sec id="s2"><title>Method and results</title><sec id="s2-1"><title>Infrastructure tailored for brain dynamics programming</title><p>To support its goal of becoming a general-purpose programming framework, BrainPy provides the infrastructure essential for brain dynamics modeling (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). This infrastructure is a collection of interconnected utilities designed to provide foundational services that enable users to easily, flexibly, and efficiently perform various types of modeling for brain dynamics. Specifically, BrainPy implements (1) mathematical operators for conventional computation based on dense matrices and event-driven computation based on sparse connections; (2) numerical integrators for various differential equations, the backbone of dynamical neural models; (3) a universal model-building interface for constructing multi-scale brain dynamics models and the associated JIT compilation for the efficient running of these models; and (4) a toolbox specialized for brain dynamics modeling.</p><p>First, BrainPy delivers rich mathematical operators as essential elements to describe diverse brain dynamics models (Appendix 4). On the one hand, brain dynamics modeling involves conventional computation based on dense matrices. In Python scientific computing ecosystem, dense matrix operators have been standardized and popularized by NumPy (<xref ref-type="bibr" rid="bib38">Harris et al., 2020</xref>), TensorFlow (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>), and PyTorch (<xref ref-type="bibr" rid="bib66">Paszke et al., 2019</xref>). To reduce the cost of learning a new set of computing languages, dense matrix operators in BrainPy (including multi-dimensional arrays, mathematical operations, linear algebra routines, Fourier transforms, and random number generations) follow the syntax of those in NumPy, TensorFlow, and PyTorch so that most Python users can directly program in BrainPy with their familiar operator syntax. On the other hand, brain dynamics modeling has specific computation properties, such as sparse connections and event-driven computations, which are difficult to efficiently implement with conventional operators. To accommodate these needs, BrainPy provides dozens of dedicated operators tailored for brain dynamics modeling, including event-driven operators, sparse operators, and JIT connectivity operators. Compared to traditional dense matrix operators, these operators can reduce the running time of typical brain dynamics models by several orders of magnitude (see Efficient performance of BrainPy).</p><p>Second, BrainPy offers a repertoire of numerical solvers for solving differential equations (Appendix 5). Differential equations are involved in most brain dynamics models. For ease of use, BrainPyâs numerical integration of differential equations is designed as a Python decorator. Users define differential equations as Python functions, whose numerical integration is accomplished by calling integrator functions, for example, <monospace>brainpy.odeint()</monospace> for ordinary differential equations (ODEs), <monospace>brainpy.sdeint()</monospace> for stochastic differential equations (SDEs), and <monospace>brainpy.fdeint()</monospace> for fractional differential equations (FDEs). These integrator functions are designed to be general, and most numerical solvers for ODEs and SDEs are provided, such as explicit RungeâKutta methods, adaptive RungeâKutta methods, and Exponential methods. For SDEs, BrainPy supports different stochastic integrals (ItÃ´ or Stratonovich) and different types of Wiener processes (scalar or multi-dimensional). As delays are ubiquitous in brain dynamics, BrainPy also supports the numerical integration of delayed ODEs, SDEs, and FDEs with various delay forms.</p><p>Third, BrainPy supports modular and composable programming and the associated object-oriented transformations (Appendix 6). To capture the fundamental characteristics of brain dynamics, which are modular, multi-scaled, and hierarchical (<xref ref-type="bibr" rid="bib62">Meunier et al., 2010</xref>), BrainPy follows the philosophy that âany dynamical model is just a Python class, and high-level models can be recursively composed by low-level onesâ (details will be illustrated in Flexible model building in BrainPy). However, such a modular and composable interface is not directly compatible with JIT compilers such as JAX and Numba, because they are designed to work with pure functions (Appendix 2). By providing object-oriented transformations, including the JIT compilation for class objects and the automatic differentiation for class variables, models defined with the above modular and composable interface can also benefit from the powerful transformations in advanced JIT compilers.</p><p>Fourth, BrainPy offers a toolbox specialized for brain dynamics modeling. A typical modeling experiment involves multiple stages or processes, such as creating synaptic connectivity, initializing connection weights, presenting stimulus inputs, and analyzing simulated results. For the convenience of running these operations repeatedly, BrainPy presets a set of utility functions, including synaptic connection, weight initialization, input construction, and data analysis. However, this presetting does not prevent users from defining their utility functions in the toolbox.</p></sec><sec id="s2-2"><title>Flexible model building in BrainPy</title><p>Brain dynamics models have the key characteristics of being modular, multi-scaled, and hierarchical, and BrainPy designs a modular, composable, and flexible programming paradigm to match these features. The paradigm is realized by the DynamicalSystem interface, which has the following appealing features.</p><p><monospace>DynamicalSystem</monospace> supports the definition of brain dynamics models at any organization level. Given a dynamical system, regardless of its complexity, users can implement it as a <monospace>DynamicalSystem</monospace> class. As an example, <xref ref-type="fig" rid="fig2">Figure 2A</xref> demonstrates how to define a potassium channel model with <monospace>DynamicalSystem</monospace>, in which the initialization function defines parameters and states, and the update function specifies how the states evolve. In this process, BrainPy toolbox can help users quickly initialize model variables, synaptic connections, weights, and delays, and BrainPy operators and integrators can support users to define model updating logic freely. In a similar fashion, other dynamical models, such as discontinuous neuron models (e.g., leaky integrate-and-fire model; <xref ref-type="bibr" rid="bib2">Abbott, 1999</xref>), continuous neuron models (e.g., FitzHughâNagumo model; <xref ref-type="bibr" rid="bib28">Fitzhugh, 1961</xref>), population models (e.g., WilsonâCowan model; <xref ref-type="bibr" rid="bib99">Wilson and Cowan, 1972</xref>), and network models (e.g., continuous attractor neural network; <xref ref-type="bibr" rid="bib101">Wu et al., 2008</xref>), can be implemented by subclassing DynamicalSystem as standalone modules.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>BrainPy supports modular and composable programming for building hierarchical brain dynamics models.</title><p>(<bold>A</bold>) An ion channel model is defined as a subclass of <monospace>brainpy.dynz.IonChannel</monospace>. The <monospace>__init__()</monospace> function specifies the parameters and states, while the update() function defines the updating rule for the states. (<bold>B</bold>) An HodgkinâHuxley (HH)-typed neuron model is defined by combining multiple ion channel models as a subclass of <monospace>brainpy.dyn.CondNeuGroup</monospace>. (<bold>C</bold>) An E/I balanced network model is defined by combining two neuron populations and their connections as a subclass of <monospace>brainpy.DynSysGroup</monospace>. (<bold>D</bold>) A ventral visual system model is defined by combining several networks, including V1, V2, V4, TEo, and TEpd, as a subclass of <monospace>brainpy.DynSysGroup</monospace>. For detailed mathematical information about the complete model, please refer to Appendix 9.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-fig2-v2.tif"/></fig><p>However, for complex dynamical models, such as HodgkinâHuxley (HH)-typed neuron models or large-scale cortical networks, their model definitions can be achieved through the composition of subcomponents. All models defined with <monospace>DynamicalSystem</monospace> can be used as modules to form more complicated high-level models. As an example, <xref ref-type="fig" rid="fig2">Figure 2B</xref> demonstrates how an HH-typed neuron model is created by combining multiple ion channel models. Such composable programming is the core of <monospace>DynamicalSystem</monospace>, and applies to almost all BrainPy models. For example, a synapse model consists of four components: synaptic dynamics (e.g., alpha, exponential, or dual exponential dynamics), synaptic communication (e.g., dense, sparse, or convolutional connections), synaptic output (e.g., conductance-, current-, or magnesium blocking-based), and synaptic plasticity (e.g., short- or long-term plasticity). Composing different realizations of these components enables to create diverse kinds of synaptic models. Similarly, various network models can be implemented by combining different neuron groups and their synaptic projections.</p><p>Remarkably, <monospace>DynamicalSystem</monospace> supports hierarchical composable programming, such that a model composed of lower-level components can hierarchically serve as a new component to form higher-level models. This property is highly useful for the construction of multi-scale brain models. <xref ref-type="fig" rid="fig2">Figure 2</xref> demonstrates an example of recursively composing a model from channels (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) to neurons (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) to networks (<xref ref-type="fig" rid="fig2">Figure 2C</xref>) and to systems (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, see Appendix 9 for details of the full model). It is worth pointing out that this hierarchical composition property is not shared by other brain simulators, and BrainPy allows for flexible control of composition depth according to usersâ needs. Moreover, for user convenience, BrainPy provides dozens of commonly used models, including channels, neurons, synapses, populations, and networks, as building blocks to simplify the building of large-scale models.</p></sec><sec id="s2-3"><title>Integrated modeling in BrainPy</title><p>BrainPy offers an integrated platform to comprehensively perform simulation, training, and analysis of brain dynamics models.</p><sec id="s2-3-1"><title>Model simulation</title><p>BrainPy designs the interface <monospace>brainpy.DSRunner</monospace> to simulate the dynamics of brain models. DSRunner can be used to simulate models at any level, including but not limited to channel (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), neuron (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), network (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), and system (<xref ref-type="fig" rid="fig3">Figure 3D</xref>) levels.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Model simulation in BrainPy.</title><p>The interface <monospace>DSRunner</monospace> supports the simulation of brain dynamics models at various levels. (<bold>A</bold>) The simulation of the potassium channel in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. (<bold>B</bold>) The simulation of the HH neuron model in <xref ref-type="fig" rid="fig2">Figure 2B</xref>. (<bold>C</bold>) The simulation of the E/I balanced network, COBAHH (<xref ref-type="bibr" rid="bib13">Brette et al., 2007</xref>) in <xref ref-type="fig" rid="fig2">Figure 2C</xref>. (<bold>D</bold>) The simulation of the ventral visual system model (the code please see <xref ref-type="fig" rid="fig2">Figure 2D</xref>, and the model please see Appendix 9). (<bold>E</bold>) Using jax.vmap to run a batch of spiking decision-making models (<xref ref-type="bibr" rid="bib95">Wang, 2002</xref>) with inputs of different coherence levels. The left panel shows the code used for batch simulations of different inputs, and the right panel illustrates the firing rates under different inputs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-fig3-v2.tif"/></fig><p>Brain dynamics models often require intensive parameter searches to fit the experimental data, which is a computationally demanding task. BrainPy facilitates this process by supporting multiple parallel simulation methods. Firstly, the <monospace>brainpy.running</monospace> module offers convenient routines for concurrent executions based on the python multiprocessing mechanism. This method is flexible, but may introduce additional time overhead due to the model recompilation and reinitialization in each process. Secondly, most BrainPy models inherently support the automatic vectorization of <monospace>jax.vmap</monospace> and automatic parallelization of <monospace>jax.pmap</monospace>. These methods can avoid the recompilation and reinitialization of models in the same batch, and automatically parallelize the model execution on the given machines. <xref ref-type="fig" rid="fig3">Figure 3E</xref> illustrates the simplicity of this batch simulation approach. By using a single line of functional calls, BrainPy models can run simultaneously with different parameter settings.</p></sec><sec id="s2-3-2"><title>Model training</title><p>The use of machine-learning methods to train neural models is becoming a new trend for studying brain functions (<xref ref-type="bibr" rid="bib60">Masse et al., 2019</xref>; <xref ref-type="bibr" rid="bib27">Finkelstein et al., 2021</xref>; <xref ref-type="bibr" rid="bib51">Laje and Buonomano, 2013</xref>; <xref ref-type="bibr" rid="bib86">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib78">Saxe et al., 2021</xref>). BrainPy provides the <monospace>brainpy.DSTrainer</monospace> interface to support this utility. Different subclasses of <monospace>DSTrainer</monospace> provide different training algorithms, which can be used to train different types of models. For instance, the trainer <monospace>brainpy.BPTT</monospace> implements the algorithm of backpropagation through time, which is helpful for training spiking neural networks (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) and recurrent neural networks (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Similarly, <monospace>brainpy.OfflineTrainer</monospace> implements offline learning algorithms such as ridge regression (<xref ref-type="bibr" rid="bib57">LukoÅ¡eviÄius, 2012</xref>), <monospace>brainpy.OnlineTrainer</monospace> implements online learning algorithms such as FORCE learning (<xref ref-type="bibr" rid="bib84">Sussillo and Abbott, 2009</xref>), which are useful for training reservoir computing models (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). In a typical training task, one may try different algorithms that can be used to train a model. The unified syntax for defining and training models in BrainPy enables users to train the same model using multiple algorithms (see Appendix 10). <xref ref-type="fig" rid="fig4">Figure 4DâF</xref> demonstrates that a reservoir network model can be trained with three different algorithms (online, offline, and backpropagation) to accomplish a classical task of chaotic time series prediction (<xref ref-type="bibr" rid="bib43">Jaeger, 2007</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Model training in BrainPy.</title><p>BrainPy supports the training of brain dynamics models from data or tasks. (<bold>A</bold>) Training a spiking neural network (<xref ref-type="bibr" rid="bib8">Bellec et al., 2020</xref>) on an evidence accumulation task (<xref ref-type="bibr" rid="bib63">Morcos and Harvey, 2016</xref>) using the backpropagation algorithm with <monospace>brainpy.BPTT</monospace>. (<bold>B</bold>) Training an artificial recurrent neural network model (<xref ref-type="bibr" rid="bib80">Song et al., 2016</xref>) on a perceptual decision-making task (<xref ref-type="bibr" rid="bib15">Britten et al., 1992</xref>) with <monospace>brainpy.BPTT</monospace>. (<bold>C</bold>) Training a reservoir computing model (<xref ref-type="bibr" rid="bib30">Gauthier et al., 2021</xref>) to infer the Lorenz dynamics with the ridge regression algorithm implemented in<monospace> brainpy.OfflineTrainer</monospace>. <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>z</mml:mi></mml:mstyle></mml:math></inline-formula> are variables in the Lorenz system. (<bold>DâF</bold>) The classical echo state machine (<xref ref-type="bibr" rid="bib43">Jaeger, 2007</xref>) has been trained using multiple algorithms to predict the chaotic Lorenz dynamics. The algorithms utilized include ridge regression (<bold>D</bold>), FORCE learning (<bold>E</bold>), and backpropagation algorithms (<bold>F</bold>) implemented in <monospace>BrainPy</monospace>. The mean squared errors between the predicted and actual Lorenz dynamics were 0.001057 for ridge regression, 0.171304 for FORCE learning, and 1.276112 for backpropagation. Please refer to Appendix 10 for the training details.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-fig4-v2.tif"/></fig><p>Since the training algorithms for brain dynamics models have not been standardized in the field, BrainPy provides interfaces to support the flexible customization of training algorithms. Specifically,<monospace> OfflineTrainer</monospace> and <monospace>OnlineTrainer</monospace> provide general interfaces for offline and online learning algorithms, respectively, and users can easily select the appropriate method by specifying the fit_method parameter in <monospace>OfflineTrainer</monospace> or <monospace>OnlineTrainer</monospace>. Furthermore, the BPTT interface is designed to capture the latest advances in backpropagation algorithms. For instance, it supports eligibility propagation algorithm (<xref ref-type="bibr" rid="bib8">Bellec et al., 2020</xref>) and surrogate gradient learning (<xref ref-type="bibr" rid="bib65">Neftci et al., 2019</xref>) for training spiking neural networks.</p></sec><sec id="s2-3-3"><title>Model analysis</title><p>Analyzing model dynamics is as essential as model simulation and training because it helps unveil the underlying mechanism of model behaviors. Given a dynamical system, BrainPy provides the interface <monospace>brainpy.DSAnalyzer</monospace> for automatic dynamic analysis, and different classes of DSAnalyzer implement different analytical methods.</p><p>First, BrainPy supports phase plane and bifurcation analyses for low-dimensional dynamical systems. The phase plane is a classical and powerful technique for the analysis of dynamical systems and has been widely used in brain dynamics studies, including neuron models (e.g., Izhikevich model; <xref ref-type="bibr" rid="bib42">Izhikevich, 2003</xref>) and population rate models (e.g., WilsonâCowan model; <xref ref-type="bibr" rid="bib99">Wilson and Cowan, 1972</xref>). <xref ref-type="fig" rid="fig5">Figure 5A</xref> shows an example where many features of phase plane analysis, including nullcline, vector field, fixed points, and their stability, for a complex rate-based decision-making model (<xref ref-type="bibr" rid="bib100">Wong and Wang, 2006</xref>) are automatically evaluated by several lines of BrainPy code. Bifurcation analysis is another utility of BrainPy, which allows users to easily investigate the changing behaviors of a dynamical system when parameters are continuously varying. <xref ref-type="fig" rid="fig5">Figure 5B</xref> demonstrates the stability changes of the classical FitzHughâNagumo model (<xref ref-type="bibr" rid="bib28">Fitzhugh, 1961</xref>) with one parameter varying can be easily inspected by the bifurcation analysis interface provided in BrainPy. Similarly, bifurcation analysis of codimension-2 (with two parameters changing simultaneously; <xref ref-type="fig" rid="fig5">Figure 5C</xref>) can be performed with the same interface. BrainPy also supports bifurcation analysis for three-dimensional fastâslow systems, for example, a bursting neuron model (<xref ref-type="bibr" rid="bib74">Rinzel, 1985</xref>). This set of low-dimensional analyzers is performed numerically so that they are not restricted to equations with smooth functions, but are equally applicable to ones with strong and complex nonlinearity.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Model analysis in BrainPy.</title><p>BrainPy supports automatic dynamics analysis for low- and high-dimensional systems. (<bold>A</bold>) Phase plane analysis of a rate-based decision-making model (<xref ref-type="bibr" rid="bib100">Wong and Wang, 2006</xref>). (<bold>B</bold>) Bifurcation analysis of codimension 1 of the FitzHughâNagumo model (<xref ref-type="bibr" rid="bib28">Fitzhugh, 1961</xref>), in which the bifurcation parameter is the external input <monospace>Iext</monospace>. (<bold>C</bold>) Bifurcation analysis of codimension 2 of the FitzHughâNagumo model (<xref ref-type="bibr" rid="bib28">Fitzhugh, 1961</xref>), in which two bifurcation parameters <monospace>Iext</monospace> and <monospace>a</monospace> are continuously varying. (<bold>D</bold>) Finding stable and unstable fixed points of a high-dimensional CANN model (<xref ref-type="bibr" rid="bib101">Wu et al., 2008</xref>). (<bold>E</bold>) Linearization analysis of the high-dimensional CANN model (<xref ref-type="bibr" rid="bib101">Wu et al., 2008</xref>) around one stable and one unstable fixed point.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-fig5-v2.tif"/></fig><p>Second, BrainPy supports slow point computation and linearization analysis for high-dimensional dynamical systems. With powerful numerical optimization methods, one can find fixed or slow points of a high-dimensional nonlinear system (<xref ref-type="bibr" rid="bib85">Sussillo and Barak, 2013</xref>). By integrating numerical methods such as gradient descent and nonlinear optimization algorithms, BrainPy provides the interface <monospace>brainpy.analysis.SlowPointFinder</monospace> as a fundamental tool for high-dimensional analysis. <xref ref-type="fig" rid="fig5">Figure 5D</xref> demonstrates that the <monospace>SlowPointFinder</monospace> can effectively find a line of stable and unstable attractors in a CANN network (<xref ref-type="bibr" rid="bib101">Wu et al., 2008</xref>). Furthermore, the linearized dynamics around the found fixed points can be easily inspected and visualized with <monospace>SlowPointFinder</monospace> interface (<xref ref-type="fig" rid="fig5">Figure 5E</xref>).</p></sec></sec><sec id="s2-4"><title>Efficient performance of BrainPy</title><p>Simulating dynamical models efficiently in Python is notoriously challenging (<xref ref-type="bibr" rid="bib11">Blundell et al., 2018</xref>). To resolve this problem, BrainPy leverages the JIT compilation of JAX/XLA and exploits dedicated primitive operators to accelerate the model running.</p><sec id="s2-4-1"><title>JIT compilation</title><p>In contrast to deep neural networks (DNNs), which mainly consist of computation-intensive operations (such as convolution and matrix multiplication), brain dynamics models are usually dominated by memory-intensive operations. Taking the classical leaky integrate-and-fire (LIF) neuron model (<xref ref-type="bibr" rid="bib2">Abbott, 1999</xref>) as an example, its computation mainly relies on operators such as addition, multiplication, and division. As shown in <xref ref-type="fig" rid="fig6">Figure 6A</xref>, we measure the running times of an LIF model and a matrix multiplication with the same number of floating-point operations (FLOPs) on both CPU and GPU devices. The results indicate that the LIF model is significantly slower than the matrix multiplication on both devices, despite having the same theoretical complexity. This reveals the existence of large overheads when executing brain dynamics models in Python. Moreover, these overheads become dominant when simulating large-scale brain networks, as they grow rapidly with the number of operators in the model.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>BrainPy accelerates the running speed of brain dynamics models through just-in-time (JIT) compilation.</title><p>(<bold>A</bold>) Performance comparison between an LIF neuron model (<xref ref-type="bibr" rid="bib2">Abbott, 1999</xref>) and a matrixâvector multiplication <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>W</mml:mi><mml:mi>v</mml:mi></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>W</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi><mml:mo>Ã</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>v</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>). By adjusting the number of LIF neurons in a network and the dimension <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula> in the matrixâvector multiplication, we compare two models under the same floating-point operations (FLOPs). The top panel: On the Central Processing Unit (CPU) device, the LIF model without JIT compilation (the â<monospace>LIF</monospace>â line) shows much slower performance than the matrixâvector multiplication (the â<monospace>Dot</monospace>â line). After compiling the whole LIF network into the CPU device through JIT compilation (the â<monospace>LIF</monospace> with <monospace>JIT</monospace>â line), two models show comparable running speeds (please refer to <xref ref-type="fig" rid="app11fig6">Appendix 11âfigure 6A</xref> for the time ratio). The bottom panel: On the Graphics Processing Unit (GPU) device, the LIF model without JIT shows several times slower than the matrixâvector multiplication under the same FLOPs. After applying the JIT compilation, the jitted LIF model shows comparable performance to the matrixâvector multiplication (please refer to <xref ref-type="fig" rid="app11fig6">Appendix 11âfigure 6B</xref> for the time ratio). (<bold>B, C</bold>) Performance comparison of a classical E/I balanced network COBA (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>) with and without JIT compilation (the â<monospace>With JIT</monospace>â line vs. the â<monospace>Without JIT</monospace>â line). (<bold>B</bold>) JIT compilation provides a speedup of over ten times for the COBA network on the CPU device (please refer to <xref ref-type="fig" rid="app11fig6">Appendix 11âfigure 6C</xref> for the acceleration ratio). (<bold>C</bold>) Similarly, after compiling the whole COBA network model into GPUs, the model achieves significant acceleration, several times faster than before (please refer to <xref ref-type="fig" rid="app11fig6">Appendix 11âfigure 6D</xref> for the acceleration ratio). For experimental details, please see Appendix 11.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-fig6-v2.tif"/></fig><p>To overcome this limitation, we employ the JIT compilation technique to dramatically reduce these overhead costs in BrainPy. The JIT compilation transforms the dynamic Python code into the static machine code during runtime, which can significantly reduce the time cost of Python interpretation. Specifically, we utilize JAX, which implements JIT compilation based on XLA (Appendix 2). The XLA JIT engine employs specialized optimizations for memory-intensive operators, for example, operator fusion, which alleviates memory access overhead by minimizing the requirement for intermediate data storage and redundant data transfers during the sequential execution of multiple unmerged operations. This renders the JIT compilation with XLA highly suitable for handling brain dynamics models. <xref ref-type="fig" rid="fig6">Figure 6A</xref> demonstrates that with the JIT compilation, the LIF model achieves a running speed comparable to that of the matrix multiplication operation Dot on the CPU and outperforms or matches it on the GPU (see <xref ref-type="fig" rid="fig6">Figure 6A</xref>, <xref ref-type="fig" rid="app11fig1">Appendix 11âfigure 6A</xref>, and <xref ref-type="fig" rid="app11fig6">Appendix 11âfigure 6B</xref>). To further illustrate the benefits of the JIT compilation, we apply it to a realistic brain simulation model, namely, the E/I balanced network model COBA (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>). The results show that the JIT compilation boosts the running speed by 10 times on both the CPU and GPU compared to the case without JIT compilation (for CPU acceleration, see <xref ref-type="fig" rid="fig6">Figure 6B</xref> and <xref ref-type="fig" rid="app11fig6">Appendix 11âfigure 6C</xref>; for GPU acceleration, see <xref ref-type="fig" rid="fig6">Figure 6C</xref> and <xref ref-type="fig" rid="app11fig6">Appendix 11âfigure 6D</xref>).</p></sec></sec><sec id="s2-5"><title>Dedicated operators</title><p>Another key feature that distinguishes brain dynamics models from DNNs is that they usually have sparse connections and perform event-driven computations. For example, neurons in a network are typically connected with each other with a probability less than 0.2 (<xref ref-type="bibr" rid="bib72">Potjans and Diesmann, 2014</xref>), and the state of a postsynaptic neuron is updated only when a presynaptic spike event occurs. These unique features greatly impair the efficiency of brain model simulation using conventional operators, even with the help of JIT compilation. To illustrate this, <xref ref-type="fig" rid="fig7">Figure 7A</xref> demonstrates that when implementing a COBA network model with dense matrix-based operators, the majority of simulation time is consumed by synaptic computations on both CPU and GPU devices, and this issue becomes more pronounced as the network size increases (see â<monospace>CPU, Dense</monospace>â and â<monospace>GPU, Dense</monospace>â lines in <xref ref-type="fig" rid="fig7">Figure 7A</xref>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>BrainPy accelerates the running speed of brain dynamics models through dedicated operators.</title><p>(<bold>A</bold>) Without dedicated event-driven operators, the majority of the time is spent on synaptic computations when simulating a COBA network model (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>). The ratio significantly increases with the network size on both Central Processing Unit (CPU) and Graphics Processing Unit (GPU) devices (please refer to the lines labeled as â<monospace>CPU, Dense</monospace>â and â<monospace>GPU, Dense</monospace>â which correspond to the models utilizing the dense operator-based synaptic computation and running on the CPU and GPU devices, respectively). With the event-based primitive operators, the proportion of time spent on synaptic computation remains constant regardless of network size (please refer to the lines labeled as â<monospace>CPU, Event</monospace>â and â<monospace>GPU, Event</monospace>â which represent the models performing event-driven computations on the CPU and GPU devices, respectively). (<bold>B</bold>) On the CPU device, the COBA network model with event-based operators (see the â<monospace>With dedicated OP</monospace>â line) is accelerated by up to three orders of magnitude compared to that without dedicated operators (see the â<monospace>Without dedicated OP</monospace>â line). Please refer to <xref ref-type="fig" rid="app11fig7">Appendix 11âfigure 7A</xref> for the acceleration ratio. (<bold>C</bold>) The COBA network model exhibited two orders of magnitude acceleration when implemented with event-based primitive operators on a GPU device. This performance improvement was more pronounced for larger network sizes on both CPU and GPU platforms. Please refer to <xref ref-type="fig" rid="app11fig7">Appendix 11âfigure 7B</xref> for the acceleration ratio. For experimental details, please see Appendix 11.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-fig7-v2.tif"/></fig><p>In order to address this challenge, BrainPy introduces specialized primitive operators designed to accelerate event-based computations within sparsely connected networks. These specialized operators encompass transformations among variables associated with presynaptic neurons, postsynaptic neurons, and synapses, as well as sparse computation operators, event-driven computation operators, and JIT connectivity operators (refer to Appendix 4 for more details). By employing these specialized operators, BrainPy significantly reduces the time required for synaptic computations. As depicted in <xref ref-type="fig" rid="fig7">Figure 7B</xref>, the specialized event-based operators result in a remarkable speedup of the classical COBA network model by orders of magnitude (see <xref ref-type="fig" rid="app11fig7">Appendix 11âfigure 7A</xref>). Similar speed improvements are observed when utilizing GPU computations, as shown in <xref ref-type="fig" rid="fig7">Figure 7C</xref> and <xref ref-type="fig" rid="app11fig7">Appendix 11âfigure 7B</xref>. Furthermore, an examination of the time proportion for synaptic computations indicates that the utilization of specialized operators ensures a consistent time ratio for synaptic computation, even as the network size increases (see â<monospace>CPU, Event</monospace>â and â<monospace>GPU, Event</monospace>â lines in <xref ref-type="fig" rid="fig7">Figure 7A</xref>).</p><sec id="s2-5-1"><title>Benchmarking</title><p>To conduct a formal assessment of the running efficiency of BrainPy, we conducted a comparative analysis against several widely used brain simulators, namely NEURON (<xref ref-type="bibr" rid="bib40">Hines and Carnevale, 1997</xref>), NEST (<xref ref-type="bibr" rid="bib31">Gewaltig and Diesmann, 2007</xref>), Brian2 (<xref ref-type="bibr" rid="bib82">Stimberg et al., 2019</xref>), Brian2CUDA (<xref ref-type="bibr" rid="bib4">Alevi et al., 2022</xref>), GeNN (<xref ref-type="bibr" rid="bib104">Yavuz et al., 2016</xref>), and Brian2GeNN (<xref ref-type="bibr" rid="bib83">Stimberg et al., 2020</xref>). Our benchmarking focused on measuring the simulation speeds of these frameworks for models with sparse and dense connectivity patterns. The tests were performed using three common computing platforms: CPU, GPU, and TPU. This comprehensive assessment provides insights into BrainPyâs efficiency relative to other mainstream simulators across different hardware configurations and network scales.</p><p>To evaluate the performance of brain simulators on sparsely connected networks, we utilized two established E/I balanced network models with LIF and HH neuron types: the COBA (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>) and COBAHH (<xref ref-type="bibr" rid="bib13">Brette et al., 2007</xref>) networks (experimental details please see Appendix 11). COBA consists of excitatory and inhibitory LIF neurons with sparse random connectivity. COBAHH uses the same network architecture but replaces the LIF neurons with biophysically detailed HH neuron models. On the CPU platform, consistent with previous benchmark experiments (<xref ref-type="bibr" rid="bib82">Stimberg et al., 2019</xref>), we find that NEURON and NEST simulators exhibit suboptimal performance when running on a single node (see <xref ref-type="fig" rid="fig8">Figure 8A</xref> and <xref ref-type="fig" rid="fig8">Figure 8B</xref>). In contrast, BrainPy and Brian2 demonstrate comparable performance, showcasing a remarkable speed advantage of one to two orders of magnitude over NEURON and NEST. As both Brian2 and BrainPy support single-precision floating-point computation (x32), we conducted an analysis of their performances in the context of x32 computation. In order to ensure accurate simulation results with x32 computation, we examined the simulation outcomes across various simulators and platforms (refer to Appendix 11). Our evaluation demonstrated that BrainPy outperforms Brian2 in terms of speedup for numerical integration using x32 arithmetic on CPU platforms. On the GPU platform, GeNN demonstrates optimal linear scaling of execution time on both COBA and COBAHH network models as the network size increases (<xref ref-type="fig" rid="fig8">Figure 8D</xref> and <xref ref-type="fig" rid="fig8">Figure 8E</xref>). In contrast, BrainPy and Brian2CUDA exhibit a slight overhead and maintain a constant running time when dealing with small network sizes. However, when it comes to network scaling, BrainPy and Brian2CUDA outperform GeNN. Particularly as the network size grows, GeNN exhibits significantly slower performance. Additionally, the utilization of single-precision floating point in GeNN, Brian2CUDA, and BrainPy further enhances their GPU performance (excluding the COBA model in Brian2CUDA). Once again, we observed that BrainPyâs x32 mode achieves a more pronounced performance gain. Particularly, in the COBAHH model, BrainPyâs x32 computation demonstrates a substantial speedup compared to other brain simulators. BrainPy also enables model deployment on TPUs. However, since TPUs currently lack native support for sparse computations and toolchains for operator customization, we could not leverage event-driven sparse operators to simulate the sparsely connected COBA and COBAHH networks. Instead, we used dense matrix multiplication with masking to approximate the sparse connectivity. Unfortunately, this led to significantly slower performance for the two sparsely connected models compared to the results obtained on GPUs (please refer to <xref ref-type="fig" rid="app11fig8">Appendix 11âfigure 8</xref>). Moreover, the use of masked matrices resulted in a quadratic increase in memory usage. Consequently, the benchmarking experiments of COBA and COBAHH networks on TPU were limited to a scale of <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>4</mml:mn><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> neurons.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Speed comparison of NEURON, Nest, Brian2, and BrainPy under different computing devices.</title><p>Comparing speeds of different brain simulation platforms using the benchmark model COBA (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>) on both the Central Processing Unit (CPU) (<bold>A</bold>) and Graphics Processing Unit (GPU) (<bold>D</bold>) devices. NEURON is truncated at 16,000 neurons due to its very slow runtime. Comparing speeds of different platforms using the benchmark model COBAHH (<xref ref-type="bibr" rid="bib13">Brette et al., 2007</xref>) on both the CPU (<bold>B</bold>) and GPU (<bold>E</bold>) devices. Speed comparison of a spiking decision-making network (<xref ref-type="bibr" rid="bib95">Wang, 2002</xref>) on CPU (<bold>C</bold>), GPU, and Tensor Processing Unit (TPU) (<bold>F</bold>) devices. Please refer to Appendix 11 for experimental details, and <xref ref-type="fig" rid="app11fig8">Appendix 11âfigure 8</xref> for more data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-fig8-v2.tif"/></fig><p>To evaluate the performance of brain simulators on densely connected networks, we utilized the decision-making network proposed by <xref ref-type="bibr" rid="bib95">Wang, 2002</xref>. Assessing computational efficiency for dense connectivity is important for simulating models that feature dense recurrent connections (<xref ref-type="bibr" rid="bib64">Motta et al., 2019</xref>) and facilitating the integration with DNNs which commonly employ dense connectivity between layers (<xref ref-type="bibr" rid="bib88">Tavanaei et al., 2019</xref>). Due to the considerably slower speeds observed and the absence of a publicly available implementation of a decision-making network model using NEURON and NEST, we have excluded them from this benchmark test. Additionally, we did not include a comparison with GeNN because Brian2GeNN does not support the translation of the advanced Brian2 feature employed in this model. Our evaluation showcases that Brian2, Brian2CUDA, and BrainPy exhibit comparable performance on networks of small sizes. However, BrainPy demonstrated substantially better scalability on larger network sizes (see <xref ref-type="fig" rid="fig8">Figure 8C</xref> and <xref ref-type="fig" rid="fig8">Figure 8F</xref>). For these types of simulation workloads with dense connectivity, TPUs significantly outperformed CPUs and GPUs. Since TPUs primarily utilize low-precision floating point (especially floating point with 16 bits) and are less optimized for double precision, we only tested the model with single-precision operations. Our evaluations clearly showcase the excellent scalability of the network as the size increases (refer to the GPU and TPU comparison in <xref ref-type="fig" rid="fig8">Figure 8F</xref>).</p></sec></sec><sec id="s2-6"><title>Extensible architecture of BrainPy</title><p>Brain science, as well as brain dynamics modeling, is progressing rapidly. Along with the gigantic projects on brain research worldwide, new data and knowledge about brain structures and functions are constantly emerging, which impose new demands on brain dynamics modeling frequently, including, for instance, the simulation and analysis of large-size neural circuits, and the training of neural models based on recorded neural data. To be a general-purpose brain dynamics programming framework, the architecture of the framework must be extensible to conveniently take up new advances in the field. Current brain simulators based on descriptive languages have difficulty achieving this goal, since the extension of a new component through the descriptive interface needs to be done in both high- and low-level programming languages (Appendix 1). Through the elaborate architecture design, BrainPy enables easy extension with new infrastructure, new utility functions, and new machine-learning methods, all performed in our convenient Python interface.</p><p>First, for infrastructure (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), BrainPy provides a convenient way of customizing a new tool by defining a new subclass. For example, a new RungeâKutta integrator can be created by inheriting from <monospace>brainpy.ode.ExplicitRKIntegrator</monospace> and specifying the Butcher tableau; a new connector can be implemented by deriving from <monospace>brainpy.conn.TwoEndConnector</monospace> and overriding initialization function and connection building function (see Appendix 7 for details). Since models and modeling methods have not yet been standardized in the field, the abstraction and summarization of primitive operators for brain dynamics modeling are largely lacking. Although BrainPy has provided dozens of dedicated operators, it would be too soon to establish a complete operator library for brain dynamics modeling. To simplify the process of operator customization, BrainPy provides the <monospace>brainpy.math.CustomOpByNumba</monospace> interface that allows users to write and register an operator directly with Python syntax. Specifically, to customize a primitive operator, users need to subclass <monospace>CustomOpByNumba</monospace> and implement two Python functions: the abstract evaluation function <monospace>eval_shape()</monospace> and concrete computation function <monospace>con_compute()</monospace> (see Appendix 8 for more information). Notably, this approach differs from the operator customization in most DNN frameworks, in which low-level operators must be implemented through C++ code. We confirmed that operators customized through the BrainPy interface have comparable and even better performance than those written in C++ (please refer to <xref ref-type="fig" rid="fig9">Figure 9</xref> for the results and Appendix 8 for the source code for comparison).</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>The speed comparison of event-based operators customized by C++ XLA custom call and our Python-level registration interface.</title><p>âC++ Operatorâ presents the simulation time of a COBA network using the event-based operator coded by C++, and âPython Operatorâ shows the simulation speed of the network that is implemented through our operator registered by the Python interface.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-fig9-v2.tif"/></fig><p>Second, for functional modules (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), BrainPy enables an extension of a new module with BrainPy infrastructure, as the latter can be arbitrarily fused, chained, or combined to create new functions. For example, an analysis toolkit can be customized with BrainPy operators. Moreover, all customizations in BrainPy can benefit from the acceleration of JIT compilation, and usersâ attention only needs to focus on the functionalities they require.</p><p>Third, for interactions with AI, BrainPy supports the easy extension of new machine-learning methods. Machine-learning approaches are becoming important tools for brain dynamics modeling (<xref ref-type="bibr" rid="bib78">Saxe et al., 2021</xref>). Existing brain simulators have difficulty incorporating the latest advances in machine-learning research (Appendix 1). Built on top of JAX, BrainPy has the inherent advantage of being linked to the latest developments in machine learning. We noticed that JAX has a rich ecosystem of machine learning, including DNNs, graph neural networks, reinforcement learning, and probabilistic programming. To integrate this rich ecosystem as part of the usersâ program, BrainPy is designed to be compatible with other JAX libraries. First, the object-oriented transformations in BrainPy can be applied to pure functions, thus enabling most JAX libraries with a functional programming style to be directly used as a part of the BrainPy program. Second, users can transform models in other libraries as BrainPy objects. For example, using <monospace>brainpy.dnn.FromFlax</monospace>, users can treat any artificial neural network model in Flax (<xref ref-type="bibr" rid="bib39">Heek, 2020</xref>) as a BrainPy module. Alternatively, users can convert a BrainPy model into a format that is compatible with other JAX libraries. For instance, <monospace>brainpy.dnn.ToFlax</monospace> supports interpreting a dynamical system in BrainPy as a Flax recurrent cell, so that brain models in BrainPy can also be used in a machine-learning context.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The field of brain dynamics modeling has long been constrained by a lack of general-purpose programming frameworks that can support users to freely define brain dynamics models across multiple scales, comprehensively perform simulation, optimization, and analysis of the built models, and conveniently prototype new modeling methods. To address this challenge, we have developed BrainPy, a general-purpose programming framework for brain dynamics modeling. With a combined focus on usability, performance, functionality, and extensibility, BrainPy offers a number of appealing properties, including:</p><list list-type="bullet"><list-item><p><italic>Pythonic programming</italic>. In contrast to other brain simulators (<xref ref-type="bibr" rid="bib31">Gewaltig and Diesmann, 2007</xref>; <xref ref-type="bibr" rid="bib22">Davison et al., 2008</xref>; <xref ref-type="bibr" rid="bib9">Beyeler et al., 2015</xref>; <xref ref-type="bibr" rid="bib82">Stimberg et al., 2019</xref>; <xref ref-type="bibr" rid="bib40">Hines and Carnevale, 1997</xref>; <xref ref-type="bibr" rid="bib25">Dura-Bernal et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Dai et al., 2020a</xref>; <xref ref-type="bibr" rid="bib35">Goodman, 2010</xref>; <xref ref-type="bibr" rid="bib11">Blundell et al., 2018</xref>; <xref ref-type="bibr" rid="bib89">Tikidji-Hamburyan et al., 2017</xref>), BrainPy enables Pythonic programming. It allows users to implement and control their models directly using native Python syntax, implicating high transparency to users. This transparency is crucial for research, as standard Python debugging tools can be integrated into the implementation process of novel models, and is also appealing for education.</p></list-item><list-item><p><italic>Integrative platform</italic>. BrainPy allows unprecedentedly integrated studying of brain dynamics models. Its multi-scale model-building interface facilitates the construction of data-driven models based on the structural, functional, or cellular data (<xref ref-type="bibr" rid="bib72">Potjans and Diesmann, 2014</xref>), while its diverse model training supports enable to training brain dynamics models based on cognitive tasks that can be used to evaluate or optimize models of different brain functions (<xref ref-type="bibr" rid="bib78">Saxe et al., 2021</xref>). BrainPy provides the first step toward an integrative framework supporting comprehensive brain modeling across different organization levels and problem dimensions (<xref ref-type="bibr" rid="bib21">DâAngelo and Jirsa, 2022</xref>).</p></list-item><list-item><p><italic>Intrinsic flexibility</italic>. Inspired by the success of general-purpose programming in Deep Learning (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>; <xref ref-type="bibr" rid="bib66">Paszke et al., 2019</xref>), BrainPy provides not only functional libraries but also infrastructure. This is essential for users to create models and modeling approaches beyond the predefined assumptions of existing libraries.</p></list-item><list-item><p><italic>Efficient performance</italic>. One of the key strengths of BrainPy lies in its ability to compile models defined in the framework into binary instructions for various devices, including CPU, GPU, and TPU. This compilation process ensures high-running performance comparable to native C or CUDA, enabling researchers to efficiently execute their models.</p></list-item><list-item><p><italic>Extensible architecture</italic>. BrainPy features an extensible architecture. New primitive operators, utilities, functional modules, machine-learning approaches, etc., can be easily customized through our Python interface.</p></list-item></list><sec id="s3-1"><title>Limitations</title><p>While BrainPyâs native Python-based object-oriented programming paradigm confers numerous advantages compared to existing brain simulators, this novel programming approach also imposes certain limitations that must be acknowledged.</p><p>Most existing brain simulators employ a domain-specific language to define brain dynamics models. For example, Brian2 (<xref ref-type="bibr" rid="bib82">Stimberg et al., 2019</xref>) designs an equation-oriented specification that can describe a wide variety of neural models; NeuroML (<xref ref-type="bibr" rid="bib17">Cannon et al., 2014</xref>) employs an XML-based specification that facilitates the sharing and reuse of neuronal models; NetPyNE (<xref ref-type="bibr" rid="bib25">Dura-Bernal et al., 2019</xref>) utilizes a high-level JSON-compatible format composed of Python lists and dictionaries to support multi-scale neuronal modeling; BMTK (<xref ref-type="bibr" rid="bib19">Dai et al., 2020a</xref>) similarly employs a JSON-based language built on the SONATA file format (<xref ref-type="bibr" rid="bib20">Dai et al., 2020b</xref>) to deliver consistent multi-resolution experiences via integration with established tools like NEURON and NEST. This declarative programming approach benefits from a clear separation between the mathematical model description and its computational realization. It frees users from low-level implementation details, and enables intuitive specification of complex models in a concise and semantically clear manner. In contrast, the object-oriented programming used in BrainPy exposes the implementation details to users, and adds some complexity to the code. For example, users should be aware of the differences between dense and sparse connectivity schemes, online or offline training schemes, nonbatch or batch computing modes, etc.</p><p>The current objectives of BrainPy center on enabling an integrative platform for simulating, training, and analyzing large-scale brain network models while retaining biologically relevant details. Incorporating excessive biological details would be extremely computationally expensive and difficult for such integration. Consequently, detailed spatial modeling with complex compartmental dynamics, as facilitated by tools like NEURON (<xref ref-type="bibr" rid="bib40">Hines and Carnevale, 1997</xref>) and Arbor (<xref ref-type="bibr" rid="bib3">Akar et al., 2019</xref>), exceeds BrainPyâs present scope. Moreover, in order to solve the governing partial differential equations, implicit numerical methods (e.g., CrankâNicolson, implicit Euler) are often essential for stable multi-compartment model simulation. As BrainPy does not currently support fully implicit solvers, it is ill suited to the needs and preferences of modelers focused on multi-compartment dynamics in its current form. Our emphasis remains on balancing biological fidelity and computational tractability for large-scale network modeling and training.</p><p>Based on the GSPMD mechanism of the XLA compiler (<xref ref-type="bibr" rid="bib103">Xu et al., 2021</xref>), the current version of BrainPy supports various parallelism paradigms, such as data parallelism and model parallelism. Data parallelism involves dividing the training data across multiple devices, where each device independently computes and updates the model parameters using its assigned data subset. On the other hand, model parallelism entails partitioning the model across multiple devices, with each device responsible for computing a specific portion of the model computations. These parallelism paradigms are particularly applicable to brain dynamics models with dense connections or structured sparsity. However, the GSPMD parallelism mechanism is not straightforwardly applicable to sparse spiking neural networks, and requires non-trivial changes to support sparse computations. Therefore, another limitation of the current BrainPy framework is that it does not support the general parallelization of sparse spiking neural network models on multiple computing devices. State-of-the-art brain simulators now offer powerful parallelization capabilities for simulating large-scale SNNs. For instance, NEST (<xref ref-type="bibr" rid="bib31">Gewaltig and Diesmann, 2007</xref>) and NEURON (<xref ref-type="bibr" rid="bib40">Hines and Carnevale, 1997</xref>) simulators provide convenient and efficient commands through the MPI interface to define, connect, and execute large-scale networks. However, the array-based data structure in BrainPy requires a different approach to parallelize spiking neural networks.</p></sec><sec id="s3-2"><title>Future works</title><p>Although BrainPy offers substantial capabilities for brain dynamics modeling, fulfilling all demands in this domain will require large efforts for further ecosystem development.</p><p>First, supporting the efficient implementation of multi-compartment neuron models is needed to enable biologically detailed modeling at the subcellular level (<xref ref-type="bibr" rid="bib70">Poirazi and Papoutsi, 2020</xref>). Multi-compartment neurons incorporate complex dendritic morphologies and spatially distributed ion channels that more precisely capture neural information processing. A substantial number of studies have demonstrated that dendritic mechanisms convey significant advantages to simplified network models of varying levels of abstraction (<xref ref-type="bibr" rid="bib12">Bono and Clopath, 2017</xref>; <xref ref-type="bibr" rid="bib55">Legenstein and Maass, 2011</xref>; <xref ref-type="bibr" rid="bib102">Wu et al., 2018</xref>). Efficiently implementing such models in BrainPy could significantly advance detailed biophysical modeling and bridge the machine-learning-oriented SNN models.</p><p>Second, developing parallel primitive operators and memory-efficient algorithms will be critical for ultra-large-scale brain simulations approaching biological realism (&gt;billions of neurons). Massive parallelization across multiple computing devices is currently the main approach to achieve such scale. For instance, the NEST simulator uses optimized data structures and algorithms (<xref ref-type="bibr" rid="bib49">Kunkel et al., 2011</xref>; <xref ref-type="bibr" rid="bib50">Kunkel et al., 2014</xref>; <xref ref-type="bibr" rid="bib44">Jordan et al., 2018</xref>) to enable large-scale simulation on supercomputers and clusters. Moving forward, a priority for BrainPy will be parallelizing its array-based data structures to simulate gigantic brain models across multiple nodes. Moreover, rather than solving large-scale networks exactly, BrainPy aims to find approximating algorithms that overcome the <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> complexity, permitting very large-scale modeling on much less computing devices.</p><p>Third, integrating BrainPy models with modern accelerators and neuromorphic computing systems (<xref ref-type="bibr" rid="bib79">Schuman et al., 2017</xref>) could offer a more efficient and scalable approach for simulating large-scale brain dynamics models on cutting-edge hardware accelerators. On the one hand, the implementation of sparse and event-driven operators is necessary for TPUs. While TPUs have demonstrated promising performance and efficiency for machine-learning workloads, our experiments indicate that they are less efficient than GPUs when simulating sparse biological brain network models (see <xref ref-type="fig" rid="app11fig8">Appendix 11âfigure 8</xref>). This inefficiency is primarily due to the lack of dedicated operators for sparse and event-driven brain computations in current TPUs. In the future, we plan to explore the development of TPU kernels to enable scalable and efficient brain dynamics programming on TPU hardware accelerators. On the other hand, neuromorphic systems incorporate custom analog circuits that mimic neurobiological architectures and dynamics, resulting in significantly higher power efficiency compared to conventional digital hardware. By mapping BrainPy models onto neuromorphic platforms, simulations can be accelerated, and large-scale models can be executed efficiently. However, the development of translation tools and mapping optimizations is necessary to fully harness the potential of these systems.</p><p>By addressing these limitations and enhancing BrainPyâs capabilities in these areas, we can further advance its goal of serving as a comprehensive programming framework for modeling brain dynamics. This will enable users to delve into the dynamics of brain or brain-inspired models that combine biological insights with machine learning. The BrainPy team encourages collaboration with the research community to expand this modeling ecosystem and facilitate a deeper understanding of brain dynamics.</p></sec></sec></body><back><sec sec-type="additional-information" id="s4"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Visualization, Methodology</p></fn><fn fn-type="con" id="con3"><p>Software, Visualization, Methodology</p></fn><fn fn-type="con" id="con4"><p>Software</p></fn><fn fn-type="con" id="con5"><p>Validation</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Supervision, Funding acquisition, Writing - original draft, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s5"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-86365-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s6"><title>Data availability</title><p>BrainPy is distributed via the pypi package index (<ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/brainpy/">https://pypi.org/project/brainpy/</ext-link>) and is publicly released on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/brainpy/BrainPy/">https://github.com/brainpy/BrainPy/</ext-link>; <xref ref-type="bibr" rid="bib98">Wang et al., 2024</xref>) under the license of GNU General Public License v3.0. Its documentation is hosted on the free documentation hosting platform Read the Docs (<ext-link ext-link-type="uri" xlink:href="https://brainpy.readthedocs.io/">https://brainpy.readthedocs.io/</ext-link>). Rich examples and illustrations of BrainPy are publicly available at the website of <ext-link ext-link-type="uri" xlink:href="https://brainpy-examples.readthedocs.io/">https://brainpy-examples.readthedocs.io/</ext-link>. The source codes of these examples are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/brainpy/examples/">https://github.com/brainpy/examples/</ext-link> (<xref ref-type="bibr" rid="bib96">Wang, 2023</xref>). All the codes to reproduce the results in the paper can be found at the following GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/brainpy/brainpy-elife-reproducibility/">https://github.com/brainpy/brainpy-elife-reproducibility/</ext-link> (copy archived at <xref ref-type="bibr" rid="bib97">Wang, 2024</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by Science and Technology Innovation 2030-Brain Science and Brain-inspired Intelligence Project (No. 2021ZD0200204) and Beijing Academy of Artificial Intelligence. We would like to express our sincere gratitude to Marcel Stimberg for his valuable insights and assistance with benchmarking brain simulators, which greatly contributed to this paper. We would like to acknowledge Xiaohan Lin, Yifeng Gong, Hongyaoxing Gu, Linfei Lu, Xiaolong Zou, Zhiyu Zhao, Yingqian Jiang, Xinyu Liu, and all other members of the Wu laboratory for their helpful discussions. We thank all GitHub users who contributed codes to BrainPy.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name><name><surname>Barham</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Davis</surname><given-names>A</given-names></name><name><surname>Dean</surname><given-names>J</given-names></name><name><surname>Devin</surname><given-names>M</given-names></name><name><surname>Ghemawat</surname><given-names>S</given-names></name><name><surname>Irving</surname><given-names>G</given-names></name><name><surname>Isard</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>TensorFlow: A System for Large-Scale Machine Learning</article-title><conf-name>In: 12th USENIX symposium on operating systems design and implementation (OSDI</conf-name><fpage>265</fpage><lpage>283</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Lapicqueâs introduction of the integrate-and-fire model neuron (1907)</article-title><source>Brain Research Bulletin</source><volume>50</volume><fpage>303</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1016/s0361-9230(99)00161-6</pub-id><pub-id pub-id-type="pmid">10643408</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Akar</surname><given-names>NA</given-names></name><name><surname>Cumming</surname><given-names>B</given-names></name><name><surname>Karakasis</surname><given-names>V</given-names></name><name><surname>Kusters</surname><given-names>A</given-names></name><name><surname>Klijn</surname><given-names>W</given-names></name><name><surname>Peyser</surname><given-names>A</given-names></name><name><surname>Yates</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Arbor â A Morphologically-Detailed Neural Network Simulation Library for Contemporary High-Performance Computing Architectures</article-title><conf-name>2019 27th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP</conf-name><conf-loc>Pavia, Italy</conf-loc><fpage>274</fpage><lpage>282</lpage><pub-id pub-id-type="doi">10.1109/EMPDP.2019.8671560</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alevi</surname><given-names>D</given-names></name><name><surname>Stimberg</surname><given-names>M</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name><name><surname>Obermayer</surname><given-names>K</given-names></name><name><surname>Augustin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Brian2CUDA: Flexible and efficient simulation of spiking neural network models on GPUs</article-title><source>Frontiers in Neuroinformatics</source><volume>16</volume><elocation-id>883700</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2022.883700</pub-id><pub-id pub-id-type="pmid">36387586</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amunts</surname><given-names>K</given-names></name><name><surname>Ebell</surname><given-names>C</given-names></name><name><surname>Muller</surname><given-names>J</given-names></name><name><surname>Telefont</surname><given-names>M</given-names></name><name><surname>Knoll</surname><given-names>A</given-names></name><name><surname>Lippert</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The human brain project: creating a european research infrastructure to decode the human brain</article-title><source>Neuron</source><volume>92</volume><fpage>574</fpage><lpage>581</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.046</pub-id><pub-id pub-id-type="pmid">27809997</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aycock</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A brief history of just-in-time</article-title><source>ACM Computing Surveys</source><volume>35</volume><fpage>97</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1145/857076.857077</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azevedo Carvalho</surname><given-names>N</given-names></name><name><surname>Contassot-Vivier</surname><given-names>S</given-names></name><name><surname>Buhry</surname><given-names>L</given-names></name><name><surname>Martinez</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Simulation of large scale neural models with event-driven connectivity generation</article-title><source>Frontiers in Neuroinformatics</source><volume>14</volume><elocation-id>522000</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2020.522000</pub-id><pub-id pub-id-type="pmid">33154719</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellec</surname><given-names>G</given-names></name><name><surname>Scherr</surname><given-names>F</given-names></name><name><surname>Subramoney</surname><given-names>A</given-names></name><name><surname>Hajek</surname><given-names>E</given-names></name><name><surname>Salaj</surname><given-names>D</given-names></name><name><surname>Legenstein</surname><given-names>R</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A solution to the learning dilemma for recurrent networks of spiking neurons</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3625</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17236-y</pub-id><pub-id pub-id-type="pmid">32681001</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Beyeler</surname><given-names>M</given-names></name><name><surname>Carlson</surname><given-names>KD</given-names></name><name><surname>Dutt</surname><given-names>N</given-names></name><name><surname>Krichmar</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>CARLsim 3: A user-friendly and highly optimized library for the creation of neurobiologically detailed spiking neural networks</article-title><conf-name>2015 International Joint Conference on Neural Networks (IJCNN</conf-name><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1109/IJCNN.2015.7280424</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bezanson</surname><given-names>J</given-names></name><name><surname>Edelman</surname><given-names>A</given-names></name><name><surname>Karpinski</surname><given-names>S</given-names></name><name><surname>Shah</surname><given-names>VB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Julia: A fresh approach to numerical computing</article-title><source>SIAM Review</source><volume>59</volume><fpage>65</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1137/141000671</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blundell</surname><given-names>I</given-names></name><name><surname>Brette</surname><given-names>R</given-names></name><name><surname>Cleland</surname><given-names>TA</given-names></name><name><surname>Close</surname><given-names>TG</given-names></name><name><surname>Coca</surname><given-names>D</given-names></name><name><surname>Davison</surname><given-names>AP</given-names></name><name><surname>Diaz-Pier</surname><given-names>S</given-names></name><name><surname>Fernandez Musoles</surname><given-names>C</given-names></name><name><surname>Gleeson</surname><given-names>P</given-names></name><name><surname>Goodman</surname><given-names>DFM</given-names></name><name><surname>Hines</surname><given-names>M</given-names></name><name><surname>Hopkins</surname><given-names>MW</given-names></name><name><surname>Kumbhar</surname><given-names>P</given-names></name><name><surname>Lester</surname><given-names>DR</given-names></name><name><surname>Marin</surname><given-names>B</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name><name><surname>MÃ¼ller</surname><given-names>E</given-names></name><name><surname>Nowotny</surname><given-names>T</given-names></name><name><surname>Peyser</surname><given-names>A</given-names></name><name><surname>Plotnikov</surname><given-names>D</given-names></name><name><surname>Richmond</surname><given-names>P</given-names></name><name><surname>Rowley</surname><given-names>A</given-names></name><name><surname>Rumpe</surname><given-names>B</given-names></name><name><surname>Stimberg</surname><given-names>M</given-names></name><name><surname>Stokes</surname><given-names>AB</given-names></name><name><surname>Tomkins</surname><given-names>A</given-names></name><name><surname>Trensch</surname><given-names>G</given-names></name><name><surname>Woodman</surname><given-names>M</given-names></name><name><surname>Eppler</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Code generation in computational neuroscience: a review of tools and techniques</article-title><source>Frontiers in Neuroinformatics</source><volume>12</volume><elocation-id>68</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2018.00068</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bono</surname><given-names>J</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Modeling somatic and dendritic spike mediated plasticity at the single neuron and network level</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>706</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-00740-z</pub-id><pub-id pub-id-type="pmid">28951585</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname><given-names>R</given-names></name><name><surname>Rudolph</surname><given-names>M</given-names></name><name><surname>Carnevale</surname><given-names>T</given-names></name><name><surname>Hines</surname><given-names>M</given-names></name><name><surname>Beeman</surname><given-names>D</given-names></name><name><surname>Bower</surname><given-names>JM</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name><name><surname>Goodman</surname><given-names>PH</given-names></name><name><surname>Harris</surname><given-names>FC</given-names></name><name><surname>Zirpe</surname><given-names>M</given-names></name><name><surname>NatschlÃ¤ger</surname><given-names>T</given-names></name><name><surname>Pecevski</surname><given-names>D</given-names></name><name><surname>Ermentrout</surname><given-names>B</given-names></name><name><surname>Djurfeldt</surname><given-names>M</given-names></name><name><surname>Lansner</surname><given-names>A</given-names></name><name><surname>Rochel</surname><given-names>O</given-names></name><name><surname>Vieville</surname><given-names>T</given-names></name><name><surname>Muller</surname><given-names>E</given-names></name><name><surname>Davison</surname><given-names>AP</given-names></name><name><surname>El Boustani</surname><given-names>S</given-names></name><name><surname>Destexhe</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Simulation of networks of spiking neurons: a review of tools and strategies</article-title><source>Journal of Computational Neuroscience</source><volume>23</volume><fpage>349</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1007/s10827-007-0038-6</pub-id><pub-id pub-id-type="pmid">17629781</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname><given-names>R</given-names></name><name><surname>Goodman</surname><given-names>DFM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Vectorized algorithms for spiking neural network simulation</article-title><source>Neural Computation</source><volume>23</volume><fpage>1503</fpage><lpage>1535</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00123</pub-id><pub-id pub-id-type="pmid">21395437</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The analysis of visual motion: a comparison of neuronal and psychophysical performance</article-title><source>The Journal of Neuroscience</source><volume>12</volume><fpage>4745</fpage><lpage>4765</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.12-12-04745.1992</pub-id><pub-id pub-id-type="pmid">1464765</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cakan</surname><given-names>C</given-names></name><name><surname>Jajcay</surname><given-names>N</given-names></name><name><surname>Obermayer</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neurolib: A simulation framework for whole-brain neural mass modeling</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.02.18.431886</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cannon</surname><given-names>RC</given-names></name><name><surname>Gleeson</surname><given-names>P</given-names></name><name><surname>Crook</surname><given-names>S</given-names></name><name><surname>Ganapathy</surname><given-names>G</given-names></name><name><surname>Marin</surname><given-names>B</given-names></name><name><surname>Piasini</surname><given-names>E</given-names></name><name><surname>Silver</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>LEMS: a language for expressing complex biological models in concise and hierarchical form and its use in underpinning NeuroML 2</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>79</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00079</pub-id><pub-id pub-id-type="pmid">25309419</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>TS</given-names></name><name><surname>Kashyap</surname><given-names>HJ</given-names></name><name><surname>Xing</surname><given-names>J</given-names></name><name><surname>Listopad</surname><given-names>S</given-names></name><name><surname>Rounds</surname><given-names>EL</given-names></name><name><surname>Beyeler</surname><given-names>M</given-names></name><name><surname>Dutt</surname><given-names>N</given-names></name><name><surname>Krichmar</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>CARLsim 4: An open source library for large scale, biologically detailed spiking neural network simulation using heterogeneous clusters</article-title><conf-name>2018 International Joint Conference on Neural Networks</conf-name><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1109/IJCNN.2018.8489326</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>K</given-names></name><name><surname>Gratiy</surname><given-names>SL</given-names></name><name><surname>Billeh</surname><given-names>YN</given-names></name><name><surname>Xu</surname><given-names>R</given-names></name><name><surname>Cai</surname><given-names>B</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Rimehaug</surname><given-names>AE</given-names></name><name><surname>Stasik</surname><given-names>AJ</given-names></name><name><surname>Einevoll</surname><given-names>GT</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Arkhipov</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Brain Modeling ToolKit: An open source software suite for multiscale modeling of brain circuits</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008386</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008386</pub-id><pub-id pub-id-type="pmid">33253147</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>K</given-names></name><name><surname>Hernando</surname><given-names>J</given-names></name><name><surname>Billeh</surname><given-names>YN</given-names></name><name><surname>Gratiy</surname><given-names>SL</given-names></name><name><surname>Planas</surname><given-names>J</given-names></name><name><surname>Davison</surname><given-names>AP</given-names></name><name><surname>Dura-Bernal</surname><given-names>S</given-names></name><name><surname>Gleeson</surname><given-names>P</given-names></name><name><surname>Devresse</surname><given-names>A</given-names></name><name><surname>Dichter</surname><given-names>BK</given-names></name><name><surname>Gevaert</surname><given-names>M</given-names></name><name><surname>King</surname><given-names>JG</given-names></name><name><surname>Van Geit</surname><given-names>WAH</given-names></name><name><surname>Povolotsky</surname><given-names>AV</given-names></name><name><surname>Muller</surname><given-names>E</given-names></name><name><surname>Courcol</surname><given-names>J-D</given-names></name><name><surname>Arkhipov</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>The SONATA data format for efficient description of large-scale network models</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007696</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007696</pub-id><pub-id pub-id-type="pmid">32092054</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DâAngelo</surname><given-names>E</given-names></name><name><surname>Jirsa</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The quest for multiscale brain modeling</article-title><source>Trends in Neurosciences</source><volume>45</volume><fpage>777</fpage><lpage>790</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2022.06.007</pub-id><pub-id pub-id-type="pmid">35906100</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davison</surname><given-names>AP</given-names></name><name><surname>BrÃ¼derle</surname><given-names>D</given-names></name><name><surname>Eppler</surname><given-names>JM</given-names></name><name><surname>Kremkow</surname><given-names>J</given-names></name><name><surname>Muller</surname><given-names>E</given-names></name><name><surname>Pecevski</surname><given-names>D</given-names></name><name><surname>Perrinet</surname><given-names>L</given-names></name><name><surname>Yger</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>PyNN: A common interface for neuronal network simulators</article-title><source>Frontiers in Neuroinformatics</source><volume>2</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.11.011.2008</pub-id><pub-id pub-id-type="pmid">19194529</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dean</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A golden decade of deep learning: computing systems &amp; applications</article-title><source>Daedalus</source><volume>151</volume><fpage>58</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1162/daed_a_01900</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dubois</surname><given-names>PF</given-names></name><name><surname>Hinsen</surname><given-names>K</given-names></name><name><surname>Hugunin</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Numerical python</article-title><source>Computers in Physics</source><volume>10</volume><elocation-id>262</elocation-id><pub-id pub-id-type="doi">10.1063/1.4822400</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dura-Bernal</surname><given-names>S</given-names></name><name><surname>Suter</surname><given-names>BA</given-names></name><name><surname>Gleeson</surname><given-names>P</given-names></name><name><surname>Cantarelli</surname><given-names>M</given-names></name><name><surname>Quintana</surname><given-names>A</given-names></name><name><surname>Rodriguez</surname><given-names>F</given-names></name><name><surname>Kedziora</surname><given-names>DJ</given-names></name><name><surname>Chadderdon</surname><given-names>GL</given-names></name><name><surname>Kerr</surname><given-names>CC</given-names></name><name><surname>Neymotin</surname><given-names>SA</given-names></name><name><surname>McDougal</surname><given-names>RA</given-names></name><name><surname>Hines</surname><given-names>M</given-names></name><name><surname>Shepherd</surname><given-names>GM</given-names></name><name><surname>Lytton</surname><given-names>WW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>NetPyNE, a tool for data-driven multiscale modeling of brain circuits</article-title><source>eLife</source><volume>8</volume><elocation-id>e44494</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.44494</pub-id><pub-id pub-id-type="pmid">31025934</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fidjeland</surname><given-names>AK</given-names></name><name><surname>Roesch</surname><given-names>EB</given-names></name><name><surname>Shanahan</surname><given-names>MP</given-names></name><name><surname>Luk</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>20th IEEE international conference on application-specific systems</article-title><conf-name>Architectures and processors (ASAP)</conf-name><fpage>137</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1109/ASAP.2009.24</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finkelstein</surname><given-names>A</given-names></name><name><surname>Fontolan</surname><given-names>L</given-names></name><name><surname>Economo</surname><given-names>MN</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Attractor dynamics gate cortical information flow during decision-making</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>843</fpage><lpage>850</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00840-6</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitzhugh</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>Impulses and physiological states in theoretical models of nerve membrane</article-title><source>Biophysical Journal</source><volume>1</volume><fpage>445</fpage><lpage>466</lpage><pub-id pub-id-type="doi">10.1016/s0006-3495(61)86902-6</pub-id><pub-id pub-id-type="pmid">19431309</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frostig</surname><given-names>R</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Leary</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Compiling machine learning programs via high-level tracing</article-title><source>Systems for Machine Learning</source><fpage>23</fpage><lpage>24</lpage></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gauthier</surname><given-names>DJ</given-names></name><name><surname>Bollt</surname><given-names>E</given-names></name><name><surname>Griffith</surname><given-names>A</given-names></name><name><surname>Barbosa</surname><given-names>WAS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Next generation reservoir computing</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>5564</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-25801-2</pub-id><pub-id pub-id-type="pmid">34548491</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gewaltig</surname><given-names>MO</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>NEST (NEural Simulation Tool)</article-title><source>Scholarpedia</source><volume>2</volume><elocation-id>1430</elocation-id><pub-id pub-id-type="doi">10.4249/scholarpedia.1430</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gleeson</surname><given-names>P</given-names></name><name><surname>Crook</surname><given-names>S</given-names></name><name><surname>Cannon</surname><given-names>RC</given-names></name><name><surname>Hines</surname><given-names>ML</given-names></name><name><surname>Billings</surname><given-names>GO</given-names></name><name><surname>Farinella</surname><given-names>M</given-names></name><name><surname>Morse</surname><given-names>TM</given-names></name><name><surname>Davison</surname><given-names>AP</given-names></name><name><surname>Ray</surname><given-names>S</given-names></name><name><surname>Bhalla</surname><given-names>US</given-names></name><name><surname>Barnes</surname><given-names>SR</given-names></name><name><surname>Dimitrova</surname><given-names>YD</given-names></name><name><surname>Silver</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>NeuroML: a language for describing data driven models of neurons and networks with a high degree of biological detail</article-title><source>PLOS Computational Biology</source><volume>6</volume><elocation-id>e1000815</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000815</pub-id><pub-id pub-id-type="pmid">20585541</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golosio</surname><given-names>B</given-names></name><name><surname>Tiddia</surname><given-names>G</given-names></name><name><surname>De Luca</surname><given-names>C</given-names></name><name><surname>Pastorelli</surname><given-names>E</given-names></name><name><surname>Simula</surname><given-names>F</given-names></name><name><surname>Paolucci</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Fast simulations of highly-connected spiking cortical models using GPUs</article-title><source>Frontiers in Computational Neuroscience</source><volume>15</volume><elocation-id>627620</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2021.627620</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname><given-names>DF</given-names></name><name><surname>Brette</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Brian: a simulator for spiking neural networks in python</article-title><source>Frontiers in Neuroinformatics</source><volume>2</volume><elocation-id>5</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.11.005.2008</pub-id><pub-id pub-id-type="pmid">19115011</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname><given-names>DFM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Code generation: a strategy for neural network simulators</article-title><source>Neuroinformatics</source><volume>8</volume><fpage>183</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1007/s12021-010-9082-x</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Grcevski</surname><given-names>N</given-names></name><name><surname>Kielstra</surname><given-names>A</given-names></name><name><surname>Stoodley</surname><given-names>K</given-names></name><name><surname>Stoodley</surname><given-names>MG</given-names></name><name><surname>Sundaresan</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Java Just-in-Time Compiler and Virtual Machine Improvements for Server and Middleware Applications</article-title><conf-name>Virtual Machine Research and Technology Symposium</conf-name><fpage>151</fpage><lpage>162</lpage></element-citation></ref><ref id="bib37"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Hagberg</surname><given-names>A</given-names></name><name><surname>Swart</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>Exploring network structure, dynamics, and function using NetworkX</source><publisher-name>Los Alamos National Lab</publisher-name><publisher-loc>Los Alamos</publisher-loc></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Picus</surname><given-names>M</given-names></name><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>van Kerkwijk</surname><given-names>MH</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Haldane</surname><given-names>A</given-names></name><name><surname>Del RÃ­o</surname><given-names>JF</given-names></name><name><surname>Wiebe</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>GÃ©rard-Marchant</surname><given-names>P</given-names></name><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Abbasi</surname><given-names>H</given-names></name><name><surname>Gohlke</surname><given-names>C</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Array programming with NumPy</article-title><source>Nature</source><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Heek</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Google/flex</data-title><source>Github</source><ext-link ext-link-type="uri" xlink:href="http://github.com/google/flax">http://github.com/google/flax</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname><given-names>ML</given-names></name><name><surname>Carnevale</surname><given-names>NT</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The NEURON simulation environment</article-title><source>Neural Computation</source><volume>9</volume><fpage>1179</fpage><lpage>1209</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.6.1179</pub-id><pub-id pub-id-type="pmid">9248061</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Matplotlib: A 2D graphics environment</article-title><source>Computing in Science &amp; Engineering</source><volume>9</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izhikevich</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Simple model of spiking neurons</article-title><source>IEEE Transactions on Neural Networks</source><volume>14</volume><fpage>1569</fpage><lpage>1572</lpage><pub-id pub-id-type="doi">10.1109/TNN.2003.820440</pub-id><pub-id pub-id-type="pmid">18244602</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaeger</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Echo state network</article-title><source>Scholarpedia</source><volume>2</volume><elocation-id>2330</elocation-id><pub-id pub-id-type="doi">10.4249/scholarpedia.2330</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jordan</surname><given-names>J</given-names></name><name><surname>Ippen</surname><given-names>T</given-names></name><name><surname>Helias</surname><given-names>M</given-names></name><name><surname>Kitayama</surname><given-names>I</given-names></name><name><surname>Sato</surname><given-names>M</given-names></name><name><surname>Igarashi</surname><given-names>J</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name><name><surname>Kunkel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Corrigendum: extremely scalable spiking neuronal network simulation code: from laptops to exascale computers</article-title><source>Frontiers in Neuroinformatics</source><volume>12</volume><elocation-id>34</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2018.00034</pub-id><pub-id pub-id-type="pmid">30008668</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jorgenson</surname><given-names>LA</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Bargmann</surname><given-names>CI</given-names></name><name><surname>Brown</surname><given-names>EN</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name><name><surname>Hudson</surname><given-names>KL</given-names></name><name><surname>Ling</surname><given-names>GSF</given-names></name><name><surname>MacLeish</surname><given-names>PR</given-names></name><name><surname>Marder</surname><given-names>E</given-names></name><name><surname>Normann</surname><given-names>RA</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name><name><surname>Tsien</surname><given-names>RY</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Wingfield</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The BRAIN Initiative: developing technology to catalyse neuroscience discovery</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>370</volume><elocation-id>20140164</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2014.0164</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaslik</surname><given-names>E</given-names></name><name><surname>Sivasundaram</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Nonlinear dynamics and chaos in fractional-order neural networks</article-title><source>Neural Networks</source><volume>32</volume><fpage>245</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2012.02.030</pub-id><pub-id pub-id-type="pmid">22386788</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knight</surname><given-names>JC</given-names></name><name><surname>Nowotny</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Larger GPU-accelerated brain simulations with procedural connectivity</article-title><source>Nature Computational Science</source><volume>1</volume><fpage>136</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1038/s43588-020-00022-7</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Krzhizhanovskaya</surname><given-names>VV</given-names></name><name><surname>ZÃ¡vodszky</surname><given-names>G</given-names></name><name><surname>Lees</surname><given-names>MH</given-names></name><name><surname>Dongarra</surname><given-names>JJ</given-names></name><name><surname>Sloot</surname><given-names>PMA</given-names></name><name><surname>Brissos</surname><given-names>S</given-names></name><name><surname>Teixeira</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><chapter-title>Computational science â ICCS 2020</chapter-title><source>An Optimizing Multi-Platform Source-to-Source Compiler Framework for the NEURON MODeling Language</source><publisher-loc>Cham</publisher-loc><publisher-name>International Conference on Computational Science Springer</publisher-name><fpage>45</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-50371-0</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kunkel</surname><given-names>S</given-names></name><name><surname>Potjans</surname><given-names>TC</given-names></name><name><surname>Eppler</surname><given-names>JM</given-names></name><name><surname>Plesser</surname><given-names>HE</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Meeting the memory challenges of brain-scale network simulation</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>35</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00035</pub-id><pub-id pub-id-type="pmid">22291636</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kunkel</surname><given-names>S</given-names></name><name><surname>Schmidt</surname><given-names>M</given-names></name><name><surname>Eppler</surname><given-names>JM</given-names></name><name><surname>Plesser</surname><given-names>HE</given-names></name><name><surname>Masumoto</surname><given-names>G</given-names></name><name><surname>Igarashi</surname><given-names>J</given-names></name><name><surname>Ishii</surname><given-names>S</given-names></name><name><surname>Fukai</surname><given-names>T</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name><name><surname>Helias</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Spiking network simulation code for petascale computers</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>78</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00078</pub-id><pub-id pub-id-type="pmid">25346682</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laje</surname><given-names>R</given-names></name><name><surname>Buonomano</surname><given-names>DV</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Robust timing and motor patterns by taming chaos in recurrent neural networks</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>925</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1038/nn.3405</pub-id><pub-id pub-id-type="pmid">23708144</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lam</surname><given-names>SK</given-names></name><name><surname>Pitrou</surname><given-names>A</given-names></name><name><surname>Seibert</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Numba: A llvm-based python jit compiler</article-title><conf-name>Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC</conf-name><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1145/2833157.2833162</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lattner</surname><given-names>C</given-names></name><name><surname>Adve</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>LLVM: A compilation framework for lifelong program analysis &amp; transformation</article-title><conf-name>International Symposium on Code Generation and Optimization, 2004. CGO 2004</conf-name><fpage>75</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1109/CGO.2004.1281665</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lattner</surname><given-names>C</given-names></name><name><surname>Amini</surname><given-names>M</given-names></name><name><surname>Bondhugula</surname><given-names>U</given-names></name><name><surname>Cohen</surname><given-names>A</given-names></name><name><surname>Davis</surname><given-names>A</given-names></name><name><surname>Pienaar</surname><given-names>J</given-names></name><name><surname>Riddle</surname><given-names>R</given-names></name><name><surname>Shpeisman</surname><given-names>T</given-names></name><name><surname>Vasilache</surname><given-names>N</given-names></name><name><surname>Zinenko</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>MLIR: A Compiler Infrastructure for the End of Mooreâs Law</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2002.11054">https://arxiv.org/abs/2002.11054</ext-link></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Legenstein</surname><given-names>R</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Branch-specific plasticity enables self-organization of nonlinear computation in single neurons</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>10787</fpage><lpage>10802</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5684-10.2011</pub-id><pub-id pub-id-type="pmid">21795531</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorenz</surname><given-names>EN</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Deterministic Nonperiodic Flow</article-title><source>Journal of the Atmospheric Sciences</source><volume>20</volume><fpage>130</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1175/1520-0469(1963)020&lt;0130:DNF&gt;2.0.CO;2</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>LukoÅ¡eviÄius</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>A practical guide to applying echo state networks</chapter-title><source>Neural Networks: Tricks of the Trade</source><publisher-name>Springer</publisher-name><fpage>659</fpage><lpage>686</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-35289-8</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lytton</surname><given-names>WW</given-names></name><name><surname>Omurtag</surname><given-names>A</given-names></name><name><surname>Neymotin</surname><given-names>SA</given-names></name><name><surname>Hines</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Just-in-time connectivity for large spiking networks</article-title><source>Neural Computation</source><volume>20</volume><fpage>2745</fpage><lpage>2756</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.10-07-622</pub-id><pub-id pub-id-type="pmid">18533821</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markov</surname><given-names>NT</given-names></name><name><surname>Ercsey-Ravasz</surname><given-names>MM</given-names></name><name><surname>Ribeiro Gomes</surname><given-names>AR</given-names></name><name><surname>Lamy</surname><given-names>C</given-names></name><name><surname>Magrou</surname><given-names>L</given-names></name><name><surname>Vezoli</surname><given-names>J</given-names></name><name><surname>Misery</surname><given-names>P</given-names></name><name><surname>Falchier</surname><given-names>A</given-names></name><name><surname>Quilodran</surname><given-names>R</given-names></name><name><surname>Gariel</surname><given-names>MA</given-names></name><name><surname>Sallet</surname><given-names>J</given-names></name><name><surname>Gamanut</surname><given-names>R</given-names></name><name><surname>Huissoud</surname><given-names>C</given-names></name><name><surname>Clavagnier</surname><given-names>S</given-names></name><name><surname>Giroud</surname><given-names>P</given-names></name><name><surname>Sappey-Marinier</surname><given-names>D</given-names></name><name><surname>Barone</surname><given-names>P</given-names></name><name><surname>Dehay</surname><given-names>C</given-names></name><name><surname>Toroczkai</surname><given-names>Z</given-names></name><name><surname>Knoblauch</surname><given-names>K</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Kennedy</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A weighted and directed interareal connectivity matrix for macaque cerebral cortex</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>17</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs270</pub-id><pub-id pub-id-type="pmid">23010748</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masse</surname><given-names>NY</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Circuit mechanisms for the maintenance and manipulation of information in working memory</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1159</fpage><lpage>1167</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0414-3</pub-id><pub-id pub-id-type="pmid">31182866</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKinney</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>pandas: a foundational Python library for data analysis and statistics</article-title><source>Python for High Performance and Scientific Computing</source><volume>14</volume><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meunier</surname><given-names>D</given-names></name><name><surname>Lambiotte</surname><given-names>R</given-names></name><name><surname>Bullmore</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modular and hierarchically modular organization of brain networks</article-title><source>Frontiers in Neuroscience</source><volume>4</volume><elocation-id>200</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2010.00200</pub-id><pub-id pub-id-type="pmid">21151783</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morcos</surname><given-names>AS</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>History-dependent variability in population dynamics during evidence accumulation in cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1672</fpage><lpage>1681</lpage><pub-id pub-id-type="doi">10.1038/nn.4403</pub-id><pub-id pub-id-type="pmid">27694990</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motta</surname><given-names>A</given-names></name><name><surname>Berning</surname><given-names>M</given-names></name><name><surname>Boergens</surname><given-names>KM</given-names></name><name><surname>Staffler</surname><given-names>B</given-names></name><name><surname>Beining</surname><given-names>M</given-names></name><name><surname>Loomba</surname><given-names>S</given-names></name><name><surname>Hennig</surname><given-names>P</given-names></name><name><surname>Wissler</surname><given-names>H</given-names></name><name><surname>Helmstaedter</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dense connectomic reconstruction in layer 4 of the somatosensory cortex</article-title><source>Science</source><volume>366</volume><elocation-id>eaay3134</elocation-id><pub-id pub-id-type="doi">10.1126/science.aay3134</pub-id><pub-id pub-id-type="pmid">31649140</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neftci</surname><given-names>EO</given-names></name><name><surname>Mostafa</surname><given-names>H</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Surrogate gradient learning in spiking neural networks: bringing the power of gradient-based optimization to spiking neural networks</article-title><source>IEEE Signal Processing Magazine</source><volume>36</volume><fpage>51</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1109/MSP.2019.2931595</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Gimelshein</surname><given-names>N</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Pytorch: An imperative style, high-performance deep learning library</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>The Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib68"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Plesser</surname><given-names>HE</given-names></name><name><surname>Eppler</surname><given-names>JM</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name><name><surname>Gewaltig</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Efficient parallel simulation of large-scale neuronal networks on clusters of multiprocessor computers</article-title><conf-name>Euro-Par 2007 Parallel Processing: 13th International Euro-Par Conference, Rennes, France, August 28-31, 2007. Proceedings 13</conf-name><fpage>672</fpage><lpage>681</lpage></element-citation></ref><ref id="bib69"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Plotnikov</surname><given-names>D</given-names></name><name><surname>Rumpe</surname><given-names>B</given-names></name><name><surname>Blundell</surname><given-names>I</given-names></name><name><surname>Ippen</surname><given-names>T</given-names></name><name><surname>Eppler</surname><given-names>JM</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>NESTML: A Modeling Language for Spiking Neurons</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1606.02882">https://arxiv.org/abs/1606.02882</ext-link></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Papoutsi</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Illuminating dendritic function with computational models</article-title><source>Nature Reviews. Neuroscience</source><volume>21</volume><fpage>303</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0301-7</pub-id><pub-id pub-id-type="pmid">32393820</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poo</surname><given-names>M</given-names></name><name><surname>Du</surname><given-names>J</given-names></name><name><surname>Ip</surname><given-names>NY</given-names></name><name><surname>Xiong</surname><given-names>Z-Q</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name><name><surname>Tan</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>China brain project: basic neuroscience, brain diseases, and brain-inspired computing</article-title><source>Neuron</source><volume>92</volume><fpage>591</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.050</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potjans</surname><given-names>TC</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The cell-type specific cortical microcircuit: relating structure and activity in a full-scale spiking network model</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>785</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs358</pub-id><pub-id pub-id-type="pmid">23203991</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramezanian-Panahi</surname><given-names>M</given-names></name><name><surname>Abrevaya</surname><given-names>G</given-names></name><name><surname>Gagnon-Audet</surname><given-names>JC</given-names></name><name><surname>Voleti</surname><given-names>VS</given-names></name><name><surname>Rish</surname><given-names>I</given-names></name><name><surname>Dumas</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Generative models of brain dynamics</article-title><source>Frontiers in Artificial Intelligence</source><volume>5</volume><elocation-id>807406</elocation-id><pub-id pub-id-type="doi">10.3389/frai.2022.807406</pub-id><pub-id pub-id-type="pmid">35910192</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rinzel</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1985">1985</year><chapter-title>Bursting Oscillations in an Excitable membrane model</chapter-title><source>Ordinary and Partial Differential Equations</source><publisher-name>Springer</publisher-name><fpage>304</fpage><lpage>316</lpage></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ros</surname><given-names>E</given-names></name><name><surname>Carrillo</surname><given-names>R</given-names></name><name><surname>Ortigosa</surname><given-names>EM</given-names></name><name><surname>Barbour</surname><given-names>B</given-names></name><name><surname>AgÃ­s</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Event-driven simulation scheme for spiking neural networks using lookup tables to characterize neuronal dynamics</article-title><source>Neural Computation</source><volume>18</volume><fpage>2959</fpage><lpage>2993</lpage><pub-id pub-id-type="doi">10.1162/neco.2006.18.12.2959</pub-id><pub-id pub-id-type="pmid">17052155</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sabne</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>XLA: compiling machine learning for peak performance</data-title><source>XLA</source><ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/xla">https://www.tensorflow.org/xla</ext-link></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanz Leon</surname><given-names>P</given-names></name><name><surname>Knock</surname><given-names>SA</given-names></name><name><surname>Woodman</surname><given-names>MM</given-names></name><name><surname>Domide</surname><given-names>L</given-names></name><name><surname>Mersmann</surname><given-names>J</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name><name><surname>Jirsa</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The Virtual Brain: a simulator of primate brain network dynamics</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00010</pub-id><pub-id pub-id-type="pmid">23781198</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Nelli</surname><given-names>S</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>If deep learning is the answer, what is the question?</article-title><source>Nature Reviews. Neuroscience</source><volume>22</volume><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-00395-8</pub-id><pub-id pub-id-type="pmid">33199854</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schuman</surname><given-names>CD</given-names></name><name><surname>Potok</surname><given-names>TE</given-names></name><name><surname>Patton</surname><given-names>RM</given-names></name><name><surname>Birdwell</surname><given-names>JD</given-names></name><name><surname>Dean</surname><given-names>ME</given-names></name><name><surname>Rose</surname><given-names>GS</given-names></name><name><surname>Plank</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A Survey of Neuromorphic Computing and Neural Networks in Hardware</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1705.06963">https://arxiv.org/abs/1705.06963</ext-link></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Training excitatory-inhibitory recurrent neural networks for cognitive tasks: a simple and flexible framework</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004792</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004792</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stimberg</surname><given-names>M</given-names></name><name><surname>Goodman</surname><given-names>DFM</given-names></name><name><surname>Benichoux</surname><given-names>V</given-names></name><name><surname>Brette</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Equation-oriented specification of neural models for simulations</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00006</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stimberg</surname><given-names>M</given-names></name><name><surname>Brette</surname><given-names>R</given-names></name><name><surname>Goodman</surname><given-names>DF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Brian 2, an intuitive and efficient neural simulator</article-title><source>eLife</source><volume>8</volume><elocation-id>e47314</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47314</pub-id><pub-id pub-id-type="pmid">31429824</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stimberg</surname><given-names>M</given-names></name><name><surname>Goodman</surname><given-names>DFM</given-names></name><name><surname>Nowotny</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Brian2GeNN: accelerating spiking neural network simulations with graphics hardware</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>410</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-54957-7</pub-id><pub-id pub-id-type="pmid">31941893</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Generating coherent patterns of activity from chaotic neural networks</article-title><source>Neuron</source><volume>63</volume><fpage>544</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.018</pub-id><pub-id pub-id-type="pmid">19709635</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks</article-title><source>Neural Computation</source><volume>25</volume><fpage>626</fpage><lpage>649</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00409</pub-id><pub-id pub-id-type="pmid">23272922</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neural network that finds A naturalistic solution for the production of muscle activity</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1025</fpage><lpage>1033</lpage><pub-id pub-id-type="doi">10.1038/nn.4042</pub-id><pub-id pub-id-type="pmid">26075643</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swadlow</surname><given-names>HA</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Efferent neurons and suspected interneurons in S-1 forelimb representation of the awake rabbit: receptive fields and axonal properties</article-title><source>Journal of Neurophysiology</source><volume>63</volume><fpage>1477</fpage><lpage>1498</lpage><pub-id pub-id-type="doi">10.1152/jn.1990.63.6.1477</pub-id><pub-id pub-id-type="pmid">2358887</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavanaei</surname><given-names>A</given-names></name><name><surname>Ghodrati</surname><given-names>M</given-names></name><name><surname>Kheradpisheh</surname><given-names>SR</given-names></name><name><surname>Masquelier</surname><given-names>T</given-names></name><name><surname>Maida</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep learning in spiking neural networks</article-title><source>Neural Networks</source><volume>111</volume><fpage>47</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2018.12.002</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tikidji-Hamburyan</surname><given-names>RA</given-names></name><name><surname>Narayana</surname><given-names>V</given-names></name><name><surname>Bozkus</surname><given-names>Z</given-names></name><name><surname>El-Ghazawi</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Software for brain network simulations: a comparative study</article-title><source>Frontiers in Neuroinformatics</source><volume>11</volume><elocation-id>46</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2017.00046</pub-id><pub-id pub-id-type="pmid">28775687</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Traub</surname><given-names>RD</given-names></name><name><surname>Miles</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1991">1991</year><source>Neuronal Networks of the Hippocampus</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511895401</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Walt</surname><given-names>S</given-names></name><name><surname>SchÃ¶nberger</surname><given-names>JL</given-names></name><name><surname>Nunez-Iglesias</surname><given-names>J</given-names></name><name><surname>Boulogne</surname><given-names>F</given-names></name><name><surname>Warner</surname><given-names>JD</given-names></name><name><surname>Yager</surname><given-names>N</given-names></name><name><surname>Gouillart</surname><given-names>E</given-names></name><name><surname>Yu</surname><given-names>T</given-names></name><collab>scikit-image contributors</collab></person-group><year iso-8601-date="2014">2014</year><article-title>scikit-image: image processing in Python</article-title><source>PeerJ</source><volume>2</volume><elocation-id>e453</elocation-id><pub-id pub-id-type="doi">10.7717/peerj.453</pub-id><pub-id pub-id-type="pmid">25024921</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>Ä°</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab><name><surname>Vijaykumar</surname><given-names>A</given-names></name><name><surname>Bardelli</surname><given-names>AP</given-names></name><name><surname>Rothberg</surname><given-names>A</given-names></name><name><surname>Hilboll</surname><given-names>A</given-names></name><name><surname>Kloeckner</surname><given-names>A</given-names></name><name><surname>Scopatz</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>A</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>Woods</surname><given-names>CN</given-names></name><name><surname>Fulton</surname><given-names>C</given-names></name><name><surname>Masson</surname><given-names>C</given-names></name><name><surname>HÃ¤ggstrÃ¶m</surname><given-names>C</given-names></name><name><surname>Fitzgerald</surname><given-names>C</given-names></name><name><surname>Nicholson</surname><given-names>DA</given-names></name><name><surname>Hagen</surname><given-names>DR</given-names></name><name><surname>Pasechnik</surname><given-names>DV</given-names></name><name><surname>Olivetti</surname><given-names>E</given-names></name><name><surname>Martin</surname><given-names>E</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Silva</surname><given-names>F</given-names></name><name><surname>Lenders</surname><given-names>F</given-names></name><name><surname>Wilhelm</surname><given-names>F</given-names></name><name><surname>Young</surname><given-names>G</given-names></name><name><surname>Price</surname><given-names>GA</given-names></name><name><surname>Ingold</surname><given-names>G-L</given-names></name><name><surname>Allen</surname><given-names>GE</given-names></name><name><surname>Lee</surname><given-names>GR</given-names></name><name><surname>Audren</surname><given-names>H</given-names></name><name><surname>Probst</surname><given-names>I</given-names></name><name><surname>Dietrich</surname><given-names>JP</given-names></name><name><surname>Silterra</surname><given-names>J</given-names></name><name><surname>Webber</surname><given-names>JT</given-names></name><name><surname>SlaviÄ</surname><given-names>J</given-names></name><name><surname>Nothman</surname><given-names>J</given-names></name><name><surname>Buchner</surname><given-names>J</given-names></name><name><surname>Kulick</surname><given-names>J</given-names></name><name><surname>SchÃ¶nberger</surname><given-names>JL</given-names></name><name><surname>de Miranda Cardoso</surname><given-names>JV</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Harrington</surname><given-names>J</given-names></name><name><surname>RodrÃ­guez</surname><given-names>JLC</given-names></name><name><surname>Nunez-Iglesias</surname><given-names>J</given-names></name><name><surname>Kuczynski</surname><given-names>J</given-names></name><name><surname>Tritz</surname><given-names>K</given-names></name><name><surname>Thoma</surname><given-names>M</given-names></name><name><surname>Newville</surname><given-names>M</given-names></name><name><surname>KÃ¼mmerer</surname><given-names>M</given-names></name><name><surname>Bolingbroke</surname><given-names>M</given-names></name><name><surname>Tartre</surname><given-names>M</given-names></name><name><surname>Pak</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Nowaczyk</surname><given-names>N</given-names></name><name><surname>Shebanov</surname><given-names>N</given-names></name><name><surname>Pavlyk</surname><given-names>O</given-names></name><name><surname>Brodtkorb</surname><given-names>PA</given-names></name><name><surname>Lee</surname><given-names>P</given-names></name><name><surname>McGibbon</surname><given-names>RT</given-names></name><name><surname>Feldbauer</surname><given-names>R</given-names></name><name><surname>Lewis</surname><given-names>S</given-names></name><name><surname>Tygier</surname><given-names>S</given-names></name><name><surname>Sievert</surname><given-names>S</given-names></name><name><surname>Vigna</surname><given-names>S</given-names></name><name><surname>Peterson</surname><given-names>S</given-names></name><name><surname>More</surname><given-names>S</given-names></name><name><surname>Pudlik</surname><given-names>T</given-names></name><name><surname>Oshima</surname><given-names>T</given-names></name><name><surname>Pingel</surname><given-names>TJ</given-names></name><name><surname>Robitaille</surname><given-names>TP</given-names></name><name><surname>Spura</surname><given-names>T</given-names></name><name><surname>Jones</surname><given-names>TR</given-names></name><name><surname>Cera</surname><given-names>T</given-names></name><name><surname>Leslie</surname><given-names>T</given-names></name><name><surname>Zito</surname><given-names>T</given-names></name><name><surname>Krauss</surname><given-names>T</given-names></name><name><surname>Upadhyay</surname><given-names>U</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>VÃ¡zquez-Baeza</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vitay</surname><given-names>J</given-names></name><name><surname>Dinkelbach</surname><given-names>HÃ</given-names></name><name><surname>Hamker</surname><given-names>FH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ANNarchy: a code generation approach to neural simulations on parallel hardware</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00019</pub-id><pub-id pub-id-type="pmid">26283957</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Signal propagation and logic gating in networks of integrate-and-fire neurons</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>10786</fpage><lpage>10795</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3508-05.2005</pub-id><pub-id pub-id-type="pmid">16291952</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Probabilistic decision making by slow reverberation in cortical circuits</article-title><source>Neuron</source><volume>36</volume><fpage>955</fpage><lpage>968</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)01092-9</pub-id><pub-id pub-id-type="pmid">12467598</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Examples for Brainpy computation</data-title><version designator="8458a74">8458a74</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/brainpy/examples/">https://github.com/brainpy/examples/</ext-link></element-citation></ref><ref id="bib97"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Brainpy-Elife-reproducibility</data-title><version designator="swh:1:rev:243f27ade4d01063f50ef79b5c219727a1265040">swh:1:rev:243f27ade4d01063f50ef79b5c219727a1265040</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:d691473d95b3110881f3e71b40d28400fdebdc59;origin=https://github.com/brainpy/brainpy-elife-reproducibility;visit=swh:1:snp:b28104016ab5b30b91c208de8d3735c37b819cac;anchor=swh:1:rev:243f27ade4d01063f50ef79b5c219727a1265040">https://archive.softwareheritage.org/swh:1:dir:d691473d95b3110881f3e71b40d28400fdebdc59;origin=https://github.com/brainpy/brainpy-elife-reproducibility;visit=swh:1:snp:b28104016ab5b30b91c208de8d3735c37b819cac;anchor=swh:1:rev:243f27ade4d01063f50ef79b5c219727a1265040</ext-link></element-citation></ref><ref id="bib98"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>He</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>BrainPy</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/brainpy/BrainPy/">https://github.com/brainpy/BrainPy/</ext-link></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>HR</given-names></name><name><surname>Cowan</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>Excitatory and inhibitory interactions in localized populations of model neurons</article-title><source>Biophysical Journal</source><volume>12</volume><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/S0006-3495(72)86068-5</pub-id><pub-id pub-id-type="pmid">4332108</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>KF</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A recurrent network mechanism of time integration in perceptual decisions</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>1314</fpage><lpage>1328</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3733-05.2006</pub-id><pub-id pub-id-type="pmid">16436619</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Hamaguchi</surname><given-names>K</given-names></name><name><surname>Amari</surname><given-names>S-I</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dynamics and computation of continuous attractors</article-title><source>Neural Computation</source><volume>20</volume><fpage>994</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.10-06-378</pub-id><pub-id pub-id-type="pmid">18085986</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Wu</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Improved expressivity through dendritic neural networks</article-title><conf-name>Advances in neural information processing systems</conf-name></element-citation></ref><ref id="bib103"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>D</given-names></name><name><surname>Hechtman</surname><given-names>B</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Joshi</surname><given-names>R</given-names></name><name><surname>Krikun</surname><given-names>M</given-names></name><name><surname>Lepikhin</surname><given-names>D</given-names></name><name><surname>Ly</surname><given-names>A</given-names></name><name><surname>Maggioni</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>GSPMD: General and Scalable Parallelization for ML Computation Graphs</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2105.04663">https://arxiv.org/abs/2105.04663</ext-link></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yavuz</surname><given-names>E</given-names></name><name><surname>Turner</surname><given-names>J</given-names></name><name><surname>Nowotny</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>GeNN: a code generation framework for accelerated brain simulations</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>18854</elocation-id><pub-id pub-id-type="doi">10.1038/srep18854</pub-id><pub-id pub-id-type="pmid">26740369</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s7"><title>Review of the existing programming paradigm</title><p>In general, the existing tools for brain dynamics programming can be roughly divided into two categories: low-level programming and descriptive language.</p><p>The representatives of the first category include NEURON (<xref ref-type="bibr" rid="bib40">Hines and Carnevale, 1997</xref>), NEST (<xref ref-type="bibr" rid="bib31">Gewaltig and Diesmann, 2007</xref>), CARLsim (<xref ref-type="bibr" rid="bib9">Beyeler et al., 2015</xref>; <xref ref-type="bibr" rid="bib18">Chou et al., 2018</xref>), NeuronGPU (<xref ref-type="bibr" rid="bib33">Golosio et al., 2021</xref>), Arbor (<xref ref-type="bibr" rid="bib3">Akar et al., 2019</xref>), and others. These simulators offer a library of standard models written in C/C++ (particularly for NEST), CUDA (for CARLsim, NeuronGPU, and Arbor), or domain-specific languages (for NEURON and Arbor) to ensure efficient execution, along with a user-friendly Python interface for ease of use. Users can create neural networks in Python by utilizing the neuron and synapse models provided in the library. However, when a new model is required, users must learn to program using the low-level language. This significantly increases the learning cost and restricts the flexibility in defining new models (<xref ref-type="bibr" rid="bib89">Tikidji-Hamburyan et al., 2017</xref>).</p><p>The second category tools include Brian (<xref ref-type="bibr" rid="bib34">Goodman and Brette, 2008</xref>), Brian2 (<xref ref-type="bibr" rid="bib81">Stimberg et al., 2014</xref>; <xref ref-type="bibr" rid="bib82">Stimberg et al., 2019</xref>), ANNarchy (<xref ref-type="bibr" rid="bib93">Vitay et al., 2015</xref>), GeNN (<xref ref-type="bibr" rid="bib104">Yavuz et al., 2016</xref>), BMTK (<xref ref-type="bibr" rid="bib19">Dai et al., 2020a</xref>), NetPyNE (<xref ref-type="bibr" rid="bib25">Dura-Bernal et al., 2019</xref>), NeuroML (<xref ref-type="bibr" rid="bib32">Gleeson et al., 2010</xref>), and NMODL (<xref ref-type="bibr" rid="bib48">Krzhizhanovskaya et al., 2020</xref>), which employ a code generation approach based on descriptive languages. Descriptive simulators allow users to create new models based on convenient descriptions (such as text, <xref ref-type="bibr" rid="bib34">Goodman and Brette, 2008</xref>; <xref ref-type="bibr" rid="bib81">Stimberg et al., 2014</xref>; <xref ref-type="bibr" rid="bib82">Stimberg et al., 2019</xref>; <xref ref-type="bibr" rid="bib93">Vitay et al., 2015</xref>; JSON, <xref ref-type="bibr" rid="bib19">Dai et al., 2020a</xref>; <xref ref-type="bibr" rid="bib25">Dura-Bernal et al., 2019</xref>; XML files, <xref ref-type="bibr" rid="bib32">Gleeson et al., 2010</xref>; or customized languages, <xref ref-type="bibr" rid="bib48">Krzhizhanovskaya et al., 2020</xref>) and then translate the descriptions into low-level codes to speed up model running. In such a way, descriptive simulators enable model customization based on high-level descriptive languages and ensure efficient running by generating low-level codes.</p><p>Currently, descriptive language has become a standard approach for brain simulation (<xref ref-type="bibr" rid="bib11">Blundell et al., 2018</xref>). Simulators employing this first approach also start to provide their descriptive language interface for code generation. For instance, the NEST simulator provided its domain-specific language NESTML (<xref ref-type="bibr" rid="bib69">Plotnikov et al., 2016</xref>) to describe stereotypical neuron and synapse models. Similarly, the NEURON simulator recently released its modern descriptive interface NetPyNE (<xref ref-type="bibr" rid="bib25">Dura-Bernal et al., 2019</xref>), which employs the standard JSON format to allow users to describe neural circuit models by composing the existing available NEURON building block models. However, users still need to code based on its low-level programming interface to customize new models for channels, neurons, or synapses.</p><p>Descriptive languages have been highly successful for brain simulation (<xref ref-type="bibr" rid="bib11">Blundell et al., 2018</xref>). A major benefit of these declarative approaches is the clear separation between model specification and implementation. This frees users from low-level programming details, enabling them to intuitively specify complex models in a concise, semantically clear way. The declarative nature of descriptive languages allows modelers to focus on computational neuroscience instead of implementation specifics. This has enabled rapid prototyping and sharing of models in the field. Overall, descriptive simulation languages have greatly improved modeler productivity thanks to their high-level, implementation-agnostic nature. However, they have intrinsic limitations on transparency, extensibility, and flexibility. One prominent feature of these descriptive languages is that they completely separate the model definition from the simulation, and therefore are not directly executable (<xref ref-type="bibr" rid="bib11">Blundell et al., 2018</xref>). This kind of programming paradigm will cause great restrictions on usability and flexibility because it disables model debugging, error correction, and direct logic controlling. Moreover, descriptive languages are usually designed for specific kinds of models or one particular modeling approach. They are written in more than two programming languages: one is based on a low-level language (e.g., C++, CUDA) to implement its core functionality, and the other is based on a high-level language (e.g., Python, Matlab) for ease of use. Once they are not tailored to usersâ needs, extensions to accommodate new components must be made in both high- and low-level languages, which is hard or nearly impossible for normal users. What is more, descriptive languages greatly reduce the expressive power of a general-purpose programming language and make it hard to describe all aspects of a simulation experiment, including clipping variables out of bounds, inputâoutput relations, model debugging, code optimization, dynamics analysis, and others.</p><p>In summary, significant challenges to transparency, flexibility, efficiency, and extensibility are still present in the existing programming paradigm for brain simulation. We can draw the conclusion that current software solutions cannot lead us to a general-purpose programming framework that allows us to freely define brain dynamics models in various application domains.</p></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s8"><title>JIT compilation and JIT compilers</title><p>A notorious challenge in scientific computing is the trade-off between usability and efficiency. The former seeks the fast prototyping of thoughts and ideas, whereas the latter pursues efficient code execution. For a long time in the past, it was difficult to strike a balance between the two. For example, statically typed compiled programming languages such as C or C++ are incredibly efficient in code execution, but their productivity is relatively low due to their complex and heavy syntax. In contrast, dynamically typed interpreted programming languages like Python and R are easy to learn and use, but they have slow running speeds. Nowadays, with the increasing complexity of models, the demand for both usability and efficiency has increased dramatically. Fortunately, recent advancements in just-in-time (JIT) compilation technology (<xref ref-type="bibr" rid="bib53">Lattner and Adve, 2004</xref>; <xref ref-type="bibr" rid="bib54">Lattner et al., 2020</xref>) have provided viable answers to this two-language problem. In particular, a new generation of computational engines based on JIT compilation (<xref ref-type="bibr" rid="bib6">Aycock, 2003</xref>) has begun to have an impact on a variety of scientific computing disciplines.</p><p>The JIT compilation can be seen as the combination of the statically typed compilation and dynamically typed interpretation. It benefits from both the convenience of dynamic high-level languages like Python and the efficiency of static low-level languages such as C++. At the start of a program execution, a JIT compiler acts like an interpreter. It runs your code step by step, and can output the intermediate results at the run-time for debugging. However, if some hot code snippets that are executed frequently, for example, certain functions or loop bodies, are detected or manually labeled, they will be submitted to the JIT compiler for compilation and storage. In this sense, it acts like a statically typed compiler. Once the compiled code snippets are entered again, the program will directly execute the compiler-generated low-level code without time-consuming interpreting again. Hot code snippets can be automatically detected by the JIT compiler, or be manually labeled by users.</p><p>JIT compilation is a mature and well-established technology. It has been adopted in modern programming languages like JAVA, Julia, and Python. JAVA language provides JIT compilation in its JAVA virtual machine (JVM) to accelerate the execution of JAVA code (<xref ref-type="bibr" rid="bib36">Grcevski et al., 2004</xref>). Java source code is first compiled into the platform-independent Java bytecode (.class file). Then, JVM loads the bytecode, interprets it, and executes it. To increase the running speed, JVM detects code that is frequently called through the hotspot detection and submits its bytecode to the JIT compiler to compile them into machine code. For the code with lower frequency, executing it through the JVM interpretation can save the time of the JIT compilation; while for the hot code frequently called, JIT compilation can significantly improve the running speed after the code is compiled. However, compared with Python, JAVA has poor ecosystem support for numerical computing. Its JIT is not specialized in numerical computing, but in general domains.</p><p>Julia (<xref ref-type="bibr" rid="bib10">Bezanson et al., 2017</xref>), another dynamic high-level programming language, is recently proposed for high-performance scientific computing. Julia features intuitive, productive, and general-purpose syntax inspired by the success of Python, Matlab, and C++. Moreover, it achieves attractive performance through the JIT compilation based on the LLVM compiler infrastructure (<xref ref-type="bibr" rid="bib53">Lattner and Adve, 2004</xref>). In a remarkably short time, Julia has provided excellent routines for mathematical functions, machine-learning algorithms, data processing tools, visualization utilities, and others. However, Julia is still young. Costs, such as lack of familiarity, rough edges, correctness bugs, and continual language changes, are still imposed on normal users.</p><p>Python is a well-known and popular interactive dynamic programming language. It has a long history in numerical computing (<xref ref-type="bibr" rid="bib24">Dubois et al., 1996</xref>; <xref ref-type="bibr" rid="bib38">Harris et al., 2020</xref>). The ecosystem of scientific computing, including array programming (<xref ref-type="bibr" rid="bib38">Harris et al., 2020</xref>), scientific algorithms (<xref ref-type="bibr" rid="bib92">Virtanen et al., 2020</xref>), machine learning (<xref ref-type="bibr" rid="bib67">Pedregosa et al., 2011</xref>), deep learning (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>; <xref ref-type="bibr" rid="bib66">Paszke et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Frostig et al., 2018</xref>), image processing (<xref ref-type="bibr" rid="bib91">van der Walt et al., 2014</xref>), data analysis and statistics (<xref ref-type="bibr" rid="bib61">McKinney, 2011</xref>), network analysis (<xref ref-type="bibr" rid="bib37">Hagberg and Swart, 2008</xref>), visualization (<xref ref-type="bibr" rid="bib41">Hunter, 2007</xref>), and many others, has been well established in Python. Before Julia, the JIT compilation was introduced into Python by PyPy in 2007. Later, other attempts, including Pyston, Pyjion, Psyco, JitPy, HOPE, etc., are proposed. With a long history of JIT development, Python nowadays has provided mature platforms of JIT compilation focusing on numerical computing. These numerical JIT platforms include Numba (<xref ref-type="bibr" rid="bib52">Lam et al., 2015</xref>), JAX (<xref ref-type="bibr" rid="bib29">Frostig et al., 2018</xref>), and XLA (<xref ref-type="bibr" rid="bib76">Sabne, 2020</xref>). Each of them has its own characteristics.</p><p>JAX (<xref ref-type="bibr" rid="bib29">Frostig et al., 2018</xref>) is a flourishing machine-learning library developed by Google. It aims to provide high-level numerical functions to help users fast prototype machine learning ideas. Moreover, these numerical functions can benefit from powerful functional transformations, like automatic differentiation <monospace>grad</monospace>, JIT compilation <monospace>jit</monospace>, automatic vectorization <monospace>vmap</monospace>, and parallelization <monospace>pmap</monospace>. JAX makes heavy use of XLA (<xref ref-type="bibr" rid="bib76">Sabne, 2020</xref>) (see the following text) for code optimization. Specifically, for ease of use, high-level numerical functions in JAX are NumPy like. JAX provides many numerical functions in NumPy, including basic mathematical operators, linear algebra functions, and Fourier transform routines. However, some fundamental designs are significantly different from NumPy, for instance, the well-established syntax for in-place updating and random samplings. This is the reason why we provide another set of numerical functions consistent with NumPy. In addition to its NumPy-like API, JAX provides a wonderful set of composable functional transformations. Among them, automatic differentiation in JAX supports both forward and backward modes for arbitrary numerical functions. It can take derivatives of a function with a large subset of Python syntax, including loops, conditions, recursions, and closures. Moreover, JAX utilizes XLA to JIT compile your Python code on modern devices, like Central Processing Units (CPUs), Graphics Processing Units (GPUs), and Tensor Processing Units (TPUs). It can significantly accelerate the execution speed of your code and allows you to get maximal performance without having to leave Python. JAX also provides automatic vectorization or batching. It supports transforming loops to vector operations via a single functional call vmap. Whatâs more, JAX delivers pmap to express single-instruction multiple-data programs. Applying pmap to a function will JIT compile and execute the code in parallel on XLA devices, like multiple GPUs or TPU cores. Similar to vmap, pmap transformation maps a function over array axes. But what is different is that the former vectorizes functions by compiling the mapped axis as primitive operations, whereas the latter replicates the function and runs each replica on its own XLA device in parallel. Automatic differentiation and compilation in JAX can be composed arbitrarily to enable rapid experimentation of novel algorithms.</p><p>XLA (<xref ref-type="bibr" rid="bib76">Sabne, 2020</xref>) is a domain-specific linear algebra compiler developed by Google that aims to improve the execution speed, memory usage, portability, and mobile footprint reduction of machine-learning algorithms. XLA compiler provides support for JIT compilation based on LLVM (<xref ref-type="bibr" rid="bib53">Lattner and Adve, 2004</xref>). The front-end program (e.g., JAX) which wants to take advantage of the JIT compilation of XLA should first define the computation graph as âHigh-Level Optimizer IRâ (HLO IR). Then, XLA takes this graph defined in HLO IR and compiles it into machine instructions for different backend architectures. Currently, XLA supports JIT compilation on backend devices of x86-64 CPUs, NVIDIA GPUs, and Google TPUs. XLA is designed for easy portability on new hardware. It provides an abstract interface that a new hardware device can implement to create a backend to run existing computation graphs. Instead of implementing every existing operator for new hardware, XLA provides a simple and scalable mechanism that can retarget different backends. This advantage may be valuable for neuromorphic computing (<xref ref-type="bibr" rid="bib79">Schuman et al., 2017</xref>), because new neuromorphic hardware can be interfaced as a new backend of XLA computation.</p><p>Numba (<xref ref-type="bibr" rid="bib52">Lam et al., 2015</xref>) is a JIT compiler for numerical functions in Python. Similar to JAX, Numba supports the JIT compilation of a subset of Python and NumPy code based on the LLVM compiler. However, different from JAX which accelerates the computation flow composed of high-level operators, Numba pays more attention to the acceleration of loop-based functions. Numba achieves excellent optimizations on Python functions with a lot of loops. It allows users to write fine-grained code with native Python control flows and meanwhile obtains the running speed approaching C. This is a huge advantage compared to JAX because JAX does not support the automatic vectorization of a for-loop function.</p></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s9"><title>Continuous integration and documentation generation</title><p>To ensure any code changes do not introduce unintended bugs or side effects, we standardized the development process and enabled the automatic continuous integration (CI) of BrainPy with GitHub Actions. Moreover, to update the tutorial and documentation with the latest code changes, we automated the documentation building of BrainPy with <italic>Read the Docs</italic>. <xref ref-type="fig" rid="app3fig1">Appendix 3âfigure 1</xref> illustrates the whole workflow of BrainPy development. First, any code change should be proposed through GitHub Pull Request. Once a Pull Request is opened, CI pipelines are triggered to test BrainPy codes on Windows, Linux, and macOS platforms (<xref ref-type="fig" rid="app3fig1">Appendix 3âfigure 1</xref> (1)). After all test suites are passed, the code reviewer should manually inspect the significance and correctness of the proposed code changes again. If all things are fine, the reviewer can merge the code into the master branch (<xref ref-type="fig" rid="app3fig1">Appendix 3âfigure 1</xref> (2)). After merging, a new set of test cases is triggered automatically to test that the latest BrainPy codebase does not have bugs (<xref ref-type="fig" rid="app3fig1">Appendix 3âfigure 1</xref> (3)). Besides, the merging operation also triggers the automatic documentation generation through the documentation hosting platform <italic>Read the Docs</italic> (<xref ref-type="fig" rid="app3fig1">Appendix 3âfigure 1</xref> (4)), in which the latest documentation, including the code annotation, user manual, and tutorials, are automatically built with Sphinx and hosted online at <ext-link ext-link-type="uri" xlink:href="https://brainpy.readthedocs.io/">https://brainpy.readthedocs.io/</ext-link>.</p><fig id="app3fig1" position="float"><label>Appendix 3âfigure 1.</label><caption><title>The pipeline of automatic continuous integration and documentation building in BrainPy.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-app3-fig1-v2.tif"/></fig></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s10"><title>Mathematical operators for brain dynamics modeling</title><p>Brain dynamics modeling involves conventional computation based on dense matrices and event-driven computation based on sparse connections. BrainPy provides operators for these two kinds of computations. For the list of the number of currently implemented operators please see <xref ref-type="table" rid="app4table1">Appendix 4âtable 1</xref>.</p><table-wrap id="app4table1" position="float"><label>Appendix 4âtable 1.</label><caption><title>Number of mathematical operators implemented in BrainPy.</title><p>This list will continue to expand since BrainPy will continue to add more operators for brain dynamics modeling. The list of implemented operators is online available at <ext-link ext-link-type="uri" xlink:href="https://brainpy.readthedocs.io/en/latest/apis/math.html">https://brainpy.readthedocs.io/en/latest/apis/math.html</ext-link>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Number</th></tr></thead><tbody><tr><td align="left" valign="bottom">Dense operators with NumPy syntax</td><td align="char" char="." valign="bottom">472</td></tr><tr><td align="left" valign="bottom">Dense operators with TensorFlow syntax</td><td align="char" char="." valign="bottom">25</td></tr><tr><td align="left" valign="bottom">Dense operators with PyTorch syntax</td><td align="char" char="." valign="bottom">10</td></tr><tr><td align="left" valign="bottom">Sparse and event-driven operators</td><td align="char" char="." valign="bottom">20</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s11"><title>Dense matrix operators</title><p>JAX (<xref ref-type="bibr" rid="bib29">Frostig et al., 2018</xref>) has provided most numerical functions in NumPy (<xref ref-type="bibr" rid="bib38">Harris et al., 2020</xref>). However, there are several significant differences between JAX and NumPy. First, the array structure in JAX does not support in-place updating. Second, numerical functions that need in-place updating are missing in JAX. Third, random sampling functions are significantly different from NumPy.</p><p>The dense matrix operators in BrainPy are based on JAXâs implementations but are designed to be seamlessly consistent with NumPy. First, we provide <monospace>brainpy.math.Array</monospace> which is consistent with NumPyâs ndarray structure, and a series of mathematical operations for <monospace>brainpy.math.Array</monospace> which is similar to those for ndarray. Second, mathematical operators for <monospace>brainpy.math.Array</monospace>, such as indexing, slicing, sorting, rounding, arithmetic operations, linear algebraic functions, and Fourier transform routines, are all supported. Many of these operators (nearly 85%) are directly implemented through the NumPy-like functions in JAX, while BrainPy provides dozens of APIs missing or inconsistent in JAX. Third, to unify random number generators, BrainPy implements most of the random sampling functions in NumPy, including its univariate distributions, multivariate distributions, standard distributions, and utility functions.</p><p>Moreover, BrainPy is working on dense operators provided in PyTorch (<xref ref-type="bibr" rid="bib66">Paszke et al., 2019</xref>) and TensorFlow (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>) libraries. In the future, BrainPy will continue to cover dense array operators in TensorFlow and PyTorch, since these implementation syntaxes have been widely accepted in the community.</p></sec><sec sec-type="appendix" id="s12"><title>Dedicated operators</title><p>Brain dynamics models differ from deep neural network (DNN) models in the way they perform computation. Brain dynamics models typically have sparse connections (less than 20% of neurons are connected to each other) and perform event-driven computations (synaptic currents are only transmitted when a presynaptic neuron spikes). These unique features make brain dynamics models less efficient when conventional dense array operators are used. To tackle this efficiency issue, traditional brain simulators heavily rely on event-driven synaptic operations. Previous works have explored event-driven synaptic operations on both CPU platforms (see <xref ref-type="bibr" rid="bib93">Vitay et al., 2015</xref>; <xref ref-type="bibr" rid="bib68">Plesser et al., 2007</xref>; <xref ref-type="bibr" rid="bib82">Stimberg et al., 2019</xref>) and GPU platforms (see <xref ref-type="bibr" rid="bib26">Fidjeland et al., 2009</xref>; <xref ref-type="bibr" rid="bib14">Brette and Goodman, 2011</xref>; <xref ref-type="bibr" rid="bib104">Yavuz et al., 2016</xref>; <xref ref-type="bibr" rid="bib4">Alevi et al., 2022</xref>).</p><p>Despite the effectiveness of these simulators, one limitation is the lack of abstraction of event-driven synaptic operations as primitive low-level operators. In other words, these operations are not treated as fundamental building blocks that can be easily manipulated and optimized. This absence of abstraction hampers the development of more efficient algorithms and restricts the flexibility and extensibility of the simulators. Therefore, it is crucial to bridge this gap and provide a higher level of abstraction for event-driven synaptic operations, which we refer to as â<italic>event-driven operators</italic>â. Note here in BrainPy, event-driven operators are employed within a <italic>clock-driven simulation</italic> schema, where the simulation advances in a synchronized manner, updating all neurons and synapses at each time step. These event-driven operators process information based on the detection of spatial spiking events. They execute computations when the presynaptic neuron fires spikes at each time step. This contrasts with an <italic>event-driven simulation</italic> approach (<xref ref-type="bibr" rid="bib75">Ros et al., 2006</xref>), where neuronal or synaptic state updates are triggered by temporal spiking events, rather than updating all elements at each time step. Therefore, the key distinction between our event-driven operators and an event-driven simulation scheme lies in their scope and application: event-driven operators serve as fundamental building blocks that define how individual components of an SNN respond to spatial events at the current time step, while an event-driven simulation scheme serves as a methodology for simulating the collective behavior of the entire network based on the occurrence of temporal spikes.</p><p>Moreover, by abstracting these operations into primitive low-level operators, BrainPy offers automatic differentiation support compatible with <monospace>jax.grad</monospace>. It also enables vectorization and parallelization support compatible with<monospace> jax.vmap</monospace> and <monospace>jax.pmap</monospace>. This compatibility further enhances the applicability of event-driven operators across a wider range of contexts.</p><p>Specifically, BrainPy provides these sparse and event-driven operators in the following modules: (1) The <monospace>brainpy.math.sparse</monospace> module. This module provides a set of sparse operators that can store synaptic connectivity compactly and compute synaptic currents efficiently. (2) The <monospace>brainpy.math.event</monospace> module. This module implements event-driven operators that only perform computations when spikes arrive. This can lead to significant improvements in efficiency, as the state of the system does not need to be constantly updated when no spikes arrive.</p><p>In addition, efficient modeling of brain dynamics encounters scalability issues. The computational resources and device memory requirements for brain dynamics models increase quadratically with the number of neurons, as the synaptic connectivity grows almost quadratically in relation to the number of neurons. This characteristic severely restricts the size of the network that can be simulated on a single device using traditional array programming solutions.</p><p>One established approach to address this challenge is the utilization of JIT connectivity (<xref ref-type="bibr" rid="bib58">Lytton et al., 2008</xref>; <xref ref-type="bibr" rid="bib7">Azevedo Carvalho et al., 2020</xref>; <xref ref-type="bibr" rid="bib47">Knight and Nowotny, 2021</xref>). This method involves regenerating the synaptic connectivity during computation by controlling the same random seed. Specifically, the <monospace>brainpy.math.jitconn</monospace> module provides JIT connectivity as primitive operators, specifically designed for cases where synaptic connections follow a fixed connectivity rule and do not require modifications after initialization. These operators eliminate the memory overhead required for connectivity storage, as synaptic connectivity can be generated JIT during the execution of the operators. Notably, when compared to conventional operators, they enable the execution of large-scale networks that are two to three orders of magnitude larger on a single device.</p><p>Moreover, BrainPy also provides operators that are involved in transformations among presynaptic neuronal data, synaptic data, and postsynaptic neuronal data in the brainpy.math module. Specifically, this module provides operators for mapping presynaptic neuronal data to its connected synaptic dimension (pre-to-syn), arithmetic operators including summation, product, max, min, mean, and softmax for transforming synaptic data to postsynaptic neuronal dimension (syn-to-post), and arithmetic operators such as summation, product, max, min, and mean for directly computing postsynaptic data from presynaptic events (pre-to-post).</p></sec></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s13"><title>Numerical solvers for differential equations</title><p>To meet the large demand for solving differential equations in brain dynamics modeling, BrainPy implements numerical solvers for ODEs, SDEs, fractional differential equations (FDEs), and delay differential equations (DDEs). In general, numerical integration in BrainPy defines the system evolution of <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">â</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> is the state, <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> is the current time, and <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> is the integration step.</p><sec sec-type="appendix" id="s13-1"><title>ODE numerical solvers</title><p>In BrainPy, the integration of an ODE system <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is performed as</p><p><code xml:space="preserve">1 integral=brainpy.odeint(f=&lt;function&gt;, 
2 âââââââââââââââââââââââââââââmethod=&lt;str&gt;, 
3 âââââââââââââââââââââââââââââdt=&lt;float&gt;)</code></p><p>where method denotes the numerical method used to integrate the ODE function, and dt controls the initial or default numerical integration step. A variety of numerical integration methods for ODEs, including RungeâKutta, adaptive RungeâKutta, and Exponential methods, are supported in BrainPy (see <xref ref-type="table" rid="app5table1">Appendix 5âtable 1</xref>).</p><table-wrap id="app5table1" position="float"><label>Appendix 5âtable 1.</label><caption><title>Numerical solvers provided in BrainPy for ordinary differential equations.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Solver type</th><th align="left" valign="bottom">Solver name</th><th align="left" valign="bottom">Keyword</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="12">RungeâKutta method</td><td align="left" valign="bottom">Euler</td><td align="left" valign="bottom">euler</td></tr><tr><td align="left" valign="bottom">Midpoint</td><td align="left" valign="bottom">midpoint</td></tr><tr><td align="left" valign="bottom">Heunâs second-order method</td><td align="left" valign="bottom">heun2</td></tr><tr><td align="left" valign="bottom">Ralstonâs second-order method</td><td align="left" valign="bottom">ralston2</td></tr><tr><td align="left" valign="bottom">Second-order RungeâKutta method</td><td align="left" valign="bottom">rk2</td></tr><tr><td align="left" valign="bottom">Third-order RungeâKutta method</td><td align="left" valign="bottom">rk3</td></tr><tr><td align="left" valign="bottom">Four-order RungeâKutta method</td><td align="left" valign="bottom">rk4</td></tr><tr><td align="left" valign="bottom">Heunâs third-order method</td><td align="left" valign="bottom">heun3</td></tr><tr><td align="left" valign="bottom">Ralstonâs third-order method</td><td align="left" valign="bottom">ralston3</td></tr><tr><td align="left" valign="bottom">Third-order strong stability preserving RungeâKutta method</td><td align="left" valign="bottom">ssprk3</td></tr><tr><td align="left" valign="bottom">Ralstonâs fourth-order method</td><td align="left" valign="bottom">ralston4</td></tr><tr><td align="left" valign="bottom">Fourth-order RungeâKutta method with 3/8-rule</td><td align="left" valign="bottom">rk4_38rule</td></tr><tr><td align="left" valign="bottom" rowspan="6">Adaptive RungeâKutta method</td><td align="left" valign="bottom">RungeâKuttaâFehlberg 4 (5)</td><td align="left" valign="bottom">rkf45</td></tr><tr><td align="left" valign="bottom">RungeâKuttaâFehlberg 1 (2)</td><td align="left" valign="bottom">rkf12</td></tr><tr><td align="left" valign="bottom">DormandâPrince method</td><td align="left" valign="bottom">rkdp</td></tr><tr><td align="left" valign="bottom">CashâKarp method</td><td align="left" valign="bottom">ck</td></tr><tr><td align="left" valign="bottom">BogackiâShampine method</td><td align="left" valign="bottom">bs</td></tr><tr><td align="left" valign="bottom">HeunâEuler method</td><td align="left" valign="bottom">heun_euler</td></tr><tr><td align="left" valign="bottom">Exponential method</td><td align="left" valign="bottom">Exponential Euler method</td><td align="left" valign="bottom">exp_euler</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s13-2"><title>SDE numerical solvers</title><p>For a general SDE system with multi-dimensional driving Wiener processes,<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>Î±</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>Î±</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>Î±</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>BrainPy supports its numerical integration with</p><p><code xml:space="preserve">1 integral = brainpy.sdeint(f=&lt;function &gt; , 
2 ââââââââââââââââââââââââââââââââg=&lt;function &gt; , 
3 ââââââââââââââââââââââââââââââââmethod =&lt;str &gt; , 
4 ââââââââââââââââââââââââââââââââdt =&lt;float &gt; , 
5 ââââââââââââââââââââââââââââââââwiener_type=&lt;âscalarâ or âvectorâ&gt;, 
6 ââââââââââââââââââââââââââââââââintg_type=&lt;âItoâ or âStratonovichâ&gt;)</code></p><p>where <monospace>method</monospace> specifies the numerical solver, <monospace>dt</monospace> the default integral step, <monospace>wiener_type</monospace> the type of Wiener process (<monospace>SCALAR_WIENER</monospace> for scalar noise or <monospace>VECTOR_WIENER</monospace> for multi-dimensional driving Wiener processes), and integral_type the integral type (<monospace>ITO_SDE</monospace> for the ItÃ´ integral or <monospace>STRA_SDE</monospace> for the Stratonovich stochastic integral). See <xref ref-type="table" rid="app5table2">Appendix 5âtable 2</xref> for the full list of SDE solvers currently implemented in BrainPy.</p><table-wrap id="app5table2" position="float"><label>Appendix 5âtable 2.</label><caption><title>Numerical solvers provided in BrainPy for stochastic differential equations.</title><p>âYâ denotes support. âNâ denotes not support.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Integral type</th><th align="left" valign="bottom">Solver name</th><th align="left" valign="bottom">Keyword</th><th align="left" valign="bottom">Scalar Wiener</th><th align="left" valign="bottom">Vector Wiener</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="7">ItÃ´ integral</td><td align="left" valign="bottom">Strong SRK scheme: SRI1W1</td><td align="left" valign="bottom">srk1w1_scalar</td><td align="left" valign="bottom">Y</td><td align="left" valign="bottom">N</td></tr><tr><td align="left" valign="bottom">Strong SRK scheme: SRI2W1</td><td align="left" valign="bottom">srk2w1_scalar</td><td align="left" valign="bottom">Y</td><td align="left" valign="bottom">N</td></tr><tr><td align="left" valign="bottom">Strong SRK scheme: KlPl</td><td align="left" valign="bottom">KlPl_scalar</td><td align="left" valign="bottom">Y</td><td align="left" valign="bottom">N</td></tr><tr><td align="left" valign="bottom">Euler method</td><td align="left" valign="bottom">euler</td><td align="left" valign="bottom">Y</td><td align="left" valign="bottom">Y</td></tr><tr><td align="left" valign="bottom">Milstein method</td><td align="left" valign="bottom">milstein</td><td align="left" valign="bottom">Y</td><td align="left" valign="bottom">Y</td></tr><tr><td align="left" valign="bottom">Derivative-free Milstein method</td><td align="left" valign="bottom">milstein2</td><td align="left" valign="bottom">Y</td><td align="left" valign="bottom">Y</td></tr><tr><td align="left" valign="bottom">Exponential Euler</td><td align="left" valign="bottom">exp_euler</td><td align="left" valign="bottom">Y</td><td align="left" valign="bottom">Y</td></tr><tr><td align="left" valign="bottom" rowspan="4">Stratonovich integral</td><td align="left" valign="bottom">Euler method</td><td align="left" valign="bottom">euler</td><td align="left" valign="bottom">Y</td><td align="left" valign="bottom">Y</td></tr><tr><td align="left" valign="bottom">Heun method</td><td align="left" valign="bottom">heun</td><td align="left" valign="bottom">Y</td><td align="left" valign="bottom">Y</td></tr><tr><td align="left" valign="bottom">Milstein method</td><td align="left" valign="bottom">milstein</td><td align="left" valign="bottom">Y</td><td align="left" valign="bottom">Y</td></tr><tr><td align="left" valign="bottom">Derivative-free Milstein method</td><td align="left" valign="bottom">milstein2</td><td align="left" valign="bottom">Y</td><td align="left" valign="bottom">Y</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s13-3"><title>FDE numerical solvers</title><p>The numerical integration of FDEs is very similar to that of ODEs, except that the initial value, memory length, and fractional order should be provided. Given the fractional-order differential equation<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">d</mml:mi><mml:mi>Î±</mml:mi></mml:msup><mml:mo>â¢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mi>Î±</mml:mi></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the fractional order <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>Î±</mml:mi><mml:mo>â¤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. BrainPy supports its numerical integration with the following format of</p><p><code xml:space="preserve">1 brainpy.fdeint(f=&lt;function&gt; , 
2 âââââââââââââââââalpha=&lt;float&gt; , 
3 ââââââââââââââââânum_memory=&lt;int&gt; , 
4 âââââââââââââââââinits=&lt;dict&gt; , 
5 âââââââââââââââââmethod=&lt;str&gt; )</code></p><p>BrainPy supports two types of FDE, that is, the Caputo derivative and GrÃ¼nwaldâLetnikov derivative. Caputo derivatives are widely used in neuroscience modeling (<xref ref-type="bibr" rid="bib46">Kaslik and Sivasundaram, 2012</xref>). However, the numerical integration of Caputo derivatives has a high memory consumption, because it requires integration over all past activities. This implies that FDEs with the Caputo derivative definition cannot be used to simulate realistic neural systems. However, the numerical method for GrÃ¼nwaldâLetnikov FDEs, <monospace>brainpy.fde.GLShortMemory</monospace>, is highly efficient because it does not require an infinite memory length for numerical solutions. With the increasing length of short memory, the accuracy of the numerical approximation will increase. Therefore, <monospace>brainpy.fde.GLShortMemory</monospace> can be applied to real-world neural modeling. See <xref ref-type="table" rid="app5table3">Appendix 5âtable 3</xref> for the full list of FDE solvers currently implemented in BrainPy.</p><table-wrap id="app5table3" position="float"><label>Appendix 5âtable 3.</label><caption><title>Numerical solvers provided in BrainPy for fractional differential equations.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Derivative type</th><th align="left" valign="bottom">Solver name</th><th align="left" valign="bottom">Keyword</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="2">Caputo derivative</td><td align="left" valign="bottom">L1 schema</td><td align="left" valign="bottom">l1</td></tr><tr><td align="left" valign="bottom">Euler method</td><td align="left" valign="bottom">euler</td></tr><tr><td align="left" valign="bottom">GrÃ¼nwaldâLetnikov derivative</td><td align="left" valign="bottom">Short Memory Principle</td><td align="left" valign="bottom">short-memory</td></tr></tbody></table></table-wrap><sec sec-type="appendix" id="s13-3-1"><title>DDE numerical solvers</title><p>Delays occur in any type of differential equation. In realistic neural modeling, delays are often inevitable. BrainPy supports equations with variables of constant delays, like<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>â²</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¦</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the time lags <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Ï</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are the positive constants. It also supports systems with state-dependent delays, for example,<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>â²</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¦</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the function that computes the delay length by the system state <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. For neutral-typed equations in which delays appear in derivative terms,<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>â²</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>â²</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>â²</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¦</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>â²</mml:mo></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>BrainPy also supports its numerical integration.</p><p>BrainPy, in particular, implements interfaces to define these various delay variables. <monospace>brainpy.math.TimeDelay</monospace> and <monospace>brainpy.math.LengthDelay</monospace> are provided to support state-dependent variables. Both <monospace>brainpy.math.TimeDelay</monospace> and <monospace>brainpy.math.LengthDelay</monospace> are used to delay neuronal signals in BrainPy, for example, Boolean spikes, real-valued synaptic conductance, or membrane potentials.</p><p>The <monospace>TimeDelay</monospace> is the class representing a connection delay measured in time units. For example, one might specify a <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula> ms delay. The TimeDelay class stores history values at times <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the current time and <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> is the time step. Users can retrieve the history values for any <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> in the interval <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>. For a <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> that exactly matches one of the stored time points, TimeDelay directly returns the stored history values; otherwise, TimeDelay uses linear interpolation to estimate values between the nearest stored data points, or rounding to return the value from the nearest data point in time.</p><p>The <monospace>LengthDelay</monospace> class represents a connection delay based on the number or length of running iterations. For instance, one can specify a delay of <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> iterations. It stores historical values at previous time steps, such as <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>. Unlike <monospace>TimeDelay</monospace>, users can only retrieve the historical values at specific discrete time steps that have been stored.</p><p>Despite the distinct usage characteristics of <monospace>TimeDelay</monospace> and <monospace>LengthDelay</monospace>, both of them employ a circular array for their delay updates. This implementation involves utilizing an array of fixed length along with a cursor that indicates the current updating position within the array (see <xref ref-type="fig" rid="app5fig1">Appendix 5âfigure 1</xref>).</p><fig id="app5fig1" position="float"><label>Appendix 5âfigure 1.</label><caption><title>The dynamic array to store the delay buffer.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-app5-fig1-v2.tif"/></fig><p>The classes <monospace>brainpy.math.NeuTimeDelay</monospace> and <monospace>brainpy.math.NeuLenDelay</monospace> are identical to <monospace>brainpy.math.TimeDelay</monospace> and <monospace>brainpy.math.LengthDelay</monospace>, respectively. However, they are specifically designed to model neutral delay variables.</p><p>With these delay supports, the differential equations with delayed variables are intrinsically supported in each integral function.</p><p>For delayed ODEs, users can use</p><p><code xml:space="preserve">1 brainpy.odeint(f=&lt;function&gt; , 
2 âââââââââââââââââmethod=&lt;str&gt; , 
3 âââââââââââââââââdt=&lt;float&gt; , 
4 âââââââââââââââââstate_delays=&lt;dict&gt; , 
5 âââââââââââââââââneutral_delays=&lt;dict&gt;)</code></p><p>Similarly, the numerical integration of delayed SDEs should utilize</p><p><code xml:space="preserve">1 brainpy.sdeint(f=&lt;function&gt; , 
2 âââââââââââââââââg=&lt;function&gt; , 
3 âââââââââââââââââmethod=&lt;str&gt; , 
4 âââââââââââââââââdt=&lt;float&gt; , 
5 âââââââââââââââââstate_delays=&lt;dict&gt;)</code></p><p>For delayed FDEs, one can use</p><p><code xml:space="preserve">1 brainpy.fdeint(f=&lt;function&gt; , 
2 âââââââââââââââââmethod=&lt;str&gt; , 
3 âââââââââââââââââdt=&lt;float&gt; , 
4 ââââââââââââââââânum_memory=&lt;int&gt; , 
5 âââââââââââââââââinits=&lt;dict&gt;, 
6 âââââââââââââââââstate_delays=&lt;dict&gt;)</code></p><p>Note that we currently do not support neutral delays for delayed SDEs and FDEs. However, users can easily customize their supports for equations with neutral delays.</p></sec></sec></sec></app><app id="appendix-6"><title>Appendix 6</title><sec sec-type="appendix" id="s14"><title>Object-oriented JIT compilation and automatic differentiation</title><p>Under minimal constraints and assumptions, BrainPy enables the JIT compilation for class objects. These assumptions include the following:</p><list list-type="bullet"><list-item><p>The class for JIT must be a subclass of brainpy.BrainPyObject.</p></list-item><list-item><p>Dynamically changed variables in the class must be labeled as brainpy.math.Variable. Otherwise, they will be compiled as constants and cannot be updated during the program execution.</p></list-item></list><p>To take advantage of the JIT compilation, we can directly apply brainpy.math.jit() onto the instantiated class objects, or functions of a class object.</p><p><monospace>brainpy.math.grad()</monospace> takes a function/object (<inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which returns a scalar value) as the input and returns a new function (<inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">â</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">â</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>) which computes the gradient of the original function/object.</p><p><code xml:space="preserve">1 grad_f=brainpy.math.grad(f, grad_vars=&lt;variables to take gradients&gt;)</code></p><p><monospace>brainpy.math.vector_grad()</monospace> takes vector-valued gradients for a function/object (<inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">â</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>). It returns a new function (<inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">â</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">â</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>) which evaluates the vector-Jacobian products.</p><p><code xml:space="preserve">1 grad_f=brainpy.math.vector_grad(f, grad_vars&lt;variables to take gradients&gt;)</code></p><p>Another way to take gradients of a vector-output value is using brainpy.math.jacobian(). It aims to automatically compute the Jacobian matrices <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">â</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi><mml:mo>Ã</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> by the given function/object <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">â</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> at the given point of <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>.</p><p><code xml:space="preserve">1 grad_f=brainpy.math.jacobian(f, grad_vars&lt;variables to take gradients&gt;)</code></p></sec></app><app id="appendix-7"><title>Appendix 7</title><sec sec-type="appendix" id="s15"><title>Extension of BrainPy infrastructure</title><p>BrainPy features extensible architecture. New extensions can be easily made by using BrainPy infrastructure. Even new tools at the infrastructure level can be customized by using BrainPy operators.</p><p>At the toolbox level, BrainPy provides a mechanism in which the extension of a new tool can be made by adding a new subclass. For instance, a new synaptic connection method can be extended by subclassing <monospace>brainpy.connect.TwoEndConnector</monospace>:</p><p><code xml:space="preserve">1 class NewConnector(bp.conn.TwoEndConnector): 
2 ââdef __init__(self): 
3 âââ# initializing connector 
4 
5 ââdef build_csr(self): 
6 âââ# build the CSR data structure 
7 
8 ââdef build_coo(self): 
9 âââ# build the COO data structure</code></p><p>As another example, customizing a new weight initialization method can be added by inheriting from brainpy.initialize.Initializer base class:</p><p><code xml:space="preserve">1 class NewInitializer(bp.init.Initializer): 
2 ââdef __init__(self): 
3 âââ# initializing connector 
4 
5 ââdef __call__(self, shape, dtype =None): 
6 âââ# building weights</code></p><p>At the model building level, BrainPy enables to flexibly customize the userâs own dynamical systems by simply subclassing brainpy.DynamicalSystem base class.</p><p>At the numerical integrator level, BrainPy provides an easy way to write new integrator methods. For example, adding a new RungeâKutta integrator can be done by subclassing <monospace>brainpy.ode.ExplicitRKIntegrator</monospace> and providing the corresponding Butcher tableau:</p><p><code xml:space="preserve">1 class NewExplicitRK(bp.ode.ExplicitRKIntegrator): 
2 ââA = [] # The A matrix in the Butcher tableau. 
3 ââB = [] # The B vector in the Butcher tableau. 
4 ââC = [] # The C vector in the Butcher tableau.</code></p><p>In a similar way, a customized adaptive RungeâKutta integrator can be extended by subclassing <monospace>brainpy.ode.AdaptiveRKIntegrator</monospace> with the specification of the corresponding Butcher tableau:</p><p><code xml:space="preserve">1 class NewAdaptiveRK(bp.ode.ExplicitRKIntegrator): 
2 ââA = [] # The A matrix in the Butcher tableau. 
3 ââB1 = [] # The B1 vector in the Butcher tableau. 
4 ââB2 = [] # The B2 vector in the Butcher tableau. 
5 ââC = [] # The C vector in the Butcher tableau.</code></p><p>At the operator level, BrainPy is trying to provide a way to write fine-granularity low-level operators in the same Python interface by adopting the same JIT compilation technology (please refer to Appendix 8).</p></sec></app><app id="appendix-8"><title>Appendix 8</title><sec sec-type="appendix" id="s16"><title>Extension of low-level operators in BrainPy</title><p>By bridging Numba (<xref ref-type="bibr" rid="bib52">Lam et al., 2015</xref>), JAX (<xref ref-type="bibr" rid="bib29">Frostig et al., 2018</xref>), and XLA (<xref ref-type="bibr" rid="bib76">Sabne, 2020</xref>), BrainPy enables the customization of primitive operators through the native Python syntax. Exposing a custom operator to JAX requires registering an XLA âcustom callâ, and providing its C callback for Python. Based on the following two properties of Numba, we are aware of the possibility of using Numba as a convenient method for writing low-level kernels that support JAXâs JIT compilation. First, unlike JAX, which only supports the JIT compilation of high-level functions, Numba is a JIT compiler that allows users to write a function with low-level fine-grained operations, for instance, looping over an array, or conditional branching over array values. This implies that <monospace>numba.jit()</monospace> can be used as a means to write low-level kernel functions. The second property of Numba is that it provides a mechanism to create a compiled function that is callable from the foreign C code, such as XLA. Specifically, <monospace>numba.cfunc()</monospace> can be used to create a C callback for Numbaâs JIT function to interface with XLA. Therefore, by integrating Numba with JAX and XLA, BrainPy provides an interface where users write primitive operators directly with Python syntax. Note that Numba supports various native Python features and a large subset of NumPy functions. Therefore, there is a large flexibility in coding low-level primitives with Numba.</p><p>Below, we illustrate how to write a primitive operator in BrainPy. Particularly, to customize a primitive operator we need to provide two functions. The first is an abstract evaluation function that tells JAX what shapes and types of outputs are produced according to the input shapes and types:</p><p><code xml:space="preserve">1 def abstract_eval(*ins): 
2 ââreturn outs</code></p><p>in which <monospace>ins</monospace> specifies the information of input shapes and types, outs denotes the array information of shapes and types of outputs. The other function is the concrete computation function, in which the output data is calculated according to the input data:</p><p><code xml:space="preserve">1 def concrete_comp(outs, ins): 
2 ââ# calculate outputs according to inputs</code></p><p>where outs and ins are output and input data owned by XLA, respectively. Note this function should not return anything. All computations must be made in place. Finally, by using</p><p><code xml:space="preserve">1 # &quot;name&quot; is the operator name 
2 # &quot;concrete_comp&quot; is the concrete computation function 
3 # &quot;abstract_eval&quot; is the abstract evaluation function 
4 op =brainpy.math.CustomOpByNumba(abstract_eval, concrete_comp, name)</code></p><p>we register a primitive operator op. This operator op can be used anywhere the user wants. The Numba-based approach for operator customization demonstrates comparable performance to the native C++-based XLA operator registration. To illustrate this, we conducted a performance comparison of the COBA network model (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>) using both an event-driven operator implementation based on Numba and C++.</p><p>This event-driven operator implements the matrixâvector multiplication <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for synaptic computation, where <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the presynaptic spikes, <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> the synaptic connection matrix, and <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> the postsynaptic current. Specifically, it performs matrixâvector multiplication in a sparse and efficient way by exploiting the event property of the input vector <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Particularly, we performs event-driven matrixâvector multiplication only for the non-zero elements of the vector, which are called events. This can reduce the number of operations and memory accesses, and improve the running performance of matrixâvector multiplication.</p><p>Based on our Python-level registration syntax, this operator can be implemented as:</p><p><code xml:space="preserve">1 from jax.core import ShapedArray 
2 import brainpy.math as bm 
3 
4 # the abstract evaluation function 
5 def abs_eval(events, indices, indptr, *, weight, post_num): 
6 âââreturn [ShapedArray((post_num,), bm.float32), ] 
7 
8 # the concrete function implementing the event-driven computation 
9 def con_compute(outs, ins): 
10 ââpost_val, =outs 
11 ââpost_val.fill(0) 
12 ââevents, indices, indptr, weight, _=ins 
13 ââweight=weight[()] 
14 ââfor i in range(events.size): 
15 ââââif events[i]: 
16 ââââââfor j in range(indptr[i], indptr[i+1]): 
17 ââââââââindex =indices[j] 
18 ââââââââpost_val[index]+=weight 
19 
20 # operator registration 
21 event_sum =bm.CustomOpByNumba(eval_shape =abs_eval, cpu_func =con_compute)</code></p><p>Listing 1: The event-driven operator implemented with our Python syntax for the computation of <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>This operator can also be implemented with the pure C++ syntax:</p><p><code xml:space="preserve">1 #include &lt;cstdint&gt; 
2 #include &lt;cstring&gt; 
3 #include &lt;cmath&gt; 
4 
5 template &lt;typename F, typename I&gt;
6 void cpu_csr_event_sum_homo(void *out, const void **in) { 
7 âââconst std::uint32_t pre_size = *reinterpret_cast&lt;const std::uint32_t *&gt;(in[0]); 
8 âââconst std::uint32_t post_size = *reinterpret_cast&lt;const std::uint32_t *&gt;(in(1)); 
9 âââconst bool *events=reinterpret_cast&lt;const bool *&gt;(in(2)); 
10 ââconst I *indices=reinterpret_cast &lt;const I *&gt;(in(3)); 
11 ââconst I *indptr=reinterpret_cast &lt;const I *&gt;(in(4)); 
12 ââconst F weight=reinterpret_cast&lt;const F *&gt;(in(5))[0]; 
13 ââF *result=reinterpret_cast&lt;F *&gt;(out); 
14 
15 ââ// algorithm 
16 ââmemset(&amp;result[0], 0, sizeof(F) * post_size); 
17 ââfor (std::uint32_t i=0; i&lt;pre_size; ++i) { 
18 ââââif (events[i]){ 
19 ââââââfor (I j=indptr[i]; j&lt;indptr[i+1]; ++j) { 
20 ââââââââresult[indices[j]]+=weight; 
21 ââââââ} 
22 ââââ} 
23 ââ} 
24 }</code></p><p>Listing 2: The event-driven operator utilizes C++ for the computation of <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here, we present the main code snippet that implements the event-driven matrixâvector multiplication. Please note that we have omitted the code for Python binding, compilation, registration, and other related aspects. For detailed instructions and a comprehensive tutorial on XLA operator customization, we encourage users to refer to the appropriate resource.</p><p>The speed comparison between the two approaches has been depicted in <xref ref-type="fig" rid="fig9">Figure 9</xref>. Although the approach shows promising results on CPU hardware, it is not directly compatible with other computing platforms like GPUs. This restricts the applicability and scalability of the proposed method, as GPUs are increasingly used to accelerate brain dynamics models. To overcome this limitation, currently, we are working on alternative approach that can be used in both CPU and GPU devices, allowing for broader utilization of available hardware resources and unlocking new possibilities for customizing any complex brain dynamics operators.</p></sec></app><app id="appendix-9"><title>Appendix 9</title><sec sec-type="appendix" id="s17"><title>Multi-scale spiking neural network for the visual system modeling</title><p>We build a spiking network model examined in <xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="fig" rid="fig3">Figure 3</xref> for modeling the visual system (V1, V2, V4, TEO, and TEpd). Simulations are performed using a network of HodgkinâHuxley neurons, with the local circuit and long-range connectivity structure derived from <xref ref-type="bibr" rid="bib59">Markov et al., 2014</xref>. Each of the five areas consists of 2000 neurons, with 1600 excitatory and 400 inhibitory neurons.</p><p>For each neuron, the membrane potential dynamics is modified from <xref ref-type="bibr" rid="bib90">Traub and Miles, 1991</xref> and is described by the following equations:<disp-formula id="equ6"><label>(1)</label><mml:math id="m6"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>K</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi></mml:mstyle></mml:math></inline-formula> is the membrane potential, <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> stands for synaptic interactions, <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>C</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the membrane capacitance per unit area, <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>90</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>50</mml:mn><mml:mspace width="thinmathspace"/><mml:mtext>mV</mml:mtext></mml:mstyle></mml:math></inline-formula> are the potassium and sodium reversal potentials, respectively, <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>60</mml:mn><mml:mspace width="thinmathspace"/><mml:mtext>mV</mml:mtext></mml:mstyle></mml:math></inline-formula> is the leak reversal potential, <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>30</mml:mn><mml:mspace width="thinmathspace"/><mml:msup><mml:mtext>mS/cm</mml:mtext><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mspace width="thinmathspace"/><mml:msup><mml:mtext>mS/cm</mml:mtext><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> are the potassium and sodium conductance per unit area, respectively, and <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn><mml:mspace width="thinmathspace"/><mml:msup><mml:mtext>mS/cm</mml:mtext><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> is the leak conductance per unit area.</p><p>Each neuron is composed of two active ion channels, including the potassium and sodium channels. Because the potassium and sodium channels are voltage sensitive, according to the biological experiments, <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>h</mml:mi></mml:mstyle></mml:math></inline-formula> are used to simulate the activation of the channels.<disp-formula id="equ7"><label>(2)</label><mml:math id="m7"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mi>Î±</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>Î²</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>m</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ8"><label>(3)</label><mml:math id="m8"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mi>Î±</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>Î²</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>h</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ9"><label>(4)</label><mml:math id="m9"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mi>Î±</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>Î²</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>n</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Specifically, <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> measures the activation of potassium channels, and <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>h</mml:mi></mml:mstyle></mml:math></inline-formula> measures the activation and inactivation of sodium channels, respectively. <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Î±</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Î²</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are rate constants for the ion channel <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> and depend exclusively on the membrane potential. The voltage-dependent expressions of the rate constants were modified from the model described by <xref ref-type="bibr" rid="bib90">Traub and Miles, 1991</xref>:<disp-formula id="equ10"><label>(5)</label><mml:math id="m10"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>Î±</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>0.32</mml:mn><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>13</mml:mn><mml:mo>â</mml:mo><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>13</mml:mn><mml:mo>â</mml:mo><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ11"><label>(6)</label><mml:math id="m11"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>Î²</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>0.28</mml:mn><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:mn>40</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:mn>40</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>5</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ12"><label>(7)</label><mml:math id="m12"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>Î±</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>0.128</mml:mn><mml:mo>â</mml:mo><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>17</mml:mn><mml:mo>â</mml:mo><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>18</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ13"><label>(8)</label><mml:math id="m13"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>Î²</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>40</mml:mn><mml:mo>â</mml:mo><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>5</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ14"><label>(9)</label><mml:math id="m14"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>Î±</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>0.032</mml:mn><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>15</mml:mn><mml:mo>â</mml:mo><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>15</mml:mn><mml:mo>â</mml:mo><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>5</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ15"><label>(10)</label><mml:math id="m15"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>Î²</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>â</mml:mo><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>10</mml:mn><mml:mo>â</mml:mo><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>40</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> = â63 mV adjusts the threshold.</p><p>For the synapse, we use conductance-based synaptic interactions. Particularly, <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is given by:<disp-formula id="equ16"><label>(11)</label><mml:math id="m16"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:munder><mml:mo>â</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msub><mml:mi>g</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the membrane potential of neuron <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>. The synaptic conductance from neuron <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>j</mml:mi></mml:mstyle></mml:math></inline-formula> to neuron <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> is represented by <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>g</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, while <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> signifies the reversal potential of that synapse. For excitatory synapses, <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> was set to 0 mV, whereas for inhibitory synapses, it was â80 mV. The dynamics of the synaptic conductance is given by:<disp-formula id="equ17"><label>(12)</label><mml:math id="m17"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mfrac><mml:msub><mml:mi>g</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:munder><mml:mo>â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>Î´</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> is the spiking time of the presynaptic spike. Whenever a spike occurred in neuron <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>j</mml:mi></mml:mstyle></mml:math></inline-formula>, the synaptic conductance <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>g</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> experienced an immediate increase by a fixed amount <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>g</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. Subsequently, the conductance <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>g</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> decayed exponentially with a time constant of <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Ï</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mstyle></mml:math></inline-formula> ms for excitation and <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Ï</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mstyle></mml:math></inline-formula> ms for inhibition.</p><p>The connection density is set according to the experimental connectivity data (<xref ref-type="bibr" rid="bib59">Markov et al., 2014</xref>). The inter-areal connectivity is measured as a weight index (see <xref ref-type="table" rid="app9table1">Appendix 9âtable 1</xref>), called the extrinsic fraction of labeled neurons (<xref ref-type="bibr" rid="bib59">Markov et al., 2014</xref>). The intra-area connectivity is set according to the setting in a standard E/I balanced network (<xref ref-type="bibr" rid="bib13">Brette et al., 2007</xref>).</p><table-wrap id="app9table1" position="float"><label>Appendix 9âtable 1.</label><caption><title>The weighted connectivity matrix among five brain areas: V1, V2, V4, TEO, and TEpd (<xref ref-type="bibr" rid="bib59">Markov et al., 2014</xref>).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">V1</th><th align="left" valign="bottom">V2</th><th align="left" valign="bottom">V4</th><th align="left" valign="bottom">TEO</th><th align="left" valign="bottom">TEpd</th></tr></thead><tbody><tr><td align="left" valign="bottom">V1</td><td align="char" char="." valign="bottom">0.0</td><td align="char" char="." valign="bottom">0.7322</td><td align="char" char="." valign="bottom">0.1277</td><td align="char" char="." valign="bottom">0.2703</td><td align="char" char="." valign="bottom">0.003631</td></tr><tr><td align="left" valign="bottom">V2</td><td align="char" char="." valign="bottom">0.7636</td><td align="char" char="." valign="bottom">0.0</td><td align="char" char="." valign="bottom">0.1513</td><td align="char" char="." valign="bottom">0.003274</td><td align="char" char="." valign="bottom">0.001053</td></tr><tr><td align="left" valign="bottom">V4</td><td align="char" char="." valign="bottom">0.0131</td><td align="char" char="." valign="bottom">0.3908</td><td align="char" char="." valign="bottom">0.0</td><td align="char" char="." valign="bottom">0.2378</td><td align="char" char="." valign="bottom">0.07488</td></tr><tr><td align="left" valign="bottom">TEO</td><td align="char" char="." valign="bottom">0.0</td><td align="char" char="." valign="bottom">0.02462</td><td align="char" char="." valign="bottom">0.2559</td><td align="char" char="." valign="bottom">0.0</td><td align="char" char="." valign="bottom">0.2313</td></tr><tr><td align="left" valign="bottom">TEpd</td><td align="char" char="." valign="bottom">0.0</td><td align="char" char="." valign="bottom">0.000175</td><td align="char" char="." valign="bottom">0.0274</td><td align="char" char="." valign="bottom">0.1376</td><td align="char" char="." valign="bottom">0.0</td></tr></tbody></table></table-wrap><p>Moreover, we introduce distance-dependent inter-areal synaptic delays by assuming a conduction velocity of 3.5 m/s (<xref ref-type="bibr" rid="bib87">Swadlow, 1990</xref>) and using a distance matrix based on experimentally measured wiring distances across areas (<xref ref-type="bibr" rid="bib59">Markov et al., 2014</xref>).</p><table-wrap id="app9table2" position="float"><label>Appendix 9âtable 2.</label><caption><title>The delay matrix (ms) among five brain areas: V1, V2, V4, TEO, and TEpd (<xref ref-type="bibr" rid="bib59">Markov et al., 2014</xref>).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">V1</th><th align="left" valign="bottom">V2</th><th align="left" valign="bottom">V4</th><th align="left" valign="bottom">TEO</th><th align="left" valign="bottom">TEpd</th></tr></thead><tbody><tr><td align="left" valign="bottom">V1</td><td align="char" char="." valign="bottom">0.</td><td align="char" char="." valign="bottom">2.6570</td><td align="char" char="." valign="bottom">4.2285</td><td align="char" char="." valign="bottom">4.2571</td><td align="char" char="." valign="bottom">7.2857</td></tr><tr><td align="left" valign="bottom">V2</td><td align="char" char="." valign="bottom">2.6570</td><td align="char" char="." valign="bottom">0.</td><td align="char" char="." valign="bottom">2.6857</td><td align="char" char="." valign="bottom">3.4857</td><td align="char" char="." valign="bottom">5.6571</td></tr><tr><td align="left" valign="bottom">V4</td><td align="char" char="." valign="bottom">4.2285</td><td align="char" char="." valign="bottom">2.6857</td><td align="char" char="." valign="bottom">0.</td><td align="char" char="." valign="bottom">2.8</td><td align="char" char="." valign="bottom">4.7143</td></tr><tr><td align="left" valign="bottom">TEO</td><td align="char" char="." valign="bottom">4.2571</td><td align="char" char="." valign="bottom">3.4857</td><td align="char" char="." valign="bottom">2.8</td><td align="char" char="." valign="bottom">0.</td><td align="char" char="." valign="bottom">3.0571</td></tr><tr><td align="left" valign="bottom">TEpd</td><td align="char" char="." valign="bottom">7.2857</td><td align="char" char="." valign="bottom">5.6571</td><td align="char" char="." valign="bottom">4.7143</td><td align="char" char="." valign="bottom">3.0571</td><td align="char" char="." valign="bottom">0.</td></tr></tbody></table></table-wrap></sec></app><app id="appendix-10"><title>Appendix 10</title><sec sec-type="appendix" id="s18"><title>Training reservoir computing model with multiple algorithms</title><p>Reservoir computing is a type of recurrent neural network that is often used for processing temporal data. Unlike traditional recurrent neural networks, reservoir computing fixes the weights of the recurrent layer (known as the âreservoirâ) and only trains the weights of the output layer. This makes training much more efficient.</p><p>A reservoir computing model can be trained using various algorithms, such as online learning, offline learning, and backpropagation learning, to optimize its performance. Online learning refers to the process of updating the model in real time as new data becomes available. This algorithm allows the model to adapt and adjust its predictions continuously. Offline learning, on the other hand, involves training the model using a fixed dataset, where the entire dataset is used to update the modelâs parameters. This method is particularly useful when a large amount of data is available upfront. Lastly, backpropagation learning utilizes a technique called backpropagation to train the model by computing the gradients of the modelâs parameters and adjusting them accordingly.</p><p>The unified building and training interface of BrainPy enables the training of the same reservoir model with multiple training algorithms. By employing BrainPy, we can compare the performance of different training algorithms and determine which approach yields the best results for the reservoir computing model.</p><p>The following lists the details of such training.</p></sec><sec sec-type="appendix" id="s19"><title>Reservoir model</title><p>The dynamics of the reservoir model used here are given by:<disp-formula id="equ18"><label>(13)</label><mml:math id="m18"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Î±</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>Î±</mml:mi><mml:mo>â</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ19"><label>(14)</label><mml:math id="m19"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is the reservoir state, <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> the readout value, <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are input and recurrent connection matrices, respectively, <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> the readout weight matrix which can be trained by either offline learning or online learning algorithms, <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Î±</mml:mi><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> the leaky rate, <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> the input at time step <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi></mml:mstyle></mml:math></inline-formula> the nonlinear activation function.</p><p>In BrainPy, the reservoir model can be easily instantiated as the following code:</p><p><code xml:space="preserve">1 reservoir =brainpy.dyn.Reservoir(input_shape, output_shape, leaky_rate)</code></p></sec><sec sec-type="appendix" id="s20"><title>Inferring Lorenz strange attractor</title><p>The reservoir model is utilized for inference tasks in this work. To generate training and testing data, we numerically integrate a simplified model of a weather system originally developed by <xref ref-type="bibr" rid="bib56">Lorenz, 1963</xref>. This model comprises three coupled nonlinear differential equations:<disp-formula id="equ20"><label>(15)</label><mml:math id="m20"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo>Ë</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>â</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ21"><label>(16)</label><mml:math id="m21"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>y</mml:mi><mml:mo>Ë</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>28</mml:mn><mml:mo>â</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ22"><label>(17)</label><mml:math id="m22"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo>Ë</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mo>â</mml:mo><mml:mn>8</mml:mn><mml:mi>z</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3.</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The state <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> represents a vector of RayleighâBÃ©nard convection observables. The system exhibits deterministic chaos, displaying sensitivity to initial conditions and forming a strange attractor in the phase space trajectory (<xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p><p>In this task, we train a reservoir model to predict the <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula>-step-ahead value of all Lorenz variables, <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>z</mml:mi></mml:mstyle></mml:math></inline-formula>. During training, we provide the model with all three variables. During testing, we only provide the model with <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>z</mml:mi></mml:mstyle></mml:math></inline-formula> at <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula>-step-ahead, and it must infer <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>z</mml:mi></mml:mstyle></mml:math></inline-formula> after <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula> steps. Here, we choose <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mstyle></mml:math></inline-formula>.</p><p>In this task, the input size was set to 3, the recurrent layer of the reservoir model contained 100 units, and the output size was 3, as used in <xref ref-type="fig" rid="fig4">Figure 4D-F</xref>.</p></sec><sec sec-type="appendix" id="s21"><title>Training with ridge regression</title><p>The training objective of reservoir models is to find the optimal <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> that minimizes the square error between <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. The common way to learn the linear output weight <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is using the ridge regression algorithm. The ridge regression, also known as regression with Tikhonov regularization, is given by:<disp-formula id="equ23"><label>(18)</label><mml:math id="m23"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>out </mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>target </mml:mtext></mml:mrow></mml:msup><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">X</mml:mi><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>Î²</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Î²</mml:mi></mml:mstyle></mml:math></inline-formula> is a regularization coefficient, <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>I</mml:mi></mml:mstyle></mml:math></inline-formula> is the identity matrix, and <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>X</mml:mi></mml:mstyle></mml:math></inline-formula> is the concatenated hidden states of all samples.</p><p>In BrainPy, the reservoir model trained with ridge regression can be implemented as:</p><p><code xml:space="preserve">1 model =ESN(num_in, num_rec, num_out) 
2 trainer =bp.RidgeTrainer(model) 
3 trainer.fit([X, Y]) 
4 outputs =trainer.predict(X)</code></p></sec><sec sec-type="appendix" id="s22"><title>Training with FORCE learning</title><p>Ridge regression is an offline learning method, meaning that it needs all of the data to be present before it can learn the model parameters. This can be a problem when training reservoir models with a lot of data, because the memory requirements can be too high for some devices. FORCE learning (<xref ref-type="bibr" rid="bib84">Sussillo and Abbott, 2009</xref>), on the other hand, is an online learning algorithm. This means that it can learn the model parameters one sample of data at a time. This makes it a much more efficient way to train reservoir models with large datasets. Therefore, FORCE learning is a good choice for training reservoir models when the amount of data is large or the memory resources are limited.</p><p>The FORCE learning is done using the recursive least squares (RLS) algorithm. It is a supervised error-driven learning rule, that is the weight changes depending on the error made by each neuron: the difference between the output of a neuron <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and a desired value <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.<disp-formula id="equ24"><label>(19)</label><mml:math id="m24"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Contrary to the delta learning rule which modifies weights proportionally to the error and to the direct input to a synapse (<inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Î</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mi>Î·</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>), the RLS learning uses a running estimate of the inverse correlation matrix of the inputs to each neuron:<disp-formula id="equ25"><label>(20)</label><mml:math id="m25"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Î</mml:mi><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:munder><mml:mo>â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>P</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>â</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Each neuron <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> therefore stores a square matrix <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>P</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, whose size depends on the number of weights arriving at the neuron. Readout neurons receive synapses from all <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi></mml:mstyle></mml:math></inline-formula> recurrent neurons, so the <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi></mml:mstyle></mml:math></inline-formula> matrix is <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>â</mml:mo><mml:mi>N</mml:mi></mml:mstyle></mml:math></inline-formula>.</p><p>The inverse correlation matrix <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi></mml:mstyle></mml:math></inline-formula> is updated at each time step with the following rule:<disp-formula id="equ26"><label>(21)</label><mml:math id="m26"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Î</mml:mi><mml:msubsup><mml:mi>P</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>P</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>â</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:munder><mml:mo>â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>â</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Each matrix <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>P</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> is initialized to the diagonal matrix and scaled by a factor <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>Î´</mml:mi></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Î´</mml:mi></mml:mstyle></mml:math></inline-formula> is 1 in the current implementation and can be used to modify implicitly the learning rate (<xref ref-type="bibr" rid="bib84">Sussillo and Abbott, 2009</xref>).</p><p>In BrainPy, the reservoir model trained with FORCE learning can be implemented as:</p><p><code xml:space="preserve">1 model =ESN(num_in, num_rec, num_out) 
2 trainer =bp.ForceTrainer(model, alpha =0.1) 
3 trainer.fit([X, Y]) 
4 trainer.fit([X, Y]) 
5 outputs =trainer.predict(X)</code></p></sec><sec sec-type="appendix" id="s23"><title>Training with backpropagation algorithm</title><p>The readout layer in the reservoir model is just a single-layer Perceptron. To train its weights, we can use the backpropagation algorithm. Backpropagation is a method used in artificial neural networks to calculate a gradient that is needed in the calculation of the weights to be used in the network. The loss function used here is the mean square error between the reservoir output and the target output:<disp-formula id="equ27"><label>(22)</label><mml:math id="m27"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munder><mml:mo>â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>j</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>The updated weight between neuron <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>j</mml:mi></mml:mstyle></mml:math></inline-formula> is calculated by:<disp-formula id="equ28"><label>(23)</label><mml:math id="m28"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Î</mml:mi><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>In BrainPy, the reservoir model trained with backpropagation algorithms can be implemented as:</p><p><code xml:space="preserve">1 reservoir = bp.dyn.Reservoir(num_in, num_rec) 
2 readout = bp.dnn.Dense(num_rec, num_out, mode =bm.training_mode) 
3 trainer = bp.BPFF(target =readout, 
4 ââââââââââââââââââââloss_fun =bp.losses.mean_squared_error, 
5 ââââââââââââââââââââoptimizer =bp.optim.Adam(1e-3)) 
6 # batch_data: the data generated by &quot;reservoir&quot; 
7 trainer.fit(batch_data, num_epoch =100)</code></p></sec><sec sec-type="appendix" id="s24"><title>Evaluation metrics</title><p>The performance of a reservoir computing model is usually measured with the mean squared error, that is, the average squared difference between the predicted and actual values:<disp-formula id="equ29"><label>(24)</label><mml:math id="m29"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>target </mml:mtext></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:munderover><mml:mo>â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover><mml:mo>â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>target </mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the number of training samples and <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula> is the number of time steps.</p></sec></app><app id="appendix-11"><title>Appendix 11</title><sec sec-type="appendix" id="s25"><title>Experimental details for benchmark comparisons</title><p>In this section, we provide a comprehensive overview of the experimental benchmark details used in <xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig7">Figure 7</xref>, and <xref ref-type="fig" rid="fig8">Figure 8</xref>. The purpose is to present a complete picture of the experimental setup and methodology employed in our study. The details encompass the following aspects: hardware specifications, software versions, algorithm hyperparameters, and performance measurements.</p><p>By providing these experimental benchmark details, we aim to ensure transparency and reproducibility, allowing readers and researchers to understand and replicate our experiments accurately.</p></sec><sec sec-type="appendix" id="s26"><title>Hardware specifications</title><p>We conducted a series of experiments on various computing devices, namely the CPU, GPU, and TPU, in order to validate and compare the simulation speeds of several widely utilized brain simulators. The brain simulators under investigation included Brian2 (<xref ref-type="bibr" rid="bib82">Stimberg et al., 2019</xref>), NEURON (<xref ref-type="bibr" rid="bib40">Hines and Carnevale, 1997</xref>), NEST (<xref ref-type="bibr" rid="bib31">Gewaltig and Diesmann, 2007</xref>), GeNN (<xref ref-type="bibr" rid="bib104">Yavuz et al., 2016</xref>), and Brian2CUDA (<xref ref-type="bibr" rid="bib4">Alevi et al., 2022</xref>).</p><p>Particularly, the computing devices we used here are:</p><list list-type="bullet"><list-item><p>CPU: The CPU used was the Intel Xeon W-2255 processor. This is a 10-core, 20-thread CPU based on Intelâs Cascade Lake microarchitecture. It has a base clock frequency of 3.7 GHz and a max turbo boost up to 4.5 GHz. The Xeon W-2255 utilizes the LGA2066 socket and supports up to 512 GB of ECC-registered DDR4-2933 RAM across 6 channels. It has 24.75 MB of L3 cache. As a server-grade CPU with a high core count, the Xeon W-2255 is well suited for heavily parallelized simulations.</p></list-item><list-item><p>GPU: The GPU used was an NVIDIA RTX A6000. This is a high-end Ampere architecture GPU aimed at professional visualization, AI, and compute workloads. The RTX A6000 has 10,752 CUDA cores, 336 tensor cores, and 84 RT cores. It comes equipped with 48 GB of GDDR6 memory clocked at 16 Gbps, providing up to 1 TB/s of memory bandwidth. The RTX A6000 has a maximum power consumption of 300 W and requires auxiliary power connectors. It uses a PCIe 4.0x16 interface. With its large number of CUDA cores and abundant memory, the RTX A6000 is well suited for massively parallel simulations and neural network training.</p></list-item><list-item><p>TPU: The TPU simulations leveraged the Kaggle free TPU v3-8 cloud instance. This provides access to one of Googleâs TPU v3 chips via their cloud platform. Specifically, the v3-8 instance gives 8 TPU v3 cores, each providing 128 GB/s of bandwidth to high-performance HBM memory. The TPU v3 is optimized for both training and inferencing of DNNs, providing up to 420 TFLOPS of mixed precision computing. By utilizing Googleâs cloud TPUs, researchers can run models with minimal coding changes while leveraging Googleâs optimized deep learning frameworks.</p></list-item></list></sec><sec sec-type="appendix" id="s27"><title>Software versions</title><p>We carried out benchmark experiments using Python 3.10.12 running on Ubuntu 20.04.6 LTS. For any experiments utilizing a GPU, we used version 11.6 of the NVIDIA CUDA Toolkit to take advantage of the parallel processing capabilities of the GPU hardware. Other dependent software versions are: NumPy 1.24.3, Numba 0.57.1, jax 0.4.16, and jaxlib 0.4.16.</p><p>The comparison brain simulators for benchmarking are:</p><list list-type="bullet"><list-item><p>NEURON at version 8.2.0</p></list-item><list-item><p>NEST at version 3.6</p></list-item><list-item><p>Brian2 at version 2.5.4</p></list-item><list-item><p>GeNN at version 4.8.1</p></list-item><list-item><p>Brian2GeNN at version of 1.7.0: Brian2GeNN is a bridge between Brian2 and GeNN that allows Brian2 users to run their simulations on GPUs using GeNN.</p></list-item><list-item><p>Brian2CUDA at version of 1.0a3: Brian2CUDA is a native CUDA implementation of Brian2 that allows Brian2 users to run their simulations on GPUs without the need for a bridge.</p></list-item></list></sec><sec sec-type="appendix" id="s28"><title>Performance measurements</title><p>By testing against these simulators on benchmark tasks, we aimed to thoroughly evaluate the performance in terms of <italic>simulation speed</italic> and <italic>accuracy</italic> across different model sizes and paradigms.</p><p>When evaluating simulation speed, we focused solely on measuring the execution time of the simulation itself, excluding any time spent on compilation, initialization, or other preparatory steps. This allowed us to make a direct comparison of the raw simulation performance between our simulator and others. We simulated networks of varying sizes. This range of model scales allowed us to determine how well our simulator performs as network size and complexity increase. The final experimental results can be obtained in <xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig7">Figure 7</xref>, and <xref ref-type="fig" rid="fig8">Figure 8</xref>.</p></sec><sec sec-type="appendix" id="s29"><title>Accuracy evaluations</title><p>To evaluate the accuracy, we configured all simulators to use identical model parameters for a fair comparison.</p><p>First, we verified that all simulators generated consistent average firing rates for a given network model (see <xref ref-type="fig" rid="app11fig1">Appendix 11âfigure 1</xref> and <xref ref-type="fig" rid="app11fig2">Appendix 11âfigure 2</xref>).</p><fig id="app11fig1" position="float"><label>Appendix 11âfigure 1.</label><caption><title>The average firing rate per neuron of the COBA network model (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>) was measured across various simulators running on both GPU and TPU devices.</title><p>However, it should be noted that the BrainPy TPU x32 simulation was limited to 40,000 neurons due to memory constraints, resulting in a truncated network size.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-app11-fig1-v2.tif"/></fig><fig id="app11fig2" position="float"><label>Appendix 11âfigure 2.</label><caption><title>The average firing rate per neuron of the COBAHH network model (<xref ref-type="bibr" rid="bib13">Brette et al., 2007</xref>) was measured across various simulators running on both GPU and TPU devices.</title><p>However, it should be noted that the BrainPy TPU x32 simulation was limited to 40,000 neurons due to memory constraints, resulting in a truncated network size.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-app11-fig2-v2.tif"/></fig><p>Second, we qualitatively compared the overall network activity patterns produced by each simulator to ensure they exhibited the same dynamics. While exact spike-to-spike reproducibility was not guaranteed between different simulator implementations, we confirmed that our simulator produced activity consistent with the reference simulators for both firing rates and network-level dynamics (see <xref ref-type="fig" rid="app11fig3">Appendix 11âfigure 3</xref> and <xref ref-type="fig" rid="app11fig4">Appendix 11âfigure 4</xref>).</p><fig id="app11fig3" position="float"><label>Appendix 11âfigure 3.</label><caption><title>Rater diagrams of COBA network model with 4000 neurons (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>) across different simulators on CPU, GPU, and TPU devices.</title><p>Here, we only present the simulation results for the initial 100 ms of the experiment.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-app11-fig3-v2.tif"/></fig><fig id="app11fig4" position="float"><label>Appendix 11âfigure 4.</label><caption><title>Rater diagrams of COBAHH network model with 4000 neurons (<xref ref-type="bibr" rid="bib13">Brette et al., 2007</xref>) across different simulators on CPU, GPU, and TPU devices.</title><p>Here, we only present the simulation results for the initial 100 ms of the experiment.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-app11-fig4-v2.tif"/></fig><p>Third, we recognize that the precision of numerical computation plays a crucial role in accurately simulating biologically detailed neural networks, such as the COBAHH network model used in our benchmarks. To assess the numerical integration accuracy of each neuron in the COBAHH network model, we conducted a 5-s simulation and examined the presence of NaN membrane potentials at the end of the simulation. The occurrence of NaN membrane potentials indicates that the corresponding neurons are no longer active and signifies a loss of simulation accuracy. This analysis was performed on the GPU backend of Brian2GeNN, Brian2CUDA, and BrainPy. The results of this analysis can be found in <xref ref-type="fig" rid="app11fig5">Appendix 11âfigure 5</xref>. We specifically focused on the analysis results of single-precision floating-point simulations, as we did not encounter any NaN results when simulating with double precision. Our observations reveal that the single-precision computation with XLA in BrainPy exhibits a higher proportion of NaN results compared to Brian2CUDA and GeNN. Notably, GeNN demonstrated very few occurrences of NaN membrane potentials after simulation, which may be attributed to the special handling of NaN within the GeNN computation.</p><fig id="app11fig5" position="float"><label>Appendix 11âfigure 5.</label><caption><title>Number of neurons with NaN membrane potential after performing a 5-s long simulation of the COBAHH network model using the single-precision floating point.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-app11-fig5-v2.tif"/></fig><p>To address the issue of a larger number of NaN membrane potentials leading to reduced neuronal spiking and communication spikes, we took steps to resolve this problem in BrainPy. Our aim was to ensure a fair comparison when benchmarking the simulation speed against other brain simulators. Specifically, the occurrence of NaN membrane potentials was attributed to the use of the <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> operation during the integration of the HodgkinâHuxley neuron model with low-precision floating-point calculations. In order to mitigate the catastrophic loss of precision when <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> is close to zero, we replaced this operation with the relative error exponential function, represented by <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. This modification ensures that the numerical calculations do not result in NaN values even during significantly long simulations. As a result of this fix, we did not encounter any instances of NaN membrane potentials, even after conducting extensive and prolonged simulations. We believe that these revised operations contribute to a more equitable benchmarking process, eliminating any potential bias caused by the presence of NaN values. The speed benchmarking can be obtained in <xref ref-type="fig" rid="fig8">Figure 8</xref>.</p></sec><sec sec-type="appendix" id="s30"><title>Experimental settings</title><p>In the below, we describe the experimental setting details used in <xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig7">Figure 7</xref>, and <xref ref-type="fig" rid="fig8">Figure 8</xref>.</p><sec sec-type="appendix" id="s30-1"><title>Figure 6</title><p>In the experiments depicted in <xref ref-type="fig" rid="fig6">Figure 6A</xref>, we exercise precise control over the equivalent theoretical FLOPs (floating-point operations) performed by the LIF (Leaky Integrate-and-Fire) neurons and the matrixâvector multiplication <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi><mml:mo>Ã</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>). In each trial, subsequent to determining the size <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula>, we modify the number of neurons in the LIF simulation, ensuring that they collectively execute the same theoretical FLOPs. To simplify the computation of the total FLOPS, we adopt the Euler method with a single time step to solve the differential equations within the LIF neuron model.</p><p>On the other hand, for the COBA network (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>) experiments showcased in <xref ref-type="fig" rid="fig6">Figure 6B</xref> and <xref ref-type="fig" rid="fig6">Figure 6C</xref>, the dynamical equations were resolved using the Exponential Euler method with a step size of 0.1ms. The synaptic computation was performed through dense matrix multiplication. Given the presynaptic spikes represented by the vector <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the postsynaptic conductance (<inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>) is computed using the equation <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>g</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> denotes the maximum synaptic conductance, and <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> represents the connection matrix. We assess the simulation time of the network using the aforementioned dense operation under two conditions: with JIT compilation and without JIT compilation.</p><p>.</p></sec><sec sec-type="appendix" id="s30-2"><title>Figure 7</title><p>The COBA network model (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>) is simulated using the Exponential Euler numerical integration method to approximate solutions to the differential equations governing network dynamics. A fixed simulation time step of 0.1 ms is utilized for numerical simulations.</p><p>Accurately isolating the computational time spent on simulating synaptic connections in the COBA network is challenging, as the synapses are computed in an event-driven manner based on spiking activity. Since the number of spikes generated in the network varies across simulations, this causes variability in synapse simulation time measurements. To separately quantify time spent on neuron versus synapse computations, we first profile the neuron simulation time in isolation. This is measured by simulating only the neuronal dynamics without any synaptic connections for a 500-ms duration. Next, we profile the end-to-end run time of the full COBA network simulation including both neurons and dynamic synapses for the same 500-ms duration. Finally, the computational time attributed specifically to simulating synapses can be directly estimated by subtracting the isolated neuron simulation time from the full network simulation time. This approach separates and quantifies the contributions of simulating neuronal and synaptic computations to the overall run time of COBA network simulations.</p><p>For the model <monospace>without dedicated OP</monospace>, we employ dense matrix multiplication to perform the synaptic computation, same as the operation used in <xref ref-type="fig" rid="fig6">Figure 6</xref>. On the other hand, For the model <monospace>with dedicated OP</monospace>, we utilize our event-driven operator called <monospace>brainpy.math.event.csrmv</monospace> for handling the synaptic computation. This specialized operator is designed specifically to efficiently handle such computations in an event-driven manner.</p><p>All simulations were performed using a single thread, excluding the acceleration effect of multi-threading concurrence.</p></sec><sec sec-type="appendix" id="s30-3"><title>Figure 8</title><p>Same as <xref ref-type="fig" rid="fig7">Figure 7</xref>, the COBA network model (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>), the COBAHH network model (<xref ref-type="bibr" rid="bib13">Brette et al., 2007</xref>), and the decision-making network model (<xref ref-type="bibr" rid="bib95">Wang, 2002</xref>) are simulated using the Exponential Euler numerical integration method with a fixed simulation time step of 0.1ms.</p><p>The simulations were conducted across a diverse range of computing devices (including CPUs, GPUs, and TPUs), encompassing various hardware configurations, and were executed using networks of different sizes. To ensure statistical robustness, each trial was repeated 10 times, thereby totaling 10 simulations for each experimental setup. These simulations were carried out for a duration of 5 s. Finally, to provide a representative measure, we reported the average time taken across these 10 simulations.</p><p>On the CPU device, we simulate the model using a single thread across different simulators. On Brian2, we open 12 threads for parallel simulation. However, we did not report the simulation speed results of Brian2 runs on the COBA network model, since single-threaded Brian2 runs were faster than using multi-threaded compartments in this case.</p></sec></sec><sec sec-type="appendix" id="s31"><title>Other supplements</title><fig id="app11fig6" position="float"><label>Appendix 11âfigure 6.</label><caption><title>The acceleration ratio of JIT compilation on LIF neuron models (<xref ref-type="bibr" rid="bib2">Abbott, 1999</xref>) and COBA network models (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>).</title><p>(<bold>A</bold>) The acceleration ratio of just-in-time (JIT) compilation on the LIF neuron model is shown in the plot. The plotted values represent the simulation time ratios of the LIF neuron without JIT and with JIT. The top panel illustrates the acceleration on the CPU device, while the bottom panel demonstrates the acceleration on the GPU device. The acceleration ratios on both devices are approximately five times faster. (<bold>B</bold>) The simulation time ratio of the dense operator compared to the LIF neuron model with JIT compilation is shown. The top panel displays the time ratio on the CPU device, and the bottom panel demonstrates the time ratio on the GPU device. The simulation time ratios on both devices are nearly one, indicating that the jitted LIF neuron, with the same number of floating-point operations (FLOPs) as the âDotâ operation, can run equivalently fast. (<bold>C</bold>) The acceleration ratio of the COBA network model (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>) with JIT compilation compared to the model without JIT compilation on the CPU device is shown. The plot demonstrates a tenfold increase in speed after JIT compilation on the CPU device. (<bold>D</bold>) Similar to (<bold>C</bold>), the experiments were conducted on the GPU. Please refer to <xref ref-type="fig" rid="fig6">Figure 6</xref> for the original data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-app11-fig6-v2.tif"/></fig><p>We evaluate the speedup of event-driven operators over traditional dense matrix operators on both CPU and GPU devices. Evaluation results are listed in <xref ref-type="fig" rid="app11fig7">Appendix 11âfigure 7</xref>. Both CPU and GPU devices demonstrate a significant speed advantage of several orders of magnitude when utilizing event-driven operators, in comparison to dense ones. Notably, as the number of neurons increases, event-driven operators showcase an even greater speedup on both platforms.</p><fig id="app11fig7" position="float"><label>Appendix 11âfigure 7.</label><caption><title>The acceleration ratio of dedicated operators of COBA network model (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>) on both CPU (<bold>A</bold>) and GPU (<bold>B</bold>) devices.</title><p>Please refer to <xref ref-type="fig" rid="fig7">Figure 7</xref> for the original data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-app11-fig7-v2.tif"/></fig><p>We also conducted COBA and COBAHH experiments on TPU devices. The experimental results are shown in <xref ref-type="fig" rid="app11fig8">Appendix 11âfigure 8</xref>. Although TPUs can perform double precision operations, they are not as efficient at it as they are at lower precisions such as float16 or bfloat16. Therefore, we here only tested models with single-precision operations.</p><fig id="app11fig8" position="float"><label>Appendix 11âfigure 8.</label><caption><title>The simulation speeds of BrainPy on kaggle TPU v3-8 device.</title><p>(<bold>A</bold>) The simulation time of the COBA (<xref ref-type="bibr" rid="bib94">Vogels and Abbott, 2005</xref>) network model across different network sizes. (<bold>B</bold>) The simulation time of the COBAHH (<xref ref-type="bibr" rid="bib13">Brette et al., 2007</xref>) network model across different network sizes.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86365-app11-fig8-v2.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86365.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Stimberg</surname><given-names>Marcel</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000zhpw23</institution-id><institution>Institut de la Vision</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.10.28.514024" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.10.28.514024"/></front-stub><body><p>The paper introduces a new, important framework for neural modelling that promises to offer efficient simulation and analysis tools for a wide range of biologically-realistic neural networks. It provides convincing support for the ease of use, flexibility, and performance of the framework, and features a solid comparison to existing solutions in terms of accuracy. The work is of potential interest to a wide range of computational neuroscientists and researchers working on biologically inspired machine learning applications.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86365.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Stimberg</surname><given-names>Marcel</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000zhpw23</institution-id><institution>Institut de la Vision</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Stimberg</surname><given-names>Marcel</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000zhpw23</institution-id><institution>Institut de la Vision</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Evans</surname><given-names>Benjamin D</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ayhx656</institution-id><institution>University of Sussex</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.10.28.514024">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.10.28.514024v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;BrainPy: a flexible, integrative, efficient, and extensible framework for general-purpose brain dynamics programming&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Panayiota Poirazi as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Improve the benchmarks and examples. The examples demonstrating the versatility of the simulator lack details (e.g. number of neurons, training method where applicable, etc.). Similarly, the benchmarking procedures are not sufficiently described, and it is not clear whether the results across simulators are comparable. In particular: What are the integration methods, simulation time steps, and floating point precision that were used, and are they identical across simulators? What time is measured exactly (total time including model construction time, synapse creation time, compilation time, or only simulation run time)? What acceleration modes offered by other simulators (multi-threading, MPI, etc.) were used? Were all efforts taken to optimize the code used for other simulators, or is it based on code provided by the developers of those tools? Finally, please scale networks in a way that keeps network activity roughly constant.</p><p>2) Verify/demonstrate the accuracy of the simulator. Please provide evidence that the chosen approach does not trade improved performance against reduced accuracy â or if it does, discuss the trade-off.</p><p>3) Improve the discussion of limitations and weaknesses of the presented tool, and beware &quot;persuasive communication devices&quot; (https://elifesciences.org/articles/88654). Some of the comparisons to existing state-of-the-art tools do not do justice to their capabilities/advantages.</p><p>4) Clarify the target audience of the tool. Is BrainPy primarily intended for computational neuroscientists or machine learning research, or is it particularly aimed at neuro-AI practitioners bridging the two domains? For &quot;pure machine learning&quot;, would the authors consider their tool giving an advantage over using a dedicated tool like PyTorch? For computational neuroscience research, the tool does seem to lack essential local plasticity rules (e.g. Hebbian plasticity or STDP â the latter is mentioned in Appendix 8 but refers to a non-existing class in the package).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>General remarks:</p><p>â The package contains automatic unit/integration tests, but from a cursory look, they (many? most?) only test for the absence of basic errors (i.e. the code does not fail to run), not for the correctness of the result. Some classes (e.g. GapJunction) never seem to be tested at all.</p><p>â Personally, I'd remove the supplementary document, it contains information that could rather go into the documentation, since it risks being outdated quickly.</p><p>Specific remarks:</p><p>â In the abstract, the sentence &quot;Brain dynamics modeling is an indispensable tool for [â¦] modeling the dynamics of the neural circuits&quot; sounds rather redundant/tautological.</p><p>â l.36: &quot;An example â¦&quot; introduces two examples (TensorFlow and PyTorch).</p><p>â References to sections seem to be broken (e.g. l. 119, l. 136).</p><p>â L.163, &quot;as standalone modules&quot; is not very clear.</p><p>â L.177: I'm not sure the authors really mean &quot;recursive&quot; here, shouldn't it be &quot;hierarchical&quot;? Also, l. 181: not sure what the authors exactly mean here, in most simulators, you can e.g. simulate a neuron individually but also include it in a network and simulate as part of the network.</p><p>â Figure 2A: maybe add to the figure/caption that BrainPy is imported as bp? What does sh refer to?</p><p>â Figure 2D: super should refer to System and not Circuit, I think.</p><p>â Figure 3E: Typo in title (&quot;Rater&quot; â &quot;Raster&quot;).</p><p>â L.199: not sure one can say that the figure &quot;demonstrates&quot; that one can simulate models in parallel.</p><p>â L.304: this needs to be qualified a bit, I think, since it is also the idea behind code generation used by several other simulators.</p><p>â Figure 7A: missing y-axis label.</p><p>â Figure 7C: having the ratio in the same plot is an odd choice, I'd at least use a second y-axis to make this clearer.</p><p>â Figure 7D: the y-axis label should not mention Ratio (not plotted here).</p><p>â L.311/Figure 8C: this (and the results) seems a problem with the code written by the authors for Brian2, maybe contact the Brian2 developers?</p><p>â Figure 8C: I'd recommend changing the colors to match the previously used colors to avoid confusion.</p><p>â L. 376: in what way can &quot;standard debugging tools&quot; debug issues with JAX/XAL?</p><p>â Appendix 1, last paragraph: &quot;Moreover, descriptive languagesâ¦&quot; â I do not quite understand this. With descriptive languages, users do not have to use low-level language. Or does this refer to the effort of the tool developers?</p><p>â Appendix 8, l. 1141: apart from the &quot;a piece of cake&quot; expression that I'd rather replace, I did not find any STDP class in BrainPy.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>There are a couple of significant points to mention here:</p><p>You state on p12: &quot;We also implemented this decision-making model with Brian2 on the CPU (although it was stated that Brian2 could target on GPU via code generation with GeNN (Stimberg et al., 2020), we could not run this model on a GPU with Brian2 successfully) and observed that the running speed of Brian2 was significantly slower than that of BrainPy.&quot;. In order to make this a fair test, I would expect more effort in getting the model to run, choose another standard model to run with both platforms, or at least provide a substantial explanation for why you were unable to. The editor is in fact the lead author of Brian2 and would very likely be able to help you get support from the GeNN team.</p><p>I was also unable to create a Docker image on an ARM-based Mac since jaxlib is unavailable for the combination of linux on ARM processors. Until this (reasonably common) combination becomes an official target for compiled binaries, it would be good to provide instructions on how to compile it from source or otherwise work around this missing dependency. Similarly, I was unable to install brainpylib.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>More care should be given to the reproducibility of the benchmarks. When using a benchmark like COBA (Figures 6, 7, and 8), the numerical method should be the same, or changes should be stated and explained. The authors should state how many runs they used for each experiment to obtain the presented timings. From the codes in the repository, we assume that they are always analyzing a single run. We would recommend performing simulation multiple times to catch the deviations in-between runs.</p><p>The authors use a fixed connection probability of 2% in all scaled benchmarks and to compensate for the increasing number of connections, the authors rescale the weights. However, this approach leads to the following consequence: with an increasing number of connections the activity in the population shrinks. For the settings used in the benchmarks, it would be averaged across 5 runs each: 20.83 +/- 0.87 Hz (4,000 neurons); 10.89 +/- 0.34 Hz (8,000 neurons); 5.25 +/- 0.11 Hz (16,000 neurons); 3.61 +/- 0.04 Hz (24,000 neurons); 2.83 +/- 0.02 Hz (32,000 neurons); 2.38 +/- 0.03 Hz (40,000 neurons).</p><p>To retain the average firing rate, one would rescale the probability instead of the weights, e.g., p = 80. / N, where N is the number of neurons while the weight values remain the same in all configurations. Such an approach was used by Stimberg et al. (2020), which has been already cited by the authors. Such an approach retains the firing frequencies at a comparable level, e.g.: 21.91 +/- 1.32 Hz (4,000 neurons); 21.59 +/- 0.71 Hz (8,000 neurons); 21.15 +/- 0.07 Hz (16,000 neurons); 21.33 +/- 0.16 Hz (24,000 neurons); 21.56 +/- 0.19 Hz (32,000 neurons); 21.59 +/- 0.39 Hz (40,000 neurons).</p><p>For the COBAHH benchmark, it is first not clear whether the base firing rate of 37 Hz is not too high. Secondly, as the authors used here also the weight rescale, we can see the same drop in activity with increasing network sizes: 37.08 +/- 0.88 Hz (4,000 neurons); 17.57 +/- 0.43 Hz (8,000 neurons); 3.61 +/- 0.01 Hz (16,000 neurons); 3.11 +/- 0.02 Hz (24,000 neurons); 2.83 +/- 0.03 Hz (32,000 neurons); 2.73 +/- 0.05 Hz (40,000 neurons); 2.53 +/- 0.03 Hz (60,000 neurons); 2.55 +/- 0.05 Hz (80,000 neurons); 2.65 +/- 0.04 Hz (120,000 neurons).</p><p>[Editorsâ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;BrainPy, a flexible, integrative, efficient, and extensible framework for general-purpose brain dynamics programming&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Panayiota Poirazi (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>â The benchmarks, accuracy evaluations and their descriptions have been greatly improved, but they still have shortcomings that at least merit discussion: while the accuracy evaluations in Appendix 11 convincingly argue for no <italic>major</italic> discrepancies between the simulations, they are not enough to unambiguously state that single precision leads to &quot;a significant speedup without the sacrifice of the simulation accuracy&quot;. From my personal experience, simulating a large COBAHH network with single precision will inevitably lead to division by 0 for a few neurons, if not using equations that use special functions such as exprel (scipy, brian2), or vtrap (NEURON) to avoid this issue. This will usually not show in the average firing rate or a raster plot, but having neurons that are no longer active with a membrane potential of NaN should certainly be considered a loss of accuracy. I am aware that a more in-depth analysis of the accuracy (as e.g. in van Albada et al. 2018; doi: 10.3389/fnins.2018.00291) is out of scope, but I think a more careful evaluation of the accuracy is warranted. It should also be noted that the raster plots for NEST and NEURON do look quite different from the other results in Appendix 11 figures 3 and 4 â probably due to different initial conditions? Finally, the authors state that &quot;Notably, BrainPy offers support for utilizing single-precision floating-point (x32) arithmetic&quot;, but this feature is present in several of the other simulators that were benchmarked as well and has been prominently mentioned in the papers introducing the GPU simulators discussed in the manuscript (GeNN, Brian2GeNN, Brian2CUDA).</p><p>â The manuscript (understandably) puts forwards the features of the simulator, but sometimes is not sufficiently making the link to existing literature on the topic and/or implementations in existing simulators. Apart from the single-precision feature mentioned above, this is in particular the case for the sparse and event-driven synaptic connectivity. Implementing such connectivity has been at the heart of simulator development for a long time, in particular in the context of GPU acceleration (for some early examples, see Plesser et al. 2007 http://link.springer.com/chapter/10.1007/978-3-540-74466-5_71; Fidjeland et al. 2009 https://doi.org/10.1109/ASAP.2009.24; Brette and Goodman 2011 https://doi.org/10.1162/NECO_a_00123). Similarly, &quot;just-in-time connectivity&quot; has been implemented before (e.g. Knight and Nowotny 2021 https://doi.org/10.1038/s43588-020-00022-7). Please also note that the review in Appendix 1 should probably mention NMODL for the NEURON simulator (which I would not describe as having &quot;a library of standard models written in C or CUDA&quot;).</p><p>â The manuscript text is still somewhat hard to follow at times due to jargon that is not common usage (which also relates to the previous point). For example, <italic>eLife</italic> readers are not necessarily familiar with technical terms around just-in-time compilation (e.g. &quot;lowering&quot; code onto CPU/GPU, &quot;fusing&quot; operators), and will therefore have a hard time understanding where the speed benefit of JIT compilation comes from. Terms such as &quot;time delays&quot; and &quot;length delays&quot; (Appendix 4) are also non-standard and need explanation (as a side note: the mechanism for delaying spikes is never explained). Finally, readers might confuse the &quot;event-driven operators&quot; mentioned in the manuscript with event-driven simulation (as opposed to clock-driven; see e.g. Ros et al. 2006 10.1162/neco.2006.18.12.2959). I'd suggest the authors carefully reconsider non-standard terms and clearly define them if no commonly used term exist or if a large part of the readership might not be familiar with them. Other examples of such terms/concepts are &quot;bfloat16&quot; (l. 359), &quot;data parallelism&quot; vs. &quot;model parallelism&quot; (l. 479), or even &quot;TPU&quot; (from the abstract on).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86365.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Improve the benchmarks and examples. The examples demonstrating the versatility of the simulator lack details (e.g. number of neurons, training method where applicable, etc.). Similarly, the benchmarking procedures are not sufficiently described, and it is not clear whether the results across simulators are comparable. In particular: What are the integration methods, simulation time steps, and floating point precision that were used, and are they identical across simulators? What time is measured exactly (total time including model construction time, synapse creation time, compilation time, or only simulation run time)? What acceleration modes offered by other simulators (multi-threading, MPI, etc.) were used? Were all efforts taken to optimize the code used for other simulators, or is it based on code provided by the developers of those tools? Finally, please scale networks in a way that keeps network activity roughly constant.</p></disp-quote><p>We have carefully considered these comments and addressed each of these concerns regarding the benchmarks and examples presented in our paper.</p><p>1. Lack of Details in Examples: In the revised version of the paper, we provide additional information and any other pertinent details to enhance the clarity and replicability of our results. Particularly, in Appendix 9, we provide the mathematical description, the number of neurons, the connection density, and delay times used in our multi-scale spiking network; in Appendix 10, we provide the detail description of reservoir models, evaluation metrics, training algorithms, and their implementations in BrainPy; in Appendix 11, we elaborate the hardware and software specifications and experimental details for benchmark comparisons.</p><p>2. Inadequate Description of Benchmarking Procedures: In the revised paper, particularly, in L328-L329 of the main text at section of &quot;Efficient performance of BrainPy&quot; and in Appendix 11, we elaborate on the integration methods, simulation time steps, and floating-point precision used in our experiments. We also ensure that these parameters are clearly stated and identical across all simulators involved in the benchmarking process, see &quot;Accuracy evaluations&quot; in Appendix 11 (L1550 â L1580).</p><p>3. Clarification on Measured Time: In the revised paper, we state that all simulations only measured the model execution time, excluding model construction time, synapse creation time, and compilation time, see &quot;Performance measurements&quot; in Appendix 11 (L1539 â L1548).</p><p>4. Consideration of Acceleration Modes: In the revised version, we provide the simulation speed of other brain simulators on different acceleration modes, see Figure 8. For instance, we utilize the fastest possible option --- the C++ standalone mode in Brian2 --- for speed evaluations. Furthermore, we have requested the developers of the comparison simulators for optimizing the benchmark models, ensuring a fair and accurate comparison.</p><p>5. Scaling Networks to Maintain Activity: In the revised manuscript, we adopt the suggestion of Reviewer #3 and apply the appropriate scaling techniques to maintain consistent network activity throughout our experiments. These details can be found in Appendix 11 (also see Appendix 11âfigure 1 and Appendix 11âfigure 2).</p><disp-quote content-type="editor-comment"><p>2) Verify/demonstrate the accuracy of the simulator. Please provide evidence that the chosen approach does not trade improved performance against reduced accuracy â or if it does, discuss the trade-off.</p></disp-quote><p>We agree that an explicit comparison would help alleviate any doubts and provide a more comprehensive understanding of our framework's accuracy. We have revised our manuscript to include a dedicated section, particularly Appendix 11. In this section, we verified that all simulators generated consistent average firing rates for the given benchmark network models (figure 1 and figure 2 in Appendix 11). These verifications were performed under different network sizes (ranging from 4e<sup>3</sup> to 4e<sup>5</sup>) and different computing platforms (CPU, GPU and TPU). We also qualitatively compared the overall network activity patterns produced by each simulator to ensure they exhibited the same dynamics (figure 3 and figure 4 in Appendix 11). While exact spike-to-spike reproducibility was not guaranteed between different simulator implementations, we confirmed that our simulator produced activity consistent with the reference simulators for both firing rates and network-level dynamics. Additionally, BrainPy did not sacrifice simulation accuracy for speed performance. Despite using single precision floating point, BrainPy was able to produce consistent firing rates and raster diagrams across all simulations (see figure 3 and figure 4 in Appendix 11).</p><p>We hope these revisions can ensure that our manuscript provides a clear and robust validation of the accuracy of our simulator.</p><disp-quote content-type="editor-comment"><p>3) Improve the discussion of limitations and weaknesses of the presented tool, and beware &quot;persuasive communication devices&quot; (https://elifesciences.org/articles/88654). Some of the comparisons to existing state-of-the-art tools do not do justice to their capabilities/advantages.</p></disp-quote><p>We agree that our paper could be improved by more clearly stating the limitations of our approach and comparing it to established approaches. We have revised the paper and added two new subsections in the Discussion section to address these specific concerns:</p><p>1. The Limitations subsection (L448 â L491) acknowledges restrictions of BrainPy paradigm which uses a Python-based object-oriented programming. It highlights three main categories of limitations: (a) approach limitations, (b) functionality limitations, (c) parallelization limitations. These limitations highlight areas where BrainPy may require further development to improve its functionality, performance, and compatibility with different modeling approaches.</p><p>2. The Future Work subsection (L493 â L526) outlines development enhancements we aim to pursue in the near term. It emphasizes the need for further development in order to meet the demands of the field. Three key areas requiring attention are highlighted: (a) multi-compartment neuron models, (b) ultra-large-scale brain simulations, (c) bridging with acceleration computing systems.</p><p>In addition to these changes, we have also made a number of other minor changes to the paper to improve its clarity and readability.</p><disp-quote content-type="editor-comment"><p>4) Clarify the target audience of the tool. Is BrainPy primarily intended for computational neuroscientists or machine learning research, or is it particularly aimed at neuro-AI practitioners bridging the two domains? For &quot;pure machine learning&quot;, would the authors consider their tool giving an advantage over using a dedicated tool like PyTorch? For computational neuroscience research, the tool does seem to lack essential local plasticity rules (e.g. Hebbian plasticity or STDP â the latter is mentioned in Appendix 8 but refers to a non-existing class in the package).</p></disp-quote><p>We appreciate the reviewer's concern regarding the scope of BrainPy and the need for clarification regarding the target audience.</p><p>BrainPy is designed to cater to both computational neuroscientists and neuro-AI practitioners by integrating detailed neural models, classical point models, rate-coded models, and deep learning models. The platform aims to provide a general-purpose programming framework for modeling brain dynamics, allowing users to explore the dynamics of brain or brain-inspired models that combines insights from biology and machine learning.</p><p>Particularly, brain dynamics models (provided in brainpy.dyn module) and deep learning models (provided in brainpy.dnn module) are closely integrated with each other in BrainPy. First, to build brain dynamics models, users should use the building blocks in brainpy.dnn module to create synaptic projections. The following code demonstrates how to use brainpy.dnn.Linear to create a synaptic model with dense connections:</p><p><code xml:space="preserve">class ExponDenseCOBA(bp.Projection):Â 
Â  Â  def __init__(self, pre, post, delay, g_max, tau, E):Â  
Â  Â  Â  Â  super().__init__()Â  Â 
Â  Â  Â  Â  self.proj = bp.dyn.ProjAlignPostMg2(Â  Â  
Â  Â  Â  Â  Â  Â  pre=pre,Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  delay=delay,Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  comm=bp.dnn.Linear(pre.num, post.num, g_max), # HEREÂ  Â  
Â  Â  Â  Â  Â  Â  syn=bp.dyn.Expon.desc(post.num, tau=tau),Â  Â  Â 
Â  Â  Â  Â  Â  Â  out=bp.dyn.COBA.desc(E=E),Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  post=post,Â  Â  Â  
Â  Â  Â  Â )</code></p><p>Or, we can use a sparse connection module brainpy.dnn.EventCSRLinear to create a synaptic model with sparse connections and event-driven computations.</p><p><code xml:space="preserve">class ExponDenseCOBA(bp.Projection):Â  
Â  Â  def __init__(self, pre, post, delay, prob, g_max, tau, E):Â 
Â  Â  Â  Â  super().__init__()Â  Â  
Â  Â  Â  Â Â self.proj = bp.dyn.ProjAlignPostMg2(Â  Â  Â  
 Â  Â  Â  Â  Â  Â  pre=pre,Â  Â  Â  
 Â  Â  Â  Â  Â  Â Â delay=delay,Â  Â  Â  
 Â  Â  Â  Â  Â  Â Â comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), # HERE
   Â  Â  Â  Â  Â  Â Â syn=bp.dyn.Expon.desc(post.num, tau=tau),Â  Â  Â  
 Â  Â  Â  Â  Â  Â Â out=bp.dyn.COBA.desc(E=E),Â  Â  Â  
 Â  Â  Â  Â  Â  Â Â post=post,Â  Â  
Â  Â  Â  Â Â )</code></p><p>Or, we can use the convolutional neural network to create a synaptic model with convolutional connections.</p><p><code xml:space="preserve">class ExponDenseCOBA(bp.Projection):Â 
Â  Â Â def __init__(self, pre, post, delay, prob, g_max, tau, E):Â  
Â  Â  Â  Â  super().__init__()Â  
Â  Â  Â  Â Â self.proj = bp.dyn.ProjAlignPostMg2(Â  Â  Â 
Â  Â  Â  Â  Â  Â Â pre=pre,Â  Â  Â 
Â  Â  Â  Â  Â  Â Â delay=delay,Â  Â  Â 
Â  Â  Â  Â  Â  Â Â # HEREÂ  
 Â  Â  Â  Â  Â  Â  comm=bp.Sequntial(Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â        lambda a: a.reshape(pre.size), # reshape to a 4D arrayÂ  Â  Â  
   Â  Â  Â  Â  Â  Â Â Â Â bp.dnn.Conv2D(1, 10, 3), # convolution operatorÂ  Â  Â 
Â  Â  Â  Â  Â  Â Â Â        lambda a: a.flatten(), # flatten to a 1D vectorÂ  Â  Â  
 Â  Â  Â  Â  Â  Â Â ),Â  
 Â  Â  Â  Â  Â  Â Â syn=bp.dyn.Expon.desc(post.num, tau=tau),Â  
 Â  Â  Â  Â  Â  Â Â out=bp.dyn.COBA.desc(E=E),Â  
 Â  Â  Â  Â  Â  Â Â post=post,Â  Â  
Â  Â  Â  Â Â )</code></p><p>Second, to build brain-inspired computing models for machine learning, users could also take advantages of neuronal and synaptic dynamics have been provided in brainpy.dyn module.</p><p>To that end, BrainPy provides building blocks of detailed conductance-based models like Hodgkin-Huxley, as well as common deep learning layers like convolutions.</p><p>Regarding the advantage of using BrainPy over PyTorch for purely deep networks, we acknowledge that existing deep learning libraries like Flax in the JAX ecosystem provide extensive tools and examples for constructing traditional deep neural networks. While BrainPy does implement standard deep learning layers, our primary focus is not to compete directly with those libraries. Instead, we provide these models for the seamless integration of deep learning layers within BrainPy's core modeling abstractions, including variables and dynamical systems. This integration allows researchers to incorporate common deep learning layers into their brain models. Additionally, the inclusion of deep learning layers in BrainPy serves as examples for customization and facilitates the development of tailored layers for neuroscience research. Researchers can modify or extend the implementations to suit their specific needs.</p><p>In summary, BrainPy's scope focuses on the general-purpose brain dynamics programming. The target audience includes computational neuroscientists who want to incorporate insights from machine learning, as well as some ML researchers interested in integrating brain-like components.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>General remarks:</p><p>â The package contains automatic unit/integration tests, but from a cursory look, they (many? most?) only test for the absence of basic errors (i.e. the code does not fail to run), not for the correctness of the result. Some classes (e.g. GapJunction) never seem to be tested at all.</p></disp-quote><p>We apologize for any inconvenience caused by the limited testing scope initially. Previously, our main focus for testing was on basic elements within the infrastructure modules, such as brainpy.math and brainpy.integrators. We appreciate the reviewer bringing this limitation to our attention. We have devoted to improving the testing coverage to ensure the reliability and accuracy of the package. As part of these improvements, we have added new test suites for models in brainpy.rates, brainpy.neurons, brainpy.synapses, brainpy.dnn, brainpy.dyn, and brainpy.channels modules.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>There are a couple of significant points to mention here:</p><p>You state on p12: &quot;We also implemented this decision-making model with Brian2 on the CPU (although it was stated that Brian2 could target on GPU via code generation with GeNN (Stimberg et al., 2020), we could not run this model on a GPU with Brian2 successfully), and observed that the running speed of Brian2 was significantly slower than that of BrainPy.&quot;. In order to make this a fair test, I would expect more effort in getting the model to run, choose another standard model to run with both platforms, or at least provide a substantial explanation for why you were unable to. The editor is in fact the lead author of Brian2 and would very likely be able to help you get support from the GeNN team.</p><p>I was also unable to create a Docker image on an ARM-based Mac since jaxlib is unavailable for the combination of linux on ARM processors. Until this (reasonably common) combination becomes an official target for compiled binaries, it would be good to provide instructions on how to compile it from source or otherwise work around this missing dependency. Similarly, I was unable to install brainpylib.</p></disp-quote><p>We appreciate the reviewer's feedback and acknowledge the issues they encountered during their evaluation. We have made the necessary revisions to address the concerns raised.</p><p>First, the Docker image has been provided. Users can directly install brainpy through docker pull brainpy.</p><p>Second, we have given a detailed installation instruction on ARM-based Mac in our official website (https://brainpy.readthedocs.io/en/latest/quickstart/installation.html#). The most important thing is that users should first install the latest M1 macOS version of Miniconda or Anaconda.</p><p>Third, brainpylib can be directly installed on ARM-based Mac through pip install brainpylib if the M1 macOS version of conda is installed.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>More care should be given to the reproducibility of the benchmarks. When using a benchmark like COBA (Figures 6, 7, and 8), the numerical method should be the same, or changes should be stated and explained. The authors should state how many runs they used for each experiment to obtain the presented timings. From the codes in the repository, we assume that they are always analyzing a single run. We would recommend performing simulation multiple times to catch the deviations in-between runs.</p></disp-quote><p>To address this concern, we add a new appendix to explicitly state all experiments in this paper (please refer to Appendix 11).</p><p>Additionally, we apologize for the lack of information regarding the number of runs performed for each experiment. We have performed 10 runs of each benchmark and updated the paper with the average execution timings (see L1634 â L1637). Please find the updated figures reflecting these changes in our revised paper (Figure 8).</p><p>[Editorsâ note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>â The benchmarks, accuracy evaluations and their descriptions have been greatly improved, but they still have shortcomings that at least merit discussion: while the accuracy evaluations in Appendix 11 convincingly argue for no major discrepancies between the simulations, they are not enough to unambiguously state that single precision leads to &quot;a significant speedup without the sacrifice of the simulation accuracy&quot;. From my personal experience, simulating a large COBAHH network with single precision will inevitably lead to division by 0 for a few neurons, if not using equations that use special functions such as exprel (scipy, brian2), or vtrap (NEURON) to avoid this issue. This will usually not show in the average firing rate or a raster plot, but having neurons that are no longer active with a membrane potential of NaN should certainly be considered a loss of accuracy. I am aware that a more in-depth analysis of the accuracy (as e.g. in van Albada et al. 2018; doi: 10.3389/fnins.2018.00291) is out of scope, but I think a more careful evaluation of the accuracy is warranted.</p></disp-quote><p>We appreciate the reviewer for bringing up the precision issue once again. We have carefully considered this feedback and taken several steps to address this concern.</p><p>Firstly, we conducted additional analysis to evaluate the x32 computation accuracy of BrainPy, Brian2CUDA, and GeNN. This analysis, depicted in Appendix 11âfigure 5, focused on examining the number of neurons displaying NaN membrane potentials after a 5-second COBAHH simulation. Interestingly, we observed that BrainPy exhibited a higher number of NaN neurons compared to the other frameworks (see L1670-L1684).</p><p>To rectify this issue and ensure a fair speed benchmark, we replaced the problematic operation of <inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with <inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> when integrating the Hodgkin-Huxley neuron models using single-precision floating-point. By making this adjustment, we successfully eliminated the occurrence of NaN neurons after the COBAHH simulation (see L1689-L1702). We have also updated the new speed data for BrainPy x32 computation in Figures 8B and 8E to reflect these changes.</p><p>Furthermore, we conducted an extensive analysis of the speed performance of Brian2, Brian2CUDA, and GeNN, specifically focusing on the x32 computation. Similar to the BrainPy x32 experiments, we performed these analyses on the three benchmark scenarios. The updated speed data for Brian2, Brian2CUDA, and GeNN with the x32 computation are included in Figure 8 to provide a more comprehensive comparison.</p><p>We hope that these efforts have improved the quality and accuracy of our research. We greatly appreciate the reviewer for drawing our attention to this issue once again, as it has allowed us to enhance the reliability of our results.</p><disp-quote content-type="editor-comment"><p>It should also be noted that the raster plots for NEST and NEURON do look quite different from the other results in Appendix 11 figures 3 and 4 â probably due to different initial conditions?</p></disp-quote><p>We apologize for the discrepancy in the initial conditions when simulating the EI balance network models using NEST and NEURON. We have addressed this issue and updated the reproducing code on our GitHub repository (https://github.com/brainpy/brainpy-paper-reproducibility, see L69 in COBA_pynn.py and L54 in COBAHH_pynn.py). As a result, the corresponding figures, specifically Appendix 11âfigure 3/4 J and K, have been revised and updated accordingly.</p><disp-quote content-type="editor-comment"><p>Finally, the authors state that &quot;Notably, BrainPy offers support for utilizing single-precision floating-point (x32) arithmetic&quot;, but this feature is present in several of the other simulators that were benchmarked as well, and has been prominently mentioned in the papers introducing the GPU simulators discussed in the manuscript (GeNN, Brian2GeNN, Brian2CUDA).</p></disp-quote><p>We appreciate the reviewer pointing out our misleading statement regarding single-precision floating point (x32) support. To address this point, we have updated the manuscript in the following two ways. Firstly, we conducted additional experiments to benchmark the x32 computation capability of Brain2, Brian2GeNN, and Brian2CUDA. The results of these experiments are now presented in the revised Figure 8. Secondly, we have corrected our statement to highlight that BrainPy's x32 computation offers a higher performance speedup. We hope these changes rectify the inaccuracies and provide a more accurate description of BrainPy's features. Thank you once again for your valuable feedback.</p><disp-quote content-type="editor-comment"><p>â The manuscript (understandably) puts forwards the features of the simulator, but sometimes is not sufficiently making the link to existing literature on the topic and/or implementations in existing simulators. Apart from the single-precision feature mentioned above, this is in particular the case for the sparse and event-driven synaptic connectivity. Implementing such connectivity has been at the heart of simulator development for a long time, in particular in the context of GPU acceleration (for some early examples, see Plesser et al. 2007 http://link.springer.com/chapter/10.1007/978-3-540-74466-5_71; Fidjeland et al. 2009 https://doi.org/10.1109/ASAP.2009.24; Brette and Goodman 2011 https://doi.org/10.1162/NECO_a_00123). Similarly, &quot;just-in-time connectivity&quot; has been implemented before (e.g. Knight and Nowotny 2021 https://doi.org/10.1038/s43588-020-00022-7). Please also note that the review in Appendix 1 should probably mention NMODL for the NEURON simulator (which I would not describe as having &quot;a library of standard models written in C or CUDA&quot;).</p></disp-quote><p>We greatly appreciate the reviewer's valuable feedback on improving the literature acknowledgement and accurately reviewing the related works. We sincerely apologize for any carelessness and neglect in our previous statements regarding the acceleration with sparse, event-driven, and just-in-time connectivity. To address this issue, we have made the necessary changes. Specifically, we have thoroughly revised Appendix 4, placing emphasis on acknowledging previous simulator works in this area. We have included citations to notable examples of CPU and GPU sparse/event-driven connectivity support, such as the works of Plesser et al. (2007), Fidjeland et al. (2009), Brette and Goodman (2011), and more. Additionally, we have incorporated citations to Knight and Nowotny (2021) and earlier works on just-in-time connectivity, including Lytton et al. (2008) and Carvalho et al. (2020). Regarding the NEURON review, thank the reviewer for clarifying that NMODL (not C/CUDA) enables model specification. We have corrected this simulator's description accordingly (see Appendix 1).</p><disp-quote content-type="editor-comment"><p>â The manuscript text is still somewhat hard to follow at times due to jargon that is not common usage (which also relates to the previous point). For example, eLife readers are not necessarily familiar with technical terms around just-in-time compilation (e.g. &quot;lowering&quot; code onto CPU/GPU, &quot;fusing&quot; operators), and will therefore have a hard time understanding where the speed benefit of JIT compilation comes from. Terms such as &quot;time delays&quot; and &quot;length delays&quot; (Appendix 4) are also non-standard and need explanation (as a side note: the mechanism for delaying spikes is never explained). Finally, readers might confuse the &quot;event-driven operators&quot; mentioned in the manuscript with event-driven simulation (as opposed to clock-driven; see e.g. Ros et al. 2006 10.1162/neco.2006.18.12.2959). I'd suggest the authors carefully reconsider non-standard terms and clearly define them if no commonly used term exist or if a large part of the readership might not be familiar with them. Other examples of such terms/concepts are &quot;bfloat16&quot; (l. 359), &quot;data parallelism&quot; vs. &quot;model parallelism&quot; (l. 479), or even &quot;TPU&quot; (from the abstract on).</p></disp-quote><p>We thank the reviewer for bringing up the concern regarding the readability for a broader readership. We have taken this into consideration and have made the following revisions to address the jargon issues raised:</p><p>1. Just-in-Time Compilation: We have replaced instances of &quot;lowering&quot; with &quot;compiling&quot; or &quot;transforming&quot; throughout the document. Additionally, we have provided an introductory explanation of operator fusion without assuming prior knowledge (L281-L285).</p><p>2. Time Delays: In Appendix 5, we have provided clearer definitions for the terms &quot;TimeDelay&quot; and &quot;LengthDelay.&quot; We have also included explanations regarding the usage and implementation of delay mechanisms (L1147-L1172, and Appendix 5âfigure 1).</p><p>3. Event-Driven Operators: We have carefully reviewed the text to separate the concepts of <italic>event-driven operators</italic> and <italic>event-driven simulation</italic>. We have also included an explanation to differentiate between these terminologies. Details please see L1021-L1033.</p><p>4. Other Concepts: We now define the concepts of &quot;bfloat16,&quot; &quot;data parallelism,&quot; &quot;model parallelism,&quot; &quot;TPUs,&quot; and other relevant terms when they are first introduced in the document.</p></body></sub-article></article>