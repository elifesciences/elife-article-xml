<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">98552</article-id><article-id pub-id-type="doi">10.7554/eLife.98552</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98552.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Structural Biology and Molecular Biophysics</subject></subj-group></article-categories><title-group><article-title>Streamlining segmentation of cryo-electron tomography datasets with Ais</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Last</surname><given-names>Mart GF</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3739-8863</contrib-id><email>mgflast@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Abendstein</surname><given-names>Leoni</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7634-5353</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Voortman</surname><given-names>Lenard M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9794-067X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Sharp</surname><given-names>Thomas H</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1990-2333</contrib-id><email>t.sharp@bristol.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/027bh9e22</institution-id><institution>Department of Cell and Chemical Biology, Leiden University Medical Center</institution></institution-wrap><addr-line><named-content content-type="city">Leiden</named-content></addr-line><country>Netherlands</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03gnh5541</institution-id><institution>Institute of Science and Technology Austria (ISTA)</institution></institution-wrap><addr-line><named-content content-type="city">Klosterneuburg</named-content></addr-line><country>Austria</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>School of Biochemistry, University of Bristol</institution></institution-wrap><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Scheres</surname><given-names>Sjors HW</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00tw3jy02</institution-id><institution>MRC Laboratory of Molecular Biology</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Campelo</surname><given-names>Felix</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03g5ew477</institution-id><institution>Institute of Photonic Sciences</institution></institution-wrap><country>Spain</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>20</day><month>12</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP98552</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-04-20"><day>20</day><month>04</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-04-04"><day>04</day><month>04</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.04.04.586917"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-06-21"><day>21</day><month>06</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98552.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-11-08"><day>08</day><month>11</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98552.2"/></event></pub-history><permissions><copyright-statement>Â© 2024, Last et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Last et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-98552-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-98552-figures-v1.pdf"/><abstract><p>Segmentation is a critical data processing step in many applications of cryo-electron tomography. Downstream analyses, such as subtomogram averaging, are often based on segmentation results, and are thus critically dependent on the availability of open-source software for accurate as well as high-throughput tomogram segmentation. There is a need for more user-friendly, flexible, and comprehensive segmentation software that offers an insightful overview of all steps involved in preparing automated segmentations. Here, we present Ais: a dedicated tomogram segmentation package that is geared towards both high performance and accessibility, available on <ext-link ext-link-type="uri" xlink:href="https://github.com/bionanopatterning/Ais">GitHub</ext-link>. In this report, we demonstrate two common processing steps that can be greatly accelerated with Ais: particle picking for subtomogram averaging, and generating many-feature segmentations of cellular architecture based on in situ tomography data. Featuring comprehensive annotation, segmentation, and rendering functionality, as well as an open repository for trained models at aiscryoet.org, we hope that Ais will help accelerate research and dissemination of data involving cryoET.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>cryoET</kwd><kwd>segmentation</kwd><kwd>software</kwd><kwd>machine learning</kwd><kwd>cryo-electron tomography</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>H202 Grant 759517</award-id><principal-award-recipient><name><surname>Sharp</surname><given-names>Thomas H</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100019180</institution-id><institution>HORIZON EUROPE European Research Council</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.3030/101094250</award-id><principal-award-recipient><name><surname>Sharp</surname><given-names>Thomas H</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>VI.Vidi.193.014</award-id><principal-award-recipient><name><surname>Sharp</surname><given-names>Thomas H</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The software package Ais simplifies automated segmentation in cryo-electron tomography, enabling users to segment, detect particles, and visualize data in a streamlined and accessible workflow.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Segmentation is a key step in processing cryo-electron tomography (cryoET) datasets that entails identifying specific structures of interest within the volumetric data and marking them as distinct features. This process forms the basis of many subsequent analysis steps, including particle picking for subtomogram averaging and the generation of 3D macromolecular feature maps that help visualize the ultrastructure of the sample.</p><p>Although it is a common task, the currently available software packages for tomogram segmentation often leave room for improvement in either scope, accessibility, or open availability of the source code. Some popular programs, such as Amira (Thermo Fisher Scientific), are not free to use, while other more general purpose EM data processing suites offer limited functionality in terms of visualization and user interaction. Specifically, we found that a deficit existed of software that is easy to use, competitively performing, freely available, and dedicated to segmentation of cryoET datasets for downstream processing.</p><p>In this report, we present Ais, an open-source tool that is designed to enable any cryoET user â whether experienced with software and segmentation or a novice â to quickly and accurately segment their cryoET data in a streamlined and largely automated fashion. Ais comprises a comprehensive and accessible user interface within which all steps of segmentation can be performed, including: the annotation of tomograms and compiling datasets for the training of convolutional neural networks (CNNs), training and monitoring performance of CNNs for automated segmentation, 3D visualization of segmentations, and exporting particle coordinates or meshes for use in downstream processes. To help generate accurate segmentations, the software contains a library of various neural network architectures and implements a system of configurable interactions between different models. Overall, the software thus aims to enable a streamlined workflow where users can interactively test, improve, and employ CNNs for automated segmentation. To ensure compatibility with other popular cryoET data processing suites, Ais employs file formats that are common in the field, using .mrc files for volumes, tab-separated .txt or .star files for particle datasets, and the .obj file format for exporting 3D meshes.</p><p>To demonstrate the use of Ais, we outline its use in two such tasks: first, to automate the particle picking step of a subtomogram averaging workflow, and second for the generation of rich three-dimensional visualizations of cellular architecture, with ten distinct cellular components, based on cryoET datasets acquired on cellular samples.</p></sec><sec id="s2" sec-type="results|discussion"><title>Results and discussion</title><p>The first step in image segmentation using CNNs is to manually annotate a subset of the data for use as a training dataset. Ais facilitates this step by providing a simple interface for browsing data, drawing overlays, and selecting boxes to use as training data (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplements 1</xref>â<xref ref-type="fig" rid="fig1s4">4</xref>). Multiple features, such as membranes, microtubules, ribosomes, and mitochondrial granules, can be segmented and edited at the same time across multiple datasets (even hundreds). These annotations are then extracted and used as ground truth labels upon which to condition multiple neural networks, each trained to segment a single feature type, with which one can automatically segment the same or any other dataset (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Segmentation in Ais is performed <italic>on-the-fly</italic> and can achieve interactive framerates, depending on the size of the datasets and network architectures that are used. With a little experience, users can generate a training dataset and then train, apply, and asses the quality of a model within a few minutes (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), including on desktop or laptop Windows and Linux systems with relatively low-end GPUs (e.g. we often use an NVIDIA T1000).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>An overview of the user interface and functionalities.</title><p>The various panels represent sequential stages in the Ais processing workflow, including annotation (<bold>A</bold>), testing convolutional neural networks (CNNs) (<bold>B</bold>), and visualizing segmentation (<bold>C</bold>). These images (<bold>AâC</bold>) are unedited screenshots of the software. (<bold>A</bold>) The interface for annotation of datasets. In this example, a tomographic slice has been annotated with various features â a detailed explanation follows in <xref ref-type="fig" rid="fig5">Figure 5</xref>. (<bold>B</bold>) After annotation, multiple neural networks are set up and trained on the aforementioned annotations. The resulting models can then be used to segment the various distinct features. In this example, double-membrane vesicles (double membrane vesicles DMVs, red), single membranes (blue), ribosomes (magenta), intermediate filaments (orange), mitochondrial granules (yellow), and molecular pores in the DMVs (lime) are segmented. (<bold>C</bold>) After training or downloading the required models and exporting segmented volumes, the resulting segmentations are immediately available within the software for 3d rendering and inspection. (<bold>D</bold>) The repository at <ext-link ext-link-type="uri" xlink:href="https://www.aiscryoet.org/">aiscryoet.org</ext-link> facilitates the sharing and reuse of trained models. After validation, submitted models can be freely downloaded by anyone. (<bold>E</bold>) Additional information, such as the pixel size and the filtering applied to the training data, is displayed alongside all entries in the repository, in order to help a user identify whether a model is suited to segment their datasets.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 1.</label><caption><title>Ais annotation interface.</title><p>The image shows an example of microtubules (green) being annotated and positive (green boxes) and negative (orange boxes) training boxes being placed. Neural networks in Ais operate on square input images (here 64 Ã 64 pixels), and the training data thus consists of square pairs of images, with the grayscale data in a placed box as the training input and the user-drawn annotations in that same box as the training output.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 2.</label><caption><title>Ais training and model testing interface.</title><p>After annotation and boxing, different training datasets can be exported by selecting which annotations to use as positives and which as negatives. In this example, the âMicrotubuleâ annotations are included as a positive feature, meaning both the input grayscale and output annotation are sampled. The feature âNot microtubuleâ is included as a negative feature, meaning input grayscale is sampled and the corresponding output annotations are all zeroes. Using separate positive and negative annotation classes is not necessary (including unlabelled boxes in the âMicrotubuleâ annotations has the same effect as using a negative class), but can be convenient.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 3.</label><caption><title>Ais volume exporting interface.</title><p>After preparing a suitable network for segmentation, users can apply this network to any tomogram in the âExportâ tab. Applying one network to one volume typically takes around 1 min, although depending on the available hardware and the size of the volume. In this example, the network is only applied to a selected area of interest in order to save time processing.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 4.</label><caption><title>Ais 3D rendering interface.</title><p>After exporting segmented volumes, these segmentations are immediately available for 3D rendering in the âRenderâ tab. Volumes are rendered as isosurface meshes, and can also be saved as such (as.obj files), or opened in other rendering software such as Blender or ChimeraX11.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig1-figsupp4-v1.tif"/></fig></fig-group><p>Many cryoET datasets look alike, especially for cellular samples. A model prepared by one user to segment, for example, ribosomes in a dataset with a pixel size of 10 Ã, might also be adequate for another userâs ribosome segmentation at 12 Ã per pixel. To facilitate this sort of reuse and sharing of models, we launched an open model repository at <ext-link ext-link-type="uri" xlink:href="http://www.aiscryoet.org">aiscryoet.org</ext-link> where users can freely upload and download successfully trained models (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Models that pass screening become public, are labelled with relevant metadata (<xref ref-type="fig" rid="fig1">Figure 1E</xref>), and can be downloaded in a format that allows for direct use in Ais. Thus, users can skip the annotation and training steps of the segmentation workflow. To kickstart the repository, all 27 models that are presented in this article have been uploaded to it.</p><p>Our software is not the first to address the challenge of segmenting cryoET datasets; established suites such as EMAN2 (<xref ref-type="bibr" rid="bib8">Galaz-Montoya et al., 2015</xref>), MIB (<xref ref-type="bibr" rid="bib4">Belevich et al., 2016</xref>), SuRVOS (<xref ref-type="bibr" rid="bib17">Luengo et al., 2017</xref>), or QuPath (<xref ref-type="bibr" rid="bib3">Bankhead et al., 2017</xref>) also provide some or most of the functionality that is available in Ais. Each comes equipped with one or various choices of neural network architectures to use, and many more designs for neural networks for semantic image segmentation can be found in the literature. Therefore, as well as creating a package geared specifically towards ease of use and fast results, we also wanted to include functionality that enables a user to quickly compare different models in order to facilitate determining which models are best suited for a particular segmentation task. The software thus includes a library of a number of well-performing architectures, including adaptations of single-model CNN architectures such as InceptionNet (<xref ref-type="bibr" rid="bib22">Szegedy et al., 2014</xref>), ResNet (<xref ref-type="bibr" rid="bib23">Szegedy et al., 2016</xref>), various UNets (<xref ref-type="bibr" rid="bib20">Ronneberger et al., 2015</xref>), VGGNet (<xref ref-type="bibr" rid="bib21">Simonyan and Zisserman, 2015</xref>), and the default architecture available in EMAN2 <xref ref-type="bibr" rid="bib8">Galaz-Montoya et al., 2015</xref>, as well as the more complex generative-adversarial network Pix2pix (<xref ref-type="bibr" rid="bib11">Isola et al., 2017</xref>). This library can also be extended by copying any Python file that adheres to a minimal template into the corresponding directory of the project (Appendix 1).</p><sec id="s2-1"><title>A library of neural network architectures supports varied applications</title><p>To illustrate how useful it can be to rapidly test various architectures before selecting one that is well suited for the segmentation of any particular feature, we used six different architectures for the segmentation of three distinct features within the same tomogram, and analyzed the results (<xref ref-type="table" rid="table1">Table 1</xref>). We used a cryoET dataset that we had previously acquired (<xref ref-type="bibr" rid="bib2">Abendstein et al., 2023</xref>), which contained liposomes with membrane-bound Immunoglobulin G3 (IgG3) antibodies that form an elevated Fragment crystallizable (Fc) platform, prepared on a lacey carbon substrate. The features of interest for segmentation were the membranes, antibody platforms, and carbon support film (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>A comparison of different neural networks for tomogram segmentation.</title><p>(<bold>A</bold>) A representative example of the manual segmentation used to prepare training datasets. Membranes are annotated in red, carbon film in bright white, and antibody platforms in green. For the antibody training set, we used annotations prepared in multiple slices of the same tomogram, but for the carbon and membrane training set the slice shown here comprised all the training data. (<bold>B</bold>) A tomographic slice from a different tomogram that contains the same features of interest, also showing membrane-bound antibodies with elevated Fc platforms that are adjacent to carbon (red arrowheads). (<bold>C</bold>) Results of segmentation of membranes (top; red), carbon (middle; white), and antibody platforms (bottom; green), with the six different neural networks.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 1.</label><caption><title>Comparison of manual annotations and UNet, Pix2pix, and improved Pix2pix antibody platform segmentations for the data in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>Training neural networks can often be an iterative process: an initial training dataset is compiled, a model trained, and the resulting segmentations are likely to contain readily identifiable false negatives (i.e. parts of antibody platforms not annotated by a model) and positives (e.g. membranes segmented as antibody platforms). To improve the model, it is then useful to perform a second iteration of annotating boxes for use in training and then training the model anew. In Ais, this is facilitated by being able to switch back and forth between the âannotationâ and âmodelsâ tabs, and by being able to quickly test models on and annotate new boxes in many different datasets. In this example, we initially trained Pix2pix on the same training dataset as used for the other networks discussed in <xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="table" rid="table1">Table 1</xref> in the main text. Although the loss of the UNet network was the lowest, we found that in comparison to most other networks the output of the Pix2pix model contained fewer false positive predictions (e.g. compare the segmentations of the liposome membranes). Since Pix2pix is a large model, it is likely to benefit the most from further training on an expanded training dataset. We thus included additional samples in the training dataset and trained a new instance of a Pix2pix model for 50 epochs. The resulting model produced segmentations that were much closer to the manual annotations. Further information and tips for preparing useful models are also discussed in the video tutorials, available at <ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/@scNodes">youtube.com/@scNodes</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig2-figsupp1-v1.tif"/></fig></fig-group><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Comparison of some of the default models available in Ais<bold>.</bold></title><p><sup>a</sup>The computational cost is only roughly proportional to the number of model parameters, which is reported in the software. The specifics of the network architecture affect the processing speed more significantly. <sup>b</sup>Time required to process one 511 Ã 720 pixel-sized tomographic slice. <sup>c</sup>These columns list the loss values after training, calculated as the binary cross-entropy (bce) between the prediction and original annotation. The loss is a (rough) metric of how well a trained network performs (see Methods). <sup>d</sup>Unlike the other architectures, Pix2pix is not trained to minimize the bce loss but uses a different loss function instead. The bce loss values shown here were computed after training and may not be entirely comparable.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Architecture</th><th align="left" valign="top">Parameters</th><th align="left" valign="top">Training time<sup>a</sup></th><th align="left" valign="top">Processing time<sup>a, b</sup></th><th align="left" valign="top">Membrane<sup>c</sup></th><th align="left" valign="top">Carbon<sup>c</sup></th><th align="left" valign="top">Antibody<sup>c</sup></th></tr></thead><tbody><tr><td align="left" valign="top">EMAN2</td><td align="char" char="." valign="top">378,081</td><td align="char" char="." valign="top">38 s</td><td align="char" char="." valign="top">50 ms</td><td align="char" char="." valign="top">0.044</td><td align="char" char="." valign="top">.072</td><td align="char" char="." valign="top">.010</td></tr><tr><td align="left" valign="top">InceptionNet</td><td align="char" char="." valign="top">550,529</td><td align="char" char="." valign="top">462 s</td><td align="char" char="." valign="top">295 ms</td><td align="char" char="." valign="top">0.046</td><td align="char" char="." valign="top">.120</td><td align="char" char="." valign="top">.008</td></tr><tr><td align="left" valign="top">UNet</td><td align="char" char="." valign="top">922,881</td><td align="char" char="." valign="top">132 s</td><td align="char" char="." valign="top">110 ms</td><td align="char" char="." valign="top">0.013</td><td align="char" char="." valign="top">.<bold>007</bold></td><td align="char" char="." valign="top">.<bold>001</bold></td></tr><tr><td align="left" valign="top">VGGNet</td><td align="char" char="." valign="top">1,493,059</td><td align="char" char="." valign="top">91 s</td><td align="char" char="." valign="top">85 ms</td><td align="char" char="." valign="top"><bold>0.011</bold></td><td align="char" char="." valign="top">.125</td><td align="char" char="." valign="top">.013</td></tr><tr><td align="left" valign="top">ResNet</td><td align="char" char="." valign="top">4,887,265</td><td align="char" char="." valign="top">1533 s</td><td align="char" char="." valign="top">980 ms</td><td align="char" char="." valign="top">0.047</td><td align="char" char="." valign="top">.060</td><td align="char" char="." valign="top">.014</td></tr><tr><td align="left" valign="top">Pix2pix<sup>d</sup></td><td align="char" char="." valign="top">29,249,409</td><td align="char" char="." valign="top">793 s</td><td align="char" char="." valign="top">225 ms</td><td align="char" char="." valign="top">0.050<sup>d</sup></td><td align="char" char="." valign="top">.129<sup>d</sup></td><td align="char" char="." valign="top">.016<sup>d</sup></td></tr></tbody></table></table-wrap><p>After training, we applied the resulting models to a different tomogram containing the same features, that were not previously used to generate the training data (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), so that there was no overlap between the training and testing datasets. Next, we compared the training times, relative loss values, and quality of the segmentations.</p><p>Based on the loss values (the loss is a metric of how well predictions match the ground truth labels), VGGNet was best suited for the segmentation of membranes, while UNet performed best on the carbon support film and antibody platforms. However, the loss values do not capture model performance in the same way as human judgement (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). For the antibody platform, the model that would be expected to be one of the worst based on the loss values, Pix2pix, actually generates segmentations that are well-suited for the downstream processing tasks. It also outputs fewer false positive segmentations for sections of membranes than many other models, including the lowest-loss model UNet. Moreover, since Pix2pix is a relatively large network, it might also be improved further by increasing the number of training epochs. We thus decided to use Pix2pix for the segmentation of antibody platforms, and increased the size of the antibody platform training dataset (from 58 to 170 positive samples) to train a much improved second iteration of the network for use in the following analyses (<xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>).</p><p>When taking the training and processing speeds into account as well as the segmentation results, there is no overall best architecture. We, therefore, included multiple well-performing model architectures in the final library, in order to allow users to select from these models to find one that works well for their specific datasets. Although it is not necessary to screen different network architectures and users may simply opt to use the default (VGGNet), these results thus show that it can be useful to test different networks to identify one that is best. Moreover, these results also highlight the utility of preparing well-performing models by iteratively improving training datasets and re-training models in a streamlined interface. To aid in this process, the software displays the loss value of a network during training and allows for the application of models to datasets during training. Thus, users can inspect how a modelâs output changes during training and decide whether to interrupt training and improve the training data or choose a different architecture.</p></sec><sec id="s2-2"><title>Fine-tuning segmentation results with <italic>model interactions</italic></title><p>Although the above results go some way towards distinguishing the three different structures, they also show demonstrate a common limitation encountered in automated tomogram segmentation: some parts of the image are assigned a high segmentation value by multiple of the networks, leading to false classifications and ambiguity in the results. For example, the InceptionNet and ResNet antibody platform models falsely label the edges of the carbon film.</p><p>To further improve the segmentation results, we decided to implement a system of proximity-based âmodel interactionsâ of two types, colocalization and avoidance (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), using which the output of one model can be adjusted based on the output of other models. In a colocalization interaction, the predictions of one model (the <italic>child</italic>) are suppressed wherever the prediction value of another model (the <italic>parent</italic>) is below some threshold. In an avoidance interaction, suppression occurs wherever the parent modelâs prediction value is above a threshold.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Model interactions can significantly increase segmentation accuracy.</title><p>(<bold>A</bold>) An overview of the settings available in the âModelsâ menu in Ais. Three models: (1) âmembraneâ (red), (2) âcarbonâ (white), and (3) âantibody platformsâ (green) are active, with each showing a different section of the model settings: the training menu (1), prediction parameters (2), and the interactions menu (3). (<bold>B</bold>) A section of a tomographic slice is segmented by two models, carbon (white; <italic>parent</italic> model) and membrane (red; <italic>child</italic> model), with the membrane model showing a clear false positive prediction on an edge of the carbon film (panel âwithout interactionsâ). By configuring an avoidance interaction between the membrane model that is conditional upon the carbon modelâs prediction, this false positive is avoided (panel âwith interactionsâ). (<bold>C</bold>) By setting up multiple model interactions, inaccurate predictions by the âantibody platformsâ model are suppressed. In this example, the membrane model avoids carbon while the antibody model is set to colocalize with the membrane model. (<bold>D</bold>) 3D renders (see Methods) of the same dataset as used in <xref ref-type="fig" rid="fig2">Figure 2</xref> processed three ways: without any interactions (left), using model competition only (middle), or by using model competition as well as multiple model interactions (right).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig3-v1.tif"/></fig><p>These interactions are implemented as follows: first, a binary mask is generated by thresholding the <italic>parent</italic> modelâs predictions using a user-specified threshold value. Next, the mask is then dilated using a circular kernel with a radius <inline-formula><mml:math id="inf1"><mml:mi>R</mml:mi></mml:math></inline-formula>, a parameter that we call the interaction radius. Finally, the <italic>child</italic> modelâs prediction values are multiplied with this mask.</p><p>Besides these specific interactions between two models, the software also enables pitching multiple models against one another in what we call âmodel competitionâ. Models can be set to âemitâ and/or âabsorbâ competition from other models. Here, to emit competition means that a modelâs prediction value is included in a list of competing models. To absorb competition means that a modelâs prediction value will be compared to all values in that list, and that this modelâs prediction value for any pixel will be set to zero if any of the competing modelsâ prediction value is higher. On a pixel-by-pixel basis, all models that absorb competition are thus suppressed whenever their prediction value for a pixel is lower than that of any of the emitting models.</p><p>With the help of these model interactions, it is possible to suppress common erroneous segmentation results. For example, an interaction like âabsorbing membrane model <underline>avoids</underline> emitting carbon model with R = 10 nmâ is effective at suppressing the prediction of edges of the carbon film as being membranes (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Another straightforward example of the utility of membrane interactions is the segmentation of membrane-bound particles. By defining the following two interactions: âantibody platform model <underline>avoids</underline> membrane model with R = 10 nmâ followed by âantibody platform model <underline>colocalizes</underline> with membrane model with R = 30 nmâ, the Fc-platforms formed by IgG3 at a distance of ~22 nm from the membrane are retained, while false positive labelling of features such as the membrane or carbon is suppressed (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><p>By conditionally combining and editing the prediction results of multiple neural networks, model interactions can thus be helpful in fine-tuning segmentations to be better suited for downstream applications. To illustrate this, we generated a comparison of segmentation results using (i) no interactions, (ii) model competition only, and (iii) model competition as well as model interactions (<xref ref-type="fig" rid="fig3">Figure 3D</xref>), which demonstrates the degree to which false positives can be reduced by the use of model interactions (although at times at the expense of increasing the rate of false negatives).</p></sec><sec id="s2-3"><title>Automating particle picking for subtomogram averaging</title><p>Protein structure determination by cryoET requires the careful selection of many subtomograms (i.e. sub-volumes of a tomogram that all contain the same structure of interest), and aligning and averaging these to generate a 3D reprojection of the structure of interest with a significantly increased signal to noise ratio. The process of selecting these sub-volumes is called âparticle picking,â and can be done either manually or in an automated fashion. Much time can be saved by automating particle picking based on segmentations. In many cases, though, segmentation results are not readily usable for particle picking, as they can often introduce numerous false positives. This is particularly the case with complex, feature-rich datasets such as those obtained within cells, where the structures of interest can visually appear highly similar to other structures that are also found in the data, or when the structures of interest are located close to other features and are, therefore, hard to isolate. An example of this latter case is the challenge of picking membrane-bound particles.</p><p>Recently, we have used cryoET and subtomogram averaging to determine the structures of membrane-bound IgG3 platforms and of IgG3 interacting with the human complement system component 1 (C1) on the surface of lipid vesicles (<xref ref-type="bibr" rid="bib2">Abendstein et al., 2023</xref>). The reconstructions of the antibody platforms alone and of the antibody-C1 complex were prepared using 1193 and 2561 manually selected subtomograms, extracted from 55 and 101 tomograms, respectively. Manual picking of structures of interest, although very precise, is very time-consuming and in this particular case took approximately 20 hr of work.</p><p>To demonstrate the utility of our software for particle picking, we re-analyzed these same datasets, this time using Ais to automate the picking of the two structures: antibody platforms and antibody-C1 complexes. For the antibody platforms, we used the same models and model interactions as described above, while we trained an additional neural network to identify C1 complexes for the segmentation of the antibody-C1 complexes. To prepare a training dataset for this latter model, we opened all 101 tomograms in Ais and browsed the data to select and annotate slices where one or multiple antibody-C1 complexes were clearly visible (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The training dataset thus consisted of samples taken from multiple different tomograms; the annotation and data selection in this case took around 1 hr of work.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Automated particle picking for sub-tomogram averaging of antibody complexes.</title><p>(<bold>A</bold>) Manually prepared annotations used to train a neural network to recognize antibody platforms (top) or antibody-C1 complexes (bottom). (<bold>B</bold>) Segmentation results as visualized within the software. Membranes (red) and carbon support film (white) were used to condition the antibody (green) and antibody-C1 complex (yellow) predictions using model interactions. (<bold>C</bold>) 3D representations of the segmented volumes rendered in Ais. (<bold>D</bold>) Tomographic slices showing particles picked automatically based on the segmented volume shown in panel c. (<bold>E</bold>) Subtomogram averaging result of the 2499 automatically picked antibody platforms. (<bold>F</bold>) Subtomogram averaging result obtained with the 602 automatically picked antibody-C1 complexes. The quadrants in panels e and f show orthogonal slices of the reconstructed density maps and a 3D isosurface model (the latter rendered in ChimeraX [<xref ref-type="bibr" rid="bib9">Goddard et al., 2018</xref>]).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 1.</label><caption><title>A visual (2D) representation of the processing steps employed in the automated picking process.</title><p>The steps employed in the process of converting a segmented.mrc volume to a list of particle coordinates. The example is shown in 2D; the actual picking process is performed in 3D. (1) The input segmentation (values 0â255), (2) a binary mask, generated by thresholding the input image at a value of 127, (3) the distance transform of the binary mask, (4) groups of pixels, labelled using a watershed algorithm (skimage.segmentation.watershed) with local maxima in the distance transform output used as sources, (5) centroid coordinates (marked by crosses) of the pixel groups, (6) example of removing particles by applying a minimum spacing, (7) example of removing particles by specifying a minimum particle size.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 2.</label><caption><title>Inspecting the autopicking results in the Ais renderer.</title><p>After picking particles, the resulting localizations are immediately available for inspection in the renderer. In this example, yellow markers indicate the coordinates of molecular pores, picked in a tomogram from the dataset by <xref ref-type="bibr" rid="bib27">Wolff et al., 2020</xref> (see main text <xref ref-type="fig" rid="fig5">Figure 5</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 3.</label><caption><title>Comparison of the automatically picked C1-IgG3 complex reconstruction versus the original reconstruction in <xref ref-type="bibr" rid="bib2">Abendstein et al., 2023</xref>.</title><p>The FSC curve for the two half maps of the automatically-picked C1-IgG3 complex reconstruction gives a resolution of 44 Ã at the FSC = 0.143 (yellow, 'auto picked half maps'). In the original publication (<xref ref-type="bibr" rid="bib2">Abendstein et al., 2023</xref>) various resolutions are reported for different maps that were obtained by focused refinement on different parts of the complex. In the current report, we did not apply further refinement steps beyond the initial even/odd reconstructions on the entire complex that the above FSC curve is based on. The value of 44 Ã must thus be compared to the corresponding initial reconstruction in the original report (in <xref ref-type="bibr" rid="bib2">Abendstein et al., 2023</xref>, their Supplementary Fig. 10c, map #1) which was reported at FSC = 0.143 with the same resolution of 44 Ã. A comparison between the full maps (black FSC curve, 'intermap') at FSC coefficient 0.5 gives a value of 46 Ã for the resolution, thus indicating that the two independent reconstructions did indeed converge on the same structure.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig4-figsupp3-v1.tif"/></fig></fig-group><p>The antibody platform and antibody-C1 complex models were then applied to the respective datasets, in combination with the membrane and carbon models and the model interactions described above (<xref ref-type="fig" rid="fig4">Figure 4B</xref>): the membrane avoiding carbon, and the antibody platforms colocalizing with the resulting membranes. We then launched a relatively large batch segmentation process (3 models Ã 156 tomograms) and left it to complete overnight.</p><p>Once complete, we used Ais to inspect the segmented volumes and original datasets in 3D, and adjusted the threshold value so that, in as far as possible, only the particles of interest remained visible and the number of false positive particles was minimized (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). After selecting these values, we then launched a batch particle-picking process to determine lists of particle coordinates based on the segmented volumes.</p><p>Particle picking in Ais comprises a number of processing steps (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref>). First, the segmented (.mrc) volumes are thresholded at a user-specified level. Second, a distance transform of the resulting binary volume is computed, in which every nonzero pixel in the binary volume is assigned a new value, equal to the distance of that pixel to the nearest zero-valued pixel in the mask. Third, a watershed transform is applied to the resulting volume, so that the sets of pixels closest to any local maximum in the distance transformed volume are assigned to one group. Fourth, groups that are smaller than a user-specified minimum volume are discarded. Fifth, the remaining groups are assigned a weight value, equal to the sum of the prediction value (i.e. the corresponding pixel value in the input.mrc volume) of the pixels in the group. For every group found within close proximity to another group (using a user-specified value for the minimum particle spacing), the group with the lower weight value is discarded. Finally, the centroid coordinate of the grouped pixels is considered the final particle coordinate, and the list of all coordinates is saved in a tab-separated text file with values rounded to the nearest integer.</p><p>As an alternative output format, segmentations can also be converted to and saved as triangulated meshes (.obj file format), which can then be used for, e.g., membrane-guided particle picking (<xref ref-type="bibr" rid="bib19">Pyle et al., 2022</xref>). After picking particles, the resulting coordinates are immediately available for inspection in the Ais 3D renderer (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig4s2">Figure 4âfigure supplement 2</xref>).</p><p>After picking, we used EMAN2 (<xref ref-type="bibr" rid="bib8">Galaz-Montoya et al., 2015</xref>, spt_boxer.py) to extract volumes using the particle coordinates as an input (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), which resulted in 2499 vol for the antibody platform reconstruction and 602 for the antibody-C1 complex (<italic>n.b</italic>. these numbers can be highly dependent on the threshold value). These volumes, or subtomograms, were used as the input for subtomogram averaging using EMAN2 <xref ref-type="bibr" rid="bib8">Galaz-Montoya et al., 2015</xref> and Dynamo (<xref ref-type="bibr" rid="bib6">CastaÃ±o-DÃ­ez et al., 2012</xref>; see Materials and methods), without further curation â i.e., we did not manually discard any of the extracted volumes.</p><p>After applying the same approach to subtomogram averaging as used previously (<xref ref-type="bibr" rid="bib2">Abendstein et al., 2023</xref>), the resulting averages were indeed highly similar to the original reconstructions (<xref ref-type="fig" rid="fig4">Figure 4E and F</xref>, <xref ref-type="fig" rid="fig4s3">Figure 4âfigure supplement 3</xref>). These results demonstrate that Ais can be successfully used to automate particle picking, and thus to significantly reduce the amount of time spent on what is often a laborious processing step.</p></sec><sec id="s2-4"><title>Many-feature segmentations of complex in situ datasets</title><p>Aside from particle picking, segmentation is also often used to visualize and study the complex internal structure of a sample, as for example encountered when applying tomography to whole cells. Here too, the accuracy of a segmentation can be a critical factor in the success of downstream analyses such as performing measurements on the basis of 3D feature maps generated <italic>via</italic> segmentation.</p><p>A challenging aspect of the segmentation of cellular samples is that these datasets typically contain many features that are biologically distinct, but visually and computationally difficult to distinguish. For example, one challenge that is often encountered is that of distinguishing between various linearly shaped components: lipid membranes, actin filaments, microtubules, and intermediate filaments, which all appear as linear features with a relatively high density. To show the utility of Ais for the accurate segmentation of complex cellular tomograms, we next demonstrate a number of examples of such feature-rich segmentations.</p><p>The first example is a segmentation of seven distinct features observed in the base of <italic>Chlamydomonas reinhardtii</italic> cilia (<xref ref-type="fig" rid="fig5">Figure 5A</xref>), using the data by <xref ref-type="bibr" rid="bib24">van den Hoek et al., 2022</xref> that was deposited in the Electron Microscopy Public Image Archive (EMPIAR, <xref ref-type="bibr" rid="bib12">Iudin et al., 2023</xref>) with accession number 11078. The features are: membranes, ribosomes, microtubule doublets, axial microtubules, non-microtubular filaments, interflagellar transport trains (IFTs), and glycocalyx. This dataset was particularly intricate (the supplementary information to the original publication lists more than 20 features that can be identified across the dataset) and some rare features, such as the IFTs, required careful annotation across all tomograms before we could compile a sufficiently large training dataset. The final segmentation correctly annotates most of the selected characteristics present in the sample: the ribosome exclusion zone that surrounds the ciliary base (<xref ref-type="bibr" rid="bib24">van den Hoek et al., 2022</xref>) is clearly recognizable, and the structures of the glycocalyx, membranes, and microtubule doublets within the cilia are well defined. Some fractions of the meshwork of stellate fiber and Y-link proteins are also detected within the cilium.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Segmentations of complex in situ tomograms.</title><p>(<bold>A</bold>) A segmentation of seven distinct features observed in the base of <italic>C. reinhardtii</italic> cilia (<xref ref-type="bibr" rid="bib24">van den Hoek et al., 2022</xref>, EMPIAR-11078, tomogram 12): membranes (gray), ribosomes (magenta), microtubule doublets (green) and axial microtubules (green), non-microtubular filaments within the cilium (blue), inter-flagellar transport trains (yellow), and glycocalyx (orange). Inset: a perpendicular view of the axis of the cilium. The arrows in the adjacent panel indicate these structures in a tomographic slice. (<bold>B</bold>) A segmentation of six features observed in and around mitochondria in a mouse neuron with Huntingtonâs disease phenotype (<xref ref-type="bibr" rid="bib28">Wu et al., 2023</xref>) (EMD-29207): membranes (gray), mitochondrial granules (yellow), membranes of the mitochondrial cristae (red), microtubules (green), actin (turquoise), and ribosomes (magenta). (<bold>C</bold>) Left: a segmentation of ten different cellular components found in tomograms of coronavirus-infected mammalian cells (<xref ref-type="bibr" rid="bib27">Wolff et al., 2020</xref>): double-membrane vesicles (double membrane vesicles DMVs, light red), single membranes (gray), viral nucleocapsid proteins (red), viral pores in the DMVs (blue), nucleic acids in the DMVs (pink), microtubules (green), actin (cyan), intermediate filaments (orange), ribosomes (magenta), and mitochondrial granules (yellow). Right: a representative slice, with examples of each of the features (except the mitochondrial granules) indicated by arrows of the corresponding colour.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 1.</label><caption><title>Individual images of the ten features segmented in the coronavirus-infected mammalian cells dataset (<xref ref-type="bibr" rid="bib27">Wolff et al., 2020</xref>, Science).</title><p>A side-by-side overview of the 10 segmented features in the coronavirus-infected mammalian cells dataset.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 2.</label><caption><title>A collage of automatically picked molecular pores in the double membrane vesicles.</title><p>As with the C1-IgG3 complexes and IgG3 platforms (main <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>), the segmentations of the molecular pores in the coronavirus replication organelle (main <xref ref-type="fig" rid="fig5">Figure 5</xref>) can be used to automatically pick particles. In the above image, tomographic slices are shown that were cropped at the location of 56 particles (picked in two tomograms). Although it can be difficult to tell at the single particle level whether an image contains the exact structure of interest, the images are all centered on sections of double-membrane vesicles and many show additional density between the membranes as well as at the outside (convex side) of the vesicle, characteristic of the molecular pore. The accuracy of picking and the quality of the resulting particle datasets are dependent on the quality of the original segmentations and on the parameters used in picking. Converting a 3D grayscale volume to a list of coordinates requires a number of parameters to be specified (in Ais, but also in many other programs). First, a threshold value is used to convert from grayscale to a binary volume. Contiguous regions of nonzero voxels in the volumes are then considered âparticles,â and various metrics can be used to determine whether particles are included in the final list of coordinates. Example are the particle volume, integrated prediction value for the voxels included in the particleâs volume, or a minimum particle spacing. In Ais, users can specify the first and the latter (we experimented with using the second metric as well, but found it very similar but less intuitive than the metric of particle volume, so we did not include it). The threshold values chosen for these metrics and for the conversion of the grayscale volume to a binary one thus define what is âdust.â In practice, choosing these values entails a trade-off between the number of particles, and the âqualityâ of the selected set of particles. Low thresholds yield a large number of particles, which can introduce the need to perform classification during reconstruction by subtomogram averaging. High thresholds yield fewer particles, but the resulting selection may be more uniform, and their images of higher quality. In practice, it is advisable to test different thresholds and to inspect the resulting particle datasets, in order to ensure that (i) good particles are not being excluded, and (ii) large numbers of bad particles (e.g. densities that correspond to structures other than the structure of interest) are not being included. As with manually picked particle sets, classification is likely required to curate the selection of particles.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-fig5-figsupp2-v1.tif"/></fig></fig-group><p>In the second example, we show a segmentation of six features found in and around a mitochondrion in a mouse neuron (<xref ref-type="fig" rid="fig5">Figure 5B</xref>), using the data by <xref ref-type="bibr" rid="bib28">Wu et al., 2023</xref> as available in the Electron Microscopy Data Bank (EMDB) (<xref ref-type="bibr" rid="bib15">Lawson et al., 2016</xref>) with accession number 29207. In the original publication, the authors developed a segmentation method to detect and perform measurements on the granules found within mitochondria. Using Ais, we were able to prepare models to segment these granules, as well as microtubules, actin filaments, and ribosomes, and to distinguish between the highly similar vesicular membranes on the one hand, and the membranes of the mitochondrial cristae on the other.</p><p>Lastly, we used Ais to generate a 3D visualization of ten distinct cellular features observed in coronavirus-infected mammalian cells (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>), using data by <xref ref-type="bibr" rid="bib27">Wolff et al., 2020</xref>: single membranes, double-membrane vesicles (DMVs), actin filaments, intermediate filaments, microtubules, mitochondrial granules, ribosomes, coronaviral nucleocapsid proteins, coronaviral pores in the DMVs, and the nucleic acids within the DMV replication organelles. The software was able to accurately distinguish between single membranes and double membranes, as well as to discriminate between the various filaments of the cytoskeleton. Moreover, we could identify the molecular pores within the DMV, and pick sets of particles that might be suitable for use in subtomogram averaging (see <xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref>). We could also segment the nucleocapsid proteins, thus distinguishing viral particles from other, similarly sized, single membrane vesicles, as well as detect the nucleic acids found within the DMVs. Although manual annotation took some hours in this case, the processing of the full volume took approximately 3 min per feature once the models were trained, and models can of course be applied to other volumes without requiring additional training.</p><p>To conclude, our aim with the development of Ais was to simplify and improve the accuracy of automated tomogram segmentation in order to make this processing step more accessible to many cryoET users. Here, we have attempted to create an intuitive and organized user interface that streamlines the whole workflow from annotation, to model preparation, to volume processing and particle picking and inspecting the results. Additionally, the model repository at <ext-link ext-link-type="uri" xlink:href="https://www.aiscryoet.org/">aiscryoet.org</ext-link> is designed to aid users in achieving results even faster, by removing the need to generate custom models for common segmentation tasks. To help users become familiar with the software, documentation and tutorials are available at <ext-link ext-link-type="uri" xlink:href="https://ais-cryoet.readthedocs.io/en/latest/">ais-cryoet.readthedocs.org</ext-link> and video tutorials can be accessed via <ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/@scNodes">youtube.com/@scNodes</ext-link>. By demonstrating the use of Ais in automating segmentation and particle picking for subtomogram averaging, and making the software available as an open-source project, we thus hope to help accelerate research and dissemination of data involving cryoET.</p></sec></sec><sec id="s3" sec-type="materials|methods"><title>Materials and methods</title><sec id="s3-1"><title>Neural network comparisons</title><p>The comparison presented in <xref ref-type="table" rid="table1">Table 1</xref> was prepared with the use of the same training datasets for all architectures, consisting of 53/52/58 positive images showing membranes/carbon/antibody platforms and corresponding annotations alongside 159/51/172 negative images that did not contain the feature of interest, but rather the other two features, reconstruction artefacts, isolated protein, or background noise. Images were 64 Ã 64 pixels in size and the dataset was resampled, in random orientations, such that every positive image was copied 10 times and that the ratio of negatives to positives was 1.3:1. Networks were trained for 50 epochs with 32 images per batch, with the exception of the ResNet and Pix2pix antibody platform models, which were trained for 30 epochs to avoid a divergence that occurred during training after a larger number of epochs, due to the large number of network parameters and relatively low number of unique input images.</p><p>The reported loss is that calculated on the training dataset itself, i.e., no validation split was applied. During regular use of the software, users can specify whether to use a validation split or not. By default, a validation split is not applied in order to prioritize training accuracy by making full use of the input set of ground truth annotations. When the training dataset is sufficiently large, we recommend increasing the size of the validation split. Depending on the chosen split size, the software reports either the overall training loss or the validation loss during training.</p></sec><sec id="s3-2"><title>Data visualization</title><p>Images shown in the figures were either captured within the software (<xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig3">3BC</xref> âoriginal imageâ, <xref ref-type="fig" rid="fig4">Figure 4ABC</xref>), output from the software that was colourized in Inkscape (<xref ref-type="fig" rid="fig2">Figures 2C</xref> and <xref ref-type="fig" rid="fig3">3BC</xref>), or output from the software that was rendered using ChimeraX<sup>11</sup> (<xref ref-type="fig" rid="fig3">Figures 3D</xref> and <xref ref-type="fig" rid="fig4">4EF</xref>, <xref ref-type="fig" rid="fig5">Figure 5</xref>). For the panels in <xref ref-type="fig" rid="fig3">Figure 3D</xref>, segmented volumes were rendered as isosurfaces at a manually chosen suitable isosurface level and with the use of the âhide dustâ function (the same settings were used for each panel, different settings used for each feature). This âdustâ corresponds to small (in comparison to the segmented structures of interest) volumes of false positive segmentations, which are present in the data due to imperfections in the used models. The rate and volume of false positives can be reduced either by improving the models (typically by including more examples of the images that would be false negatives or positives in the training data) or, if the dust particles are indeed smaller than the structures of interest, they can simply be discarded by filtering particles based on their volume, as applied here. In particle picking a âminimum particle volumeâ is specified â particles with a smaller volume are considered âdustâ.</p></sec><sec id="s3-3"><title>Hardware</title><p>The software does not require a GPU, but works optimally when a CUDA-capable GPU is available. For the measurements shown in <xref ref-type="table" rid="table1">Table 1</xref> we used an NVIDIA Quadro P2200 GPU on a PC with an Intel i9-10900K CPU. Weâve also extensively used the software on a less powerful system equipped with an NVIDIA T1000 and an Intel i3-10100 CPU, as well as on various systems with intermediate specifications, and found that the software reaches interactive segmentation rates in most cases. For batch processing of many volumes, a more powerful GPU is useful.</p></sec><sec id="s3-4"><title>Tomogram reconstruction and subtomogram averaging</title><p>Data collection and subtomogram averaging (<xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>) was performed as described in a previously published article (<xref ref-type="bibr" rid="bib2">Abendstein et al., 2023</xref>). Briefly, tilt series were collected on a Talos Arctica 200 kV system equipped with a Gatan K3 detector with an energy filter at a pixel size of 1.74 Ã per pixel using a dose-symmetric tilt scheme with a range Â±57Â° and tilt increments of 3Â° with a total dose of 60 e/Ã (<xref ref-type="bibr" rid="bib4">Belevich et al., 2016</xref>). Tomograms were reconstructed using IMOD (<xref ref-type="bibr" rid="bib18">Mastronarde and Held, 2017</xref>). Particle picking was done in Ais (and is explained in more detail in the online documentation). Subtomogram averaging was done using a combination of EMAN (<xref ref-type="bibr" rid="bib8">Galaz-Montoya et al., 2015</xref>) and Dynamo (<xref ref-type="bibr" rid="bib6">CastaÃ±o-DÃ­ez et al., 2012</xref>). For a detailed description of the subtomogram averaging procedure, see <xref ref-type="bibr" rid="bib2">Abendstein et al., 2023</xref>.</p></sec><sec id="s3-5"><title>Open-source software</title><p>This project depends critically on a number of open-source software components, including: Python, Tensorflow (<xref ref-type="bibr" rid="bib1">Abadi, 2015</xref>), numpy (<xref ref-type="bibr" rid="bib10">Harris et al., 2020</xref>), scipy (<xref ref-type="bibr" rid="bib26">Virtanen et al., 2020</xref>), sci-kit-image (<xref ref-type="bibr" rid="bib25">van der Walt et al., 2014</xref>), mrcfile (<xref ref-type="bibr" rid="bib5">Burnley et al., 2017</xref>), and imgui (<xref ref-type="bibr" rid="bib7">Cornut, 2023</xref>).</p></sec><sec id="s3-6"><title>Software availability</title><p>A standalone version of the software is available as âAis-cryoETâ on the Python package index and on <ext-link ext-link-type="uri" xlink:href="https://github.com/bionanopatterning/Ais">GitHub</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib14">Last, 2024</xref>), under the GNU GPLv3 license. We have also integrated the functionality into scNodes (<xref ref-type="bibr" rid="bib13">Last et al., 2023</xref>), our dedicated processing suite for correlated light and electron microscopy. In the combined package, the segmentation editor contains additional features for visualization of fluorescence data and the scNodes correlation editor can be used to prepare correlated datasets for segmentation. Documentation for both versions of Ais can be found at <ext-link ext-link-type="uri" xlink:href="http://www.ais-cryoet.readthedocs.org">ais-cryoet.readthedocs.org</ext-link>. Video tutorials are available via <ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/@scNodes">youtube.com/@scNodes</ext-link>.</p></sec></sec></body><back><sec sec-type="additional-information" id="s4"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing â original draft, Writing â review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Data curation, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing â original draft, Writing â review and editing</p></fn><fn fn-type="con" id="con3"><p>Supervision, Validation, Writing â original draft, Writing â review and editing</p></fn><fn fn-type="con" id="con4"><p>Supervision, Funding acquisition, Validation, Writing â original draft, Project administration, Writing â review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s5"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-98552-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s6"><title>Data availability</title><p>Source code is available via <ext-link ext-link-type="uri" xlink:href="https://github.com/bionanopatterning/Ais">GitHub</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib14">Last, 2024</xref>), software available on the python package index as 'Ais-cryoET'. Trained neural networks have been uploaded to <ext-link ext-link-type="uri" xlink:href="https://aiscryoet.org/">aiscryoet.org</ext-link>.</p><p>The following previously published datasets were used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><collab>Wu et al.</collab></person-group><year iso-8601-date="2023">2023</year><data-title>CryoET tomogram of mitochondria in BACHD mouse model neuron</data-title><source>Massive</source><pub-id pub-id-type="accession" xlink:href="https://www.ebi.ac.uk/emdb/EMD-29207">EMDB-29207</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Hoek</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>In situ cryo-electron tomography of the <italic>C. reinhardtii</italic> ciliary transition zone</data-title><source>Massive</source><pub-id pub-id-type="accession" xlink:href="https://www.ebi.ac.uk/empiar/EMPIAR-11078/">EMPIAR-11078</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank A Koster and M Barcena for helpful discussions and kindly sharing the coronaviral replication organelle datasets. We are also grateful to <xref ref-type="bibr" rid="bib24">van den Hoek et al., 2022</xref> and <xref ref-type="bibr" rid="bib28">Wu et al., 2023</xref>, for uploading the data that we used for <xref ref-type="fig" rid="fig5">Figure 5</xref> onto EMPIAR and EMDB, as well as to the authors of various other datasets uploaded to these databases that are not discussed in this manuscript but that were useful for testing the software. We also thank the reviewers, whose comments were very helpful in improving the manuscript and the software. Finally, we are grateful the early Ais users who provided us with feedback on the software and reported issues. This research was supported by the following grants to THS: European Research Council H202 Grant 759517; European Unionâs Horizon Europe Program IMAGINE grant 101094250, and the Netherlands Organization for Scientific Research Grant VI.Vidi.193.014.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Large-Scale Machine Learning on Heterogeneous Distributed Systems</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.04467">https://arxiv.org/abs/1603.04467</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abendstein</surname><given-names>L</given-names></name><name><surname>Dijkstra</surname><given-names>DJ</given-names></name><name><surname>Tjokrodirijo</surname><given-names>RTN</given-names></name><name><surname>van Veelen</surname><given-names>PA</given-names></name><name><surname>Trouw</surname><given-names>LA</given-names></name><name><surname>Hensbergen</surname><given-names>PJ</given-names></name><name><surname>Sharp</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Complement is activated by elevated IgG3 hexameric platforms and deposits C4b onto distinct antibody domains</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>4027</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-39788-5</pub-id><pub-id pub-id-type="pmid">37419978</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bankhead</surname><given-names>P</given-names></name><name><surname>Loughrey</surname><given-names>MB</given-names></name><name><surname>FernÃ¡ndez</surname><given-names>JA</given-names></name><name><surname>Dombrowski</surname><given-names>Y</given-names></name><name><surname>McArt</surname><given-names>DG</given-names></name><name><surname>Dunne</surname><given-names>PD</given-names></name><name><surname>McQuaid</surname><given-names>S</given-names></name><name><surname>Gray</surname><given-names>RT</given-names></name><name><surname>Murray</surname><given-names>LJ</given-names></name><name><surname>Coleman</surname><given-names>HG</given-names></name><name><surname>James</surname><given-names>JA</given-names></name><name><surname>Salto-Tellez</surname><given-names>M</given-names></name><name><surname>Hamilton</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>QuPath: Open source software for digital pathology image analysis</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>16878</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-17204-5</pub-id><pub-id pub-id-type="pmid">29203879</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belevich</surname><given-names>I</given-names></name><name><surname>Joensuu</surname><given-names>M</given-names></name><name><surname>Kumar</surname><given-names>D</given-names></name><name><surname>Vihinen</surname><given-names>H</given-names></name><name><surname>Jokitalo</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Microscopy image browser: a platform for segmentation and analysis of multidimensional datasets</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e1002340</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002340</pub-id><pub-id pub-id-type="pmid">26727152</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burnley</surname><given-names>T</given-names></name><name><surname>Palmer</surname><given-names>CM</given-names></name><name><surname>Winn</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Recent developments in the CCP-EM software suite</article-title><source>Acta Crystallographica. Section D, Structural Biology</source><volume>73</volume><fpage>469</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1107/S2059798317007859</pub-id><pub-id pub-id-type="pmid">28580908</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>CastaÃ±o-DÃ­ez</surname><given-names>D</given-names></name><name><surname>Kudryashev</surname><given-names>M</given-names></name><name><surname>Arheit</surname><given-names>M</given-names></name><name><surname>Stahlberg</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Dynamo: a flexible, user-friendly development tool for subtomogram averaging of cryo-EM data in high-performance computing environments</article-title><source>Journal of Structural Biology</source><volume>178</volume><fpage>139</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1016/j.jsb.2011.12.017</pub-id><pub-id pub-id-type="pmid">22245546</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Cornut</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Imgui</data-title><version designator="70b6ba4">70b6ba4</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/ocornut/imgui">https://github.com/ocornut/imgui</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galaz-Montoya</surname><given-names>JG</given-names></name><name><surname>Flanagan</surname><given-names>J</given-names></name><name><surname>Schmid</surname><given-names>MF</given-names></name><name><surname>Ludtke</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Single particle tomography in EMAN2</article-title><source>Journal of Structural Biology</source><volume>190</volume><fpage>279</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1016/j.jsb.2015.04.016</pub-id><pub-id pub-id-type="pmid">25956334</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goddard</surname><given-names>TD</given-names></name><name><surname>Huang</surname><given-names>CC</given-names></name><name><surname>Meng</surname><given-names>EC</given-names></name><name><surname>Pettersen</surname><given-names>EF</given-names></name><name><surname>Couch</surname><given-names>GS</given-names></name><name><surname>Morris</surname><given-names>JH</given-names></name><name><surname>Ferrin</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>UCSF ChimeraX: Meeting modern challenges in visualization and analysis</article-title><source>Protein Science</source><volume>27</volume><fpage>14</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1002/pro.3235</pub-id><pub-id pub-id-type="pmid">28710774</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Picus</surname><given-names>M</given-names></name><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>van Kerkwijk</surname><given-names>MH</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Haldane</surname><given-names>A</given-names></name><name><surname>Del RÃ­o</surname><given-names>JF</given-names></name><name><surname>Wiebe</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>GÃ©rard-Marchant</surname><given-names>P</given-names></name><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Abbasi</surname><given-names>H</given-names></name><name><surname>Gohlke</surname><given-names>C</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Array programming with NumPy</article-title><source>Nature</source><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Isola</surname><given-names>P</given-names></name><name><surname>Zhu</surname><given-names>JY</given-names></name><name><surname>Zhou</surname><given-names>T</given-names></name><name><surname>Efros</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Image-to-image translation with conditional adversarial networks</article-title><conf-name>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><conf-loc>Honolulu, HI</conf-loc><fpage>5967</fpage><lpage>5976</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.632</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iudin</surname><given-names>A</given-names></name><name><surname>Korir</surname><given-names>PK</given-names></name><name><surname>Somasundharam</surname><given-names>S</given-names></name><name><surname>Weyand</surname><given-names>S</given-names></name><name><surname>Cattavitello</surname><given-names>C</given-names></name><name><surname>Fonseca</surname><given-names>N</given-names></name><name><surname>Salih</surname><given-names>O</given-names></name><name><surname>Kleywegt</surname><given-names>GJ</given-names></name><name><surname>Patwardhan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>EMPIAR: the electron microscopy public image archive</article-title><source>Nucleic Acids Research</source><volume>51</volume><fpage>D1503</fpage><lpage>D1511</lpage><pub-id pub-id-type="doi">10.1093/nar/gkac1062</pub-id><pub-id pub-id-type="pmid">36440762</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Last</surname><given-names>MGF</given-names></name><name><surname>Voortman</surname><given-names>LM</given-names></name><name><surname>Sharp</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>scNodes: a correlation and processing toolkit for super-resolution fluorescence and electron microscopy</article-title><source>Nature Methods</source><volume>20</volume><fpage>1445</fpage><lpage>1446</lpage><pub-id pub-id-type="doi">10.1038/s41592-023-01991-z</pub-id><pub-id pub-id-type="pmid">37596472</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Last</surname><given-names>MGF</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Ais</data-title><version designator="swh:1:rev:5494df88a67178b858f57b892603473c5b92115e">swh:1:rev:5494df88a67178b858f57b892603473c5b92115e</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:31a70077abc4603329f59153f10ed824af7e7727;origin=https://github.com/bionanopatterning/Ais;visit=swh:1:snp:07483a149be32b6fbef18df04edbe7698d9bc88e;anchor=swh:1:rev:5494df88a67178b858f57b892603473c5b92115e">https://archive.softwareheritage.org/swh:1:dir:31a70077abc4603329f59153f10ed824af7e7727;origin=https://github.com/bionanopatterning/Ais;visit=swh:1:snp:07483a149be32b6fbef18df04edbe7698d9bc88e;anchor=swh:1:rev:5494df88a67178b858f57b892603473c5b92115e</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawson</surname><given-names>CL</given-names></name><name><surname>Patwardhan</surname><given-names>A</given-names></name><name><surname>Baker</surname><given-names>ML</given-names></name><name><surname>Hryc</surname><given-names>C</given-names></name><name><surname>Garcia</surname><given-names>ES</given-names></name><name><surname>Hudson</surname><given-names>BP</given-names></name><name><surname>Lagerstedt</surname><given-names>I</given-names></name><name><surname>Ludtke</surname><given-names>SJ</given-names></name><name><surname>Pintilie</surname><given-names>G</given-names></name><name><surname>Sala</surname><given-names>R</given-names></name><name><surname>Westbrook</surname><given-names>JD</given-names></name><name><surname>Berman</surname><given-names>HM</given-names></name><name><surname>Kleywegt</surname><given-names>GJ</given-names></name><name><surname>Chiu</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>EMDataBank unified data resource for 3DEM</article-title><source>Nucleic Acids Research</source><volume>44</volume><fpage>D396</fpage><lpage>D403</lpage><pub-id pub-id-type="doi">10.1093/nar/gkv1126</pub-id><pub-id pub-id-type="pmid">26578576</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Linder-NorÃ©n</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Keras-GAN</data-title><version designator="3ff3be4">3ff3be4</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/eriklindernoren/Keras-GAN">https://github.com/eriklindernoren/Keras-GAN</ext-link></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luengo</surname><given-names>I</given-names></name><name><surname>Darrow</surname><given-names>MC</given-names></name><name><surname>Spink</surname><given-names>MC</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Dai</surname><given-names>W</given-names></name><name><surname>He</surname><given-names>CY</given-names></name><name><surname>Chiu</surname><given-names>W</given-names></name><name><surname>Pridmore</surname><given-names>T</given-names></name><name><surname>Ashton</surname><given-names>AW</given-names></name><name><surname>Duke</surname><given-names>EMH</given-names></name><name><surname>Basham</surname><given-names>M</given-names></name><name><surname>French</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>SuRVoS: super-region volume segmentation workbench</article-title><source>Journal of Structural Biology</source><volume>198</volume><fpage>43</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1016/j.jsb.2017.02.007</pub-id><pub-id pub-id-type="pmid">28246039</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mastronarde</surname><given-names>DN</given-names></name><name><surname>Held</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Automated tilt series alignment and tomographic reconstruction in IMOD</article-title><source>Journal of Structural Biology</source><volume>197</volume><fpage>102</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/j.jsb.2016.07.011</pub-id><pub-id pub-id-type="pmid">27444392</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pyle</surname><given-names>E</given-names></name><name><surname>Hutchings</surname><given-names>J</given-names></name><name><surname>Zanetti</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Strategies for picking membrane-associated particles within subtomogram averaging workflows</article-title><source>Faraday Discussions</source><volume>240</volume><fpage>101</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1039/d2fd00022a</pub-id><pub-id pub-id-type="pmid">35924570</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fisher</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Very Deep Convolutional Networks for Large-Scale Image Regognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Sermanet</surname><given-names>P</given-names></name><name><surname>Reed</surname><given-names>S</given-names></name><name><surname>Anguelov</surname><given-names>D</given-names></name><name><surname>Erhan</surname><given-names>D</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Rabinovich</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Going deeper with convolutions</article-title><conf-name>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2015.7298594</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Alemi</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Inception-v4, inception-ResNet and the impact of residual connections on learning</article-title><conf-name>Proceedings of the AAAI Conference on Artificial Intelligence</conf-name><pub-id pub-id-type="doi">10.1609/aaai.v31i1.11231</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Hoek</surname><given-names>H</given-names></name><name><surname>Klena</surname><given-names>N</given-names></name><name><surname>Jordan</surname><given-names>MA</given-names></name><name><surname>Alvarez Viar</surname><given-names>G</given-names></name><name><surname>Righetto</surname><given-names>RD</given-names></name><name><surname>Schaffer</surname><given-names>M</given-names></name><name><surname>Erdmann</surname><given-names>PS</given-names></name><name><surname>Wan</surname><given-names>W</given-names></name><name><surname>Geimer</surname><given-names>S</given-names></name><name><surname>Plitzko</surname><given-names>JM</given-names></name><name><surname>Baumeister</surname><given-names>W</given-names></name><name><surname>Pigino</surname><given-names>G</given-names></name><name><surname>Hamel</surname><given-names>V</given-names></name><name><surname>Guichard</surname><given-names>P</given-names></name><name><surname>Engel</surname><given-names>BD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>In situ architecture of the ciliary base reveals the stepwise assembly of intraflagellar transport trains</article-title><source>Science</source><volume>377</volume><fpage>543</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1126/science.abm6704</pub-id><pub-id pub-id-type="pmid">35901159</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Walt</surname><given-names>S</given-names></name><name><surname>SchÃ¶nberger</surname><given-names>JL</given-names></name><name><surname>Nunez-Iglesias</surname><given-names>J</given-names></name><name><surname>Boulogne</surname><given-names>F</given-names></name><name><surname>Warner</surname><given-names>JD</given-names></name><name><surname>Yager</surname><given-names>N</given-names></name><name><surname>Gouillart</surname><given-names>E</given-names></name><name><surname>Yu</surname><given-names>T</given-names></name><collab>scikit-image contributors</collab></person-group><year iso-8601-date="2014">2014</year><article-title>scikit-image: image processing in Python</article-title><source>PeerJ</source><volume>2</volume><elocation-id>e453</elocation-id><pub-id pub-id-type="doi">10.7717/peerj.453</pub-id><pub-id pub-id-type="pmid">25024921</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>Ä°</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolff</surname><given-names>G</given-names></name><name><surname>Limpens</surname><given-names>RWAL</given-names></name><name><surname>Zevenhoven-Dobbe</surname><given-names>JC</given-names></name><name><surname>Laugks</surname><given-names>U</given-names></name><name><surname>Zheng</surname><given-names>S</given-names></name><name><surname>de Jong</surname><given-names>AWM</given-names></name><name><surname>Koning</surname><given-names>RI</given-names></name><name><surname>Agard</surname><given-names>DA</given-names></name><name><surname>GrÃ¼newald</surname><given-names>K</given-names></name><name><surname>Koster</surname><given-names>AJ</given-names></name><name><surname>Snijder</surname><given-names>EJ</given-names></name><name><surname>BÃ¡rcena</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A molecular pore spans the double membrane of the coronavirus replication organelle</article-title><source>Science</source><volume>369</volume><fpage>1395</fpage><lpage>1398</lpage><pub-id pub-id-type="doi">10.1126/science.abd3629</pub-id><pub-id pub-id-type="pmid">32763915</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>G-H</given-names></name><name><surname>Smith-Geater</surname><given-names>C</given-names></name><name><surname>Galaz-Montoya</surname><given-names>JG</given-names></name><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Gupte</surname><given-names>SR</given-names></name><name><surname>Aviner</surname><given-names>R</given-names></name><name><surname>Mitchell</surname><given-names>PG</given-names></name><name><surname>Hsu</surname><given-names>J</given-names></name><name><surname>Miramontes</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>KQ</given-names></name><name><surname>Geller</surname><given-names>NR</given-names></name><name><surname>Hou</surname><given-names>C</given-names></name><name><surname>Danita</surname><given-names>C</given-names></name><name><surname>Joubert</surname><given-names>L-M</given-names></name><name><surname>Schmid</surname><given-names>MF</given-names></name><name><surname>Yeung</surname><given-names>S</given-names></name><name><surname>Frydman</surname><given-names>J</given-names></name><name><surname>Mobley</surname><given-names>W</given-names></name><name><surname>Wu</surname><given-names>C</given-names></name><name><surname>Thompson</surname><given-names>LM</given-names></name><name><surname>Chiu</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>CryoET reveals organelle phenotypes in huntington disease patient iPSC-derived and mouse primary neurons</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>692</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-36096-w</pub-id><pub-id pub-id-type="pmid">36754966</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s7"><title>Supplementary note 1: Python file format for Ais models</title><sec sec-type="appendix" id="s7-1"><title>Adding a Keras model</title><p>Most models in the standard Ais library are Keras models (tensorflow.keras). Adding an extra keras model with a new architecture is relatively straightforward and can be achieved by adding a.py file to Ais/models directory. The.py file requires three components: a title for the model, a boolean that specifies whether the model should be available in the software, and a function âcreateâ that returns a keras model. The implementation of the VGGNet model (vggnet.py) is copied below as an example.</p><p><code xml:space="preserve">from tensorflow.keras.models import Model 
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose 
from tensorflow.keras.optimizers import Adam 

title = &quot;VGGNet&quot; 
include = True 

def create(input_shape): 
inputs = Input(input_shape) 

# Block 1 
conv1 = Conv2D(64, (3, 3), activation='relu', padding=âsame')(inputs) 
conv2 = Conv2D(64, (3, 3), activation='relu', padding=âsame')(conv1) 
pool1 = MaxPooling2D(pool_size=(2, 2))(conv2) 

# Block 2 
conv3 = Conv2D(128, (3, 3), activation='relu', padding=âsame')(pool1) 
conv4 = Conv2D(128, (3, 3), activation='relu', padding=âsame')(conv3) 
pool2 = MaxPooling2D(pool_size=(2, 2))(conv4) 

# Block 3 
conv5 = Conv2D(256, (3, 3), activation='relu', padding=âsame')(pool2) 
conv6 = Conv2D(256, (3, 3), activation='relu', padding=âsame')(conv5) 
pool3 = MaxPooling2D(pool_size=(2, 2))(conv6) 

# Upsampling and Decoding 
up1 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding=âsame')(pool3) 
conv7 = Conv2D(128, (3, 3), activation='relu', padding=âsame')(up1) 

up2 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding=âsame')(conv7) 
conv8 = Conv2D(64, (3, 3), activation='relu', padding=âsame')(up2) 

up3 = Conv2DTranspose(1, (2, 2), strides=(2, 2), padding=âsame')(conv8) 
output = Conv2D(1, (1, 1), activation=âsigmoid')(up3) 

# create the model 
model = Model(inputs=[inputs], outputs=[output]) 
model.compile(optimizer=Adam(), loss='binary_crossentropy') 

return model</code></p></sec><sec sec-type="appendix" id="s7-2"><title>Adding a custom model</title><p>Adding a non-Keras model is also possible but requires a little bit of extra work. Only a small number of methods of the Keras model object type are directly accessed by Ais. These are: count_params, fit, predict, save, and load. Adding a custom model thus requires adding a.py file to the <italic>Ais/models</italic> that contains four components: a title, a boolean that specifies whether the model is available in the software, and a function âcreateâ that returns model object (these are as before, with adding a keras model), and additionally a definition of a class that implements the required methods. The return types of these methods should be the same as those returned by the corresponding Keras methods. The content of the <italic>model_template.py</italic> template file is copied below as an example.</p><p><code xml:space="preserve">title = &quot;Template_model&quot; 
include = False 


def create(input_shape): 
return TemplateModel(input_shape) 


class TemplateModel: 
def __init__(self, input_shape): 
self.img_shape = input_shape 
self.generator, self.discriminator = self.compile_custom_model() 

def compile_custom_model(self): 
# e.g.: compile generator, compile discriminator, return. 
return 0, 0 

def count_params(self): 
# e.g. return self.generator.count_params() 
# for the default models, the number of parameters that is returned is the amount that are involved in processing, not in training. So for e.g. a GAN, the discriminator params are not included. 
return 0 

def fit(self, train_x, train_y, epochs, batch_size=1, shuffle=True, 
callbacks=[]): 
for c in callbacks: 
c.params['epochs'] = epochs 

# fit model, e.g.: 
for e in range(epochs): 
for i in range(len(train_x) // batch_size): 
# fit batch 
pass 

logs = {'loss': 0.0} 
for c in callbacks: 
c.on_batch_end(i, logs) 

def predict(self, images): 
# e.g.: return self.generator.predict(images) 
return None 

def save(self, path): 
pass 

def load(self, path): 
pass</code></p><p>A more concrete example of the implementation of a custom model can be found in Ais/models/pix2pix.py. The pix2pix model (<xref ref-type="bibr" rid="bib11">Isola et al., 2017</xref>) is also implemented in Keras, but since it internally uses two separate Keras model objects (the generator and the discriminator), some additional effort is required to make it compatible with the class format used in Ais. See: <ext-link ext-link-type="uri" xlink:href="https://github.com/bionanopatterning/Ais/blob/master/Ais/models/pix2pix.py">here</ext-link> and <xref ref-type="bibr" rid="bib16">Linder-NorÃ©n, 2019</xref>.</p></sec></sec><sec sec-type="appendix" id="s8"><title>Importing and exporting models</title><sec sec-type="appendix" id="s8-1"><title>Export</title><p>After training, Ais models can be saved for future re-use or for uploading to the repository <ext-link ext-link-type="uri" xlink:href="https://aiscryoet.org/">aiscryoet.org</ext-link>. The resulting savefiles, with extension .<italic>scnm</italic>, are uncompressed .tar archives that contain an number of required files:</p><list list-type="bullet"><list-item><p>A .h5 file that fully describes the CNN architecture and weights. <italic>The .h5 file is generated using the standard keras procedure model.save</italic>.</p></list-item><list-item><p>A .json metadata file, which contains information on the name and processing parameters used for the model.</p></list-item></list><p>If a tomogram was open in Ais at the moment of saving the model, two additional files are also included:</p><list list-type="bullet"><list-item><p>A .tiff file, containing a single slice from that tomogram, saved so that the model performance can be validated prior to releasing it on the repository.</p></list-item><list-item><p>A .png file, an image (downsized 512 Ã 512 pixels) depicting the validation slice with a segmentation overlaid on top, which is used as the thumbnail on the model repository.</p></list-item></list><p>Of these last two files, only the .png file is publically available on the repository. The image that will be publically visible is also displayed on the <ext-link ext-link-type="uri" xlink:href="https://aiscryoet.org">aiscryoet.org</ext-link> upload page, prior to actually uploading the model.</p><fig id="app1fig1" position="float"><label>Appendix 1âfigure 1.</label><caption><title>Uploading a model to the model repository via aiscryoet.org/upload.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-app1-fig1-v1.tif"/></fig></sec><sec sec-type="appendix" id="s8-2"><title>Import</title><p>A.scnm tar archive that contains correctly formatted.h5 and.json files can be loaded into Ais. Models generated elsewhere can also be imported, if these two files are combined into a.tar archive and renamed.scnm.</p><p><italic>The .h5 file is read using the standard keras load_model procedure</italic>.</p><p>The following fields are expected in the.json:</p><p><code xml:space="preserve">{ 
&quot;title&quot;: &quot;Membrane&quot;, 
&quot;colour&quot;: [1.0, 0.40784314274787903, 0.0], 
&quot;apix&quot;: 14.04, 
&quot;compiled&quot;: true, 
&quot;box_size&quot;: 64, 
&quot;model_enum&quot;: 5, 
&quot;epochs&quot;: 50, 
&quot;batch_size&quot;: 64, 
&quot;active&quot;: true, 
&quot;blend&quot;: false, 
&quot;show&quot;: true, 
&quot;alpha&quot;: 0.75, 
&quot;threshold&quot;: 0.5, 
&quot;overlap&quot;: 0.20000000298023224, 
&quot;active_tab&quot;: 0, 
&quot;n_parameters&quot;: 17521921, 
&quot;n_copies&quot;: 10, 
&quot;info&quot;: &quot;VGGNet L (17521921, 64, 14.040, 0.0189)&quot;, 
&quot;info_short&quot;: &quot;(VGGNet L, 64, 14.040, 0.0189)&quot;, 
&quot;excess_negative&quot;: 100, 
&quot;emit&quot;: false, 
&quot;absorb&quot;: false, 
&quot;loss&quot;: 0.01889071986079216 
}</code></p></sec></sec><sec sec-type="appendix" id="s9"><title>Model groups</title><p>Instead of saving single models, it is also possible to save multiple models and the interactions between them as a group.</p><p>In this case, a single file with extension.scnmgroup is created. This file is also an uncompressed.tar archive, and it contains one.scnm file for each model in the group, as well as a.json file that describes the interactions between the models.</p></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98552.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Scheres</surname><given-names>Sjors HW</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>MRC Laboratory of Molecular Biology</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This work describes a new software platform for machine-learning-based segmentation of and particle-picking in cryo-electron tomograms. The program and its corresponding online database of trained models will allow experimentalists to conveniently test different models and share their results with others. The paper provides <bold>convincing</bold> evidence that the software will be <bold>valuable</bold> to the community.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98552.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This paper describes &quot;Ais&quot;, a new software tool for machine-learning based segmentation and particle picking of electron tomograms. The software can visualise tomograms as slices and allows manual annotation for the training of a provided set of various types of neural networks. New networks can be added, provided they adhere to a python file with an (undescribed) format. Once networks have been trained on manually annotated tomograms, they can be used to segment new tomograms within the same software. The authors also set up an online repository to which users can upload their models, so they might be re-used by others with similar needs. By logically combining the results from different types of segmentations, they further improve the detection of distinct features. The authors demonstrate the usefulness of their software on various data sets. Thus, the software appears to be a valuable tool for the cryo-ET community that will lower the boundaries of using a variety of machine-learning methods to help interpret tomograms.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98552.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Last et al. present Ais, a new deep learning based software package for segmentation of cryo electron tomography data sets. The distinguishing factor of this package is its orientation to the joint use of different models, rather than the implementation of a given approach: Notably, the software is supported by an online repository of segmentation models, open to contributions from the community.</p><p>The usefulness of handling different models in one single environment is showcased with a comparative study on how different models perform on a given data set; then with an explanation on how the results of several models can be manually merged by the interactive tools inside Ais.</p><p>The manuscripts presents two applications of Ais on real data sets; one oriented to showcase its particle picking capacities on a study previously completed by the authors; a second one refers to a complex segmentation problem on two different data sets (representing different geometries as bacterial cilia and mitochondria in a mouse neuron), both from public databases.</p><p>The software described in the paper is compactly documented in its website, additionally providing links to some youtube videos (less than an hour it toral) where the authors videocapture and comment major workflows.</p><p>In short, the manuscript describes a valuable resource for the community of tomography practitioners.</p><p>Strengths:</p><p>Public repository of segmentation models; easiness of working with several models and comparing/merging the results.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98552.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this manuscript, Last and colleagues describe Ais, an open-source software package for the semi-automated segmentation of cryo-electron tomography (cryo-ET) maps. Specifically, Ais provides a graphical user interface (GUI) for the manual segmentation and annotation of specific features of interest. These manual annotations are then used as input ground-truth data for training a convolutional neural network (CNN) model, which can then be used for automatic segmentation. Ais provides the option of several CNNs so that users can compare their performance on their structures of interest in order to determine the CNN that best suits their needs. Additionally, pretrained models can be uploaded and shared to an online database.</p><p>Algorithms are also provided to characterize &quot;model interactions&quot; which allows users to define heuristic rules on how the different segmentations interact. For instance, a membrane adjacent protein can have rules where it must colocalize a certain distance away from a membrane segmentation. Such rules can help reduce false positives; as in the case above, false negatives predicted away from membranes are eliminated.</p><p>The authors then show how Ais can be used for particle picking and subsequent subtomogram averaging and for segmentation of cellular tomograms for visual analysis. For subtomogram averaging, they used a previously published dataset and compared the averages of their automated picking with the published manual picking. Analysis of cellular tomogram segmentations were primarily visual.</p><p>Strengths:</p><p>CNN-based segmentation of cryo-ET data is a rapidly developing area of research, as it promises substantially faster results than manual segmentation as well as the possibility for higher accuracy. However, this field is still very much in the development and the overall performance of these approaches, even across different algorithms, still leaves much to be desired. In this context, I think Ais is an interesting packages, as it aims to provide both new and experienced users streamlined approaches for manual annotation, access to a number of CNNs, and methods to refine the outputs of CNN models against each other. I think this can be quite useful for users, particularly as these methods develop.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98552.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Last</surname><given-names>Mart GF</given-names></name><role specific-use="author">Author</role><aff><institution>Leiden University Medical Center</institution><addr-line><named-content content-type="city">Leiden</named-content></addr-line><country>Netherlands</country></aff></contrib><contrib contrib-type="author"><name><surname>Abendstein</surname><given-names>Leoni</given-names></name><role specific-use="author">Author</role><aff><institution>Institute of Science and Technology Austria</institution><addr-line><named-content content-type="city">Klosterneuburg</named-content></addr-line><country>Austria</country></aff></contrib><contrib contrib-type="author"><name><surname>Voortman</surname><given-names>Lenard M</given-names></name><role specific-use="author">Author</role><aff><institution>Leiden University Medical Center</institution><addr-line><named-content content-type="city">Leiden</named-content></addr-line><country>Netherlands</country></aff></contrib><contrib contrib-type="author"><name><surname>Sharp</surname><given-names>Thomas H</given-names></name><role specific-use="author">Author</role><aff><institution>University of Bristol</institution><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authorsâ response to the original reviews.</p><p>We would like to thank the reviewers for helping us improve our article and software. The feedback that we received was very helpful and constructive, and we hope that the changes that we have made are indeed effective at making the software more accessible, the manuscript clearer, and the online documentation more insightful as well. A number of comments related to shared concerns, such as:</p><p>â¢ the need to describe various processing steps more clearly (e.g. particle picking, or the nature of âdustâ in segmentations)</p><p>â¢ describing the features of Ais more clearly, and explaining how it can interface with existing tools that are commonly used in cryoET</p><p>â¢ a degree of subjectivity in the discussion of results (e.g. about Pix2pix performing better than other networks in some cases.)</p><p>We have now addressed these important points, with a focus on streamlining not only the workflow within Ais but also making interfacing between Ais and other tools easier. For instance, we explain more clearly which file types Ais uses and we have added the option to export .star files for use in, e.g., Relion, or meshes instead of coordinate lists. We also include information in the manuscript about how the particle picking process is implemented, and how false positives (âdustâ) can be avoided. Finally, all reviewers commented on our notion that Pix2pix can work âbetterâ despite reaching a higher loss after training. As suggested, we included a brief discussion about this idea in the supplementary information (Fig. S6) and used it to illustrate how Ais enables iteratively improving segmentation results.</p><p>Since receiving the reviews we have also made a number of other changes to the software that are not discussed below but that we nonetheless hope have made the software more reliable and easier to use. These include expanding the available settings, slight changes to the image processing that can help speed it up or avoid artefacts in some cases, improving the GUI-free usability of Ais, and incorporating various tools that should help make it easier to use Ais with remote data (e.g. doing annotation on an office PC, but model training on a more powerful remote PC). We have also been in contact with a number of users of the software, who reported issues or suggested various other miscellaneous improvements, and many of whom had found the software via the reviewed preprint.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 1 (Public Review):</bold></p><p>This paper describes &quot;Ais&quot;, a new software tool for machine-learning-based segmentation and particle picking of electron tomograms. The software can visualise tomograms as slices and allows manual annotation for the training of a provided set of various types of neural networks. New networks can be added, provided they adhere to a Python file with an (undescribed) format. Once networks have been trained on manually annotated tomograms, they can be used to segment new tomograms within the same software. The authors also set up an online repository to which users can upload their models, so they might be re-used by others with similar needs. By logically combining the results from different types of segmentations, they further improve the detection of distinct features. The authors demonstrate the usefulness of their software on various data sets. Thus, the software appears to be a valuable tool for the cryo-ET community that will lower the boundaries of using a variety of machine-learning methods to help interpret tomograms.</p></disp-quote><p>We thank the reviewer for their kind feedback and for taking the time to review our article. On the basis of their comments, we have made a number of changes to the software, article, and documentation, that we think have helped improve the project and render it more accessible (especially for interfacing with different tools, e.g. the suggestions to describe the file formats in more detail). We respond to all individual comments one-by-one below.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations:</bold></p><p>I would consider raising the level of evidence that this program is useful to *convincing* if the authors would adequately address the suggestions for improvement below.</p><p>(1) It would be helpful to describe the format of the Python files that are used to import networks, possibly in a supplement to the paper.</p></disp-quote><p>We have now included this information in both the online documentation and as a supplementary note (Supplementary Note 1).</p><disp-quote content-type="editor-comment"><p>(2) Likewise, it would be helpful to describe the format in which particle coordinates are produced. How can they be used in subsequent sub-tomogram averaging pipelines? Are segmentations saved as MRC volumes? Or could they be saved as triangulations as well? More implementation details like this would be good to have in the paper, so readers don't have to go into the code to investigate.</p></disp-quote><p>Coordinates: previously, we only exported arrays of coordinates as tab-separated .txt files, compatible with e.g. EMAN2. We now added a selection menu where users can specify whether to export either .star files or tsv .txt files, which together we think should cover most software suites for subtomogram averaging.</p><p>Triangulations: We have now improved the functionality for exporting triangulations. In the particle picking menu, there is now the option to output either coordinates or meshes (as .obj files). This was previously possible in the Rendering tab, but with the inclusion in the picking menu exporting triangulations can now be done for all tomograms at once rather than manually one by one.</p><p>Edits in the text: the output formats were previously not clear in the text. We have now included this information in the introduction:</p><p>â[â¦] To ensure compatibility with other popular cryoET data processing suites, Ais employs file formats that are common in the field, using .mrc files for volumes, tab-separated .txt or .star files for particle datasets, and the .obj file format for exporting 3D meshes.â</p><disp-quote content-type="editor-comment"><p>(3) In Table 2, pix2pix has much higher losses than alternatives, yet the text states it achieves fewer false negatives and fewer false positives. An explanation is needed as to why that is. Also, it is mentioned that a higher number of epochs may have improved the results. Then why wasn't this attempted?</p></disp-quote><p>The architecture of Pix2pix is quite different from that of the other networks included in the test. Whereas all others are trained to minimize a binary cross entropy (BCE) loss, Pix2pix uses a composite loss function that is a weighted combination of the generator loss and a discriminator penalty, neither of which employ BCE. However, to be able to compare loss values, we do compute a BCE loss value for the Pix2pix generator after every training epoch. This is the value reported in the manuscript and in the software. Although Pix2pixâ BCE loss does indeed diminish during training, the model is not actually optimized to minimize this particular value and a comparison by BCE loss is therefore not entirely fair to Pix2pix. This is pointed out (in brief) in the legend to the able:</p><p>âUnlike the other architectures, Pix2pix is not trained to minimize the bce loss but uses a different loss function instead. The bce loss values shown here were computed after training and may not be entirely comparable.â</p><p>Regarding the extra number of epochs for Pix2pix: here, we initially ran in to the problem that the number of samples in the training data was low for the number of parameters in Pix2pix, leading to divergence later during training. This problem did not occur for most other models, so we decided to keep the data for the discussion around Table 1 and Figure 2 limited to that initial training dataset. After that, we increased the sample size (from 58 to 170 positive samples) and trained the model for longer. The resulting model was used in the subsequent analyses. This was previously implicit in the text but is now mentioned explicitly and in a new supplementary figure.</p><p>âFor the antibody platform, the model that would be expected to be one of the worst based on the loss values, Pix2pix, actually generates segmentations that are seem well-suited for the downstream processing tasks. It also output fewer false positive segmentations for sections of membranes than many other models, including the lowest-loss model UNet. Moreover, since Pix2pix is a relatively large network, it might also be improved further by increasing the number of training epochs. We thus decided to use Pix2pix for the segmentation of antibody platforms, and increased the size of the antibody platform training dataset (from 58 to 170 positive samples) to train a much improved second iteration of the network for use in the following analyses (Fig. S6).â</p><disp-quote content-type="editor-comment"><p>(4) It is not so clear what absorb and emit mean in the text about model interactions. A few explanatory sentences would be useful here.</p></disp-quote><p>We have expanded this paragraph to include some more detail.</p><p>âBesides these specific interactions between two models, the software also enables pitching multiple models against one another in what we call âmodel competitionâ. Models can be set to âemitâ and/or âabsorbâ competition from other models. Here, to emit competition means that a modelâs prediction value is included in a list of competing models. To absorb competition means that a modelâs prediction value will be compared to all values in that list, and that this modelâs prediction value for any pixel will be set to zero if any of the competing modelsâ prediction value is higher. On a pixel-by-pixel basis, all models that absorb competition are thus suppressed whenever their prediction value for a pixel is lower than that of any of the emitting models.â</p><disp-quote content-type="editor-comment"><p>(5) Under Figure 4, the main text states &quot;the model interactions described above&quot;, but because multiple interactions were described it is not clear which ones they were. Better to just specify again.</p></disp-quote><p>Changed as follows:</p><p>âThe antibody platform and antibody-C1 complex models were then applied to the respective datasets, in combination with the membrane and carbon models and the model interactions described above (Fig. 4b): the membrane avoiding carbon, and the antibody platforms colocalizing with the resulting membranesâ.</p><p>(6) The next paragraph mentions a &quot;batch particle picking process to determine lists of particle coordinates&quot;, but the algorithm for how coordinates are obtained from segmented volumes is not described.</p><p>We have added a paragraph to the main text to describe the picking process:</p><p>âThis picking step comprises a number of processing steps (Fig. S7). First, the segmented (.mrc) volumes are thresholded at a user-specified level. Second, a distance transform of the resulting binary volume is computed, in which every nonzero pixel in the binary volume is assigned a new value, equal to the distance of that pixel to the nearest zero-valued pixel in the mask. Third, a watershed transform is applied to the resulting volume, so that the sets of pixels closest to any local maximum in the distance transformed volume are assigned to one group. Fourth, groups that are smaller than a user-specified minimum volume are discarded. Fifth, groups are assigned a weight value, equal to the sum of the prediction value (i.e. the corresponding pixel value in the input .mrc volume) of the pixels in the group. For every group found within close proximity to another group (using a user-specified value for the minimum particle spacing), the group with the lower weight value is discarded. Finally, the centroid coordinate of the grouped pixels is considered the final particle coordinate, and the list of all coordinates is saved in a tab-separated text file.</p><p>âAs an alternative output format, segmentations can also be converted to and saved as triangulated meshes, which can then be used for, e.g., membrane-guided particle picking. After picking particles, the resulting coordinates are immediately available for inspection in the Ais 3D renderer (Fig. S8).â</p><p>The two supplementary figures are pasted below for convenience. Fig. S7 is new, while Fig. S8 was previously Fig. S10 -the reference to this figure was originally missing in the main text, but is now included.</p><disp-quote content-type="editor-comment"><p>(7) In the Methods section, it is stated that no validation splits are used &quot;in order to make full use of an input set&quot;. This sounds like an odd decision, given the importance of validation sets in the training of many neural networks. Then how is overfitting monitored or prevented? This sounds like a major limitation of the method.</p></disp-quote><p>In our experience, the best way of preparing a suitable model is to (iteratively) annotate a set of training images and visually inspect the result. Since the manual annotation step is the bottleneck in this process, we decided not to use validation split in order to make full use of an annotated training dataset (i.e. a validation split of 20% would mean that 20% of the manually annotated training data is not used for training)</p><p>We do recognize the importance of using separate data for validation, or at least offering the possibility of doing so. We have now added a parameter to the settings (and made a Settings menu item available in the top menu bar) where users can specify what fraction (0, 10, 20, or 50%) of training datasets should be set aside for validation. If the chosen value is not 0%, the software reports the validation loss as well as the size of the split during training, rather than (as was done previously) the training loss. We have, however, set the default value for the validation split to 0%, for the same reason as before. We also added a section to the online documentation about using validation splits, and edited the corresponding paragraph in the methods section:</p><p>âThe reported loss is that calculated on the training dataset itself, i.e., no validation split was applied. During regular use of the software, users can specify whether to use a validation split or not. By default, a validation split is not applied, in order to make full use of an input set of ground truth annotations. Depending on the chosen split size, the software reports either the overall training loss or the validation loss during training.â</p><disp-quote content-type="editor-comment"><p>(8) Related to this point: how is the training of the models in the software modelled? It might be helpful to add a paragraph to the paper in which this process is described, together with indicators of what to look out for when training a model, e.g. when should one stop training?</p></disp-quote><p>We have expanded the paragraph where we write about the utility of comparing different networks architectures to also include a note on how Ais facilitates monitoring the output of a model during training:</p><p>âWhen taking the training and processing speeds in to account as well as the segmentation results, there is no overall best architecture. We therefore included multiple well-performing model architectures in the final library, in order to allow users to select from these models to find one that works well for their specific datasets. Although it is not necessary to screen different network architectures and users may simply opt to use the default (VGGNet), these results thus show that it can be useful to test different networks in order to identify one that is best. Moreover, these results also highlight the utility of preparing well-performing models by iteratively improving training datasets and re-training models in a streamlined interface. To aid in this process, the software displays the loss value of a network during training and allows for the application of models to datasets during training. Thus, users can inspect how a modelâs output changes during training and decide whether to interrupt training and improve the training data or choose a different architecture.â</p><disp-quote content-type="editor-comment"><p>(9) Figure 1 legend: define the colours of the different segmentations.</p></disp-quote><p>Done</p><disp-quote content-type="editor-comment"><p>(10) It may be better to colour Figure 2B with the same colours as Figure 2A.</p></disp-quote><p>We tried this, but the effect is that the underlying density is much harder to see. We think the current grayscale image paired with the various segmentations underneath is better for visually identifying which density corresponds to membranes, carbon film, or antibody platforms.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 2 (Public Review):</bold></p><p>Summary:</p><p>Last et al. present Ais, a new deep learning-based software package for the segmentation of cryo-electron tomography data sets. The distinguishing factor of this package is its orientation to the joint use of different models, rather than the implementation of a given approach. Notably, the software is supported by an online repository of segmentation models, open to contributions from the community.</p><p>The usefulness of handling different models in one single environment is showcased with a comparative study on how different models perform on a given data set; then with an explanation of how the results of several models can be manually merged by the interactive tools inside Ais.</p><p>The manuscripts present two applications of Ais on real data sets; one is oriented to showcase its particlepicking capacities on a study previously completed by the authors; the second one refers to a complex segmentation problem on two different data sets (representing different geometries as bacterial cilia and mitochondria in a mouse neuron), both from public databases.</p><p>The software described in the paper is compactly documented on its website, additionally providing links to some YouTube videos (less than an hour in total) where the authors videocapture and comment on major workflows.</p><p>In short, the manuscript describes a valuable resource for the community of tomography practitioners.</p><p>Strengths:</p><p>A public repository of segmentation models; easiness of working with several models and comparing/merging the results.</p><p>Weaknesses:</p><p>A certain lack of concretion when describing the overall features of the software that differentiate it from others.</p></disp-quote><p>We thank the reviewer for their kind and constructive feedback. Following the suggestion to use the Pix2pix results to illustrate the utility of Ais for analyzing results, we have added a new supplementary figure (Fig. S6) and brief discussion, showing the use of Ais in iteratively improving segmentation results. We have also expanded the online documentation and included a note in the supplementary information about how models are saved/loaded (Supplemetary note 1).</p><disp-quote content-type="editor-comment"><p><bold>Recommendations:</bold></p><p>I would like to ask the authors about some concerns about the Ais project as a whole:</p><p>(1) The website that accompanies the paper (aiscryoet.org), albeit functional, seems to be in its first steps. Is it planned to extend it? In particular, one of the major contributions of the paper (the maintenance of an open repository of models) could use better documentation describing the expected formats to submit models. This could even be discussed in the supplementary material of the manuscript, as this feature is possibly the most distinctive one of the paper. Engaging third-party users would require giving them an easier entry point, and the superficial mention of this aspect in the online documentation could be much more generous.</p></disp-quote><p>We have added a new page to the online documentation, titled âSharing modelsâ where we include an explanation of the structure of model files and demonstrate the upload page. We also added a note to the Supplementary Information that explains the file format for models, and how they are loaded/saved (i.e., that these standard keras model obects).</p><p>To make it easier to interface Ais with other tools, we have now also made some of the core functionality available (e.g. training models, batch segmentation) via the command line interface. Information on how to use this is included in the online documentation. All file formats are common formats used in cryoET, so that using Ais in a workflow with, e.g. AreTomo -&gt; Ais -&gt; Relion should now be more straightforward.</p><disp-quote content-type="editor-comment"><p>(2) A different major line advanced by the authors to underpin the novelty of the software, is its claimed flexibility and modularity. In particular, the restrictions of other packages in terms of visualization and user interaction are mentioned. Although in the manuscript it is also mentioned that most of the functionalities in Ais are already available in major established packages, as a reader I am left confused about what exactly makes the offer of Ais different from others in terms of operation and interaction: is it just the two aspects developed in the manuscript (possibility of using different models and tools to operate model interaction)? If so, it should probably be stated; but if the authors want to pinpoint other aspects of the capacity of Ais to drive smoothly the interactions, they should be listed and described, instead of leaving it as an unspecific comment. As a potential user of Ais, I would suggest the authors add (maybe in the supplementary material) a listing of such features. Figure 1 does indeed carry the name &quot;overview of (...) functionalities&quot;, but it is not clear to me which functionalities I can expect to be absent or differently solved on the other tools they mention.</p></disp-quote><p>We have rewritten the part of the introduction where we previously listed the features as below. We think it should now be clearer for the reader to know what features to expect, as well as how Ais can interface with other software (i.e. what the inputs and outputs are). We have also edited the caption for Figure 1 to make it explicit that panels A to C represent the annotation, model preparation, and rendering steps of the Ais workflow and that the images are screenshots from the software.</p><p>âIn this report we present Ais, an open-source tool that is designed to enable any cryoET user â whether experienced with software and segmentation or a novice â to quickly and accurately segment their cryoET data in a streamlined and largely automated fashion. Ais comprises a comprehensive and accessible user interface within which all steps of segmentation can be performed, including: the annotation of tomograms and compiling datasets for the training of convolutional neural networks (CNNs), training and monitoring performance of CNNs for automated segmentation, 3D visualization of segmentations, and exporting particle coordinates or meshes for use in downstream processes. To help generate accurate segmentations, the software contains a library of various neural network architectures and implements a system of configurable interactions between different models. Overall, the software thus aims to enable a streamlined workflow where users can interactively test, improve, and employ CNNs for automated segmentation. To ensure compatibility with other popular cryoET data processing suites, Ais employs file formats that are common in the field, using .mrc files for volumes, tab-separated .txt or .star files for particle datasets, and the .obj file format for exporting 3D meshes.â</p><p>âFigure 1 â an overview of the user interface and functionalities. The various panels represent sequential stages in the Ais processing workflow, including annotation (a), testing CNNs (b), visualizing segmentation (c). These images (a-c) are unedited screenshots of the software. (a) [â¦]â</p><disp-quote content-type="editor-comment"><p>(3) Table 1 could have the names of the three last columns. The table has enough empty space in the other columns to accommodate this.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>(4) The comment about Pix2pix needing a larger number of training epochs (being a larger model than the other ones considered) is interesting. It also lends itself for the authors to illustrate the ability of their software to precisely do this: allow the users to flexibly analyze results and test hypothesis</p></disp-quote><p>Please see the response to Reviewer 1 comment #3. We agree that this is a useful example of the ability to iterate between annotation and training, and have added an explicit mention of this in the text:</p><p>âMoreover, since Pix2pix is a relatively large network, it might also be improved further by increasing the number of training epochs. In a second iteration of annotation and training, we thus increased the size of the antibody platform training dataset (from 58 to 170 positive samples) and generated an improved Pix2pix model for use in the following analyses.â</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 3 (Public Review):</bold></p><p>We appreciate the reviewerâs extensive and very helpful feedback and are glad to read that they consider Ais potentially quite useful for the users. To address the reviewerâs comments, we have made various edits to the text, figures, and documentation, that we think have helped improve the clarity of our work. We list all edits below.</p><p>Summary</p><p>In this manuscript, Last and colleagues describe Ais, an open-source software package for the semi-automated segmentation of cryo-electron tomography (cryo-ET) maps. Specifically, Ais provides a graphical user interface (GUI) for the manual segmentation and annotation of specific features of interest. These manual annotations are then used as input ground-truth data for training a convolutional neural network (CNN) model, which can then be used for automatic segmentation. Ais provides the option of several CNNs so that users can compare their performance on their structures of interest in order to determine the CNN that best suits their needs. Additionally, pre-trained models can be uploaded and shared to an online database.</p><p>Algorithms are also provided to characterize &quot;model interactions&quot; which allows users to define heuristic rules on how the different segmentations interact. For instance, a membrane-adjacent protein can have rules where it must colocalize a certain distance away from a membrane segmentation. Such rules can help reduce false positives; as in the case above, false negatives predicted away from membranes are eliminated.</p><p>The authors then show how Ais can be used for particle picking and subsequent subtomogram averaging and for the segmentation of cellular tomograms for visual analysis. For subtomogram averaging, they used a previously published dataset and compared the averages of their automated picking with the published manual picking. Analysis of cellular tomogram segmentation was primarily visual.</p><p>Strengths:</p><p>CNN-based segmentation of cryo-ET data is a rapidly developing area of research, as it promises substantially faster results than manual segmentation as well as the possibility for higher accuracy. However, this field is still very much in the development and the overall performance of these approaches, even across different algorithms, still leaves much to be desired. In this context, I think Ais is an interesting package, as it aims to provide both new and experienced users with streamlined approaches for manual annotation, access to a number of CNNs, and methods to refine the outputs of CNN models against each other. I think this can be quite useful for users, particularly as these methods develop.</p><p>Weaknesses:</p><p>Whilst overall I am enthusiastic about this manuscript, I still have a number of comments:</p><p>(1) On page 5, paragraph 1, there is a discussion on human judgement of these results. I think a more detailed discussion is required here, as from looking at the figures, I don't know that I agree with the authors' statement that Pix2pix is better. I acknowledge that this is extremely subjective, which is the problem. I think that a manual segmentation should also be shown in a figure so that the reader has a better way to gauge the performance of the automated segmentation.</p></disp-quote><p>Please see the answer to Reviewer 1âs comment #3.</p><disp-quote content-type="editor-comment"><p>(2) On page 7, the authors mention terms such as &quot;emit&quot; and &quot;absorb&quot; but never properly define them, such that I feel like I'm guessing at their meaning. Precise definitions of these terms should be provided.</p></disp-quote><p>We have expanded this paragraph to include some more detail:</p><p>âBesides these specific interactions between two models, the software also enables pitching multiple models against one another in what we call âmodel competitionâ. Models can be set to âemitâ and/or âabsorbâ competition from other models. Here, to emit competition means that a modelâs prediction value is included in a list of competing models. To absorb competition means that a modelâs prediction value will be compared to all values in that list, and that this modelâs prediction value for any pixel will be set to zero if any of the competing modelsâ prediction value is higher. On a pixel-by-pixel basis, all models that absorb competition are thus suppressed whenever their prediction value for a pixel is lower than that of any of the emitting models.â</p><disp-quote content-type="editor-comment"><p>(3) For Figure 3, it's unclear if the parent models shown (particularly the carbon model) are binary or not.</p><p>The figure looks to be grey values, which would imply that it's the visualization of some prediction score. If so, how is this thresholded? This can also be made clearer in the text.</p></disp-quote><p>The figures show the grayscale output of the parent model, but this grayscale output is thresholded to produce a binary mask that is used in an interaction. We have edited the text to include a mention of thresholding at a user-specified threshold value:</p><p>âThese interactions are implemented as follows: first, a binary mask is generated by thresholding the parent modelâs predictions using a user-specified threshold value. Next, the mask is then dilated using a circular kernel with a radius ð, a parameter that we call the interaction radius. Finally, the child modelâs prediction values are multiplied with this mask.â</p><p>To avoid confusion, we have also edited the figure to show the binary masks rather than the grayscale segmentations.</p><disp-quote content-type="editor-comment"><p>(4) Figure 3D was produced in ChimeraX using the hide dust function. I think some discussion on the nature of this &quot;dust&quot; is in order, e.g. how much is there and how large does it need to be to be considered dust? Given that these segmentations can be used for particle picking, this seems like it may be a major contributor to false positives.</p></disp-quote><p>âDustâ in segmentations is essentially unavoidable; it would require a perfect model that does not produce any false positives. However, when models are sufficiently accurate, the volume of false positives is typically smaller than that of the structures that were intended to be segmented. In these cases, discarding particles based on size is a practical way of filtering the segmentation results. Since it is difficult to generalize when to consider something âdustâ we decided to include this additional text in the Methodâs section rather than in the main text:</p><p>ââ¦ with the use of the âhide dustâ function (the same settings were used for each panel, different settings used for each feature).</p><p>This âdustâ corresponds to small (in comparison to the segmented structures of interest) volumes of false positive segmentations, which are present in the data due to imperfections in the used models. The rate and volume of false positives can be reduced either by improving the models (typically by including more examples of the images of what would be false negatives or positives in the training data) or, if the dust particles are indeed smaller than the structures of interest, they can simply be discarded by filtering particles based on their volume, as applied here. In particle picking a âminimum particle volumeâ is specified â particles with a smaller volume are considered âdustâ.</p><p>In combination with the newly included text about the method of converting volumes into lists of coordinates (see Reviewer 1âs comment #6).</p><p>âThird, a watershed transform is applied to the resulting volume, so that the sets of pixels closest to any local maximum in the distance transformed volume are assigned to one group. Fourth, groups that are smaller than a user-specified minimum volume are discardedâ¦â</p><p>We think it should now be clearer that (some form of) discarding âdustâ is a step that is typically included in the particle picking process.</p><disp-quote content-type="editor-comment"><p>(5) Page 9 contains the following sentence: &quot;After selecting these values, we then launched a batch particle picking process to determine lists of particle coordinates based on the segmented volumes.&quot; Given how important this is, I feel like this requires significant description, e.g. how are densities thresholded, how are centers determined, and what if there are overlapping segmentations?</p></disp-quote><p>Please see the response to Reviewer 1âs comment #6.</p><disp-quote content-type="editor-comment"><p>(6) The FSC shown in Figure S6 for the auto-picked maps is concerning. First, a horizontal line at FSC = 0 should be added. It seems that starting at a frequency of ~0.045, the FSC of the autopicked map increases above zero and stays there. Since this is not present in the FSC of the manually picked averages, this suggests the automatic approach is also finding some sort of consistent features. This needs to be discussed.</p></disp-quote><p>Thank you for pointing this out. Awkwardly, this was due to a mistake made while formatting the figure. In the two separate original plots, the Y axes had slightly different ranges, but this was missed when they were combined to prepare the joint supplementary figure. As a result, the FSC values for the autopicked half maps are displayed incorrectly. The original separate plots are shown below to illustrate the discrepancy:</p><fig id="sa4fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-sa4-fig1-v1.tif"/></fig><p>The corrected figure is Figure S9 in the manuscript. The values of 44 Ã and 46 Ã were not determined from the graph and remain unchanged.</p><disp-quote content-type="editor-comment"><p>(7) Page 11 contains the statement &quot;the segmented volumes found no immediately apparent false positive predictions of these pores&quot;. This is quite subjective and I don't know that I agree with this assessment. Unless the authors decide to quantify this through subtomogram classification, I don't think this statement is appropriate.</p></disp-quote><p>We originally included this statement and the supplementary figure because we wanted to show another example of automated picking, this time in the more crowded environment of the cell. We do agree that it requires better substantiation, but also think that the demonstration of automated picking of the antibody platforms and IgG3-C1 complexes for subtomogram averaging suffices to demonstrate Aisâ picking capabilities. Since the supplementary information includes an example of picked coordinates rendered in the Ais 3D viewer (Figure S7) that also used the pore dataset, we still include the supplementary figure (S10) but have edited the statement to read:</p><p>âMoreover, we could identify the molecular pores within the DMV, and pick sets of particles that might be suitable for use in subtomogram averaging (see Fig. S11).â</p><p>We have also expanded the text that accompanies the supplementary figure to emphasize that results from automated picking are likely to require further curation, e.g. by classification in subtomogram averaging, and that the selection of particles is highly dependent on the thresholds used in the conversion from volumes to lists of coordinates.</p><disp-quote content-type="editor-comment"><p>(8) In the methods, the authors note that particle picking is explained in detail in the online documentation. Given that this is a key feature of this software, such an explanation should be in the manuscript.</p></disp-quote><p>Please see the response to Reviewer 1âs comment #6.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations:</bold></p><p>(9) The word &quot;model&quot; seems to be used quite ambiguously. Sometimes it seems to refer to the manual segmentations, the CNN architectures, the trained models, or the output predictions. More precision in this language would greatly improve the readability of the manuscript.</p></disp-quote><p>This was indeed quite ambiguous, especially in the introduction. We have edited the text to be clearer on these differences. The word âmodelâ is now only used to refer to trained CNNs that segment a particular feature (as in âmembrane modelâ or âmodel interactionsâ). Where we used terms such as â3D modelsâ to describe scenes rendered in 3D, we now use â3D visualizationsâ or similar terms. Where we previously used the term âmodelsâ to refer to CNN architectures, we now use terms such as âneural network architecturesâ or âarchitectureâ. Some examples:</p><p>â¦ with which one can automatically segment the same or any other dataset â¦</p><p>Moreover, since Pix2pix is a relatively large network, â¦</p><p>â¦ to generate a 3D visualization of ten distinct cellular â¦</p><p>â¦ with the use of the same training datasets for all network architectures â¦</p><disp-quote content-type="editor-comment"><p>In Figure 1, the text in panels D and E is illegible.</p></disp-quote><p>We have edited the figure to show the text more clearly (the previous images were unedited screenshots of the website).</p><disp-quote content-type="editor-comment"><p>(10) Prior to the section on model interactions, I was under the impression that all annotations were performed simultaneously. I think it could be clarified that models are generated per annotation type.</p></disp-quote><p>Multiple different features can be annotated (i.e. drawn by hand by the user) at the same time, but each trained CNN only segments one feature. CNNs that output segmentations for multiple features can be implemented straightforwardly, but this introduces the need to provide training data where for every grayscale image, every feature is annotated. This can make preparing the training data much more cumbersome. Reusability of the models is also hampered. We now mention the separateness of the networks explicitly in the introduction:</p><p>âMultiple features, such as membranes, microtubules, ribosomes, and phosphate crystals, can be segmented and edited at the same time across multiple datasets (even hundreds). These annotations are then extracted and used as ground truth labels upon which to condition multiple separate neural networks, â¦â</p><disp-quote content-type="editor-comment"><p>(11) On page 6, there is the text &quot;some features are assigned a high segmentation value by multiple of the networks, leading to ambiguity in the results&quot;. Do they mean some false features?</p></disp-quote><p>To avoid ambiguity of the word âfeaturesâ, we have edited the sentence to read:</p><p>ââ¦ some parts of the image are assigned a high segmentation value by multiple of the networks, leading to false classifications and ambiguity in the results.â</p><disp-quote content-type="editor-comment"><p>(12) Figures 2 and 3 would be easier to follow if they had consistent coloring.</p></disp-quote><p>We have changed the colouring in Figure 2 to match that of Figure 3 better:</p><disp-quote content-type="editor-comment"><p>(13) For Figure 3D, I'm confused as to why the authors showed results from the tomogram in Figure 2B. It seems like the tomogram in Figure 3C would be a more obvious choice, as we would be able to see how the 2D slices look in 3D. This would also make it easier to see the effect of interactions on false negatives. Also, since the orientation of the tomogram in 2B is quite different than that shown in 3D, it's a bit difficult to relate the two.</p></disp-quote><p>We chose to show this dataset because it exemplifies the effects of both model competition and model interactions better than the tomogram in Figure 3C. See Figure 3D and Author response image 2 for a comparison:</p><fig id="sa4fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98552-sa4-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>(14) I'm confused as to why the tomographic data shown in Figures 4D, E, and F are black on white while all other cryo-ET data is shown as white on black.</p></disp-quote><p>The images in Figure 4DEF are now inverted.</p><disp-quote content-type="editor-comment"><p>(15) For Figure 5, there needs to be better visual cueing to emphasize which tomographic slices are related to the segmentations in Panels A and B.</p></disp-quote><p>We have edited the figure to show more clearly which grayscale image corresponds to which segmentation:</p><disp-quote content-type="editor-comment"><p>(16) I don't understand what I should be taking away from Figures S1 and S2. There are a lot of boxes around membrane areas and I don't know what these boxes mean.</p></disp-quote><p>We have added a more descriptive text to these figures. The boxes are placed by the user to select areas of the image that will be sampled when saving training datasets.</p></body></sub-article></article>