<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">104041</article-id><article-id pub-id-type="doi">10.7554/eLife.104041</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.104041.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Preparatory attentional templates in prefrontal and sensory cortex encode target-associated information</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Zhou</surname><given-names>Zhiheng</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0687-2398</contrib-id><email>zhou@sicnu.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Geng</surname><given-names>Joy</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5663-9637</contrib-id><email>jgeng@ucdavis.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/043dxc061</institution-id><institution>College of Psychology, Sichuan Normal University</institution></institution-wrap><addr-line><named-content content-type="city">Chengdu</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05rrcem69</institution-id><institution>Center for Mind and Brain, University of California, Davis</institution></institution-wrap><addr-line><named-content content-type="city">Davis</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05rrcem69</institution-id><institution>Department of Psychology, University of California, Davis</institution></institution-wrap><addr-line><named-content content-type="city">Davis</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Meng</surname><given-names>Ming</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008s83205</institution-id><institution>University of Alabama at Birmingham</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Stanford University, Howard Hughes Medical Institute</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>08</day><month>09</month><year>2025</year></pub-date><volume>14</volume><elocation-id>RP104041</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-10-25"><day>25</day><month>10</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-10-29"><day>29</day><month>10</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.07.01.601634"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-20"><day>20</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.104041.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-08-19"><day>19</day><month>08</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.104041.2"/></event></pub-history><permissions><copyright-statement>© 2025, Zhou and Geng</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Zhou and Geng</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-104041-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-104041-figures-v1.pdf"/><abstract><p>Visual search relies on the ability to use information about the target in working memory to guide attention and make target-match decisions. The ‘attentional’ or ‘target’ template is thought to be encoded within an inferior frontal junction (IFJ)-visual attentional network. While this template typically contains veridical target features, behavioral studies have shown that target-associated information, such as statistically co-occurring object pairs, can also guide attention. However, preparatory activation of associated information within the IFJ-visual attentional network has never been demonstrated. We used fMRI and multivariate pattern analysis to test if target-associated information is explicitly represented in advance of visual search. Participants learned four face-scene category pairings and then completed a cued visual search task for a face. Face information was decoded in the fusiform face area, superior parietal lobule, and dorsolateral prefrontal cortex during the cue period, but was absent during the delay period. In contrast, associated scene information was decoded in the ventrolateral prefrontal cortex during the cue period, and most importantly, in the IFJ and the parahippocampal place area during the delay period. These results are a novel demonstration of how target-associated information from memory can supplant the veridical target in the brain’s ‘target template’ in anticipation of difficult visual search.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual search</kwd><kwd>target template</kwd><kwd>associative learning</kwd><kwd>visual working memory</kwd><kwd>attentional network</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01-MH113855</award-id><principal-award-recipient><name><surname>Geng</surname><given-names>Joy</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002338</institution-id><institution>Ministry of Education of the People's Republic of China</institution></institution-wrap></funding-source><award-id>24XJC190010</award-id><principal-award-recipient><name><surname>Zhou</surname><given-names>Zhiheng</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Sichuan Psychological Society</institution></institution-wrap></funding-source><award-id>SCSXLXH2023001</award-id><principal-award-recipient><name><surname>Zhou</surname><given-names>Zhiheng</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>fMRI decoding reveals that the inferior frontal junction-visual network reinstates target-related information as a proxy for target localization before visual search.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>We engage in visual search repeatedly throughout the day whenever we look for something, such as a friend in a café, a phone on the table, or ingredients at a store. When visual search is efficient, behavior feels frictionless, but failures result in frustration or even the derailment of behavioral goals. The canonical mechanism supporting visual search is thought to involve a network of frontal regions that maintain target information in a working memory ‘template’ that is used to adjust gain in sensory neurons that encode matching features (<xref ref-type="bibr" rid="bib21">Chelazzi et al., 1993</xref>; <xref ref-type="bibr" rid="bib31">Desimone and Duncan, 1995</xref>; <xref ref-type="bibr" rid="bib65">Liu et al., 2007</xref>; <xref ref-type="bibr" rid="bib81">O’Connor et al., 2002</xref>; <xref ref-type="bibr" rid="bib82">O’Craven et al., 1999</xref>; <xref ref-type="bibr" rid="bib91">Reynolds and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib95">Serences and Boynton, 2007</xref>; <xref ref-type="bibr" rid="bib101">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="bib104">Sylvester et al., 2007</xref>; <xref ref-type="bibr" rid="bib107">Treue and Martínez Trujillo, 1999</xref>). Preparatory changes in sensory gain lead to increases in baseline activity that enhance subsequent stimulus-evoked responses by target-matching stimuli (<xref ref-type="bibr" rid="bib13">Boettcher et al., 2020</xref>; <xref ref-type="bibr" rid="bib21">Chelazzi et al., 1993</xref>; <xref ref-type="bibr" rid="bib31">Desimone and Duncan, 1995</xref>; <xref ref-type="bibr" rid="bib41">Gayet and Peelen, 2022</xref>; <xref ref-type="bibr" rid="bib81">O’Connor et al., 2002</xref>; <xref ref-type="bibr" rid="bib87">Peelen and Kastner, 2011</xref>; <xref ref-type="bibr" rid="bib95">Serences and Boynton, 2007</xref>; <xref ref-type="bibr" rid="bib101">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="bib104">Sylvester et al., 2007</xref>; <xref ref-type="bibr" rid="bib110">van Loon et al., 2018</xref>; <xref ref-type="bibr" rid="bib115">Witkowski and Geng, 2023</xref>). In this way, information in the target template is thought to dictate sensory processing priorities that impact the ability to perceive and act on behaviorally relevant objects. Despite its importance for behavior, the bulk of work on target templates in the brain has focused on target veridical features (e.g., the color ‘red’ or small round shapes when looking for an apple). Few studies have examined how non-target information might also be held in the template and used as a proxy for the target. The current study tests if objects that are statistically associated with the target are ever prioritized in the target template in conjunction with, or even instead of, the actual target in anticipation of visual search (<xref ref-type="bibr" rid="bib12">Boettcher et al., 2018</xref>; <xref ref-type="bibr" rid="bib49">Helbing et al., 2022</xref>; <xref ref-type="bibr" rid="bib69">Mack and Eckstein, 2011</xref>; <xref ref-type="bibr" rid="bib119">Yu et al., 2023</xref>; <xref ref-type="bibr" rid="bib121">Zhou and Geng, 2024</xref>).</p><p>Information in the target template is considered a source of goal-directed sensory-motor control (<xref ref-type="bibr" rid="bib78">Nee, 2021</xref>). Recently, the posterior end of the inferior frontal sulcus and its junction with the precentral sulcus has been identified as holding object and feature-based information consistent with the definition of target template (<xref ref-type="bibr" rid="bib5">Baldauf and Desimone, 2014</xref>; <xref ref-type="bibr" rid="bib8">Bedini et al., 2023</xref>; <xref ref-type="bibr" rid="bib10">Bichot et al., 2015</xref>; <xref ref-type="bibr" rid="bib43">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="bib84">O’Reilly, 2010</xref>; <xref ref-type="bibr" rid="bib98">Soyuhos and Baldauf, 2023</xref>; <xref ref-type="bibr" rid="bib115">Witkowski and Geng, 2023</xref>; <xref ref-type="bibr" rid="bib120">Zanto et al., 2010</xref>). These regions include a posterior-to-anterior cortical organization labeled as premotor eye field (PEF), posterior inferior frontal junction (IFJp), and anterior inferior frontal junction (IFJa) in the Human Connectome Project Multi-Modal Parcellation (HCP-MMP1; <xref ref-type="bibr" rid="bib8">Bedini et al., 2023</xref>; <xref ref-type="bibr" rid="bib42">Glasser et al., 2016</xref>). For example, <xref ref-type="bibr" rid="bib5">Baldauf and Desimone, 2014</xref> found that attention to superimposed faces versus houses resulted in induced gamma synchrony between the IFJ and fusiform face area (FFA) or parahippocampal place area (PPA), respectively; this was coupled with selective sensory enhancements in stimulus processing. A likely homologue of this region has also been identified in non-human primates in the ventral prearcuate region (VPA) of the prefrontal cortex (<xref ref-type="bibr" rid="bib10">Bichot et al., 2015</xref>; <xref ref-type="bibr" rid="bib11">Bichot et al., 2019</xref>). Consistent with its role as a source for feature-based attention, inactivation of VPA resulted in poorer visual search performance and reduced V4 visual responses to the target. These results provide evidence for a causal link between the target representation in IFJ and sensory biases in V4 that facilitate visual search behavior. Together, the data suggest that regions encompassed by IFJ maintain target information and provide feedback signals to visual neurons encoding those same features. The role IFJ plays for feature-based attention appears to be analogous to that of the frontal eye fields (FEFs) for spatial attention (<xref ref-type="bibr" rid="bib10">Bichot et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Corbetta et al., 2008</xref>; <xref ref-type="bibr" rid="bib72">Moore et al., 2003</xref>; <xref ref-type="bibr" rid="bib93">Ruff et al., 2006</xref>; <xref ref-type="bibr" rid="bib106">Thompson et al., 2005</xref>).</p><p>In addition to IFJ, which appears to encode the source of target information, visual search involves a number of other cognitive control functions that recruit other frontal and parietal regions. In particular, the dorsolateral and ventrolateral prefrontal cortices (dLPFC and vLPFC) are involved in proactive and reactive cognitive control, including the maintenance and manipulation of goal-relevant information across stimulus types, and flexibly updating those goal-based representations when unexpected errors are encountered (<xref ref-type="bibr" rid="bib4">Badre and Nee, 2018</xref>; <xref ref-type="bibr" rid="bib9">Bettencourt and Xu, 2016</xref>; <xref ref-type="bibr" rid="bib16">Braver et al., 2009</xref>; <xref ref-type="bibr" rid="bib24">Christophel et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Christophel et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Emrich et al., 2013</xref>; <xref ref-type="bibr" rid="bib35">Ester et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Finn et al., 2019</xref>; <xref ref-type="bibr" rid="bib60">Lee et al., 2013</xref>; <xref ref-type="bibr" rid="bib64">Liu et al., 2003</xref>; <xref ref-type="bibr" rid="bib66">Long and Kuhl, 2018</xref>; <xref ref-type="bibr" rid="bib77">Nee and D’Esposito, 2017</xref>; <xref ref-type="bibr" rid="bib97">Soon et al., 2013</xref>; <xref ref-type="bibr" rid="bib102">Stokes et al., 2013</xref>). Working memory representations in lateral prefrontal and parietal regions are also engaged in cognitive control computations that are task non-specific but essential to task functioning (<xref ref-type="bibr" rid="bib9">Bettencourt and Xu, 2016</xref>; <xref ref-type="bibr" rid="bib59">Kwak and Curtis, 2022</xref>; <xref ref-type="bibr" rid="bib66">Long and Kuhl, 2018</xref>; <xref ref-type="bibr" rid="bib86">Panichello and Buschman, 2021</xref>). Thus, a network of frontoparietal areas is expected to share prospective encoding of target information, and we include these regions of interest (ROIs) in our analyses, defined by the HCP-MMP1 (<xref ref-type="bibr" rid="bib42">Glasser et al., 2016</xref>) and the 17-network atlases (<xref ref-type="bibr" rid="bib94">Schaefer et al., 2018</xref>; see ‘Materials and methods).</p><p>The ability to flexibly update goals is an important aspect of visual search because finding a target requires an iterative cycle of choosing an object based on the current template to attend to or look at, and then deciding if it is the correct target or not (<xref ref-type="bibr" rid="bib2">Alexander and Zelinsky, 2011</xref>; <xref ref-type="bibr" rid="bib50">Hout and Goldinger, 2014</xref>; <xref ref-type="bibr" rid="bib70">Malcolm and Henderson, 2010</xref>; <xref ref-type="bibr" rid="bib118">Yu et al., 2022</xref>; <xref ref-type="bibr" rid="bib119">Yu et al., 2023</xref>). When it is the target, search is terminated, but when it is not, attention is guided to a new potential target and the cycle continues. Thus, targets must be maintained in memory over time, while control mechanisms flexibly adjust goals based on sensory outcomes (<xref ref-type="bibr" rid="bib3">Badre and Wagner, 2007</xref>; <xref ref-type="bibr" rid="bib33">Egner, 2023</xref>; <xref ref-type="bibr" rid="bib58">Kurtin et al., 2023</xref>; <xref ref-type="bibr" rid="bib89">Poskanzer and Aly, 2023</xref>; <xref ref-type="bibr" rid="bib92">Rossi et al., 2009</xref>; <xref ref-type="bibr" rid="bib108">Tünnermann et al., 2021</xref>). Target information in multiple ROIs may evolve over time for these different computations, but we expect IFJ to encode the target template prospectively to facilitate attentional guidance and target decisions.</p><p>In contrast to the relatively straightforward idea that veridical target features are held in working memory and used to bias sensory processing, there is substantial evidence that during naturalistic scene viewing, attention is often guided by objects that are associated with the target but not the target itself (<xref ref-type="bibr" rid="bib12">Boettcher et al., 2018</xref>; <xref ref-type="bibr" rid="bib49">Helbing et al., 2022</xref>; <xref ref-type="bibr" rid="bib69">Mack and Eckstein, 2011</xref>; <xref ref-type="bibr" rid="bib121">Zhou and Geng, 2024</xref>). For example, people are faster to find targets (e.g., sand toys) that are near associated ‘anchor’ objects (e.g., sandbox), because the anchor serves as a large and more easily perceived spatial predictor of the target’s location (<xref ref-type="bibr" rid="bib12">Boettcher et al., 2018</xref>; <xref ref-type="bibr" rid="bib49">Helbing et al., 2022</xref>; <xref ref-type="bibr" rid="bib109">Turini and Võ, 2022</xref>). Using such proxy information is beneficial because it reduces the space over which the target must be searched for (<xref ref-type="bibr" rid="bib12">Boettcher et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Castelhano and Krzyś, 2020</xref>; <xref ref-type="bibr" rid="bib47">Hall and Geng, 2024</xref>; <xref ref-type="bibr" rid="bib52">Josephs et al., 2016</xref>). A recent study found that target-associated anchor objects implied by a visual scene could be decoded in parietal and visual cortex before the target appeared (<xref ref-type="bibr" rid="bib62">Lerebourg et al., 2024</xref>), supporting the notion that objects associated with the target are used to bias sensory processing and guide attention (<xref ref-type="bibr" rid="bib41">Gayet and Peelen, 2022</xref>; <xref ref-type="bibr" rid="bib88">Peelen et al., 2024</xref>; <xref ref-type="bibr" rid="bib119">Yu et al., 2023</xref>). Despite these studies, it remains unknown if target-associated information in memory is encoded in attentional control networks during the preparatory period at all, and if it is, if it occurs along with target information or instead of target information.</p><p>In this study, we address this gap in knowledge by asking if associated information in memory can be found in the IFJ, frontoparietal attention network, and category-selective sensory cortex during the delay period in anticipation of visual search. Such an outcome would suggest that information associated with the target is prioritized in preparation for visual search, potentially even at the expense of the target itself. We base our paradigm on behavioral work showing that participants can learn new face-scene category associations and use this information to guide target search, but only when target-distractor discrimination is difficult (<xref ref-type="bibr" rid="bib121">Zhou and Geng, 2024</xref>). In this previous study, we trained participants on four face-scene associations in advance of a cued-visual search task. During visual search, a single face was cued to indicate the target on each trial. Next, a search display appeared with two lateralized faces superimposed on scenes. The target appeared on its associated scene on 75% of ‘scene-valid’ trials. Different sub-experiments used a variety of scene-invalid trials, including ones in which the associated scene was never present, was co-located with the distractor only, or appeared with both the target and the distractor. Here, we only use one of the ‘scene-invalid’ conditions because the search trials are not of primary interest. The main purpose of the study is to determine if the face cue activates a memory representation of the associated scene during the delay period in attentional control regions that encode the target template, namely IFJ. This outcome would be novel evidence for the hypothesis that the associated scene is represented within the target template during the delay period for the purpose of facilitating visual search. Such a result would be consistent with studies in humans and non-human primates showing that preparatory activation of target information in prefrontal and visual cortex is used to guide attention and make target decisions during visual search (<xref ref-type="bibr" rid="bib5">Baldauf and Desimone, 2014</xref>; <xref ref-type="bibr" rid="bib10">Bichot et al., 2015</xref>; <xref ref-type="bibr" rid="bib21">Chelazzi et al., 1993</xref>; <xref ref-type="bibr" rid="bib44">Gregoriou et al., 2014</xref>; <xref ref-type="bibr" rid="bib55">Kastner et al., 1999</xref>; <xref ref-type="bibr" rid="bib98">Soyuhos and Baldauf, 2023</xref>; <xref ref-type="bibr" rid="bib115">Witkowski and Geng, 2023</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavioral tasks</title><p>All participants (N=26) were first trained outside the scanner on four face-scene category associations (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). First, they saw the displays introducing a face with a scene, for example, the text ‘Emma is a lifeguard and can be found on the beach’ presented with an image of a face, ‘Emma’, superimposed on a beach scene (see ‘Materials and methods’). Second, participants were given a match-nonmatch decision task in which they indicated if a specific face-scene pair was a previously learned pair (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). All participants reached a decision accuracy greater than 90%, with mean accuracy = 97.4% ± 2.7% (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Associative learning task.</title><p>(<bold>A</bold>) The four target face stimuli and their associated scenes. (<bold>B</bold>) Associative learning task to test memory for face-scene pairs. Participants viewed a series of face-scene pairs and made a judgment about whether the face and scene were matched or not. (<bold>C</bold>) Memory performance for both match and nonmatch conditions was high, suggesting a strong association was formed before the face search task. Error bars refer to 95% CIs. Facial images from The Chicago Face Database (<xref ref-type="bibr" rid="bib68">Ma et al., 2015</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104041-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>The four target face stimuli and their three distractor face counterparts.</title><p>Four target faces were approved for release to the public following the Chicago Face Database (CFD; <ext-link ext-link-type="uri" xlink:href="https://www.chicagofaces.org/">https://www.chicagofaces.org/</ext-link>) copyright rules. The image file names of the twelve distractor faces are listed for reference. Facial images from The Chicago Face Database (<xref ref-type="bibr" rid="bib68">Ma et al., 2015</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104041-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>The four scene categories used in the search task.</title><p>Each category consisted of 16 exemplars.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104041-fig1-figsupp2-v1.tif"/></fig></fig-group><p>During the scan session, participants were given a cued visual search task (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Each trial began with a <italic>search cue</italic> of the target face for 1 second, followed by an 8-second blank <italic>search delay</italic> period, and then a search display for 0.25 second. Trials ended with an 8.75-second inter-trial interval to allow the BOLD response to return to baseline. The delay period was the temporal interval of greatest interest. While the cue-evoked stimulus response should contain decodable face information, any scene information during the delay period must reflect memory-evoked representations because no scene information was visually presented with the face cue. Search trials were composed of two face stimuli superimposed on scene images. The target appeared on the associated scene on 75% of trials (scene-valid) and on one of the three other scenes on 25% of trials (scene-invalid). Thus, the scene was probabilistically predictive of the target’s location but not deterministic. The distractor face was a race-gender match. We previously found that this distractor condition was sufficiently difficult to elicit the use of scene information for guiding attention (<xref ref-type="bibr" rid="bib121">Zhou and Geng, 2024</xref>). After the main search task, participants completed a face and scene 1-back task (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Data from this task were used as a benchmark test set for neural activation patterns trained in response to faces and scenes from the main visual search task.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>fMRI tasks and analysis procedure.</title><p>(<bold>A</bold>) Face search task. Each trial started with a 1-second <italic>search cue</italic> indicating the target face for that trial. This was followed by an 8-second blank <italic>search delay</italic> period, and then the search display for 0.25 second. Participants pressed a button to indicate the location of the target on the left or right. (<bold>B</bold>) Illustration of the separate 1-back task used for cross-task classifier testing. Consistent with the main visual search task, the image was presented for 1 second followed by an 8-second <italic>delay</italic> period. (<bold>C</bold>) Classification scheme. Classifiers were trained on the neural response patterns from the <italic>search cue</italic> face stimulus and <italic>delay</italic> periods from the visual search task; the classifier was tested on face or scene <italic>sample</italic> stimulus and <italic>delay</italic> period from the 1-back task. (<bold>D</bold>) Visualization of the twelve functional regions of interest (ROIs) on the cortical surface of a representative participant. All ROIs were defined in an individual’s native space. Facial images from The Chicago Face Database (<xref ref-type="bibr" rid="bib68">Ma et al., 2015</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104041-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Illustration of overlap between the HCP-MMP1 atlas (<xref ref-type="bibr" rid="bib42">Glasser et al., 2016</xref>) and the Schaefer resting-state 17-network atlas (<xref ref-type="bibr" rid="bib94">Schaefer et al., 2018</xref>) in the inferior frontal junction regions.</title><p>Black outlines correspond to the PEF, IFJp, and IFJa from the HCP-MMP1 atlas. The red ROI corresponds to the 17-network atlas precentral label from the dorsal attention network in the left hemisphere and the ventral attention network in the right.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104041-fig2-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Cue-evoked face information is translated into a preparatory scene template prior to visual search</title><p>To test if face and/or scene information was present in the IFJ, frontoparietal attention networks, and category-selective visual cortex during the cue and delay periods prior to visual search, we adopted a cross-classification multivariate pattern analysis (MVPA) approach (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Separate classifiers were created for the cue and delay periods, but both classifiers were trained on data from the main search task and tested on data from the separate 1-back task using trials with the same face and scene stimuli. Analyses were focused on 12 ROIs within the frontoparietal network and the sensory cortex that are known to be involved in representing task structure, target templates for feature-based attention, and category-selective visual processing (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Frontoparietal ROIs were defined by the HCP-MMP1 (<xref ref-type="bibr" rid="bib42">Glasser et al., 2016</xref>) and the 17-network atlases (<xref ref-type="bibr" rid="bib94">Schaefer et al., 2018</xref>). Category-selective visual regions for faces (FFA) and scenes (PPA) in the ventral visual pathway were identified for each participant using the independent functional localizer task (<xref ref-type="bibr" rid="bib100">Stigliani et al., 2015</xref>).</p><p>The cross-classification procedure for the cue stimulus involved training the classifier on activation patterns in response to the <italic>search cue</italic> in the face search task, and testing on activation patterns in response to <italic>face</italic> and <italic>scene</italic> stimulus <italic>samples</italic> seen during an independent 1-back task (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). We expected the <italic>search cue</italic> classifier to successfully decode face stimuli but not necessarily scene stimuli since this time period should primarily reflect the stimulus-evoked sensory processing of the face cue. The <italic>delay</italic> period cross-classification procedure involved training the classifier on patterns of activation during the 8-second <italic>search delay</italic> period after the face <italic>search cue</italic> had disappeared and testing the classifier on activation patterns from the 8-second <italic>delay</italic> following the <italic>face</italic> or <italic>scene</italic> sample stimulus from the 1-back task (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Here, we hypothesized that we would see decoding of scene information in the IFJ-PPA network either in conjunction with face information in IFJ, or even in place of it. Critically, the decodable information during the delay period for the <italic>face</italic> could be contaminated by the stimulus-driven activity during the cue. This is because the temporal separation between the face and the delay period is too short for the BOLD response to the face cue to completely resolve. However, this is not a concern for <italic>scene</italic> decoding because the scene is never visually shown during the cue period. Thus, the ability to decode scene information during the delay period has to reflect memory-evoked representations in response to seeing the specific face cue.</p><p>Application of this approach to the <italic>search cue</italic> period (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) revealed significant decoding of face information in bilateral FFA (left p=0.0074, uncorrected; right p&lt;0.005). As expected, each of the four face cues could be distinguished from each other in FFA. Faces were also significantly decoded in bilateral superior parietal lobule (SPL; left p=0.018; right p&lt;0.005), right dLPFC (p&lt;0.005), and left IFJp (p=0.012, uncorrected), consistent with the storage of face stimuli in working memory for task-based control. In contrast, scene information was only decoded in bilateral vLPFC (left p=0.0066, uncorrected; right p=0.023); there was no significant decoding of scene information in the scene-selective regions PPA (left p=0.12; right p=0.51). Decoding in vLPFC is important and consistent with the retrieval of associated-scene information from long-term memory in response to the face cue. In addition to the ROI results, we also conducted a whole-brain searchlight using a cluster-corrected threshold of <italic>p</italic>TFCE&lt;0.005 to identify additional areas that may have encoded face or scene information during the <italic>search cue</italic> period. This revealed additional significant clusters where the face cue could be decoded in the left dLPFC (<xref ref-type="fig" rid="fig3">Figure 3B</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). These results indicate that the <italic>search cue</italic> period is dominated by face information in a widespread network of sensory and frontoparietal regions, with limited scene information in vLPFC, related to the cognitive control of memory (<xref ref-type="bibr" rid="bib3">Badre and Wagner, 2007</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Decoding of face and scene information during the <italic>search cue</italic> period.</title><p>(<bold>A</bold>) Evidence of face information in a priori defined regions of interest (ROIs). Greater than chance-level classification accuracies were found in the dorsolateral prefrontal cortices (dLPFC), superior parietal lobule (SPL), and fusiform face area (FFA). Evidence for scene information was found in ventrolateral prefrontal cortices (vLPFC). N=26 participants. †p&lt;0.05 uncorrected, *p&lt;0.05, **p&lt;0.005. (<bold>B, C</bold>) Significant brain regions revealed by a whole-brain searchlight procedure with information about the face cue (<bold>B</bold>) or associated scene (<bold>C</bold>). Note that scene information was never shown during the cue period and therefore decoding of scene information reflects memory-evoked responses to the cued face.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104041-fig3-v1.tif"/></fig><p>In contrast to the <italic>search cue</italic> period, face information during the <italic>search delay</italic> period (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) could only be decoded in the left intraparietal sulcus (IPS; p=0.019, uncorrected), perhaps reflecting working memory storage of the target face (<xref ref-type="bibr" rid="bib9">Bettencourt and Xu, 2016</xref>; <xref ref-type="bibr" rid="bib51">Jeong and Xu, 2016</xref>). However, the associated scene was now reliably decoded in bilateral PPA (left p=0.007; right p&lt;0.005), bilateral PEF (left p=0.03; right p&lt;0.005), and right IFJp (p=0.027, uncorrected). Additional searchlight analyses revealed clusters (<italic>p</italic>TFCE&lt;0.05) in bilateral retrosplenial cortex (<xref ref-type="fig" rid="fig4">Figure 4B</xref> and <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>). The results show robust decoding of scene information in the IFJ-PPA network only during the delay period, suggesting that the face cue was translated into a ‘guiding template’ of the associated scene in anticipation of visual search. Strikingly, the overall pattern shows a double dissociation between face information in both the prefrontal and category-selective visual cortex during the <italic>search cue</italic> period and scene information during the <italic>search delay</italic> period. Importantly, during the <italic>search delay</italic> period, the target face was not represented in the frontal attentional control regions, nor category-selective visual cortex. Whole-brain analyses found that no brain regions showed decoding of both face and scene information. Instead, the associated scene was activated during the delay period in IFJ and category-selective visual cortex, providing strong evidence for the hypothesis that target-associated information is activated from memory in preparation for guiding attention during target search. A final exploratory analysis was conducted by splitting the delay period into first and second half time periods. The results revealed no differences in the decoding of target-associated information in the first versus second half of the delay period (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Decoding of face and scene information during the <italic>search delay</italic> period.</title><p>(<bold>A</bold>) Evidence of face information in a priori defined regions of interest (ROIs) was only found in the left intraparietal sulcus (IPS). However, scene information was decoded in both inferior frontal junction (IFJ) and parahippocampal place area (PPA), reflecting memory-evoked target-associated information in a network that encodes the target template. N=26 participants. †p&lt;0.05 uncorrected, *p&lt;0.05, **p&lt;0.005. (<bold>B, C</bold>) Whole-brain searchlight analyses showed no additional brain regions carried significant information about the face (<bold>B</bold>), but additional scene information was found in the retrosplenial cortex (<bold>C</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104041-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Decoding of face and scene information during the <italic>search cue</italic> (<bold>A</bold>), <italic>search delay1</italic> (<bold>B</bold>)<italic>, and search delay2</italic> (<bold>C</bold>) periods.</title><p>An exploratory analysis was conducted to examine possible differences in decoding of target-associated information in the earlier versus later portions of the delay period. New GLMs were run. They were identical to the main analysis, except that the delay period was split into two 4-second regressors (i.e., the first and second half of the 8-second delay period) in both the <italic>face search task</italic> and the <italic>face and scene 1-back task</italic>. The decoding schemes were identical to the main analysis but were now conducted separately for the <italic>delay1</italic> and <italic>delay2</italic> time periods. The ROI decoding results were similar for target faces during the <italic>search cue</italic> period using this new GLM compared to the main analysis results – this was expected since there were no differences in the model for the <italic>search cue</italic> period. However, decoding of scenes and faces during the <italic>delay1</italic> and <italic>delay2</italic> periods was not reliably found based on the new GLMs. The only significant effect was in LH PPA during <italic>delay2</italic>. The null results could be due to insufficient power when the data are divided, individual differences in when preparatory activation is the strongest, or truly no difference in activation over the delay period. Other methods with higher temporal resolution may be better suited to answer the question of exactly when preparatory activation for target-associated information is initiated and how long it lasts. LH, left hemisphere. N=26 participants. †p&lt;0.05 uncorrected, *p&lt;0.05, **p&lt;0.005.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104041-fig4-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Scene-invalid trials require more cognitive control and delay target localization</title><p>The decoding results from the <italic>search delay</italic> period suggest that scene information is prioritized over that of the target face in preparation for visual search. If true, then scene-invalid trials should produce a prediction error and require reactive cognitive control to switch the search template back to the target face. The behavioral results support this interpretation: participants were more accurate (<italic>t</italic>(25) = 4.96, p&lt;0.001, <italic>d</italic>=0.97) and had shorter RTs (<italic>t</italic>(25) = –3.46, p=0.002, <italic>d</italic>=0.68) on scene-valid compared to scene-invalid trials (<xref ref-type="fig" rid="fig5">Figure 5A</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Behavioral and brain results from the face <italic>search</italic> period.</title><p>(<bold>A</bold>) Behavioral accuracy and RT both showed a scene-validity effect, suggesting scene information was used to guide attention during search. N=26 participants. **p&lt;0.005, ***p&lt;0.001. Error bars refer to 95% CIs. (<bold>B</bold>) Whole-brain group-level univariate contrast results showing significantly greater activations for the scene-invalid than scene-valid conditions are illustrated in blue (cold colors), and the reverse contrast in red (hot colors). (<bold>C</bold>) Contrast betas from the scene-invalid minus scene-valid conditions within each of the a priori regions of interest (ROIs). *p&lt;0.05, **p&lt;0.005.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104041-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Univariate contrast results from the search period shown in a volumetric MNI standard brain.</title><p>(<bold>A</bold>) Scene-invalid minus scene-valid trials. (<bold>B</bold>) Scene-valid minus scene-invalid trials. Both activation maps are shown with correction at the cluster level, <italic>p</italic>TFCE&lt;0.005.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104041-fig5-figsupp1-v1.tif"/></fig></fig-group><p>To test for prediction error and reactive cognitive control, we conducted a univariate analysis contrasting scene-invalid and scene-valid trials. This ‘validity effect’ contrast showed greater activation on scene-invalid trials in bilateral IFJ extending to the middle frontal gyrus, bilateral IPS, insula, and anterior cingulate cortex (ACC; <xref ref-type="fig" rid="fig5">Figure 5B</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> and <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). These regions are similar to those reported in previous studies when cognitive control is needed for task switching, attentional shifting, and resolving cognitive conflict (<xref ref-type="bibr" rid="bib28">Corbetta et al., 2008</xref>; <xref ref-type="bibr" rid="bib32">Dombert et al., 2016</xref>; <xref ref-type="bibr" rid="bib45">Guerin et al., 2012</xref>; <xref ref-type="bibr" rid="bib64">Liu et al., 2003</xref>). In particular, the combined activation of the lateral prefrontal cortex and ACC is indicative of conflict detection in ACC and the updating of goals in the lateral prefrontal cortex necessary for successful goal adaptation (<xref ref-type="bibr" rid="bib33">Egner, 2023</xref>).</p><p>Moreover, looking specifically within our a priori ROIs, we find significant activation in bilateral IFJp, IFJa, vLPFC, and the left dLPFC and IPS during scene-invalid compared to scene-valid trials (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Notably, scene-valid trials did not produce significantly greater activation in any of the ROIs, suggesting that scene-invalid trials required cognitive control to reset attentional priorities when expectations that the scene could be used to find the target were violated. Together, these results are consistent with the idea that scene-invalid trials required the active reinstatement of the target face into working memory and the target template to make a response. These results are consistent with scene-invalid trials producing a prediction error that requires flexible, reactive, cognitive control mechanisms to switch search templates (<xref ref-type="bibr" rid="bib25">Cole et al., 2013</xref>), a process that slows responses and reduces accuracy. Finally, it is worth noting that both the univariate contrast maps and ROI analyses revealed a potential distinction between the posterior IFJ subregion, PEF, and the relatively anterior IFJp and IFJa in feature-based attention and cognitive control (<xref ref-type="bibr" rid="bib73">Muhle-Karbe et al., 2016</xref>; <xref ref-type="bibr" rid="bib98">Soyuhos and Baldauf, 2023</xref>). Consistent with the definition of ROIs based on the two atlases, the IFJp/IFJa from the HCP-MMP1 overlaps with the vLPFC parcel from the 17-network, but the PEF does not overlap with the vLPFC (<xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The canonical mechanism for visual attention involves preparatory changes in sensory gain that increases stimulus-driven responsivity when a matching stimulus appears (<xref ref-type="bibr" rid="bib21">Chelazzi et al., 1993</xref>; <xref ref-type="bibr" rid="bib28">Corbetta et al., 2008</xref>; <xref ref-type="bibr" rid="bib55">Kastner et al., 1999</xref>; <xref ref-type="bibr" rid="bib67">Luck et al., 1997</xref>; <xref ref-type="bibr" rid="bib91">Reynolds and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib95">Serences and Boynton, 2007</xref>; <xref ref-type="bibr" rid="bib112">Vossel et al., 2014</xref>). This sensory modulation is thought to have its source in a network of frontoparietal regions that encode and hold target information as a ‘template’ for sensory modulation (<xref ref-type="bibr" rid="bib21">Chelazzi et al., 1993</xref>; <xref ref-type="bibr" rid="bib36">Ester et al., 2016</xref>; <xref ref-type="bibr" rid="bib37">Esterman and Yantis, 2010</xref>; <xref ref-type="bibr" rid="bib43">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="bib64">Liu et al., 2003</xref>; <xref ref-type="bibr" rid="bib87">Peelen and Kastner, 2011</xref>; <xref ref-type="bibr" rid="bib101">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="bib110">van Loon et al., 2018</xref>). These studies provide a foundation for understanding how working memory representations that are distributed across frontal and parietal regions are used to bias visual processing towards relevant information for achieving behavioral goals. However, most of the work on this topic has focused on target-veridical features. Much less is known about how these mechanisms support adaptive updates to information used to find the target during active visual search.</p><p>In contrast to the neural evidence for veridical templates, there is considerable behavioral evidence showing that preparatory attention is flexible, highly adaptive, and sensitive to changing contexts. Many behavioral studies have shown that scene structure, statistically co-occurring object pairs, and large, stable, predictive ‘anchor objects’ are all used to guide attention and locate smaller objects (<xref ref-type="bibr" rid="bib6">Battistoni et al., 2017</xref>; <xref ref-type="bibr" rid="bib12">Boettcher et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Castelhano and Krzyś, 2020</xref>; <xref ref-type="bibr" rid="bib18">Castelhano et al., 2009</xref>; <xref ref-type="bibr" rid="bib26">Collegio et al., 2019</xref>; <xref ref-type="bibr" rid="bib30">de Lange et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">Gayet and Peelen, 2022</xref>; <xref ref-type="bibr" rid="bib47">Hall and Geng, 2024</xref>; <xref ref-type="bibr" rid="bib49">Helbing et al., 2022</xref>; <xref ref-type="bibr" rid="bib52">Josephs et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Mack and Eckstein, 2011</xref>; <xref ref-type="bibr" rid="bib71">Malcolm and Shomstein, 2015</xref>; <xref ref-type="bibr" rid="bib75">Nah and Geng, 2022</xref>; <xref ref-type="bibr" rid="bib88">Peelen et al., 2024</xref>; <xref ref-type="bibr" rid="bib111">Võ et al., 2019</xref>; <xref ref-type="bibr" rid="bib119">Yu et al., 2023</xref>; <xref ref-type="bibr" rid="bib121">Zhou and Geng, 2024</xref>). For example, in a previous behavioral study, we showed that when the target is hard to find, scene information is used as a proxy in the target template to guide attention toward the likely target location more efficiently (<xref ref-type="bibr" rid="bib121">Zhou and Geng, 2024</xref>).</p><p>Despite the vast behavioral evidence for the use of non-target information in target-guided visual search, there has been little evidence so far for the presence of target-associated information in preparatory ‘target templates’ encoded in frontal-parietal-sensory attentional control networks. One exception is a recent study that found preparatory activation in the lateral occipital complex (LOC) for a target-associated anchor object in advance of target detection (<xref ref-type="bibr" rid="bib62">Lerebourg et al., 2024</xref>). In this study, participants were asked to find either a book or a bowl located on two visually unique tables within two living room scenes. The scene context predicted which table would hold each object. When participants were given a target cue and shown a scene with the table regions masked, the associated table, but not the target, could be decoded from LOC. This study provided compelling evidence that target-associated information is preferentially represented in advance of object detection. However, because the tables were implied by the scene preview, it is somewhat unclear if decoding of the tables reflected the preparatory reinstatement of a target-associated memory or priming of task-relevant information by the scene. There was also no information about template source regions in the frontal cortex, such as the IFJ (<xref ref-type="bibr" rid="bib5">Baldauf and Desimone, 2014</xref>; <xref ref-type="bibr" rid="bib8">Bedini et al., 2023</xref>; <xref ref-type="bibr" rid="bib98">Soyuhos and Baldauf, 2023</xref>).</p><p>Our study builds on this prior literature by teaching participants novel face-scene associations and then using the faces as cues and targets in a visual search task. Scenes were technically task-irrelevant in the search task and were not shown during the cue period. Thus, the presence of any pre-search preparatory scene information in the brain has to be reinstated from memory based on a previously learned association without direct visual prompting. This is important because any scene information represents memory-evoked response to a specific visually presented face cue. First, however, we confirmed that the identity of the face shown as the cue was decoded in bilateral FFA, right dLPFC, bilateral SPL, and left IFJp using a priori ROIs. Decoding in left dLPFC was found in subsequent searchlight analyses, which revealed symmetric clusters of voxels located bilaterally in the middle frontal gyrus extending to the superior frontal gyrus (<xref ref-type="fig" rid="fig3">Figure 3B</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Overall, these results were as expected and showed that the onset of the face cue was represented in sensory and task-based control regions (<xref ref-type="bibr" rid="bib22">Chiu et al., 2011</xref>; <xref ref-type="bibr" rid="bib27">Contreras et al., 2013</xref>; <xref ref-type="bibr" rid="bib46">Guntupalli et al., 2017</xref>; <xref ref-type="bibr" rid="bib51">Jeong and Xu, 2016</xref>; <xref ref-type="bibr" rid="bib66">Long and Kuhl, 2018</xref>; <xref ref-type="bibr" rid="bib79">Nestor et al., 2011</xref>).</p><p>Next, and in contrast to the cue-evoked face representations, the only brain region containing scene category information during the cue period was in vLPFC. In prior studies, scene decoding has been found in vLPFC during perceptual scene categorization tasks (<xref ref-type="bibr" rid="bib53">Jung et al., 2018</xref>; <xref ref-type="bibr" rid="bib54">Jung and Walther, 2021</xref>; <xref ref-type="bibr" rid="bib90">Qin et al., 2011</xref>; <xref ref-type="bibr" rid="bib113">Walther et al., 2009</xref>), but more generally, vLPFC is engaged when cognitive control is required to adjudicate task-based memories (<xref ref-type="bibr" rid="bib3">Badre and Wagner, 2007</xref>; <xref ref-type="bibr" rid="bib33">Egner, 2023</xref>; <xref ref-type="bibr" rid="bib63">Levy and Wagner, 2011</xref>; <xref ref-type="bibr" rid="bib74">Muhle-Karbe et al., 2018</xref>; <xref ref-type="bibr" rid="bib76">Nee and D’Esposito, 2016</xref>; <xref ref-type="bibr" rid="bib80">Neubert et al., 2014</xref>; <xref ref-type="bibr" rid="bib105">Tamber-Rosenau et al., 2018</xref>). This suggests that the face cue evoked the retrieval of scene category information from memory. The most important question is, therefore, whether this cue-related scene information in vLPFC would be translated into a preparatory search template.</p><p>We predicted that, if the associated scene was used as the target template, we should observe successful decoding of scene categories in scene-selective sensory cortex during the delay period, reflecting enhancements in sensory gain, and in parietal and lateral prefrontal cortex, reflecting scene information in working memory and cognitive control. It is well-documented that template information can be decoded in distributed cortical regions, but that each region likely plays a different computational role (<xref ref-type="bibr" rid="bib35">Ester et al., 2015</xref>; <xref ref-type="bibr" rid="bib36">Ester et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="bib61">Lee and Geng, 2017</xref>; <xref ref-type="bibr" rid="bib66">Long and Kuhl, 2018</xref>; <xref ref-type="bibr" rid="bib87">Peelen and Kastner, 2011</xref>; <xref ref-type="bibr" rid="bib97">Soon et al., 2013</xref>; <xref ref-type="bibr" rid="bib101">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="bib110">van Loon et al., 2018</xref>; <xref ref-type="bibr" rid="bib115">Witkowski and Geng, 2023</xref>). Consistent with this, we found that the associated scene category was decoded in IFJ regions and PPA during the delay period. Scene information decoded in vLPFC during the cue period appeared to trigger retrieval of the associated scene, which was turned into a scene template in IFJ and PPA in preparation for target search (<xref ref-type="bibr" rid="bib3">Badre and Wagner, 2007</xref>; <xref ref-type="bibr" rid="bib116">Xu et al., 2022</xref>). Additional searchlight analyses revealed clusters in bilateral retrosplenial cortex that contained scene information (<xref ref-type="fig" rid="fig4">Figure 4C</xref> and <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>).</p><p>Interestingly, no face information was decodable from FFA or IFJ during the search delay period, suggesting it was not actively maintained in the target template. However, there was evidence of decoding in IPS, indicating that face information was still held in working memory (<xref ref-type="bibr" rid="bib9">Bettencourt and Xu, 2016</xref>; <xref ref-type="bibr" rid="bib24">Christophel et al., 2018</xref>; <xref ref-type="bibr" rid="bib35">Ester et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Favila et al., 2018</xref>; <xref ref-type="bibr" rid="bib43">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="bib51">Jeong and Xu, 2016</xref>; <xref ref-type="bibr" rid="bib59">Kwak and Curtis, 2022</xref>; <xref ref-type="bibr" rid="bib83">Olmos-Solis et al., 2021</xref>; <xref ref-type="bibr" rid="bib117">Yu and Postle, 2021</xref>). It is also worth noting that there was no significant decoding of the face or scene information in FEF in the delay period, a region commonly found in top-down attentional control. This may be because there was no spatial prediction given by the cue in our task, as has been suggested by others (<xref ref-type="bibr" rid="bib7">Bedini and Baldauf, 2021</xref>; <xref ref-type="bibr" rid="bib10">Bichot et al., 2015</xref>).</p><p>The consequences of prioritizing associated scenes over the target face were seen in longer RT and more errors on scene-invalid trials. The univariate results suggested that scene-invalid trials triggered a prediction error in ACC that led to an updating of what information should be used to find the target in vLPFC (including IFJp/IFJa) and IPS. However, because activation was more widespread in this analysis, it was not possible to determine if the patterns of activation differed in subregions of IFJ and other areas of vLPFC. This pattern of data supports the notion that preparatory scene activity in the delay period supplanted the target in the ‘target’ template in service of facilitating behavior; when it was inaccurate, however, reactive cognitive control was necessary to update search adaptively (<xref ref-type="bibr" rid="bib14">Botvinick et al., 2004</xref>; <xref ref-type="bibr" rid="bib17">Braver, 2012</xref>; <xref ref-type="bibr" rid="bib33">Egner, 2023</xref>; <xref ref-type="bibr" rid="bib56">Kolling et al., 2016</xref>; <xref ref-type="bibr" rid="bib85">O’Reilly et al., 2013</xref>). Our current findings are likely due to the utility of scene information for finding the face. In this task, the target and distractor faces were perceptually similar, but scene information was highly distinctive. If the target face was easy to discriminate, the utility of the scene disappears (<xref ref-type="bibr" rid="bib121">Zhou and Geng, 2024</xref>). Under those conditions, we would not expect to see scene information loaded into the target template. Thus, we expect our results to generalize only when target-associated information is easier to discriminate than target information. Despite our clear evidence for scene activation in IFJ and PPA during the delay period and changes in visual search performance based on scene associations, it is not possible to derive causality between the two given the correlational nature of fMRI and behavioral performance.</p><p>In summary, our study builds on a growing body of literature on preparatory attention, focusing specifically on what information is used to generate the most predictive target template. We provide novel evidence that target-associated information can supplant the actual target in the ‘target’ template when the associated information is expected to more easily facilitate behavioral goals. In our case, the cued target face was translated into a template for a probabilistically predictive associated scene in the IFJ and sensory cortex in preparation for search. This suggests that when there is uncertainty in target detection and decisions, additional information is dynamically recalled to facilitate performance (<xref ref-type="bibr" rid="bib48">Hansen et al., 2012</xref>; <xref ref-type="bibr" rid="bib103">Summerfield et al., 2011</xref>; <xref ref-type="bibr" rid="bib115">Witkowski and Geng, 2023</xref>). The results are consistent with decades of behavioral work showing the adaptive nature of attention in supporting goal-directed behavior. It goes further in providing a first demonstration of how this mechanism unfolds using target-associated information from memory in frontal-parietal-sensory networks to anticipate visual search.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>The sample size was chosen based on a power analysis of a previous behavioral study (<xref ref-type="bibr" rid="bib121">Zhou and Geng, 2024</xref>) and similar fMRI MVPA studies (<xref ref-type="bibr" rid="bib1">Albers et al., 2013</xref>; <xref ref-type="bibr" rid="bib110">van Loon et al., 2018</xref>). Using the more conservative estimate based on the behavioral validity effect size (<italic>d</italic>z = 0.678), the estimated minimum sample size to achieve significant effects (p=0.05, two-tailed) with a power 0.9 was N=25. A final number of 26 participants (mean age = 21.1 years, range = 18–29 years, females = 14, males = 12) were recruited from the University of California, Davis, and were given a combination of course credit and monetary compensation ($50) for a 2-hour MRI session. All participants were right-handed and native English speakers or had spoken English before the age of 5. None of them reported a history of neurological or psychiatric disorders. In addition, they all had normal or corrected-to-normal vision and passed the color blindness test (<ext-link ext-link-type="uri" xlink:href="https://colormax.org/color-blind-test/">https://colormax.org/color-blind-test/</ext-link>). Two additional participants were removed due to excessive motion during scanning (head movement &gt;3 mm), and a third participant was removed due to falling asleep in the scanner. The study adhered to the ethical principles of the Declaration of Helsinki and the NIH guidelines for ethical research. All consent forms and procedures were approved by the Institutional Review Board of the University of California, Davis (IRB number 1296375).</p></sec><sec id="s4-2"><title>Stimuli</title><p>Stimuli for all experiments consisted of 16 faces (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) selected from the Chicago Face Database (CFD, <xref ref-type="bibr" rid="bib68">Ma et al., 2015</xref>) and 64 scenes (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>) selected from a scene categorization study (<xref ref-type="bibr" rid="bib53">Jung et al., 2018</xref>). The ratings for gender and race for all faces were restricted to be larger than 0.9 (based on normalized rating scores ranging between 0 and 1 provided by the CFD; a higher score indicates being more representative of that category). As in our previous study (<xref ref-type="bibr" rid="bib121">Zhou and Geng, 2024</xref>), we used four target faces derived from crossings between two genders (woman and man) and races (black and white). An additional three faces were selected from each category to serve as distractors for the study. An oval mask was used to crop all selected faces to reduce the visibility of extra-facial features, for example, hair. The scene categories were crossed between content (nature and urban) and layout (open and closed). There were 16 exemplar images from each scene category used in the experiment. The resolution of the face images was 150×200 pixels (2°×2.7° visual angles) and the scenes were 800×600 pixels (10.7°×8° visual angles). All stimuli were presented against a gray background (RGB: 128, 128, 128). The low-level luminance and contrast of all face and scene images were controlled with the SHINE_color toolbox (<xref ref-type="bibr" rid="bib114">Willenbockel et al., 2010</xref>).</p></sec><sec id="s4-3"><title>Experimental design</title><sec id="s4-3-1"><title>Face-scene associative learning task</title><p>Before the MRI scan session, participants first learned four face-scene category associations (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). During the learning phase, each of the face-scene pairs was shown along with a narrative, for example, “This is Holly. She is a ranger, so you will find her in the forest.” Next, participants were tested for their memory on the newly learned associations in a face-scene pair matching task (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). On each test trial, both a face and scene appeared on the screen, and participants were asked to judge whether the face was associated with the scene by pressing the ‘m’ for a match, or ‘n’ for a nonmatch. Feedback was given with the word ‘correct’ or ‘incorrect’ presented for 0.5 seconds after each response. The inter-trial interval (ITI) was 0.5 seconds. Each of the four faces was tested on 18 scene-match trials and 9 scene-nonmatch trials. There was a total of 108 trials presented in a pseudorandomized order. For the 18 match trials, participants viewed the target face on 18 different scene exemplars from the same category. For the nine nonmatch trials, participants viewed the target face on three different scene images (randomly selected) from the remaining three scene categories.</p></sec><sec id="s4-3-2"><title>Face search task</title><p>After the learning task, participants completed the face search task in the first scan session (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). At the beginning of each trial, a face cue was presented for 1 second followed by an 8-second fixation cross. Next, the search display appeared for 0.25 second. The search display was followed by an 8.75-second ITI. The search display was composed of the target face and one randomly selected distractor face. Each face was superimposed on a scene image. The face-scene pairs appeared on the left and right of the screen split by a central fixation cross. The faces were centered in each of the scenes and the distance from the inner edge of the scene to the central fixation was 1° visual angle. The target face was always presented while the distractor face was randomly drawn from the set of three faces that matched the target in both gender and race. Participants indicated the location of the target face by pressing the ‘1’ or ‘2’ key on the MRI-compatible keypad corresponding to the left or right side. The response was limited to within 2 seconds.</p><p>Search trials were divided into scene-valid and scene-invalid conditions. The scene was valid on 75% of trials and invalid on the remaining 25%. On the scene-valid trials, the target face was always superimposed on its associated scene while the distractor face was superimposed on a scene from one of the three unassociated categories. On scene-invalid trials, the scenes that appeared with the target and distractor faces were from two different scene categories, both of which were unassociated with the target face.</p><p>All participants completed eight runs of the face search task. There were 16 trials (12 scene-valid trials and 4 scene-invalid trials) in each run and 4 trials with each of the four target faces. The participants were not informed about the exact probability of the valid to invalid trials in the experiment. The trial order as well as the location of the target face in the search display was counterbalanced and pseudorandomized. Participants completed a full practice run in the scanner at the beginning to familiarize them with the task and the testing environment.</p></sec><sec id="s4-3-3"><title>Face and scene 1-back task</title><p>After completing the face search task, a 6-minute T1-weighted structural scan was collected. Then, each participant completed a 1-back task in a second scan session with four runs (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). On each trial, one stimulus was presented at the center for 1 second, followed by an 8-second fixation. Participants were instructed to make a button press when the current stimulus was the same as the previous one. There were four trials for each target face and scene category. Different exemplars were used for each repetition of the same scene category. An additional six trials were designated as the 1-back trial with a repeated stimulus. The trial sequence was counterbalanced and pseudorandomized. Participants completed a full practice run in the scanner before the formal test.</p></sec><sec id="s4-3-4"><title>Functional category-selective localizer</title><p>We identified two ROIs, the FFA and the PPA, for each individual participant in an independent functional localizer task (<xref ref-type="bibr" rid="bib100">Stigliani et al., 2015</xref>). Stimuli were 2-D grayscale images (~16°×16° for all stimuli) consisting of faces, houses, corridors, cars, instruments, and phase-scrambled images. Eight images from the same object category were on for 0.4 second and off for 0.1 second in a 4-second mini-block. Participants performed a 1-back memory task and repeats occurred in half of the blocks in each run. Each run consisted of 54 blocks evenly divided by the six stimulus categories, and the block order was counterbalanced and pseudorandomized across runs. Two runs were collected for each participant.</p></sec></sec><sec id="s4-4"><title>Stimulus apparatus</title><p>The <italic>Face-scene associative learning task</italic> was conducted on a 14-inch MacBook Pro laptop with a spatial resolution of 1920×1080, and all stimuli were presented using the Testable platform (<ext-link ext-link-type="uri" xlink:href="https://www.testable.org/">https://www.testable.org/</ext-link>). The remaining experimental tasks were conducted while the participants were in the scanner. All stimuli for the scanner experiments were generated using Psychotoolbox-3 installed on a Dell desktop PC and displayed on a 24-inch BOLDscreen LCD monitor with a spatial resolution of 1920×1200 pixels. Participants viewed stimuli through a mirror attached to the head coil which projected the monitor ~120 cm away outside of the scanner bore.</p></sec><sec id="s4-5"><title>MRI acquisition and preprocessing</title><p>All scans were performed on a 3-Tesla Siemens Skyra scanner with a 32-channel phased-array head coil at the University of California, Davis. Functional scans using T2-weighted echoplanar imaging (EPI) sequence with an acceleration factor of 2 were acquired with whole-brain volumes of 48 axial slices of 3 mm thickness (TR/TE 1500/24.6 ms, flip angle 75°, base/phase resolution 70/100, 3×3 mm<sup>2</sup>, FOV 210 mm). High-resolution structural MPRAGE T1-weighted images (TR/TE 1800/2.97 ms, flip angle 7°, base/phase resolution 256/100, FOV 256 mm, 208 sagittal slices) were acquired and were used for anatomical normalization, co-registration, and cortical surface reconstruction. The whole MRI session was finished within 2 hours.</p><p>Functional and structural data were preprocessed using SPM12 (Wellcome Department of Imaging Neuroscience), FreeSurfer (<xref ref-type="bibr" rid="bib29">Dale et al., 1999</xref>; <xref ref-type="bibr" rid="bib40">Fischl et al., 1999</xref>), and in-house MATLAB (MathWorks) code. The first six initial functional scans of each run were discarded to allow for equilibrium effects. The preprocessing for functional EPI data included slice-time correction and spatial realignment. Using a two-pass procedure, fMRI data from all three scan sessions were aligned to the mean of the EPI images of the first session run. Participants with head motion &gt;3 mm were excluded. The structural image was coregistered with the mean image. Cortical hemispheric surface reconstruction was performed using the ‘recon-all’ command in the FreeSurfer. All fMRI analyses were performed in the native individual space without spatial smoothing, except that a 4 mm full-width at half-maximum (FWHM) smoothing was applied to the <italic>functional category-selective localizer</italic> runs.</p><p>The segmentation utility in SPM12 was applied to estimate gray and white matter boundary parameters for spatial normalization. Then, the analyzed native space functional data (i.e., univariate beta images and searchlight classification accuracy images) was normalized into the standard 2×2×2 mm<sup>3</sup> MNI reference space and smoothed with an 8 mm FWHM isotropic kernel for group-based whole-brain analyses.</p></sec><sec id="s4-6"><title>fMRI general linear model (GLM)</title><p>For each scan session, a GLM was generated by convolving the canonical hemodynamic response functions with experimental conditions for each voxel. Six motion realignment parameters in each experimental run were included as nuisance regressors to control for head motion confounds. For the <italic>face search task</italic>, the GLM was based on different time segments of the task corresponding to the face <italic>search cue</italic>, <italic>search delay</italic> period, and search display. The GLM was constructed with 11 regressors for each experimental run. The first four regressors had a duration of 1 second and were used to estimate the four different face search cue identities. The next four regressors were 8 seconds long and estimated the search delay period following each face. Three additional regressors with a duration of 1 second were used to model the three types of search displays: scene-valid trials, scene-invalid trials, and error response trials.</p><p>A separate GLM was used to model the <italic>face and scene 1-back task</italic>. The GLM was constructed with 17 regressors for each experimental condition in each run. There were eight regressors for each of the four face images and its corresponding delay period, and another eight regressors for each scene category image and its corresponding delay period. The BOLD responses to the stimulus image and its corresponding delay period were modeled separately, just as it was for the <italic>face search task</italic>. The same model parameters were used so that the two tasks would be compatible for our cross-task decoding procedure. The last regressor with 1-second duration was used to define the 1-back event, that is, response to a repeated image, which was not a trial type of interest.</p><p>A third GLM was used to model the <italic>functional category-selective localizer</italic> in order to identify FFA and PPA in each participant. The GLM was constructed with six regressors with a 4-second duration to estimate all object categories (faces, houses, corridors, cars, instruments, and phase-scrambled images) within two runs. Contrast beta images between specific types of stimulus categories were used to identify category-selective regions in the visual cortex at the individual level.</p></sec><sec id="s4-7"><title>ROIs definition</title><p>The ROIs of primary interest focused on subregions of the frontal, parietal, and visual cortex that are known to be involved in attentional and cognitive control. As shown in <xref ref-type="fig" rid="fig2">Figure 2D and A</xref>, a total of 12 ROIs were defined in the native space of each participant through a combination of the Human Connectome Project Multi-Modal Parcellation (HCP-MMP1; <xref ref-type="bibr" rid="bib42">Glasser et al., 2016</xref>), the resting-state 17-network (<xref ref-type="bibr" rid="bib94">Schaefer et al., 2018</xref>), and our independent category-selective localizer. Each ROI selected from the surface-based HCP-MMP1 atlas and the resting-state 17-network was first transformed from the group-based fsaverage surface into an individual surface, and then remapped onto the volumetric native space using the ‘mri_surf2vol’.</p><p>The HCP-MMP1 atlas was used to define three ROIs corresponding to the IFJ (<xref ref-type="bibr" rid="bib8">Bedini et al., 2023</xref>), located along the posterior-to-anterior axis of the inferior frontal sulcus, namely the premotor eyefield (PEF), posterior IFJ (IFJp), and anterior IFJ (IFJa). Using the resting-state 17-network, we extracted the dorsal attention network, the ventral attention network, and the frontoparietal control network ROIs from the lateral frontal and parietal cortex. The cortical parcellations in the three networks in the lateral prefrontal cortex correspond to the FEFs, vLPFC, and dLPFC; the cortical parcellations in the lateral parietal cortex correspond to the SPL, IPS, and inferior parietal lobule (IPL). In addition, the early visual cortex (V1) ROI was defined from the visual network located in areas labeled the striate cortex and striate calcarine. The ROIs defined by atlases are imperfect because they do not map precisely onto individual functional anatomy. Thus, the ROI analyses may include functional activations from anatomically adjacent regions. The last two visual ROIs, the FFA and PPA, were identified from an independent functional category-selective localizer. The preprocessed data of the two functional localizer runs were fitted with a first-level GLM to estimate BOLD responses to each of the stimulus categories in the individual volumetric brain. The FFA was defined by a contiguous cluster of voxels in the fusiform gyrus from the contrast between faces vs. all remaining categories, at p&lt;0.001, uncorrected; the PPA was defined as the contiguous cluster of voxels in the parahippocampal gyrus from the contrast between scenes/corridors vs. all remaining categories, at <italic>p</italic>(FWE)&lt;0.005. No activation was observed at these thresholds for six participants, thus a more lenient threshold was used to allow sufficient voxels in FFA and PPA to be identified for analyses. The average number of voxels for each ROI is reported in <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>. Note that a large portion of IFJp and IFJa from the HCP-MMP1 atlas overlapped with the vLPFC from the 17-network atlas. In addition, PEF from the HCP-MMP1 atlas highly overlapped with the precentral label from the dorsal attention network in the left hemisphere and the ventral attention network in the right hemisphere using the 17-network atlas (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p></sec><sec id="s4-8"><title>Univariate whole-brain analysis</title><p>Although our primary goal was to identify cortical involvement in holding the search template during the face cue and delay periods, we also examined the scene validity effect during the search display period of the <italic>face search task</italic>. To do this, the normalized and smoothed data for the scene-valid and scene-invalid conditions were entered into a second-level GLM with random effects modeled at the group level. Linear contrasts between the two conditions were used to reveal selective regions with significant BOLD activations to either the scene-valid (valid&gt;invalid) or scene-invalid (invalid&gt;valid) conditions. The resultant <italic>t</italic>-value maps were corrected for multiple comparisons at the cluster level using the threshold-free cluster enhancement (TFCE; <xref ref-type="bibr" rid="bib96">Smith and Nichols, 2009</xref>; <xref ref-type="bibr" rid="bib99">Spisák et al., 2019</xref>) implemented in SPM12. The threshold was set at <italic>p</italic>TFCE&lt;0.005 for clusters with greater than 50 voxels.</p></sec><sec id="s4-9"><title>Multivariate pattern analysis</title><p>The estimated beta parameters from the <italic>face search task</italic> and the <italic>face and scene 1-back task</italic> first-level GLMs were used to decode whether the target face and/or its associated scene was held as the search template during the <italic>search cue</italic> or <italic>search delay</italic> periods. The beta parameters extracted from each of the 12 ROIs were normalized to remove univariate differences between conditions before being submitted to a binary linear SVM classifier implemented in LIBSVM (<xref ref-type="bibr" rid="bib20">Chang and Lin, 2011</xref>), with a default cost parameter of 1. <xref ref-type="fig" rid="fig2">Figure 2C</xref> illustrates the single-step cross-classification scheme (<xref ref-type="bibr" rid="bib15">Brandman and Peelen, 2017</xref>; <xref ref-type="bibr" rid="bib41">Gayet and Peelen, 2022</xref>) adopted in the current study. For the decoding of faces during the search cue period, the classifiers were trained to discriminate between pairs of faces (with a full combination of six pairs based on four types of face cues) on the <italic>face search task</italic> runs. These classifiers were then tested using <italic>face and scene</italic> information from the 1-back task stimulus sample period. Following the same procedure for the delay period, classifiers were trained to discriminate between pairs of <italic>search delay</italic> period information following each face cue (six pairs of delay periods)<italic>;</italic> the classifiers were then tested on the delay period following face or scene sample stimuli from the <italic>face and scene 1-back task</italic>. The classification accuracy was estimated by averaging performance from the six binary classifiers for each type of cross-classification.</p></sec><sec id="s4-10"><title>Statistical inference</title><p>Group-level statistical significance testing was established based on a nonparametric permutation method in which data labels were randomized. The null hypothesis classification accuracy distribution was generated by 10,000 iterations of each type of cross-classification for each participant. The group-level null distribution was then calculated by averaging these classification accuracies across participants. The permuted p-value was defined by the proportion of counts in the null distribution that were equal to or higher than the observed real group average classification accuracy, (n+1)/(10,000+1). This p-value was used as the assessment for statistical significance, and all resulting p-values were controlled for family-wise error by using Bonferroni correction.</p></sec><sec id="s4-11"><title>Searchlight analysis</title><p>We also investigated the whole-brain multivariate decoding results using the searchlight approach (<xref ref-type="bibr" rid="bib57">Kriegeskorte et al., 2006</xref>). At the individual level, the linear SVM decoding accuracy was assigned to the center voxel within a 9 mm radius sphere in volumetric space. This procedure estimates classification accuracies for all voxels within a whole-brain mask. The pairwise classification schemes were identical to those implemented for the ROI multivariate decoding analysis. The resulting individual participant whole-brain information maps for different classifications were first smoothed using a 4 mm FWHM Gaussian kernel and then normalized and tested for statistical significance against 50% chance decoding using a one-sample <italic>t-</italic>test (one-tailed). The statistical threshold of all group-level searchlight maps was corrected for multiple comparisons at the cluster level using TFCE.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The study adhered to the ethical principles of the Declaration of Helsinki and the NIH guidelines for ethical research. All consent forms and procedures were approved by the Institutional Review Board of the University of California, Davis (IRB number 1296375).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Whole-brain searchlight results of brain regions showing significant decoding during the search cue period.</title></caption><media xlink:href="elife-104041-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Whole-brain searchlight results of brain regions showing significant decoding during the search delay period.</title></caption><media xlink:href="elife-104041-supp2-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Univariate contrasts related to scene validity during the search period.</title></caption><media xlink:href="elife-104041-supp3-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>Mean (SE) number of voxels in each ROI.</title></caption><media xlink:href="elife-104041-supp4-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-104041-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The behavioral data, deidentified preprocessed fMRI data, analysis code, and supplemental materials are available on Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/xw8hm/">https://osf.io/xw8hm/</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Preparatory attentional templates in prefrontal and sensory cortex encode target-associated information</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/xw8hm/">xw8hm</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This research received funding supported by the National Institutes of Health under Grant R01-MH113855 to Joy J Geng, and the Humanity and Social Science Youth Foundation of the Ministry of Education in China under Grant 24XJC190010 and the Research Planning Project of the Sichuan Psychological Society Grant SCSXLXH2023001 to Zhiheng Zhou.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albers</surname><given-names>AM</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Toni</surname><given-names>I</given-names></name><name><surname>Dijkerman</surname><given-names>HC</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Shared representations for working memory and mental imagery in early visual cortex</article-title><source>Current Biology</source><volume>23</volume><fpage>1427</fpage><lpage>1431</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.05.065</pub-id><pub-id pub-id-type="pmid">23871239</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname><given-names>RG</given-names></name><name><surname>Zelinsky</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual similarity effects in categorical search</article-title><source>Journal of Vision</source><volume>11</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.1167/11.8.9</pub-id><pub-id pub-id-type="pmid">21757505</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname><given-names>D</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Left ventrolateral prefrontal cortex and the cognitive control of memory</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>2883</fpage><lpage>2901</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.06.015</pub-id><pub-id pub-id-type="pmid">17675110</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname><given-names>D</given-names></name><name><surname>Nee</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Frontal cortex and the hierarchical control of behavior</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>170</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.11.005</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldauf</surname><given-names>D</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural mechanisms of object-based attention</article-title><source>Science</source><volume>344</volume><fpage>424</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1126/science.1247003</pub-id><pub-id pub-id-type="pmid">24763592</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Battistoni</surname><given-names>E</given-names></name><name><surname>Stein</surname><given-names>T</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Preparatory attention in visual cortex</article-title><source>Annals of the New York Academy of Sciences</source><volume>1396</volume><fpage>92</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1111/nyas.13320</pub-id><pub-id pub-id-type="pmid">28253445</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bedini</surname><given-names>M</given-names></name><name><surname>Baldauf</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Structure, function and connectivity fingerprints of the frontal eye field versus the inferior frontal junction: A comprehensive comparison</article-title><source>The European Journal of Neuroscience</source><volume>54</volume><fpage>5462</fpage><lpage>5506</lpage><pub-id pub-id-type="doi">10.1111/ejn.15393</pub-id><pub-id pub-id-type="pmid">34273134</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bedini</surname><given-names>M</given-names></name><name><surname>Olivetti</surname><given-names>E</given-names></name><name><surname>Avesani</surname><given-names>P</given-names></name><name><surname>Baldauf</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Accurate localization and coactivation profiles of the frontal eye field and inferior frontal junction: an ALE and MACM fMRI meta-analysis</article-title><source>Brain Structure &amp; Function</source><volume>228</volume><fpage>997</fpage><lpage>1017</lpage><pub-id pub-id-type="doi">10.1007/s00429-023-02641-y</pub-id><pub-id pub-id-type="pmid">37093304</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bettencourt</surname><given-names>KC</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Decoding the content of visual short-term memory under distraction in occipital and parietal areas</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>150</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1038/nn.4174</pub-id><pub-id pub-id-type="pmid">26595654</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bichot</surname><given-names>NP</given-names></name><name><surname>Heard</surname><given-names>MT</given-names></name><name><surname>DeGennaro</surname><given-names>EM</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A source for feature-based attention in the prefrontal cortex</article-title><source>Neuron</source><volume>88</volume><fpage>832</fpage><lpage>844</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.10.001</pub-id><pub-id pub-id-type="pmid">26526392</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bichot</surname><given-names>NP</given-names></name><name><surname>Xu</surname><given-names>R</given-names></name><name><surname>Ghadooshahy</surname><given-names>A</given-names></name><name><surname>Williams</surname><given-names>ML</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The role of prefrontal cortex in the control of feature attention in area V4</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>5727</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-13761-7</pub-id><pub-id pub-id-type="pmid">31844117</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boettcher</surname><given-names>SEP</given-names></name><name><surname>Draschkow</surname><given-names>D</given-names></name><name><surname>Dienhart</surname><given-names>E</given-names></name><name><surname>Võ</surname><given-names>ML-H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Anchoring visual search in scenes: assessing the role of anchor objects on eye movements during visual search</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.1167/18.13.11</pub-id><pub-id pub-id-type="pmid">30561493</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boettcher</surname><given-names>SEP</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>One thing leads to another: anticipating visual object identity based on associative-memory templates</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>4010</fpage><lpage>4020</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2751-19.2020</pub-id><pub-id pub-id-type="pmid">32284338</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Carter</surname><given-names>CS</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Conflict monitoring and anterior cingulate cortex: an update</article-title><source>Trends in Cognitive Sciences</source><volume>8</volume><fpage>539</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.10.003</pub-id><pub-id pub-id-type="pmid">15556023</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandman</surname><given-names>T</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Interaction between scene and object processing revealed by human fMRI and MEG decoding</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>7700</fpage><lpage>7710</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0582-17.2017</pub-id><pub-id pub-id-type="pmid">28687603</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braver</surname><given-names>TS</given-names></name><name><surname>Paxton</surname><given-names>JL</given-names></name><name><surname>Locke</surname><given-names>HS</given-names></name><name><surname>Barch</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Flexible neural mechanisms of cognitive control within human prefrontal cortex</article-title><source>PNAS</source><volume>106</volume><fpage>7351</fpage><lpage>7356</lpage><pub-id pub-id-type="doi">10.1073/pnas.0808187106</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braver</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The variable nature of cognitive control: a dual mechanisms framework</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>106</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.12.010</pub-id><pub-id pub-id-type="pmid">22245618</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castelhano</surname><given-names>MS</given-names></name><name><surname>Mack</surname><given-names>ML</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Viewing task influences eye movement control during active scene perception</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.1167/9.3.6</pub-id><pub-id pub-id-type="pmid">19757945</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castelhano</surname><given-names>MS</given-names></name><name><surname>Krzyś</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Rethinking space: a review of perception, attention, and memory in scene processing</article-title><source>Annual Review of Vision Science</source><volume>6</volume><fpage>563</fpage><lpage>586</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-121219-081745</pub-id><pub-id pub-id-type="pmid">32491961</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>CC</given-names></name><name><surname>Lin</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>LIBSVM: a library for support vector machines</article-title><source>ACM Transactions on Intelligent Systems and Technology</source><volume>2</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1145/1961189.196119</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>A neural basis for visual search in inferior temporal cortex</article-title><source>Nature</source><volume>363</volume><fpage>345</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1038/363345a0</pub-id><pub-id pub-id-type="pmid">8497317</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chiu</surname><given-names>YC</given-names></name><name><surname>Esterman</surname><given-names>M</given-names></name><name><surname>Han</surname><given-names>Y</given-names></name><name><surname>Rosen</surname><given-names>H</given-names></name><name><surname>Yantis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Decoding task-based attentional modulation during face categorization</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>1198</fpage><lpage>1204</lpage><pub-id pub-id-type="doi">10.1162/jocn.2010.21503</pub-id><pub-id pub-id-type="pmid">20429856</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Klink</surname><given-names>PC</given-names></name><name><surname>Spitzer</surname><given-names>B</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The distributed nature of working memory</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>111</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.12.007</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Iamshchinina</surname><given-names>P</given-names></name><name><surname>Yan</surname><given-names>C</given-names></name><name><surname>Allefeld</surname><given-names>C</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cortical specialization for attended versus unattended working memory</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>494</fpage><lpage>496</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0094-4</pub-id><pub-id pub-id-type="pmid">29507410</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname><given-names>MW</given-names></name><name><surname>Reynolds</surname><given-names>JR</given-names></name><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Repovs</surname><given-names>G</given-names></name><name><surname>Anticevic</surname><given-names>A</given-names></name><name><surname>Braver</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Multi-task connectivity reveals flexible hubs for adaptive task control</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1348</fpage><lpage>1355</lpage><pub-id pub-id-type="doi">10.1038/nn.3470</pub-id><pub-id pub-id-type="pmid">23892552</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collegio</surname><given-names>AJ</given-names></name><name><surname>Nah</surname><given-names>JC</given-names></name><name><surname>Scotti</surname><given-names>PS</given-names></name><name><surname>Shomstein</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Attention scales according to inferred real-world object size</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>40</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1038/s41562-018-0485-2</pub-id><pub-id pub-id-type="pmid">30932061</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Contreras</surname><given-names>JM</given-names></name><name><surname>Banaji</surname><given-names>MR</given-names></name><name><surname>Mitchell</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Multivoxel patterns in fusiform face area differentiate faces by sex and race</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e69684</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0069684</pub-id><pub-id pub-id-type="pmid">23936077</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Patel</surname><given-names>G</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The reorienting system of the human brain: from environment to theory of mind</article-title><source>Neuron</source><volume>58</volume><fpage>306</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.04.017</pub-id><pub-id pub-id-type="pmid">18466742</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How do expectations shape perception?</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>764</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.06.002</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Neural mechanisms of selective visual attention</article-title><source>Annual Review of Neuroscience</source><volume>18</volume><fpage>193</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.001205</pub-id><pub-id pub-id-type="pmid">7605061</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dombert</surname><given-names>PL</given-names></name><name><surname>Kuhns</surname><given-names>A</given-names></name><name><surname>Mengotti</surname><given-names>P</given-names></name><name><surname>Fink</surname><given-names>GR</given-names></name><name><surname>Vossel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Functional mechanisms of probabilistic inference in feature- and space-based attentional systems</article-title><source>NeuroImage</source><volume>142</volume><fpage>553</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.08.010</pub-id><pub-id pub-id-type="pmid">27523448</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Principles of cognitive control over task focus and task switching</article-title><source>Nature Reviews Psychology</source><volume>2</volume><fpage>702</fpage><lpage>714</lpage><pub-id pub-id-type="doi">10.1038/s44159-023-00234-4</pub-id><pub-id pub-id-type="pmid">39301103</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emrich</surname><given-names>SM</given-names></name><name><surname>Riggall</surname><given-names>AC</given-names></name><name><surname>Larocque</surname><given-names>JJ</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Distributed patterns of activity in sensory cortex reflect the precision of multiple items maintained in visual short-term memory</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>6516</fpage><lpage>6523</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5732-12.2013</pub-id><pub-id pub-id-type="pmid">23575849</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Parietal and frontal cortex encode stimulus-specific mnemonic representations during visual working memory</article-title><source>Neuron</source><volume>87</volume><fpage>893</fpage><lpage>905</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.07.013</pub-id><pub-id pub-id-type="pmid">26257053</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Sutterer</surname><given-names>DW</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Awh</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Feature-selective attentional modulations in human frontoparietal cortex</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>8188</fpage><lpage>8199</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3935-15.2016</pub-id><pub-id pub-id-type="pmid">27488638</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esterman</surname><given-names>M</given-names></name><name><surname>Yantis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Perceptual expectation evokes category-selective cortical activity</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>1245</fpage><lpage>1253</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp188</pub-id><pub-id pub-id-type="pmid">19759124</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Favila</surname><given-names>SE</given-names></name><name><surname>Samide</surname><given-names>R</given-names></name><name><surname>Sweigart</surname><given-names>SC</given-names></name><name><surname>Kuhl</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Parietal representations of stimulus features are amplified during memory retrieval and flexibly aligned with top-down goals</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7809</fpage><lpage>7821</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0564-18.2018</pub-id><pub-id pub-id-type="pmid">30054390</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finn</surname><given-names>ES</given-names></name><name><surname>Huber</surname><given-names>L</given-names></name><name><surname>Jangraw</surname><given-names>DC</given-names></name><name><surname>Molfese</surname><given-names>PJ</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Layer-dependent activity in human prefrontal cortex during working memory</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1687</fpage><lpage>1695</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0487-z</pub-id><pub-id pub-id-type="pmid">31551596</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis. II: Inflation, flattening, and a surface-based coordinate system</article-title><source>NeuroImage</source><volume>9</volume><fpage>195</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0396</pub-id><pub-id pub-id-type="pmid">9931269</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gayet</surname><given-names>S</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Preparatory attention incorporates contextual expectations</article-title><source>Current Biology</source><volume>32</volume><fpage>687</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.11.062</pub-id><pub-id pub-id-type="pmid">34919809</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Coalson</surname><given-names>TS</given-names></name><name><surname>Robinson</surname><given-names>EC</given-names></name><name><surname>Hacker</surname><given-names>CD</given-names></name><name><surname>Harwell</surname><given-names>J</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Andersson</surname><given-names>J</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A multi-modal parcellation of human cerebral cortex</article-title><source>Nature</source><volume>536</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1038/nature18933</pub-id><pub-id pub-id-type="pmid">27437579</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>MY</given-names></name><name><surname>Chen</surname><given-names>YL</given-names></name><name><surname>Liu</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Preparatory attention to visual features primarily relies on non-sensory representation</article-title><source>Scientific Reports</source><volume>12</volume><elocation-id>21726</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-022-26104-2</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gregoriou</surname><given-names>GG</given-names></name><name><surname>Rossi</surname><given-names>AF</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Lesions of prefrontal cortex reduce attentional modulation of neuronal responses and synchrony in V4</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1003</fpage><lpage>1011</lpage><pub-id pub-id-type="doi">10.1038/nn.3742</pub-id><pub-id pub-id-type="pmid">24929661</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guerin</surname><given-names>SA</given-names></name><name><surname>Robbins</surname><given-names>CA</given-names></name><name><surname>Gilmore</surname><given-names>AW</given-names></name><name><surname>Schacter</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Interactions between visual attention and episodic retrieval: dissociable contributions of parietal regions during gist-based false recognition</article-title><source>Neuron</source><volume>75</volume><fpage>1122</fpage><lpage>1134</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.08.020</pub-id><pub-id pub-id-type="pmid">22998879</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guntupalli</surname><given-names>JS</given-names></name><name><surname>Wheeler</surname><given-names>KG</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Disentangling the representation of identity from head view along the human face processing pathway</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>46</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw344</pub-id><pub-id pub-id-type="pmid">28051770</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname><given-names>EH</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Object-based attention during scene perception elicits boundary contraction in memory</article-title><source>Memory &amp; Cognition</source><volume>53</volume><fpage>6</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.3758/s13421-024-01540-9</pub-id><pub-id pub-id-type="pmid">38530622</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansen</surname><given-names>KA</given-names></name><name><surname>Hillenbrand</surname><given-names>SF</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Effects of prior knowledge on decisions made under perceptual vs. categorical uncertainty</article-title><source>Frontiers in Neuroscience</source><volume>6</volume><elocation-id>163</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2012.00163</pub-id><pub-id pub-id-type="pmid">23162424</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helbing</surname><given-names>J</given-names></name><name><surname>Draschkow</surname><given-names>D</given-names></name><name><surname>L-H Võ</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Auxiliary scene-context information provided by anchor objects guides attention and locomotion in natural search behavior</article-title><source>Psychological Science</source><volume>33</volume><fpage>1463</fpage><lpage>1476</lpage><pub-id pub-id-type="doi">10.1177/09567976221091838</pub-id><pub-id pub-id-type="pmid">35942922</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hout</surname><given-names>MC</given-names></name><name><surname>Goldinger</surname><given-names>SD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Target templates: the precision of mental representations affects attentional guidance and decision-making in visual search</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>77</volume><fpage>128</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.3758/s13414-014-0764-6</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeong</surname><given-names>SK</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Behaviorally relevant abstract object identity representation in the human parietal cortex</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>1607</fpage><lpage>1619</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1016-15.2016</pub-id><pub-id pub-id-type="pmid">26843642</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Josephs</surname><given-names>E</given-names></name><name><surname>Drew</surname><given-names>T</given-names></name><name><surname>Wolfe</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Shuffling your way out of change blindness</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>193</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0886-4</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung</surname><given-names>Y</given-names></name><name><surname>Larsen</surname><given-names>B</given-names></name><name><surname>Walther</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Modality-independent coding of scene categories in prefrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>5969</fpage><lpage>5981</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0272-18.2018</pub-id><pub-id pub-id-type="pmid">29858483</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung</surname><given-names>Y</given-names></name><name><surname>Walther</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural representations in the prefrontal cortex are task dependent for scene attributes but not for scene categories</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>7234</fpage><lpage>7245</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2816-20.2021</pub-id><pub-id pub-id-type="pmid">34103357</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastner</surname><given-names>S</given-names></name><name><surname>Pinsk</surname><given-names>MA</given-names></name><name><surname>De Weerd</surname><given-names>P</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Increased activity in human visual cortex during directed attention in the absence of visual stimulation</article-title><source>Neuron</source><volume>22</volume><fpage>751</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80734-5</pub-id><pub-id pub-id-type="pmid">10230795</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolling</surname><given-names>N</given-names></name><name><surname>Behrens</surname><given-names>T</given-names></name><name><surname>Wittmann</surname><given-names>MK</given-names></name><name><surname>Rushworth</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Multiple signals in anterior cingulate cortex</article-title><source>Current Opinion in Neurobiology</source><volume>37</volume><fpage>36</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.12.007</pub-id><pub-id pub-id-type="pmid">26774693</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Information-based functional brain mapping</article-title><source>PNAS</source><volume>103</volume><fpage>3863</fpage><lpage>3868</lpage><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id><pub-id pub-id-type="pmid">16537458</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurtin</surname><given-names>DL</given-names></name><name><surname>Araña-Oiarbide</surname><given-names>G</given-names></name><name><surname>Lorenz</surname><given-names>R</given-names></name><name><surname>Violante</surname><given-names>IR</given-names></name><name><surname>Hampshire</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Planning ahead: predictable switching recruits task-active and resting-state networks</article-title><source>Human Brain Mapping</source><volume>44</volume><fpage>5030</fpage><lpage>5046</lpage><pub-id pub-id-type="doi">10.1002/hbm.26430</pub-id><pub-id pub-id-type="pmid">37471699</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwak</surname><given-names>Y</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Unveiling the abstract format of mnemonic representations</article-title><source>Neuron</source><volume>110</volume><fpage>1822</fpage><lpage>1828</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.03.016</pub-id><pub-id pub-id-type="pmid">35395195</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>SH</given-names></name><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Goal-dependent dissociation of visual and prefrontal cortices during working memory</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>997</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1038/nn.3452</pub-id><pub-id pub-id-type="pmid">23817547</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Idiosyncratic patterns of representational similarity in prefrontal cortex predict attentional performance</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>1257</fpage><lpage>1268</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1407-16.2016</pub-id><pub-id pub-id-type="pmid">28028199</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lerebourg</surname><given-names>M</given-names></name><name><surname>Lange</surname><given-names>FP</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Preparatory activity during visual search reflects attention-guiding objects rather than search targets</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2024.02.02.578555</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>BJ</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cognitive control and right ventrolateral prefrontal cortex: reflexive reorienting, motor inhibition, and action updating</article-title><source>Annals of the New York Academy of Sciences</source><volume>1224</volume><fpage>40</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2011.05958.x</pub-id><pub-id pub-id-type="pmid">21486295</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Slotnick</surname><given-names>SD</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Yantis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cortical mechanisms of feature-based attentional control</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>1334</fpage><lpage>1343</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhg080</pub-id><pub-id pub-id-type="pmid">14615298</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Larsson</surname><given-names>J</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Feature-based attention modulates orientation-selective responses in human visual cortex</article-title><source>Neuron</source><volume>55</volume><fpage>313</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.030</pub-id><pub-id pub-id-type="pmid">17640531</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>NM</given-names></name><name><surname>Kuhl</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bottom-up and top-down factors differentially influence stimulus representations across large-scale attentional networks</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>2495</fpage><lpage>2504</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2724-17.2018</pub-id><pub-id pub-id-type="pmid">29437930</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Neural mechanisms of spatial selective attention in areas V1, V2, and V4 of macaque visual cortex</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>24</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.77.1.24</pub-id><pub-id pub-id-type="pmid">9120566</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>DS</given-names></name><name><surname>Correll</surname><given-names>J</given-names></name><name><surname>Wittenbrink</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The Chicago face database: a free stimulus set of faces and norming data</article-title><source>Behavior Research Methods</source><volume>47</volume><fpage>1122</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.3758/s13428-014-0532-5</pub-id><pub-id pub-id-type="pmid">25582810</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mack</surname><given-names>SC</given-names></name><name><surname>Eckstein</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Object co-occurrence serves as a contextual cue to guide and facilitate visual search in a natural viewing environment</article-title><source>Journal of Vision</source><volume>11</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1167/11.9.9</pub-id><pub-id pub-id-type="pmid">21856869</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malcolm</surname><given-names>GL</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Combining top-down processes to guide eye movements during real-world scene search</article-title><source>Journal of Vision</source><volume>10</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.1167/10.2.4</pub-id><pub-id pub-id-type="pmid">20462305</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malcolm</surname><given-names>GL</given-names></name><name><surname>Shomstein</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Object-based attention in real-world scenes</article-title><source>Journal of Experimental Psychology. General</source><volume>144</volume><fpage>257</fpage><lpage>263</lpage><pub-id pub-id-type="doi">10.1037/xge0000060</pub-id><pub-id pub-id-type="pmid">25844622</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>T</given-names></name><name><surname>Armstrong</surname><given-names>KM</given-names></name><name><surname>Fallah</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Visuomotor origins of covert spatial attention</article-title><source>Neuron</source><volume>40</volume><fpage>671</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00716-5</pub-id><pub-id pub-id-type="pmid">14622573</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muhle-Karbe</surname><given-names>PS</given-names></name><name><surname>Derrfuss</surname><given-names>J</given-names></name><name><surname>Lynn</surname><given-names>MT</given-names></name><name><surname>Neubert</surname><given-names>FX</given-names></name><name><surname>Fox</surname><given-names>PT</given-names></name><name><surname>Brass</surname><given-names>M</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Co-activation-based parcellation of the lateral prefrontal cortex delineates the inferior frontal junction area</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>2225</fpage><lpage>2241</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv073</pub-id><pub-id pub-id-type="pmid">25899707</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muhle-Karbe</surname><given-names>PS</given-names></name><name><surname>Jiang</surname><given-names>J</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Causal evidence for learning-dependent frontal lobe contributions to cognitive control</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>962</fpage><lpage>973</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1467-17.2017</pub-id><pub-id pub-id-type="pmid">29229706</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nah</surname><given-names>JC</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Thematic object pairs produce stronger and faster grouping than taxonomic pairs</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>48</volume><fpage>1325</fpage><lpage>1335</lpage><pub-id pub-id-type="doi">10.1037/xhp0001031</pub-id><pub-id pub-id-type="pmid">36442038</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nee</surname><given-names>DE</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The hierarchical organization of the lateral prefrontal cortex</article-title><source>eLife</source><volume>5</volume><elocation-id>e12112</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.12112</pub-id><pub-id pub-id-type="pmid">26999822</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nee</surname><given-names>DE</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Causal evidence for lateral prefrontal cortex dynamics supporting cognitive control</article-title><source>eLife</source><volume>6</volume><elocation-id>e28040</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.28040</pub-id><pub-id pub-id-type="pmid">28901287</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nee</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Integrative frontal-parietal dynamics supporting cognitive control</article-title><source>eLife</source><volume>10</volume><elocation-id>e57244</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57244</pub-id><pub-id pub-id-type="pmid">33650966</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nestor</surname><given-names>A</given-names></name><name><surname>Plaut</surname><given-names>DC</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Unraveling the distributed neural code of facial identity through spatiotemporal pattern analysis</article-title><source>PNAS</source><volume>108</volume><fpage>9998</fpage><lpage>10003</lpage><pub-id pub-id-type="doi">10.1073/pnas.1102433108</pub-id><pub-id pub-id-type="pmid">21628569</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neubert</surname><given-names>FX</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Thomas</surname><given-names>AG</given-names></name><name><surname>Sallet</surname><given-names>J</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Comparison of human ventral frontal cortex areas for cognitive control and language with areas in monkey frontal cortex</article-title><source>Neuron</source><volume>81</volume><fpage>700</fpage><lpage>713</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.012</pub-id><pub-id pub-id-type="pmid">24485097</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Connor</surname><given-names>DH</given-names></name><name><surname>Fukui</surname><given-names>MM</given-names></name><name><surname>Pinsk</surname><given-names>MA</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Attention modulates responses in the human lateral geniculate nucleus</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>1203</fpage><lpage>1209</lpage><pub-id pub-id-type="doi">10.1038/nn957</pub-id><pub-id pub-id-type="pmid">12379861</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Craven</surname><given-names>KM</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>fMRI evidence for objects as the units of attentional selection</article-title><source>Nature</source><volume>401</volume><fpage>584</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1038/44134</pub-id><pub-id pub-id-type="pmid">10524624</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olmos-Solis</surname><given-names>K</given-names></name><name><surname>van Loon</surname><given-names>AM</given-names></name><name><surname>Olivers</surname><given-names>CNL</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Content or status: Frontal and posterior cortical representations of object category and upcoming task goals in working memory</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>135</volume><fpage>61</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2020.11.011</pub-id><pub-id pub-id-type="pmid">33360761</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The What and How of prefrontal cortical organization</article-title><source>Trends in Neurosciences</source><volume>33</volume><fpage>355</fpage><lpage>361</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2010.05.002</pub-id><pub-id pub-id-type="pmid">20573407</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>JX</given-names></name><name><surname>Schüffelgen</surname><given-names>U</given-names></name><name><surname>Cuell</surname><given-names>SF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dissociable effects of surprise and model update in parietal and anterior cingulate cortex</article-title><source>PNAS</source><volume>110</volume><fpage>E3660</fpage><lpage>E3669</lpage><pub-id pub-id-type="doi">10.1073/pnas.1305373110</pub-id><pub-id pub-id-type="pmid">23986499</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panichello</surname><given-names>MF</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Shared mechanisms underlie the control of working memory and attention</article-title><source>Nature</source><volume>592</volume><fpage>601</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03390-w</pub-id><pub-id pub-id-type="pmid">33790467</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A neural basis for real-world visual search in human occipitotemporal cortex</article-title><source>PNAS</source><volume>108</volume><fpage>12125</fpage><lpage>12130</lpage><pub-id pub-id-type="doi">10.1073/pnas.1101042108</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Berlot</surname><given-names>E</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Predictive processing of scenes and objects</article-title><source>Nature Reviews Psychology</source><volume>3</volume><fpage>13</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1038/s44159-023-00254-0</pub-id><pub-id pub-id-type="pmid">38989004</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poskanzer</surname><given-names>C</given-names></name><name><surname>Aly</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Switching between external and internal attention in hippocampal networks</article-title><source>The Journal of Neuroscience</source><volume>43</volume><fpage>6538</fpage><lpage>6552</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0029-23.2023</pub-id><pub-id pub-id-type="pmid">37607818</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname><given-names>S</given-names></name><name><surname>van Marle</surname><given-names>HJF</given-names></name><name><surname>Hermans</surname><given-names>EJ</given-names></name><name><surname>Fernández</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Subjective sense of memory strength and the objective amount of information accurately remembered are related to distinct neural correlates at encoding</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>8920</fpage><lpage>8927</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2587-10.2011</pub-id><pub-id pub-id-type="pmid">21677175</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The normalization model of attention</article-title><source>Neuron</source><volume>61</volume><fpage>168</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.002</pub-id><pub-id pub-id-type="pmid">19186161</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossi</surname><given-names>AF</given-names></name><name><surname>Pessoa</surname><given-names>L</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The prefrontal cortex and the executive control of attention</article-title><source>Experimental Brain Research</source><volume>192</volume><fpage>489</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1007/s00221-008-1642-z</pub-id><pub-id pub-id-type="pmid">19030851</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruff</surname><given-names>CC</given-names></name><name><surname>Blankenburg</surname><given-names>F</given-names></name><name><surname>Bjoertomt</surname><given-names>O</given-names></name><name><surname>Bestmann</surname><given-names>S</given-names></name><name><surname>Freeman</surname><given-names>E</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name><name><surname>Josephs</surname><given-names>O</given-names></name><name><surname>Deichmann</surname><given-names>R</given-names></name><name><surname>Driver</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Concurrent TMS-fMRI and psychophysics reveal frontal influences on human retinotopic visual cortex</article-title><source>Current Biology</source><volume>16</volume><fpage>1479</fpage><lpage>1488</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.06.057</pub-id><pub-id pub-id-type="pmid">16890523</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaefer</surname><given-names>A</given-names></name><name><surname>Kong</surname><given-names>R</given-names></name><name><surname>Gordon</surname><given-names>EM</given-names></name><name><surname>Laumann</surname><given-names>TO</given-names></name><name><surname>Zuo</surname><given-names>XN</given-names></name><name><surname>Holmes</surname><given-names>AJ</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Yeo</surname><given-names>BTT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Local-global parcellation of the human cerebral cortex from intrinsic functional connectivity MRI</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>3095</fpage><lpage>3114</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx179</pub-id><pub-id pub-id-type="pmid">28981612</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Boynton</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Feature-based attentional modulations in the absence of direct visual stimulation</article-title><source>Neuron</source><volume>55</volume><fpage>301</fpage><lpage>312</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.015</pub-id><pub-id pub-id-type="pmid">17640530</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Threshold-free cluster enhancement: addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title><source>NeuroImage</source><volume>44</volume><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.061</pub-id><pub-id pub-id-type="pmid">18501637</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soon</surname><given-names>CS</given-names></name><name><surname>Namburi</surname><given-names>P</given-names></name><name><surname>Chee</surname><given-names>MWL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Preparatory patterns of neural activity predict visual category search speed</article-title><source>NeuroImage</source><volume>66</volume><fpage>215</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.10.036</pub-id><pub-id pub-id-type="pmid">23103518</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soyuhos</surname><given-names>O</given-names></name><name><surname>Baldauf</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Functional connectivity fingerprints of the frontal eye field and inferior frontal junction suggest spatial versus nonspatial processing in the prefrontal cortex</article-title><source>The European Journal of Neuroscience</source><volume>57</volume><fpage>1114</fpage><lpage>1140</lpage><pub-id pub-id-type="doi">10.1111/ejn.15936</pub-id><pub-id pub-id-type="pmid">36789470</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spisák</surname><given-names>T</given-names></name><name><surname>Spisák</surname><given-names>Z</given-names></name><name><surname>Zunhammer</surname><given-names>M</given-names></name><name><surname>Bingel</surname><given-names>U</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name><name><surname>Nichols</surname><given-names>T</given-names></name><name><surname>Kincses</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Probabilistic TFCE: A generalized combination of cluster size and voxel intensity to increase statistical power</article-title><source>NeuroImage</source><volume>185</volume><fpage>12</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.09.078</pub-id><pub-id pub-id-type="pmid">30296561</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stigliani</surname><given-names>A</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Temporal processing capacity in high-level visual cortex is domain specific</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>12412</fpage><lpage>12424</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4822-14.2015</pub-id><pub-id pub-id-type="pmid">26354910</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>M</given-names></name><name><surname>Thompson</surname><given-names>R</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Shape-specific preparatory activity mediates attention to targets in human visual cortex</article-title><source>PNAS</source><volume>106</volume><fpage>19569</fpage><lpage>19574</lpage><pub-id pub-id-type="doi">10.1073/pnas.0905306106</pub-id><pub-id pub-id-type="pmid">19887644</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Kusunoki</surname><given-names>M</given-names></name><name><surname>Sigala</surname><given-names>N</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Gaffan</surname><given-names>D</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic coding for cognitive control in prefrontal cortex</article-title><source>Neuron</source><volume>78</volume><fpage>364</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.039</pub-id><pub-id pub-id-type="pmid">23562541</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Perceptual classification in a rapidly changing environment</article-title><source>Neuron</source><volume>71</volume><fpage>725</fpage><lpage>736</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.06.022</pub-id><pub-id pub-id-type="pmid">21867887</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sylvester</surname><given-names>CM</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name><name><surname>Jack</surname><given-names>AI</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Asymmetry of anticipatory activity in visual cortex predicts the locus of attention and perception</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>14424</fpage><lpage>14433</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3759-07.2007</pub-id><pub-id pub-id-type="pmid">18160650</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tamber-Rosenau</surname><given-names>BJ</given-names></name><name><surname>Asplund</surname><given-names>CL</given-names></name><name><surname>Marois</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Functional dissociation of the inferior frontal junction from the dorsal attention network in top-down attentional control</article-title><source>Journal of Neurophysiology</source><volume>120</volume><fpage>2498</fpage><lpage>2512</lpage><pub-id pub-id-type="doi">10.1152/jn.00506.2018</pub-id><pub-id pub-id-type="pmid">30156458</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>KG</given-names></name><name><surname>Biscoe</surname><given-names>KL</given-names></name><name><surname>Sato</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neuronal basis of covert spatial attention in the frontal eye field</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>9479</fpage><lpage>9487</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0741-05.2005</pub-id><pub-id pub-id-type="pmid">16221858</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname><given-names>S</given-names></name><name><surname>Martínez Trujillo</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Feature-based attention influences motion processing gain in macaque visual cortex</article-title><source>Nature</source><volume>399</volume><fpage>575</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1038/21176</pub-id><pub-id pub-id-type="pmid">10376597</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tünnermann</surname><given-names>J</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Schubö</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>How feature context alters attentional template switching</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>47</volume><fpage>1431</fpage><lpage>1444</lpage><pub-id pub-id-type="doi">10.1037/xhp0000951</pub-id><pub-id pub-id-type="pmid">34591520</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turini</surname><given-names>J</given-names></name><name><surname>Võ</surname><given-names>MLH</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Hierarchical organization of objects in scenes is reflected in mental representations of objects</article-title><source>Scientific Reports</source><volume>12</volume><elocation-id>20068</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-022-24505-x</pub-id><pub-id pub-id-type="pmid">36418411</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Loon</surname><given-names>AM</given-names></name><name><surname>Olmos-Solis</surname><given-names>K</given-names></name><name><surname>Fahrenfort</surname><given-names>JJ</given-names></name><name><surname>Olivers</surname><given-names>CN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Current and future goals are represented in opposite patterns in object-selective cortex</article-title><source>eLife</source><volume>7</volume><elocation-id>e38677</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.38677</pub-id><pub-id pub-id-type="pmid">30394873</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Võ</surname><given-names>ML-H</given-names></name><name><surname>Boettcher</surname><given-names>SE</given-names></name><name><surname>Draschkow</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reading scenes: how scene grammar guides attention and aids perception in real-world environments</article-title><source>Current Opinion in Psychology</source><volume>29</volume><fpage>205</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.copsyc.2019.03.009</pub-id><pub-id pub-id-type="pmid">31051430</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vossel</surname><given-names>S</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name><name><surname>Fink</surname><given-names>GR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dorsal and ventral attention systems: distinct neural circuits but collaborative roles</article-title><source>The Neuroscientist</source><volume>20</volume><fpage>150</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1177/1073858413494269</pub-id><pub-id pub-id-type="pmid">23835449</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>DB</given-names></name><name><surname>Caddigan</surname><given-names>E</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name><name><surname>Beck</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Natural scene categories revealed in distributed patterns of activity in the human brain</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>10573</fpage><lpage>10581</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0559-09.2009</pub-id><pub-id pub-id-type="pmid">19710310</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willenbockel</surname><given-names>V</given-names></name><name><surname>Sadr</surname><given-names>J</given-names></name><name><surname>Fiset</surname><given-names>D</given-names></name><name><surname>Horne</surname><given-names>GO</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Tanaka</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Controlling low-level image properties: the SHINE toolbox</article-title><source>Behavior Research Methods</source><volume>42</volume><fpage>671</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.3758/BRM.42.3.671</pub-id><pub-id pub-id-type="pmid">20805589</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witkowski</surname><given-names>PP</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Prefrontal cortex codes representations of target identity and feature uncertainty</article-title><source>The Journal of Neuroscience</source><volume>43</volume><fpage>8769</fpage><lpage>8776</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1117-23.2023</pub-id><pub-id pub-id-type="pmid">37875376</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>R</given-names></name><name><surname>Bichot</surname><given-names>NP</given-names></name><name><surname>Takahashi</surname><given-names>A</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The cortical connectome of primate lateral prefrontal cortex</article-title><source>Neuron</source><volume>110</volume><fpage>312</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.10.018</pub-id><pub-id pub-id-type="pmid">34739817</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Q</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The neural codes underlying internally generated representations in visual working memory</article-title><source>Journal of Cognitive Neuroscience</source><volume>33</volume><fpage>1142</fpage><lpage>1157</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01702</pub-id><pub-id pub-id-type="pmid">34428785</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>X</given-names></name><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Attentional guidance and match decisions rely on different template information during visual search</article-title><source>Psychological Science</source><volume>33</volume><fpage>105</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1177/09567976211032225</pub-id><pub-id pub-id-type="pmid">34878949</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>X</given-names></name><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Becker</surname><given-names>SI</given-names></name><name><surname>Boettcher</surname><given-names>SEP</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Good-enough attentional guidance</article-title><source>Trends in Cognitive Sciences</source><volume>27</volume><fpage>391</fpage><lpage>403</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2023.01.007</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zanto</surname><given-names>TP</given-names></name><name><surname>Rubens</surname><given-names>MT</given-names></name><name><surname>Bollinger</surname><given-names>J</given-names></name><name><surname>Gazzaley</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Top-down modulation of visual feature processing: the role of the inferior frontal junction</article-title><source>NeuroImage</source><volume>53</volume><fpage>736</fpage><lpage>745</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.06.012</pub-id><pub-id pub-id-type="pmid">20600999</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Learned associations serve as target proxies during difficult but not easy visual search</article-title><source>Cognition</source><volume>242</volume><elocation-id>105648</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2023.105648</pub-id><pub-id pub-id-type="pmid">37897882</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104041.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Meng</surname><given-names>Ming</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of Alabama at Birmingham</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study decoded target-associated information in prefrontal and sensory cortex during the preparatory period of a visual search task, suggesting a memory component of human subjects performing such visual attention task. The evidence supporting this claim is <bold>compelling</bold>, based on multivariate pattern analyses of fMRI data. The results will be of interest to psychologists and cognitive neuroscientists.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104041.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>When you search for something, you need to maintain some representation (a &quot;template&quot;) of that target in your mind/brain. Otherwise, how would you know what you were looking for? If your phone is in a shocking pink case, you can guide your attention to pink things based on a target template that includes the attribute 'pink'. That guidance should get you to the phone pretty effectively, if it is in view. Most real-world searches are more complicated. If you are looking for the toaster, you will make use of your knowledge of where toasters can be. Thus, if you are asked to find a toaster, you might first activate a template of a kitchen or a kitchen counter. You might worry about pulling up the toaster template only after you are reasonably sure you have restricted your attention to a sensible part of the scene.</p><p>Zhou and Geng are looking for evidence of this early stage of guidance by information about the surrounding scene in a search task. They train Os to associate four faces with four places. Then, with Os in the scanner, they show one face - the target for a subsequent search. After an 8 sec delay, they show a search display where the face is placed on the associated scene 75% of the time. Thus, attending to the associated scene is a good idea. The questions of interest are &quot;When can the experimenters decode which face Os saw from fMRI recording?&quot; &quot;When can the experimenters decode the associated scene?&quot; and &quot;Where in the brain can the experimenters see evidence of this decoding? The answer is that the face but not the scene can be read out during the face's initial presentation. The key finding is that the scene can be read out (imperfectly but above chance) during the subsequent delay when Os are looking at just a fixation point. Apparently, seeing the face conjures up the scene in the mind's eye.</p><p>This is a solid and believable result. The only issue, for me, is whether it is telling us anything specifically about search. Suppose you trained Os on the face-scene pairing but never did anything connected to search. If you presented the face, would you not see evidence of recall of the associated scene? Maybe you would see the activation of the scene in different areas and you could identify some areas as search specific. I don't think anything like that was discussed here.</p><p>You might also expect this result to be asymmetric. The idea is that the big scene gives the search information about the little face. The face should activate the larger useful scene more than the scene should activate the more incidental face, if the task was reversed. That might be true if finding is related to search where the scene context is presumed to be the useful attention guiding stimulus. You might not expect an asymmetry if Os were just learning an association.</p><p>It is clear in this study that the face and the scene have been associated and that this can be seen in the fMRI data. It is also clear that a valid scene background speeds the behavioral response in the search task. The linkage between these two results is not entirely clear but perhaps future research will shed more light.</p><p>It is also possible that I missed the clear evidence of the search-specific nature of the activation by the scene during the delay period. If so, I apologize and suggest that the point be underlined for readers like me.</p><p>Comments on revised version:</p><p>I am satisfied with the revision.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104041.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This work is one of the best instances of a well-controlled experiment and theoretically impactful findings within the literature on templates guiding attentional selection. I am a fan of the work that comes out of this lab and this particular manuscript is an excellent example as to why that is the case. Here, the authors use fMRI (employing MVPA) to test whether during the preparatory search period, a search template is invoked within the corresponding sensory regions, in the absence of physical stimulation. By associating faces with scenes, a strong association was created between two types of stimuli that recruit very specific neural processing regions - FFA for faces and PPA for scenes. The critical results showed that scene information that was associated with a particular cue could be decoded from PPA during the delay period. This result strongly supports invoking of a very specific attentional template.</p><p>Strengths:</p><p>There is so much to be impressed with in this report. The writing of the manuscript is incredibly clear. The experimental design is clever and innovative. The analysis is sophisticated and also innovative. The results are solid and convincing.</p><p>Weaknesses:</p><p>I only have a few weaknesses to point out.</p><p>This point is not so much of a weakness, but a further test of the hypothesis put forward by the authors. The delay period was long - 8 seconds. It would be interesting to split the delay period into the first 4seconds and the last 4seconds and run the same decoding analyses. The hypothesis here is that semantic associations take time to evolve, and it would be great to show that decoding gets stronger in the second delay period as opposed to the period right after the cue. I think it would be a stronger test of the template hypothesis.</p><p>Typo in the abstract &quot;curing&quot; vs &quot;during.&quot;</p><p>It is hard to know what to do with significant results in ROIs that are not motivated by specific hypotheses. However, for Figure 3, what are explanations for ROIs that show significant differences above and beyond the direct hypotheses set out by the authors?</p><p>Following the revision, I have no further comments or concerns.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104041.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The manuscript contains a carefully designed fMRI study, using MVPA patter analysis to investigate which high-level associate cortices contain target-related information to guide visual search. A special focus is hereby on so-called 'target-associated' information, that has previously been shown to help in guiding attention during visual search. For this purpose the author trained their participants and made them learn specific target-associations, in order to then test which brain regions may contain neural representations of those learnt associations. They found that at least some of the associations tested were encoded in prefrontal cortex during the cue and delay period.</p><p>The manuscript is very carefully prepared. As far as I can see, the statistical analyses are all sound and the results integrate well with previous findings.</p><p>I have no strong objections against the presented results and their interpretation.</p><p>The authors have addressed all my previous comments and questions in their revision of the text.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104041.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Zhiheng</given-names></name><role specific-use="author">Author</role><aff><institution>College of Psychology, Sichuan Normal University</institution><addr-line><named-content content-type="city">Chengdu</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Geng</surname><given-names>Joy</given-names></name><role specific-use="author">Author</role><aff><institution>University of California Davis</institution><addr-line><named-content content-type="city">Davis</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>When you search for something, you need to maintain some representation (a &quot;template&quot;) of that target in your mind/brain. Otherwise, how would you know what you were looking for? If your phone is in a shocking pink case, you can guide your attention to pink things based on a target template that includes the attribute 'pink'. That guidance should get you to the phone pretty effectively if it is in view. Most real-world searches are more complicated. If you are looking for the toaster, you will make use of your knowledge of where toasters can be. Thus, if you are asked to find a toaster, you might first activate a template of a kitchen or a kitchen counter. You might worry about pulling up the toaster template only after you are reasonably sure you have restricted your attention to a sensible part of the scene.</p><p>Zhou and Geng are looking for evidence of this early stage of guidance by information about the surrounding scene in a search task. They train Os to associate four faces with four places. Then, with Os in the scanner, they show one face - the target for a subsequent search. After an 8 sec delay, they show a search display where the face is placed on the associated scene 75% of the time. Thus, attending to the associated scene is a good idea. The questions of interest are &quot;When can the experimenters decode which face Os saw from fMRI recording?&quot; &quot;When can the experimenters decode the associated scene?&quot; and &quot;Where in the brain can the experimenters see evidence of this decoding? The answer is that the face but not the scene can be read out during the face's initial presentation. The key finding is that the scene can be read out (imperfectly but above chance) during the subsequent delay when Os are looking at just a fixation point. Apparently, seeing the face conjures up the scene in the mind's eye.</p><p>This is a solid and believable result. The only issue, for me, is whether it is telling us anything specifically about search. Suppose you trained Os on the face-scene pairing but never did anything connected to the search. If you presented the face, would you not see evidence of recall of the associated scene? Maybe you would see the activation of the scene in different areas and you could identify some areas as search specific. I don't think anything like that was discussed here.</p><p>You might also expect this result to be asymmetric. The idea is that the big scene gives the search information about the little face. The face should activate the larger useful scene more than the scene should activate the more incidental face, if the task was reversed. That might be true if the finding is related to a search where the scene context is presumed to be the useful attention guiding stimulus. You might not expect an asymmetry if Os were just learning an association.</p><p>It is clear in this study that the face and the scene have been associated and that this can be seen in the fMRI data. It is also clear that a valid scene background speeds the behavioral response in the search task. The linkage between these two results is not entirely clear but perhaps future research will shed more light.</p><p>It is also possible that I missed the clear evidence of the search-specific nature of the activation by the scene during the delay period. If so, I apologize and suggest that the point be underlined for readers like me.</p></disp-quote><p>We have added text related to this issue, particularly in the discussion (page 19, line 6), and have also added citations of studies in humans and non-human primates showing a causal relationship between preparatory activity in prefrontal and visual cortex and visual search performance (page 6, line 16).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>This work is one of the best instances of a well-controlled experiment and theoretically impactful findings within the literature on templates guiding attentional selection. I am a fan of the work that comes out of this lab and this particular manuscript is an excellent example as to why that is the case. Here, the authors use fMRI (employing MVPA) to test whether during the preparatory search period, a search template is invoked within the corresponding sensory regions, in the absence of physical stimulation. By associating faces with scenes, a strong association was created between two types of stimuli that recruit very specific neural processing regions - FFA for faces and PPA for scenes. The critical results showed that scene information that was associated with a particular cue could be decoded from PPA during the delay period. This result strongly supports the invoking of a very specific attentional template.</p><p>Strengths:</p><p>There is so much to be impressed with in this report. The writing of the manuscript is incredibly clear. The experimental design is clever and innovative. The analysis is sophisticated and also innovative. The results are solid and convincing.</p><p>Weaknesses:</p><p>I only have a few weaknesses to point out.</p><p>This point is not so much of a weakness, but a further test of the hypothesis put forward by the authors. The delay period was long - 8 seconds. It would be interesting to split the delay period into the first 4seconds and the last 4seconds and run the same decoding analyses. The hypothesis here is that semantic associations take time to evolve, and it would be great to show that decoding gets stronger in the second delay period as opposed to the period right after the cue. I don't think this is necessary for publication, but I think it would be a stronger test of the template hypothesis.</p></disp-quote><p>We conducted the suggested analysis, and we did not find clear evidence of differences in decoding scene information between the earlier and later portions of the delay period. This may be due to insufficient power when the data are divided, individual differences in when preparatory activation is the strongest, or truly no difference in activation over the delay period. More details of this analysis can be found in the supplementary materials (page 12, line 16; Figure S1).</p><disp-quote content-type="editor-comment"><p>Type in the abstract &quot;curing&quot; vs &quot;during.&quot;</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>It is hard to know what to do with significant results in ROIs that are not motivated by specific hypotheses. However, for Figure 3, what are the explanations for ROIs that show significant differences above and beyond the direct hypotheses set out by the authors?</p></disp-quote><p>We added reasoning for the other a priori ROIs in the introduction (page 4, line 26). There is substantial evidence suggesting that frontoparietal areas are involved in cognitive control, attentional control, and working memory. The ROIs we selected from frontal and parietal cortex are based on parcels within resting state networks defined by the s17-network atlases (Schaefer et al., 2018). The IFJ was defined by the HCP-MMP1 (Glasser et al., 2016). These regions are commonly used in studies of attention and cognitive control, and the exact ROIs selected are described in the section on “Regions of interest (ROI) definition”. While we have the strongest hypothesis for IFJ based on relatively recent work from the Desimone lab, the other ROIs in lateral frontal cortex and parietal cortex, are also well documented in similar studies, although the exact computation being done by these regions during tasks can be hard to differentiate with fMRI.\</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public review):</bold></p><p>The manuscript contains a carefully designed fMRI study, using MVPA pattern analysis to investigate which high-level associate cortices contain target-related information to guide visual search. A special focus is hereby on so-called 'target-associated' information, that has previously been shown to help in guiding attention during visual search. For this purpose the author trained their participants and made them learn specific target-associations, in order to then test which brain regions may contain neural representations of those learnt associations. They found that at least some of the associations tested were encoded in prefrontal cortex during the cue and delay period.</p><p>The manuscript is very carefully prepared. As far as I can see, the statistical analyses are all sound and the results integrate well with previous findings.</p><p>I have no strong objections against the presented results and their interpretation.</p><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>One bit of trivia. In the abstract, you should define IFJ on its first appearance in the text. You get to that a bit later.</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>I really don't have much to suggest, as I thought that this was a clearly written report that offered a clever paradigm and data that supported the conclusions. My only suggestion would be to split the delay period activity and test whether the strength of the template evolves over time. Even though fMRI is not the best tool for this, still you would predict stronger decoding in the second half of the delay period</p></disp-quote><p>Please see above for our response to the same comment.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations for the authors):</bold></p><p>I would just like to point out some minor aspects that might be worth improving before publishing this work.</p><p>Abstract: While in general, the writing is clear and concise, I felt that the abstract of the manuscript was particularly hard to follow, probably because the authors at some point re-arranged individual sentences. For example, they write in line 12 about 'the preparatory period', but explain only in the following sentence that the preparatory period ensues 'before search begins'. This made it a bit hard to follow the overall logic and I think could easily be fixed.</p></disp-quote><p>We have addressed this comment and updated the abstract.</p><disp-quote content-type="editor-comment"><p>Also in the abstract: 'The CONTENTS of the template typically CONTAIN...' sounds weird, no? Also, 'information is used to modulate sensory processing in preparation for guiding attention during search' sounds like a very over-complicated description of attentional facilitation. I'm not convinced either whether the sequence is correct here. Is the information really used to (first) modulate sensory processing (which is a sort of definition of attention in itself) to (then) prepare the guidance of attention in visual search?</p></disp-quote><p>We have addressed this comment and updated the abstract.</p><disp-quote content-type="editor-comment"><p>The sentence in line 7, 'However, many behavioral studies have shown that target-associated information is used to guide attention,...' (and the following sentence) assumes that the reader is somewhat familiar with the term 'target-associations'. I'm afraid that, for a naive reader, this term may only become fully understandable once the idea is introduced a bit later when mentioning that participants of the study were trained on face-scene pairings. I think it could help to give some very short explanation of 'target-associations' already when it is first mentioned. The term 'statistically co-occurring object pairs', for example, could be of great help here.</p></disp-quote><p>Thank you for the suggestion. We have added it to the abstract.</p><disp-quote content-type="editor-comment"><p>page 2, line 22: 'prefrotnal'</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>page 2, line 24/25: 'information ... can SUPPLANT (?) ... information'. (That's also a somewhat unfortunate repetition of 'information')</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>page 4, line 23-25: 'Working memory representations in lateral prefrontal and parietal regions are engaged in cognitive control computations that ARE (?) task non-specific but essential to their functioning'</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>page 7, line 1: maybe a comma before 'suggesting'?</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>page 7, line 14-16: Something seems wrong with this sentence: 'The distractor face was a race-gender match, which we previously FOUND MADE (?) target discrimination difficult enough to make the scene useful for guiding attention'</p></disp-quote><p>We have addressed this comment and rewritten this part (now on page 7, line 18).</p><disp-quote content-type="editor-comment"><p>Results / Discussion sections:</p><p>In several figures, like in Fig3A, the three different IFJ regions, are grouped separately from the other frontal areas, which makes sense given the special role IFJ plays for representing task-related templates. However, IFJ is still part of PFC. I think it would be more correct to group the other frontal areas (like FEF vLPFC etc.) as 'Other Frontal' or even 'Other PFC'.</p></disp-quote><p>We have made the changes based on the reviewer’s suggestion.</p><disp-quote content-type="editor-comment"><p>In some of the Figures, e.g. Fig 3 and 5, I had the impression that the activation patterns of some conditions in vLPFC were rather close to the location of IFJ, which is just a bit posterior. I think I remember that functional localisers of IFJ can actually vary quite a bit in localisation (see e.g. in the Baldauf/Desimone paper). Also, I think it has been shown in the context of other regions, like the human FEF that its position when defined by localisation tasks is not always nicely and fully congruent with the respective labels in an atlas like the Glasser atlas. It might help to take this in consideration when discussing the results, particularly since the term vLPFC is a rather vague collection of several brain parcels and not a parcel name in the Glasser atlas. Some people might even argue that vLPFC in the broad sense contains IFJ, similar to how 'Frontal' contains IFJ (see above). How strong of a point do the authors want to make about activation in IFJ versus in vlPFC?</p></disp-quote><p>We have now added text discussing the inability to truly differentiate between subregions of IFJ and other parts of vLPFC in the methods section on ROIs (page 25, line 13) and in the discussion (page 18, line 25). However, one might think that it is even more surprising given the likely imprecision of ROI boundaries that we see distinct patterns between the subregions of IFG defined by Glasser HCP-MMP1 and the other vLPFC regions defined by the 17-network atlases. We do not wish to overstate the precision of IFJ regions, but note the ROI results within the context of the larger literature. We are sure that our findings will have to be reinterpreted when newer methods allow for better localization of functional subregions of the vLPFC in individuals.</p><disp-quote content-type="editor-comment"><p>Given that the authors nicely explain in the introduction how important templates are in visual search, and given that FEF has such an important role in serially guiding saccades through visual search templates, I think it would be worth discussing the finding that FEF did not hold representation of these targets. Of course, this could be in part due to the specific task at hand, but it may still be interesting to note in the Discussion section that here FEF, although important for some top-down attention signals, did not keep representations of the 'search' templates. Is it because there is no spatial component to the task at hand (like proposed in Bedini 2021)?</p></disp-quote><p>We have now added text directly addressing this point and citing the Bedini et al. paper in the discussion (page 18, line 18). Besides our current findings, the relationship between IFJ and FEF is really interesting and will hopefully be investigated more in the future.</p><disp-quote content-type="editor-comment"><p>Page 18, line 5: 'we the(N) associated...'</p></disp-quote><p>Fixed.</p></body></sub-article></article>