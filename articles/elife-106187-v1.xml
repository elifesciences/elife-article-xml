<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">106187</article-id><article-id pub-id-type="doi">10.7554/eLife.106187</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Review Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Medicine</subject></subj-group></article-categories><title-group><article-title>Critique of impure reason: Unveiling the reasoning behaviour of medical large language models</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Sim</surname><given-names>Shamus Zi Yang</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0000-1701-7747</contrib-id><email>shamus@qmed.asia</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Chen</surname><given-names>Tyrone</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9207-0385</contrib-id><email>tyrone.chen@petermac.org</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>QueueMed Healthtech</institution><addr-line><named-content content-type="city">Kuala Lumpur</named-content></addr-line><country>Malaysia</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02a8bt934</institution-id><institution>Peter MacCallum Cancer Centre</institution></institution-wrap><addr-line><named-content content-type="city">Melbourne</named-content></addr-line><country>Australia</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Yang</surname><given-names>Yongliang</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03ns6aq57</institution-id><institution>Shanghai University of Medicine and Health Sciences</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moses</surname><given-names>Alan M</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>University of Toronto</institution></institution-wrap><country>Canada</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>28</day><month>10</month><year>2025</year></pub-date><volume>14</volume><elocation-id>e106187</elocation-id><history><date date-type="received" iso-8601-date="2025-01-28"><day>28</day><month>01</month><year>2025</year></date><date date-type="accepted" iso-8601-date="2025-10-07"><day>07</day><month>10</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2024-12-20"><day>20</day><month>12</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2412.15748"/></event></pub-history><permissions><copyright-statement>© 2025, Sim and Chen</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Sim and Chen</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-106187-v1.pdf"/><abstract><p>Despite the current ubiquity of large language models (LLMs) across the medical domain, there is a surprising lack of studies which address their <italic>reasoning behaviour</italic>. We emphasise the importance of understanding <italic>reasoning behaviour</italic> as opposed to high-level prediction accuracies, since it is equivalent to explainable AI (XAI) in this context. In particular, achieving XAI in medical LLMs used in the clinical domain will have a significant impact across the healthcare sector. Therefore, in this work, we adapt the existing concept of <italic>reasoning behaviour</italic> and articulate its interpretation within the specific context of medical LLMs. We survey and categorise current state-of-the-art approaches for modelling and evaluating <italic>reasoning</italic> in medical LLMs. Additionally, we propose theoretical frameworks which can empower medical professionals or machine learning engineers to gain insight into the low-level reasoning operations of these previously obscure models. We also outline key open challenges facing the development of <italic>large reasoning models</italic>. The subsequent increased transparency and trust in medical machine learning models by clinicians as well as patients will accelerate the integration, application as well as further development of medical AI for the healthcare system as a whole.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>natural language processing</kwd><kwd>large language models</kwd><kwd>reasoning behaviour</kwd><kwd>explainable AI</kwd><kwd>deep learning</kwd><kwd>medical AI</kwd></kwd-group><funding-group><funding-statement>No external funding was received for this work.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A survey of reasoning behaviour in medical large language models uncovers emerging trends, highlights open challenges, and introduces theoretical frameworks that enhance reasoning behaviour transparency, ultimately fostering greater trust among clinicians, developers, and patients in their deployment.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p><italic>Reasoning</italic> drives problem-solving activities and is ubiquitous in our daily lives. The rising adoption of the field of artificial intelligence and its proximity to the concept of reasoning then naturally provokes the question: what is the <italic>reasoning behaviour</italic> of machine learning models commonly used in artificial intelligence (<xref ref-type="fig" rid="fig1">Figure 1</xref>)? This is particularly pertinent with regard to the increasing use of large language models (LLMs).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>A graphical abstract illustrating the current state of medical large language models (LLMs) in the context of <italic>reasoning behaviour</italic>.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106187-fig1-v1.tif"/></fig><p>In this review, we focus specifically on <bold>transformer-based LLMs</bold>, which are built on the transformer architecture, an attention-based mechanism capable of capturing complex dependencies in sequential data (<xref ref-type="bibr" rid="bib84">Vaswani et al., 2017</xref>). These models are characterised by a large number of trainable parameters and are pre-trained on vast corpora of textual data.</p><p>We define medical LLMs in an application-centric manner, referring broadly to any LLMs that are employed as a core component in medical or clinical workflows. This broad definition accommodates a diverse range of use cases, including applications in diagnosis (<xref ref-type="bibr" rid="bib97">Yang et al., 2024a</xref>; <xref ref-type="bibr" rid="bib58">Nori et al., 2025</xref>; <xref ref-type="bibr" rid="bib65">Savage et al., 2024</xref>), medical image analysis (<xref ref-type="bibr" rid="bib61">Pan et al., 2025</xref>; <xref ref-type="bibr" rid="bib44">Lai et al., 2025</xref>), clinical summarisation (<xref ref-type="bibr" rid="bib109">Zi Yang et al., 2024</xref>), and EHR question-answering tasks (<xref ref-type="bibr" rid="bib70">Shi et al., 2024</xref>). This includes both domain-specific models pre-trained or fine-tuned on biomedical corpora, as well as general-purpose LLMs (e.g. GPT or LLAMA models) that are adapted or prompted for use in medical settings. While such general models may not have been originally developed for healthcare, they are increasingly leveraged in a wide range of medical applications and thus fall within the scope of our discussion. In this review, we observed the frequent use of various variants of GPT- and LLaMA-based models throughout the literature. Due to their sheer scale and complexity, LLMs are inherently less interpretable than traditional machine learning models, which are themselves often considered ‘black boxes’ under normal circumstances. We note, however, that although LLMs have also been applied directly to genomic and molecular sequence data (<xref ref-type="bibr" rid="bib12">Chen et al., 2023</xref>) with potential medical use, the present review does not cover these applications.</p><p>The question of <italic>how</italic> LLMs arrive at their answers—particularly in high-stakes applications like medicine—is surprisingly underexplored. This gap is especially striking given the widespread deployment of these models across domains, often without a comprehensive understanding of their underlying reasoning mechanisms. Instead, evaluations tend to focus on performance metrics such as accuracy, F1 scores, precision, and recall. These metrics are typically benchmarked against curated subsets of state-of-the-art (SOTA) models as well as specialised datasets which consist of medical licence examination questions from the United States (USMLE), Mainland China (MCMLE), Taiwan (TWMLE) (<xref ref-type="bibr" rid="bib38">Jin et al., 2020</xref>), India (AIIMS/NEET) (<xref ref-type="bibr" rid="bib60">Pal et al., 2022</xref>), and other broader questions less directly related to clinical fields (<xref ref-type="bibr" rid="bib37">Jin et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Hendrycks et al., 2020</xref>). While these may be effective in some cases, such metrics provide limited insight into the complex and obscure inferential processes that LLMs apply to generate answers.</p><p>This neglect in understanding <italic>reasoning behaviour</italic> leads to their unintentional misuse with direct real-world effects, including data fabrication (<xref ref-type="bibr" rid="bib51">Lorek, 2024</xref>), false accusations (<xref ref-type="bibr" rid="bib24">Gegg-Harrison and Quarterman, 2024</xref>), and suicide (<xref ref-type="bibr" rid="bib17">Dewitte, 2024</xref>). Further obfuscating <italic>reasoning behaviour</italic> in LLMs, particularly generative models, is their ability to mimic the semantics of question and answer processes convincingly while being surprisingly accurate, to the point where individuals have mistakenly assumed their sentience (<xref ref-type="bibr" rid="bib79">Tiku, 2022</xref>). Such issues are amplified in LLMs used for medical purposes, given their proximity to life-and-death decisions, for example, in the case of acute heart failure (<xref ref-type="bibr" rid="bib42">Kwon et al., 2019</xref>).</p><p>We highlight the importance of gaining insight into the process-driven, <italic>reasoning behaviour</italic> by observing its functional similarity to <italic>interpretability metrics</italic> in machine learning (<xref ref-type="bibr" rid="bib75">Sundararajan et al., 2017</xref>; <xref ref-type="bibr" rid="bib67">Selvaraju et al., 2017</xref>; <xref ref-type="bibr" rid="bib103">Zhang et al., 2021</xref>; <xref ref-type="bibr" rid="bib2">Alammar, 2021</xref>; <xref ref-type="bibr" rid="bib78">Tenney et al., 2020</xref>). In both cases, the aim is to inspect the learning process of the model, with corresponding metrics used to gain information into how a model is correctly or incorrectly predisposed towards a certain outcome, in an attempt to address the common ‘black-box’ problem of machine learning models (<xref ref-type="fig" rid="fig2">Figure 2</xref>). We re-emphasise the lack of LLM studies tackling the question of general inference to focus on a noticeable gap in the field: <bold>remarkably few studies investigate</bold> <italic><bold>reasoning behaviour</bold></italic> <bold>in the medical LLM space</bold>.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>An illustration of the contrast in modalities between computer vision and natural language processing.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106187-fig2-v1.tif"/></fig><p>Given the increased stakes of medical LLMs in clinical decision making, achieving a deeper understanding of medical LLMs carries a greater weight than with general-purpose LLMs. Thus, their intense scrutiny by both medical experts and the general public is unsurprising. Therefore, it is necessary to supplement clinicians with insight into the <italic>reasoning behaviour</italic> of medical LLMs to better understand how they arrive at their conclusions and expose potential logical fallacies. An ability for LLMs to provide reasoning for their outputs, for example, in a medical recommendation or diagnosis, allows clinicians to clarify discrepancies between machine and expert suggestions. This transparency fosters trust, encouraging integration of LLMs and other machine learning models into clinical decisions and subsequently improving patient outcomes.</p><p>In our review, we will address a few specific points:</p><list list-type="order" id="list1"><list-item><p>We provide a primer introducing fundamental AI concepts covered in this review.</p></list-item><list-item><p>We adopt the existing concept of <italic>reasoning behaviour</italic> and articulate its interpretation within the specific context of medical LLMs.</p></list-item><list-item><p>We discuss the importance of evaluating <italic>reasoning behaviour</italic> in addition to performance metrics</p></list-item><list-item><p>We compare and contrast the current SOTA in <italic>reasoning behaviour</italic> for the medical field, and note a surprising lack of such studies.</p></list-item><list-item><p>We propose strategies to improve and evaluate the <italic>reasoning behaviour</italic> of medical LLMs, which will grant greater transparency</p></list-item></list></sec><sec id="s2"><title>Primer of foundational concepts in reasoning</title><sec id="s2-1"><title>Prompting methods for reasoning</title><p>Prompting methods are lightweight techniques that guide LLMs to perform more structured reasoning without additional training. In this section, we focus on two representative prompting-based strategies: chain-of-thought (CoT) prompting, which encourages sequential reasoning through intermediate steps, and tree-of-thought (ToT) prompting, which allows the model to explore multiple reasoning paths through structured search and self-evaluation.</p><sec id="s2-1-1"><title>Chain of thought reasoning</title><p>Chain-of-Thought (CoT)CoT (<xref ref-type="fig" rid="fig3">Figure 3</xref>) reasoning is a prompting technique used to improve an LLM’s ability to solve reasoning-related problems. Rather than producing an answer directly, the model decomposes the problem into a sequence of intermediate steps, thereby facilitating more structured and transparent reasoning (<xref ref-type="bibr" rid="bib90">Wei et al., 2022</xref>). However, in some cases, LLMs may still generate incorrect intermediate steps (<xref ref-type="bibr" rid="bib86">Wang et al., 2022</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>A schematic diagram illustrating different strategies for solving problems using large language models (LLMs).</title><p>Each rectangular box represents a ‘thought’—a meaningful segment of language that functions as an intermediate step in the reasoning or problem-solving process.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106187-fig3-v1.tif"/></fig></sec><sec id="s2-1-2"><title>Tree of thought reasoning</title><p>ToT (<xref ref-type="fig" rid="fig3">Figure 3</xref>) is an advanced reasoning framework that extends the problem-solving capabilities of LLMs. (<xref ref-type="bibr" rid="bib99">Yao et al., 2023</xref>). Instead of token-by-token generating responses in a linear fashion, ToT prompts the model to generate coherent reasoning steps or ‘thoughts’, which are organised into a tree structure (<xref ref-type="bibr" rid="bib99">Yao et al., 2023</xref>). Each node in this tree represents a partial solution, and the model can explore multiple possible continuations (branching), evaluate their promise using heuristic reasoning (state evaluation), and make decisions through structured search strategies such as breadth-first or depth-first search (<xref ref-type="bibr" rid="bib99">Yao et al., 2023</xref>). This modular approach includes four core components: decomposing problems into thought steps, generating multiple candidate thoughts, heuristically evaluating them, and using search algorithms to navigate the tree (<xref ref-type="bibr" rid="bib99">Yao et al., 2023</xref>). By combining these elements, ToT enables LLMs to reason with greater depth, backtrack to previous nodes when necessary, and make more globally informed decisions, features that are particularly relevant for complex biomedical tasks such as differential diagnosis or clinical planning.</p></sec></sec><sec id="s2-2"><title>Agentic-based methods</title><p>Agent-based methods for LLM reasoning are an emerging paradigm designed to enhance the problem-solving capabilities of LLMs by structuring their operations as autonomous or semi-autonomous agents. These approaches address the limitations of standard LLM prompting by introducing explicit planning, memory, iterative decision-making, and tool usage.</p><p>In the medical domain, agent-based methods leverage LLMs as modular agents, each assigned a specific function. Examples include clinical triage, medical literature retrieval, summarisation of patient data (<xref ref-type="bibr" rid="bib109">Zi Yang et al., 2024</xref>), decision support, or guideline compliance checking.</p><p>Common features of agent-based medical reasoning with LLMs include</p><list list-type="bullet" id="list2"><list-item><p><bold>Iterative planning:</bold> The model decomposes complex clinical problems into sub-questions, enabling step-by-step analysis and dynamic adjustments as new information becomes available (<xref ref-type="bibr" rid="bib89">Wang et al., 2025</xref>).</p></list-item><list-item><p><bold>Memory integration:</bold> Some agents maintain both short- and long-term memory, allowing them to track patient history, previous actions, and evolving diagnostic hypotheses over time (<xref ref-type="bibr" rid="bib89">Wang et al., 2025</xref>).</p></list-item><list-item><p><bold>Tool augmentation:</bold> LLM agents can interact with external tools—such as medical databases like EHRs (<xref ref-type="bibr" rid="bib70">Shi et al., 2024</xref>), drug databases (<xref ref-type="bibr" rid="bib102">Yue et al., 2024</xref>), or medical knowledge graphs (<xref ref-type="bibr" rid="bib102">Yue et al., 2024</xref>), medical calculators (<xref ref-type="bibr" rid="bib106">Zhu et al., 2024</xref>), or literature search engines—to retrieve up-to-date information and perform specialised computations.</p></list-item><list-item><p><bold>Reflection:</bold> Agents incorporate feedback mechanisms (<xref ref-type="bibr" rid="bib34">Hong et al., 2024</xref>) and reflective decision-making (<xref ref-type="bibr" rid="bib102">Yue et al., 2024</xref>) to revise and improve reasoning dynamically, reducing hallucinations and adapting to new inputs.</p></list-item><list-item><p><bold>Multi-agent collaborative group reasoning:</bold> A multiagent reasoning framework involves deploying multiple specialised LLM-based agents (<xref ref-type="bibr" rid="bib34">Hong et al., 2024</xref>; <xref ref-type="bibr" rid="bib102">Yue et al., 2024</xref>)—such as efficacy, safety, or diagnostic agents (<xref ref-type="bibr" rid="bib102">Yue et al., 2024</xref>)—that collaboratively analyse clinical information, challenge each other’s conclusions, and synthesise decisions through structured dialogue or argumentation (<xref ref-type="bibr" rid="bib34">Hong et al., 2024</xref>). This mirrors multidisciplinary clinical teams and enhances reasoning transparency, robustness, and safety.</p></list-item></list></sec><sec id="s2-3"><title>Learning-based approaches for reasoning: Supervised to reinforcement learning</title><p>LLMs can be prompted into producing CoT- sequence of tokens representing intermediate steps in the reasoning process. However, LLMs lack explicit training objectives that encourage deep, stepwise CoT before arriving at a final answer. Large reasoning models (LRMs), a subclass of LLMs, close this gap by being trained to perform extended reasoning in CoT, before taking actions or producing a final answer. The release of OpenAI’s o1 series exemplifies this emerging trajectory.</p><p>In the emerging landscape of <italic>learning-to-reason</italic> in LRMs, the training process often begins with <bold>supervised fine-tuning (SFT)</bold>, where models are trained on labelled reasoning datasets to capture task-specific logic and patterns. However, as reasoning tasks grow more nuanced and open-ended, SFT alone becomes insufficient. To overcome these limitations, <bold>reinforcement learning from human feedback (RLHF)</bold> has been introduced. RLHF refines model output by training a reward model based on human preferences, enabling the LLM to generate responses that are more aligned with human-like reasoning: coherent, responsible and contextually appropriate.</p><p>Beyond training, recent research also demonstrates the benefits of <italic>inference-time scaling</italic>, sometimes also known as <italic>test-time scaling</italic> (<xref ref-type="bibr" rid="bib73">Snell et al., 2024</xref>; <xref ref-type="bibr" rid="bib104">Zhao et al., 2024</xref>), where prompting LLMs to generate longer or multiple reasoning paths (e.g. via tree-of-thought prompting) can significantly improve inference-time performance. These advances in both training and inference signal a shift towards what has been described as LRMs—LLMs specifically optimised for robust, interpretable, and multi-step reasoning in CoT.</p><p>In the sections that follow, we describe two foundational learning strategies—SFT and RLHF—both of which have been shown to align LLM outputs more closely with expert medical reasoning and improve clinical utility.</p><sec id="s2-3-1"><title>Supervised fine-tuning</title><p>SFT is a process used to improve a pre-trained AI model, such as an LLM, so it performs better on specific tasks or domains (<xref ref-type="bibr" rid="bib71">Singhal et al., 2023</xref>). It involves training the model on a carefully prepared dataset that includes input examples paired with the correct outputs (labels). This helps the model learn the desired behaviour more precisely by adjusting its parameters based on these examples.</p><p>The key steps in SFT are (<xref ref-type="bibr" rid="bib71">Singhal et al., 2023</xref>)</p><list list-type="order" id="list3"><list-item><p>Starting with a pre-trained model that already understands general language or knowledge.</p></list-item><list-item><p>Preparing a labelled dataset relevant to the specific task, where each input has a correct output.</p></list-item><list-item><p>Training the model on this dataset to fine-tune its parameters, so it responds accurately in the targeted context.</p></list-item><list-item><p>Evaluating and iterating to ensure the model improves without overfitting.</p></list-item></list><p>This method transforms a general-purpose model into a specialised model that delivers better performance on domain-specific tasks (<xref ref-type="bibr" rid="bib71">Singhal et al., 2023</xref>).</p></sec><sec id="s2-3-2"><title>Reinforcement learning with human feedback</title><p>RLHF (<xref ref-type="bibr" rid="bib16">Christiano et al., 2017</xref>) in the context of LLMs is a technique used to teach LLMs to behave in ways that align better with human preferences (<xref ref-type="bibr" rid="bib71">Singhal et al., 2023</xref>; <xref ref-type="bibr" rid="bib1">Achiam et al., 2023</xref>). Instead of relying solely on fixed rules or labelled data, RLHF uses human feedback to guide the learning process.</p><p>The standard RLHF pipeline has three stages: (<xref ref-type="bibr" rid="bib59">Ouyang et al., 2022</xref>; <xref ref-type="bibr" rid="bib1">Achiam et al., 2023</xref>):</p><list list-type="order" id="list4"><list-item><p><bold>SFT (optional):</bold> Often one starts with an LLM that has been fine-tuned on high-quality responses with reasoning traces to some sample of prompts (datasets).</p></list-item><list-item><p><bold>Reward model training:</bold></p><list list-type="bullet" id="list4subList1"><list-item><p>Humans see multiple candidate outputs for the same prompt.</p></list-item><list-item><p>They rank or compare outputs by which one they prefer.</p></list-item><list-item><p>A small neural network (<italic>reward model</italic>) is trained to predict those human preference scores as a single scalar reward.</p></list-item></list></list-item><list-item><p><bold>Policy optimisation:</bold></p><list list-type="bullet" id="list4subList2"><list-item><p>The LLM (now called the <italic>policy</italic>) is fine-tuned with a reinforcement-learning algorithm (e.g. PPO).</p></list-item><list-item><p>At each update, the policy generates outputs, the reward model scores them, and the RL algorithm nudges the policy towards higher-reward actions.</p></list-item></list></list-item></list><p>Over successive iterations, the model learns to produce answers that are more helpful, more accurate, and more aligned with human values (<xref ref-type="bibr" rid="bib59">Ouyang et al., 2022</xref>; <xref ref-type="bibr" rid="bib1">Achiam et al., 2023</xref>). As an example of its applicability, RLHF underpins the effectiveness of systems like GPT-4 (<xref ref-type="bibr" rid="bib1">Achiam et al., 2023</xref>).</p><p>There are several variants of the RL step (PPO [<xref ref-type="bibr" rid="bib66">Schulman et al., 2017</xref>], DPO [<xref ref-type="bibr" rid="bib64">Rafailov et al., 2023</xref>], GRPO [<xref ref-type="bibr" rid="bib68">Shao et al., 2024</xref>], etc.), each with its own trade-offs in stability, efficiency, and memory usage. We will not dive into their details here; instead, our focus will be on how this human-in-the-loop process is key to building LLMs that can carry out complex medical reasoning tasks.</p></sec></sec><sec id="s2-4"><title>Directed acyclic graphs</title><p>A directed acyclic graph (DAG) is a graphical structure made up of nodes (also called vertices) connected by arrows (edges) that indicate a specific direction of influence from one variable to another (<xref ref-type="bibr" rid="bib21">Foraita et al., 2014</xref>). The term ‘acyclic’ means that the graph contains no closed loops—following the arrows from node to node will never bring you back to the starting point (<xref ref-type="bibr" rid="bib21">Foraita et al., 2014</xref>). This enforces a clear, unidirectional flow of information, which is essential for modelling causal relationships (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>A sample directed acyclic graph.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106187-fig4-v1.tif"/></fig><p>DAGs are commonly used to represent processes or systems where order matters and repetition is not allowed (<xref ref-type="bibr" rid="bib21">Foraita et al., 2014</xref>; <xref ref-type="fig" rid="fig4">Figure 4</xref>). For example, DAGs can clarify whether the observed link between paracetamol use and childhood wheezing is due to a true causal effect or confounded by factors like viral infections (<xref ref-type="bibr" rid="bib91">Williams et al., 2018</xref>).</p><p>This structure is particularly useful in biomedical AI for modeling causal relationships (<xref ref-type="bibr" rid="bib91">Williams et al., 2018</xref>), clinical reasoning pathways (<xref ref-type="bibr" rid="bib40">Kiciman et al., 2023</xref>), or decision-making logic in a transparent and interpretable way (<xref ref-type="bibr" rid="bib21">Foraita et al., 2014</xref>; <xref ref-type="bibr" rid="bib56">Naik et al., 2023</xref>; <xref ref-type="bibr" rid="bib85">Wang et al., 2021</xref>).</p></sec></sec><sec id="s3"><title>What is <italic>reasoning behaviour</italic> in the context of medical LLMs?</title><p>First, we specifically define <italic>reasoning behaviour</italic> in the context of our review. It is important to note that the general term <italic>reasoning</italic> is used loosely across LLM-related literature, and often <italic>reasoning behaviour</italic> is not the focus of the experiment but high-level performance metrics are. Here, we slightly adapt the specific definitions of <italic>reasoning</italic> and <italic>reasoning behaviour</italic> respectively from <xref ref-type="bibr" rid="bib55">Mondorf and Plank, 2024</xref>, who define these concepts in the context of general LLMs. We also add a third definition: <italic>reasoning outcome</italic>.</p><list list-type="simple" id="list5"><list-item><p><italic>Reasoning: ‘The process of drawing conclusions based on available information’.</italic></p></list-item><list-item><p><italic>Reasoning outcome: ‘An event where reasoning reaches a conclusion.’</italic></p></list-item><list-item><p><italic>Reasoning behaviour: ‘The specific flow of logic within the scope of available information in a system that leads to a reasoning outcome.’</italic></p></list-item></list><p>Specifically, while the <italic>outcome</italic> of <italic>reasoning</italic> is an event where a conclusion is obtained, <italic>reasoning behaviour</italic> describes the <italic>process of how</italic> the conclusion is obtained (<xref ref-type="fig" rid="fig5">Figure 5</xref>). The vast majority of generic and medical LLMs focus on the former while disregarding the latter.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>A graphical representation of <italic>reasoning</italic>, <italic>reasoning outcome,</italic> and <italic>reasoning behaviour</italic>.</title><p><italic>Reasoning</italic> encapsulates the process of drawing conclusions, arriving at a <italic>reasoning outcome</italic>. At a more fundamental level, <italic>reasoning behaviour</italic> describes the logical flow through the system that occurs during <italic>reasoning.</italic></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106187-fig5-v1.tif"/></fig><p>We apply the same definition to this review for medical LLMs.</p></sec><sec id="s4"><title>Types of reasoning applicable to medical LLMs</title><p>This section reviews studies that extend beyond a high-level focus on task accuracy, focusing instead on evaluating the <italic>reasoning behaviour</italic> of LLMs. <italic>Reasoning behaviour</italic> can be subdivided into multiple categories (<xref ref-type="table" rid="table1">Table 1</xref>). However, for the purposes of this study, we focus mainly on subtypes of logical reasoning (<xref ref-type="bibr" rid="bib32">Holyoak and Morrison, 1999</xref>) and causal reasoning (<xref ref-type="bibr" rid="bib72">Sloman, 2009</xref>) that are common in medical LLMs. In addition, we explore the less visible field of neurosymbolic reasoning. We note that other reasoning types such as mathematical reasoning (<xref ref-type="bibr" rid="bib35">Horsten, 2007</xref>) may be more applicable to other categories of LLMs, which are not the focus of this review.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>A table showing types of reasoning, their definition and examples.</title><p>Reasoning types are colour-coded for clarity. Logical reasoning encompasses abductive, deductive, and inductive subtypes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Type of reasoning</th><th align="left" valign="bottom">Definition</th><th align="left" valign="bottom">General example</th><th align="left" valign="bottom">Medical example</th></tr></thead><tbody><tr><td style="background-color: #FFB74D;">Abductive</td><td align="left" valign="bottom">Inferring the most likely explanation for observed data or evidence.</td><td align="left" valign="bottom">Ali, Muthu, and Ah Hock breathe oxygen. Therefore, Ali, Muthu, and Ah Hock are likely human.</td><td align="left" valign="bottom">A patient has increased intercranial pressure, blurred vision and nausea. Therefore, the patient may have a brain aneurysm or ischaemic stroke.</td></tr><tr><td style="background-color: #FFB74D;">Deductive</td><td align="left" valign="bottom">Reasoning from a set of premises to reach a certain conclusion.</td><td align="left" valign="bottom">All humans breathe oxygen. Rentap is human. Therefore, Rentap breathes oxygen.</td><td align="left" valign="bottom">A patient has increased intercranial pressure, blurred vision and nausea. A CT scan shows no bleeding or swelling. Therefore, the patient does not have a brain aneurysm.</td></tr><tr><td style="background-color: #FFB74D;">Inductive</td><td align="left" valign="bottom">Inferring general principles based on specific observations.</td><td align="left" valign="bottom">All humans that I have seen breathe oxygen. Therefore, Rentap probably breathes oxygen.</td><td align="left" valign="bottom">A patient has increased intercranial pressure, blurred vision and nausea. A CT scan shows no bleeding or swelling. Therefore, the patient probably has an ischaemic stroke.</td></tr><tr><td style="background-color: #90CAF9;">Symbolic<sup><xref ref-type="table-fn" rid="table1fn1">*</xref></sup></td><td align="left" valign="bottom">The abstraction of a system into its component parts, which enables a more direct application of mathematics.</td><td align="left" valign="bottom">Rule: If an organism breathes oxygen and nitrogen, and exhales carbon dioxide → likely human.<break/>Observation: Ali, Muthu, Ah Hock, and Rentap exhibit this respiratory pattern.<break/>Conclusion: Therefore, they are probably human.</td><td align="left" valign="bottom">Rule 1: If a patient presents with increased intracranial pressure (ICP), blurred vision, and nausea → infer high intracranial pathology.<break/>Observation 1: The patient shows increased ICP, blurred vision, and nausea.<break/>Rule 2: If CT scan shows no bleeding or swelling → rule out haemorrhagic causes.<break/>Observation 2: CT scan reveals no evidence of bleeding or swelling.<break/>Rule 3: If high ICP and haemorrhage is ruled out → suspect ischaemic stroke.<break/>Conclusion: Therefore, the patient most likely has an ischaemic stroke.</td></tr><tr><td style="background-color: #C5E1A5;">Causal/<break/>Counterfactual</td><td align="left" valign="bottom">Establishing a cause-and-effect relationship between events.</td><td align="left" valign="bottom">Ali, Muthu, Ah Hock, and Rentap exhibit this respiratory pattern.</td><td align="left" valign="bottom">A blood clot probably caused blockage in the brain leading to the stroke.</td></tr></tbody></table><table-wrap-foot><fn id="table1fn1"><label>*</label><p>We note that the term <italic>symbolic reasoning</italic> may be misleading as it is fundamentally an abstract <italic>data representation</italic> which simplifies the process of translating a scenario into a <italic>reasoning</italic> framework.</p></fn></table-wrap-foot></table-wrap><sec id="s4-1"><title>Logical reasoning</title><p>The study of logical reasoning addresses the question of how individuals infer valid conclusions from a set of given premises within a structured framework of logical rules and principles (<xref ref-type="bibr" rid="bib55">Mondorf and Plank, 2024</xref>). <xref ref-type="bibr" rid="bib55">Mondorf and Plank, 2024</xref> classify logical reasoning into deductive, inductive, and abductive reasoning. Deductive and inductive reasoning both work towards a general conclusion, with the key distinction being that deductive reasoning begins with a premise while inductive reasoning begins with observations. On a broader scale, abductive reasoning involves formulating plausible hypotheses that explain incomplete observations. The key distinction between deductive reasoning and both inductive/abductive reasoning is that deductive reasoning results in clear conclusions, while inductive/abductive reasoning may not necessarily achieve this. For more nuanced and low-level details on the distinction between the three types of logical reasoning in the context of LLMs, we refer the reader to other publications (<xref ref-type="bibr" rid="bib55">Mondorf and Plank, 2024</xref>; <xref ref-type="bibr" rid="bib74">Sun et al., 2023</xref>; <xref ref-type="bibr" rid="bib101">Yu et al., 2023b</xref>).</p></sec><sec id="s4-2"><title>Causal/counterfactual reasoning</title><p>Causal reasoning refers to the ability to connect cause and effect in scenarios. In the context of medical and general LLMs, their capabilities are a matter of debate. Intuitively, providing cause and effect relationships improves one’s understanding of a system. Correspondingly, providing this information to medical LLMs would improve a model’s ‘understanding’ and has unsurprisingly emerged as an area of interest. This capability is essential in applications like medical diagnosis, where identifying causal links—such as between symptoms and potential conditions—can inform accurate and actionable insights. Causal reasoning involves not just recognising associations but distinguishing <italic>directional</italic> influences. In theory, knowing directionality grants the model the ability to infer, for instance, whether ‘A causes B’ or ‘B causes A’. In real-world medical applications, larger LLMs like GPT-4.0 are capable of inferring causal direction between variables, allowing accurate diagnosis of neuropathic pain (<xref ref-type="bibr" rid="bib40">Kiciman et al., 2023</xref>).</p></sec><sec id="s4-3"><title>Symbolic reasoning</title><p>Symbolic reasoning—also known as <italic>symbolic AI</italic> or ‘good old-fashioned artificial intelligence’—is a process that involves the use of mathematical symbols to represent concepts, objects, or relationships in order to facilitate reasoning, problem-solving, and decision-making. Unlike machine learning which relies on learning from vast datasets, this form of reasoning is characterised by its reliance on formal logic and structured representations, allowing for the manipulation of abstract symbols according to defined rules without requiring vast datasets. Symbolic systems execute explicit inference chains—for example, ‘if symptom A and test result B, then diagnosis C’—mirroring the logic of clinical guidelines and expert systems such as MYCIN (<xref ref-type="bibr" rid="bib83">van Melle, 1978</xref>) or INTERNIST (<xref ref-type="bibr" rid="bib53">Miller et al., 1985</xref>). This approach enhances interpretability and trustworthiness in biomedical applications, since the reasoning steps are transparent and auditable.</p><sec id="s4-3-1"><title>Neurosymbolic reasoning</title><p>Neuro-symbolic AI (N-SAI) is an interdisciplinary field that aims to harmoniously integrate neural networks with symbolic reasoning techniques. Its overarching objective is to establish a synergistic connection between symbolic reasoning and statistical learning, harnessing the strengths of each approach. In the context of N-SAI, the symbolic system is used to represent predefined rulesets and knowledge bases, which then streamlines the process of making inferences and highlighting relationships between entities. Crucially, it is more transparent and more interpretable to humans. Meanwhile, the <italic>neuro</italic> component refers to artificial neural networks in the context of large-scale statistical learning. Artificial neural networks are adept at scale in classification, prediction, and pattern recognition as well as processing unstructured data. Therefore, unifying and leveraging the strengths of both would hypothetically lead to the best of both worlds (<xref ref-type="bibr" rid="bib69">Sheth et al., 2023</xref>).</p></sec></sec><sec id="s4-4"><title>Trends in existing medical LLMs</title><p>While there is no shortage of LLMs applied to medical problems, there is a striking lack of methods which leverage <italic>reasoning behaviour</italic> in their operation (<xref ref-type="table" rid="table2">Table 2</xref>). Comparing and contrasting this small subset of methods reveals some interesting trends.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>A table showing medical reasoning methods, their defining characteristics, and approach to reasoning.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Method name</th><th align="left" valign="bottom">Base architecture/method</th><th align="left" valign="bottom">Reasoning improvement strategy</th><th align="left" valign="bottom">Type of Reasoning</th><th align="left" valign="bottom">Advantages</th><th align="left" valign="bottom">Disadvantages</th><th align="left" valign="bottom">Dataset</th><th align="left" valign="bottom">Github</th></tr></thead><tbody><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib65">Savage et al., 2024</xref></td><td align="left" valign="bottom">GPT-3.5; GPT-4.0</td><td align="left" valign="bottom">Chain-of-thought (diagnostic reasoning)</td><td align="left" valign="bottom">Deductive</td><td align="left" valign="bottom">Easy to implement</td><td align="left" valign="bottom">Scope is limited to GPT models, focusing exclusively on English medical questions</td><td align="left" valign="bottom">Modified MedQA USMLE; NEJM (<italic>New England Journal of Medicine</italic>) case series</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib43">Kwon et al., 2024</xref></td><td align="left" valign="bottom">GPT-4.0; OPT; LLaMA-2; 3D ResNet</td><td align="left" valign="bottom">Chain-of-thought; knowledge distillation (via SFT)</td><td align="left" valign="bottom">Deductive</td><td align="left" valign="bottom">Lightweight and practical to use</td><td align="left" valign="bottom">Tight scope to limited disease conditions</td><td align="left" valign="bottom">Alzheimer’s Disease Neuroimaging Initiative (ADNI); Australian Imaging Biomarkers and Lifestyle Study of Ageing (AIBL)</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/ktio89/ClinicalCoT">https://github.com/ktio89/ClinicalCoT</ext-link></td></tr><tr><td align="left" valign="bottom">MEDDM Binbin et al <xref ref-type="bibr" rid="bib45">Li et al., 2023</xref></td><td align="left" valign="bottom">GPT</td><td align="left" valign="bottom">Chain-of-thought; clinical decision trees</td><td align="left" valign="bottom">Deductive</td><td align="left" valign="bottom">Adaptable to different systems</td><td align="left" valign="bottom">Heavy data collection to generate clinical guidance trees</td><td align="left" valign="bottom">Medical books, treatment guidelines, and other medical literature</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">DRHOUSE <xref ref-type="bibr" rid="bib97">Yang et al., 2024a</xref></td><td align="left" valign="bottom">GPT-3.5; GPT-4.0; LLaMA-3 70b; HuatuoGPT-II; MEDDM</td><td align="left" valign="bottom">Chain-of-thought; clinical decision trees</td><td align="left" valign="bottom">Deductive</td><td align="left" valign="bottom">Objective sensor measurement</td><td align="left" valign="bottom">Available datasets are currently limited</td><td align="left" valign="bottom">MedDG; KaMed; DialMed</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">DR. KNOWS <xref ref-type="bibr" rid="bib23">Gao et al., 2023b</xref></td><td align="left" valign="bottom">Vanilla T5; Flan T5; ClinicalT5; GPT</td><td align="left" valign="bottom">Chain-of-thought; extracted explainable diagnostic pathway</td><td align="left" valign="bottom">Deductive; neurosymbolic</td><td align="left" valign="bottom">Hybrid method improves accuracy; provides explainable diagnostic pathways</td><td align="left" valign="bottom">Particularly fragile to missing data</td><td align="left" valign="bottom">MIMIC-III; In-house EHR</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">TEMED-LLM <xref ref-type="bibr" rid="bib7">Bisercic et al., 2023</xref></td><td align="left" valign="bottom">text-davinchi-003; GPT-3.5; logistic regression; decision tree; XGBoost</td><td align="left" valign="bottom">Few-shot learning, tabular ML modelling; Neurosymbolic</td><td align="left" valign="bottom">Deductive</td><td align="left" valign="bottom">End-to-end interpretability, from data extraction to ML analysis</td><td align="left" valign="bottom">Requires human experts</td><td align="left" valign="bottom">EHR dataset (kaggle; see referenced publication for details)</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">EHRAgent <xref ref-type="bibr" rid="bib70">Shi et al., 2024</xref></td><td align="left" valign="bottom">GPT-4</td><td align="left" valign="bottom">Autonomous code generation and execution for multi-tabular reasoning in EHRs</td><td align="left" valign="bottom">Deductive</td><td align="left" valign="bottom">Facilitates automated solutions in complex medical scenarios</td><td align="left" valign="bottom">Non-deterministic; limited generalisability</td><td align="left" valign="bottom">MIMIC-III; eICU; TREQS</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/wshi83/EhrAgent">https://github.com/wshi83/EhrAgent</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://wshi83.github.io/EHR-Agent-page">https://wshi83.github.io/EHR-Agent-page</ext-link></td></tr><tr><td align="left" valign="bottom">AMIE <xref ref-type="bibr" rid="bib81">Tu et al., 2024</xref></td><td align="left" valign="bottom">PaLM 2</td><td align="left" valign="bottom">Reinforcement learning</td><td align="left" valign="bottom">Deductive</td><td align="left" valign="bottom">Effectively handles noisy and ambiguous real-world medical dialogues</td><td align="left" valign="bottom">Computationally expensive and resource-intensive; simulated data may not fully capture real-world clinical nuances</td><td align="left" valign="bottom">MedQA; HealthSearchQA; LiveQA; Medication QA in MultiMedBench, MIMIC-III</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">ArgMed-Agents <xref ref-type="bibr" rid="bib34">Hong et al., 2024</xref></td><td align="left" valign="bottom">GPT-3.5-turbo; GPT-4</td><td align="left" valign="bottom">Chain-of-Thought; symbolic reasoning: neurosymbolic</td><td align="left" valign="bottom">Deductive</td><td align="left" valign="bottom">Training-free enhancement; explainability matches fully transparent, knowledge-based systems</td><td align="left" valign="bottom">Artificially restricted responses that do not match real-world cases</td><td align="left" valign="bottom">MedQA; PubMedQA</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Fansi <xref ref-type="bibr" rid="bib76">Tchango et al., 2022a</xref></td><td align="left" valign="bottom">BASD (baseline ASD): multi-layer perceptron (MLP) diaformer</td><td align="left" valign="bottom">Reinforcement learning</td><td align="left" valign="bottom">Deductive</td><td align="left" valign="bottom">Closely align with clinical reasoning protocols</td><td align="left" valign="bottom">Limited testing on real patient data</td><td align="left" valign="bottom">DDxPlus</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/mila-iqia/Casande-RL">https://github.com/mila-iqia/Casande-RL</ext-link></td></tr><tr><td align="left" valign="bottom">MEDIQ <xref ref-type="bibr" rid="bib46">Li et al., 2024</xref></td><td align="left" valign="bottom">LLaMA-3-Instruct (8B, 70B); GPT-3.5; GPT-4</td><td align="left" valign="bottom">Chain-of-thought; information-seeking dialogues</td><td align="left" valign="bottom">Abductive</td><td align="left" valign="bottom">Robust to missing information</td><td align="left" valign="bottom">Available datasets are limited<break/>Proprietary; artificially restricted responses that do not match real-world cases</td><td align="left" valign="bottom">iMEDQA; iCRAFT-MD</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/stellalisy/mediQ">https://github.com/stellalisy/mediQ</ext-link></td></tr><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib56">Naik et al., 2023</xref></td><td align="left" valign="bottom">GPT-4</td><td align="left" valign="bottom">Causal network generation</td><td align="left" valign="bottom">Causal/ counterfactual</td><td align="left" valign="bottom">Uses general LLMs</td><td align="left" valign="bottom">Lacks a specialised medical knowledge base</td><td align="left" valign="bottom">Providence St.Joseph Health (PSJH’s) clinical data warehouse</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib25">Gopalakrishnan et al., 2024</xref></td><td align="left" valign="bottom">BioBERT; DistilBERT; BERT; GPT-4; LLaMA</td><td align="left" valign="bottom">Causality extraction</td><td align="left" valign="bottom">Causal/ counterfactual</td><td align="left" valign="bottom">Easy to implement</td><td align="left" valign="bottom">Tight scope to limited disease conditions</td><td align="left" valign="bottom">American Diabetes Association (ADA); US Preventive Services Task Force (USPSTF); American College of Obstetrics Gynecology (ACOG); American Academy of Family Physician (AAFP); Endocrine Society</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/gseetha04/LLMs-Medicaldata">https://github.com/gseetha04/LLMs-Medicaldata</ext-link></td></tr><tr><td align="left" valign="bottom">InferBERT <xref ref-type="bibr" rid="bib85">Wang et al., 2021</xref></td><td align="left" valign="bottom">ALBERT; Judea Pearl’s Do-calculus</td><td align="left" valign="bottom">Causal inference using do-calcus</td><td align="left" valign="bottom">Causal/ counterfactual; mathematical</td><td align="left" valign="bottom">Establishes causal inference</td><td align="left" valign="bottom">Tight scope to limited disease conditions; highly restrictive input format</td><td align="left" valign="bottom">FAERS case reports from the PharmaPendium database</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/XingqiaoWang/DeepCausalPV-master">https://github.com/XingqiaoWang/DeepCausalPV-master</ext-link></td></tr><tr><td align="left" valign="bottom">Emre Kıcıman</td><td align="left" valign="bottom">text-davinci-003, GPT-3.5-turbo, and GPT-4</td><td align="left" valign="bottom">Determine direction of causality between pairs of variables</td><td align="left" valign="bottom">Causal/ counterfactual</td><td align="left" valign="bottom">Highly accurate for large models</td><td align="left" valign="bottom">Limited reproducibility due to dependency on tailored prompts</td><td align="left" valign="bottom">Tübingen cause-effect pairs dataset.</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/py-why/pywhy-llm">https://github.com/py-why/pywhy-llm</ext-link></td></tr><tr><td align="left" valign="bottom">Huatuo GPT-o1 <xref ref-type="bibr" rid="bib13">Chen et al., 2024</xref></td><td align="left" valign="bottom">LLaMA-3.1-8B-Instruct and LLaMA-3.1-70B-Instruct</td><td align="left" valign="bottom">Supervised fine-tuning and PPO</td><td align="left" valign="bottom">Deductive reasoning</td><td align="left" valign="bottom">Instils multi-step reasoning in medical LLMs; built-in interpretability as LLM can output reasoning traces along with answer</td><td align="left" valign="bottom">Limited evaluations as evaluations cover accuracy scores on medical MCQ benchmarks</td><td align="left" valign="bottom">Adapted from MedQA-USMLE and MedMcQA</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/FreedomIntelligence/HuatuoGPT-o1">https://github.com/FreedomIntelligence/HuatuoGPT-o1</ext-link></td></tr><tr><td align="left" valign="bottom">Med-R1 <xref ref-type="bibr" rid="bib44">Lai et al., 2025</xref></td><td align="left" valign="bottom">Qwen2-VL-2B</td><td align="left" valign="bottom">Supervised fine-tuning and GRPO</td><td align="left" valign="bottom">Deductive reasoning</td><td align="left" valign="bottom">Joint image-text and multi-task reasoning; built-in interpretability as LLM can output reasoning traces along with answer</td><td align="left" valign="bottom">Rethinking the ‘More Thinking is Better’ Assumption</td><td align="left" valign="bottom">OmniMedVQA</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/Yuxiang-Lai117/Med-R1">https://github.com/Yuxiang-Lai117/Med-R1</ext-link></td></tr><tr><td align="left" valign="bottom">MedVLM-R1 <xref ref-type="bibr" rid="bib61">Pan et al., 2025</xref></td><td align="left" valign="bottom">Qwen2-VL-2B</td><td align="left" valign="bottom">Supervised Fine-tuning and GRPO</td><td align="left" valign="bottom">Deductive Reasoning</td><td align="left" valign="bottom">Joint image-text reasoning; Built-in interpretability as LLM can output reasoning traces along with answer</td><td align="left" valign="bottom">Limited evaluations as evaluations cover accuracy scores on medical MCQ benchmarks</td><td align="left" valign="bottom">VQA-RAD, SLAKE, PathVQA, OmniMedVQA, and PMC-VQA</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://huggingface.co/JZPeterPan/MedVLM-R1">https://huggingface.co/JZPeterPan/MedVLM-R1</ext-link></td></tr><tr><td align="left" valign="bottom">MedFound <xref ref-type="bibr" rid="bib50">Liu et al., 2025</xref></td><td align="left" valign="bottom">176 billion parameter LLM pretrained from scratch</td><td align="left" valign="bottom">Supervised fine-tuning and DPO</td><td align="left" valign="bottom">Deductive reasoning</td><td align="left" valign="bottom">- Self-bootstrapped Chain-of-Thought fine-tuning; Rigorous human evaluation of reasoning traces with rubric; built-in interpretability as LLM can output reasoning traces along with answer</td><td align="left" valign="bottom">Proprietary EHR datasets aren’t fully open, hindering exact reproduction</td><td align="left" valign="bottom">MedCorpus, MedDX-FT and MedDX-Bench</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/medfound/medfound">https://github.com/medfound/medfound</ext-link></td></tr><tr><td align="left" valign="bottom">DeepSeekR1 <xref ref-type="bibr" rid="bib27">Guo et al., 2025</xref></td><td align="left" valign="bottom">DeepSeek-V3-Base</td><td align="left" valign="bottom">Supervised fine-tuning and GRPO</td><td align="left" valign="bottom">Deductive reasoning</td><td align="left" valign="bottom">Built-in interpretability as LLM can output reasoning traces along with answer</td><td align="left" valign="bottom">Pre-training and reasoning datasets aren’t open-sourced</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/deepseek-ai/DeepSeek-R1">https://github.com/deepseek-ai/DeepSeek-R1</ext-link></td></tr></tbody></table></table-wrap><p>First, inspecting their foundational or base models shows that unsurprisingly, most of these methods are built on generic LLMs, commonly GPT (<xref ref-type="bibr" rid="bib92">Wu et al., 2023</xref>) or LLaMA (<xref ref-type="bibr" rid="bib80">Touvron et al., 2023</xref>) variants. This is likely due to their demonstrated effectiveness in day-to-day tasks, with more modern variants being shown to be surprisingly effective in clinical applications as-is (<xref ref-type="bibr" rid="bib33">Homolak, 2023</xref>). However, it is notable that many approaches utilise multiple base models, with no single method relying on one model type. Relying on multiple models is unsurprising, as combining the strengths of multiple models is likely to boost overall effectiveness.</p><p>Second, most of their <italic>reasoning behaviour</italic> is derived from variants of CoT (<xref ref-type="bibr" rid="bib90">Wei et al., 2022</xref>) processes or reinforcement learning, likely because both techniques closely mimic cognitive processes fundamental to reasoning. CoT enables models to break down complex medical cases into a series of logical steps, mirroring the structured, stepwise reasoning that healthcare professionals apply. Additionally, few-shot learning complements CoT, allowing LLMs to ‘learn’ from the input prompt, generalise from minimal clinical examples, and adapt quickly to nuanced cases—a useful capability in medicine where data can be sparse or highly specialised. Meanwhile, reinforcement learning allows models to refine their reasoning capabilities through practice in a virtual simulation, improving accuracy through iterative feedback.</p><p>We note with interest that SFT and RLHF have been widely adopted to train medical LLMs for reasoning tasks, giving rise to several domain-specific models such as Huatuo GPT-o1 (<xref ref-type="bibr" rid="bib13">Chen et al., 2024</xref>), MedR (<xref ref-type="bibr" rid="bib44">Lai et al., 2025</xref>), MedVLM-R1 (<xref ref-type="bibr" rid="bib61">Pan et al., 2025</xref>), and MedFound (<xref ref-type="bibr" rid="bib50">Liu et al., 2025</xref>). We refer to this class of systems as LTMs, a subclass of LLM, trained to perform extensive CoT reasoning. These LRMs employ a variety of RLHF strategies, including Proximal Policy Optimization (PPO) (<xref ref-type="bibr" rid="bib66">Schulman et al., 2017</xref>) for Huatuo GPT-o1 (<xref ref-type="bibr" rid="bib13">Chen et al., 2024</xref>), Direct Preference Optimization (DPO) (<xref ref-type="bibr" rid="bib64">Rafailov et al., 2023</xref>) for MedFound (<xref ref-type="bibr" rid="bib50">Liu et al., 2025</xref>), and Group Relative Policy Optimization (GRPO) (<xref ref-type="bibr" rid="bib68">Shao et al., 2024</xref>) for both MedR1 (<xref ref-type="bibr" rid="bib44">Lai et al., 2025</xref>) and MedVLM-R1 (<xref ref-type="bibr" rid="bib61">Pan et al., 2025</xref>). Interestingly, findings from the MedR1 study revealed that models trained with GRPO not to output intermediate reasoning traces performed better in terms of final accuracy compared to those trained to explicitly generate intermediate steps (<xref ref-type="bibr" rid="bib44">Lai et al., 2025</xref>). This challenges the prevailing assumption that ‘more reasoning always leads to better outcomes’, suggesting that, in some cases, compressed or implicit reasoning may yield higher task performance.</p><p>Third, the <italic>reasoning behaviour</italic> of most methods can be categorised as deductive reasoning, although there are a few cases of abductive and causal/counterfactual reasoning. Here, it is also worth noting that while LLMs excel in abductive reasoning tasks in multiple-choice scenarios, they are considerably less effective in generating hypotheses from scratch which may be of value in clinical use (<xref ref-type="bibr" rid="bib26">Gouveia and Malík, 2024</xref>). Since the overall goal of clinical diagnosis is to determine the disease affecting a patient from causative agents, the prevalence of deductive and, to a lesser extent, causal/counterfactual reasoning makes sense.</p><p>Finally, training datasets used vary widely in both scope and size, ranging across many different medical conditions, source material and between hundreds to thousands of samples. We observed no single standardised training dataset used by each approach, and as with architecture types many approaches used multiple training datasets. Most datasets were of the same modality (text data only), though some medical imaging datasets (MRI scans) were present. MIMIC-III was the most commonly used text dataset, with a combination of medical literature and other publicly available datasets (<xref ref-type="bibr" rid="bib39">Johnson et al., 2016</xref>). Therefore, due to the differences in scope, strategy and data used by each approach, directly comparing <italic>reasoning behaviour</italic> across all models simultaneously is not presently feasible.</p><p>Aside from deductive reasoning, causal reasoning and neurosymbolic reasoning have also been demonstrably effective (<xref ref-type="table" rid="table2">Table 2</xref>). However, example use cases are considerably less common. Current causal inference tests have a limited scope, such as determining the direction of causality between variable pairs, and their performance in more open-ended or nuanced causal inference as well as counterfactual reasoning remains unexplored. Meanwhile, neurosymbolic reasoning strategies exploit their inherently grounding properties to address the more fundamental issue of hallucinations in LLMs (<xref ref-type="bibr" rid="bib36">Huang et al., 2023</xref>). The diversity of strategies is striking—some methods exploit agent-based approaches to tailor argumentation schema and symbolic solvers for clinical reasoning (<xref ref-type="bibr" rid="bib34">Hong et al., 2024</xref>), while others integrate dynamic medical ontologies in an attempt to more closely align <italic>reasoning behaviour</italic> with medical knowledge (<xref ref-type="bibr" rid="bib22">Gao et al., 2023a</xref>).</p><p>As each approach varies widely in scope and implementation, the advantages and disadvantages of each approach are broad. Generally, approaches using graph and decision tree-based strategies are easier to interpret due to their more deterministic nature, but may be less effective in ambiguous or complex cases (which are common in clinical practice). Meanwhile, methods which are more robust to noise or complex use cases are limited by a highly restricted scope, availability of training resources, and a large computational footprint. Among these methods, DR HOUSE (<xref ref-type="bibr" rid="bib97">Yang et al., 2024a</xref>) is unique due to its EHR-free approach, only relying on objective sensor data to circumvent variance in clinical note interpretation. Unfortunately, the code associated with many of these methods is not publicly available under an open-source licence, which limits our ability to inspect them in close detail. It is worth mentioning that medical LLMs are equally affected by some of the deeply rooted issues that plague general purpose LLMs as well, for instance, memorisation (<xref ref-type="bibr" rid="bib30">Hartmann et al., 2023</xref>) and hallucination (<xref ref-type="bibr" rid="bib36">Huang et al., 2023</xref>).</p></sec></sec><sec id="s5"><title>Evaluating <italic>reasoning behaviour</italic> in medical LLMs</title><p>To date, a standardised methodology for assessing the reasoning capabilities of LLMs is absent. We review the current state-of-the-art in evaluation frameworks for analysing the <italic>reasoning behaviour</italic> of LLMs in medical tasks and we categorise evaluation methodologies into four distinct groups: (i) conclusion-based, (ii) rationale-based, (iii) interactive, and (iv) mechanistic evaluations (<xref ref-type="table" rid="table3">Table 3</xref>).</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Comparison of reasoning evaluation paradigms in medical LLMs.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Evaluation paradigm</th><th align="left" valign="bottom">Conceptual focus</th><th align="left" valign="bottom">Typical implementation and metrics</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Conclusion-based</bold></td><td align="left" valign="bottom">Assesses correctness of the <bold>final answer</bold> only, without inspecting the reasoning path.</td><td align="left" valign="bottom">Automated scoring on Q-&amp;-A benchmarks (e.g. MedQA, MedMCQA). Metrics: Accuracy, exact-match, F1. Fast, reproducible, but offers only high-level insight.</td></tr><tr><td align="left" valign="bottom"><bold>Rationale-based</bold></td><td align="left" valign="bottom">Evaluates the <bold>logic chain</bold> or narrative explanation produced by the model. Focuses on <bold>coherence, validity, and completeness</bold> of reasoning traces.</td><td align="left" valign="bottom">Manual expert review or rubric-based grading of CoT. Automated graph checks (e.g. DAG similarity, causal-direction tests). Metrics: Bayesian Dirichlet score, Normalised Hamming Distance.</td></tr><tr><td align="left" valign="bottom"><bold>Mechanistic</bold></td><td align="left" valign="bottom">Probes <bold>low-level internal signals</bold> to answer “<italic>why did the model arrive here?</italic>”. Targets feature attribution and internal attention contributions.</td><td align="left" valign="bottom">Explainable-AI toolkits (Integrated Gradients, SHAP, attention rollout). Outputs saliency maps or keyword heat-maps for clinician inspection.</td></tr><tr><td align="left" valign="bottom"><bold>Interactive</bold></td><td align="left" valign="bottom">Treats evaluation as a <bold>dialogue or game</bold>; dynamically stresses the model in real time. Explores the <italic>response space</italic> by challenging, re-prompting, or role-playing.</td><td align="left" valign="bottom">Game-theoretic tasks (e.g. debate, self-play). Rich insights but lower reproducibility; requires human-in-the-loop or scripted agents.</td></tr></tbody></table></table-wrap><sec id="s5-1"><title>Conclusion-based evaluation</title><p>In conclusion-based evaluation schemes, the focus is on the model’s final answer rather than the reasoning process that led to it. Although this outcome-focused approach may overlook the model’s underlying rationales, it can still offer valuable but limited insights into the model’s reasoning patterns, especially if there is a clear cause and effect between different premises and conclusions in which the <italic>reasoning behaviour</italic> may be more self-evident.</p><p>A wealth of benchmark data exists for the purpose of straightforward score-wise conclusion-based evaluation. A subset of simple benchmarks is available on the open medical LLM leaderboard, which covers question-and-answer tasks (<xref ref-type="bibr" rid="bib38">Jin et al., 2020</xref>; <xref ref-type="bibr" rid="bib60">Pal et al., 2022</xref>; <xref ref-type="bibr" rid="bib37">Jin et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Hendrycks et al., 2020</xref>). Notably, some datasets consist of multiple-choice questions and some consist of short-answer questions. More sophisticated examples incorporating detailed data in multiple formats (<xref ref-type="bibr" rid="bib77">Tchango et al., 2022b</xref>) or with more refined metrics also exist (<xref ref-type="bibr" rid="bib48">Liao et al., 2024</xref>).</p><p>Inroads into more nuanced frameworks to gain deeper insight in <italic>reasoning behaviour</italic> for conclusion-based evaluation have been made. These take the form of frameworks evaluating paradigms at various levels, with high-level theoretical frameworks available (<xref ref-type="bibr" rid="bib8">Bragazzi and Garbarino, 2024</xref>). A lower-level and detailed framework more suited for direct implementation is DR BENCH, which specifically assesses medical knowledge representation, clinical evidence integration and diagnosis accuracy (<xref ref-type="bibr" rid="bib22">Gao et al., 2023a</xref>). In the process, an expanded suite of specific tasks is carried out to evaluate these three interdependent elements, and an accuracy score is reported for each sub-task to evaluate reasoning at a high level. However, we emphasise that conclusion-based evaluation is only capable of yielding high-level information due to its intrinsic nature.</p></sec><sec id="s5-2"><title>Rationale-based evaluation</title><p>In contrast to high-level conclusion-based evaluation schemes, rationale-based evaluation methods are process-driven instead of being outcome-driven. Their focus is on examining the reasoning process or <italic>reasoning traces</italic> generated by the model, typically assessing their logical validity and coherence. As rationale-based evaluation methods targeted at medical language models are relatively scarce and operate under distinct paradigms, we will discuss them individually on a case-by-case basis.</p><p>The most straightforward but resource-heavy approach was to manually evaluate answers using the skills of domain experts. These domain experts were blinded to the questions and identified logical fallacies as well as inaccuracies directly in provided rationale (<xref ref-type="bibr" rid="bib65">Savage et al., 2024</xref>). More structured variants of this method have emerged, where expert clinicians employ clinically validated rubrics to score LLM-generated reasoning traces. Notable examples include the CLEVER (CLinical EValuation for Effective Reasoning in Diagnosis) rubric (<xref ref-type="bibr" rid="bib50">Liu et al., 2025</xref>) and the Revised-IDEA (R-IDEA) rubric (<xref ref-type="bibr" rid="bib19">Esteitieh et al., 2025</xref>; <xref ref-type="bibr" rid="bib10">Cabral et al., 2024</xref>).</p><p>Furthermore, clinicians have conducted deeper analyses of models such as DeepSeekR1 using USMLE-style multiple-choice questions, uncovering several notable failure modes. These included anchoring on initial symptoms while disregarding contradictory findings, omission of standard-of-care treatments, misuse of laboratory values or drug mechanisms, and confusion between clinically similar entities (e.g. troponin vs. CK-MB) (<xref ref-type="bibr" rid="bib54">Moëll et al., 2025</xref>). Of particular interest was a failure mode termed CoT mismatch (<xref ref-type="bibr" rid="bib54">Moëll et al., 2025</xref>), where the reasoning path supported one answer, but the model selected a different final option (<xref ref-type="bibr" rid="bib54">Moëll et al., 2025</xref>)—highlighting that reasoning traces, while useful, are not a perfect proxy for model interpretability or correctness. We note the value and effectiveness of clinician-led evaluations of LLM reasoning traces as they offer nuanced insights into clinical reasoning quality. However, these methods are inherently time-consuming and reliant on the availability of expert clinicians.</p><p>To reduce the reliance on time-intensive clinician reviews, recent work has turned to automated reasoning-trace evaluation (<xref ref-type="bibr" rid="bib105">Zhou et al., 2025</xref>; <xref ref-type="bibr" rid="bib93">Wu et al., 2025</xref>). When a high-quality ‘gold-standard’ CoT exists, generated traces can be compared step-by-step using text-similarity metrics such as BLEU, METEOR, or BERTScore (<xref ref-type="bibr" rid="bib105">Zhou et al., 2025</xref>). Additionally, a secondary ‘judge’ LLM can be prompted to assess another model’s chain of thought—either by comparing it against a provided standard (<xref ref-type="bibr" rid="bib93">Wu et al., 2025</xref>; <xref ref-type="bibr" rid="bib105">Zhou et al., 2025</xref>; <xref ref-type="bibr" rid="bib63">Qiu et al., 2025</xref>) or in the case where no references exist the ‘judge’ LLM can be prompted to evaluate for logical coherence and clinical relevance based on its own internal reasoning (<xref ref-type="bibr" rid="bib105">Zhou et al., 2025</xref>). Another LLM evaluation workflow integrated online search to fact check reasoning traces (<xref ref-type="bibr" rid="bib63">Qiu et al., 2025</xref>).</p><p>Conversely, an automated approach applied DAG to represent underlying relationships in complex medical datasets, including cancer (<xref ref-type="bibr" rid="bib56">Naik et al., 2023</xref>). In implementation, a DAG was constructed by predicting which factors might influence others, and accuracy was scored with a Bayesian Dirichlet metric measuring the similarity of the resultant graph with the ground truth of real patient data. In addition, a separate method also applied DAG, but exploited it to infer the direction of causality between variable pairs (<xref ref-type="bibr" rid="bib40">Kiciman et al., 2023</xref>). Accuracy was then measured using normalised Hamming distance (NHD) as a similarity metric between the resultant and ground truth patient outcome or diagnostic graph.</p></sec><sec id="s5-3"><title>Mechanistic evaluation</title><p>Similarly, mechanistic evaluation of <italic>reasoning behaviour</italic> is process driven with the aim of examining low-level reasoning traces. In contrast to rationale-based evaluation, mechanistic evaluation delves deeper into the underlying processes that drive a model’s response, aiming to uncover the fundamental questions of ‘how’ and ‘why’ associated with an outcome.</p><p>In practice, feature attribution methods can be exploited to study <italic>reasoning behaviour</italic> by highlighting keywords which are conceptually identical to features of interest in medical LLMs. These explainable AI (XAI) methods compute an attribution score for each input feature to represent its contribution to a model’s prediction, which can be calculated and represented with a variety of metrics (<xref ref-type="bibr" rid="bib75">Sundararajan et al., 2017</xref>; <xref ref-type="bibr" rid="bib52">Lundberg, 2017</xref>). For example, a hypothetical medical scenario may show that the key words &quot;blocked nose&quot; are strongly weighted in a positive influenza prediction. In this context, the key word is equivalent to the reasoning trace, and is shown to impact the model’s <italic>reasoning behaviour</italic>. A conceptually similar strategy has been applied to explain predicted diseases from patient-doctor dialogues (<xref ref-type="bibr" rid="bib57">Ngai and Rudzicz, 2022</xref>).</p></sec><sec id="s5-4"><title>Interactive evaluation</title><p>Finally, a more open-ended approach to reviewing <italic>reasoning behaviour</italic> is interactive evaluation. Unique to other strategies, it is reactive and engages with the LLM directly during evaluation, adjusting questions to fit the model’s response. This deeper exploration of the ‘response space’ tests and further exposes the model’s reasoning capabilities as well as limitations (<xref ref-type="bibr" rid="bib108">Zhuang et al., 2024</xref>). Variants of interactive evaluation may challenge the model’s conclusions directly (<xref ref-type="bibr" rid="bib87">Wang et al., 2023</xref>) or use game-theoretical scenarios to probe reasoning depth (<xref ref-type="bibr" rid="bib6">Bertolazzi et al., 2024</xref>).</p><p>One notable implementation of this paradigm is Sequential Diagnosis Benchmark (SDBench) (<xref ref-type="bibr" rid="bib58">Nori et al., 2025</xref>). Most existing medical LLM benchmarks present models with all clinical facts at once and assess multiple-choice accuracy—conditions far removed from real-world diagnostic workflows. To address this, SDBench (<xref ref-type="bibr" rid="bib58">Nori et al., 2025</xref>) introduces a simulation-based framework that converts 304 NEJM Clinicopathological Conference (CPC) cases into interactive diagnostic scenarios. Here, the LLM or physician must sequentially ask patient history questions, order diagnostic tests (each with an associated monetary cost), and commit to a final diagnosis (<xref ref-type="bibr" rid="bib58">Nori et al., 2025</xref>)—closely mimicking real clinical cases under uncertainty and cost constraints. This setup not only enables evaluation of diagnostic accuracy but also the diagnostic yield of tests as measured by total cost per case. By transforming passive vignette-based evaluation into a structured, decision-based simulation, SDBench enhances realism while maintaining standardisation and reproducibility within interactive evaluation. However, it is not without limitations: the CPC dataset is biased towards complex cases where there are no healthy patients in the medical cases, test costs are drawn only from US price lists, and benchmarking physicians were restricted from using web search to prevent them from finding the original CPC cases (<xref ref-type="bibr" rid="bib58">Nori et al., 2025</xref>).</p><p>Unfortunately, a critical flaw of this evaluation method is its lack of reproducibility and standardisation due to its reactive nature. Currently, one exception exists, circumventing irreproducibility by side-stepping the requirement for a prompt (<xref ref-type="bibr" rid="bib88">Wang and Zhou, 2024</xref>). Nevertheless, we note the strong advantages of interactive evaluation and note that it remains relatively unexplored in the current medical LLM literature. Refinement of the core method and further investigation of strategies (such as the aforementioned prompt skipping; <xref ref-type="bibr" rid="bib88">Wang and Zhou, 2024</xref>) to counteract its limitations have the potential to raise its reproducibility to reasonable levels.</p></sec><sec id="s5-5"><title>Summary of evaluation strategies</title><p>To our surprise, we found few existing studies of <italic>reasoning behaviour</italic> evaluation in a medical LLM context. Nevertheless, we note several broad insights from the few existing studies matching our scope: (a) graph-theoretic approaches are intuitively applicable to evaluating causal or counterfactual <italic>reasoning behaviour</italic> due to their representation of cause-and-effect, (b) feature attribution methods provide a low-level glimpse into medical reasoning, (c) interactive evaluations like SDBench (<xref ref-type="bibr" rid="bib58">Nori et al., 2025</xref>) transform static multiple-choice medical benchmarks into interactive simulations that more realistically reflect clinical reasoning under uncertainty and resource constraints, (d) while manual clinician evaluations of reasoning traces offer nuanced insights that traditional metrics miss, but they are difficult to scale due to time and expertise requirements, and (e) <italic>reasoning behaviour</italic> evaluation methods are complementary, with the potential of being applied simultaneously to obtain a better understanding in cases where the model configuration allows.</p></sec></sec><sec id="s6"><title>Towards transparency in medical LLMs</title><p>Given our findings, we pose the central question: <bold>How can we develop methods that expose the underlying</bold> <italic><bold>reasoning behaviour</bold></italic> <bold>in medical LLMs?</bold></p><p>To answer this, we introduce two conceptual and theoretical frameworks intended to expose reasoning behaviour in medical LLMs with orthogonal design principles. These frameworks are designed to meet two important criteria: (a) low-level <italic>reasoning behaviour</italic> must be visible and the framework should be (b) task-agnostic. Each framework would consist of three broad stages: (a) data preprocessing, (b) model training, and (c) interpretability via extraction of <italic>reasoning behaviour</italic>. These designs—one straightforward and one relatively more complex—are detailed in ‘Theoretical frameworks for transparent reasoning behaviour’ and visualised in <xref ref-type="fig" rid="fig6">Figure 6</xref>.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Two frameworks with a focus on exposing <italic>reasoning behaviour</italic>.</title><p>Note that the two frameworks are independent but shown together to facilitate easier comparison. Top: input data is standardised and fed to tree-based models. The deterministic nature of trees is exploited for achieving transparency for <italic>reasoning behaviour</italic>. Bottom: an integrative framework of combining the complementary strengths of LLM and Symbolic Reasoning. The medical LLM extracts diagnostic rules from clinical algorithms, along with its chain-of-thought (CoT) reasoning and attention weights. These diagnostic rules, together with patient case inputs, are provided to the symbolic solver, which determines the final diagnosis and generates inference chains as its reasoning trace.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106187-fig6-v1.tif"/></fig><p>Additionally, we observe with interest the growing prominence and impressive capabilities of LRMs such as Anthropic’s Claude 3.7 Sonnet, Google’s Gemini 2.5, and OpenAI’s o1. Similarly, these models demonstrate key attributes aligned with our framework criteria: (a) visibility into low-level reasoning behaviour that supports interpretability, and (b) task-agnostic flexibility across domains. Although the precise details of their training pipelines remain undisclosed, these models likely rely on a combination of extensive supervised fine-tuning on curated reasoning datasets, large-scale reinforcement learning for reasoning alignment, and inference-time scaling techniques. These approaches have enabled such models to excel on existing benchmarks from math and logic benchmarks to coding benchmarks (<xref ref-type="bibr" rid="bib27">Guo et al., 2025</xref>; <xref ref-type="bibr" rid="bib98">Yang et al., 2024b</xref>; <xref ref-type="bibr" rid="bib5">Bai et al., 2025</xref>). Open-source alternatives such as DeepSeek R1 (<xref ref-type="bibr" rid="bib27">Guo et al., 2025</xref>) and Alibaba’s Qwen 2.0 (<xref ref-type="bibr" rid="bib98">Yang et al., 2024b</xref>), Qwen 2.5 (<xref ref-type="bibr" rid="bib5">Bai et al., 2025</xref>), as well as domain-specialised models like MedVLM-R1 (<xref ref-type="bibr" rid="bib61">Pan et al., 2025</xref>), MedR1 (<xref ref-type="bibr" rid="bib44">Lai et al., 2025</xref>), and MedFound (<xref ref-type="bibr" rid="bib50">Liu et al., 2025</xref>), have adopted broadly similar strategies tailored to medical contexts. In <bold>‘</bold>Open challenges in large reasoning models’, we outline several open challenges that constrain further progress in this space.</p><sec id="s6-1"><title>Theoretical frameworks with more transparent <italic>reasoning behaviour</italic></title><p>We begin the simplistic framework by restricting input data scope to standardised data formats. To this end, TEMED-LLM (<xref ref-type="bibr" rid="bib7">Bisercic et al., 2023</xref>) can be used to parse textual data into tables in the preprocessing stage with a predetermined format. Structured input has multiple advantages, being consistent and more easily ingested into software. In addition, a side effect is further simplification of data. An advantage of this which may benefit machine learning algorithms is a noise reduction while increasing variance in the data. However, we note that a degree of low-level feature loss is possible. Next, we consider that while deep learning is a powerful tool, more conventional machine learning approaches are often sufficient in many cases. We exploit the tabular nature of the data and leverage tree-based methods, which include examples such as XGBoost (<xref ref-type="bibr" rid="bib11">Chen and Guestrin, 2016</xref>) or Random Forest (<xref ref-type="bibr" rid="bib9">Breiman, 2001</xref>). While straightforward, these are effective and particularly suited to <italic>P</italic>&gt;&gt;n problems common across the biological sciences, where there are far more features per sample than there are samples, that is, the ‘curse of dimensionality’ (<xref ref-type="bibr" rid="bib96">Xu and Jackson, 2019</xref>). In addition, tree-like approaches have additionally benefited from properties that make them more interpretable. Exploiting this property allows us to generate decision sets which are interpretable during model training (<xref ref-type="bibr" rid="bib100">Yu et al., 2023a</xref>), hence exposing <italic>reasoning behaviour</italic>.</p><p>Our second proposed framework requires specific context. We refer to the categorisation of reasoning into two systems of thinking (<xref ref-type="bibr" rid="bib20">Evans and Stanovich, 2013</xref>; <xref ref-type="fig" rid="fig7">Figure 7</xref>). ‘System 1’ thinking refers to more instinctive decision-making based on learned patterns and experiences. Although conventional LLMs often fall into this category, they perform effectively given their vast input corpora, similar to a human memorising vast quantities of data. Conversely, ‘System 2’ thinking refers to more thorough and conscious thinking used in problem-solving and requires relatively more effort to achieve, both in humans and machine learning. Neither system operates fully independently, where the rapid assessments in ‘System 1’ thinking form the foundation for more methodical ‘System 2’ thinking. We note that we neither support nor disregard this overall viewpoint, but find value in using this angle to frame our proposed strategy, which addresses ‘System 1’ limitations inherent to conventional LLMs.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>An illustration of the spectrum of ‘System 1’ fundamental thought processes to ‘System 2’ analytical thought processes.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106187-fig7-v1.tif"/></fig><p>Next, we observe with interest that Symbolic AI (SAI) incorporating symbolic reasoning excels in ‘System 2’ thinking. In contrast, SAI has limited performance in ‘System 1’ thinking, lacking the capacity for rapid, intuitive pattern recognition and memorisation, hindering its performance in tasks where large volumes of unstructured data are processed. In order to utilise SAI more effectively, a more defined knowledge representation representing the search space with an optimal solution is often required.</p><p>Therefore, our final proposed framework synthesises LLMs and SAI, leveraging their complementary traits to supplement each other. The methodology consists of three core stages: (a) data preprocessing, (b) parallel model integration, and (c) reasoning extraction. One would first aim to generate a structured dataset which can be leveraged by LLM and SAI in data preprocessing, considering that many medical datasets contain highly unstructured data. Next, both LLM and SAI would be implemented. The purpose of the LLM would be to form a knowledge base capable of generating initial hypotheses, suggesting pertinent medical literature and proposing logical diagnostic rules inferred from clinical practice guidelines. Meanwhile, one would develop a formal SAI reasoning system encoding medical knowledge, clinical guidelines and diagnostic rules. By applying logical reasoning to patient data, conclusions would be consistent with established medical protocols. Finally, to trace <italic>reasoning behaviour</italic> it is possible to extract both attention weights and CoT-generated explanations from LLMs, as well as output logical inference chains from SAI. Subsequently combining the reasoning traces provides a more comprehensive glimpse into how the final conclusions were reached for medical professionals. Similar to its use in our second proposal, the PRM can be trained on reasoning traces—CoT explanations from the LLM, and inference chains from SAI—to evaluate reasoning behaviour. More specifically, the complementary nature of (a) assessing the logical coherence of these traces and (b) identifying potential logical fallacies lends additional credibility to the combined reasoning outputs.</p></sec><sec id="s6-2"><title>Open challenges in large reasoning models</title><sec id="s6-2-1"><title>Reasoning data and evaluation</title><sec id="s6-2-1-1"><title>Reasoning-knowledge entanglement</title><p>Current benchmarks conflate reasoning ability with domain knowledge, reflecting a tension between intuitive, memory retrieval-based (System 1) and analytic (System 2) reasoning processes. It remains challenging to determine whether the performance of a model is the result of genuine logical reasoning or simply retrieval of memorised facts with some generalisation from what was memorised. Evaluating genuine reasoning performance requires benchmarks intentionally designed to minimise prior core knowledge dependency, to isolate reasoning capabilities from knowledge recall. The <bold>ARC-AGI</bold> (‘Abstraction and Reasoning Corpus’ for Artificial General Intelligence) benchmark exemplifies this approach, assessing how efficiently an AI can abstract and generalise from limited input on visual tasks with minimal knowledge priors, relying on <italic>reasoning</italic> rather than memorised content (<xref ref-type="bibr" rid="bib14">Chollet, 2019</xref>; <xref ref-type="bibr" rid="bib15">Chollet et al., 2025</xref>). To the best of our knowledge, no equivalent benchmark currently exists for the medical domain.</p></sec><sec id="s6-2-1-2"><title>Limited availability of high-quality reasoning traces</title><p>CoT data are critical for training reasoning models, yet richly annotated, diverse medical CoT corpora remain relatively scarce and costly to produce with human annotators (i.e. clinicians). One promising remedy is to harness advanced LLMs themselves to generate synthetic CoT examples: their long context windows and fine-grained instruction-following abilities enable them to produce detailed, multi-step rationales on demand.</p></sec><sec id="s6-2-1-3"><title>Evaluation metrics beyond accuracy</title><p>Evaluating reasoning behaviour in medical LRMs requires moving beyond simple accuracy metrics, which often fail to capture the quality and structure of clinical reasoning processes. Instead, the focus should shift towards metrics that assess reasoning trace fidelity—such as logical soundness, coherence, and completeness. This approach is supported by recent work on CoT monitorability (<xref ref-type="bibr" rid="bib41">Korbak et al., 2025</xref>), which argues that CoTs offer a fragile but valuable window into model reasoning. Integrating CoT monitoring into model oversight enables early detection of incorrect or unsafe reasoning patterns, an important consideration in high-stakes domains like medicine.</p><p>Rubrics like CLEVER (<xref ref-type="bibr" rid="bib50">Liu et al., 2025</xref>) and R-IDEA (<xref ref-type="bibr" rid="bib19">Esteitieh et al., 2025</xref>; <xref ref-type="bibr" rid="bib10">Cabral et al., 2024</xref>), which have been developed to evaluate clinical reasoning in humans, offer a promising template for benchmarking LRMs. By scoring LRM-generated CoTs against these rubrics and comparing them directly with evaluations of human clinicians on the same tasks, we can derive more nuanced and clinically relevant assessments of LRM reasoning performance.</p><p>While the use of ‘judge’ LRMs offers a scalable path for automated reasoning trace evaluation—discussed in detail in the rationale-based evaluation section—it is important to acknowledge their limitations, particularly concerning bias (<xref ref-type="bibr" rid="bib47">Li et al., 2025</xref>). These models inherit patterns from their training data, which may embed societal stereotypes associated with race, gender, religion, culture, and ideology (<xref ref-type="bibr" rid="bib47">Li et al., 2025</xref>). Evaluating the robustness and fairness of such systems in medical contexts remains an essential direction for future work.</p></sec></sec></sec><sec id="s6-3"><title>Training and inference optimisations</title><sec id="s6-3-1"><title>Scalability of RL-based approaches</title><p>Reinforcement learning (RL) methods such as PPO <xref ref-type="bibr" rid="bib66">Schulman et al., 2017</xref>, DPO <xref ref-type="bibr" rid="bib64">Rafailov et al., 2023</xref>, and GRPO <xref ref-type="bibr" rid="bib68">Shao et al., 2024</xref> have shown strong potential in improving LRM reasoning capabilities. However, these approaches are computationally intensive and require significant resources, which can hinder widespread adoption. The key challenge is to develop RL-based reasoning techniques that are less computationally intensive without compromising performance.</p></sec><sec id="s6-3-2"><title>Medicine/healthcare specific reward models or reward functions</title><p>The generalisability of reward models and functions developed for broad-domain LRMs to medical reasoning tasks remains uncertain, given the domain-specific nuances of clinical decision-making. One promising direction involves incorporating high-quality evaluations of reasoning traces directly into the reward modelling process, thereby enhancing reasoning alignment. This approach aligns with the principles of process supervision (<xref ref-type="bibr" rid="bib49">Lightman et al., 2023</xref>), which has been shown to outperform outcome supervision—particularly in tackling complex problems in mathematics (<xref ref-type="bibr" rid="bib49">Lightman et al., 2023</xref>; <xref ref-type="bibr" rid="bib82">Uesato et al., 2022</xref>). While process supervision traditionally depends on labour-intensive human annotation of step-by-step reasoning, this limitation can be addressed through process reward models (PRMs) (<xref ref-type="bibr" rid="bib49">Lightman et al., 2023</xref>): customised reward systems trained on annotated CoT traces to automate reasoning evaluation during RLHF. These models assess logical coherence and detect fallacies, preserving transparency without requiring constant human input. An intriguing extension is to integrate LRM-based reasoning trace evaluators as those covered in ’Rationale-based evaluations’ directly into the RLHF pipeline, enabling scalable process supervision as explored in the rationale-based evaluation paradigm.</p></sec><sec id="s6-3-3"><title>Scaling laws for LRM inference</title><p>Studies have demonstrated that increasing compute during inference-time—rather than solely during training—can significantly enhance the reasoning performance of LRMs (<xref ref-type="bibr" rid="bib73">Snell et al., 2024</xref>; <xref ref-type="bibr" rid="bib104">Zhao et al., 2024</xref>). Techniques like CoT and ToT (<xref ref-type="bibr" rid="bib99">Yao et al., 2023</xref>) allow models to generate multiple intermediate reasoning paths, which can then be evaluated or expanded upon. This scaling behaviour is further supported by methods such as self-reflection (<xref ref-type="bibr" rid="bib104">Zhao et al., 2024</xref>), Monte Carlo Tree Search (MCTS) (<xref ref-type="bibr" rid="bib104">Zhao et al., 2024</xref>), and PRMs (<xref ref-type="bibr" rid="bib73">Snell et al., 2024</xref>), which help verify and select coherent reasoning sequences during inference. Additionally, integrating fact-checking mechanisms via web-enabled LLMs adds a layer of external validation, enabling more accurate and trustworthy output. Together, these approaches illustrate how smarter use of inference-time compute can unlock deeper reasoning capabilities without retraining.</p></sec></sec><sec id="s6-4"><title>Alternative reasoning architectures</title><p>LLMs and subsequently LRMs rely on left-to-right, next-token prediction, limiting their capacity to plan, revise, or backtrack during inference. This property occurs as a result of the autoregressive nature of generative LLMs, which produce an output token by selecting a token with the highest probability score based on previously generated tokens. Should a model choose one ‘nonsense’ token in context, subsequent tokens are similarly affected. A compounding effect can occur, quickly worsening the error unless the model has the ability to backtrack. To achieve more structured and accurate reasoning, alternative architectures that emphasise planning-based non-autoregressive strategies, such as JEPA (<xref ref-type="bibr" rid="bib4">Assran et al., 2023</xref>), warrant further exploration. Another promising alternative is latent reasoning (<xref ref-type="bibr" rid="bib107">Zhu et al., 2025</xref>; <xref ref-type="bibr" rid="bib29">Hao et al., 2024</xref>), where models perform internal, iterative computations on hidden states before outputting tokens, enabling internal planning, error correction, and revision prior to generating any output.</p></sec><sec id="s6-5"><title>Multimodal clinical reasoning integration</title><p>Clinical data is inherently multimodal, involving the integration of information across text, medical imaging, electronic health records (EHRs), sensor data (e.g. from smartwatches), and physiological time-series such as ECG or EEG. The ability of LRMs to unify understanding and reasoning across these diverse medical modalities within a single cohesive system indicates an exciting direction for future research. To advance LRM’s multimodal reasoning capabilities, we require complex reasoning data synthesis pipelines that go beyond the current paradigm, which largely focuses on single modalities. The path forward involves aligning multiple modalities simultaneously and building rich, interleaved reasoning chains that reflect how clinicians synthesise diverse data sources to arrive at diagnostic and treatment decisions.</p></sec></sec><sec id="s7" sec-type="discussion"><title>Discussion</title><sec id="s7-1"><title>A striking lack of evaluation methods for reasoning behaviour</title><p>As part of our study, we intended to investigate the current SOTA of medical LLM performance in the context of <italic>reasoning behaviour</italic>. However, we found that only a few existing evaluation frameworks adequately capture the nuances involved in assessing <italic>reasoning behaviour</italic> in medical LLMs. This is perhaps unsurprising, given the inherent complexity of such evaluations—particularly those requiring human clinicians. To manually assess reasoning traces of LLMs is time-consuming and difficult to scale.</p><p>While conventional conclusion-based evaluations are not without value, they offer only a limited view of <italic>reasoning behaviour</italic> as they focus solely on final answers without analysing the reasoning process. Evaluating reasoning traces in LLMs is crucial, especially in medical contexts, as it is possible for models to reach the correct conclusion through erroneous reasoning. Therefore, an AI that not only gives a diagnosis but also provides a corresponding rationale behind it can significantly aid adoption among clinicians and other healthcare professionals by virtue of its transparency which fosters trust in its recommendations. Broadly, we consider that rationale-based, interactive and mechanistic evaluation are more naturally predisposed to decrypting the <italic>reasoning behaviour</italic> of medical LLMs. Interactive evaluations such as the SDBench offer a promising alternative to static MCQ benchmarks by simulating interactive diagnostic scenarios that better mirror real clinical workflows.</p></sec><sec id="s7-2"><title>Memorisation vs. planning: The ‘stochastic parrot’ problem</title><p>Another potential issue is the lack of memorisation tests and benchmarks in LLM. This is pertinent as (like humans) medical LLMs have the ability to memorise the dataset they are given but on a much grander scale, giving the illusion of reasoning, although in reality regurgitating related or unrelated information from a vast knowledge base (like humans) (<xref ref-type="bibr" rid="bib30">Hartmann et al., 2023</xref>). Hence, in many cases it was not possible to answer the question: &quot;<bold>to what extent is this model a stochastic parrot and to what extent is this model performing logical reasoning?</bold>&quot; To answer this question, a structured approach would involve testing its ability to work from foundational first principles, or ‘base facts’, embedded within its training corpus. These base facts encompass simple yet essential principles across areas such as medicine, physiology, and pharmacology, often representing core medical knowledge. For example, a base fact in medicine could be: ‘The heart pumps blood throughout the body’. Reasoning tests can then be designed to see if the model can apply such base facts to answer more complex questions.</p><p>In practice, however, access to high-volume training corpora for closed-source enterprise models like GPT-4 (<xref ref-type="bibr" rid="bib92">Wu et al., 2023</xref>), Gemini (<xref ref-type="bibr" rid="bib3">Anil et al., 2023</xref>), or Anthropic (<xref ref-type="bibr" rid="bib18">Enis and Hopkins, 2024</xref>) is restricted. This limitation calls for designing medical tests that embed low-level fundamental knowledge, and relying primarily on the model’s ability to reason from these base facts. Nevertheless, we do not intend to diminish the usefulness of ‘System 1’ rote-memorisation type LLMs (which are a prerequisite for ‘System 2’ advanced-reasoning type systems), but instead wish to highlight the lack of insight into a model’s <italic>reasoning behaviour</italic> without this layer of validation.</p></sec><sec id="s7-3"><title>Hallucination and security implications of opaque reasoning</title><p>A natural side effect of obtaining transparency into medical <italic>reasoning behaviour</italic> is its neutralising effect on hallucination events common across LLMs in all domains (<xref ref-type="bibr" rid="bib36">Huang et al., 2023</xref>). Such events occur due to the autoregressive nature of generative LLMs, which produce an output by selecting a token with a highest probability score. Should a model choose one ‘nonsense’ token in context, subsequent tokens are similarly affected. A compounding effect can occur, quickly worsening the error unless the model has the ability to backtrack. However, exploring <italic>reasoning behaviour</italic> allows greater insight into hallucination events by exposing the involved logic chain, complementing the current state of the art of retrieval-augmented generation (RAG) (<xref ref-type="bibr" rid="bib95">Xia et al., 2024b</xref>).</p><p>More ominously, it is an unfortunate reality that cybercrime is increasingly common, and it is not impossible that healthcare infrastructure, including associated medical LLMs, may be targeted. While this commonly takes the form of ransomware, it takes disturbingly little effort to ‘poison’ medical LLMs with misinformation (<xref ref-type="bibr" rid="bib28">Han et al., 2024</xref>), with myriad implications for those used for clinical diagnosis or hospital operations. This aspect of medical LLMs is a relatively unexplored field, with most studies focusing on generic use cases (<xref ref-type="bibr" rid="bib62">Peng et al., 2024</xref>), though a more comprehensive framework incorporating the paradigms discussed in our review as well as this security aspect exists for medical vision language models (<xref ref-type="bibr" rid="bib94">Xia et al., 2024a</xref>). As with exposing hallucination events, greater transparency into medical models will assist in identifying such events should the situation arise.</p></sec><sec id="s7-4"><title>Reasoning behaviour as a form of explainable AI</title><p>Enhanced medical reasoning transparency may contribute to solving ongoing problems such as addressing differential diagnosis or providing clinical management plans. Differential diagnosis is an ongoing problem in medical AI development, where similar conditions may confound a prediction with potentially severe consequences. By viewing reasoning traces, both method developers and clinicians will be able to better discern why an accurate or inaccurate choice is made and adjust the model accordingly as well as gaining potentially unknown clinical insights.</p><p><bold>Finally, we highlight the point that understanding</bold> <italic><bold>reasoning behaviour</bold></italic> <bold>of medical LLMs is functionally equivalent to achieving XAI</bold> and is not mutually exclusive with other XAI techniques or evaluation paradigms discussed. Given the understandably high level of scrutiny placed on medical methods, achieving this deeper level of understanding is necessary to demonstrate the effectiveness of medical LLMs. At the same time, we may be able to answer an interesting question: ‘Is improved reasoning correlated to improved performance?’ Ultimately, understanding the <italic>reasoning behaviour</italic> of medical LLMs and LLMs in general has applications across all domains, especially since LLMs are also effective in computer vision tasks.</p></sec></sec><sec id="s8" sec-type="conclusions"><title>Conclusion</title><p>In summary, it is intuitive that a greater understanding of the <italic>reasoning behaviour</italic> of medical LLMs empowers clinicians, improves patient trust in them, and allows machine learning engineers to troubleshoot underperforming models. However, the lack of studies focusing on understanding <italic>reasoning behaviour</italic> is striking, where the majority of studies are focused on achieving high-level performance metrics with a conspicuous lack of focus on XAI. Most <italic>reasoning behaviour</italic> evaluation strategies are in their infancy, though there is notable potential for growth and further studies. Our theoretical proposed frameworks, while limited, can contribute to XAI in clinical LLMs, with the ultimate goal of improving transparency in medical AI and subsequently patient outcomes.</p></sec></body><back><sec sec-type="additional-information" id="s9"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Eric Lee Kuan Hui for facilitating the initial meeting between the authors and subsequent collaboration. We thank Jim Liew Jun Fei, Pui Wee Yang, and Winston Lee Meng Yih for critically reviewing the manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Achiam</surname><given-names>J</given-names></name><name><surname>Adler</surname><given-names>S</given-names></name><name><surname>Agarwal</surname><given-names>S</given-names></name><name><surname>Ahmad</surname><given-names>L</given-names></name><name><surname>Akkaya</surname><given-names>I</given-names></name><name><surname>Aleman</surname><given-names>FL</given-names></name><name><surname>Almeida</surname><given-names>D</given-names></name><name><surname>Altenschmidt</surname><given-names>J</given-names></name><name><surname>Altman</surname><given-names>S</given-names></name><name><surname>Anadkat</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Gpt-4 technical report</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2303.08774</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Alammar</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Ecco: an open source library for the explainability of transformer language models</article-title><conf-name>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</conf-name><fpage>249</fpage><lpage>257</lpage><pub-id pub-id-type="doi">10.18653/v1/2021.acl-demo.30</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Anil</surname><given-names>R</given-names></name><name><surname>Borgeaud</surname><given-names>S</given-names></name><name><surname>Alayrac</surname><given-names>J</given-names></name><name><surname>Yu</surname><given-names>J</given-names></name><name><surname>Soricut</surname><given-names>R</given-names></name><name><surname>Schalkwyk</surname><given-names>J</given-names></name><name><surname>Dai</surname><given-names>AM</given-names></name><name><surname>Hauth</surname><given-names>A</given-names></name><name><surname>Millican</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Gemini: A family of highly capable multimodal models</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2312.11805</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Assran</surname><given-names>M</given-names></name><name><surname>Duval</surname><given-names>Q</given-names></name><name><surname>Misra</surname><given-names>I</given-names></name><name><surname>Bojanowski</surname><given-names>P</given-names></name><name><surname>Vincent</surname><given-names>P</given-names></name><name><surname>Rabbat</surname><given-names>M</given-names></name><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Ballas</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Self-supervised learning from images with a joint-embedding predictive architecture</article-title><conf-name>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><conf-loc>Vancouver, BC, Canada</conf-loc><fpage>15619</fpage><lpage>15629</lpage><pub-id pub-id-type="doi">10.1109/CVPR52729.2023.01499</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bai</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>K</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Ge</surname><given-names>W</given-names></name><name><surname>Song</surname><given-names>S</given-names></name><name><surname>Dang</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Tang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Qwen2. 5-vl technical report</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2502.13923</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bertolazzi</surname><given-names>L</given-names></name><name><surname>Gatt</surname><given-names>A</given-names></name><name><surname>Bernardi</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>A systematic analysis of large language models as soft reasoners: the case of syllogistic inferences</article-title><conf-name>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</conf-name><pub-id pub-id-type="doi">10.18653/v1/2024.emnlp-main.769</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bisercic</surname><given-names>A</given-names></name><name><surname>Nikolic</surname><given-names>M</given-names></name><name><surname>Delibasic</surname><given-names>B</given-names></name><name><surname>Lio</surname><given-names>P</given-names></name><name><surname>Petrovic</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Interpretable medical diagnostics with structured data extraction by large language models</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2306.05052</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bragazzi</surname><given-names>NL</given-names></name><name><surname>Garbarino</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Toward clinical generative ai: conceptual framework</article-title><source>JMIR AI</source><volume>3</volume><elocation-id>e55957</elocation-id><pub-id pub-id-type="doi">10.2196/55957</pub-id><pub-id pub-id-type="pmid">38875592</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Random Forests</article-title><source>Machine Learning</source><volume>45</volume><fpage>5</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cabral</surname><given-names>S</given-names></name><name><surname>Restrepo</surname><given-names>D</given-names></name><name><surname>Kanjee</surname><given-names>Z</given-names></name><name><surname>Wilson</surname><given-names>P</given-names></name><name><surname>Crowe</surname><given-names>B</given-names></name><name><surname>Abdulnour</surname><given-names>R-E</given-names></name><name><surname>Rodman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Clinical reasoning of a generative artificial intelligence model compared with physicians</article-title><source>JAMA Internal Medicine</source><volume>184</volume><fpage>581</fpage><lpage>583</lpage><pub-id pub-id-type="doi">10.1001/jamainternmed.2024.0295</pub-id><pub-id pub-id-type="pmid">38557971</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Guestrin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Xgboost: a scalable tree boosting system</article-title><conf-name>In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</conf-name><fpage>785</fpage><lpage>794</lpage><pub-id pub-id-type="doi">10.1145/2939672.2939785</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Vahab</surname><given-names>N</given-names></name><name><surname>Tyagi</surname><given-names>N</given-names></name><name><surname>Cummins</surname><given-names>E</given-names></name><name><surname>Peleg</surname><given-names>AY</given-names></name><name><surname>Tyagi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title><italic>genomicBERT</italic>: A light-weight foundation model for genome analysis using unigram tokenization and specialized DNA vocabulary</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.05.31.542682</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Cai</surname><given-names>Z</given-names></name><name><surname>Ji</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Hou</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Huatuogpt-O1, towards medical complex reasoning with Llms</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2412.18925</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chollet</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>On the measure of intelligence</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1911.01547</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chollet</surname><given-names>F</given-names></name><name><surname>Knoop</surname><given-names>M</given-names></name><name><surname>Kamradt</surname><given-names>G</given-names></name><name><surname>Landers</surname><given-names>B</given-names></name><name><surname>Pinkard</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Arc-agi-2: a new challenge for frontier ai reasoning systems</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2505.11831</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Christiano</surname><given-names>PF</given-names></name><name><surname>Leike</surname><given-names>J</given-names></name><name><surname>Brown</surname><given-names>T</given-names></name><name><surname>Martic</surname><given-names>M</given-names></name><name><surname>Legg</surname><given-names>S</given-names></name><name><surname>Amodei</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Deep reinforcement learning from human preferences</article-title><conf-name>Advances in Neural Information Processing Systems 30</conf-name></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dewitte</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Better alone than in bad company: Addressing the risks of companion chatbots through data protection by design</article-title><source>Computer Law &amp; Security Review</source><volume>54</volume><elocation-id>106019</elocation-id><pub-id pub-id-type="doi">10.1016/j.clsr.2024.106019</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Enis</surname><given-names>M</given-names></name><name><surname>Hopkins</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>From Llm to Nmt: advancing low-resource machine translation with claude</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2404.13813</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Esteitieh</surname><given-names>Y</given-names></name><name><surname>Mandal</surname><given-names>S</given-names></name><name><surname>Laliotis</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Towards metacognitive clinical reasoning: benchmarking MD-PIE against state-of-the-Art LLMs in medical decision-making</article-title><source>medRxiv</source><pub-id pub-id-type="doi">10.1101/2025.01.28.25321282</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>JSBT</given-names></name><name><surname>Stanovich</surname><given-names>KE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dual-process theories of higher cognition: advancing the debate</article-title><source>Perspectives on Psychological Science</source><volume>8</volume><fpage>223</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1177/1745691612460685</pub-id><pub-id pub-id-type="pmid">26172965</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Foraita</surname><given-names>R</given-names></name><name><surname>Spallek</surname><given-names>J</given-names></name><name><surname>Zeeb</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Directed acyclic graphs</chapter-title><person-group person-group-type="editor"><name><surname>Foraita</surname><given-names>R</given-names></name><name><surname>Spallek</surname><given-names>J</given-names></name></person-group><source>In Handbook of Epidemiology</source><publisher-name>Springer Nature</publisher-name><fpage>1481</fpage><lpage>1517</lpage><pub-id pub-id-type="doi">10.1007/978-0-387-09834-0_65</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Dligach</surname><given-names>D</given-names></name><name><surname>Miller</surname><given-names>T</given-names></name><name><surname>Caskey</surname><given-names>J</given-names></name><name><surname>Sharma</surname><given-names>B</given-names></name><name><surname>Churpek</surname><given-names>MM</given-names></name><name><surname>Afshar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023a</year><article-title>DR.BENCH: diagnostic reasoning benchmark for clinical natural language processing</article-title><source>Journal of Biomedical Informatics</source><volume>138</volume><elocation-id>104286</elocation-id><pub-id pub-id-type="doi">10.1016/j.jbi.2023.104286</pub-id><pub-id pub-id-type="pmid">36706848</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>R</given-names></name><name><surname>Croxford</surname><given-names>E</given-names></name><name><surname>Tesch</surname><given-names>S</given-names></name><name><surname>To</surname><given-names>D</given-names></name><name><surname>Caskey</surname><given-names>J</given-names></name><name><surname>W. Patterson</surname><given-names>B</given-names></name><name><surname>M. Churpek</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>T</given-names></name><name><surname>Dligach</surname><given-names>D</given-names></name><name><surname>Afshar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023b</year><article-title>Large language models and medical knowledge grounding for diagnosis prediction</article-title><source>medRxiv</source><pub-id pub-id-type="doi">10.1101/2023.11.24.23298641</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gegg-Harrison</surname><given-names>W</given-names></name><name><surname>Quarterman</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2024">2024</year><chapter-title>AI detection’s high false positive rates and the psychological and material impacts on students</chapter-title><person-group person-group-type="editor"><name><surname>Gegg-Harrison</surname><given-names>W</given-names></name><name><surname>Quarterman</surname><given-names>C</given-names></name></person-group><source>In Academic Integrity in the Age of Artificial Intelligence</source><publisher-name>IGI Global</publisher-name><fpage>199</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.4018/979-8-3693-0240-8.ch011</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gopalakrishnan</surname><given-names>S</given-names></name><name><surname>Garbayo</surname><given-names>L</given-names></name><name><surname>Zadrozny</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Causality extraction from medical text using Large Language Models (LLMs)</article-title><source>Information</source><volume>16</volume><elocation-id>10013</elocation-id><pub-id pub-id-type="doi">10.3390/info16010013</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouveia</surname><given-names>SS</given-names></name><name><surname>Malík</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Crossing the trust gap in medical AI: building an abductive bridge for xAI</article-title><source>Philosophy &amp; Technology</source><volume>37</volume><elocation-id>105</elocation-id><pub-id pub-id-type="doi">10.1007/s13347-024-00790-4</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>D</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Song</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Xu</surname><given-names>R</given-names></name><name><surname>Zhu</surname><given-names>Q</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>P</given-names></name><name><surname>Bi</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Deepseek-R1: incentivizing reasoning capability in Llms via reinforcement learning</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2501.12948</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>T</given-names></name><name><surname>Nebelung</surname><given-names>S</given-names></name><name><surname>Khader</surname><given-names>F</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Müller-Franzes</surname><given-names>G</given-names></name><name><surname>Kuhl</surname><given-names>C</given-names></name><name><surname>Försch</surname><given-names>S</given-names></name><name><surname>Kleesiek</surname><given-names>J</given-names></name><name><surname>Haarburger</surname><given-names>C</given-names></name><name><surname>Bressem</surname><given-names>KK</given-names></name><name><surname>Kather</surname><given-names>JN</given-names></name><name><surname>Truhn</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Medical large language models are susceptible to targeted misinformation attacks</article-title><source>NPJ Digital Medicine</source><volume>7</volume><elocation-id>288</elocation-id><pub-id pub-id-type="doi">10.1038/s41746-024-01282-7</pub-id><pub-id pub-id-type="pmid">39443664</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hao</surname><given-names>S</given-names></name><name><surname>Sukhbaatar</surname><given-names>S</given-names></name><name><surname>Su</surname><given-names>D</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Hu</surname><given-names>Z</given-names></name><name><surname>Weston</surname><given-names>J</given-names></name><name><surname>Tian</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Training large language models to reason in a continuous latent space</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2412.06769</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hartmann</surname><given-names>V</given-names></name><name><surname>Suri</surname><given-names>A</given-names></name><name><surname>Bindschaedler</surname><given-names>V</given-names></name><name><surname>Evans</surname><given-names>D</given-names></name><name><surname>Tople</surname><given-names>S</given-names></name><name><surname>West</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>SoK: memorization in general-purpose large language models</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2310.18362</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hendrycks</surname><given-names>D</given-names></name><name><surname>Burns</surname><given-names>C</given-names></name><name><surname>Basart</surname><given-names>S</given-names></name><name><surname>Zou</surname><given-names>A</given-names></name><name><surname>Mazeika</surname><given-names>M</given-names></name><name><surname>Song</surname><given-names>D</given-names></name><name><surname>Steinhardt</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Measuring massive multitask language understanding</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2009.03300</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Holyoak</surname><given-names>KJ</given-names></name><name><surname>Morrison</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="1999">1999</year><source>Cambridge Handbook of Thinking and Reasoning</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Homolak</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Opportunities and risks of ChatGPT in medicine, science, and academic publishing: a modern Promethean dilemma</article-title><source>Croatian Medical Journal</source><volume>64</volume><fpage>1</fpage><lpage>3</lpage><pub-id pub-id-type="doi">10.3325/cmj.2023.64.1</pub-id><pub-id pub-id-type="pmid">36864812</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>S</given-names></name><name><surname>Xiao</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>ArgMed-agents: explainable clinical decision reasoning with LLM disscusion via argumentation schemes</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2403.06294</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Horsten</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Philosophy of Mathematics</source><publisher-name>Stanford Encyclopedia of Philosophy</publisher-name></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Yu</surname><given-names>W</given-names></name><name><surname>Ma</surname><given-names>W</given-names></name><name><surname>Zhong</surname><given-names>W</given-names></name><name><surname>Feng</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Peng</surname><given-names>W</given-names></name><name><surname>Feng</surname><given-names>X</given-names></name><name><surname>Qin</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A survey on hallucination in large language models: principles, taxonomy, challenges, and open questions</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2311.05232</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>Q</given-names></name><name><surname>Dhingra</surname><given-names>B</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Cohen</surname><given-names>W</given-names></name><name><surname>Lu</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PubMedQA: A dataset for biomedical research question answering</article-title><conf-name>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</conf-name><conf-loc>Hong Kong, China</conf-loc><pub-id pub-id-type="doi">10.18653/v1/D19-1259</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>D</given-names></name><name><surname>Pan</surname><given-names>E</given-names></name><name><surname>Oufattole</surname><given-names>N</given-names></name><name><surname>Weng</surname><given-names>WH</given-names></name><name><surname>Fang</surname><given-names>H</given-names></name><name><surname>Szolovits</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>What disease does this patient have? a large-scale open domain question answering dataset from medical exams</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2009.13081</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>AEW</given-names></name><name><surname>Pollard</surname><given-names>TJ</given-names></name><name><surname>Shen</surname><given-names>L</given-names></name><name><surname>Lehman</surname><given-names>LWH</given-names></name><name><surname>Feng</surname><given-names>M</given-names></name><name><surname>Ghassemi</surname><given-names>M</given-names></name><name><surname>Moody</surname><given-names>B</given-names></name><name><surname>Szolovits</surname><given-names>P</given-names></name><name><surname>Celi</surname><given-names>LA</given-names></name><name><surname>Mark</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>MIMIC-III, a freely accessible critical care database</article-title><source>Scientific Data</source><volume>3</volume><elocation-id>160035</elocation-id><pub-id pub-id-type="doi">10.1038/sdata.2016.35</pub-id><pub-id pub-id-type="pmid">27219127</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kiciman</surname><given-names>E</given-names></name><name><surname>Ness</surname><given-names>R</given-names></name><name><surname>Sharma</surname><given-names>A</given-names></name><name><surname>Tan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Causal reasoning and large language models: opening a new frontier for causality</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2305.00050</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Korbak</surname><given-names>T</given-names></name><name><surname>Balesni</surname><given-names>M</given-names></name><name><surname>Barnes</surname><given-names>E</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Benton</surname><given-names>J</given-names></name><name><surname>Bloom</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Cooney</surname><given-names>A</given-names></name><name><surname>Dafoe</surname><given-names>A</given-names></name><name><surname>Dragan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Chain of thought monitorability: a new and fragile opportunity for Ai safety</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2507.11473</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwon</surname><given-names>JM</given-names></name><name><surname>Kim</surname><given-names>KH</given-names></name><name><surname>Jeon</surname><given-names>KH</given-names></name><name><surname>Lee</surname><given-names>SE</given-names></name><name><surname>Lee</surname><given-names>HY</given-names></name><name><surname>Cho</surname><given-names>HJ</given-names></name><name><surname>Choi</surname><given-names>JO</given-names></name><name><surname>Jeon</surname><given-names>ES</given-names></name><name><surname>Kim</surname><given-names>MS</given-names></name><name><surname>Kim</surname><given-names>JJ</given-names></name><name><surname>Hwang</surname><given-names>KK</given-names></name><name><surname>Chae</surname><given-names>SC</given-names></name><name><surname>Baek</surname><given-names>SH</given-names></name><name><surname>Kang</surname><given-names>SM</given-names></name><name><surname>Choi</surname><given-names>DJ</given-names></name><name><surname>Yoo</surname><given-names>BS</given-names></name><name><surname>Kim</surname><given-names>KH</given-names></name><name><surname>Park</surname><given-names>HY</given-names></name><name><surname>Cho</surname><given-names>MC</given-names></name><name><surname>Oh</surname><given-names>BH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Artificial intelligence algorithm for predicting mortality of patients with acute heart failure</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0219302</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0219302</pub-id><pub-id pub-id-type="pmid">31283783</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kwon</surname><given-names>T</given-names></name><name><surname>Ong</surname><given-names>KT</given-names></name><name><surname>Kang</surname><given-names>D</given-names></name><name><surname>Moon</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>JR</given-names></name><name><surname>Hwang</surname><given-names>D</given-names></name><name><surname>Sohn</surname><given-names>B</given-names></name><name><surname>Sim</surname><given-names>Y</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Yeo</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Large language models are clinical reasoners: reasoning-aware diagnosis framework with prompt-generated rationales</article-title><conf-name>Proceedings of the AAAI Conference on Artificial Intelligence</conf-name><fpage>18417</fpage><lpage>18425</lpage><pub-id pub-id-type="doi">10.1609/aaai.v38i16.29802</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lai</surname><given-names>Y</given-names></name><name><surname>Zhong</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Zhao</surname><given-names>S</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Med-R1: reinforcement learning for generalizable medical reasoning in vision-language models</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2503.13939</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Li</surname><given-names>B</given-names></name><name><surname>Meng</surname><given-names>T</given-names></name><name><surname>Shi</surname><given-names>X</given-names></name><name><surname>Zhai</surname><given-names>J</given-names></name><name><surname>Ruan</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Meddm: Llm-executable clinical guidance tree for clinical decision-making</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2312.02441</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Li</surname><given-names>SS</given-names></name><name><surname>Balachandran</surname><given-names>V</given-names></name><name><surname>Feng</surname><given-names>S</given-names></name><name><surname>Ilgen</surname><given-names>J</given-names></name><name><surname>Pierson</surname><given-names>E</given-names></name><name><surname>Koh</surname><given-names>PW</given-names></name><name><surname>Tsvetkov</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>MEDIQ: question-asking llms for adaptive and reliable medical reasoning</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2406.00922</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Jiang</surname><given-names>B</given-names></name><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Beigi</surname><given-names>A</given-names></name><name><surname>Zhao</surname><given-names>C</given-names></name><name><surname>Tan</surname><given-names>Z</given-names></name><name><surname>Bhattacharjee</surname><given-names>A</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Wu</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>From generation to judgment: opportunities and challenges of Llm-as-a-Judge</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2411.16594</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>Y</given-names></name><name><surname>Meng</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Automatic interactive evaluation for large language models with state aware patient simulator</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2403.08495</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lightman</surname><given-names>H</given-names></name><name><surname>Kosaraju</surname><given-names>V</given-names></name><name><surname>Burda</surname><given-names>Y</given-names></name><name><surname>Edwards</surname><given-names>H</given-names></name><name><surname>Baker</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>T</given-names></name><name><surname>Leike</surname><given-names>J</given-names></name><name><surname>Schulman</surname><given-names>J</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Cobbe</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Let’s verify step by step</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2305.20050</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Yang</surname><given-names>G</given-names></name><name><surname>Jiang</surname><given-names>Z</given-names></name><name><surname>Cui</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Tao</surname><given-names>L</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Song</surname><given-names>Z</given-names></name><name><surname>Hong</surname><given-names>T</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Gao</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Sang</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Xue</surname><given-names>K</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Song</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>A generalist medical language model for disease diagnosis assistance</article-title><source>Nature Medicine</source><volume>31</volume><fpage>932</fpage><lpage>942</lpage><pub-id pub-id-type="doi">10.1038/s41591-024-03416-6</pub-id><pub-id pub-id-type="pmid">39779927</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lorek</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="2024">2024</year><source>AI Legal Innovations: The Benefits and Drawbacks of Chat-Gpt and Generative AI in the Legal Industry</source><publisher-name>Ohio Northern University Law Review</publisher-name></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lundberg</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A unified approach to interpreting model predictions</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1705.07874</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>RA</given-names></name><name><surname>Pople</surname><given-names>HE</given-names></name><name><surname>Myers</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="1985">1985</year><chapter-title>Internist-i, an experimental computer-based diagnostic consultant for general internal medicine</chapter-title><person-group person-group-type="editor"><name><surname>Miller</surname><given-names>RA</given-names></name><name><surname>Pople</surname><given-names>HE</given-names></name></person-group><source>In Computer-Assisted Medical Decision Making</source><publisher-name>Springer</publisher-name><fpage>139</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1007/978-1-4612-5108-8_8</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moëll</surname><given-names>B</given-names></name><name><surname>Sand Aronsson</surname><given-names>F</given-names></name><name><surname>Akbar</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Medical reasoning in LLMs: an in-depth analysis of DeepSeek R1</article-title><source>Frontiers in Artificial Intelligence</source><volume>8</volume><elocation-id>1616145</elocation-id><pub-id pub-id-type="doi">10.3389/frai.2025.1616145</pub-id><pub-id pub-id-type="pmid">40607450</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mondorf</surname><given-names>P</given-names></name><name><surname>Plank</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Beyond accuracy: evaluating the reasoning behavior of large language models–a survey</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2404.01869</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Naik</surname><given-names>N</given-names></name><name><surname>Khandelwal</surname><given-names>A</given-names></name><name><surname>Joshi</surname><given-names>M</given-names></name><name><surname>Atre</surname><given-names>M</given-names></name><name><surname>Wright</surname><given-names>H</given-names></name><name><surname>Kannan</surname><given-names>K</given-names></name><name><surname>Hill</surname><given-names>S</given-names></name><name><surname>Mamidipudi</surname><given-names>G</given-names></name><name><surname>Srinivasa</surname><given-names>G</given-names></name><name><surname>Bifulco</surname><given-names>CB</given-names></name><name><surname>Piening</surname><given-names>B</given-names></name><name><surname>Matlock</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Applying large language models for causal structure learning in non small cell lung cancer</article-title><conf-name>2024 IEEE 12th International Conference on Healthcare Informatics (ICHI)</conf-name><conf-loc>Orlando, FL, USA</conf-loc><fpage>688</fpage><lpage>693</lpage><pub-id pub-id-type="doi">10.1109/ICHI61247.2024.00110</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ngai</surname><given-names>H</given-names></name><name><surname>Rudzicz</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Doctor XAvIer: explainable diagnosis on physician-patient dialogues and XAI evaluation</article-title><conf-name>Proceedings of the 21st Workshop on Biomedical Language Processing</conf-name><conf-loc>Dublin, Ireland</conf-loc><pub-id pub-id-type="doi">10.18653/v1/2022.bionlp-1.33</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Nori</surname><given-names>H</given-names></name><name><surname>Daswani</surname><given-names>M</given-names></name><name><surname>Kelly</surname><given-names>C</given-names></name><name><surname>Lundberg</surname><given-names>S</given-names></name><name><surname>Ribeiro</surname><given-names>MT</given-names></name><name><surname>Wilson</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Sounderajah</surname><given-names>V</given-names></name><name><surname>Carlson</surname><given-names>J</given-names></name><name><surname>Lungren</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Sequential diagnosis with language models</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2506.22405</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ouyang</surname><given-names>L</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Jiang</surname><given-names>X</given-names></name><name><surname>Almeida</surname><given-names>D</given-names></name><name><surname>Wainwright</surname><given-names>C</given-names></name><name><surname>Mishkin</surname><given-names>P</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Agarwal</surname><given-names>S</given-names></name><name><surname>Slama</surname><given-names>K</given-names></name><name><surname>Ray</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Training language models to follow instructions with human feedback</article-title><conf-name>NIPS’22: Proceedings of the 36th International Conference on Neural Information Processing Systems</conf-name><fpage>27730</fpage><lpage>27744</lpage></element-citation></ref><ref id="bib60"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pal</surname><given-names>A</given-names></name><name><surname>Umapathi</surname><given-names>LK</given-names></name><name><surname>Sankarasubbu</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Medmcqa: a large-scale multi-subject multi-choice dataset for medical domain question answering</article-title><conf-name>In Conference on health, inference, and learning</conf-name><fpage>248</fpage><lpage>260</lpage></element-citation></ref><ref id="bib61"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>F</given-names></name><name><surname>Zhu</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>HB</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Ouyang</surname><given-names>C</given-names></name><name><surname>Rueckert</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Medvlm-R1: incentivizing medical reasoning capability of Vision-Language Models (Vlms) via reinforcement learning</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.1007/978-3-032-04981-0_32</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>B</given-names></name><name><surname>Chen</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Feng</surname><given-names>P</given-names></name><name><surname>Bi</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Niu</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Securing large language models: addressing bias, misinformation, and prompt attacks</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2409.08087</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>P</given-names></name><name><surname>Wu</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Zhao</surname><given-names>W</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Gu</surname><given-names>H</given-names></name><name><surname>Peng</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Xie</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Quantifying the reasoning abilities of llms on real-world clinical cases</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2503.04691</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rafailov</surname><given-names>R</given-names></name><name><surname>Sharma</surname><given-names>A</given-names></name><name><surname>Mitchell</surname><given-names>E</given-names></name><name><surname>Manning</surname><given-names>CD</given-names></name><name><surname>Ermon</surname><given-names>S</given-names></name><name><surname>Finn</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Direct preference optimization: your language model is secretly a reward model</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>53728</fpage><lpage>53741</lpage></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savage</surname><given-names>T</given-names></name><name><surname>Nayak</surname><given-names>A</given-names></name><name><surname>Gallo</surname><given-names>R</given-names></name><name><surname>Rangan</surname><given-names>E</given-names></name><name><surname>Chen</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine</article-title><source>NPJ Digital Medicine</source><volume>7</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.1038/s41746-024-01010-1</pub-id><pub-id pub-id-type="pmid">38267608</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schulman</surname><given-names>J</given-names></name><name><surname>Wolski</surname><given-names>F</given-names></name><name><surname>Dhariwal</surname><given-names>P</given-names></name><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Klimov</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Proximal policy optimization algorithms</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1707.06347</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Selvaraju</surname><given-names>RR</given-names></name><name><surname>Cogswell</surname><given-names>M</given-names></name><name><surname>Das</surname><given-names>A</given-names></name><name><surname>Vedantam</surname><given-names>R</given-names></name><name><surname>Parikh</surname><given-names>D</given-names></name><name><surname>Batra</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Grad-CAM: visual explanations from deep networks via gradient-based localization</article-title><conf-name>2017 IEEE International Conference on Computer Vision (ICCV)</conf-name><conf-loc>Venice</conf-loc><fpage>618</fpage><lpage>626</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2017.74</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Shao</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>P</given-names></name><name><surname>Zhu</surname><given-names>Q</given-names></name><name><surname>Xu</surname><given-names>R</given-names></name><name><surname>Song</surname><given-names>J</given-names></name><name><surname>Bi</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Zhang</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Deepseekmath: pushing the limits of mathematical reasoning in open language models</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2402.03300</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheth</surname><given-names>A</given-names></name><name><surname>Roy</surname><given-names>K</given-names></name><name><surname>Gaur</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neurosymbolic artificial intelligence (why, what, and how)</article-title><source>IEEE Intelligent Systems</source><volume>38</volume><fpage>56</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1109/MIS.2023.3268724</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>W</given-names></name><name><surname>Xu</surname><given-names>R</given-names></name><name><surname>Zhuang</surname><given-names>Y</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Ho</surname><given-names>JC</given-names></name><name><surname>Yang</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>EHRAgent: code empowers large language models for few-shot complex tabular reasoning on electronic health records</article-title><conf-name>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</conf-name><conf-loc>Miami, Florida, USA</conf-loc><pub-id pub-id-type="doi">10.18653/v1/2024.emnlp-main.1245</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singhal</surname><given-names>K</given-names></name><name><surname>Azizi</surname><given-names>S</given-names></name><name><surname>Tu</surname><given-names>T</given-names></name><name><surname>Mahdavi</surname><given-names>SS</given-names></name><name><surname>Wei</surname><given-names>J</given-names></name><name><surname>Chung</surname><given-names>HW</given-names></name><name><surname>Scales</surname><given-names>N</given-names></name><name><surname>Tanwani</surname><given-names>A</given-names></name><name><surname>Cole-Lewis</surname><given-names>H</given-names></name><name><surname>Pfohl</surname><given-names>S</given-names></name><name><surname>Payne</surname><given-names>P</given-names></name><name><surname>Seneviratne</surname><given-names>M</given-names></name><name><surname>Gamble</surname><given-names>P</given-names></name><name><surname>Kelly</surname><given-names>C</given-names></name><name><surname>Babiker</surname><given-names>A</given-names></name><name><surname>Schärli</surname><given-names>N</given-names></name><name><surname>Chowdhery</surname><given-names>A</given-names></name><name><surname>Mansfield</surname><given-names>P</given-names></name><name><surname>Demner-Fushman</surname><given-names>D</given-names></name><name><surname>Agüera Y Arcas</surname><given-names>B</given-names></name><name><surname>Webster</surname><given-names>D</given-names></name><name><surname>Corrado</surname><given-names>GS</given-names></name><name><surname>Matias</surname><given-names>Y</given-names></name><name><surname>Chou</surname><given-names>K</given-names></name><name><surname>Gottweis</surname><given-names>J</given-names></name><name><surname>Tomasev</surname><given-names>N</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Rajkomar</surname><given-names>A</given-names></name><name><surname>Barral</surname><given-names>J</given-names></name><name><surname>Semturs</surname><given-names>C</given-names></name><name><surname>Karthikesalingam</surname><given-names>A</given-names></name><name><surname>Natarajan</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Large language models encode clinical knowledge</article-title><source>Nature</source><volume>620</volume><fpage>172</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1038/s41586-023-06291-2</pub-id><pub-id pub-id-type="pmid">37438534</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sloman</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Causal Models: How People Think about the World and Its Alternatives</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195183115.001.0001</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Snell</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>K</given-names></name><name><surname>Kumar</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Scaling Llm test-time compute optimally can be more effective than scaling model parameters</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2408.03314</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>J</given-names></name><name><surname>Zheng</surname><given-names>C</given-names></name><name><surname>Xie</surname><given-names>E</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Chu</surname><given-names>R</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Ding</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Geng</surname><given-names>M</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Yin</surname><given-names>Z</given-names></name><name><surname>Ren</surname><given-names>X</given-names></name><name><surname>Fu</surname><given-names>J</given-names></name><name><surname>He</surname><given-names>J</given-names></name><name><surname>Yuan</surname><given-names>W</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Dong</surname><given-names>H</given-names></name><name><surname>Cheng</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>M</given-names></name><name><surname>Heng</surname><given-names>PA</given-names></name><name><surname>Dai</surname><given-names>J</given-names></name><name><surname>Luo</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Wen</surname><given-names>JR</given-names></name><name><surname>Qiu</surname><given-names>X</given-names></name><name><surname>Guo</surname><given-names>Y</given-names></name><name><surname>Xiong</surname><given-names>H</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A survey of reasoning with foundation models</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2312.11562">https://doi.org/10.48550/arXiv.2312.11562</ext-link></element-citation></ref><ref id="bib75"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sundararajan</surname><given-names>M</given-names></name><name><surname>Taly</surname><given-names>A</given-names></name><name><surname>Yan</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Axiomatic attribution for deep networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1703.01365">https://doi.org/10.48550/arXiv.1703.01365</ext-link></element-citation></ref><ref id="bib76"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tchango</surname><given-names>AF</given-names></name><name><surname>Goel</surname><given-names>R</given-names></name><name><surname>Martel</surname><given-names>J</given-names></name><name><surname>Wen</surname><given-names>Z</given-names></name><name><surname>Caron</surname><given-names>GM</given-names></name><name><surname>Ghosn</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>Towards trustworthy automatic diagnosis systems by emulating doctors’ reasoning with deep reinforcement learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2210.07198">https://doi.org/10.48550/arXiv.2210.07198</ext-link></element-citation></ref><ref id="bib77"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tchango</surname><given-names>AF</given-names></name><name><surname>Goel</surname><given-names>R</given-names></name><name><surname>Wen</surname><given-names>Z</given-names></name><name><surname>Martel</surname><given-names>J</given-names></name><name><surname>Ghosn</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>Ddxplus: A new dataset for automatic medical diagnosis</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2205.09148">https://doi.org/10.48550/arXiv.2205.09148</ext-link></element-citation></ref><ref id="bib78"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tenney</surname><given-names>I</given-names></name><name><surname>Wexler</surname><given-names>J</given-names></name><name><surname>Bastings</surname><given-names>J</given-names></name><name><surname>Bolukbasi</surname><given-names>T</given-names></name><name><surname>Coenen</surname><given-names>A</given-names></name><name><surname>Gehrmann</surname><given-names>S</given-names></name><name><surname>Jiang</surname><given-names>E</given-names></name><name><surname>Pushkarna</surname><given-names>M</given-names></name><name><surname>Radebaugh</surname><given-names>C</given-names></name><name><surname>Reif</surname><given-names>E</given-names></name><name><surname>Yuan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The language interpretability tool: extensible, interactive visualizations and analysis for NLP Models</article-title><conf-name>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</conf-name><pub-id pub-id-type="doi">10.18653/v1/2020.emnlp-demos.15</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Tiku</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The google engineer who thinks the company’s ai has come to life</article-title><ext-link ext-link-type="uri" xlink:href="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/">https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/</ext-link><date-in-citation iso-8601-date="2022-06-11">June 11, 2022</date-in-citation></element-citation></ref><ref id="bib80"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Touvron</surname><given-names>H</given-names></name><name><surname>Lavril</surname><given-names>T</given-names></name><name><surname>Izacard</surname><given-names>G</given-names></name><name><surname>Martinet</surname><given-names>X</given-names></name><name><surname>Lachaux</surname><given-names>M</given-names></name><name><surname>Lacroix</surname><given-names>T</given-names></name><name><surname>Rozière</surname><given-names>B</given-names></name><name><surname>Goyal</surname><given-names>N</given-names></name><name><surname>Hambro</surname><given-names>E</given-names></name><name><surname>Azhar</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Llama: open and efficient foundation language models</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2302.13971</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tu</surname><given-names>T</given-names></name><name><surname>Palepu</surname><given-names>A</given-names></name><name><surname>Schaekermann</surname><given-names>M</given-names></name><name><surname>Saab</surname><given-names>K</given-names></name><name><surname>Freyberg</surname><given-names>J</given-names></name><name><surname>Tanno</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>B</given-names></name><name><surname>Amin</surname><given-names>M</given-names></name><name><surname>Tomasev</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Towards conversational diagnostic Ai</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2401.05654</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Uesato</surname><given-names>J</given-names></name><name><surname>Kushman</surname><given-names>N</given-names></name><name><surname>Kumar</surname><given-names>R</given-names></name><name><surname>Song</surname><given-names>F</given-names></name><name><surname>Siegel</surname><given-names>N</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Creswell</surname><given-names>A</given-names></name><name><surname>Irving</surname><given-names>G</given-names></name><name><surname>Higgins</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Solving math word problems with process- and outcome-based feedback</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2211.14275</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Melle</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>MYCIN: a knowledge-based consultation program for infectious disease diagnosis</article-title><source>International Journal of Man-Machine Studies</source><volume>10</volume><fpage>313</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1016/S0020-7373(78)80049-2</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name><name><surname>Kaiser</surname><given-names>Ł</given-names></name><name><surname>Polosukhin</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention is all you need</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1706.03762</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>X</given-names></name><name><surname>Tong</surname><given-names>W</given-names></name><name><surname>Roberts</surname><given-names>R</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>InferBERT: A transformer-based causal inference framework for enhancing pharmacovigilance</article-title><source>Frontiers in Artificial Intelligence</source><volume>4</volume><elocation-id>659622</elocation-id><pub-id pub-id-type="doi">10.3389/frai.2021.659622</pub-id><pub-id pub-id-type="pmid">34136800</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Wei</surname><given-names>J</given-names></name><name><surname>Schuurmans</surname><given-names>D</given-names></name><name><surname>Le</surname><given-names>Q</given-names></name><name><surname>Chi</surname><given-names>E</given-names></name><name><surname>Narang</surname><given-names>S</given-names></name><name><surname>Chowdhery</surname><given-names>A</given-names></name><name><surname>Zhou</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Self-consistency improves chain of thought reasoning in language models</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2203.11171</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>G</given-names></name><name><surname>Yang</surname><given-names>K</given-names></name><name><surname>Shi</surname><given-names>N</given-names></name><name><surname>Zhou</surname><given-names>W</given-names></name><name><surname>Hao</surname><given-names>S</given-names></name><name><surname>Xiong</surname><given-names>G</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Sim</surname><given-names>MY</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Interactive natural language processing</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2305.13246</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Zhou</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Chain-of-thought reasoning without prompting</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2402.10200</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Ma</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Wu</surname><given-names>C</given-names></name><name><surname>Ji</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Yuan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>A Survey of LLM-based Agents in Medicine: How far are we from Baymax?</article-title><conf-name>Findings of the Association for Computational Linguistics</conf-name><conf-loc>Vienna, Austria</conf-loc><pub-id pub-id-type="doi">10.18653/v1/2025.findings-acl.539</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Schuurmans</surname><given-names>D</given-names></name><name><surname>Bosma</surname><given-names>M</given-names></name><name><surname>Xia</surname><given-names>F</given-names></name><name><surname>Chi</surname><given-names>E</given-names></name><name><surname>Le</surname><given-names>QV</given-names></name><name><surname>Zhou</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2201.11903">https://doi.org/10.48550/arXiv.2201.11903</ext-link></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>TC</given-names></name><name><surname>Bach</surname><given-names>CC</given-names></name><name><surname>Matthiesen</surname><given-names>NB</given-names></name><name><surname>Henriksen</surname><given-names>TB</given-names></name><name><surname>Gagliardi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Directed acyclic graphs: a tool for causal studies in paediatrics</article-title><source>Pediatric Research</source><volume>84</volume><fpage>487</fpage><lpage>493</lpage><pub-id pub-id-type="doi">10.1038/s41390-018-0071-3</pub-id><pub-id pub-id-type="pmid">29967527</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>T</given-names></name><name><surname>He</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Sun</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>K</given-names></name><name><surname>Han</surname><given-names>QL</given-names></name><name><surname>Tang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A brief overview of ChatGPT: The history, status quo and potential future development</article-title><source>IEEE/CAA Journal of Automatica Sinica</source><volume>10</volume><fpage>1122</fpage><lpage>1136</lpage><pub-id pub-id-type="doi">10.1109/JAS.2023.123618</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>K</given-names></name><name><surname>Wu</surname><given-names>E</given-names></name><name><surname>Thapa</surname><given-names>R</given-names></name><name><surname>Wei</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>A</given-names></name><name><surname>Suresh</surname><given-names>A</given-names></name><name><surname>Tao</surname><given-names>JJ</given-names></name><name><surname>Sun</surname><given-names>MW</given-names></name><name><surname>Lozano</surname><given-names>A</given-names></name><name><surname>Zou</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>MedCaseReasoning: evaluating and learning diagnostic reasoning from clinical case reports</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2505.11733</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Xia</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Tian</surname><given-names>J</given-names></name><name><surname>Gong</surname><given-names>Y</given-names></name><name><surname>Hou</surname><given-names>R</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Wu</surname><given-names>Z</given-names></name><name><surname>Fan</surname><given-names>Z</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Zhu</surname><given-names>K</given-names></name><name><surname>Zheng</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Bansal</surname><given-names>C</given-names></name><name><surname>Niethammer</surname><given-names>M</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Zhu</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name><name><surname>Ge</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>G</given-names></name><name><surname>Zou</surname><given-names>J</given-names></name><name><surname>Yao</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2024">2024a</year><article-title>CARES: a comprehensive benchmark of trustworthiness in medical vision language models</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2406.06007</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Xia</surname><given-names>P</given-names></name><name><surname>Zhu</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Shi</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Zou</surname><given-names>J</given-names></name><name><surname>Yao</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2024">2024b</year><article-title>MMed-Rag: versatile multimodal rag system for medical vision language models</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.18653/v1/2024.emnlp-main.62</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>C</given-names></name><name><surname>Jackson</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Machine learning and complex biological data</article-title><source>Genome Biology</source><volume>20</volume><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1186/s13059-019-1689-0</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>B</given-names></name><name><surname>Jiang</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Xing</surname><given-names>G</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Jiang</surname><given-names>X</given-names></name><name><surname>Yan</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2024">2024a</year><article-title>DrHouse: An LLM-empowered diagnostic reasoning system through harnessing outcomes from sensor data and expert knowledge</article-title><source>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</source><volume>8</volume><fpage>1</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1145/3699765</pub-id><pub-id pub-id-type="pmid">39639863</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>AN</given-names></name><name><surname>Yang</surname><given-names>B</given-names></name><name><surname>Hui</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2024">2024b</year><article-title>Qwen2 technical report</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2407.10671</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>S</given-names></name><name><surname>Yu</surname><given-names>D</given-names></name><name><surname>Zhao</surname><given-names>J</given-names></name><name><surname>Shafran</surname><given-names>I</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Narasimhan</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Tree of thoughts: deliberate problem solving with large language models</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2305.10601</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>J</given-names></name><name><surname>Ignatiev</surname><given-names>A</given-names></name><name><surname>Stuckey</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2023">2023a</year><article-title>From formal boosted tree explanations to interpretable rule sets</article-title><conf-name>In 29th International Conference on Principles and Practice of Constraint Programming</conf-name></element-citation></ref><ref id="bib101"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>W</given-names></name><name><surname>Jiang</surname><given-names>M</given-names></name><name><surname>Clark</surname><given-names>P</given-names></name><name><surname>Sabharwal</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023b</year><article-title>IfQA: A dataset for open-domain question answering under counterfactual presuppositions</article-title><conf-name>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</conf-name><conf-loc>Singapore</conf-loc><pub-id pub-id-type="doi">10.18653/v1/2023.emnlp-main.515</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yue</surname><given-names>L</given-names></name><name><surname>Xing</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Fu</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>ClinicalAgent: clinical trial multi-agent system with large language model-based reasoning</article-title><conf-name>Proceedings of the 15th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics</conf-name><conf-loc>Shenzhen, China</conf-loc><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1145/3698587.3701359</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Q</given-names></name><name><surname>Rao</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A novel visual interpretability for deep neural networks by optimizing activation maps with perturbation</article-title><conf-name>Proceedings of the AAAI Conference on Artificial Intelligence</conf-name><fpage>3377</fpage><lpage>3384</lpage><pub-id pub-id-type="doi">10.1609/aaai.v35i4.16450</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Y</given-names></name><name><surname>Yin</surname><given-names>H</given-names></name><name><surname>Zeng</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Shi</surname><given-names>T</given-names></name><name><surname>Lyu</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Luo</surname><given-names>W</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Marco-O1: towards open reasoning models for open-ended solutions</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2411.14405</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>S</given-names></name><name><surname>Xie</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Zhan</surname><given-names>Z</given-names></name><name><surname>Song</surname><given-names>M</given-names></name><name><surname>Yang</surname><given-names>H</given-names></name><name><surname>Espinoza</surname><given-names>C</given-names></name><name><surname>Welton</surname><given-names>L</given-names></name><name><surname>Mai</surname><given-names>X</given-names></name><name><surname>Jin</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Automating expert-level medical reasoning evaluation of large language models</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2507.07988</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Wei</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Xue</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>MeNTi: bridging medical calculator and LLM agent with nested tool calling</article-title><conf-name>Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics</conf-name><conf-loc>Albuquerque, New Mexico</conf-loc><pub-id pub-id-type="doi">10.18653/v1/2025.naacl-long.263</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>R</given-names></name><name><surname>Peng</surname><given-names>T</given-names></name><name><surname>Cheng</surname><given-names>T</given-names></name><name><surname>Qu</surname><given-names>X</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Zhu</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Xue</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Shan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>A survey on latent reasoning</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2507.06203</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>Y</given-names></name><name><surname>Lin</surname><given-names>YH</given-names></name><name><surname>Liyanawatta</surname><given-names>M</given-names></name><name><surname>Saputro</surname><given-names>AH</given-names></name><name><surname>Utami</surname><given-names>YD</given-names></name><name><surname>Wang</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>An interactive programming learning environment supporting paper computing and immediate evaluation for making thinking visible and traceable</article-title><source>Interactive Learning Environments</source><volume>32</volume><fpage>5253</fpage><lpage>5266</lpage><pub-id pub-id-type="doi">10.1080/10494820.2023.2212709</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zi Yang</surname><given-names>SS</given-names></name><name><surname>Fye</surname><given-names>GM</given-names></name><name><surname>Yap</surname><given-names>WC</given-names></name><name><surname>Yu</surname><given-names>DZ</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Enhancing medical summarization with parameter efficient fine tuning on local CPUs</article-title><conf-name>2024 International Conference on Electrical, Communication and Computer Engineering (ICECCE)</conf-name><conf-loc>Kuala Lumpur, Malaysia</conf-loc><pub-id pub-id-type="doi">10.1109/ICECCE63537.2024.10823619</pub-id></element-citation></ref></ref-list></back></article>