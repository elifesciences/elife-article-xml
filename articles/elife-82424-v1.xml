<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">82424</article-id><article-id pub-id-type="doi">10.7554/eLife.82424</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Position representations of moving objects align with real-time position in the early visual response</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-288401"><name><surname>Johnson</surname><given-names>Philippa Anne</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6125-3138</contrib-id><email>pajohnson@student.unimelb.edu.au</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-289781"><name><surname>Blom</surname><given-names>Tessel</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-88377"><name><surname>van Gaal</surname><given-names>Simon</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6628-4534</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="pa2">‡</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-216522"><name><surname>Feuerriegel</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177371"><name><surname>Bode</surname><given-names>Stefan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-193604"><name><surname>Hogendoorn</surname><given-names>Hinze</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01ej9dk98</institution-id><institution>University of Melbourne</institution></institution-wrap><addr-line><named-content content-type="city">Melbourne</named-content></addr-line><country>Australia</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dkp9463</institution-id><institution>University of Amsterdam</institution></institution-wrap><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Press</surname><given-names>Clare</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02mb95055</institution-id><institution>Birkbeck, University of London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Howard Hughes Medical Institute, Stanford University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Melbourne School of Psychological Sciences, University of Melbourne, Melbourne, Australia</p></fn><fn fn-type="present-address" id="pa2"><label>‡</label><p>ABC Amsterdam Brain and Cognition, University of Amsterdam, Amsterdam, Netherlands</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>19</day><month>01</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e82424</elocation-id><history><date date-type="received" iso-8601-date="2022-08-03"><day>03</day><month>08</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-11-16"><day>16</day><month>11</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-06-29"><day>29</day><month>06</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.06.26.496535"/></event></pub-history><permissions><copyright-statement>© 2023, Johnson et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Johnson et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-82424-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-82424-figures-v1.pdf"/><related-article related-article-type="commentary" ext-link-type="doi" xlink:href="10.7554/eLife.85301" id="ra1"/><abstract><p>When interacting with the dynamic world, the brain receives outdated sensory information, due to the time required for neural transmission and processing. In motion perception, the brain may overcome these fundamental delays through predictively encoding the position of moving objects using information from their past trajectories. In the present study, we evaluated this proposition using multivariate analysis of high temporal resolution electroencephalographic data. We tracked neural position representations of moving objects at different stages of visual processing, relative to the real-time position of the object. During early stimulus-evoked activity, position representations of moving objects were activated substantially earlier than the equivalent activity evoked by unpredictable flashes, aligning the earliest representations of moving stimuli with their real-time positions. These findings indicate that the predictability of straight trajectories enables full compensation for the neural delays accumulated early in stimulus processing, but that delays still accumulate across later stages of cortical processing.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>The survival of animals depends on their ability to respond to different stimuli quickly and efficiently. From flies fluttering away when a swatter approaches, to deer running away at the sight of a lion to humans ducking to escape a looming punch, fast-paced reactions to harmful stimuli is what keep us (and other fauna) from getting injured or seriously maimed. This entire process is orchestrated by the nervous system, where cells called neurons carry signals from our senses to higher processing centres in the brain, allowing us to react appropriately.</p><p>However, this relay process from the sensory organs to the brain accumulates delays: it takes time for signals to be transmitted from cell to cell, and also for the brain to process these signals. This means that the information received by our brains is usually outdated, which could lead to delayed responses. Experiments done in cats and monkeys have shown that the brain can compensate for these delays by predicting how objects might move in the immediate future, essentially extrapolating the trajectories of objects moving in a predictable manner. This might explain why rabbits run in an impulsive zigzag manner when trying to escape a predator: if they change direction often enough, the predator may not be able to predict where they are going next.</p><p>Johnson et al. wanted to find out whether human brains can also compensate for delays in processing the movement of objects, and if so, at what point (early or late) in the processing pipeline the compensation occurs. To do this, they recorded the electrical activity of neurons using electroencephalography from volunteers who were presented with both static and moving stimuli. Electroencephalography or EEG records the average activity of neurons in a region of the brain over a period of time.</p><p>The data showed that the volunteers’ brains responded to moving stimuli significantly faster than to static stimuli in the same position on the screen, essentially being able to track the real-time position of the moving stimulus. Johnson et al. further analysed and compared the EEG recordings for moving versus static stimuli to demonstrate that compensation for processing delays occurred early on in the processing journey. Indeed, the compensation likely happens before the signal reaches a part of the brain called the visual cortex, which processes stimuli from sight. Any delays accrued beyond this point were not accommodated for.</p><p>Johnson et al. clearly demonstrate that the human brain can work around its own shortcomings to allow us to perceive moving objects in real time. These findings start to explain, for example, how sportspersons are able to catch fast-moving balls and hit serves coming to them at speeds of approximately 200 kilometres per hour. The results also lay the foundation for studying processing delays in other senses, such as hearing and touch.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>EEG</kwd><kwd>prediction</kwd><kwd>motion</kwd><kwd>neural delays</kwd><kwd>latency</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000923</institution-id><institution>Australian Research Council</institution></institution-wrap></funding-source><award-id>DP180102268</award-id><principal-award-recipient><name><surname>Hogendoorn</surname><given-names>Hinze</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000923</institution-id><institution>Australian Research Council</institution></institution-wrap></funding-source><award-id>FT200100246</award-id><principal-award-recipient><name><surname>Hogendoorn</surname><given-names>Hinze</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>To accurately represent object position in real time, the human visual system predictively encodes the location of moving objects, compensating for the time required for transmission and processing of information.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Responding quickly to the world around us is a primary function of the central nervous system: catching prey, escaping predators, and avoiding moving objects (e.g. falling rocks) are all crucial to survival. This task is complicated by delays that accumulate during the neural transmission of information from the sensory organs to the brain. As a result, the brain only has access to outdated sensory information. Furthermore, additional delays accumulate during subsequent cortical processing of information. The world will inevitably change during this time, so how can the brain overcome this fundamental problem and keep up with an ever-changing world?</p><p>Several lines of evidence suggest that the brain can compensate for neural transmission delays through prediction: using information from the past to predict what is happening in the present (<xref ref-type="bibr" rid="bib56">Nijhawan, 1994</xref>; <xref ref-type="bibr" rid="bib38">Kiebel et al., 2008</xref>; <xref ref-type="bibr" rid="bib22">Friston et al., 2010</xref>). Indeed, many researchers consider prediction to be a core objective of the central nervous system (<xref ref-type="bibr" rid="bib21">Friston, 2010</xref>; <xref ref-type="bibr" rid="bib14">Clark, 2013</xref>). This is relevant to all sensory processing, whether anticipating haptic input while manipulating an object or auditory input while listening to a melody. While experimental paradigms in cognitive neuroscience often remove the dimension of time by using artificially static displays (<xref ref-type="bibr" rid="bib52">Millidge et al., 2022</xref>), delays are a fundamental obstacle in neural processing and should be accounted for in any comprehensive theory of brain function. This problem has received particular interest within the field of motion perception, as the future positions of a predictably moving object can be determined by the object’s current acceleration, velocity, and position. In this case, there is evidence that predictive processes help to compensate both for the neural delays incurred before visual input reaches the visual cortex and for the delays incurred during subsequent cortical processing (<xref ref-type="bibr" rid="bib59">Orban et al., 1985</xref>; <xref ref-type="bibr" rid="bib5">Berry et al., 1999</xref>; <xref ref-type="bibr" rid="bib33">Jancke et al., 2004</xref>; <xref ref-type="bibr" rid="bib71">Subramaniyan et al., 2018</xref>; <xref ref-type="bibr" rid="bib72">Sundberg et al., 2006</xref>).</p><p>For example, neurophysiological recordings in animals reveal motion extrapolation mechanisms as early as the retina (<xref ref-type="bibr" rid="bib5">Berry et al., 1999</xref>; <xref ref-type="bibr" rid="bib13">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib37">Johnston and Lagnado, 2015</xref>; <xref ref-type="bibr" rid="bib46">Liu et al., 2021</xref>; <xref ref-type="bibr" rid="bib70">Souihel and Cessac, 2021</xref>). By responding to the leading edge of moving stimuli, retinal ganglion cells extrapolate the represented position of those stimuli, and are thought to transmit these extrapolated representations to visual cortex, thereby compensating for some of the lag that arises during transmission. These pre-cortical extrapolation mechanisms should effectively allow primary visual cortex to represent the position of a moving object with reduced latency, as observed in both cat and macaque V1 (<xref ref-type="bibr" rid="bib33">Jancke et al., 2004</xref>; <xref ref-type="bibr" rid="bib71">Subramaniyan et al., 2018</xref>). The existence of these extrapolation mechanisms opens the possibility that transmission delays on the way to visual cortex might be partially or fully compensated, allowing the early visual system to represent moving objects on predictable trajectories closer to their real-time locations.</p><p>It is unclear whether similar mechanisms operate along the cortical visual processing hierarchy to compensate for additional delays that accumulate as visual information is processed. On the one hand, there is suggestive evidence that position representations in areas V4 (<xref ref-type="bibr" rid="bib72">Sundberg et al., 2006</xref>) and V5 (<xref ref-type="bibr" rid="bib49">Maus et al., 2013a</xref>) are shifted for moving objects, potentially reflecting the effect of motion extrapolation in those areas. That interpretation is consistent with recent fMRI (<xref ref-type="bibr" rid="bib69">Schneider et al., 2019</xref>; <xref ref-type="bibr" rid="bib27">Harvey and Dumoulin, 2016</xref>), theoretical (<xref ref-type="bibr" rid="bib30">Hogendoorn and Burkitt, 2019</xref>) and psychophysical (<xref ref-type="bibr" rid="bib76">van Heusden et al., 2019</xref>) work suggesting that motion extrapolation mechanisms operate at multiple levels of the visual system. On the other hand, shifted position representations in higher areas might simply result from those areas inheriting extrapolated information from upstream areas such as V1. To our knowledge, no study to-date has investigated how the represented position of a smoothly moving object evolves over time as visual information about that object flows along the visual hierarchy.</p><p>Here, we address this question by using time-resolved EEG decoding to probe the position representations of smoothly moving objects across all levels of the human visual system in real time. We show that early position representations of moving objects are in close alignment with the veridical position of the object, providing the first direct evidence in humans that extrapolation processes allow the early visual system to localise moving objects in real time. We further show that during the course of cortical visual processing, position representations increasingly lag behind real-time stimulus position as information progresses through the visual hierarchy. This suggests that delay compensation is primarily achieved at very early stages of stimulus processing, and that subsequent cortical visual areas do not implement further compensation for neural delays. Nevertheless, this early compensation ensures that the represented position of a moving object throughout the entire visual hierarchy is far more up-to-date than could be expected on the basis of the latencies of neural responses to static objects. These findings demonstrate the existence of significant predictive processing during motion perception, but constrain any predictive mechanisms to acting relatively early in processing.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Twelve observers viewed sequences of black discs that were either flashed in one of 37 possible positions on a hexagonal grid (static trials) or smoothly moved through a series of positions on the grid along a straight trajectory (motion trials, <xref ref-type="fig" rid="fig1">Figure 1</xref>). Static trials were presented 252 times per position, and each of the 42 motion vectors was presented 108 times. EEG data were recorded over six testing sessions and analysed offline (see Methods). Multivariate pattern classifiers (linear discriminant analysis) were trained to discriminate stimulus position for all pairwise combinations of positions, using EEG activity evoked by static stimuli in those positions. These classifiers were subsequently tested on EEG data recorded during an independent subset of static trials, or during motion trials. Results of this classification analysis were combined to estimate the likelihood of the stimulus being present in each of the possible stimulus positions, <inline-formula><mml:math id="inf1"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>37</mml:mn></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf3"><mml:mi>s</mml:mi></mml:math></inline-formula> is the presented position. From this, we traced the evolution over time of the estimated likelihood of the stimulus being present in the position in which it was actually presented (static trials) or moved through (motion trials), <inline-formula><mml:math id="inf4"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, hereafter referred to as the <italic>stimulus-position likelihood</italic>.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Stimuli in static and motion trials.</title><p>(<bold>a</bold>) Stimulus configuration. Stimuli were presented in a hexagonal grid. In static trials, a black circle was shown centred in 1 of the 37 positions (marked by black dots, not visible during the experiment). In motion trials, the same stimulus moved at 10.36 degrees visual angle/second (dva/s) in a straight line through the grid. A fixation point was presented in the centre of the screen and the background was 50% grey. All measurements are in dva. (<bold>b</bold>) Trial structure. A trial consisted of a black circle flashed in one position for 250 ms (static trials) or moving in a straight line for between 3350 and 4550 ms (motion trials). Trials were randomly shuffled and presented separated by an inter-stimulus interval randomly selected from a uniform distribution between 350 and 450 ms. (<bold>c</bold>) Motion trials. The moving stimulus travelled along 1 of 42 possible straight trajectories through the grid: six possible stimulus directions along the hexagonal grid axes with seven parallel trajectories for each direction. The moving stimulus passed through four to seven flash locations, depending on the eccentricity of the trajectory.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82424-fig1-v1.tif"/></fig><p>This analysis was repeated for multiple combinations of training timepoint (i.e. time after onset of a static stimulus) and test timepoint (i.e. time after onset of a static stimulus or within a motion vector). Using different training timepoints allowed us to probe neural representations at different levels of the visual hierarchy, and testing at multiple timepoints allowed us to characterise how information flows through those levels over time during the epoch of interest (<xref ref-type="bibr" rid="bib39">King and Dehaene, 2014</xref>). In this way, we were able to evaluate whether the neural position representation of a moving object flows through the visual hierarchy at the same latency as the position representation of a static flash. Additionally, this allowed us to evaluate how much the position representation of the moving object lags behind that object’s physical position.</p><sec id="s2-1"><title>Decoding position of static stimuli</title><p>First, we investigated the ability of classifiers to discriminate the presented position of static flashes based on the EEG signal. <xref ref-type="fig" rid="fig2">Figure 2a</xref> shows average classification accuracy across pairwise combinations of positions over time, grouped by distance between the two positions. Classifiers were trained and tested using subsets of data from the same timepoints. As expected, the performance of pairwise classifiers improved with increasing stimulus separation. This is due to the retinotopic organisation of visual cortex; stimuli elicit more distinct patterns of activity when they are further apart.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Classification results for decoding the position of static stimuli.</title><p>(<bold>a</bold>) Group-level pairwise classification performance of static stimulus-position discrimination sorted by distance between stimulus positions (separate lines). Classifiers were trained and tested on matched timepoints from 0 to 350 ms (i.e. the time diagonal). (<bold>b</bold>) Timepoints along the time diagonal at which likelihood of the stimulus being in the presented position (stimulus-position likelihood) is significantly above chance (<italic>n</italic>=12, p&lt;0.05, cluster-based correction) are marked by the bar above the x-axis. The stimulus-position likelihood was significantly above chance from 58 ms onward. Shaded error bars show one standard deviation around the mean across observers. Chance level has been subtracted from all likelihoods to demonstrate the divergence from chance, in this graph and all others showing stimulus-position likelihood. (<bold>c</bold>) Stimulus-position likelihood (colour bar; <xref ref-type="bibr" rid="bib42">Kovesi, 2015</xref>) was calculated from classification results at each combination of training and test times. Results averaged across all stimulus positions and participants are displayed as a temporal generalisation matrix (TGM). (<bold>d</bold>) Topographic maps show participant-averaged topographic activity patterns used by classifiers to distinguish stimulus positions at 141 ms post stimulus onset, the time of peak decoding (marked by an arrow in panel b). Insets in the top left of each scalp map show which two stimulus positions the classifier has been trained to discriminate. Scalp maps were obtained by combining classification weights with the relevant covariance matrix. As expected, for all four comparisons, activation was predominantly occipital and, when the stimulus positions were on either side of the vertical meridian, lateralised.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82424-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Stimulus-position likelihood as a function of eccentricity.</title><p>Graph showing the difference from chance likelihood over time, for flashed stimuli at six different eccentricities. Data were averaged over stimulus positions with the same eccentricity relative to fixation. Likelihood of the stimulus being present when presented at fixation is shown in the darkest grey; likelihood of the stimulus being present when presented at the edge of the grid (12 degrees visual angle/second [dva] from fixation) is shown in the lightest grey. Stimulus-position likelihood is higher for stimuli that are closer to fixation, with stimuli at fixation showing by far the best decoding performance. This is to be expected as a larger patch of cortex is dedicated to processing the central visual field than the periphery (<xref ref-type="bibr" rid="bib26">Harvey and Dumoulin, 2011</xref>), and it has been shown that foveally presented stimuli elicit a much larger event-related potential than peripherally presented stimuli (<xref ref-type="bibr" rid="bib64">Rousselet et al., 2005</xref>). It can also be observed that the relationship between likelihood and eccentricity is not as clear for stimuli located far from fixation. We believe this is because the distances between the target position and comparison positions are not uniformly distributed for central compared to peripheral stimuli. Likelihood calculations for peripheral stimuli included pairwise comparisons between stimuli that are located on opposite sides of the grid (25 dva separation), for which we obtain the best pairwise classification performance (see <xref ref-type="fig" rid="fig2">Figure 2a</xref>). By comparison, the target/comparison position separation is only ∼12 dva maximum for a stimulus presented at fixation. This could lead to a relative bias towards higher decoding performance for peripheral compared to central locations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82424-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Analysis of eye movements.</title><p>(a) The same decoding analysis that was applied to the EEG response to the static stimuli was applied to the x-y position of the eyes (measured using concurrent eyetracking). The graph shows mean stimulus-position likelihood at each timepoint from stimulus onset. Error bars show one standard deviation around the mean across observers. Stimulus-position likelihood was not significantly above chance at any timepoint between 0 and 350 ms (<italic>n</italic>=12, p&gt;0.05, cluster-based correction). (b) Distribution of eye angles (density plot) following presentation of stimuli in the six outermost stimulus positions, 12.4 degrees visual angle/second (dva) from fixation. The angle of the stimulus around fixation is marked by the greyscale circles; corresponding coloured lines show the distribution of eye angle for that stimulus. For each trial and each participant, the average eye angle between 50 and 350 ms was included. It can be observed that the distribution of eye angle does not change depending on the angular position of the stimulus. This observation was confirmed with a multi-sample test for equal median distributions (<xref ref-type="bibr" rid="bib4">Berens, 2009</xref>), which showed that the median eye angle, averaged from 50 to 350 ms post flash onset, was not significantly different for any of the outermost corner stimulus position: <italic>P</italic>(11) = 2.00, p = 0.849. Both these analyses suggest that participants’ eye positions did not systematically vary according to the stimulus location.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82424-fig2-figsupp2-v1.tif"/></fig></fig-group><p>Pairwise classification results were combined to calculate the stimulus-position likelihood. Stimulus-position likelihoods by eccentricity (distance from fixation) are plotted in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. We then averaged across all stimulus positions and participants. This likelihood was compared to a permuted null distribution to establish whether it was significantly above chance at each timepoint (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, see Methods). The stimulus-position likelihood was above chance starting at 58 ms after stimulus onset, meaning that stimulus positions were decodable from this timepoint onward. An equivalent analysis was applied to eye position during static trials to ensure that EEG decoding results were not confounded by systematic microsaccades (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), either due to neural activity related to motor planning, movement-related EEG artefacts, or shifts in retinotopy (<xref ref-type="bibr" rid="bib55">Mostert et al., 2018</xref>). For eye position, stimulus-position likelihood was not significantly above chance at any timepoint, which is in line with previous findings (<xref ref-type="bibr" rid="bib7">Blom et al., 2020</xref>; <xref ref-type="bibr" rid="bib66">Salti et al., 2015</xref>; <xref ref-type="bibr" rid="bib74">Tse et al., 2002</xref>). Therefore, we can conclude that the classifiers trained on the EEG response to the static stimulus did not exploit eye movements to determine the stimulus position.</p><p>To assess whether position-related information was stable or variable across the time-course of the visual evoked response, the classification analysis was generalised across time (<xref ref-type="bibr" rid="bib39">King and Dehaene, 2014</xref>): classifiers were trained and tested at all combinations of timepoints. <xref ref-type="fig" rid="fig2">Figure 2c</xref> shows the resulting temporal generalisation matrix (TGM), averaged across all stimulus positions and participants. The TGM was typical of position decoding plots seen in previous work (<xref ref-type="bibr" rid="bib29">Hogendoorn and Burkitt, 2018</xref>). It appears that the stimulus representations before 150 ms training/test time did not generalise to other timepoints, but later reactivation of representations is apparent after 150 ms (off-diagonal red blobs). Finally, <xref ref-type="fig" rid="fig2">Figure 2d</xref> shows topographic maps of activation which contributed to classification of stimulus position (<xref ref-type="bibr" rid="bib28">Haufe et al., 2014</xref>); these show that the relevant signal was mainly recorded from occipital electrodes, suggesting a source within the visual cortex, as expected.</p></sec><sec id="s2-2"><title>Decoding position of moving stimuli</title><p>To decode the position of moving stimuli, we again trained classifiers on pairwise combinations of static stimuli, then applied these classifiers to EEG data recorded during motion trials. An illustration of each step in the analysis of motion trials is shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. As before, the stimulus-position likelihood was calculated, this time at each timepoint during motion epochs.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Analysis pipeline for motion trials.</title><p>Panels describe steps in calculating the time to peak stimulus-position likelihood in motion trials, including graphs of relevant data for each step. Steps 1 and 2 describe the classification analysis applied to obtain the stimulus-position likelihoods. The figure in step 3 shows the group-level temporal generalisation matrix for training on static stimuli and testing on moving stimuli. The black dotted line shows the ‘diagonal’ timepoints, where the time elapsed since the moving stimulus was at the flash position equals the training time. Step 4 shows timepoints along this diagonal at which the stimulus-position likelihood was significantly above chance, as established through permutation testing. Significance is marked by the solid black line above the x-axis; the likelihood is significantly above chance from 102 to 180 ms (<italic>n</italic>=12, p&lt;0.05, cluster-based correction). Shaded error bars show one standard deviation around the mean. The figure in step 5 shows the same data as step 3 for selected training times (arrows above temporal generalisation matrix [TGM] correspond to subplot titles). Each subplot shows a vertical slice of the TGM. Blue points show data, to which we fit a Gaussian curve (black lines) to estimate the time of peak likelihood for each training time (dashed orange lines). These are the data points plotted in <xref ref-type="fig" rid="fig4">Figure 4a and b</xref>. Step 6 shows adjusted <italic>R</italic><sup>2</sup> of Gaussian fits for each training timepoint. A cutoff of 0.5 was used to select timepoints at which the Gaussian fit meaningfully explains the pattern of data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82424-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Fitted Gaussian curves at extended training times.</title><p>Each subplot shows a vertical slice of the temporal generalisation matrix (TGM) in <xref ref-type="fig" rid="fig3">Figure 3</xref>, step 3. Blue points show data, to which we fit a Gaussian curve (black lines) to estimate the time of peak likelihood for each training time (dashed orange lines).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82424-fig3-figsupp1-v1.tif"/></fig></fig-group><p>We considered EEG epochs from 500 ms before to 500 ms after the timepoint at which the moving stimulus was exactly in each possible static stimulus location. This time-window was chosen to be broad enough to capture stimulus-evoked activity as the stimulus approached and receded from each position (moving from one position to the next took 400 ms). We then averaged the time-course of stimulus-position likelihoods across all six motion directions and 37 stimulus positions. The first position along each trajectory was excluded due to observed strong EEG responses to the initial onset of the stimulus.</p><p>The TGM derived from classifiers trained on static trials and tested on motion trials (<xref ref-type="fig" rid="fig3">Figure 3</xref>, step 3) revealed that classifiers trained on timepoints from around 100 ms were able to decode the position of moving objects. To identify timepoints at which classification was significantly above chance, we considered the performance of classifiers trained and tested on matching timepoints (diagonal of the TGM). Permutation testing revealed that decoding was significantly above chance for timepoints between 102 and 180 ms (<xref ref-type="fig" rid="fig3">Figure 3</xref>, step 4). Note that because we are investigating possible latency differences between the neural response to static and moving stimuli, maximal decoding is likely achieved off-diagonal, making this a conservative analysis choice.</p><p>Although the average stimulus-position likelihood was smaller in magnitude for moving stimuli compared to static stimuli, we observed that the location-specific neural response to motion over time was characterised by a gradual increase of the stimulus-position likelihood as the stimulus approached the centre of the position, then a decrease as the stimulus moved away on the other side. This is illustrated in <xref ref-type="fig" rid="fig3">Figure 3</xref>, step 5 (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for more training timepoints), and is similar to the pattern of activity found in response to a moving bar with direct recordings from cat V1 (<xref ref-type="bibr" rid="bib33">Jancke et al., 2004</xref>).</p></sec><sec id="s2-3"><title>Establishing the latency of position representations of moving stimuli</title><p>To investigate the latency at which neural position representations are activated for moving objects, we calculated the timepoint at which the peak stimulus-position likelihood was reached during motion sequences. Again, this was repeated for different training times as a proxy for different stages of neural processing. The time to peak likelihood in the test data for each training timepoint reflects the time at which the location-specific activity is most similar in the train and test set. This is assumed to be the time that the brain is representing the moving stimulus at the centre of a flash position. We use <italic>peak</italic> likelihood, as opposed to onset or a peak percentage, as the latency measure due to possible variations in receptive field (RF) size over the course of visual processing. As time elapses during stimulus processing, visual information reaches visual areas further up the processing hierarchy, which contain stimulus-selective neurons with larger RFs (<xref ref-type="bibr" rid="bib35">Johnson et al., 2021</xref>; <xref ref-type="bibr" rid="bib26">Harvey and Dumoulin, 2011</xref>). This means that a moving stimulus would enter the RF earlier in these later-activated brain regions. Looking at the peak neural response avoids this problem, because peak response would be expected when the stimulus is at the centre of the RF, irrespective of RF size.</p><p>To establish the latency with which the position of a moving object is represented at different stages of visual processing, we identified the timepoint at which our classification analysis yielded maximum stimulus-position likelihood. This was achieved by fitting a Gaussian curve to the observed time-course of the calculated likelihood averaged across participants, separately for each training time (<xref ref-type="fig" rid="fig3">Figure 3</xref>, step 5). Adjusted <italic>R</italic><sup>2</sup> of these fits can be found in <xref ref-type="fig" rid="fig3">Figure 3</xref>, step 6. For training timepoints later than ∼80 ms, the Gaussian curves provided a very good fit to the evolution of stimulus-position likelihood over time, with <italic>R</italic><sup>2</sup> values over 0.5. Although the window of significant cross-classification of static stimuli to moving stimuli is restricted (<xref ref-type="fig" rid="fig3">Figure 3</xref>, step 4), the sustained high adjusted <italic>R</italic><sup>2</sup> values indicate that even for training times at which the stimulus-position likelihood was close to chance level, the likelihood increased and decreased as the stimulus traversed each flash location.</p><p><xref ref-type="fig" rid="fig4">Figure 4a</xref> shows the time to peak likelihood for motion across all training timepoints at which adjusted <italic>R</italic><sup>2</sup> exceeded a minimum value of 0.5. The choice of <italic>R</italic><sup>2</sup> cutoff is essentially arbitrary, but the pattern of points in <xref ref-type="fig" rid="fig3">Figure 3</xref>, step 6 shows that this selection is relatively robust to changes in the cutoff value. Up to ∼150 ms training time, the time to peak likelihood increases with increasing training time. This follows the same pattern as the static trials (see <xref ref-type="fig" rid="fig4">Figure 4b</xref>), where earlier representations of the stimulus (i.e. early training times) were activated at a shorter latency in the testing epoch than later representations of the stimulus. This sequential pattern is consistent with the first feedforward sweep of stimulus-driven activation. As information flows through the visual processing hierarchy, representations of the stimulus will gradually change over time. The order of these changes appeared to be consistent between static and motion trials.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Neural response latency during motion processing.</title><p>(<bold>a</bold>) Latencies of the peak stimulus-position likelihood values during motion processing. The timepoint at which peak likelihood was reached is plotted against training time. Error bars around points show bootstrapped 95% confidence intervals of the peak shift parameter of the Gaussian fit, computed from <italic>n</italic> = 12 participants (see <xref ref-type="fig" rid="fig3">Figure 3</xref>, step 5). It can be observed that the peak time increases and decreases, then levels out. Points of inflection within this timeseries were identified using piecewise regression (shown in black). The number of inflection points, or knots, was established by comparing the <italic>R</italic><sup>2</sup> of piecewise regression fits, as shown in the inset graph. It was determined that six knots was optimal; positions of these knots are marked by grey dotted lines on the main graph. (<bold>b</bold>) Time to peak likelihood during the initial feedforward sweep of activity through the visual cortex. Displayed is a subset of points from those shown in panel a, corresponding to a restricted time-window between the first two knots, during which the first feedforward sweep of activity was most likely occurring. The dotted diagonal shows the 45° line, where the time of peak likelihood would equal the training time. Data points from static trials (green) should theoretically lie along this line, as, in this case, the training and test data were subsets of the same trials. Straight lines were fit separately for static (<italic>F</italic>(21,19) = 805.45, p=1.24 × 10<sup>-17</sup>) and motion trials (<italic>F</italic>(21,19) = 40.91, p=3.07 × 10<sup>–6</sup>). Both lines had similar gradients, close to unity, indicating equivalent cumulative processing delays for static and motion trials within this training time-window. However, the intercept for motion was much earlier at –80 ms. The mean distance between the two lines is marked, indicating that position representations were activated ∼70 ms earlier in response to a moving stimulus compared to a flashed one in the same location. Time to peak likelihood at the beginning of the feedforward sweep was approximately 0 ms, indicating near-perfect temporal alignment with the physical position of the stimulus. (<bold>c</bold>) Illustration of compensation for neural delays at different cortical processing levels. The static stimulus (green, top) and the moving stimulus (orange, bottom) are in the same position at time = 0 ms (black dashed line), but there is a 73 ms latency advantage for the neural representations of the stimulus in this position when it is moving. Each separate curve represents neural responses emerging at different times during stimulus processing, where higher contrast corresponds to early visual representations and lower contrast corresponds to later visual representations. The earliest neural response, likely originating in early visual cortex, represents the moving stimulus in its real-time location. The consequence of this is a shift in the spatial encoding of the moving stimulus: by the time neural representations of the flash emerge, the moving stimulus will be represented in a new location further along its motion trajectory. The relative distance between the subsequent curves is the same for moving and static stimuli, because there is no further compensation for delays during subsequent cortical processing.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82424-fig4-v1.tif"/></fig><p>This pattern subsequently reverses between 150 and 200 ms, indicating that hierarchically later representations were activated at a shorter latency. Finally, from a training time of ∼250 ms the time to peak likelihood was stable at approximately 50 ms. The non-monotonic relationship between training time and time to peak likelihood could emerge because there was variable compensation for neural delays at different training times. Perhaps more likely, this pattern could reflect feedforward and feedback sweeps of activity in the visual cortex evoked by the static stimulus: the feedforward sweep activates sequential representations, and information flowing backward along the hierarchy reactivates the same activity patterns in reverse order. The timescale of this wave of activity was in line with previous findings from <xref ref-type="bibr" rid="bib17">Dijkstra et al., 2020</xref>, who showed approximately 10 Hz oscillations evoked by face/house stimuli. Additionally, previous TMS, MEG, and fMRI results suggest that 150 ms is a reasonable estimate for the time it takes for visual information to reach hierarchically later visual processing areas, such as V5/MT+ (<xref ref-type="bibr" rid="bib65">Sack et al., 2006</xref>; <xref ref-type="bibr" rid="bib1">Ahlfors et al., 1999</xref>; <xref ref-type="bibr" rid="bib32">Hotson et al., 1994</xref>; <xref ref-type="bibr" rid="bib87">Yoshor et al., 2007</xref>; <xref ref-type="bibr" rid="bib53">Mohsenzadeh et al., 2018</xref>; <xref ref-type="bibr" rid="bib44">Lamme and Roelfsema, 2000</xref>). This would imply that the activity evoked by static compared to moving stimuli is different after about 150 ms, due to a divergence in further stimulus processing. Therefore, cross-generalisation of position decoding across stimulus types is possible after this timepoint because a portion of the flash-evoked signal reflects reactivation of earlier activity patterns, which are common to static and moving stimulus processing. If this later activity (&gt;150 ms training time) does indeed reflect feedback processing of the static stimulus, then, for these later timepoints, the latency measure we have calculated might not be informative about the time necessary to first represent the moving object, because the initial activations and the reactivations are indistinguishable.</p><p>In order to confirm which timepoints predominantly correspond to feedforward processes rather than feedback, we applied piecewise regression, as implemented in the Shape Language Modeling toolbox (<xref ref-type="bibr" rid="bib16">D’Errico, 2022</xref>). In this approach, several polynomials are smoothly joined together at ‘knots’. Placement of knots, at the start and end of each segment, is optimised by reducing root mean squared error. We fit straight lines, and varied the number of knots between four and seven, in order to identify the optimal number. The best piecewise regression fit, with six knots, is shown in <xref ref-type="fig" rid="fig4">Figure 4a</xref>. Corroborating our observation, the first internal knot was placed at 158 ms. We took this inflection point as the end of the initial feedforward sweep of information through the visual cortex. The piecewise regression revealed further positive and negative slopes, suggesting that feedforward and feedback sweeps of activity continue during later stimulus processing (<xref ref-type="bibr" rid="bib17">Dijkstra et al., 2020</xref>).</p></sec><sec id="s2-4"><title>Latency advantages for moving stimuli during feedforward processing</title><p>Having identified the timepoints during the motion epochs corresponding to early feedforward processing, we further investigated the relationship between training time and the time of peak stimulus-position likelihood, to compare the latency with which the brain represents static and moving stimuli. <xref ref-type="fig" rid="fig4">Figure 4b</xref> shows the peak time for moving objects during the feedforward sweep, along with the first segment of the fitted piecewise linear regression. We were interested in comparing the latency and time-course of stimulus-related processing of static and motion trials in this restricted time-window, to establish whether the position of moving objects was predictively encoded. We therefore established time to peak likelihood for these same training timepoints in the static trials. Because the participant-averaged time-course of the stimulus-position likelihood for each training time was much less noisy for static than motion trials, the time of peak likelihood was computed as a simple maximum for each training time. Qualitatively, it can be observed that this lay along the diagonal of the TGM (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, green points).</p><p>For the static stimuli, a linear fit relating training time and time to peak likelihood (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, upper line) was very similar to the 45° line (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, grey dashed line), revealing that each representation of the stimulus was active at roughly the same time in the training and test data. A linear fit to the static data points was significantly better than a constant model (<italic>F</italic>(21,19) = 805.45, p=1.24 × 10<sup>–17</sup>). The line which best described the relationship between static time to peak likelihood and training time was:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mn>0.85</mml:mn><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mn>19.</mml:mn></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Both the intercept and gradient parameters were significantly different from zero (<italic>t</italic>=5.48, p=2.32 × 10<sup>–5</sup>; <italic>t</italic>=28.38, p=1.24 × 10<sup>–17</sup>). The line had a gradient close to one (95% CIs: 0.79–0.92) and a small intercept (95% CIs: 12–27 ms), indicating only a small shift in the peak time between training and testing. This line fit indicates that the patterns of activity on which the classifiers were trained were most similar to activity at approximately the same timepoint in the test trials. This is as expected, as train and test data are subsets of the same static trials.</p><p>In contrast, we found evidence of a shift in the latency of activation of representations of the moving stimulus. A regression line was computed also for time to peak likelihood in the motion epochs (<italic>F</italic>(21,19) = 40.91, p=3.07 × 10<sup>–6</sup>), with equation:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mn>1.09</mml:mn><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mn>82.</mml:mn></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Both parameters were again significantly different from zero (intercept: <italic>t</italic>=–4.06, p=6.12 × 10<sup>–4</sup>, 95% CIs: 95% CIs: –124 to –40 ms; gradient: <italic>t</italic>=6.40, p=3.07 × 10<sup>–6</sup>, 95% CIs: 0.73–1.45). Furthermore, the difference between the static and motion gradients was not significantly different from zero (<italic>t</italic>=–1.58, p=0.13) but the difference between the two intercepts was (<italic>t</italic>=5.32, p=3.32 × 10<sup>–5</sup>).</p><p>This lack of difference between the gradients means that, once position information was available in the cortex, successive cortical representations were sequentially activated along the same time-course for moving and static stimuli. In other words, delays that accumulate during cortical processing did not appear to be compensated when processing motion. This is illustrated in <xref ref-type="fig" rid="fig4">Figure 4c</xref>, where the relative delay between neural representations is preserved regardless of whether the stimulus is moving or not. Importantly, however, the static and motion intercepts were significantly different. The linear fit to time to peak likelihood for motion stimuli had a large negative intercept of –82 ms (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>), which was substantially lower than the intercept for static trials (19 ms, <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). At the beginning of the time-window of interest (<italic>t</italic><sub>train</sub> = 75 ms), the motion regression line crossed the y-axis at –1 ms, while the static regression line crossed at 83 ms (see <xref ref-type="fig" rid="fig4">Figure 4b</xref>).</p><p>To summarise, the lack of difference in the slopes of the regression lines indicates that, once early representations of position are activated, similar delays are incurred as position-related information flows through the visual cortical hierarchy. The observed shift in the intercept signifies that stimulus positions are represented much earlier for moving compared to static stimuli.</p><p>The mean distance between the two lines was calculated at 73 ms, implying that the position of a moving object was represented with a latency that was approximately 70 ms shorter than a static object in the same position. For early neural position representations (training times around 70–80 ms), the latency of the peak in the position representation was approximately 0ms. In turn, this means that these neural representations were activated at the time that the moving object was physically centred on the corresponding position. This can also be seen in <xref ref-type="fig" rid="fig4">Figure 4c</xref>: the representations of the moving stimulus emerge 73 ms earlier than the same representations of the flashed stimulus. The earliest representation of the moving stimulus peaks when <italic>t</italic>=0 ms, the time at which the moving stimulus is in the same position as the flash. This corresponds to a spatial shift in the encoding of the moving stimulus, as, by the time the representations of the flash emerge, the moving stimulus will be represented further ahead on its path. Based on the training time, these early stimulus representations likely originated in early visual cortex (V1-3), meaning that the early visual system was able to almost completely compensate for neural delays accumulated during processing up to that point and represent moving objects close to their real-time position.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study we investigated how the visual system compensates for neural transmission delays when tracking the positions of moving objects. We investigated the latency of neural position representations of moving stimuli compared to unpredictably presented static stimuli, as well as the real-time position of the stimulus. By computing the timepoint at which each position representation of the moving stimulus was most similar to a static stimulus in the same location, we tracked the represented position of the stimulus over time across the visual hierarchy.</p><p>We demonstrate that classifiers trained to locate static stimuli based on the stimulus-evoked EEG signal could also localise moving stimuli. This is the first study, to our knowledge, to demonstrate cross-classification between stationary and smoothly moving stimuli with EEG, showing that population neural codes for location are (at least partially) shared between the two stimulus types. We, therefore, were able to investigate the timing of neural responses to motion in humans with a fine temporal resolution. We subsequently showed that, during the first feedforward sweep of activity, the neural response encoding the position of a moving stimulus was shifted approximately 75 ms earlier than the response to a static stimulus. The early decoded representations of the position of a moving stimulus aligned with the real-time position of the object, rather than the position corresponding to afferent retinal input (subject to transmission and processing delays), which would instead signal outdated position information in visual cortex. Finally, we showed that delay compensation was primarily achieved before information reached visual cortex, as the recorded cortical processing of static and motion stimuli followed a similar time-course. Overall, this study shows direct neural evidence of motion extrapolation enabling accurate real-time representation of moving objects in humans.</p><p>These results are consistent with findings of RF shifts across the visual cortex in response to motion. Many earlier fMRI studies showing RF shifts against the direction of motion <xref ref-type="bibr" rid="bib45">Liu et al., 2006</xref>; <xref ref-type="bibr" rid="bib83">Whitney et al., 2003</xref>; <xref ref-type="bibr" rid="bib63">Raemaekers et al., 2009</xref> have been dismissed because of the ‘aperture-inward’ bias, in which the trailing edge of a motion stimulus evokes larger responses than the leading edge (<xref ref-type="bibr" rid="bib79">Wang et al., 2014</xref>). This is not an issue for the present study, as we can determine the timing of neural responses at a fine temporal scale, rather than looking at aggregate responses over whole motion trajectories. Neural recordings from animals and more recent fMRI studies in humans have reliably shown RF shifts throughout the visual cortex in response to motion (<xref ref-type="bibr" rid="bib27">Harvey and Dumoulin, 2016</xref>), and that these displacements are against the direction of motion (<xref ref-type="bibr" rid="bib69">Schneider et al., 2019</xref>; <xref ref-type="bibr" rid="bib72">Sundberg et al., 2006</xref>; <xref ref-type="bibr" rid="bib23">Fu et al., 2004</xref>). However, several differences remain between the previous fMRI results and the present study. <xref ref-type="bibr" rid="bib27">Harvey and Dumoulin, 2016</xref>, found that RF shifts in response to motion scale with the size of the RF across the visual hierarchy. This implies that visual areas higher up the processing hierarchy that are activated later in time would shift their RFs more than lower visual areas which are activated earlier. In contrast, our results suggest that later visual areas show RF shifts of the same magnitude as earlier visual areas. However, it is not clear whether it is possible to map time elapsing after stimulus onset in EEG to processing in different visual areas as recorded by fMRI. The longer timescale of the fMRI signal means that it could be indexing later activity than we are recording with EEG, or include signals that emerge after integration of many feedforward and feedback sweeps of activity. While further research is needed to understand how extrapolation operates at different spatial scales, this converging evidence of RF shifts against the direction of motion suggests that the positions of moving objects are predictively encoded during processing. Furthermore, we provide evidence that RF shifts occur during the initial feedforward sweep of the visual response.</p><sec id="s3-1"><title>Mechanisms of compensation for delays</title><p>Our findings point to several mechanisms that have been proposed to compensate for neural delays. We found that the early visual response to moving stimuli is shifted in time, such that the neural delays accumulated up to that point are compensated. However, during subsequent cortical processing, there is no further compensation for delays. As discussed in the Introduction, retinal ganglion cells respond strongly to the leading edge of moving stimuli (<xref ref-type="bibr" rid="bib5">Berry et al., 1999</xref>; <xref ref-type="bibr" rid="bib37">Johnston and Lagnado, 2015</xref>). This effectively shifts the encoding of the position of a moving stimulus forward relative to a static stimulus at the earliest stage of processing. Additionally, evidence of a latency advantage for moving stimuli has been identified in the cat lateral geniculate nucleus of the thalamus (<xref ref-type="bibr" rid="bib59">Orban et al., 1985</xref>), where visual information is transmitted en route to the visual cortex. However, none of the previous evidence suggests that these pre-cortical mechanisms are sufficient to account for compensation for neural delays, to the extent we observe here. In particular, the retinal compensation mechanisms only appear to act up to speeds of about 5 degrees visual angle/second (dva/s) (<xref ref-type="bibr" rid="bib5">Berry et al., 1999</xref>; <xref ref-type="bibr" rid="bib33">Jancke et al., 2004</xref>), roughly half the speed used in the current study. This is thought to be achieved by a large response to the leading edge of the stimulus, followed by gain control mechanisms reducing neuronal firing rates. In the present study, it takes 400 ms for the stimulus to travel from one static stimulus position to the next, so the leading edge of the stimulus is closest to the stimulus position it is approaching 200 ms before it reaches that position. In general, we see a ramping in the likelihood earlier than –200 ms (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), so we believe that this adaptation is unlikely to be the sole mechanism responsible for the observed shift in encoded location.</p><p>Therefore, it is likely that some cortical mechanisms play a role. For example, there is evidence that a model of object motion is encoded in MT+, and influences neural response profiles in earlier visual areas through feedback connections (<xref ref-type="bibr" rid="bib49">Maus et al., 2013a</xref>; <xref ref-type="bibr" rid="bib50">Maus et al., 2013b</xref>; <xref ref-type="bibr" rid="bib57">Nijhawan and Kirschfeld, 2003</xref>). These feedback connections could transmit information to neurons into whose RFs the moving stimulus will soon enter, driving an anticipatory response. Similarly, within-layer horizontal connections might activate neurons further ahead on the motion path (<xref ref-type="bibr" rid="bib3">Benvenuti et al., 2020</xref>). <xref ref-type="bibr" rid="bib3">Benvenuti et al., 2020</xref>, show that this input from feedback and horizontal connections can drive spiking responses in cat V1. If information about object motion is used to compensate for delays via feedback connections, it could be expected that the particular speed of the stimulus would be exploited to maximise the accuracy of the position representation. In the present study, we only tested one stimulus speed, so further research including a range of speeds, as found in natural scenes (<xref ref-type="bibr" rid="bib85">Wojtach et al., 2008</xref>), is necessary to examine whether compensatory mechanisms work in this way. Crucially, our findings suggest that these mechanisms act <italic>only</italic> early in the course of stimulus processing, and therefore are present only early in the visual cortical hierarchy.</p><p>Even though we find temporal alignment between the early representations of the stimulus and its physical position, this alignment is lost during further processing. In a recent theoretical paper, <xref ref-type="bibr" rid="bib30">Hogendoorn and Burkitt, 2019</xref>, argue that cortical motion extrapolation is necessary to minimise the discrepancy (prediction error) between an internal model of object position and the external world in the case of time-varying stimulation. Proposed are two possible implementations of this cortical extrapolation: either delays are compensated through extrapolation in both feedforward and feedback activity, or, alternatively, extrapolation only occurs in feedback activity. Although the authors claim that the model including feedforward and feedback extrapolation is more parsimonious, this study suggests that feedforward cortical delays are not compensated. We therefore support the proposition that, if prediction errors are to be minimised, extrapolation might be implemented only in feedback connections. However, the present analysis approach may not be suitable to uncover this process, as cortical extrapolation could be a motion-specific computation enacted by different neural populations from those that encode static stimuli. Although this analysis captured feedforward and feedback sweeps of activity through the visual cortex, we believe that these oscillations are present in the processing of the flash, not the motion. This would mean that the portion of the later flash-evoked signal that that cross-generalises to motion reflects reactivation of earlier stimulus representations rather than further stimulus processing. However, there remains the possibility that these oscillations are evoked by motion. This would imply that, during feedback activity, the position of the stimulus is not updated as the stimulus moves, and that the stimulus is represented in multiple locations concurrently in the visual system (<xref ref-type="bibr" rid="bib40">King and Wyart, 2021</xref>). Further research is needed to tease apart these options. Nevertheless, a complete model of compensation for neural delays in motion perception should account for extensive extrapolation early in visual processing, as observed here.</p></sec><sec id="s3-2"><title>Comparison between smooth and apparent motion</title><p>Previous EEG research has investigated whether neural delays are compensated when viewing apparent motion. Apparent motion is a visual illusion in which stimuli that appear sequentially along a motion path, but are temporally and spatially separated, are perceived as a single stimulus in motion. Although apparent motion is an impoverished motion signal, two studies have found evidence of a 15–20 ms latency advantage when processing a stimulus within an apparent motion sequence compared to an unpredictable flash (<xref ref-type="bibr" rid="bib29">Hogendoorn and Burkitt, 2018</xref>; <xref ref-type="bibr" rid="bib8">Blom et al., 2021</xref>). This is substantially less compensation than we find in the present study, likely due to the lack of continuous stimulation in the case of apparent motion. For example, an apparent motion stimulus is static on the retina, precluding the gain control mechanism described above from acting. Smoothly moving objects also trigger a travelling wave of activity that propagates in the visual cortex in front of the retinotopic position of the stimulus (<xref ref-type="bibr" rid="bib3">Benvenuti et al., 2020</xref>), which is different from the non-linear combination of activity elicited by an apparent motion stimulus (<xref ref-type="bibr" rid="bib12">Chemla et al., 2019</xref>). As a lot of natural motion stimuli provide continuous input and, therefore, recruit the entire motion-sensitive visual pathway, it is important to characterise how smooth motion is processed. <xref ref-type="bibr" rid="bib7">Blom et al., 2020</xref>, additionally found evidence that a sensory template of an expected apparent motion stimulus is activated before the onset of the stimulus, which has additionally been found in monkey V1 (<xref ref-type="bibr" rid="bib24">Guo et al., 2007</xref>). An equivalent preactivation could be present in the case of smooth motion, but in the current study, processing of the presented stimulus would obscure any predictive activity. <xref ref-type="bibr" rid="bib7">Blom et al., 2020</xref>, also found that when the stimulus reversed direction unexpectedly, it was briefly represented in the expected next position, and that there is a latency disadvantage associated with the first unexpected stimulus after a reversal (<xref ref-type="bibr" rid="bib8">Blom et al., 2021</xref>); future research should extend the present findings by investigating the neural consequences of an unexpected change of direction, and generally unpredictable motion, in a smooth motion trajectory. Delays seem to be compensated to a greater extent in this case of smooth motion, so the visual system would have to employ a larger correction for erroneous position encoding if the stimulus changed direction.</p></sec><sec id="s3-3"><title>Limitations</title><p>A limitation of the present study is that the localisation accuracy of moving stimuli was considerably lower than that of static stimuli. This is because classifiers were trained and tested on different stimulus types; neural populations that encode the position of static stimuli do not completely overlap with neural populations that encode the position of moving objects (<xref ref-type="bibr" rid="bib58">Noda et al., 1971</xref>; <xref ref-type="bibr" rid="bib71">Subramaniyan et al., 2018</xref>). Additionally, previous fMRI studies show that, following a strong onset response, the neural response to moving stimuli decreases over time (<xref ref-type="bibr" rid="bib67">Schellekens et al., 2016</xref>; <xref ref-type="bibr" rid="bib68">Schellekens et al., 2017</xref>; <xref ref-type="bibr" rid="bib25">Harrison et al., 2007</xref>; <xref ref-type="bibr" rid="bib51">McKeefry et al., 1997</xref>). This potentially leads to a decreasing signal-to-noise ratio over the course of a single motion trial. Furthermore, because motion epochs were quite long (up to 5 s), the later parts of each motion trial could have been susceptible to slow drift of the EEG signal. Nevertheless, significant cross-classification between static and motion trials was still achieved, ruling this out as a major problem.</p><p>Because of the spatial uncertainty associated with EEG, we do not know exactly where signals originate in the brain; source localisation in EEG is an ill-posed problem without co-registration with fMRI (<xref ref-type="bibr" rid="bib34">Jatoi et al., 2014</xref>). While the initial response is likely to be dominated by a feedforward cascade through the visual hierarchy (<xref ref-type="bibr" rid="bib44">Lamme and Roelfsema, 2000</xref>), it is possible that activity recorded at later timepoints in the training epoch reflects ongoing processing in hierarchically early visual areas, as well as additional processing in hierarchically later visual areas. However, this issue does not alter our conclusions concerning the relative timing of activity when viewing static or moving stimuli.</p><p>We additionally found that the earliest signals containing information about the location of static stimuli were not informative about moving stimuli. The timing of the earliest flash-evoked activity (∼60 ms after stimulus onset) suggests a source within V1 (<xref ref-type="bibr" rid="bib2">Alilović et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Fahrenfort et al., 2007</xref>; <xref ref-type="bibr" rid="bib18">Di Russo et al., 2002</xref>; <xref ref-type="bibr" rid="bib84">Wibral et al., 2009</xref>; <xref ref-type="bibr" rid="bib77">Vanni et al., 2004</xref>). In contrast, the position of the moving stimulus was decodable only on the basis of representations formed after approximately 100 ms, suggesting that this analysis approach does not capture the earliest motion-evoked V1 activity. One possibility is that, due to variability in when stimulus processing begins across trials (<xref ref-type="bibr" rid="bib78">Vidaurre et al., 2019</xref>), the signal-to-noise ratio in the static trials at these earlier timepoints may be too low to cross-generalise to moving stimuli. Alternatively, early processing of motion could be different to static stimuli: there is some evidence that direct connections from either LGN or the pulvinar to MT+ (bypassing V1) are used when processing motion (<xref ref-type="bibr" rid="bib20">ffytche et al., 1995</xref>). This issue is hard to overcome, as training classifiers on moving stimuli would render capturing latency differences impossible; any latency shift in the test data would also be present in the training data. However, one promising approach was taken by <xref ref-type="bibr" rid="bib3">Benvenuti et al., 2020</xref>, who used recordings of monkey V1 to compare responses to trajectories of different lengths. They found that response latency decreased with increasing trajectory length: sub-threshold activation built up in front of the moving stimuli, preparing neural populations to fire upon the arrival of the stimulus in their RF. A similar approach could be taken in human EEG research to avoid the comparison between moving and non-moving stimuli. Additionally, this line of research would benefit from use of fMRI co-registered with EEG, which provides the temporal and spatial resolution necessary to pinpoint signals to a particular time, stimulus position, and neural source.</p></sec><sec id="s3-4"><title>The flash-lag effect</title><p>Of relevance to these results is the flash-lag effect (FLE), a visual illusion in which a moving bar is perceived ahead of a flashed bar despite them being physically aligned (<xref ref-type="bibr" rid="bib56">Nijhawan, 1994</xref>). This illusion demonstrates that moving objects are indeed perceived in an extrapolated position. Theories of the FLE can largely be sorted into two camps: spatial and temporal explanations (<xref ref-type="bibr" rid="bib50">Maus et al., 2013b</xref>). Spatial models, for example motion extrapolation (<xref ref-type="bibr" rid="bib56">Nijhawan, 1994</xref>; <xref ref-type="bibr" rid="bib31">Hogendoorn, 2020</xref>), suggest that the encoded positions of moving objects are shifted forwards to compensate for neural delays. In contrast, temporal models, for example differential latencies (<xref ref-type="bibr" rid="bib80">Whitney and Murakami, 1998</xref>; <xref ref-type="bibr" rid="bib81">Whitney and Cavanagh, 2000</xref>), suggest that motion is processed faster than flashes or that there is a temporal integration window over which position signals are averaged (<xref ref-type="bibr" rid="bib43">Krekelberg et al., 2000</xref>; <xref ref-type="bibr" rid="bib82">Whitney et al., 2000</xref>). A range of psychophysical evidence has been presented to support each of these theories (and others), suggesting they all play a role in the FLE and, therefore, motion processing. However, our results are congruent only with spatial explanations; temporal models cannot explain how latency shifts could be greater than the latency of the unshifted neural response. We show that parts of the visual system encode moving objects at a position that afferent sensory information could not yet indicate. A similar result was found using EEG analysis of apparent motion (<xref ref-type="bibr" rid="bib7">Blom et al., 2020</xref>), where a sensory template of an expected stimulus within the apparent motion sequence was pre-activated, before any sensory evidence was present. An outstanding question remains about whether neural representations of moving objects flexibly incorporate information about stimulus speed, as seen in animal V1 recordings (<xref ref-type="bibr" rid="bib33">Jancke et al., 2004</xref>; <xref ref-type="bibr" rid="bib71">Subramaniyan et al., 2018</xref>) and the FLE (<xref ref-type="bibr" rid="bib85">Wojtach et al., 2008</xref>).</p></sec><sec id="s3-5"><title>Conclusion</title><p>This study used multivariate analysis of EEG data to investigate the latency of position representations of moving and static stimuli. We show that, during the first feedforward sweep of activity, the latency of the neural response to moving stimuli is substantially reduced compared to the response to unpredictable static stimuli. The effect of this latency advantage is that early visual areas represent moving objects in their real-time position, suggesting that (potentially a combination of) retinal, subcortical, and cortical extrapolation mechanisms can overcome neural delays very early on in visual processing. Additional delays accumulated during subsequent cortical processing appear not to be compensated. These results demonstrate that the visual system predictively encodes the position of moving stimuli, and provide an evidence base to constrain models of how and when motion extrapolation is achieved in the human visual system.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>Twelve participants (two male; mean age = 27.0 years, s.d.=4.93 years) completed all six testing sessions and were included in analyses. While no a priori sample size calculation was conducted, we chose to collect a large EEG dataset from relatively few participant to ensure reliable classification of the stimulus. The 12 included participants were drawn from a larger initial pool, including an additional 15 participants that completed only the first session, which was used for screening. Of these additional participants, two withdrew from the study, three were excluded as the eyetracker could not consistently track their eye position, and the remaining ten were excluded after analysis of their first session data, due to poor fixation (more than 15% of trials with fixation lost) or poor EEG classification performance (less than 51.5% average classification accuracy when discriminating the location of static stimuli). Exclusion criteria included requiring glasses to view the computer screen and a personal or family history of epilepsy. Participants were recruited online and gave written informed consent before participation. Participants were reimbursed AU$15 /hr for their time, as well as an additional AU$20 if they completed all six sessions. Ethical approval was granted by the University of Melbourne Human Research Ethics Committee (Ethics ID: 1954628.2).</p></sec><sec id="s4-2"><title>Experimental design</title><p>Stimuli were presented using MATLAB Version R2018a and the Psychophysics Toolbox extension Version 3 (<xref ref-type="bibr" rid="bib10">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib60">Pelli, 1997</xref>; <xref ref-type="bibr" rid="bib41">Kleiner et al., 2007</xref>). Stimuli were presented on an ASUS ROG PG258 monitor (ASUS, Taipei, Taiwan) with a resolution of 1920×1080 running at a refresh rate of 200 Hz. Participants were seated, resting their heads on a chinrest, at a viewing distance of 50 cm from the screen in a quiet, dimly-lit room.</p><p><xref ref-type="fig" rid="fig1">Figure 1</xref> shows the stimulus configuration and trial structure of the experiment. Stimuli were presented on a grey background, with a central fixation target (<xref ref-type="bibr" rid="bib73">Thaler et al., 2013</xref>). Stimuli were black, filled circles with a radius of 1.29 dva presented in a hexagonal configuration with 37 possible stimulus positions. A trial consisted of the stimulus flashing in a single location on the grid for 250 ms (static trials), or moving in a straight line at a velocity of 10.36 dva/s through the grid (motion trials), such that the amount of time spent travelling the length of the stimulus diameter was the same as the duration of the static stimulus. Motion vectors started and finished 7 dva away from the grid to reduce the effects of stimulus onset on the EEG signal. The stimulus passed through between four and seven flash positions, depending on the eccentricity of the vector, taking 400 ms to travel between grid positions. Static and motion trials were randomly shuffled within each experimental session, with an inter-stimulus interval randomly selected from a uniform distribution between 350 and 450 ms. In each testing session, each static stimulus location was repeated 42 times, while each of the 42 motion vectors (6 directions × 7 parallel starting positions; <xref ref-type="fig" rid="fig1">Figure 1c</xref>) was repeated 18 times. Trials were split into seven blocks, with a duration of approximately 9 min each. After each block, participants could rest and sit back from the chinrest. Six times within each block (every 50 trials), participants could take a mini-break, in which the experiment was paused but they were required to remain in the chinrest. This procedure was repeated over six sessions, totalling 252 static trials in each location and 108 repetitions of each motion vector.</p><p>Participants performed a simple target detection task in order to ensure they attended to the stimuli. While maintaining fixation on the fixation point at the centre of the screen, they responded as quickly as possible with the space-bar when the stimulus flashed red for 40 ms. This happened at random 45 times per block, and trials containing a target were discarded from analysis to ensure that the target and response did not interfere with EEG analysis. Each of the target trials was then repeated at the end of the block without a target to maintain equal trial numbers for each static stimulus position/motion vector. Participants completed one practice block of 20 trials at the start of the first session to become acquainted with the task. The practice block could be repeated upon request.</p><p>EEG and eyetracking data were collected from participants while they viewed the stimuli. Eyetracking data were collected using an EyeLink 1000 eyetracker (SR Research). The eyetracker was calibrated at the start of each block, and drift correction was applied after each mini-break. The conversion of the EyeLink 1000.<italic>edf</italic> files to.<italic>mat</italic> files and offline fixation checks were performed with the EyeCatch toolbox (<xref ref-type="bibr" rid="bib6">Bigdely-Shamlo et al., 2013</xref>).</p><p>Continuous EEG data were recorded at 2048 Hz using a 64-channel BioSemi Active-Two system (BioSemi, Amsterdam, The Netherlands), connected to a standard 64-electrode EEG cap. Two external electrodes were placed on the mastoids, to be used as a reference. Electrooculography was recorded using six electrodes: on the canthi (horizontal) and above and below the eyes (vertical).</p></sec><sec id="s4-3"><title>EEG pre-processing</title><p>EEG pre-processing was conducted using EEGLAB Version 2021.1 (<xref ref-type="bibr" rid="bib15">Delorme and Makeig, 2004</xref>), running in MATLAB Version R2017b. First, EEG data were re-referenced to the mastoid channels. Data were down-sampled to 128 Hz to reduce computation time and memory load required for further pre-processing and analysis. No filtering was applied to data so as not to distort event timing (<xref ref-type="bibr" rid="bib75">van Driel et al., 2019</xref>). Bad channels were noted during data collection and were interpolated using spherical interpolation. On average, 0.49 electrodes were interpolated per recording session. Additionally, one complete session was dropped from further analysis for one participant, due to a poor connection to the mastoid channels. Data were epoched from 100 ms before flash/motion onset to 100 ms after flash/motion offset. The 100 ms period before onset was used to baseline correct each epoch, by subtracting the mean amplitude in this period from the whole epoch.</p><p>Eye movement data were used to check fixation: static trials in which gaze deviated more than 2.1 dva from fixation (i.e. was closer to another stimulus position than the central fixation point) at any point while the stimulus was on screen were discarded from analysis, as these eye movements would disrupt retinotopy. On average, 11.2% of trials were rejected on this basis. Participants’ eye positions during flashes were further analysed to ensure that there were no systematic eye movements which could be exploited by classifiers during the EEG analysis (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). No motion trials were rejected on the basis of eye movements. This is because motion trials were only used for testing classifiers; if no systematic eye movements are present in the training data, then the classifier cannot learn to distinguish trials on the basis of eye movements, so any eye movements in the test data are irrelevant to the analysis.</p><p>Epochs were then automatically rejected through an amplitude threshold. For static trials, epochs were rejected if the standard deviation of the amplitude of any channel exceeded four standard deviations from the mean standard deviation of that channel across all epochs. This resulted in 8.3% of epochs being rejected across all observers. Motion trials were rejected with a threshold of five standard deviations from the mean standard deviation. This less stringent threshold reflects the longer duration of motion trials; more variability in amplitude can be expected; 7.5% of motion trials were rejected across all observers. Finally, static and motion epochs were demeaned. The average amplitude of each electrode across all static stimulus locations was subtracted from each trial amplitude, while for motion trials, the average amplitude from motion vectors of the same length was subtracted. This ensured that the classifiers could leverage any changes in the signal corresponding to stimulus location, without the potential confound of overall amplitude differences in static compared to motion trials (<xref ref-type="bibr" rid="bib17">Dijkstra et al., 2020</xref>).</p></sec><sec id="s4-4"><title>EEG analysis</title><p>Analyses were programmed using MATLAB Version R2017b and run on the University of Melbourne Spartan High Performance Computing system. Time-resolved multivariate pattern analysis was used to classify EEG data according to the location of the static stimuli. Linear discriminant analysis classifiers with a shrinkage regularisation parameter of 0.05 (<xref ref-type="bibr" rid="bib54">Mostert et al., 2015</xref>) were trained to discriminate the location of static stimuli and tested on unseen static trials or motion trials. Code for classification analysis was adapted from <xref ref-type="bibr" rid="bib54">Mostert et al., 2015</xref>, and <xref ref-type="bibr" rid="bib29">Hogendoorn and Burkitt, 2018</xref>.</p><p>To avoid bias that often emerges from multi-class classification (<xref ref-type="bibr" rid="bib86">Yan et al., 2021</xref>), classifiers were trained using pairwise combinations of stimulus positions, such that a classifier was trained to discriminate each location from every other location. As it is redundant to train classifiers to discriminate for example, position 1 vs. 2 and also 2 vs. 1, this resulted in 666 trained classifiers at 90 timepoints over a 350 ms period (from stimulus onset at 0 to 100 ms after stimulus offset). The number of trials in each class was balanced by sampling trials without replacement from the majority class to equal the number of trials in the minority class.</p><p>These classifiers were then tested on every timepoint in either unseen static trials (0–350 ms; fivefold cross-validation between train and test sets) or motion trials (−500 to 500 ms). At each timepoint, pairwise classification results were combined to estimate the likelihood of the stimulus being in a given position (<xref ref-type="bibr" rid="bib47">Manyakov and Van Hulle, 2010</xref>; <xref ref-type="bibr" rid="bib62">Price et al., 1995</xref>). We can estimate <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mtext>position </mml:mtext><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mtext>stimulus is in </mml:mtext><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as<disp-formula id="equ3">.<label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf6"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the classification performance for position <inline-formula><mml:math id="inf7"><mml:mi>i</mml:mi></mml:math></inline-formula> vs. position <inline-formula><mml:math id="inf8"><mml:mi>j</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf9"><mml:mi>k</mml:mi></mml:math></inline-formula> is the total number of classes (the 37 stimulus positions, in this case). Such that the probability across all positions was equal to 1, the estimated likelihoods were then normalised between 0 and 1. If decoding performance was at chance level, we would expect uniform likelihood across all stimulus positions, at:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mfrac></mml:mstyle><mml:mo>≈</mml:mo><mml:mrow><mml:mn>0.027027</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>A likelihood greater than this indicates a location-specific neural response to the stimulus. An example probability mass function of the likelihood across all stimulus positions can be found in <xref ref-type="fig" rid="fig3">Figure 3</xref>, step 2. For the main analysis, we investigated the evolution over time of the likelihood of the stimulus being at the presented position, <inline-formula><mml:math id="inf10"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, referred to as the <italic>stimulus-position likelihood</italic>. Where relevant, chance level (1/37) was subtracted from the likelihood for easier interpretation in graphs.</p><p>In this analysis, we trained classifiers at multiple timepoints because time elapsing post stimulus onset can be seen as a proxy for processing stage. As time passes, stimulus-evoked activity will progress through the visual system (<xref ref-type="bibr" rid="bib17">Dijkstra et al., 2020</xref>; <xref ref-type="bibr" rid="bib40">King and Wyart, 2021</xref>). Our aim was to establish, for each training timepoint, the timepoint in the test data at which the stimulus was most likely to be at a certain position. This tells us the latency of a particular pattern of activity, or representation of the stimulus, in the training data compared to the test data.</p><p>We first demonstrated that stimulus position could be discriminated even when static stimuli were close together, by averaging classification results according to distance between stimulus locations (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). Next, we calculated the latency of representations when training and testing on static stimuli, by taking the maximum stimulus-position likelihood over the test time-window for each training timepoint. This was used as a baseline to which the motion was compared, as the static stimulus locations were unpredictable. Any shifts in latency seen in the motion trials must be due to the predictable preceding trajectory. The key analysis was, therefore, training classifiers to discriminate the location of static stimuli and testing on motion vectors. In this case, in the training data, the stimulus was centred at a certain position, so the timepoint at which the test data is most similar should be the timepoint at which the stimulus was represented in the brain at this position in the trajectory.</p><p>For motion trials, to overcome the noise of individual data points, we fit a Gaussian curve with four free parameters to the observed time-course of the calculated likelihood averaged across participants, separately for each training time:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The parameter of interest is <italic>b</italic><sub>2</sub>, which describes the horizontal shift of the peak of the Gaussian curve.</p></sec><sec id="s4-5"><title>Statistical analysis</title><p>Statistical significance of classification results was ascertained through permutation testing. After running the classification analyses as described above, class labels were randomly shuffled when calculating the stimulus-position likelihood, ensuring that the permuted classification results were uninformative about stimulus location. This procedure was repeated 1000 times per participant, providing a null distribution against which our results could be compared with Yuen’s <italic>t</italic>-test, one-tailed, <inline-formula><mml:math id="inf11"><mml:mi>α</mml:mi></mml:math></inline-formula> = 0.05 (<xref ref-type="bibr" rid="bib88">Yuen, 1974</xref>). Cluster-based correction for multiple comparisons was applied with 1000 permutations (cluster-forming <inline-formula><mml:math id="inf12"><mml:mi>α</mml:mi></mml:math></inline-formula> = 0.05, <xref ref-type="bibr" rid="bib11">Bullmore et al., 1999</xref>; <xref ref-type="bibr" rid="bib48">Maris and Oostenveld, 2007</xref>). Code for the cluster-based correction came from the Decision Decoding Toolbox (<xref ref-type="bibr" rid="bib9">Bode et al., 2019</xref>), which uses code originally from LIMO EEG (<xref ref-type="bibr" rid="bib61">Pernet et al., 2011</xref>) to implement Yuen’s <italic>t</italic>-test.</p><p>To test significance of linear regression models against a constant model, we used one-tailed <italic>F</italic>-tests. To test whether individual regression coefficients were significantly different from zero, we used two-tailed <italic>t</italic>-test.</p></sec><sec id="s4-6"><title>Materials availability</title><p>Code is available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/jbw9m/">https://osf.io/jbw9m/</ext-link>. Processed data is available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.vx0k6djw0">https://doi.org/10.5061/dryad.vx0k6djw0</ext-link> (<xref ref-type="bibr" rid="bib36">Johnson et al., 2022</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Resources, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Methodology, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Participants gave written informed consent for participation and publication of results before participation. Ethical approval was granted by the University of Melbourne Human Research Ethics Committee (Ethics ID: 1954628.2).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-82424-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data files have been deposited on Dryad at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.vx0k6djw0">https://doi.org/10.5061/dryad.vx0k6djw0</ext-link>. Code is available on the Open Science Framework at <ext-link ext-link-type="uri" xlink:href="https://osf.io/jbw9m/">https://osf.io/jbw9m/</ext-link>.</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>PA</given-names></name><name><surname>Blom</surname><given-names>T</given-names></name><name><surname>van Gaal</surname><given-names>S</given-names></name><name><surname>Feuerriegel</surname><given-names>D</given-names></name><name><surname>Bode</surname><given-names>S</given-names></name><name><surname>Hogendoorn</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>EEG and eyetracking response to static and moving stimuli</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.vx0k6djw0</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>P</given-names></name></person-group><source>Open Science Framework</source><year iso-8601-date="2020">2020</year><data-title>Position representations of moving objects align with real-time position in the early visual response</data-title><pub-id pub-id-type="accession" xlink:href="https://osf.io/jbw9m/">jbw9m</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors gratefully acknowledge support from the Australian Research Council to HH (DP180102268 and FT200100246). This research was further supported by The University of Melbourne’s Research Computing Services and the Petascale Campus Initiative. Thanks to Andrea Titton for discussion on calculating and reporting likelihoods, and to Jane Yook and Vinay Mepani for help with data collection.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahlfors</surname><given-names>SP</given-names></name><name><surname>Simpson</surname><given-names>GV</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name><name><surname>Liu</surname><given-names>AK</given-names></name><name><surname>Korvenoja</surname><given-names>A</given-names></name><name><surname>Virtanen</surname><given-names>J</given-names></name><name><surname>Huotilainen</surname><given-names>M</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name><name><surname>Aronen</surname><given-names>HJ</given-names></name><name><surname>Ilmoniemi</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Spatiotemporal activity of a cortical network for processing visual motion revealed by MEG and fmri</article-title><source>Journal of Neurophysiology</source><volume>82</volume><fpage>2545</fpage><lpage>2555</lpage><pub-id pub-id-type="doi">10.1152/jn.1999.82.5.2545</pub-id><pub-id pub-id-type="pmid">10561425</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alilović</surname><given-names>J</given-names></name><name><surname>Timmermans</surname><given-names>B</given-names></name><name><surname>Reteig</surname><given-names>LC</given-names></name><name><surname>van Gaal</surname><given-names>S</given-names></name><name><surname>Slagter</surname><given-names>HA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>No evidence that predictions and attention modulate the first feedforward sweep of cortical information processing</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>2261</fpage><lpage>2278</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhz038</pub-id><pub-id pub-id-type="pmid">30877784</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Benvenuti</surname><given-names>G</given-names></name><name><surname>Chemla</surname><given-names>S</given-names></name><name><surname>Boonman</surname><given-names>A</given-names></name><name><surname>Perrinet</surname><given-names>L</given-names></name><name><surname>Masson</surname><given-names>GS</given-names></name><name><surname>Chavane</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Anticipatory Responses along Motion Trajectories in Awake Monkey Area V1</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.03.26.010017</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berens</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>CircStat: A MATLAB toolbox for circular statistics</article-title><source>Journal of Statistical Software</source><volume>31</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.18637/jss.v031.i10</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berry</surname><given-names>MJ</given-names></name><name><surname>Brivanlou</surname><given-names>IH</given-names></name><name><surname>Jordan</surname><given-names>TA</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Anticipation of moving stimuli by the retina</article-title><source>Nature</source><volume>398</volume><fpage>334</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1038/18678</pub-id><pub-id pub-id-type="pmid">10192333</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bigdely-Shamlo</surname><given-names>N</given-names></name><name><surname>Kreutz-Delgado</surname><given-names>K</given-names></name><name><surname>Kothe</surname><given-names>C</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>EyeCatch: data-mining over half a million EEG independent components to construct a fully-automated eye-component detector</article-title><conf-name>Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference</conf-name><fpage>5845</fpage><lpage>5848</lpage><pub-id pub-id-type="doi">10.1109/EMBC.2013.6610881</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blom</surname><given-names>T</given-names></name><name><surname>Feuerriegel</surname><given-names>D</given-names></name><name><surname>Johnson</surname><given-names>P</given-names></name><name><surname>Bode</surname><given-names>S</given-names></name><name><surname>Hogendoorn</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Predictions drive neural representations of visual events ahead of incoming sensory information</article-title><source>PNAS</source><volume>117</volume><fpage>7510</fpage><lpage>7515</lpage><pub-id pub-id-type="doi">10.1073/pnas.1917777117</pub-id><pub-id pub-id-type="pmid">32179666</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blom</surname><given-names>T</given-names></name><name><surname>Bode</surname><given-names>S</given-names></name><name><surname>Hogendoorn</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The time-course of prediction formation and revision in human visual motion processing</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>138</volume><fpage>191</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2021.02.008</pub-id><pub-id pub-id-type="pmid">33711770</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bode</surname><given-names>S</given-names></name><name><surname>Feuerriegel</surname><given-names>D</given-names></name><name><surname>Bennett</surname><given-names>D</given-names></name><name><surname>Alday</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The decision decoding toolbox (DDTBOX) - a multivariate pattern analysis toolbox for event-related potentials</article-title><source>Neuroinformatics</source><volume>17</volume><fpage>27</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1007/s12021-018-9375-z</pub-id><pub-id pub-id-type="pmid">29721680</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bullmore</surname><given-names>ET</given-names></name><name><surname>Suckling</surname><given-names>J</given-names></name><name><surname>Overmeyer</surname><given-names>S</given-names></name><name><surname>Rabe-Hesketh</surname><given-names>S</given-names></name><name><surname>Taylor</surname><given-names>E</given-names></name><name><surname>Brammer</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Global, voxel, and cluster tests, by theory and permutation, for a difference between two groups of structural MR images of the brain</article-title><source>IEEE Transactions on Medical Imaging</source><volume>18</volume><fpage>32</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1109/42.750253</pub-id><pub-id pub-id-type="pmid">17896594</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chemla</surname><given-names>S</given-names></name><name><surname>Reynaud</surname><given-names>A</given-names></name><name><surname>di Volo</surname><given-names>M</given-names></name><name><surname>Zerlaut</surname><given-names>Y</given-names></name><name><surname>Perrinet</surname><given-names>L</given-names></name><name><surname>Destexhe</surname><given-names>A</given-names></name><name><surname>Chavane</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Suppressive traveling waves shape representations of illusory motion in primary visual cortex of awake primate</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>4282</fpage><lpage>4298</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2792-18.2019</pub-id><pub-id pub-id-type="pmid">30886010</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>EY</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Fisher</surname><given-names>C</given-names></name><name><surname>Schwartz</surname><given-names>G</given-names></name><name><surname>Levy</surname><given-names>J</given-names></name><name><surname>da Silveira</surname><given-names>RA</given-names></name><name><surname>da Silviera</surname><given-names>RA</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Alert response to motion onset in the retina</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>120</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3749-12.2013</pub-id><pub-id pub-id-type="pmid">23283327</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Whatever next? predictive brains, situated agents, and the future of cognitive science</article-title><source>The Behavioral and Brain Sciences</source><volume>36</volume><fpage>181</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1017/S0140525X12000477</pub-id><pub-id pub-id-type="pmid">23663408</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>Journal of Neuroscience Methods</source><volume>134</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id><pub-id pub-id-type="pmid">15102499</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>D’Errico</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>SLM -shape language modeling</article-title><source>MATLAB Central File Exchange</source><ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/24443-slm-shape-language-modeling">https://www.mathworks.com/matlabcentral/fileexchange/24443-slm-shape-language-modeling</ext-link><date-in-citation iso-8601-date="2022-03-11">March 11, 2022</date-in-citation></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname><given-names>N</given-names></name><name><surname>Ambrogioni</surname><given-names>L</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>van Gerven</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural dynamics of perceptual inference and its reversal during imagery</article-title><source>eLife</source><volume>9</volume><elocation-id>e53588</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.53588</pub-id><pub-id pub-id-type="pmid">32686645</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Russo</surname><given-names>F</given-names></name><name><surname>Martínez</surname><given-names>A</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Pitzalis</surname><given-names>S</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Cortical sources of the early components of the visual evoked potential</article-title><source>Human Brain Mapping</source><volume>15</volume><fpage>95</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1002/hbm.10010</pub-id><pub-id pub-id-type="pmid">11835601</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fahrenfort</surname><given-names>JJ</given-names></name><name><surname>Scholte</surname><given-names>HS</given-names></name><name><surname>Lamme</surname><given-names>VAF</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Masking disrupts reentrant processing in human visual cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>19</volume><fpage>1488</fpage><lpage>1497</lpage><pub-id pub-id-type="doi">10.1162/jocn.2007.19.9.1488</pub-id><pub-id pub-id-type="pmid">17714010</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>ffytche</surname><given-names>DH</given-names></name><name><surname>Guy</surname><given-names>CN</given-names></name><name><surname>Zeki</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The parallel visual motion inputs into areas V1 and V5 of human cerebral cortex</article-title><source>Brain</source><volume>118 (Pt 6)</volume><fpage>1375</fpage><lpage>1394</lpage><pub-id pub-id-type="doi">10.1093/brain/118.6.1375</pub-id><pub-id pub-id-type="pmid">8595471</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The free-energy principle: a unified brain theory?</article-title><source>Nature Reviews. Neuroscience</source><volume>11</volume><fpage>127</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1038/nrn2787</pub-id><pub-id pub-id-type="pmid">20068583</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Kilner</surname><given-names>J</given-names></name><name><surname>Kiebel</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Action and behavior: a free-energy formulation</article-title><source>Biol Cybern</source><volume>102</volume><fpage>227</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1007/s00422-010-0364-z</pub-id><pub-id pub-id-type="pmid">20148260</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>YX</given-names></name><name><surname>Shen</surname><given-names>Y</given-names></name><name><surname>Gao</surname><given-names>H</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Asymmetry in visual cortical circuits underlying motion-induced perceptual mislocalization</article-title><source>The Journal of Neuroscience</source><volume>24</volume><fpage>2165</fpage><lpage>2171</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5145-03.2004</pub-id><pub-id pub-id-type="pmid">14999067</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>K</given-names></name><name><surname>Robertson</surname><given-names>RG</given-names></name><name><surname>Pulgarin</surname><given-names>M</given-names></name><name><surname>Nevado</surname><given-names>A</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name><name><surname>Young</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spatio-temporal prediction and inference by V1 neurons</article-title><source>The European Journal of Neuroscience</source><volume>26</volume><fpage>1045</fpage><lpage>1054</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2007.05712.x</pub-id><pub-id pub-id-type="pmid">17714195</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>LM</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Extra-classical receptive field effects measured in striate cortex with fmri</article-title><source>NeuroImage</source><volume>34</volume><fpage>1199</fpage><lpage>1208</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.10.017</pub-id><pub-id pub-id-type="pmid">17169579</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname><given-names>BM</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The relationship between cortical magnification factor and population receptive field size in human visual cortex: constancies in cortical architecture</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>13604</fpage><lpage>13612</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2572-11.2011</pub-id><pub-id pub-id-type="pmid">21940451</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname><given-names>BM</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual motion transforms visual space representations similarly throughout the human visual hierarchy</article-title><source>NeuroImage</source><volume>127</volume><fpage>173</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.11.070</pub-id><pub-id pub-id-type="pmid">26666897</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haufe</surname><given-names>S</given-names></name><name><surname>Meinecke</surname><given-names>F</given-names></name><name><surname>Görgen</surname><given-names>K</given-names></name><name><surname>Dähne</surname><given-names>S</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name><name><surname>Blankertz</surname><given-names>B</given-names></name><name><surname>Bießmann</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>On the interpretation of weight vectors of linear models in multivariate neuroimaging</article-title><source>NeuroImage</source><volume>87</volume><fpage>96</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.067</pub-id><pub-id pub-id-type="pmid">24239590</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hogendoorn</surname><given-names>H</given-names></name><name><surname>Burkitt</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Predictive coding of visual object position ahead of moving objects revealed by time-resolved EEG decoding</article-title><source>NeuroImage</source><volume>171</volume><fpage>55</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.12.063</pub-id><pub-id pub-id-type="pmid">29277651</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hogendoorn</surname><given-names>H</given-names></name><name><surname>Burkitt</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Predictive coding with neural transmission delays: a real-time temporal alignment hypothesis</article-title><source>ENeuro</source><volume>6</volume><elocation-id>ENEURO.0412-18.2019</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0412-18.2019</pub-id><pub-id pub-id-type="pmid">31064839</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hogendoorn</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Motion extrapolation in visual processing: lessons from 25 years of flash-lag debate</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>5698</fpage><lpage>5705</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0275-20.2020</pub-id><pub-id pub-id-type="pmid">32699152</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hotson</surname><given-names>J</given-names></name><name><surname>Braun</surname><given-names>D</given-names></name><name><surname>Herzberg</surname><given-names>W</given-names></name><name><surname>Boman</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Transcranial magnetic stimulation of extrastriate cortex degrades human motion direction discrimination</article-title><source>Vision Research</source><volume>34</volume><fpage>2115</fpage><lpage>2123</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(94)90321-2</pub-id><pub-id pub-id-type="pmid">7941409</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jancke</surname><given-names>D</given-names></name><name><surname>Erlhagen</surname><given-names>W</given-names></name><name><surname>Schöner</surname><given-names>G</given-names></name><name><surname>Dinse</surname><given-names>HR</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Shorter latencies for motion trajectories than for flashes in population responses of cat primary visual cortex</article-title><source>The Journal of Physiology</source><volume>556</volume><fpage>971</fpage><lpage>982</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2003.058941</pub-id><pub-id pub-id-type="pmid">14978201</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jatoi</surname><given-names>MA</given-names></name><name><surname>Kamel</surname><given-names>N</given-names></name><name><surname>Malik</surname><given-names>AS</given-names></name><name><surname>Faye</surname><given-names>I</given-names></name><name><surname>Begum</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A survey of methods used for source localization using EEG signals</article-title><source>Biomedical Signal Processing and Control</source><volume>11</volume><fpage>42</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1016/j.bspc.2014.01.009</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>P</given-names></name><name><surname>Grootswagers</surname><given-names>T</given-names></name><name><surname>Moran</surname><given-names>C</given-names></name><name><surname>Hogendoorn</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Temporal dynamics of visual population receptive fields</article-title><conf-name>In: 43rd European Conference on Visual Perception (ECVP) 2021 Online, vol</conf-name><fpage>1</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.1177/03010066211059887</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>PA</given-names></name><name><surname>Blom</surname><given-names>T</given-names></name><name><surname>Van Gaal</surname><given-names>S</given-names></name><name><surname>Feuerriegel</surname><given-names>D</given-names></name><name><surname>Bode</surname><given-names>S</given-names></name><name><surname>Hogendoorn</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>EEG and eyetracking response to static and moving stimuli</data-title><source>Dryad</source><pub-id pub-id-type="doi">10.5061/dryad.vx0k6djw0</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnston</surname><given-names>J</given-names></name><name><surname>Lagnado</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>General features of the retinal connectome determine the computation of motion anticipation</article-title><source>eLife</source><volume>4</volume><elocation-id>e06250</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06250</pub-id><pub-id pub-id-type="pmid">25786068</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiebel</surname><given-names>SJ</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A hierarchy of time-scales and the brain</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000209</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000209</pub-id><pub-id pub-id-type="pmid">19008936</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>JR</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.01.002</pub-id><pub-id pub-id-type="pmid">24593982</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>J-R</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The human brain encodes a chronicle of visual events at each instant of time through the multiplexing of traveling waves</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>7224</fpage><lpage>7233</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2098-20.2021</pub-id><pub-id pub-id-type="pmid">33811150</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Pelli</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in psychtoolbox-3</article-title><source>Perception</source><volume>36</volume><fpage>1</fpage><lpage>16</lpage></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kovesi</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Good Colour Maps: How to Design Them</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1509.03700">https://arxiv.org/abs/1509.03700</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krekelberg</surname><given-names>B</given-names></name><name><surname>Lappe</surname><given-names>M</given-names></name><name><surname>Whitney</surname><given-names>D</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name><name><surname>Eagleman</surname><given-names>DM</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The position of moving objects</article-title><source>Science</source><volume>289</volume><elocation-id>1107a</elocation-id><pub-id pub-id-type="doi">10.1126/science.289.5482.1107a</pub-id><pub-id pub-id-type="pmid">17833394</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>V</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The distinct modes of vision offered by feedforward and recurrent processing</article-title><source>Trends in Neurosciences</source><volume>23</volume><fpage>571</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1016/s0166-2236(00)01657-x</pub-id><pub-id pub-id-type="pmid">11074267</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>JV</given-names></name><name><surname>Ashida</surname><given-names>H</given-names></name><name><surname>Smith</surname><given-names>AT</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Assessment of stimulus-induced changes in human V1 visual field maps</article-title><source>Journal of Neurophysiology</source><volume>96</volume><fpage>3398</fpage><lpage>3408</lpage><pub-id pub-id-type="doi">10.1152/jn.00556.2006</pub-id><pub-id pub-id-type="pmid">17005617</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Hong</surname><given-names>A</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name><name><surname>Manookin</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Predictive encoding of motion begins in the primate retina</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1280</fpage><lpage>1291</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00899-1</pub-id><pub-id pub-id-type="pmid">34341586</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manyakov</surname><given-names>NV</given-names></name><name><surname>Van Hulle</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Decoding grating orientation from microelectrode array recordings in monkey cortical area V4</article-title><source>International Journal of Neural Systems</source><volume>20</volume><fpage>95</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1142/S0129065710002280</pub-id><pub-id pub-id-type="pmid">20411593</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maus</surname><given-names>GW</given-names></name><name><surname>Fischer</surname><given-names>J</given-names></name><name><surname>Whitney</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>Motion-dependent representation of space in area MT+</article-title><source>Neuron</source><volume>78</volume><fpage>554</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.03.010</pub-id><pub-id pub-id-type="pmid">23664618</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maus</surname><given-names>GW</given-names></name><name><surname>Ward</surname><given-names>J</given-names></name><name><surname>Nijhawan</surname><given-names>R</given-names></name><name><surname>Whitney</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>The perceived position of moving objects: transcranial magnetic stimulation of area MT+ reduces the flash-lag effect</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>241</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs021</pub-id><pub-id pub-id-type="pmid">22302116</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKeefry</surname><given-names>DJ</given-names></name><name><surname>Watson</surname><given-names>JDG</given-names></name><name><surname>Frackowiak</surname><given-names>RSJ</given-names></name><name><surname>Fong</surname><given-names>K</given-names></name><name><surname>Zeki</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The activity in human areas V1/V2, V3, and V5 during the perception of coherent and incoherent motion</article-title><source>NeuroImage</source><volume>5</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1006/nimg.1996.0246</pub-id><pub-id pub-id-type="pmid">9038280</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Millidge</surname><given-names>B</given-names></name><name><surname>Seth</surname><given-names>A</given-names></name><name><surname>Buckley</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Predictive Coding: A Theoretical and Experimental Review</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2107.12979">https://arxiv.org/abs/2107.12979</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohsenzadeh</surname><given-names>Y</given-names></name><name><surname>Qin</surname><given-names>S</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Ultra-rapid serial visual presentation reveals dynamics of feedforward and feedback processes in the ventral visual pathway</article-title><source>eLife</source><volume>7</volume><elocation-id>e36329</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.36329</pub-id><pub-id pub-id-type="pmid">29927384</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mostert</surname><given-names>P</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dissociating sensory from decision processes in human perceptual decision making</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>18253</elocation-id><pub-id pub-id-type="doi">10.1038/srep18253</pub-id><pub-id pub-id-type="pmid">26666393</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mostert</surname><given-names>P</given-names></name><name><surname>Albers</surname><given-names>AM</given-names></name><name><surname>Brinkman</surname><given-names>L</given-names></name><name><surname>Todorova</surname><given-names>L</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Eye movement-related confounds in neural decoding of visual working memory representations</article-title><source>ENeuro</source><volume>5</volume><elocation-id>ENEURO.0401-17.2018</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0401-17.2018</pub-id><pub-id pub-id-type="pmid">30310862</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nijhawan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Motion extrapolation in Catching</article-title><source>Nature</source><volume>370</volume><fpage>256</fpage><lpage>257</lpage><pub-id pub-id-type="doi">10.1038/370256b0</pub-id><pub-id pub-id-type="pmid">8035873</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nijhawan</surname><given-names>R</given-names></name><name><surname>Kirschfeld</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Analogous mechanisms compensate for neural delays in the sensory and the motor pathways: evidence from motor flash-lag</article-title><source>Current Biology</source><volume>13</volume><fpage>749</fpage><lpage>753</lpage><pub-id pub-id-type="doi">10.1016/s0960-9822(03)00248-3</pub-id><pub-id pub-id-type="pmid">12725732</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noda</surname><given-names>H</given-names></name><name><surname>Freeman</surname><given-names>RB</given-names></name><name><surname>Gies</surname><given-names>B</given-names></name><name><surname>Creutzfeldt</surname><given-names>OD</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Neuronal responses in the visual cortex of awake cats to stationary and moving targets</article-title><source>Experimental Brain Research</source><volume>12</volume><fpage>389</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1007/BF00234494</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orban</surname><given-names>GA</given-names></name><name><surname>Hoffmann</surname><given-names>KP</given-names></name><name><surname>Duysens</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Velocity selectivity in the cat visual system. I. responses of LGN cells to moving bar stimuli: a comparison with cortical areas 17 and 18</article-title><source>Journal of Neurophysiology</source><volume>54</volume><fpage>1026</fpage><lpage>1049</lpage><pub-id pub-id-type="doi">10.1152/jn.1985.54.4.1026</pub-id><pub-id pub-id-type="pmid">4067619</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The videotoolbox software for visual psychophysics: transforming numbers into movies</article-title><source>Spatial Vision</source><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="pmid">9176953</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pernet</surname><given-names>CR</given-names></name><name><surname>Chauveau</surname><given-names>N</given-names></name><name><surname>Gaspar</surname><given-names>C</given-names></name><name><surname>Rousselet</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>LIMO EEG: a toolbox for hierarchical linear modeling of electroencephalographic data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><elocation-id>831409</elocation-id><pub-id pub-id-type="doi">10.1155/2011/831409</pub-id><pub-id pub-id-type="pmid">21403915</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Price</surname><given-names>D</given-names></name><name><surname>Knerr</surname><given-names>S</given-names></name><name><surname>Personnaz</surname><given-names>L</given-names></name><name><surname>Dreyfus</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Pairwise neural network classifiers with probabilistic outputs</article-title><conf-name>Neural Information Processing Systems</conf-name><fpage>1109</fpage><lpage>1116</lpage></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raemaekers</surname><given-names>M</given-names></name><name><surname>Lankheet</surname><given-names>MJM</given-names></name><name><surname>Moorman</surname><given-names>S</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>van Wezel</surname><given-names>RJA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Directional anisotropy of motion responses in retinotopic cortex</article-title><source>Human Brain Mapping</source><volume>30</volume><fpage>3970</fpage><lpage>3980</lpage><pub-id pub-id-type="doi">10.1002/hbm.20822</pub-id><pub-id pub-id-type="pmid">19449333</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rousselet</surname><given-names>GA</given-names></name><name><surname>Husk</surname><given-names>JS</given-names></name><name><surname>Bennett</surname><given-names>PJ</given-names></name><name><surname>Sekuler</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Spatial scaling factors explain eccentricity effects on face erps</article-title><source>Journal of Vision</source><volume>5</volume><fpage>755</fpage><lpage>763</lpage><pub-id pub-id-type="doi">10.1167/5.10.1</pub-id><pub-id pub-id-type="pmid">16441183</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sack</surname><given-names>AT</given-names></name><name><surname>Kohler</surname><given-names>A</given-names></name><name><surname>Linden</surname><given-names>DEJ</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Muckli</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The temporal characteristics of motion processing in hmt/V5+: combining fmri and neuronavigated TMS</article-title><source>NeuroImage</source><volume>29</volume><fpage>1326</fpage><lpage>1335</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.08.027</pub-id><pub-id pub-id-type="pmid">16185899</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salti</surname><given-names>M</given-names></name><name><surname>Monto</surname><given-names>S</given-names></name><name><surname>Charles</surname><given-names>L</given-names></name><name><surname>King</surname><given-names>JR</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinct cortical codes and temporal dynamics for conscious and unconscious percepts</article-title><source>eLife</source><volume>4</volume><elocation-id>e05652</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.05652</pub-id><pub-id pub-id-type="pmid">25997100</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schellekens</surname><given-names>W</given-names></name><name><surname>van Wezel</surname><given-names>RJA</given-names></name><name><surname>Petridou</surname><given-names>N</given-names></name><name><surname>Ramsey</surname><given-names>NF</given-names></name><name><surname>Raemaekers</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Predictive coding for motion stimuli in human early visual cortex</article-title><source>Brain Structure &amp; Function</source><volume>221</volume><fpage>879</fpage><lpage>890</lpage><pub-id pub-id-type="doi">10.1007/s00429-014-0942-2</pub-id><pub-id pub-id-type="pmid">25445839</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schellekens</surname><given-names>W</given-names></name><name><surname>Ramsey</surname><given-names>NF</given-names></name><name><surname>van Wezel</surname><given-names>RJA</given-names></name><name><surname>Raemaekers</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Changes in fMRI BOLD dynamics reflect anticipation to moving objects</article-title><source>NeuroImage</source><volume>161</volume><fpage>188</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.09.017</pub-id><pub-id pub-id-type="pmid">27620983</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>M</given-names></name><name><surname>Marquardt</surname><given-names>I</given-names></name><name><surname>Sengupta</surname><given-names>S</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Martino</surname><given-names>FD</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Motion Displaces Population Receptive Fields in the Direction Opposite to Motion</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/759183</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Souihel</surname><given-names>S</given-names></name><name><surname>Cessac</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>On the potential role of lateral connectivity in retinal anticipation</article-title><source>Journal of Mathematical Neuroscience</source><volume>11</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1186/s13408-020-00101-z</pub-id><pub-id pub-id-type="pmid">33420903</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Subramaniyan</surname><given-names>M</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Patel</surname><given-names>SS</given-names></name><name><surname>Cotton</surname><given-names>RJ</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Faster processing of moving compared with flashed bars in awake macaque V1 provides a neural correlate of the flash lag illusion</article-title><source>Journal of Neurophysiology</source><volume>120</volume><fpage>2430</fpage><lpage>2452</lpage><pub-id pub-id-type="doi">10.1152/jn.00792.2017</pub-id><pub-id pub-id-type="pmid">30365390</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sundberg</surname><given-names>KA</given-names></name><name><surname>Fallah</surname><given-names>M</given-names></name><name><surname>Reynolds</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A motion-dependent distortion of retinotopy in area V4</article-title><source>Neuron</source><volume>49</volume><fpage>447</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.12.023</pub-id><pub-id pub-id-type="pmid">16446147</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thaler</surname><given-names>L</given-names></name><name><surname>Schütz</surname><given-names>AC</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>What is the best fixation target? the effect of target shape on stability of fixational eye movements</article-title><source>Vision Research</source><volume>76</volume><fpage>31</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2012.10.012</pub-id><pub-id pub-id-type="pmid">23099046</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tse</surname><given-names>PU</given-names></name><name><surname>Sheinberg</surname><given-names>DL</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Fixational eye movements are not affected by abrupt onsets that capture attention</article-title><source>Vision Research</source><volume>42</volume><fpage>1663</fpage><lpage>1669</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(02)00076-7</pub-id><pub-id pub-id-type="pmid">12079794</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>van Driel</surname><given-names>J</given-names></name><name><surname>Olivers</surname><given-names>CNL</given-names></name><name><surname>Fahrenfort</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>High-Pass Filtering Artifacts in Multivariate Classification of Neural Time Series Data</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/530220</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Heusden</surname><given-names>E</given-names></name><name><surname>Harris</surname><given-names>AM</given-names></name><name><surname>Garrido</surname><given-names>MI</given-names></name><name><surname>Hogendoorn</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Predictive coding of visual motion in both monocular and binocular human visual processing</article-title><source>Journal of Vision</source><volume>19</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1167/19.1.3</pub-id><pub-id pub-id-type="pmid">30630191</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanni</surname><given-names>S</given-names></name><name><surname>Dojat</surname><given-names>M</given-names></name><name><surname>Warnking</surname><given-names>J</given-names></name><name><surname>Delon-Martin</surname><given-names>C</given-names></name><name><surname>Segebarth</surname><given-names>C</given-names></name><name><surname>Bullier</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Timing of interactions across the visual field in the human cortex</article-title><source>NeuroImage</source><volume>21</volume><fpage>818</fpage><lpage>828</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.10.035</pub-id><pub-id pub-id-type="pmid">15006648</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Myers</surname><given-names>NE</given-names></name><name><surname>Stokes</surname><given-names>M</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Temporally unconstrained decoding reveals consistent but time-varying stages of stimulus processing</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>863</fpage><lpage>874</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy290</pub-id><pub-id pub-id-type="pmid">30535141</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>HX</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Motion direction biases and decoding in human visual cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>12601</fpage><lpage>12615</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1034-14.2014</pub-id><pub-id pub-id-type="pmid">25209297</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitney</surname><given-names>D</given-names></name><name><surname>Murakami</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Latency difference, not spatial extrapolation</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>39</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1038/3659</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitney</surname><given-names>D</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Motion distorts visual space: shifting the perceived position of remote stationary objects</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>954</fpage><lpage>959</lpage><pub-id pub-id-type="doi">10.1038/78878</pub-id><pub-id pub-id-type="pmid">10966628</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitney</surname><given-names>D</given-names></name><name><surname>Murakami</surname><given-names>I</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Illusory spatial offset of a flash relative to a moving stimulus is caused by differential latencies for moving and flashed stimuli</article-title><source>Vision Research</source><volume>40</volume><fpage>137</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(99)00166-2</pub-id><pub-id pub-id-type="pmid">10793892</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitney</surname><given-names>D</given-names></name><name><surname>Goltz</surname><given-names>HC</given-names></name><name><surname>Thomas</surname><given-names>CG</given-names></name><name><surname>Gati</surname><given-names>JS</given-names></name><name><surname>Menon</surname><given-names>RS</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Flexible retinotopy: motion-dependent position coding in the visual cortex</article-title><source>Science</source><volume>302</volume><fpage>878</fpage><lpage>881</lpage><pub-id pub-id-type="doi">10.1126/science.1087839</pub-id><pub-id pub-id-type="pmid">14500849</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wibral</surname><given-names>M</given-names></name><name><surname>Bledowski</surname><given-names>C</given-names></name><name><surname>Kohler</surname><given-names>A</given-names></name><name><surname>Singer</surname><given-names>W</given-names></name><name><surname>Muckli</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The timing of feedback to early visual cortex in the perception of long-range apparent motion</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>1567</fpage><lpage>1582</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn192</pub-id><pub-id pub-id-type="pmid">19008460</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wojtach</surname><given-names>WT</given-names></name><name><surname>Sung</surname><given-names>K</given-names></name><name><surname>Truong</surname><given-names>S</given-names></name><name><surname>Purves</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>An empirical explanation of the flash-lag effect</article-title><source>PNAS</source><volume>105</volume><fpage>16338</fpage><lpage>16343</lpage><pub-id pub-id-type="doi">10.1073/pnas.0808916105</pub-id><pub-id pub-id-type="pmid">18852459</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>C</given-names></name><name><surname>Chang</surname><given-names>X</given-names></name><name><surname>Luo</surname><given-names>M</given-names></name><name><surname>Zheng</surname><given-names>Q</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Nie</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Self-weighted robust LDA for multiclass classification with edge classes</article-title><source>ACM Transactions on Intelligent Systems and Technology</source><volume>12</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1145/3418284</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshor</surname><given-names>D</given-names></name><name><surname>Bosking</surname><given-names>WH</given-names></name><name><surname>Ghose</surname><given-names>GM</given-names></name><name><surname>Maunsell</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Receptive fields in human visual cortex mapped with surface electrodes</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2293</fpage><lpage>2302</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl138</pub-id><pub-id pub-id-type="pmid">17172632</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuen</surname><given-names>KK</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>The two-sample trimmed T for unequal population variances</article-title><source>Biometrika</source><volume>61</volume><fpage>165</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1093/biomet/61.1.165</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82424.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Press</surname><given-names>Clare</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02mb95055</institution-id><institution>Birkbeck, University of London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.06.26.496535" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.06.26.496535"/></front-stub><body><p>This article presents compelling visual motion processing data generated via clever analyses, to shed fundamental insights into how the brain compensates for neural processing delays. The approach and findings will be of interest to all neuroscientists – where neural delays prove a sticking point for many theories. One of its major strengths lies in the successful attempt to generalize neural representations of static objects to moving objects.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82424.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Press</surname><given-names>Clare</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02mb95055</institution-id><institution>Birkbeck, University of London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Press</surname><given-names>Clare</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02mb95055</institution-id><institution>Birkbeck, University of London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Chota</surname><given-names>Samson</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04pp8hn57</institution-id><institution>Utrecht University</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.06.26.496535">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.06.26.496535v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Position representations of moving objects align with real-time position in the early visual response&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Clare Press as Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Tirin Moore as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Samson Chota (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Summary of Revisions:</p><p>Overall assessment:</p><p>The reviewers find the patterns in the data interesting and believe this view would be shared by other visual neuroscientists as well as neuroscientists from other fields – where neural delays are likely to prove a universal sticking point to many theories. One of its major strengths lies in the successful attempt to generalize neural representations of static objects to moving objects. They also found the manuscript to be well-written – well-tuned, clear, and interesting, with well-presented figures.</p><p>Introduction:</p><p>1. A little more work could be done to sell this to readers who do not study motion perception in vision. Compensating for neural processing delays will be a problem that most neuroscientific theories need to incorporate; hence why we believe this is especially suitable for publication in <italic>eLife</italic>. We would suggest adding some literature considering an application to other sensory (e..g, auditory) processing, as well as potentially higher-level cognitive domains.</p><p>Results:</p><p>2. We found the Results section to be well-written, like the rest of the manuscript, but due to the complexity of the analysis we think the reader could be given a little more help. E.g., in the final paragraph of the Results section can some clear flags be inserted to show which aspects of the regression result support which of the conclusions? We think this would help more non-specialised readers grasp the reasons for the conclusion.</p><p>3. We would recommend the authors include an illustration of the compensation achieved at different cortical processing stages based on their findings. Perhaps it also makes sense to include approximate neural delays at different stages for a direct comparison. In addition, it might also help to illustrate how this compensation is presumably achieved in space by shifting encoding locations. We believe that these concepts can sometimes be tricky to communicate and the paper would benefit from those visual aids.</p><p>4. Have the authors tested for potential differences in how well static positions and movements can be decoded at different locations in the visual field? We can imagine that many readers would be curious if (at least static) decoding is uniform across the visual field. If certain positions/movements are decoded better then the authors could also consider the following analysis: Was there a group of subjects (or selection of best stimulus locations for decoding) for which the early static representations (&lt;100 ms) reliably generalized to motion? This might help to distinguish between poor signal-to-noise and entirely different codes (or direct connections from LGN/pulvinar) between early processing of static and motion stimuli.</p><p>Discussion:</p><p>5. The findings reported are derived from a specific case of motion perception which may not reflect the general mechanisms optimized for motion perception. The limitations related to task designs and the neural readouts should be discussed as they affect the way that the reported results will be interpreted.</p><p>6. This study is clearly different from the authors' earlier work, e.g., Blom et al. However, we think it would be useful to add some discussion to the Discussion section to outline what these Blom et al. studies showed and clearly, therefore, flag the novelty of this study/analysis.</p><p>7. The approach of using different epochs to train to approximate different stages of neural processing is clever. However, it seems that later epochs should reflect processing in later hierarchical points while still also reflecting processing in early points in the hierarchy. The authors should discuss the implications of this for their conclusions.</p><p>8. Could there be an impact on these findings of a large neural response when a stimulus enters the edge of a neuron's RF, followed by some adaptation? This impact should be discussed.</p><p>9. The findings displayed in Figure 4 (reversal of time to peak likelihood) are very intriguing. We believe the manuscript would benefit from discussing these particular results in a bit more detail in the discussion, especially in the light of feedback signals during motion processing. If the observed 'reversed' patterns indeed reflect feedback then by the time the feedback arrives back in the early visual cortex the original stimulus will have moved on and might be replaced by activity from subsequent (mismatching) bottom-up stimulus processing. In addition, referring to Hogendoorn and Burkitt (2019) the authors make a point that delayed compensation should occur in feedback activity, parts of which they do seem to capture with their method (Figure 4). Can the authors elaborate a bit on the potential functional relevance of the observed feedback signals and how they might differ from motion extrapolation-specific feedback?</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1. I think a little more work could be done to sell this to readers who do not study motion perception in vision. Compensating for neural processing delays will be a problem that most neuroscientific theories need to incorporate; hence why I believe this is especially suitable for publication in <italic>eLife</italic>. I would suggest adding some literature considering an application to other sensory (e.g, auditory) processing, as well as potentially higher-level cognitive domains.</p><p>2. This study is clearly different from the authors' earlier work, e.g., Blom et al. However, I think it would be useful to add some discussion to the Discussion section to outline what these Blom et al. studies showed and clearly, therefore, flag the novelty of this study/analysis.</p><p>3. I found the Results section to be well-written, like the rest of the manuscript, but due to the complexity of analysis, I think the reader could be given a little more help. E.g., in the final paragraph of the Results section can some clear flags be inserted to show which aspects of the regression result support which of the conclusions? I think this would help more non-specialised readers grasp the reasons for the conclusion.</p><p>4. Could there be an impact on these findings of a large neural response when a stimulus enters the edge of a neuron's RF, followed by some adaptation?</p><p>5. The approach of using different epochs to train to approximate different stages of neural processing is clever. However, it seems that later epochs should reflect processing in later hierarchical points while still also reflecting processing in early points. The authors should discuss the implications of this for their conclusions.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I honestly have very few remarks on this manuscript all of which concern minor details on form and interpretation. It's fantastic to work, and I am happy to recommend this paper for publication in <italic>eLife</italic>.</p><p>Remarks</p><p>1. I would recommend the authors include an illustration of the compensation achieved at different cortical processing stages based on their findings. Perhaps it also makes sense to include approximate neural delays at different stages for a direct comparison. In addition, it might also help to illustrate how this compensation is presumably achieved in space by shifting encoding locations. I believe that these concepts can sometimes be tricky to communicate and the paper would benefit from those visual aids.</p><p>2. Have the authors tested for potential differences in how well static positions and movements can be decoded at different locations in the visual field? I'm happy for the authors that the analysis works when collapsing across all positions, but I can imagine that many readers would be curious if (at least static) decoding is uniform across the visual field. If certain positions/movements are decoded better then the authors could consider the analysis in point 4 (at their own discretion).</p><p>3. The findings displayed in Figure 4 (reversal of time to peak likelihood) are very intriguing. I was hoping the authors could discuss these particular results in a bit more detail in the discussion, especially in the light of feedback signals during motion processing. If the observed 'reversed' patterns indeed reflect feedback then by the time the feedback arrives back in the early visual cortex the original stimulus will have moved on and might be replaced by activity from subsequent (mismatching) bottom-up stimulus processing. This seems to be somewhat problematic for their interpretation. In addition, referring to Hogendoorn and Burkitt (2019) the authors make a point that delay compensation should occur in feedback activity, parts of which they do seem to capture with their method (Figure 4). Can the authors elaborate a bit on the potential functional relevance of the observed feedback signals and how they might differ from motion extrapolation specific feedback?</p><p>4. Was there a group of subjects (or selection of best stimulus locations for decoding) for which the early static representations (&lt;100 ms) reliably generalized to motion? This might help to distinguish between poor signal-to-noise and entirely different codes (or direct connections from LGN/pulvinar) between early processing of static and motion stimuli.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I found the question under investigation to be interesting and I appreciate the authors' choice of recording modality and analytical methods. The paper is also well-organized and the results are presented with clarity. However, I believe the scope of this work is rather narrow and is better suited for a more specialized journal with a focus on human cognition/perception.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82424.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Summary of Revisions (for the authors):</p><p>Overall assessment:</p><p>The reviewers find the patterns in the data interesting and believe this view would be shared by other visual neuroscientists as well as neuroscientists from other fields – where neural delays are likely to prove a universal sticking point to many theories. One of its major strengths lies in the successful attempt to generalize neural representations of static objects to moving objects. They also found the manuscript to be well-written – well-tuned, clear, and interesting, with well-presented figures.</p></disp-quote><p>We thank the reviewers for their thoughtful, positive and constructive comments on the manuscript. Please find responses to the outstanding questions below. We believe that the edits made while addressing these questions have substantively improved the manuscript.</p><disp-quote content-type="editor-comment"><p>Introduction:</p><p>1. A little more work could be done to sell this to readers who do not study motion perception in vision. Compensating for neural processing delays will be a problem that most neuroscientific theories need to incorporate; hence why we believe this is especially suitable for publication in eLife. We would suggest adding some literature considering an application to other sensory (e.g, auditory) processing, as well as potentially higher-level cognitive domains.</p></disp-quote><p>Thank you for pointing out the broader application of our work. We certainly agree that the problem of overcoming neural delays is not restricted to motion perception. We have therefore broadened the scope of the introductory paragraphs:</p><p>Lines 23-43: “Responding quickly to the world around us is a primary function of the central nervous system: catching prey, escaping predators, and avoiding moving objects (e.g., falling rocks) are all crucial to survival. […] In this case, there is evidence that predictive processes help to compensate both for the neural delays incurred before visual input reaches the visual cortex and for the delays incurred during subsequent cortical processing (Berry et al., 1999; Jancke et al., 2004; Orban et al., 1985; Subramaniyan et al., 2018; Sundberg et al., 2006).”</p><disp-quote content-type="editor-comment"><p>Results:</p><p>2. We found the Results section to be well-written, like the rest of the manuscript, but due to the complexity of the analysis we think the reader could be given a little more help. E.g., in the final paragraph of the Results section can some clear flags be inserted to show which aspects of the regression result support which of the conclusions? We think this would help more non-specialised readers grasp the reasons for the conclusion.</p></disp-quote><p>Thank you for this comment. We have added some pointers to the specific equations that support our conclusions in the final section of the Results (see below), as well as rephrasing some sections throughout the Results, which we hope will improve clarity. We have also added an additional statistical test that enables us to directly compare the gradient and intercept of the relationship between training time and peak times for static and motion stimuli, which we hope makes it clearer from where our conclusions are coming. Additionally, please see the response to Question 3 for a new figure, which we hope helps illustrate our findings.</p><p>Results, Lines 260-290: “Furthermore, the difference between the static and motion gradients was not significantly different from zero (<italic>t</italic> = -1.58, <italic>p</italic> = 0.13) but the difference between the two intercepts was (<italic>t</italic> = 5.32, <italic>p</italic> = 3.32x10<sup>-5</sup>).[…] Based on the training time, these early stimulus representations likely originated in early visual cortex (V1-3), meaning that the early visual system was able to almost completely compensate for neural delays accumulated during processing up to that point and represent moving objects close to their real-time position.”</p><disp-quote content-type="editor-comment"><p>3. We would recommend the authors include an illustration of the compensation achieved at different cortical processing stages based on their findings. Perhaps it also makes sense to include approximate neural delays at different stages for a direct comparison. In addition, it might also help to illustrate how this compensation is presumably achieved in space by shifting encoding locations. We believe that these concepts can sometimes be tricky to communicate and the paper would benefit from those visual aids.</p></disp-quote><p>Thank you for this suggestion. An additional panel has been added to Figure 4, as well as text to explain it in the final section of the Results. We hope this illustration helps to communicate the delays that are compensated at different levels of processing. We chose not to illustrate the spatial lag, but have added additional text to explain how the temporal shift corresponds to a spatial shift.</p><p>Lines 266-268: In other words, delays that accumulate during cortical processing did not appear to be compensated when processing motion. This is illustrated in Figure 4c, where the relative delay between neural representations is preserved regardless of whether the stimulus is moving or not.</p><p>Lines 280-287: In turn, this means that these neural representations were activated at the time that the moving object was physically centred on the corresponding position. This can also be seen in Figure 4c: the representations of the moving stimulus emerge 73ms earlier than the same representations of the flashed stimulus. The earliest representation of the moving stimulus peaks when t = 0ms, the time at which the moving stimulus is in the same position of the flash. This corresponds to a spatial shift in the encoding of the moving stimulus, as, by the time the representations of the flash emerge, the moving stimulus will be represented further ahead on its path.</p><disp-quote content-type="editor-comment"><p>4. Have the authors tested for potential differences in how well static positions and movements can be decoded at different locations in the visual field? We can imagine that many readers would be curious if (at least static) decoding is uniform across the visual field. If certain positions/movements are decoded better then the authors could also consider the following analysis: Was there a group of subjects (or selection of best stimulus locations for decoding) for which the early static representations (&lt;100 ms) reliably generalized to motion? This might help to distinguish between poor signal-to-noise and entirely different codes (or direct connections from LGN/pulvinar) between early processing of static and motion stimuli.</p></disp-quote><p>We had not tested this, but we agree that these aspects of the decoding analysis are worth exploring further.</p><p>Firstly, we have investigated how stimulus-position likelihood varies over the visual field for the static stimulus presentations. We have now included the graph as a supplement to Figure 2. It shows the difference from chance likelihood as a function of time, with different lines corresponding to stimuli at different eccentricities. We decided to average by eccentricity, as the graph is quite difficult to read with 37 lines (i.e. one for each stimulus position; this can be seen in Figure II, top left panel).</p><p>It can be observed that decoding performance is highest for stimuli at fixation, and that performance generally decreases with increasing eccentricity. However, such differences are not clearly visible for the three furthest eccentricities relative to fixation. We believe that this is because, although the most eccentric stimuli probably elicit the smallest EEG signal, these likelihood calculations include comparisons between stimuli that are very far apart. For example, some pairwise comparisons included in the likelihood calculation for the stimulus on the far left are up to 24dva apart, but at fixation the maximum distance between the stimulus and the comparison positions is half the grid, 12dva. In Figure 2A in the manuscript, it can be seen that classification performance is highest for pairwise comparisons between stimuli further apart. Additionally, it has been shown that foveally presented stimuli elicit a much larger event-related potential (ERP) than peripheral stimuli (Rousselet et al., 2005), which would likely result in larger-magnitude ERPs for static stimuli at fixation. This could explain why the likelihood is dramatically higher for stimuli presented at fixation. We have noted these issues in the Figure 2 supplement caption.</p><p>Results, Lines 112-114: Stimulus-position likelihoods by eccentricity (distance from fixation) are plotted in Figure 2 —figure supplement 1.</p><p>Secondly, we explored whether selecting certain participants/positions with better position decoding would enable us to investigate a wider time window of motion processing. To this end, we selected good participants or good positions on the basis of the strength of decoding of flashes (to avoid cherry-picking within the motion decoding). More specifically, we selected either participants (<xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>) or positions (<xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>) for which decoding of the position of the flash was above 2*chance likelihood (i.e. 2*1/37). Through this approach we selected three participants and six positions. With regards to position, the best decoding of stimulus location was possible when the stimulus was horizontally aligned with, or just below, fixation. This is consistent with known anisotropies in visual field processing and representation (e.g. Carrasco et al., 2001; Van Essen et al., 1984). This selection did not enable us to meaningfully extend our results to earlier timepoints and, due to the exclusion of significant amounts of data, we believe our signal-to-noise ratio was lowered. In the case of looking at the best positions, we can now fit a Gaussian with R<sup>2</sup> greater than 0.5 at training timepoints before 50ms training time, when it is impossible that stimulus information is present in the brain. This indicates that the selection of a subset of participants and positions may be biasing our results toward finding decodable information at times when this would not be available to participants in the flashed condition.</p><p>One point of interest is that with both of these selection methods, the time of peak likelihood for training times 80-100ms is well before 0ms. This implies that these early representations of the stimulus might be over-extrapolated and that the brain may be representing the stimulus in a position it has not yet reached. However, we are reluctant to overinterpret analyses that lack power and as such we would prefer not to include these additional analyses in the main manuscript.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>Selection of participants for whom flash decoding is best.</title><p>Participants were selected if the difference from chance stimulus-position likelihood (averaged across all stimulus positions) was above a threshold of 1/37 (which is the same as the likelihood being twice as high as chance level). The top left panel shows the difference from chance likelihood against training time. The three selected participants are shown in a darker colour. The remaining panels show graphs corresponding to the graphs in the main manuscript: the analysis is exactly the same.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82424-sa2-fig1-v1.tif"/></fig><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>Selection of positions for which flash decoding is best.</title><p>Positions were selected if the difference from chance stimulus-position likelihood (averaged across all participants) was above a threshold of 1/37. In the top left panel, lines show difference from chance likelihood for all of the 37 possible stimulus locations. Inset on the top left panel shows the selected positions, highlighted in red (fixation) and black on the graph. Everything else is the same as above.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82424-sa2-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>Discussion:</p><p>5. The findings reported are derived from a specific case of motion perception which may not reflect the general mechanisms optimized for motion perception. The limitations related to task designs and the neural readouts should be discussed as they affect the way that the reported results will be interpreted.</p></disp-quote><p>We agree that these findings reflect just one specific case of motion, in particular because the stimulus moved at a constant (rather slow) speed, and along a predictable path. We have added the following sections to the Discussion to ensure this limitation is explicit. Some limitations of the neural readout approach are already discussed, namely the inability to pinpoint the signal to a particular neural source, and the low signal-to-noise ratio. To flag this section more clearly, and to better organise the Discussion in general, we have added subheadings to the Discussion. This section is now titled ‘Limitations’. We would welcome any further suggestions of limitations that we should include in this section. We have also compared the smooth motion used in our study to apparent motion, and have highlighted important differences between these, in our response to comment 6.</p><p>Lines 364 to 369: “If information about object motion is used to compensate for delays via feedback connections, it could be expected that the particular speed of the stimulus would be exploited to maximise the accuracy of the position representation. In the present study, we only tested one stimulus speed, so further research including a range of speeds, as found in natural scenes (Wojtach et al., 2008), is necessary to examine whether compensatory mechanisms work in this way.”</p><p>Lines 415 to 417: “Future research should extend the present findings by investigating the neural consequences of an unexpected change of direction, and generally unpredictable motion, in a smooth motion trajectory.”</p><disp-quote content-type="editor-comment"><p>6. This study is clearly different from the authors' earlier work, e.g., Blom et al. However, we think it would be useful to add some discussion to the Discussion section to outline what these Blom et al. studies showed and clearly, therefore, flag the novelty of this study/analysis.</p></disp-quote><p>Thank you for pointing this out. We have added a paragraph to the discussion comparing our results with Hogendorrn and Burkitt's (2018) and Blom et al.’s (2020; 2021) previous findings regarding apparent motion. Importantly, this prior work differs from ours in that we investigated smooth (rather than apparent) motion in the manuscript. The primary difference between the previous findings and ours is the extent of the compensation found. We have outlined these discrepancies and the possible reasons for differences in findings below:</p><p>Lines 304 – 420: “Comparison between smooth and apparent motion</p><p>Previous EEG research has investigated whether neural delays are compensated when viewing apparent motion. […] Delays seem to be compensated to a greater extent in this case of smooth motion, so the visual system would have to employ a larger correction for erroneous position encoding if the stimulus changed direction.”</p><disp-quote content-type="editor-comment"><p>7. The approach of using different epochs to train to approximate different stages of neural processing is clever. However, it seems that later epochs should reflect processing in later hierarchical points while still also reflecting processing in early points in the hierarchy. The authors should discuss the implications of this for their conclusions.</p></disp-quote><p>We agree that later timepoints in the epoch could capture ongoing processing at early points in the visual hierarchy, as well as processing in later hierarchical stages. Previous research suggests that the early visual response is dominated by a feedforward cascade, due to the timing of neural responses and delays between activity in different cortical areas (Lamme and Roelfsema, 2000). Additionally, the TGM in Figure 2c shows that there is not substantial cross-generalisation across time before 150ms (i.e. the red blob is quite restricted to the diagonal). Because our regression analyses were restricted to multivariate patterns occurring less than 150 post stimulus onset, we therefore believe this is not a significant issue in the present research. We have now added a sentence to the Results about this issue. However, even if there is lingering activity from earlier visual areas, we would argue that this does not alter the main conclusions of the paper (processing happens at the same rate regardless of whether the stimulus is moving or not; there is a lot of compensation for delays very early in processing). This is now mentioned in the Limitations section of the Discussion.</p><p>Results, Lines 130-132: “It appears that the stimulus representations before 150ms training/test time did not generalise to other timepoints, but later reactivation of representations is apparent after 150ms (off-diagonal red blobs).”</p><p>Discussion, Lines 435-440: “While the initial response is likely to be dominated by a feedforward cascade through the visual hierarchy (Lamme and Roelfsema, 2000), it is possible that activity recorded at later timepoints in the training epoch reflects ongoing processing in hierarchically early visual areas, as well as additional processing in hierarchically later visual areas. However, this issue does not alter our conclusions concerning the relative timing of activity when viewing static or moving stimuli.”</p><p>Relatedly, we agree with the suggestion that later timepoints within the training epoch could reflect reactivation of earlier stimulus representations, as well as further processing of the stimulus. We believe this is the reason that the observed oscillatory pattern emerges. For more discussion of this point, please refer to our response to Question 9.</p><disp-quote content-type="editor-comment"><p>8. Could there be an impact on these findings of a large neural response when a stimulus enters the edge of a neuron's RF, followed by some adaptation? This impact should be discussed.</p></disp-quote><p>Thank you for this suggestion. We agree that adaptation in response to the leading edge of the stimulus is a possible mechanism for the observed effect. This idea is similar to theories about how delays are compensated in the retina when viewing motion (e.g., Berry et al., 1999). Previous studies have suggested that similar computation could indeed occur in the visual cortex, as the organisation of neurons could bring about the balance of excitation and inhibition necessary to compensate for delays in this way (Johnston and Lagnado, 2015). It takes the stimulus 400ms to move from one flash position to the next, so the leading edge of the stimulus is closer to a particular flash position than the previous at -200ms. We see that the likelihood of the stimulus being present in that position starts ramping up much earlier that -200ms, so we think that this compensation is unlikely to be completely driven by a large response to the leading edge, followed by adaptation. We have added words to this effect on Lines 267-273, as shown below:</p><p>Lines 347 – 356: “In particular, the retinal compensation mechanisms only appear to act up to speeds of about 5dva/s (Berry et al., 1999; Jancke et al., 2004). This is thought to be achieved by a large response to the leading edge of the stimulus, followed by gain control mechanisms reducing neuronal firing rates. In the present study, it takes 400ms for the stimulus to travel from one static stimulus position to the next, so the leading edge of the moving stimulus is closest to the stimulus position it is approaching 200ms before it reaches that position. In general, we see a ramping in the likelihood earlier than -200ms (see Figure 3 – Supplementary Figure 1), so we believe that this adaptation is unlikely to be the sole mechanism responsible for the observed shift in encoded location.”</p><disp-quote content-type="editor-comment"><p>9. The findings displayed in Figure 4 (reversal of time to peak likelihood) are very intriguing. We believe the manuscript would benefit from discussing these particular results in a bit more detail in the discussion, especially in the light of feedback signals during motion processing. If the observed 'reversed' patterns indeed reflect feedback then by the time the feedback arrives back in the early visual cortex the original stimulus will have moved on and might be replaced by activity from subsequent (mismatching) bottom-up stimulus processing. In addition, referring to Hogendoorn and Burkitt (2019) the authors make a point that delayed compensation should occur in feedback activity, parts of which they do seem to capture with their method (Figure 4). Can the authors elaborate a bit on the potential functional relevance of the observed feedback signals and how they might differ from motion extrapolation-specific feedback?</p></disp-quote><p>Thank you for this comment; we agree that feedback activity likely plays an important role in compensation for delays. However, while we do find evidence of continuing feedforward and feedback sweeps of activity, we do not think that this is particularly informative to the research question. This is because, rather than corresponding to further processing of the moving stimulus, we believe the oscillatory pattern emerges because part of the flash-evoked signal reflects reactivation of earlier stimulus representations. To explain further, as time passes after the onset of a static stimulus, stimulus-evoked activity progresses through different visual areas. However, there is also an increase in recurrent processing, whereby earlier representations of the stimulus are reactivated. We believe that after ~150ms, the further processing of the static and moving stimuli diverges. However, because part of the flash-evoked signal reflects recurrent processing in earlier visual areas, it is still possible to decode the position of the motion on the basis of these reactivated representations. We therefore think this oscillatory pattern (which is similar to that seen in Dijkstra et al. (2020)) is more informative about processing of static stimuli than motion. We have added a few sentences to this effect in the Discussion section about feedback. We additionally realised that this line of reasoning was not clear in the Results, so we have tried to clarify this point there as well.</p><p>Results, Lines 199-219: <bold>“</bold>This pattern subsequently reverses between 150 and 200ms, indicating that hierarchically later representations were activated at a shorter latency. Finally, from a training time of ~250ms the time to peak likelihood was stable at approximately 50ms. […] If this later activity (&gt;150ms training time) does indeed reflect feedback processing of the static stimulus, then, for these later timepoints, the latency measure we have calculated might not be informative about the time necessary to first represent the moving object, because the initial activations and the reactivations are indistinguishable.”</p><p>Discussion, Lines 383 – 391<bold>: “</bold>Although this analysis captures feedforward and feedback sweeps of activity through the visual cortex, we believe that these oscillations are present in the processing of the flash, not the motion. This would mean that the portion of the later flash-evoked signal that cross-generalises to motion reflects reactivation of earlier stimulus representations rather than further stimulus processing. However, there remains the possibility that these oscillations are evoked by motion. This would imply that, during feedback activity, the position of the stimulus is not updated as the stimulus moves, and that the stimulus is likely represented in multiple locations concurrently in the visual system (King and Wyart, 2021). Further research is needed to tease apart these options.”</p><p>References</p><p>Benvenuti, G., Chemla, S., Boonman, A., Perrinet, L., Masson, G. S., and Chavane, F. (2020). Anticipatory responses along motion trajectories in awake monkey area V1. <italic>BioRXiv</italic>. https://doi.org/10.1101/2020.03.26.010017</p><p>Berry, M. J., Brivanlou, I. H., Jordan, T. A., and Meister, M. (1999). Anticipation of moving stimuli by the retina. <italic>Nature</italic>, <italic>398</italic>(6725), 334–338. https://doi.org/10.1038/18678</p><p>Blom, T., Bode, S., and Hogendoorn, H. (2021). The time-course of prediction formation and revision in human visual motion processing. <italic>Cortex</italic>, <italic>138</italic>, 191–202. https://doi.org/10.1016/j.cortex.2021.02.008</p><p>Blom, T., Feuerriegel, D., Johnson, P., Bode, S., and Hogendoorn, H. (2020). Predictions drive neural representations of visual events ahead of incoming sensory information. Proceedings of the National Academy of Sciences of the United States of America, 117(13). https://doi.org/10.1073/pnas.1917777117</p><p>Carrasco, M., P.Talgar, C., and Cameron, E. L. (2001). Characterizing visual performance fields: Effects of transient covert attention, spatial frequency, eccentricity, task and set size. <italic>Spatial Vision</italic>, <italic>15</italic>(1), 61–75. https://doi.org/10.1163/15685680152692015</p><p>Chemla, S., Reynaud, X. A., Di Volo, M., Yann Zerlaut, X., Perrinet, X. L., Destexhe, A., and Chavane, X. F. (2019). Suppressive traveling waves shape representations of illusory motion in primary visual cortex of awake primate. <italic>Journal of Neuroscience</italic>, <italic>39</italic>(22). https://doi.org/10.1523/JNEUROSCI.2792-18.2019</p><p>Clark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive science. <italic>Behavioral and Brain Sciences</italic>, <italic>36</italic>(3), 181–204. https://doi.org/10.1017/S0140525X12000477</p><p>Dijkstra, N., Ambrogioni, L., Vidaurre, D., and van Gerven, M. (2020). Neural dynamics of perceptual inference and its reversal during imagery. <italic>ELife</italic>, <italic>9</italic>. https://doi.org/10.7554/<italic>eLife</italic>.53588</p><p>Friston, K. (2010). The free-energy principle: A unified brain theory? <italic>Nature Reviews Neuroscience</italic>, <italic>11</italic>(2), 127–138. https://doi.org/10.1038/nrn2787</p><p>Friston, K. J., Daunizeau, J., Kilner, J., and Kiebel, S. J. (2010). Action and behavior: A free-energy formulation. <italic>Biological Cybernetics</italic>, <italic>102</italic>(3), 227–260. https://doi.org/10.1007/s00422-010-0364-z</p><p>Guo, K., Robertson, R. G., Pulgarin, M., Nevado, A., Panzeri, S., Thiele, A., and Young, M. P. (2007). Spatio-temporal prediction and inference by V1 neurons. <italic>European Journal of Neuroscience</italic>, <italic>26</italic>(4), 1045–1054. https://doi.org/10.1111/j.1460-9568.2007.05712.x</p><p>Harvey, B. M., and Dumoulin, S. O. (2011). The relationship between cortical magnification factor and population receptive field size in human visual cortex: Constancies in cortical architecture. <italic>Journal of Neuroscience</italic>, <italic>31</italic>(38), 13604–13612. https://doi.org/10.1523/JNEUROSCI.2572-11.2011</p><p>Hogendoorn, H., and Burkitt, A. N. (2018). Predictive coding of visual object position ahead of moving objects revealed by time-resolved EEG decoding. <italic>NeuroImage</italic>, <italic>171</italic>, 55–61. https://doi.org/10.1016/j.neuroimage.2017.12.063</p><p>Jancke, D., Erlhagen, W., Schöner, G., and Dinse, H. R. (2004). Shorter latencies for motion trajectories than for flashes in population responses of cat primary visual cortex. <italic>J Physiol</italic>, <italic>556</italic>, 971–982. https://doi.org/10.1113/jphysiol.2003.058941</p><p>Johnston, J., and Lagnado, L. (2015). General features of the retinal connectome determine the computation of motion anticipation. <italic>ELife</italic>, <italic>2015</italic>(4). https://doi.org/10.7554/<italic>eLife</italic>.06250</p><p>Kiebel, S. J., Daunizeau, J., and Friston, K. J. (2008). A Hierarchy of Time-Scales and the Brain. <italic>PLoS Computational Biology</italic>, <italic>4</italic>(11), e1000209. https://doi.org/10.1371/journal.pcbi.1000209</p><p>King, J.-R., and Wyart, V. (2021). The Human Brain Encodes a Chronicle of Visual Events at each Instant of Time thanks to the Multiplexing of Traveling Waves. <italic>The Journal of Neuroscience</italic>. https://doi.org/10.1523/JNEUROSCI.2098-20.2021</p><p>Lamme, V. A. F. F., and Roelfsema, P. R. (2000). The distinct modes of vision offered by feedforward and recurrent processing. <italic>Trends in Neurosciences</italic>, <italic>23</italic>(11), 571–579. https://doi.org/10.1016/S0166-2236(00)01657-X</p><p>Millidge, B., Seth, A., and Buckley, C. L. (2022). Predictive Coding: A Theoretical and Experimental Review. <italic>ArXiv</italic>. https://doi.org/10.48550/arXiv.2107.12979</p><p>Nijhawan, R. (1994). Motion extrapolation in catching. <italic>Nature</italic>, <italic>370</italic>(6487), 256–257. https://doi.org/10.1038/370256b0</p><p>Orban, G. A., Hoffmann, K. P., and Duysens, J. (1985). Velocity selectivity in the cat visual system. I. Responses of LGN cells to moving bar stimuli: A comparison with cortical areas 17 and 18. <italic>Journal of Neurophysiology</italic>, <italic>54</italic>(4), 1026–1049. https://doi.org/10.1152/jn.1985.54.4.1026</p><p>Rousselet, G. A., Husk, J. S., Bennett, P. J., and Sekuler, A. B. (2005). Spatial scaling factors explain eccentricity effects on face ERPs. <italic>Journal of Vision</italic>, <italic>5</italic>(10), 755–763. https://doi.org/10.1167/5.10.1</p><p>Subramaniyan, M., Ecker, A. S., Patel, S. S., Cotton, R. J., Bethge, M., Pitkow, X., Berens, P., and Tolias, A. S. (2018). Faster processing of moving compared with flashed bars in awake macaque V1 provides a neural correlate of the flash lag illusion. <italic>Journal of Neurophysiology</italic>, <italic>120</italic>(5), 2430–2452. https://doi.org/10.1152/jn.00792.2017</p><p>Sundberg, K. A., Fallah, M., and Reynolds, J. H. (2006). A motion-dependent distortion of retinotopy in area V4. <italic>Neuron</italic>, <italic>49</italic>(3), 447–457. https://doi.org/10.1016/j.neuron.2005.12.023</p><p>Van Essen, D. C., Newsome, W. T., and Maunsell, J. H. R. (1984). The visual field representation in striate cortex of the macaque monkey: Asymmetries, anisotropies, and individual variability. <italic>Vision Research</italic>, <italic>24</italic>(5), 429–448. https://doi.org/10.1016/0042-6989(84)90041-5</p><p>Wojtach, W. T., Sung, K., Truong, S., and Purves, D. (2008). An empirical explanation of the flash-lag effect. Proceedings of the National Academy of Sciences of the United States of America, 105(42), 16338–16343. https://doi.org/10.1073/pnas.0808916105</p></body></sub-article></article>