<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">80979</article-id><article-id pub-id-type="doi">10.7554/eLife.80979</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title><italic>Efficient value synthesis</italic> in the orbitofrontal cortex explains how loss aversion adapts to the ranges of gain and loss prospects</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Brochard</surname><given-names>Jules</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Daunizeau</surname><given-names>Jean</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9142-1270</contrib-id><email>jean.daunizeau@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02en5vm52</institution-id><institution>Sorbonne Université</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/050gn5214</institution-id><institution>Institut du Cerveau</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff><aff id="aff3"><label>3</label><institution>INSERM UMR S1127</institution><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>National Institute on Drug Abuse Intramural Research Program</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>09</day><month>12</month><year>2024</year></pub-date><volume>13</volume><elocation-id>e80979</elocation-id><history><date date-type="received" iso-8601-date="2022-06-10"><day>10</day><month>06</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2024-11-05"><day>05</day><month>11</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2020-09-09"><day>09</day><month>09</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.09.08.287714"/></event></pub-history><permissions><copyright-statement>© 2024, Brochard and Daunizeau</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Brochard and Daunizeau</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-80979-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-80979-figures-v1.pdf"/><abstract><p>Is irrational behavior the incidental outcome of biological constraints imposed on neural information processing? In this work, we consider the paradigmatic case of gamble decisions, where gamble values integrate prospective gains and losses. Under the assumption that neurons have a limited firing response range, we show that mitigating the ensuing information loss within artificial neural networks that synthetize value involves a specific form of self-organized plasticity. We demonstrate that the ensuing efficient value synthesis mechanism induces value range adaptation. We also reveal how the ranges of prospective gains and/or losses eventually determine both the behavioral sensitivity to gains and losses and the information content of the network. We test these predictions on two fMRI datasets from the OpenNeuro.org initiative that probe gamble decision-making but differ in terms of the range of gain prospects. First, we show that peoples' loss aversion eventually adapts to the range of gain prospects they are exposed to. Second, we show that the strength with which the orbitofrontal cortex (in particular: Brodmann area 11) encodes gains and expected value also depends upon the range of gain prospects. Third, we show that, when fitted to participant’s gambling choices, self-organizing artificial neural networks generalize across gain range contexts and predict the geometry of information content within the orbitofrontal cortex. Our results demonstrate how self-organizing plasticity aiming at mitigating information loss induced by neurons’ limited response range may result in value range adaptation, eventually yielding irrational behavior.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>value</kwd><kwd>decision</kwd><kwd>efficient coding</kwd><kwd>range adaptation</kwd><kwd>artificial neural networks</kwd><kwd>representational similarity analysis.</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-20-CE37-0006</award-id><principal-award-recipient><name><surname>Daunizeau</surname><given-names>Jean</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Computational modeling shows how neural mechanisms for mitigating biological constraints (such as neurons’ limited firing range) may eventually result in complex though predictable irrational behavior.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Why do we maintain unrealistic expectations or engage in irresponsible conduct? Maybe one of the most substantial and ubiquitous violations of rationality is peoples’ sensitivity to modifications and/or manipulations of contextual factors that are irrelevant to the decision problem (<xref ref-type="bibr" rid="bib30">Kahneman, 2011</xref>; <xref ref-type="bibr" rid="bib71">Seymour and McClure, 2008</xref>). A prominent example is that people’s attitude towards risk depends upon whether alternative choice options are framed either in terms of losses or in terms of gains (<xref ref-type="bibr" rid="bib31">Kahneman and Tversky, 2012</xref>). More generally, many forms of irrational behaviors stem from peoples’ relative (as opposed to absolute) perception of value, that is: value is perceived in relation to a contextual reference point (<xref ref-type="bibr" rid="bib71">Seymour and McClure, 2008</xref>; <xref ref-type="bibr" rid="bib73">Srivastava and Schrater, 2011</xref>). Because it provides a mechanistic interpretation of such relative/context-dependent decision processes, range adaptation in value-sensitive neurons is currently under intense scrutiny (<xref ref-type="bibr" rid="bib43">Louie et al., 2013</xref>; <xref ref-type="bibr" rid="bib66">Rangel and Clithero, 2012</xref>; <xref ref-type="bibr" rid="bib67">Rigoli et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Rustichini et al., 2017</xref>; <xref ref-type="bibr" rid="bib74">Steverson et al., 2019</xref>). Neural range adaptation was first observed in the brain’s perceptual system: neurons in the retina normalize their response to incoming light in their receptive field w.r.t. to the illumination context, such that output firing rates span the variation range of surrounding light intensities (see <xref ref-type="bibr" rid="bib11">Carandini and Heeger, 2011</xref> for a review). Importantly, this neural mechanism provides a principled explanation for some forms of context-dependent visual illusions (<xref ref-type="bibr" rid="bib46">May and Zhaoping, 2016</xref>; <xref ref-type="bibr" rid="bib60">Pooresmaeili et al., 2013</xref>; <xref ref-type="bibr" rid="bib79">Troscianko and Osorio, 2023</xref>). The underlying assumption here is that perceptual neurons <italic>transmit</italic> the information that they receive, i.e., a neuron’s input is the physical quantity that is signaled to the brain (e.g. light intensity within a certain frequency band), whereas the neuron’s output is the percept (e.g. perceived amount of red). In turn, range adaptation (to a neuron’s input signal) directly induces perceptual context-dependent effects. But neural range adaptation may not be a glitch in the brain’s perceptual system. Rather, it may be understood as the brain’s best attempt to produce optimal information processing, given its own hard-wired biological constraints. This is the perspective afforded by <italic>efficient coding</italic>: light-sensitive neurons <italic>should</italic> adapt their firing properties to mitigate visual information loss resulting from their limited firing range (<xref ref-type="bibr" rid="bib7">Brenner et al., 2000</xref>; <xref ref-type="bibr" rid="bib38">Laughlin, 1981</xref>; <xref ref-type="bibr" rid="bib81">Valerio and Navarro, 2003</xref>; <xref ref-type="bibr" rid="bib83">Wark et al., 2007</xref>). In other words, range adaptation would improve the average reliability of neural information transmission, at the cost of inducing visual illusions in some circumstances.</p><p>Range adaptation was later evidenced on neural value coding, i.e., value-sensitive neurons was shown to normalize their response w.r.t. the set of alternative options within a given choice context and/or to the recent history of experienced/prospective rewards (<xref ref-type="bibr" rid="bib43">Louie et al., 2013</xref>; <xref ref-type="bibr" rid="bib66">Rangel and Clithero, 2012</xref>). In particular, gradual range adaptation effects have been the focus of intense research over the past decade, because they hold the promise of explaining persistent forms of irrational behavior. In line with the existing literature on value processing in the brain, they have been repeatedly documented in non-human primates, mostly using electrophysiological recordings in the orbitofrontal cortex or OFC (<xref ref-type="bibr" rid="bib14">Conen and Padoa-Schioppa, 2019</xref>; <xref ref-type="bibr" rid="bib35">Kobayashi et al., 2010</xref>; <xref ref-type="bibr" rid="bib54">Padoa-Schioppa, 2009</xref>; <xref ref-type="bibr" rid="bib78">Tremblay and Schultz, 1999</xref>; <xref ref-type="bibr" rid="bib86">Yamada et al., 2018</xref>), though similar effects have been demonstrated in the anterior cingulate cortex (<xref ref-type="bibr" rid="bib10">Cai and Padoa-Schioppa, 2012</xref>) and the amygdala (<xref ref-type="bibr" rid="bib2">Bermudez and Schultz, 2010</xref>; <xref ref-type="bibr" rid="bib70">Saez et al., 2017</xref>). Although comparatively sparser, neural evidence for gradual value normalization in the human OFC and ventral striatum also exists (<xref ref-type="bibr" rid="bib8">Burke et al., 2016</xref>; <xref ref-type="bibr" rid="bib15">Cox and Kable, 2014</xref>; <xref ref-type="bibr" rid="bib22">Elliott et al., 2008</xref>). Importantly, when included in computational models of value-based decision-making, efficient coding in value-sensitive neurons partially explains specific forms of irrational behavior away (<xref ref-type="bibr" rid="bib58">Polanía et al., 2019</xref>; <xref ref-type="bibr" rid="bib87">Zimmermann et al., 2018</xref>).</p><p>Having said this, the neurophysiological bases of range adaptation in value-sensitive neurons are virtually unknown and their behavioral consequences are debated (<xref ref-type="bibr" rid="bib32">Khaw et al., 2017</xref>; <xref ref-type="bibr" rid="bib69">Rustichini et al., 2017</xref>). For example, that overt preferences do not shift along with the observed changes in the value-sensitivity of OFC neurons is puzzling. A possibility is that range adaptation in OFC neurons may be ‘“undone’ by Hebbian-like plasticity mechanisms that fine-tune the synaptic efficacy of downstream ‘value comparison’ neurons (<xref ref-type="bibr" rid="bib55">Padoa-Schioppa and Rustichini, 2014</xref>; <xref ref-type="bibr" rid="bib69">Rustichini et al., 2017</xref>). Implicit in this reasoning is the assumption that option values are typically considered as input signals to value-sensitive OFC neurons, which then transmit this information to downstream decision systems, in analogy to the transmission of light-intensity information by neurons in the retina (<xref ref-type="bibr" rid="bib42">Louie and Glimcher, 2012</xref>). But another possibility is that value coding in OFC neurons departs from the logic of efficient information transmission in the visual system (<xref ref-type="bibr" rid="bib8">Burke et al., 2016</xref>; <xref ref-type="bibr" rid="bib14">Conen and Padoa-Schioppa, 2019</xref>). For example, OFC neurons may be constructing (as opposed to receiving) value signals, out of input signals conveying information about possibly conflicting decision-relevant attributes (<xref ref-type="bibr" rid="bib40">Lim et al., 2013</xref>; <xref ref-type="bibr" rid="bib51">O’Doherty et al., 2021</xref>; <xref ref-type="bibr" rid="bib56">Pessiglione and Daunizeau, 2021</xref>; <xref ref-type="bibr" rid="bib62">Raghuraman and Padoa-Schioppa, 2014</xref>). We refer to this as <italic>value synthesis</italic>. In what follows, we consider the paradigmatic case of risky decisions, which require integrating attributes such as prospective gains and losses. Here, value synthesis implies weighing prospective gains and losses, such that the ensuing subjective value effectively arbitrates between pro- versus anti-gamble behavioral tendencies. Our working assumption is twofold: (i) attribute-integration units in the OFC receive idiosyncratic mixtures of signals from attribute-specific units, and (ii) value is read out from integration units using a dedicated population code. This enables us to extend existing models of efficient coding to the case of value synthesis. Under mild conditions regarding units’ response properties, we show that a simple form of self-organized plasticity between attribute-specific and attribute-integration units can mitigate information loss induced by the limited firing range of attribute-integration units. Under such an <italic>efficient value synthesis</italic> scenario, OFC neurons would not adapt to the range of (output) values; rather, they would adapt to the range of their native input signals. Importantly, the ensuing neural and behavioral consequences depend upon how the underlying self-organized plasticity mechanism modifies the shape of integration units’ receptive fields over the spanned gain/loss domain. In this work, we derive the self-organized plasticity rule that operates efficient value synthesis, highlight its neural and behavioral consequences, and test the ensuing quantitative predictions against behavioral and neural data.</p><p>In particular, we show how the ranges of value-relevant attributes (i.e. here: prospective gains and losses) eventually determine the geometry of information encoded in the population of integration units, as well as the landscape of output value signals over the spanned domain of attributes. This is important, because the latter drives peoples’ loss aversion, i.e., their tendency to overweigh prospective losses over prospective gains when considering whether to accept or reject risky gambles. We then perform an entirely novel re-analysis of two independent fMRI datasets, which are made available in the context of the <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/">https://openneuro.org/</ext-link> initiative (<xref ref-type="bibr" rid="bib59">Poldrack et al., 2013</xref>). In both studies, participants are asked to accept or reject a series of gambles, but the two studies differ w.r.t. to the range of prospective gains. First, we test the neural and behavioral predictions of efficient value synthesis. In particular, we provide evidence that peoples’ loss aversion progressively adapts to the range of prospective gains. We also evaluate the neural predictions of the efficient value synthesis scenario by quantifying the geometry of information in five subregions of the OFC. Finally, we fit the artificial neural network models to peoples’ gambling choices and show that, only when endowed with the self-organized plasticity mechanism for efficient value synthesis do they generalize across gain range context (i.e. across participants’ groups) and predict (out-of-sample) the full geometry of information content within the OFC.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Efficient value synthesis: Computational mechanism and model predictions</title><p>We consider that value synthesis is operated by neural networks composed of two layers (see Methods): (i) an attribute-specific layer further divided into two sets of units that differ in terms of their inputs (either trial-by-trial gains or losses), and (ii) an integration layer receiving outputs from both attribute-specific units (see <xref ref-type="fig" rid="fig1">Figure 1A</xref> below).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Efficient value synthesis.</title><p>(<bold>A</bold>) Schematic structure of the artificial neural network (ANN). Trial-by-trial prospective gains (G) and losses (L) first enter attribute-specific units (white circles), which then send their outputs to integration units (gray circles). The outputs of these units are then combined to yield trial-by-trial gamble values, using a linear population code. (<bold>B</bold>) The impact of self-organized plasticity on integration units’ response. Integration units receive a weighted mixture (v) of attribute units’ outputs (x), and their firing response is a sigmoidal mapping <italic>z</italic>=<italic>f(v</italic>) of their input. In principle, inputs to integration units (black dots) may sample the saturating range of their sigmoidal activation function (black vertical bar on the y-axis). However, self-organized plasticity modifies the connection strengths between attribute and integration units, such that the ensuing inputs to integration units (red dots) eventually fall within the responsive range of the activation function (red vertical bar on the y-axis).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Summary of sigmoidal integration units’ response profiles.</title><p>Top row: the receptive field of each artificial neural network /(ANN) integration unit (color code: from blue -minimal output response- to yellow -maximal output response-) is displayed as a function of prospective losses (x-axis) and gains (y-axis). Middle row: the units’ output response (y-axis) is plotted against the gamble’s expected value (x-axis). Black dots show the possibly different response magnitudes of integration units for a given expected value (EV) (which may be obtained by different combinations of prospective gains and losses). Bottom row: ANN value readout weights (y-axis), ANN’s readout value profile (color code) displayed as a function of prospective losses (x-axis) and gains (y-axis), ANN’s readout value (y-axis) plotted as a function of EV (x-axis), and the ANN readout value’s sensitivity to prospective gains (<inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and losses (<inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Efficiency of value synthesis.</title><p>(<bold>A</bold>) The expected log steepness of units’ activation functions (y-axis) is plotted before (black) and after self-organized plasticity for both the symmetrical (red) and asymmetrical (blue) gain/loss domains. (<bold>B</bold>) The stable choices’ rate (y-axis) is plotted as a function of neural noise variance, (x-axis) before (black dots and dotted lines) and after (plain lines) self-organized plasticity for both the symmetrical (red) and asymmetrical (blue) gain/loss domains.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Summary of integration units’ response profiles after self-organized plasticity.</title><p>Same format as <xref ref-type="fig" rid="fig1">Figure 1</xref>. Top row: the black boxes depict the (‘wide’) domain of prospective gains and losses that are spanned during efficient integration. Middle row: the gray shaded areas depict the range of expected values (EVs) that corresponds to the spanned (‘wide’) domain of prospective gains and losses.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Analysis of the information content within the artificial neural networks (ANN’s) integration layer.</title><p>Upper panels: representational dissimilarity matrices after self-organized plasticity has modified the integration layer’s receptive fields (color code: from blue -minimal dissimilarity- to yellow -maximal dissimilarity-) are displayed as a function of either prospective gains, prospective losses, or expected value (EV) (from left to right, 10 bins, x-axis, and y-axis). The black boxes show the spanned range of either gains, losses or EV. Lower panels: average neural dissimilarity (y-axis) is plotted as a function of absolute difference in prospective gains, prospective losses, or EV (from left to right, x-axis), both before (black) and after (red) self-organized plasticity has modified the integration layer’s receptive fields.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig1-figsupp4-v1.tif"/></fig></fig-group><p>By assumption, the gamble’s value <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> at trial <inline-formula><mml:math id="inf4"><mml:mi>t</mml:mi></mml:math></inline-formula> is read out from the response pattern in the integration layer using a linear population code (<xref ref-type="bibr" rid="bib56">Pessiglione and Daunizeau, 2021</xref>):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>υ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>υ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf5"><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math></inline-formula> (respectively, <inline-formula><mml:math id="inf6"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math></inline-formula>) is the output response of the <inline-formula><mml:math id="inf7"><mml:mi>k</mml:mi></mml:math></inline-formula><sup>th</sup> integration unit (respectively, <inline-formula><mml:math id="inf8"><mml:mi>j</mml:mi></mml:math></inline-formula><sup>th</sup> unit within the <inline-formula><mml:math id="inf9"><mml:mi>i</mml:mi></mml:math></inline-formula><sup>th</sup> attribute sublayer), <inline-formula><mml:math id="inf10"><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> is the corresponding value readout weight in the population code, <inline-formula><mml:math id="inf11"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math></inline-formula> is its input-output activation function, <inline-formula><mml:math id="inf12"><mml:msubsup><mml:mrow><mml:mi>υ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math></inline-formula> is the input signal to the <inline-formula><mml:math id="inf13"><mml:mi>k</mml:mi></mml:math></inline-formula><sup>th</sup> integration unit (which is made of a weighted mixture of attribute units’ outputs), <inline-formula><mml:math id="inf14"><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> is its connection strength with the <inline-formula><mml:math id="inf15"><mml:mi>j</mml:mi></mml:math></inline-formula><sup>th</sup> unit of the <inline-formula><mml:math id="inf16"><mml:mi>i</mml:mi></mml:math></inline-formula><sup>th</sup> attribute sublayer, and <inline-formula><mml:math id="inf17"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math></inline-formula> is the input signal to the <inline-formula><mml:math id="inf18"><mml:mi>i</mml:mi></mml:math></inline-formula><sup>th</sup> attribute sublayer (i.e. either gain or loss information) at trial <inline-formula><mml:math id="inf19"><mml:mi>t</mml:mi></mml:math></inline-formula>. Importantly, we assume that units have a bounded response range, i.e., their firing rate cannot exceed some predefined physiological limit. Recall that the main mechanistic constraint that acts on a neuron’s firing rate is its action potential’s refractory period, which depends upon how long it takes ion channels to complete a whole voltage activate-deactivate cycle (about a few milliseconds). Typically, even fast-spiking neocortical neurons cannot fire at a frequency higher than about 500 or 600 Hz (<xref ref-type="bibr" rid="bib82">Wang et al., 2016</xref>). In our context, this can be simply modeled using saturating (more precisely: sigmoidal) input-output activation functions. In turn, the receptive field of integration units over the bidimensional domain of prospective gains and losses can exhibit arbitrary idiosyncratic shapes (see e.g. <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><p>In principle, artificial neural networks (ANNs) described by <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> can be trained to output almost any subjective value landscape over the bidimensional domain spanned by prospective gains and losses. In particular, they can be trained to output actions’ expected value (EV), which would yield a neural implementation of normative decision-making. For example, consider a gamble decision that entails a 50/50 chance of either earning an amount G of money or losing an amount L. In this case, the gamble’s EV is simply the sum of prospective gains and losses, weighted by their occurrence probability, i.e., <inline-formula><mml:math id="inf20"><mml:mi>E</mml:mi><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mi>G</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mi>L</mml:mi></mml:math></inline-formula>. Training the network to operate this kind of (linear) value synthesis is trivial. If the units’ responses were perfectly reliable (i.e. noiseless), then this would eventually yield rational behavioral responses. But, due to the limited response range of neural units, even small amounts of noise on the integration layer may induce a strong loss of information on value. This happens when inputs to integration units fall outside their responsive range, because it saturates the output contrast. By analogy with efficient coding in the brain’s perceptual systems, <italic>efficient value synthesis</italic> would rely upon some form of unsupervised adaptation mechanism to mitigate such information loss. For ANNs that obey <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, one can show that the following self-organized plasticity rule operates efficient value synthesis (see Methods section):<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mi>β</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf21"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> is the trial-by-trial change in connection strength between attribute and integration units and <inline-formula><mml:math id="inf22"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math></inline-formula> is the slope parameter of the <inline-formula><mml:math id="inf23"><mml:mi>k</mml:mi></mml:math></inline-formula><sup>th</sup> integration unit’s activation function. Here, <inline-formula><mml:math id="inf24"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf25"><mml:mi>β</mml:mi></mml:math></inline-formula> determine both the magnitude and temporal scale at which self-organized plasticity unfolds (see Methods section). <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> modifies the network connections such that inputs to integration units (i.e. <inline-formula><mml:math id="inf26"><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) fall within the responsive range of their activation function, which maximizes the output variability induced by attribute inputs (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>). This eventually improves the behavioral resilience of the system to neural noise (see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). The plasticity rule in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> is ‘self-organized’ in the sense that it does not require any teaching or feedback signal. It is also ‘local,’ in that a connection only changes in response to (the recent history of) the outputs of the corresponding pair of connected units. Finally, it is ‘anti-Hebbian,’ in that connections tend to weaken when units co-activate. The exact form of self-organized plasticity that operates efficient value synthesis depends upon the units’ activation function, but these properties generalize to any nonlinear activation function that is continuous and monotonically increasing.</p><p>Setting <inline-formula><mml:math id="inf27"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> or <inline-formula><mml:math id="inf28"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> yields non-adaptive value synthesis, whereby the value readout is independent of the history of spanned prospective gains and losses. In what follows, we refer to this as <italic>static</italic> value synthesis. Otherwise (i.e. when <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), the self-organized plasticity mechanism in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> progressively modifies the shape of integration units’ receptive fields over the spanned gain/loss domain (see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). This has three main notable consequences.</p><p>First, this eventually induces apparent value range adaptation, i.e., the average response of integration units settles between two (almost) invariant bounds which systematically map onto the gambles’ EV extrema over the spanned gain/loss domain. In addition, despite potentially strong nonlinearities in the receptive fields of integration units, the average relationship between their activity and gambles’ EV is almost linear. Interestingly, this apparent value range adaptation effect comes in two variants, depending upon the sign of the correlation between units’ activity and gambles’ EV.</p><p><xref ref-type="fig" rid="fig2">Figure 2</xref> exemplifies the apparent value range adaptation effect. This is an average of over 1000 Monte-Carlo simulations, where we randomized the trained connections of ANNs that operate (either <italic>efficient</italic> or <italic>static</italic>) value synthesis prior to exposing them to four different series of 256 decision trials made of prospective gains and losses with a predefined range. More precisely, we considered two ranges (either narrow or wide) for both prospective gains and losses, and exposed the ANNs to each of the 2×2 range combinations. By chance, some units become less sensitive to prospective gains than to prospective losses: those will tend to show a negative correlation with EV (‘-EV-units’). Interestingly, the responses of ‘+EV-units’ and ‘-EV-units’ shown on panels A and B are reminiscent of value range adaptation effects evidenced using electrophysiological recordings of OFC neuron activity (<xref ref-type="bibr" rid="bib14">Conen and Padoa-Schioppa, 2019</xref>; <xref ref-type="bibr" rid="bib54">Padoa-Schioppa, 2009</xref>). In particular, one can see that the slope of the relationship between EV and integration units’ activity only depends upon the EV range, and not upon the actual bounds of the spanned domain of EVs. However, this value range adaptation effect is only apparent, in the sense that integration units do not respond to value: they respond to prospective gains and losses. This is important, because the underlying plasticity mechanism reacts to the relative range of spanned gains and losses, which is partially orthogonal to the induced range of EVs. On thus needs to consider the shape of the spanned domain of decision-relevant attributes to properly understand the neural and behavioral consequences of efficient value synthesis.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Apparent value range adaptation.</title><p>(<bold>A</bold>) The average units’ response (y-axis) to pairs of prospective gains and losses that fall within predefined expected value (EV) bins (x-axis) is shown, while artificial neural networks (ANNs) that operate efficient value synthesis are exposed to four different spanned gain/loss domains (blue: narrow ranges of gains and losses, violet: wide ranges of gains and losses, red: narrow gain range and wide loss range, yellow: wide gain range and narrow loss range, see main text). Only integration units that correlate positively with EV are shown. (<bold>B</bold>) integration units that correlate negatively with EV, same format as panel (<bold>A</bold>). C/D: same format as panels (<bold>A</bold> and <bold>B</bold>), for ANNs that operate static value synthesis.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig2-v1.tif"/></fig><p>Second, self-organized plasticity changes the information content within the integration layer. To show this, we measure the neural dissimilarity between response patterns within the integration layer for any pair of decision trials, and then quantify its change when pairwise differences in either prospective gains, losses or EV vary. We define the ‘neural encoding strength’ of gains, losses, or EV in terms of the gradient of neural dissimilarity per unit of absolute difference in gains, losses, or EV, respectively (see <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>). In brief, the neural encoding strength of gains (resp. losses) decreases when the spanned range of gains (resp. losses) increases. This is also true for EV, whose neural encoding strength in the integration layer decreases when either the range of gains or losses increases.</p><p>Third, this modifies the relative sensitivity of readout value to prospective gains and losses. We quantify the sensitivity of readout value w.r.t. to its constituent attributes in terms of its gradient per unit of prospective gain and losses. In brief, the sensitivity of the ANN’s readout value signal to prospective gains (resp. losses) decreases when the spanned range of gains (resp. losses) increases. In turn, behavioral loss aversion - as defined by the ratio between loss and gain sensitivities- increases (resp. decreases) when the range of gains (resp. losses) increases. This is important, because this suggests how peoples’ behavior will deviate from classical decision theory and exhibit irrational context-dependency effects.</p><p><xref ref-type="fig" rid="fig3">Figure 3</xref> below summarizes the impact of the shape of the spanned bidimensional gain/loss domain. This is an average of over 1000 Monte-Carlo simulations, where the trained ANN’s connections are randomized prior to self-organizing according to <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> in response to a series of 256 decision trials made of prospective gains and losses with a predefined range, which is systematically varied.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Impact of spanned ranges of gains and losses.</title><p>(<bold>A</bold>) The neural encoding strength of gains (color code: from blue -minimal encoding strength- to yellow -maximal encoding strength-) is shown as a function of the spanned range of losses (x-axis, range increases from left to right) and gains (y-axis, range increases from bottom to top). Note that the maximal range of prospective gains and losses is arbitrarily set to unity. (<bold>B</bold>) Neural encoding strength of losses, same format. (<bold>C</bold>) Neural encoding strength of EV, same format. (<bold>D</bold>) Behavioral sensitivity to gains, same format. (<bold>E</bold>) Behavioral sensitivity to losses, same format. (<bold>F</bold>) Behavioral loss aversion, same format.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig3-v1.tif"/></fig><p>Strikingly, the behavior will exhibit positive (respectively, negative) loss aversion when the context is favorable (respectively, unfavorable), i.e., when the spanned range of gains is greater (respectively, smaller) than that of losses (<xref ref-type="fig" rid="fig3">Figure 3F</xref>). Also, behavioral loss aversion is expected to be neutral when the spanned ranges of gains and losses are comparable (symmetrical gain/loss domain).</p><p>In addition to the main effects described above, one can see that there is a cross-attribute spillover effect, such that the behavioral and neural sensitivities to prospective gains (respectively, losses) also decrease when the spanned range of losses (respectively, gains) increases (<xref ref-type="fig" rid="fig3">Figure 3A, B, D and E</xref>). The magnitude of these spillover effects is comparatively weaker and may thus be more difficult to detect in an empirical setting. In fact, the only quantity that is similarly impacted by the ranges of gains and losses is the neural encoding strength of EV (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). This effect is partly driven by changes in the neural sensitivity to gains and losses (<xref ref-type="fig" rid="fig3">Figure 3A and B</xref>), which constrains the availability of information on EV within the integration layer. But it also derives from the distortion of the readout value profile (<xref ref-type="fig" rid="fig3">Figure 3D and E</xref>), which weakens the statistical relationship between EV and integration units’ activity patterns. Both are consequences of the changes in integration units’ receptive fields that are induced by self-organized plasticity (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplements 1</xref> and <xref ref-type="fig" rid="fig1s3">3</xref>). This eventually translates into an apparent value range adaptation phenomenon that generalizes the univariate effect reported in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p><p>Note that all these range adaptation effects actually unfold over time, as the network progressively self-organizes in response to prospective gains and losses. Importantly, however, numerical simulations show that the ensuing dynamics of neural and behavioral sensitivities converge, i.e., they eventually reach a steady-state. The convergence rate is governed by the parameter <inline-formula><mml:math id="inf31"><mml:mi>β</mml:mi></mml:math></inline-formula>, whereas the overall magnitude of these context-dependency effects is determined by the parameter <inline-formula><mml:math id="inf32"><mml:mi>α</mml:mi></mml:math></inline-formula>. Although the exact setting of the plasticity magnitude and rate parameters do modify the global magnitude of neural and behavioral sensitivity changes, the results shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> are representative of the impact of the spanned gain/loss domain’s shape.</p></sec><sec id="s2-2"><title>Model-free analysis of the NARPS dataset</title><p>We now present our re-analysis of the NARPS dataset (<xref ref-type="bibr" rid="bib6">Botvinik-Nezer et al., 2020b</xref>). This dataset includes two studies, each of which is composed of a group of 54 participants who make a series of risky decisions. On each trial, a gamble was presented, entailing a 50/50 chance of gaining an amount G of money or losing an amount L. As in <xref ref-type="bibr" rid="bib76">Tom et al., 2007</xref>, participants were asked to evaluate whether or not they would like to accept or reject the gambles presented to them. In the first study (hereafter referred to as the ‘narrow range’ group), participants decided on gambles made of gain and loss levels that were sampled from the same range (G and L independently varied between 5$ and 20$). In the second study (hereafter: the ‘wide range’ group), gain levels scaled to double the loss levels (L varied between 5$ and 20$, and G independently varied between 10$ and 40$). Importantly, both groups experience the exact same range of losses. In both studies, all 256 possible combinations of gains and losses were presented across trials (see Methods section). Importantly, the gambles’ outcomes were not revealed until the end of the experiment.</p><p>To begin with, we ask whether peoples’ loss aversion exhibits range adaptation, as predicted by efficient value synthesis. In our context, this implies that (i) peoples’ gambling rate should depend upon the gain range context (even within the EV range common to both groups), (ii) peoples’ behavioral sensitivity to gains should be higher in the narrow gain range group than in the wide gain range group, (iii) within-group averages of loss aversion should be initially similar and then progressively diverge as time unfolds, and (iv) participants from the wide gain range group should eventually exhibit strong loss aversion while participants from the narrow gain range group should be loss-neutral (cf. symmetrical gain/loss domain). <xref ref-type="fig" rid="fig4">Figure 4</xref> below summarizes the results of behavioral data analyses that aim at testing these predictions.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Do peoples’ loss aversion exhibit range adaptation?</title><p>(<bold>A</bold>) The group-average probability of gamble acceptance (y-axis) is plotted against deciles of gambles’ expected value (EV=(G–L)/2, x-axis), in both groups (red: narrow range, blue: wide range). Dots show raw data (error bars depict s.e.m.), and plain lines show predicted data under a logistic regression model (see main text). The gray-shaded area highlights the range of expected values that is common to both groups. (<bold>B</bold>) Estimates of gamble bias (w<sub>0</sub>) as well as sensitivity to gains (w<sub>G</sub>) and losses (w<sub>L</sub>) for both groups, under the logistic model (same color code as panel A, errorbars depict s.e.m.). (<bold>C</bold>) Average loss aversion (y-axis) is plotted for both groups (same color code, errorbars depict s.e.m.). (<bold>D</bold>) Temporal dynamics of group-average loss aversion (log w<sub>L</sub>/w<sub>G</sub>, y-axis, same color code) are plotted against time (a.u., x-axis). Shaded areas depict s.e.m.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Postdiction error and out-of-sample predictions of the logistic model.</title><p>(<bold>A</bold>) ‘Postdiction’ error (y-axis) is plotted against deciles of gambles’ expected value (x-axis), in both groups (red: narrow range, blue: wide range). The gray-shaded area highlights the range of expected values that is common to both groups. (<bold>B</bold>) The probability of gamble acceptance (y-axis) is plotted against the gambles’ expected value (x-axis), in both groups (red: narrow range, blue: wide range). Dots show raw data (errorbars depict s.em.), and Plain and dashed lines show postdictions and out-of-sample predictions, respectively. (<bold>C</bold>) Peoples’ gambling rate within the common expected value (EV) range (y-axis) is plotted as a function of estimated loss aversion (x-axis), for each group of subjects. (<bold>D</bold>) People’s gambling rate within the common EV range (y-axis) is plotted as a function of estimated gambling bias (x-axis), same format.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig4-figsupp1-v1.tif"/></fig></fig-group><p>Overall, the average gambling rate of people from the wide range group (65±2%) is much higher than that of people from the narrow range group (44±2%), and the group difference is significant (p&lt;10<sup>–4</sup>, <italic>F</italic>=44.8, dof=101). This is of course expected, given that people from the wide range group are exposed to gambles with higher value on average. However, and most importantly, within the range of EV that is common to both groups, people from the wide range group are <italic>less</italic> likely to gamble than people from the narrow range group (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Here, the average gambling rate of people from the wide range group is 41±3%, whereas it is 54±3% for people in the narrow range group, and this group difference is significant (p=0.003, <italic>F</italic>=9.2). This difference is likely due to the context in which people made these decisions, which is more favorable (higher gain prospects on average) in the wide-range group. This is the hallmark of context-dependency effects.</p><p>From <xref ref-type="fig" rid="fig4">Figure 4A</xref>, it seems that the variation in peoples’ tendency to accept risky gambles approximately spans the range of gambles’ values that they are exposed to. Under the efficient value synthesis scenario, this apparent value range adaptation effect is due to context-dependent changes in loss aversion. To investigate this effect, we first performed a within-subject logistic regression of trial-by-trial choice data onto gains and losses (including an intercept, see Methods section). In terms of balanced accuracy, this regression accurately explains 91±0.7% (respectively, 87±0.8%) of individual choices in the narrow (respectively, wide) range group (cf. plain lines in <xref ref-type="fig" rid="fig4">Figure 4A</xref>). A random effect analysis on regression weight estimates shows that all regression weights are significant at the group level (all p&lt;10<sup>–3</sup>), except for the intercept parameters (narrow range: p=0.26, wide range: p=0.14). This implies that peoples’ gambling behavior exhibits no systematic bias above and beyond the effects of prospective gains and losses. Regarding group differences, this analysis also failed to identify a group difference in the constant gambling bias (w<sub>0</sub>: p=0.82, <italic>F</italic>=0.05). However, peoples’ sensitivity to gains is significantly higher in the narrow range group than in the wide range group (w<sub>G</sub>: p=0.022, <italic>F</italic>=17.5), and this difference is almost significant for loss sensitivity (w<sub>L</sub>: p=0.068, <italic>F</italic>=3.4). This means that increasing the range of gain prospects decreases peoples’ sensitivity to gains (and maybe to losses as well, though to a lesser extent; see <xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p><p>We then derived indices of individual loss aversion, which we define as the log-transformed ratio of loss sensitivity to gain sensitivity, i.e., log(w<sub>L</sub>/w<sub>G</sub>) (<xref ref-type="bibr" rid="bib76">Tom et al., 2007</xref>). This definition is not confounded by possible behavioral temperature differences between groups of participants. Mean loss aversion indices are shown in <xref ref-type="fig" rid="fig4">Figure 4C</xref>. We found that people from the wide range group exhibit significant loss aversion (mean loss aversion index=0.41, sem=0.05, p&lt;10<sup>–4</sup>) whereas people from the narrow range group do not (mean loss aversion index=0.037, sem = 0.05, p=0.46), and the ensuing group difference is significant (p=0.0031, <italic>F</italic>=9.2). Importantly, inter-individual differences in loss aversion explain the observed inter-individual differences in peoples’ gambling rate within the common EV range across all participants (p&lt;10<sup>–4</sup>, <italic>F</italic>=39.9, see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>But is this loss aversion difference due to inter-individual trait differences, or did it grow over time as people are exposed to more gambles? To address this question, we repeated the within-subject logistic regression, this time on consecutive chunks of 16 trials (see Methods section). The resulting temporal dynamics of loss aversion are shown in <xref ref-type="fig" rid="fig3">Figure 3D</xref>. We found no significant time-by-group interaction (p=0.43, <italic>F</italic>=0.61), which is why we report separate (instantaneous) group comparisons. At the start of the experiment (first 16 trials), loss aversion is significant in both groups (wide range: mean loss aversion index=0.26, sem = 0.1, p=0.027, narrow range: mean loss aversion=0.25, sem=0.1, p=0.039), and there is no significant difference between groups (p=0.96, <italic>F</italic>=0.003). However, as time unfolds, loss aversion in both groups tends to spread apart: the difference between groups starts becoming significant after 32 trials (p=0.005, <italic>F</italic>=13.0) and stays significant thereafter (all p&lt;0.05) except for two chunks of trials (p=0.12 and p=0.054). At the end of the experiment (last 16 trials), loss aversion is significant in participants of the wide range group (mean loss aversion=0.41, sem=0.07, p&lt;10<sup>–4</sup>) but not in the narrow range group (mean loss aversion=0.07, sem=0.06, p=0.24), and the group difference is significant (p=0.00042, <italic>F</italic>=13.3).</p><p>Those results validate the behavioral predictions of the efficient value synthesis scenario. We now wish to test its neural predictions, namely: (i) EV, as well as prospective gains and losses, should be encoded in neural activity patterns within the OFC, (ii) the neural encoding strength of prospective gains should be higher in the narrow gain range group than in the wide gain range group, (iii) the neural encoding strength of prospective losses should be equivalent in both groups (up to cross-attribute spillover effects), and (iv) the neural encoding strength of EV should be higher in the narrow gain range group than in the wide gain range group.</p><p>We thus extracted the multivariate trial-by-trial BOLD response in five OFC subregions: the lateral and medial parts of Brodmann area 11, Brodmann area 13, Brodmann area 14, and Brodmann area 32 (see Figure 11 in the Methods section). After correcting for between-session and temporal autocorrelation confounding effects (see Methods), we derived the ROI-specific representational dissimilarity matrices and measured the neural encoding strengths of gains (<xref ref-type="fig" rid="fig5">Figure 5</xref>), losses (<xref ref-type="fig" rid="fig6">Figure 6</xref>), and EV (<xref ref-type="fig" rid="fig7">Figure 7</xref>). The corresponding RDMs are shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1,</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>, respectively. For the sake of completeness, the results of standard univariate fMRI data analyses can also be eyeballed in <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Neural encoding of prospective gains.</title><p>The upper panels show the trial-by-trial neural dissimilarity (y-axis) plotted as a function of trial-by-trial absolute difference in prospective gains (x-axis), for both groups of participants (red: narrow range, blue: wide range), and within each subregion of the OFC. The lower panels show the ensuing neural encoding strength of gains (y-axis), for both groups (same color code), and within each subregion of the OFC. The dotted line indicates the y-axis origin. Errorbars depict s.e.m., and p-values are uncorrected for multiple comparisons.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Gain-representational dissimilarity matrices (RDMs).</title><p>Average trial-by-trial neural dissimilarity is shown, having binned trials according to prospectives gains (average gains within bins increase from left to right and from bottom to top). Upper row: wide range group, lower row: narrow range group. Each column shows an OFC subregion (from left to right: medial part of BA13, lateral part of BA11, rostral part of BA14, medial part of BA11, rostral part of BA32). The color code is the same for all panels.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig5-figsupp1-v1.tif"/></fig></fig-group><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Neural encoding of prospective losses.</title><p>Same format as <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Loss-representational dissimilarity matrices (RDMs).</title><p>Average trial-by-trial neural dissimilarity is plotted, having binned trials according to prospectives losses. Same format as <xref ref-type="fig" rid="fig9">Figure 9</xref> (and the same color code).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig6-figsupp1-v1.tif"/></fig></fig-group><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Neural encoding of expected value (EV).</title><p>Same format as <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Expected value (EV)-representational dissimilarity matrices (RDMs).</title><p>Average trial-by-trial neural dissimilarity is plotted, having binned trials according to EVs. Same format as <xref ref-type="fig" rid="fig9">Figure 9</xref> (and the same color code).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig7-figsupp1-v1.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Univariate fMRI analyses.</title><p>First row: the average BOLD response (y-axis) is plotted as a function of prospective gains (x-axis), for both groups of participants (red: narrow group, blue: wide group). Second row: same as first row, for prospective losses. Third row: GLM parameter estimates (y-axis) for gains and losses. Fourth row: regression parameter estimates for expected value (EV). Each column shows an OFC subregion (from left to right: medial part of BA13, lateral part of BA11, rostral part of BA14, medial part of BA11, rostral part of BA32). The dashed black lines depict the y-axis origin.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig7-figsupp2-v1.tif"/></fig></fig-group><p>Of course, prospective gain distances for the wide range-group extend beyond those of the narrow-range group. One can see that, in all subregions of the OFC, neural dissimilarity tends to increase when the absolute difference in prospective gains increases, though this gradient typically attenuates for extreme gain differences. The lateral part of Brodmann area 11 is the only OFC subregion that exhibits a significant encoding of prospective gains in both groups of participants (wide range: p0.0051, narrow range: p&lt;10<sup>–3</sup>), as well as a significantly higher encoding strength of gains in the narrow range group than in the wide range group (p=0.032).</p><p>Overall, it seems that prospective losses are encoded less strongly in OFC neural activity than prospective gains (though typically about four times stronger in magnitude). Nevertheless, the lateral part of Brodmann area 11 still exhibits a significant encoding of prospective losses in both groups of participants wide range: p=0.0049, narrow range: p=0.0056, without a significant group difference (p=0.59).</p><p>The pattern of neural encoding of EV is globally similar to that of prospective gains. Importantly, the lateral part of Brodmann area 11 is the only OFC subregion that exhibits a significant encoding of EV in both groups of participants (wide range: p=0.0061, narrow range: p&lt;10<sup>–3</sup>), as well as a significantly higher encoding strength of EV in the narrow range group than in the wide range group (p=0.0012).</p><p>Note that ignoring the fMRI confounding effects does not alter qualitatively the results, though it tends to bury the signal within structured noise, which dampens statistical significance.</p><p>In brief, qualitative predictions of the efficient value synthesis scenario at both the behavioral and neural levels have been confirmed (at least in the lateral part of Brodmann area 11). We will now provide further quantitative evidence that efficient value synthesis in the OFC can explain range adaptation of loss aversion.</p></sec><sec id="s2-3"><title>Model-based analysis of the NARPS dataset</title><p>As can be seen from <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1; 2</xref>, quantitative predictions from the efficient value synthesis scenario actually depend upon model parameters that may vary across individuals. For example, differences in the connectivity matrix <inline-formula><mml:math id="inf33"><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> (at the start of the experiment) and/or value readout weights <inline-formula><mml:math id="inf34"><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> can, in principle, account for a broad range of inter-individual differences in gambling behavior (and, possibly, in the neural encoding strength of prospective losses and gains). This raises the question: can the observed neural and behavioral differences between groups be explained by inter-individual differences in static value synthesis, without caring about self-organized plasticity?</p><p>To address this question, we fit the ANN model of value synthesis, with and without self-organized plasticity, to each participant’s series of gamble decisions (given the corresponding prospective gains and losses). In what follows, we refer to the models’ predictions about fitted behavioral data as models’ <italic>postdiction</italic>. We then perform counterfactual model simulations: for each subject-specific fitted model, we simulate the trial-by-trial gamble decisions that would have been observed, had this subject/model been exposed to the sequence of prospective gains and losses that each subject <italic>of the other group</italic> was exposed to (see Methods). That is, we ask what an ANN trained on the gambling decisions of a participant in the narrow gain range group would predict when exposed to the trial-by-trial series of gains and losses of participants from the wide gain range group (and reciprocally). These out-of-sample predictions provide a strong test of the model’s generalization ability. <xref ref-type="fig" rid="fig8">Figure 8</xref> below shows both postdiction and out-of-sample predictions of the two ANN model variants (static versus efficient value synthesis).</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Efficient value synthesis: Artificial neural network (ANN) analysis of behavioral data.</title><p>(<bold>A</bold>) The probability of gamble acceptance under plastic ANNs (y-axis) is plotted against gambles’ expected value (EV) (x-axis), in both groups (red: narrow range, blue: wide range). Dots show raw data (error bars depict s.em.). Plain and dashed lines show postdiction and out-of-sample predictions (see main text), respectively. The gray-shaded area highlights the range of expected values that is common to both groups. (<bold>B</bold>) Same as panel A, for static ANN. (<bold>C</bold>) Postdicted loss aversion (y-axis) is plotted as a function of time (x-axis) for both groups (same color code), under the efficient value synthesis model. (<bold>D</bold>) Same as panel C, for static value synthesis.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig8-v1.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Artificial neural network (ANNs’) postdiction error rate.</title><p>(<bold>A</bold>) The rate of postdiction error (y-axis) under the efficient value synthesis scenario is plotted against gambles’ expected value (x-axis), for both groups (red: narrow range, blue: wide range). (<bold>B</bold>) Static value synthesis, same format.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig8-figsupp1-v1.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>Self-organized plasticity parameter estimates.</title><p>(<bold>A</bold>) Empirical distribution of plasticity magnitude, for both groups (red: narrow range, blue: wide range). (<bold>B</bold>) Empirical distribution of plasticity rate. (<bold>C</bold>) Observed loss aversion (y-axis) is plotted as a function of plasticity magnitude (x-axis, log scale), for both groups. Each dot is a participant; plain lines depict the best-fitting linear trend. (<bold>D</bold>) Observed loss aversion as a function of plasticity rate (log scale).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig8-figsupp2-v1.tif"/></fig></fig-group><p>Unsurprisingly, both model <italic>postdictions</italic> accurately describe the qualitative group difference in gambling behavior. In addition, both candidate ANNs perform similarly to the logistic model, in terms of both percentage of explained variance (wide range group: 65.8±2.5%, narrow range group: 75.1±1.9%) and balanced fit accuracy (wide range group: 87.3±0.9%, narrow range group: 92.1±0.7%). We note that both ANN models yield postdiction error rates that are similar to the logistic model, in that they are maximal for hard decisions, i.e., when EV lies around zero (see <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). In addition, under the efficient value synthesis scenario, empirical distributions of self-organized plasticity parameter estimates are comparable across both groups of participants (see <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>). But did ANN models capture a mechanism that faithfully generalizes to different gain range contexts (i.e. across groups)? First, static ANNs do not yield accurate out-of-sample predictions. This is expected, because static ANNs cannot exhibit range adaptation. Thus, they leave gambling behavior unchanged within the common range of expected values, and simply extrapolate postdicted behavior outside that range (as is the case for the logistic model, see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). In other terms, within the range of expected values that is common to both groups, static ANNs wrongly predict that the gambling rate should be higher in the wide range group than in the narrow range group (mean gambling rate group difference=9.7%). The situation is quite different for plastic ANNs, which yield more accurate out-of-sample predictions of peoples’ risk attitudes within the common range of expected values. In particular, plastic ANNs correctly predict that gambling rate should be lower in the wide-range group than in the narrow-range group (mean gambling rate group difference=−6.2%, p=0.038, <italic>F</italic>=1.8). We then measured the absolute out-of-sample prediction error of both plastic and static ANN models, for both participant groups. We found that it was significantly greater for static than for plastic ANN models (wide range group: p=0.013, <italic>F</italic>=9.45, narrow range group: p=0.032, <italic>F</italic>=6.43).</p><p>We also quantified postdicted loss aversion dynamics under both types of models (<xref ref-type="fig" rid="fig8">Figure 8C and D</xref>). One can see that ANNs that operate efficient value synthesis do exhibit realistic loss aversion dynamics, whereby both groups are initially comparable and then progressively spread apart as time unfolds (and the impact of the range of prospective gains accumulates). Note that this systematic dynamical change in peoples’ behavior is the information that plastic ANNs exploit to calibrate both the magnitude and the rate of self-organized plasticity, which reacts to the past history of prospective gains and losses. This does not hold, however, for ANNs that operate static value synthesis, which overlook dynamical changes and attempt to explain gambling choices in terms of an idiosyncratic value landscape. Note that, under the efficient value synthesis scenario, the dynamics of self-organized plasticity are determined by magnitude (<inline-formula><mml:math id="inf35"><mml:mi>α</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) and rate (<inline-formula><mml:math id="inf36"><mml:mi>β</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) parameters. Accordingly, inter-individual differences in fitted plasticity magnitudes -but nor plasticity rates- significantly correlate with inter-individual differences in behavioral loss aversion indices (narrow range: p=0.024, wide range: p&lt;10<sup>–3</sup>, see <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>). Taken together, these results suggest that the self-organized plasticity mechanism in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> is necessary to capture the context-dependency of peoples’ loss aversion.</p><p>We now aim to evaluating the neurophysiological validity of fitted ANN models of value synthesis. To address this question, we ask whether the activity patterns in ANN models that were fitted to each participant’s gambling choices resemble the corresponding within-subject fMRI activity patterns in the OFC. We approach this problem using representational similarity analysis (RSA) within each subregion of the OFC. This allows us to compare the trial-by-trial multivariate activity patterns of candidate ANNs with those of fMRI signals in the OFC, without any additional ANN parameter adjustment. In brief, we compute four types of within-subject Representational Dissimilarity Matrices or RDMs (see Methods): (i) full trial-by-trial RDMs, (ii) gain-RDMs, where trials have been binned according to prospective gains, (iii) loss-RDMs, and (iv) EV-RDMs. We then measure the correlation between ANN-based and fMRI-based RDMs, for each OFC subregion and each participant. We then test for the statistical significance of this correlation within each group of participants (one-sample t-tests on Fisher-transformed within-subject correlations). <xref ref-type="fig" rid="fig9">Figure 9</xref> below summarizes the RSA results in terms of the group-average RDM correlations, for both plastic and static ANNs.</p><fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Efficient value synthesis: Representational similarity analysis (RSA) analysis results.</title><p>Within each panel, the correlation between artificial neural network (ANN)-based and fMRI-based representational dissimilarity matrices (RDMs) (y-axis) is shown for both groups of participants (red: narrow range group, blue: wide range group), and both ANN variants (left: plastic ANN, right: static ANN). Errorbars depict within-group s.e.m., and p-values are uncorrected for multiple comparisons. Each column shows the representational similarity analysis (RSA) results of a given OFC subregion from left to right: BA13 (medial), BA11 (lateral), BA14 (rostral), BA11 (medial), and BA32 (rostral). Each row shows one type of RDM (from top to bottom: trial-by-trial RDMs, gain-RDMs, loss-RDMs, and expected value (EV)-RDMs).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig9-v1.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 1.</label><caption><title>Response profile diversity in HP-artificial neural networks (ANNs’) integration units.</title><p>The average proportion of HP-ANNs integration units (y-data) that shows a significant correlation with choice, chosen value, and offer value (or none of these) is plotted for both groups (blue: wide range, red: narrow range). Error bars depict s.e.m. (across participants). Horizontal gray bars show the normalized frequency of ‘offer value,’ ‘chosen value,’ and ‘choice’ cells detected at OFC neurons’ response peak (see Figure 4 in <xref ref-type="bibr" rid="bib52">Padoa-Schioppa and Assad, 2006</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig9-figsupp1-v1.tif"/></fig></fig-group><p>Intriguingly, both plastic and static ANN variants yield trial-by-trial activity patterns that significantly correlate with fMRI activity patterns in almost all OFC subregions (upper panels in <xref ref-type="fig" rid="fig9">Figure 9</xref>). This suggests that raw fMRI estimates of trial-by-trial dynamics of neural activity are not reliable enough to reveal the functional segregation of OFC subregions. This is not the case, however, when considering gain/loss/EV-RDMs. In brief, irrespective of the ANN model variant, no OFC subregion reaches statistical significance in both groups, for all types of RDM. Nevertheless, the lateral part of Brodmann area 11 almost meets this criterion, in that all RSA analyses are significant except for loss-RDMs in the narrow range group of participants, for both ANN variants (plastic ANN: p=0.22, static ANN: p=0.20). Given the anatomical specificity of this result, this is strong evidence that ANNs that operate value synthesis (whether plastic or static) provide a reasonably realistic prediction of the representational geometry of OFC neurons within the lateral part of Brodmann area 11. We note that, irrespective of the type of RDM considered, nowhere in the OFC is the comparison between the two model variants statistically significant.</p><p>Interestingly, ANNs that operate efficient value synthesis also reproduce other known features of value-coding neurons in the OFC. Recall that OFC neurons are notoriously diverse in their response profile, but a consistent finding is that, in the context of value-based decision-making, they can be classified in terms of so-called ‘choice cells,’ ‘chosen value cells,’ and ‘offer value cells’ (<xref ref-type="bibr" rid="bib52">Padoa-Schioppa and Assad, 2006</xref>; <xref ref-type="bibr" rid="bib53">Padoa-Schioppa and Assad, 2008</xref>). Given that this can be considered a pre-requisite for any computational model of value coding in the OFC, we asked whether plastic ANNs reproduce this known property of OFC neurons. For each subject, we thus tested whether the response of integration units correlates (across trials) with choice, chosen value, and/or gamble value, where value is defined as the weighted sum of gains and losses (according to the static logistic model parameter estimates). The results of this analysis are shown on <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>: in brief, plastic ANNs do exhibit this type of apparent coding variability, and predicted category proportions are qualitatively comparable to those reported in the existing literature. This provides additional neurobiological validity to ANN models of efficient value synthesis in the OFC.</p><p>Finally, we show that other adaptation models (in particular: efficient coding at the level of decision attributes) cannot explain neural data on value range adaptation. This is summarized in <xref ref-type="fig" rid="fig10">Figure 10</xref> below. Although they predict qualitatively similar behavioral range adaptation effects (see <xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1</xref>), they do not predict value range adaptation in the ANN’s integration layer (<xref ref-type="fig" rid="fig10">Figure 10AB</xref>). They also predict that increasing the range of prospective gains should <italic>increase</italic> the neural encoding strengths of gains within the integration layer (<xref ref-type="fig" rid="fig10">Figure 10C</xref>), which is at odds with the empirical data that we report here (<xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>). Finally, when fitted on participants’ behavioral choices, they do not generalize well across gain range contexts (<xref ref-type="fig" rid="fig10s2">Figure 10—figure supplement 2</xref>), and their RSA results are less convincing (even in the lateral part of Brodman area 11, see <xref ref-type="fig" rid="fig10s3">Figure 10—figure supplement 3</xref>). The mathematical derivation of such models, as well as the analysis of their predictions, are summarized in the Supplementary material.</p><fig-group><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Main divergent predictions of the <italic>efficient coding of attributes</italic> scenario.</title><p>(<bold>A</bold>) The average units’ response (y-axis) to pairs of prospective gains and losses that fall within predefined expected value (EV) bins (x-axis) is shown, while ANNs that operate efficient value synthesis are exposed to four different spanned gain/loss domains (blue: narrow ranges of gains and losses, violet: wide ranges of gains and losses, red: narrow gain range and wide loss range, yellow: wide gain range and narrow loss range, see main text). Only integration units that correlate positively with EV are shown. (<bold>B</bold>) Integration units that correlate negatively with EV, same format as panel (<bold>A</bold>). (<bold>C</bold>) The neural encoding strength of gains (color code: from blue -low encoding strength- to yellow -high encoding strength-) is shown as a function of the spanned range of losses (x-axis, range increases from left to right) and gains (y-axis, range increases from bottom to top). (<bold>D</bold>) Neural encoding strength of losses, same format.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig10-v1.tif"/></fig><fig id="fig10s1" position="float" specific-use="child-fig"><label>Figure 10—figure supplement 1.</label><caption><title>Efficient coding of attributes: impact of spanned ranges of gains and losses.</title><p>(<bold>A</bold>) The neural encoding strength of gains (color code: from blue -minimal encoding strength- to yellow -maximal encoding strength-) is shown as a function of the spanned range of losses (x-axis, range increases from left to right) and gains (y-axis, range increases from bottom to top). (<bold>B</bold>) Neural encoding strength of losses, same format. (<bold>C</bold>) Neural encoding strength of expected value (EV), same format. (<bold>D</bold>) Behavioral sensitivity to gains, same format. (<bold>E</bold>) Behavioral sensitivity to losses, same format. (<bold>F</bold>) Behavioral loss aversion, same format.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig10-figsupp1-v1.tif"/></fig><fig id="fig10s2" position="float" specific-use="child-fig"><label>Figure 10—figure supplement 2.</label><caption><title>Efficient coding of attributes: Artificial neural network (ANN) analysis of behavioral data.</title><p>(<bold>A</bold>) The probability of gamble acceptance (y-axis) is plotted against the gambles’ expected value (EV) (x-axis), in both groups (red: narrow range, blue: wide range). Dots show raw data (error bars depict s.em.). Plain and dashed lines show postdictions and out-of-sample predictions (see main text), respectively. The gray-shaded area highlights the range of expected values that is common to both groups. (<bold>B</bold>) Postdicted loss aversion (y-axis) is plotted as a function of time (x-axis) for both groups (color code).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig10-figsupp2-v1.tif"/></fig><fig id="fig10s3" position="float" specific-use="child-fig"><label>Figure 10—figure supplement 3.</label><caption><title>Efficient coding of attributes: Representational similarity analysis (RSA) analysis results.</title><p>Within each panel, the correlation between artificial neural network (ANN)-based and fMRI-based representational dissimilarity matrices (RDMs) (y-axis) is shown for both groups of participants (red: narrow range, blue: wide range). Error bars depict within-group s.e.m. Each column shows the representational similarity analysis (RSA) results of a given OFC subregion from left to right: BA13 (medial), BA11 (lateral), BA14 (rostral), BA11 (medial), and BA32 (rostral). Each row shows one type of RDM (from top to bottom: full-RDMs, gain-RDMs, loss-RDMs, and expected value (EV)-RDMs).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig10-figsupp3-v1.tif"/></fig></fig-group><p>Taken together these behavioral and neural analyses provide converging evidence that self-organized plasticity that operates efficient value synthesis in the lateral part of BA11 is a likely explanation for range adaptation of loss aversion.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this work, we investigate the neural range adaptation mechanism in OFC neurons that underlies the irrational context-dependency of value-based decisions. We focus on risky decisions, where value needs to be constructed out of primitive decision attributes (here: prospective gains and losses). This eventually disambiguates the neural and behavioral implications of candidate computational scenarios for range adaptation. We show that a specific form of self-organized plasticity between attribute-specific and attribute-integration neurons best predicts (out-of-sample) both context-dependent behavioral biases and range adaptation in OFC neurons.</p><p>The processing of reward signals in OFC neurons is known to exhibit range adaptation (<xref ref-type="bibr" rid="bib14">Conen and Padoa-Schioppa, 2019</xref>; <xref ref-type="bibr" rid="bib42">Louie and Glimcher, 2012</xref>; <xref ref-type="bibr" rid="bib54">Padoa-Schioppa, 2009</xref>; <xref ref-type="bibr" rid="bib66">Rangel and Clithero, 2012</xref>). The typical explanation is that OFC neurons adapt their output firing properties to match the recent history of values (<xref ref-type="bibr" rid="bib58">Polanía et al., 2019</xref>). Implicit in this reasoning is the idea that OFC neurons are receiving value signals, which they are transmitting to downstream decision systems (<xref ref-type="bibr" rid="bib55">Padoa-Schioppa and Rustichini, 2014</xref>; <xref ref-type="bibr" rid="bib69">Rustichini et al., 2017</xref>). However, this assumption is at odds with the notion that OFC neurons are rather constructing value from input signals about decision-relevant attributes (<xref ref-type="bibr" rid="bib51">O’Doherty et al., 2021</xref>; <xref ref-type="bibr" rid="bib56">Pessiglione and Daunizeau, 2021</xref>; <xref ref-type="bibr" rid="bib62">Raghuraman and Padoa-Schioppa, 2014</xref>). An important contribution of this work is to show that such a value synthesis scenario is compatible with known value range adaptation effects in OFC neurons (<xref ref-type="fig" rid="fig2">Figure 2</xref>). In particular, our results suggest that value range adaptation may be the byproduct of self-organized plasticity that aims at mitigating information loss induced by limited neural response ranges. At the behavioral level, this scenario predicts that peoples’ sensitivity to decision attributes inversely scales with the range of each decision attribute. In the context of gamble decisions, this implies that loss aversion follows the ratio of spanned ranges of gain w.r.t. losses (<xref ref-type="fig" rid="fig3">Figure 3</xref>). This systematic dependence of peoples’ loss aversion on the relative ranges of spanned gains and losses has already been documented (<xref ref-type="bibr" rid="bib63">Rakow et al., 2020</xref>). However, when considering behavioral data alone, the interpretative power of this kind of experimental design is limited (<xref ref-type="bibr" rid="bib84">Williams et al., 2021</xref>).</p><p>In fact, the same behavioral pattern can be predicted under simpler efficient coding scenarios that operate at the level of attribute-specific layers (see the section on ‘efficient coding of gains and losses’ in the Supplementary materials). Interestingly, these models make neural predictions that are distinct from the efficient value synthesis scenario. In particular, ANNs that operate efficient coding of attributes do not exhibit value range adaptation effects in integration units (see <xref ref-type="fig" rid="fig10">Figure 10A, B</xref>). Also, they predict that the neural encoding strength of gains in integration units increases when the spanned range of gains increases (see <xref ref-type="fig" rid="fig10">Figure 10C</xref>). This clearly goes against the results of our model-free analyses of fMRI data in the OFC. The distinction between these two scenarios (i.e. efficient coding of attributes versus efficient value synthesis) is important, because it may confound the relationship between range adaptation in OFC neurons and its behavioral consequences. In this sense, our results complement and extend previous computational modeling studies that focus on the behavioural impact of range adaptation in attribute-specific units (<xref ref-type="bibr" rid="bib72">Soltani et al., 2012</xref>). In principle, the two mechanisms may coexist. Importantly, however, range adaptation within the attribute layers does not obviate the need for range adaptation within the integration layer. This is simply because each integration unit receives an arbitrary mixture of inputs. More generally, within a hierarchical system relying on units equipped with saturating activation functions, efficient information processing requires range adaptation at all levels of the hierarchy. Having said this, many other candidate neural mechanisms may, in principle, compete or interact with the self-organized plasticity that we have considered here, eventually crystalizing or destabilizing plastic changes. This is the case for, e.g., Hebbian and homeostatic plasticities, which are known to induce slow neural hysteretic effects (<xref ref-type="bibr" rid="bib25">Fox and Stryker, 2017</xref>; <xref ref-type="bibr" rid="bib57">Pezzulo et al., 2015</xref>; <xref ref-type="bibr" rid="bib77">Toyoizumi et al., 2014</xref>; <xref ref-type="bibr" rid="bib80">Turrigiano, 2017</xref>). Recent theoretical arguments also suggest that flexible attribute integration in OFC neurons may necessitate plastic changes in the synaptic gain of upstream attribute-specific neurons (<xref ref-type="bibr" rid="bib51">O’Doherty et al., 2021</xref>). More precisely, the wiring between attribute-specific and attribute-integration neurons should self-organize according to the contextual relevance of attributes. The extent to which the properties of these or similar kinds of neurophysiological mechanisms may explain contextual dependence and/or irrational behavioral responses is an open and challenging issue.</p><p>At the neural level, it is reassuring to see that fMRI patterns of activity in the lateral part of Brodmann area 11 strongly resemble the quantitative predictions of plastic ANN models. One might find it disappointing that these predictions turn out not to be verified in Brodmann area 32, owing to the known value encoding within the ventromedial PFC (see, e.g. <xref ref-type="bibr" rid="bib13">Clairis and Pessiglione, 2022</xref>; <xref ref-type="bibr" rid="bib41">Lopez-Persem et al., 2020</xref>). In fact, there is an ongoing debate regarding the relative contribution of OFC subregions w.r.t. value processing. For example, lateral, but not medial, OFC may host representations of attributes that presumably compose value judgements (<xref ref-type="bibr" rid="bib75">Suzuki et al., 2017</xref>). Although this clearly aligns with our model-free fMRI data analysis results, we do not claim that the evidence we provide regarding the anatomical location of value synthesis generalizes beyond decision contexts that probe peoples’ loss aversion. In fact, our main claim is about whether and how efficient value synthesis operates within the OFC, as opposed to which specific subregion of the OFC drives the adaptation of loss aversion and/or related behavioral processes.</p><p>Having said this, we note that ANN integration units do exhibit response profiles that are reminiscent of typical OFC neurons electrophysiological activity during value-based decision-making. For example, they reproduce the diversity of coding that has been repeatedly observed in OFC neurons during value-based decision-making (‘offer value cells,’ ‘chosen value cells,’ and ‘choice cells;’ <xref ref-type="bibr" rid="bib52">Padoa-Schioppa and Assad, 2006</xref>, <xref ref-type="bibr" rid="bib53">Padoa-Schioppa and Assad, 2008</xref>). This is summarized in <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>. We see this as a non-specific byproduct of the mixed selectivity of integration units, which exhibit arbitrary complex and idiosyncratic receptive fields (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplements 1</xref> and <xref ref-type="fig" rid="fig1s3">3</xref>). More importantly, integration units also exhibit the known properties of value range adaptation in these same neurons (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Intriguingly, however, value range adaptation in ‘offer value cells’ had been observed without any significant behavioral preference change. Under the assumption that preferences between offers derive from the direct comparison of output signals from distinct ‘offer value cells,’ this is surprising. To solve this puzzle, later theoretical work proposed that value range adaptation is somehow ‘undone’ downstream value coding in the OFC (<xref ref-type="bibr" rid="bib55">Padoa-Schioppa and Rustichini, 2014</xref>). In our context, this would suggest that readout weights (<inline-formula><mml:math id="inf37"><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) would compensate for value-related adaptation, effectively thwarting the behavioral consequences of self-organized plasticity between attribute and integration layers. However, this reasoning critically relies upon the assumed computational role of ‘offer value cells.’ In fact, this puzzle may effectively dissolve under other scenarios of how ‘offer value cells’ contribute to decision-making. Recall that this null result was obtained in a decision context where choice options were characterized in terms of the type of offer (i.e. juices that differ w.r.t. palatability), whose quantity was systematically varied. Here, value synthesis would effectively aggregate two attributes, namely palatability and quantity. Under this view, ‘offer value cells’ simply are integration units that show a certain form of mixed selectivity, whereby units’ sensitivity to quantity strongly depends upon palatability. At this point, one needs to consider candidate scenarios of how the OFC may operate value synthesis for multiple options in a choice set. A possibility is that the OFC is automatically computing the value of the option that is currently under the attentional focus (<xref ref-type="bibr" rid="bib39">Lebreton et al., 2009</xref>; <xref ref-type="bibr" rid="bib41">Lopez-Persem et al., 2020</xref>), while storing the value of previously attended options within an orthogonal population code (<xref ref-type="bibr" rid="bib56">Pessiglione and Daunizeau, 2021</xref>). In principle, this implies that the OFC is wired such that it can handle arbitrary switches in attentional locus without compromising the integration of option-specific attributes. In this scenario, integration units (including those that look like ‘offer value cells’) would adapt to the range of all incoming attribute signals, irrespective of which option in the choice set is currently attended. In turn, ‘offer value cells’ would look like they are only partially adapting to the value range of a given offer type (<xref ref-type="bibr" rid="bib8">Burke et al., 2016</xref>; <xref ref-type="bibr" rid="bib14">Conen and Padoa-Schioppa, 2019</xref>). More importantly, to the extent that between-attribute spillover effects are negligible, changes in the range of offer quantities would distort the readout value profile along the quantity dimension without altering the palatability dimension. This would effectively leave the relative preference between offer types unchanged. Of course, this is only one candidate scenario among many. Nevertheless, we would still argue that the behavioral consequences of range adaptation in ‘offer value cells’ actually depend upon their underlying computational role.</p><p>Now, whether this sort of ANN model produces ‘realistic’ electrophysiological activity profiles beyond this kind of empirical observation is questionable. The reason is twofold. First, they are agnostic w.r.t. within-trial temporal dynamics. Second, there is some level of arbitrariness in the modeling assumptions (e.g. ANN structural constraints) that cannot be finessed using either behavioral or neuroimaging data. What we argue is robust in these ANN models is the information content that they carry, which is distributed over the activity profiles of their artificial neural unit layers. This is the main reason why we resort to variants of RSA analyses for comparing their predictions to multivariate fMRI activity patterns.</p><p>At this point, let us comment on a seemingly innocuous neural modeling assumption: namely, that units’ input-output activation functions are saturating. This was motivated by the fact that neurons’ firing rate cannot exceed some predefined physiological limit (see, e.g. <xref ref-type="bibr" rid="bib82">Wang et al., 2016</xref>). Under the framework of efficient coding, such response range limitation is eventually what creates the need for range adaptation. This is because information loss mostly follows from inputs reaching the saturating domain of units’ activation functions. However, one may wonder how robust our efficient value synthesis scenario to deviations from this assumption is. Analytical derivations show that other monotonic (and bounded) activation functions would yield very similar self-organized plasticity rules. This means that our results would generalize to any monotonic activation function. However, it turns out that efficient value synthesis yields unstable self-organized plasticity dynamics under non-monotonic (e.g. Gaussian or bell-shaped) activation functions. To understand this, recall that the self-organized plasticity rule derives from aligning the connectivity change with the gradient of information loss w.r.t. connection strengths. This gradient explodes when inputs fall within domains where the derivative of the activation function approaches zero. This unavoidably happens with non-monotonic activation functions because the plasticity mechanism eventually focuses the weighted inputs within the vicinity of their mode. In other terms, one may argue that only monotonic activation functions are compatible with the efficient value synthesis scenario.</p><p>Now, how generalizable is the neural mechanism we disclose here? We argue that self-organized plasticity may explain many forms of persistent irrational behavioral changes, through gradual range adaptation effects in OFC neurons. We note that, in our context, these changes seem to unfold over several minutes (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), which is consistent with the fastest time scale of long-term potentiation/depression (<xref ref-type="bibr" rid="bib1">Abraham, 2003</xref>). However, we contend that the evidence we provide here is insufficient to establish whether these changes remain stable over longer periods and whether they can be overcome by explicit instructions or intensive training (<xref ref-type="bibr" rid="bib12">Cicchini et al., 2012</xref>). A related issue is whether similar plasticity mechanisms may explain virtually instantaneous range adaptation in value-coding neurons (<xref ref-type="bibr" rid="bib44">Louie et al., 2015</xref>), eventually driving behavioral phenomena such as the framing effect. Here, we speculate that the framing of decisions may automatically trigger contextual expectations regarding expected gain and/or loss ranges, which may induce fast plastic changes within value-constructing networks through, e.g., short-term potentiation (<xref ref-type="bibr" rid="bib24">Fiebig and Lansner, 2017</xref>).</p><p>That the brain’s biology is to blame for all kinds of cognitive and/or behavioral flaws is not a novel idea (<xref ref-type="bibr" rid="bib9">Buschman et al., 2011</xref>; <xref ref-type="bibr" rid="bib45">Marois and Ivanoff, 2005</xref>; <xref ref-type="bibr" rid="bib48">Miller and Buschman, 2015</xref>; <xref ref-type="bibr" rid="bib64">Ramsey et al., 2004</xref>). However, providing neuroscientific evidence that a hard-wired biological constraint shapes and/or distorts the way the brain processes information is not an easy task. This is because whether the brain deviates from how it <italic>should</italic> process a piece of information is virtually unknown. This is particularly true for value-guided decision-making, which relates to subjective assessments of preferences rather than objective processing of decision-relevant evidence (<xref ref-type="bibr" rid="bib65">Rangel et al., 2008</xref>). Nevertheless, value-guided decision-making is known to exhibit many irrational biases, the neurocognitive explanations of which have been the focus of intense research over the past decades. From a methodological standpoint, our main contribution is to show how to leverage computational models (in particular: ANNs) to test hypotheses regarding neurophysiological mechanisms that may constrain or distort behaviorally-relevant information processing. On the one hand, we retain the simplicity of established ‘model-based’ fMRI approaches (<xref ref-type="bibr" rid="bib3">Borst et al., 2011</xref>; <xref ref-type="bibr" rid="bib50">O’Doherty et al., 2007</xref>), which proceed by cross-validating the identification of hidden computational determinants of behavior with neural data. On the other hand, our dual ANN/RSA approach enables us to quantify the statistical evidence for neurophysiological mechanisms that are difficult –if not impossible- to include in computational models that are defined at Marr’s <italic>algorithmic</italic> level (<xref ref-type="bibr" rid="bib47">McClamrock, 1991</xref>), e.g., normative models of behavior (as derived from, e.g. learning or decision theories) and/or cognitive extensions thereof. Self-organized plasticity between attribute-specific and attribute-integration units is a paradigmatic example of what we mean here. More generally, hard-wired biological mechanisms or constraints may not always be instrumental to the cognitive process of interest. In turn, it may be challenging to account for incidental biological disturbances of neural information processing, when described at the algorithmic level. A possibility here is to conceive of these disturbances as some form of random noise that perturbs cognitive computations (<xref ref-type="bibr" rid="bib20">Drugowitsch et al., 2016</xref>; <xref ref-type="bibr" rid="bib85">Wyart and Koechlin, 2016</xref>). In contrast, we rather suggest relying on computational models that solve a well-defined computational problem (here: constructing the gambles’ subjective value from prospective gains and losses) but operate at the neural level. Accounting for possibly incidental, biological constraints and/or hard-wired mechanisms then enables comparing quantitative/deterministic scenarios for sub-optimal disturbances of covert cognitive processes of interest.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Artificial neural network models of value synthesis</title><p>Artificial neural networks or ANNs decompose a possibly complex form of information processing in terms of a combination of very simple computations performed by connected ‘units,’ which are mathematical abstractions of neurons. Here, we take inspiration from a growing number of studies that use ANNs as mechanistic models of neural information processing (<xref ref-type="bibr" rid="bib28">Güçlü and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="bib33">Kietzmann et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Kietzmann et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">Kriegeskorte and Golan, 2019</xref>), with the added requirement that they eventually explain (possibly irrational) behavioural data.</p><p>In abstract terms, any decision can be thought of as a cognitive process that transforms some input information <inline-formula><mml:math id="inf38"><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></inline-formula> into a behavioral output response <inline-formula><mml:math id="inf39"><mml:mi>r</mml:mi></mml:math></inline-formula>. Here, participants have to accept or reject a risky gamble composed of a 50% chance of obtaining a gain G and a 50% chance of experiencing a loss L, i.e., <inline-formula><mml:math id="inf40"><mml:mi>u</mml:mi></mml:math></inline-formula> is composed of <inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> input attributes: <inline-formula><mml:math id="inf42"><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>. Under an ANN model of such decisions, people’s behavioral response is the output of a neural network that processes the attributes <inline-formula><mml:math id="inf43"><mml:mi>u</mml:mi></mml:math></inline-formula>, i.e.,: <inline-formula><mml:math id="inf44"><mml:mi>r</mml:mi><mml:mo>≈</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>ϑ</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, where <inline-formula><mml:math id="inf45"><mml:mi>ϑ</mml:mi></mml:math></inline-formula> are unknown ANN parameters and <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the ANN’s input-output transformation function. So-called ‘shallow’ ANNs effectively reduce <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> to a combination of neural units organized in a single hidden layer. In what follows, we rather rely on (moderately) deep ANNs with two hidden layers: namely, an attribute-specific layer (which is itself decomposed into gain-specific and loss-specific layers) and an integration layer (which receives inputs from both attribute-specific layers). The units that compose the latter then collectively determine gamble decisions by integrating prospective gains and losses.</p><p>We assume that each attribute <inline-formula><mml:math id="inf48"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> is encoded into the activity of neurons <inline-formula><mml:math id="inf49"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math></inline-formula> of its dedicated ‘attribute-specific layer,’ where <inline-formula><mml:math id="inf50"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of attribute-specific neurons per attribute. What we mean here is that the neuron <inline-formula><mml:math id="inf51"><mml:mi>j</mml:mi></mml:math></inline-formula> in the attribute-specific layer <inline-formula><mml:math id="inf52"><mml:mi>i</mml:mi></mml:math></inline-formula> responds to <inline-formula><mml:math id="inf53"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> as follows:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf54"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> is the activation function of neural units that compose the ANN’s attribute-specific layer:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="italic">e</mml:mi><mml:mi mathvariant="italic">x</mml:mi><mml:mi mathvariant="italic">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>μ</mml:mi><mml:mo>−</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>which yields a sigmoidal transform of inputs. Critically, such activation functions are bounded, i.e., we assume that neural units cannot fire beyond a certain rate. In neocortical neurons, the main mechanistic constraint that acts on firing rate is the action potential’s refractory period, which depends upon how long it takes ion channels to complete a whole voltage activate-deactivate cycle (about a few milliseconds). Typically, even fast-spiking neocortical neurons cannot fire at a frequency higher than about 500 or 600 Hz (<xref ref-type="bibr" rid="bib82">Wang et al., 2016</xref>). As we will see, this type of response saturation is a critical component of range adaptation.</p><p>The parameters <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> captures idiosyncratic properties of the neuron <inline-formula><mml:math id="inf56"><mml:mi>j</mml:mi></mml:math></inline-formula> in the input layer <inline-formula><mml:math id="inf57"><mml:mi>i</mml:mi></mml:math></inline-formula> (e.g. its firing rate threshold <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and the slope parameter <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>Collectively, the activity vector <inline-formula><mml:math id="inf60"><mml:msub><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> forms a multivariate representation of attribute <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> in the form of a population code (<xref ref-type="bibr" rid="bib21">Ebitz and Hayden, 2021</xref>). Then the output of the attribute-integration layers is passed to the ‘integration layer’ <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, i.e., the neuron <inline-formula><mml:math id="inf63"><mml:mi>k</mml:mi></mml:math></inline-formula> of the integration layer responds to <inline-formula><mml:math id="inf64"><mml:msubsup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:math></inline-formula> as follows:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>z</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>υ</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>υ</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the connection weight from the neuron <inline-formula><mml:math id="inf66"><mml:mi>j</mml:mi></mml:math></inline-formula> in the attribute-specific layer <inline-formula><mml:math id="inf67"><mml:mi>i</mml:mi></mml:math></inline-formula> to the neuron <inline-formula><mml:math id="inf68"><mml:mi>k</mml:mi></mml:math></inline-formula> of the integration layer, <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> capture idiosyncratic properties of the integration neuron <inline-formula><mml:math id="inf70"><mml:mi>k</mml:mi></mml:math></inline-formula> (i.e. its firing rate threshold and slope), and <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>υ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> are the inputs of integration units.</p><p>Collectively, integration neurons form a representation of decision value in the form of a population code, i.e., the gambles’ subjective value <inline-formula><mml:math id="inf72"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> at the time or trial <inline-formula><mml:math id="inf73"><mml:mi>t</mml:mi></mml:math></inline-formula> is read out from the integration layer as follows:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are the population readout weights.</p><p>Taken together, <xref ref-type="disp-formula" rid="equ3 equ4 equ5 equ6">Equations 3–6</xref> define the end-to-end ANN’s transformation of prospective gains and losses into decision value <inline-formula><mml:math id="inf75"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>ϑ</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>ϑ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≜</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf76"><mml:mi>ϑ</mml:mi></mml:math></inline-formula> lumps all ANN parameters together, i.e.,: <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϑ</mml:mi><mml:mo>≜</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>υ</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. This is what we coin <italic>value synthesis</italic>. A schematic summary of the ANN’s double-layer structure is shown on panel A of <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p></sec><sec id="s4-2"><title>Efficient value synthesis and self-organized plasticity</title><p>We start with the premise that the brain system that integrates value-relevant option features (here: prospective gains and losses) to construct value signals may be doing this under neural noise, which degrades the information about value. In particular, the limited range of physiological responses of neural units that perform this integration induces some information loss on value signals. This is because, when inputs to integration units fall too far away from their firing threshold (say outside a <inline-formula><mml:math id="inf78"><mml:mo>±</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mi>σ</mml:mi></mml:math></inline-formula> range), activation functions saturate, i.e., they produce non-discriminable outputs (close to 0 or 1). In this context, <italic>efficient value synthesis</italic> refers to the idea that neural networks that perform the integration of prospective gains and losses to construct value may adapt their response properties to mitigate information loss, hence the ‘efficiency’ of value synthesis. We now sketch how <italic>efficient</italic> value synthesis can be achieved within ANNs whose 2-layer structure is described in <xref ref-type="disp-formula" rid="equ3 equ4 equ5 equ6">Equations 3-6</xref>.</p><p>In the presence of neural noise, the ANN’s readout value <inline-formula><mml:math id="inf79"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of a gamble made of a pair <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> of prospective gain and loss is given by (in lieu of <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>):<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msubsup><mml:mover><mml:mi>z</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mover><mml:mi>z</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf81"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> is some (uncontrollable) neural noise that competes with the ‘utile’ component <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> that is given in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>.</p><p><xref ref-type="disp-formula" rid="equ8">Equation 8</xref> can serve to measure the information loss <inline-formula><mml:math id="inf83"><mml:mi>I</mml:mi><mml:mi>L</mml:mi></mml:math></inline-formula> that is induced by neural noise under units’ limited response range:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>I</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>z</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>υ</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Equation 9 states that the information loss increases when the mutual information between the noisy responses of integration units and their “utile” (i.e. noiseless) component decreases. Here,  <inline-formula><mml:math id="inf84"><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> is Shannon’s mutual information, <inline-formula><mml:math id="inf85"><mml:mi>K</mml:mi></mml:math></inline-formula> is a constant, <inline-formula><mml:math id="inf86"><mml:mi>H</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> is Shannon’s entropy, and <inline-formula><mml:math id="inf87"><mml:mi>E</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> is the standard expectation operator. The right-hand term in <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> arises at the small noise limit (<xref ref-type="bibr" rid="bib49">Nadal, 1994</xref>), and the expectation is taken under the distribution of integration units’ inputs <inline-formula><mml:math id="inf88"><mml:mi>υ</mml:mi></mml:math></inline-formula>. The last term in <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> is simply the average steepness (in log space) of units’ activation functions. Importantly, <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> holds irrespective of the type of nonlinearity of ANN units’ activation functions.</p><p>The entropy <inline-formula><mml:math id="inf89"><mml:mi>H</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>υ</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> has no closed-form expression, but can be given a multivariate gaussian approximation, i.e.,: <inline-formula><mml:math id="inf90"><mml:mi>H</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>υ</mml:mi></mml:mrow></mml:mfenced><mml:mo>≈</mml:mo><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi>C</mml:mi><mml:mi>S</mml:mi><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mi>`</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf91"><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></inline-formula> is the covariance matrix of the output of the ANN’s first layer and <inline-formula><mml:math id="inf92"><mml:mi>K</mml:mi><mml:mi>`</mml:mi></mml:math></inline-formula> is a constant. In principle, this approximation works because, when the size of the network grows, the central limit theorem implies that the distribution of integration units’ inputs <inline-formula><mml:math id="inf93"><mml:mi>υ</mml:mi></mml:math></inline-formula> will tend towards normality. The robustness of this approximation has been established in the context of undercomplete ICA (<xref ref-type="bibr" rid="bib61">Porrill and Stone, 1998</xref>).</p><p>Efficient value synthesis can then be simply achieved by modifying the connectivity matrix <inline-formula><mml:math id="inf94"><mml:mi>C</mml:mi></mml:math></inline-formula> to decrease the information loss <inline-formula><mml:math id="inf95"><mml:mi>I</mml:mi><mml:mi>L</mml:mi></mml:math></inline-formula>, i.e., along the direction of the information loss gradient:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>I</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>α</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>H</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>υ</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:mfrac><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf96"><mml:mi>α</mml:mi></mml:math></inline-formula> controls the magnitude of the gradient-following step and the first term in the right-hand side of <xref ref-type="disp-formula" rid="equ10">Equation 10</xref> is given by:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>υ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:mfrac><mml:mo>≈</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>S</mml:mi><mml:msup><mml:mi>C</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>C</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:munder></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The matrix multiplier in the right-hand side of <xref ref-type="disp-formula" rid="equ11">Equation 11</xref> is non-local, i.e., the gradient <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>H</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>υ</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> depends upon all connection weights in the network. This is unrealistic for biological systems, and we thus drop this term in the remainder of this manuscript. In turn, <xref ref-type="disp-formula" rid="equ11">Equation 11</xref> can be approximated as a collection of local changes to the connectivity matrix:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>≈</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We will see that <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> only involves the output response <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> of the pair of attribute and integration units that are connected through <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> implies that efficient integration will tend to change the distribution of inputs <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> to each integration unit such that they span the range where the steepness of its activation function is maximal. Focusing inputs to the responsive range of integration units’ activation functions then maximizes the output variability induced by attribute inputs. This makes sense, since this is expected to yield maximal contrast over the response outputs of integration units.</p><p>But <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> still requires a last modification to derive a realistic self-organized plasticity rule for efficient value synthesis. This is because self-organized plasticity is a dynamical process, which reacts to recent network activity, as trials and/or time unfolds.</p><p>Note that the expectation in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> is taken under the distribution of prospective gains and losses, and can, therefore, be defined as a sample average over trial-by-trial gamble attributes. If the underlying distribution is non-stationary, then <inline-formula><mml:math id="inf102"><mml:mi>E</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mo>∙</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> can be estimated at trial or time <inline-formula><mml:math id="inf103"><mml:mi>t</mml:mi></mml:math></inline-formula> using a simple weighted moving average operator <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mo>⋅</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msup><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mtext> </mml:mtext><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf105"><mml:mi>β</mml:mi></mml:math></inline-formula> (note: <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>β</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) controls the exponential decay of past samples’ weights in the moving average operator.</p><p>Let <inline-formula><mml:math id="inf107"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> be the change of connectivity at trial or time <inline-formula><mml:math id="inf108"><mml:mi>t</mml:mi></mml:math></inline-formula>. Replacing the expectation in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> with the moving average operator <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mo>⋅</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> now yields:<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>≈</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mi>β</mml:mi><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>υ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the local gradient can be written as:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Under sigmoidal activation functions, then:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>f</mml:mi><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>υ</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>υ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>f</mml:mi><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Replacing <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> into <xref ref-type="disp-formula" rid="equ14 equ15">Equations 14-15</xref> then yields:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mi>β</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>which only depends upon the output response of connected pairs of attribute-specific and attribute-integration units.</p><p>Note that accounting for the nonlocal component of <xref ref-type="disp-formula" rid="equ10 equ11">Equations 10-11</xref> would require inserting the correction term <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>υ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi>υ</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>υ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in the right-hand side of <xref ref-type="disp-formula" rid="equ17">Equation 17</xref>. In our experience, its magnitude is typically small when compared to the Hebbian term in <xref ref-type="disp-formula" rid="equ17">Equation 17</xref>. In turn, this term can be neglected without altering the main properties of efficient value synthesis.</p><p><xref ref-type="disp-formula" rid="equ17">Equation 17</xref> states that efficient value synthesis can be operated by local, history-dependent, <italic>self-organized plasticity</italic> within the network. The plasticity in <xref ref-type="disp-formula" rid="equ17">Equation 17</xref> is ‘self-organized’ in the sense that it does not require any teaching or feedback signal. In this context, <inline-formula><mml:math id="inf111"><mml:mi>β</mml:mi></mml:math></inline-formula> determines the adaptation rate of the network’s connectivity to changes in the distribution of prospective gains and losses. Importantly, the anti-Hebbian component of self-organized plasticity generalizes to any nonlinear activation function that is continuous and monotonically increasing. This is not the case, however, for non-monotonic activation functions (e.g. pseudo-gaussian activation functions).</p><p>In summary, as long as ANN units have monotonically increasing activation functions, efficient value synthesis can be implemented through some self-organized plasticity rule of the form given in <xref ref-type="disp-formula" rid="equ17">Equation 17</xref>. It turns out that the self-organized plasticity rule in <xref ref-type="disp-formula" rid="equ17">Equation 17</xref> essentially modifies the integration units’ receptive fields, i.e., their pattern of response to a given pair of prospective gain and loss. This has two main consequences: it changes the information content of the network, and it distorts the readout value. We unpack these two phenomena using numerical simulations, which we report in the two first section of the Supplementary materials. At this point, we simply note that the effect of self-organized plasticity on both the readout value profile and the information content within the integration layer depends upon the ranges of prospective gains and losses that the ANN is exposed to. The neural and behavioral impacts of the shape of the spanned domain of gains and losses are summarized in <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref> of the Results section.</p></sec><sec id="s4-3"><title>Behavioral and fMRI data: Experimental paradigm</title><p>In this work, we perform a re-analysis of the NARPS dataset (<xref ref-type="bibr" rid="bib4">Botvinik-Nezer et al., 2019</xref>; <xref ref-type="bibr" rid="bib5">Botvinik-Nezer et al., 2020a</xref>), openly available on <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/">https://openneuro.org/</ext-link>; <xref ref-type="bibr" rid="bib59">Poldrack et al., 2013</xref>. This dataset includes two studies, each of which is composed of a group of 54 participants who make a series of decisions made of 256 risky gambles. On each trial, a gamble was presented, entailing a 50/50 chance of gaining an amount G of money or losing an amount L. As in <xref ref-type="bibr" rid="bib76">Tom et al., 2007</xref>, participants were asked to evaluate whether or not they would like to play each of the gambles presented to them (strongly accept, weakly accept, weakly reject, or strongly reject). They were told that, at the end of the experiment, four trials would be selected at random: for those trials in which they had accepted the corresponding gamble, the outcome would be decided with a coin toss and for the other ones -if any- the gamble would not be played. In the first study (hereafter: ‘narrow range’ group), participants decided on gambles made of gain and loss levels that were sampled from within the same range (G and L varied between 5 and 20 $). In the second study (hereafter: the ‘wide range’ group), gain levels scaled to double the loss levels (L varied between 5 and 20$, and G varied between 10 and 40$). In both studies, all 16×16=256 possible combinations of gains and losses were presented across trials, which were separated by 7 s on average with some random jitter (min 6, max 10).</p><p>MRI scanning was performed on a 3T Siemens Prisma scanner. High-resolution T1-weighted structural images were acquired using a magnetization-prepared rapid gradient echo (MPRAGE) pulse sequence with the following parameters: TR=2530ms, TE=2.99ms, FA=7, FOV=224 × 224 mm, resolution=1 × 1 × 1 mm. Whole-brain fMRI data were acquired using echo-planar imaging with a multi-band acceleration factor of 4 and parallel imaging factor (iPAT) of 2, TR=1000 ms, TE=30 ms, flip angle=68 degrees, in plane resolution of 2 × 2 mm 30 degrees of the anterior commissure-posterior commissure line to reduce the frontal signal dropout, with a slice thickness of 2 mm and a gap of 0.4 mm between slices to cover the entire brain. See <ext-link ext-link-type="uri" xlink:href="https://www.narps.info/analysis.html#protocol">https://www.narps.info/analysis.html#protocol</ext-link> for more details.</p></sec><sec id="s4-4"><title>Extraction of trial-by-trial BOLD responses within OFC subregions</title><p>In the results Section, we focus on five subregions of the OFC: namely, the lateral and medial parts of Brodmann area 11, Brodmann area 13, Brodmann area 14, and the subgenual part of Brodmann area 32. This parcellation is based on anatomical masks in standard MNI coordinates obtained from the BRAINNETOME atlas (<ext-link ext-link-type="uri" xlink:href="https://atlas.brainnetome.org/">https://atlas.brainnetome.org/</ext-link>, <xref ref-type="bibr" rid="bib23">Fan et al., 2016</xref>). As can be seen in <xref ref-type="fig" rid="fig11">Figure 11</xref> below, these areas tile the entire OFC, except its most rostro-lateral part (which is Brodmann area 12).</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Anatomical masks of OFC subregions.</title><p>Pink: medial part of BA11, yellow: lateral part of BA11, green: A13, blue: BA14, red: BA32. Dark and light colors correspond to left and right hemispheric analogous regions, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80979-fig11-v1.tif"/></fig><p>The standard MNI coordinates of each subregion barycenter are given in <xref ref-type="table" rid="table1">Table 1</xref> below.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Anatomical coordinates of OFC subregions’ barycenter.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Anatomical region</th><th align="left" valign="top">Barycenter coordinates (left)</th><th align="left" valign="top">Barycenter coordinates (right)</th></tr></thead><tbody><tr><td align="left" valign="top">Medial part of BA11</td><td align="char" char="." valign="top">68,166,125</td><td align="char" char="." valign="top">114,164,126</td></tr><tr><td align="left" valign="top">Lateral part of BA11</td><td align="char" char="." valign="top">81,146,127</td><td align="char" char="." valign="top">100,149,127</td></tr><tr><td align="left" valign="top">BA13</td><td align="char" char="." valign="top">81,146,127</td><td align="char" char="." valign="top">100,149,127</td></tr><tr><td align="left" valign="top">BA14</td><td align="char" char="." valign="top">84,182,114</td><td align="char" char="." valign="top">97,176,114</td></tr><tr><td align="left" valign="top">Subgenual part of BA32</td><td align="char" char="." valign="top">87,167,110</td><td align="char" char="." valign="top">96,169,101</td></tr></tbody></table></table-wrap><p>To balance the statistical power across OFC subregions, we then removed the voxels that fall outside a 200-voxel sphere centered on the barycenter of the masks. This procedure yielded spherical ROIs with similar sizes across all ROIs.</p><p>FMRI data were preprocessed using SPM (<ext-link ext-link-type="uri" xlink:href="https://www.fil.ion.ucl.ac.uk/spm/">https://www.fil.ion.ucl.ac.uk/spm/</ext-link>), following standard realignment and movement correction guidelines. Note that we excluded five participants from the narrow range group because the misalignment between functional and anatomical scans could not be corrected. In each ROI, we regressed trial-by-trial activations with SPM through a GLM that included one stick regressor for each trial (at the time of the gamble presentation onset), which was convolved with the canonical HRF. To account for variations in hemodynamic delays, we added the basis function set induced by the HRF temporal derivative. To correct for movement artifacts, we also included the six head movement regressors and their squared values as covariates of no interest. We then extracted the 256 trial-wise regression coefficients in each voxel of each ROI. Next, we removed potential between-session confounding effects by projecting the ensuing trial series onto the null space of a categorical session-encoding design matrix. This effectively provided a BOLD trial series <inline-formula><mml:math id="inf112"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> that are deconvolved from the hemodynamic response function (<xref ref-type="bibr" rid="bib16">Dale, 1999</xref>) and corrected for standard confounding effects. No spatial smoothing was applied to preserve information buried in spatial fMRI activity patterns. Finally, we concatenated the corrected multivariate fMRI activity patterns of left and right analogous ROIs, eventually yielding five OFC subregions.</p></sec><sec id="s4-5"><title>Model-free analysis of behavioral data</title><p>First, we describe peoples’ behavior in terms of the probability of gambling given the gamble’s expected value EV=0.5*(G-L), where G and L are the gamble’s prospective gain and loss, respectively. For each participant, we binned trials according to deciles of EV, and measured the rate of gamble acceptance (<xref ref-type="fig" rid="fig2">Figure 2</xref>, upper-left panel).</p><p>Second, we regressed peoples’ decision to gamble onto gains and losses. Within each participant, we fit the following logistic regression model: <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">w</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">G</mml:mi></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where g<sub>i</sub> is the binary gamble decision at trial <italic>i</italic>, G<sub>i</sub> and L<sub>i</sub> are the prospective gain and loss of trial <italic>i</italic>, w<sub>0</sub> is the intercept or gambling bias, w<sub>G</sub> and w<sub>L</sub> are the sensitivity to gains and losses, respectively, and s(.) is the standard sigmoid mapping. Note that logistic model parameter estimates can be recombined to measure peoples’ loss aversion (log(w<sub>L</sub>/w<sub>G</sub>)). We then report within-subject parameter estimates at the group-level for random effect analyses (see <xref ref-type="fig" rid="fig3">Figure 3</xref> of the Results section). The logistic model can also be used to perform counterfactual model simulations. For each subject, we use the corresponding fitted parameters to evaluate the trial-by-trial probability of gamble acceptance that would have been observed, had this subject/model been exposed to the sequence of prospective gains and losses that each subject <italic>of the other group</italic> was exposed to. It turns out that such out-of-sample predictions of peoples’ behaviour are (expectedly) inaccurate. More precisely, such logistic regression cannot predict the observed group-difference in peoples’ gambling rate (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>Third, we performed a sliding window analysis: decisions were first partitioned into chunks of 16 consecutive trials each, which were then regressed against corresponding gains and losses using the same logistic model as above. From this, we obtain a set of logistic parameter estimates (intercept and sensitivity to gains and losses) per temporal window, per subject. Temporal changes in the ensuing loss aversion index can thus be followed as time unfolds (<xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p></sec><sec id="s4-6"><title>RSA</title><p>Each ANN model of value synthesis makes specific trial-by-trial predictions of activity patterns within the integration layer that can be compared to multivariate fMRI signals in each OFC subregion. This enables us to evaluate the neurophysiological validity of candidate models. Here, we have chosen to rely on representational similarity analysis or RSA (<xref ref-type="bibr" rid="bib19">Diedrichsen and Kriegeskorte, 2017</xref>; <xref ref-type="bibr" rid="bib27">Friston et al., 2019</xref>; <xref ref-type="bibr" rid="bib36">Kriegeskorte et al., 2008</xref>). In brief, RSA consists of evaluating the statistical resemblance between model-based and data-based <italic>representational dissimilarity matrices</italic> or RDMs, which we derive as follows. Let <inline-formula><mml:math id="inf114"><mml:mi>Y</mml:mi></mml:math></inline-formula> be the <inline-formula><mml:math id="inf115"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> multivariate time series of (modeled or empirical) neural activity, where <inline-formula><mml:math id="inf116"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf117"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the number of units and trials, respectively. Note that, for model predictions, ‘units’ mean artificial elementary units in ANNs, whereas they mean voxels in a given ROI for fMRI data. First, we derive the <inline-formula><mml:math id="inf118"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> raw RDM <inline-formula><mml:math id="inf119"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi>`</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math></inline-formula>, where the matrix element <inline-formula><mml:math id="inf120"><mml:msubsup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi>`</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> measures the dissimilarity of neural patterns of activity between trial <inline-formula><mml:math id="inf121"><mml:mi>t</mml:mi></mml:math></inline-formula> and trial <inline-formula><mml:math id="inf122"><mml:mi>t</mml:mi><mml:mi>`</mml:mi></mml:math></inline-formula>: <inline-formula><mml:math id="inf123"><mml:msubsup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi>`</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>`</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula>. By construction, these RDMs are invariant to affine transformations of activity patterns. In particular, this implies that the ensuing RSA are orthogonal to univariate analyses that rely on the mean activity within OFC subregions.</p><p>Second, we correct the raw RDM for autocorrelation confounds. To do this, we remove the average neural dissimilarity for each possible delay between trial pairs from the raw RDM. Note that this correction does not confound the existing relationship between neural dissimilarity and prospective gains and losses or EV, because these are randomized across trials.</p><p>When quantifying the neural encoding strength of prospective gains and losses, we simply regress the vectorized lower-left triangular part of <inline-formula><mml:math id="inf124"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> against Euclidean distances in either gains or losses concurrently (having included a constant term). This measures the gradient of neural dissimilarity per unit of gains and losses. We quantify the neural encoding strength of EV similarly (using a distinct regression analysis, to prevent regressor collinearities).</p><p>Finally, we measure the statistical similarity of <inline-formula><mml:math id="inf125"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf126"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id="inf127"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> is derived from activity patterns of the ANNs’ integration layer and <inline-formula><mml:math id="inf128"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> is derived from HRF-deconvolved multi-voxel fMRI trial series in each ROI, in terms of the Pearson correlation coefficient <inline-formula><mml:math id="inf129"><mml:mi>ρ</mml:mi></mml:math></inline-formula> between the vectorized lower-left triangular part of <inline-formula><mml:math id="inf130"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. We then assess the group-level statistical significance of RDMs' correlations using one-sample t-tests on the group mean of Fisher-transformed RDM correlation coefficients <inline-formula><mml:math id="inf131"><mml:mi>ρ</mml:mi></mml:math></inline-formula>. Note that ANN-RSA summary statistics (such as RDM correlation coefficients) <italic>do not</italic> favor more complex ANNs (i.e. ANNs with more parameters, such as plastic ANNs). This is because, once fitted to behavioural data, ANNs produce activity patterns that have no degree of freedom whatsoever when they enter RDM derivations. In particular, this means that static ANNs can a priori show a greater RDM correlation than plastic ANNs. In turn, this enables a simple yet unbiased statistical procedure for comparing candidate ANN models. Importantly, this procedure is immune to arbitrary modeling choices such as the total number of units in ANN models.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Resources, Data curation, Software, Formal analysis, Visualization, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Methodology, Writing – original draft, Project administration</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-80979-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data analysed during this study are openly available from the <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/">https://openneuro.org/</ext-link> website (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds001734.v1.0.5">https://doi.org/10.18112/openneuro.ds001734.v1.0.5</ext-link>). All the modelling and analysis code are available as part of the academic freeware VBA (<ext-link ext-link-type="uri" xlink:href="https://github.com/MBB-team/VBA-toolbox/">https://github.com/MBB-team/VBA-toolbox/</ext-link>, <xref ref-type="bibr" rid="bib68">Rigoux et al., 2023</xref>), which is under a GNU open-source license.</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Botvinik-Nezer</surname><given-names>R</given-names></name><name><surname>Iwanir</surname><given-names>R</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Schonberg</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>NARPS</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds001734.v1.0.5</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname><given-names>WC</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>How long will long-term potentiation last?</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>358</volume><fpage>735</fpage><lpage>744</lpage><pub-id pub-id-type="doi">10.1098/rstb.2002.1222</pub-id><pub-id pub-id-type="pmid">12740120</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bermudez</surname><given-names>MA</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Responses of amygdala neurons to positive reward-predicting stimuli depend on background reward (contingency) rather than stimulus-reward pairing (contiguity)</article-title><source>Journal of Neurophysiology</source><volume>103</volume><fpage>1158</fpage><lpage>1170</lpage><pub-id pub-id-type="doi">10.1152/jn.00933.2009</pub-id><pub-id pub-id-type="pmid">20032233</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borst</surname><given-names>JP</given-names></name><name><surname>Taatgen</surname><given-names>NA</given-names></name><name><surname>van Rijn</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Using a symbolic process model as input for model-based fMRI analysis: locating the neural correlates of problem state replacements</article-title><source>NeuroImage</source><volume>58</volume><fpage>137</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.05.084</pub-id><pub-id pub-id-type="pmid">21703351</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinik-Nezer</surname><given-names>R</given-names></name><name><surname>Iwanir</surname><given-names>R</given-names></name><name><surname>Holzmeister</surname><given-names>F</given-names></name><name><surname>Huber</surname><given-names>J</given-names></name><name><surname>Johannesson</surname><given-names>M</given-names></name><name><surname>Kirchler</surname><given-names>M</given-names></name><name><surname>Dreber</surname><given-names>A</given-names></name><name><surname>Camerer</surname><given-names>CF</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Schonberg</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>fMRI data of mixed gambles from the Neuroimaging Analysis Replication and Prediction Study</article-title><source>Scientific Data</source><volume>6</volume><elocation-id>106</elocation-id><pub-id pub-id-type="doi">10.1038/s41597-019-0113-7</pub-id><pub-id pub-id-type="pmid">31263104</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinik-Nezer</surname><given-names>R</given-names></name><name><surname>Holzmeister</surname><given-names>F</given-names></name><name><surname>Camerer</surname><given-names>CF</given-names></name><name><surname>Dreber</surname><given-names>A</given-names></name><name><surname>Huber</surname><given-names>J</given-names></name><name><surname>Johannesson</surname><given-names>M</given-names></name><name><surname>Kirchler</surname><given-names>M</given-names></name><name><surname>Iwanir</surname><given-names>R</given-names></name><name><surname>Mumford</surname><given-names>JA</given-names></name><name><surname>Adcock</surname><given-names>RA</given-names></name><name><surname>Avesani</surname><given-names>P</given-names></name><name><surname>Baczkowski</surname><given-names>BM</given-names></name><name><surname>Bajracharya</surname><given-names>A</given-names></name><name><surname>Bakst</surname><given-names>L</given-names></name><name><surname>Ball</surname><given-names>S</given-names></name><name><surname>Barilari</surname><given-names>M</given-names></name><name><surname>Bault</surname><given-names>N</given-names></name><name><surname>Beaton</surname><given-names>D</given-names></name><name><surname>Beitner</surname><given-names>J</given-names></name><name><surname>Benoit</surname><given-names>RG</given-names></name><name><surname>Berkers</surname><given-names>R</given-names></name><name><surname>Bhanji</surname><given-names>JP</given-names></name><name><surname>Biswal</surname><given-names>BB</given-names></name><name><surname>Bobadilla-Suarez</surname><given-names>S</given-names></name><name><surname>Bortolini</surname><given-names>T</given-names></name><name><surname>Bottenhorn</surname><given-names>KL</given-names></name><name><surname>Bowring</surname><given-names>A</given-names></name><name><surname>Braem</surname><given-names>S</given-names></name><name><surname>Brooks</surname><given-names>HR</given-names></name><name><surname>Brudner</surname><given-names>EG</given-names></name><name><surname>Calderon</surname><given-names>CB</given-names></name><name><surname>Camilleri</surname><given-names>JA</given-names></name><name><surname>Castrellon</surname><given-names>JJ</given-names></name><name><surname>Cecchetti</surname><given-names>L</given-names></name><name><surname>Cieslik</surname><given-names>EC</given-names></name><name><surname>Cole</surname><given-names>ZJ</given-names></name><name><surname>Collignon</surname><given-names>O</given-names></name><name><surname>Cox</surname><given-names>RW</given-names></name><name><surname>Cunningham</surname><given-names>WA</given-names></name><name><surname>Czoschke</surname><given-names>S</given-names></name><name><surname>Dadi</surname><given-names>K</given-names></name><name><surname>Davis</surname><given-names>CP</given-names></name><name><surname>Luca</surname><given-names>AD</given-names></name><name><surname>Delgado</surname><given-names>MR</given-names></name><name><surname>Demetriou</surname><given-names>L</given-names></name><name><surname>Dennison</surname><given-names>JB</given-names></name><name><surname>Di</surname><given-names>X</given-names></name><name><surname>Dickie</surname><given-names>EW</given-names></name><name><surname>Dobryakova</surname><given-names>E</given-names></name><name><surname>Donnat</surname><given-names>CL</given-names></name><name><surname>Dukart</surname><given-names>J</given-names></name><name><surname>Duncan</surname><given-names>NW</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Eed</surname><given-names>A</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Erhart</surname><given-names>A</given-names></name><name><surname>Fontanesi</surname><given-names>L</given-names></name><name><surname>Fricke</surname><given-names>GM</given-names></name><name><surname>Fu</surname><given-names>S</given-names></name><name><surname>Galván</surname><given-names>A</given-names></name><name><surname>Gau</surname><given-names>R</given-names></name><name><surname>Genon</surname><given-names>S</given-names></name><name><surname>Glatard</surname><given-names>T</given-names></name><name><surname>Glerean</surname><given-names>E</given-names></name><name><surname>Goeman</surname><given-names>JJ</given-names></name><name><surname>Golowin</surname><given-names>SAE</given-names></name><name><surname>González-García</surname><given-names>C</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Grady</surname><given-names>CL</given-names></name><name><surname>Green</surname><given-names>MA</given-names></name><name><surname>Guassi Moreira</surname><given-names>JF</given-names></name><name><surname>Guest</surname><given-names>O</given-names></name><name><surname>Hakimi</surname><given-names>S</given-names></name><name><surname>Hamilton</surname><given-names>JP</given-names></name><name><surname>Hancock</surname><given-names>R</given-names></name><name><surname>Handjaras</surname><given-names>G</given-names></name><name><surname>Harry</surname><given-names>BB</given-names></name><name><surname>Hawco</surname><given-names>C</given-names></name><name><surname>Herholz</surname><given-names>P</given-names></name><name><surname>Herman</surname><given-names>G</given-names></name><name><surname>Heunis</surname><given-names>S</given-names></name><name><surname>Hoffstaedter</surname><given-names>F</given-names></name><name><surname>Hogeveen</surname><given-names>J</given-names></name><name><surname>Holmes</surname><given-names>S</given-names></name><name><surname>Hu</surname><given-names>CP</given-names></name><name><surname>Huettel</surname><given-names>SA</given-names></name><name><surname>Hughes</surname><given-names>ME</given-names></name><name><surname>Iacovella</surname><given-names>V</given-names></name><name><surname>Iordan</surname><given-names>AD</given-names></name><name><surname>Isager</surname><given-names>PM</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Jahn</surname><given-names>A</given-names></name><name><surname>Johnson</surname><given-names>MR</given-names></name><name><surname>Johnstone</surname><given-names>T</given-names></name><name><surname>Joseph</surname><given-names>MJE</given-names></name><name><surname>Juliano</surname><given-names>AC</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name><name><surname>Kassinopoulos</surname><given-names>M</given-names></name><name><surname>Koba</surname><given-names>C</given-names></name><name><surname>Kong</surname><given-names>XZ</given-names></name><name><surname>Koscik</surname><given-names>TR</given-names></name><name><surname>Kucukboyaci</surname><given-names>NE</given-names></name><name><surname>Kuhl</surname><given-names>BA</given-names></name><name><surname>Kupek</surname><given-names>S</given-names></name><name><surname>Laird</surname><given-names>AR</given-names></name><name><surname>Lamm</surname><given-names>C</given-names></name><name><surname>Langner</surname><given-names>R</given-names></name><name><surname>Lauharatanahirun</surname><given-names>N</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>S</given-names></name><name><surname>Leemans</surname><given-names>A</given-names></name><name><surname>Leo</surname><given-names>A</given-names></name><name><surname>Lesage</surname><given-names>E</given-names></name><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>MYC</given-names></name><name><surname>Lim</surname><given-names>PC</given-names></name><name><surname>Lintz</surname><given-names>EN</given-names></name><name><surname>Liphardt</surname><given-names>SW</given-names></name><name><surname>Losecaat Vermeer</surname><given-names>AB</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name><name><surname>Mack</surname><given-names>ML</given-names></name><name><surname>Malpica</surname><given-names>N</given-names></name><name><surname>Marins</surname><given-names>T</given-names></name><name><surname>Maumet</surname><given-names>C</given-names></name><name><surname>McDonald</surname><given-names>K</given-names></name><name><surname>McGuire</surname><given-names>JT</given-names></name><name><surname>Melero</surname><given-names>H</given-names></name><name><surname>Méndez Leal</surname><given-names>AS</given-names></name><name><surname>Meyer</surname><given-names>B</given-names></name><name><surname>Meyer</surname><given-names>KN</given-names></name><name><surname>Mihai</surname><given-names>G</given-names></name><name><surname>Mitsis</surname><given-names>GD</given-names></name><name><surname>Moll</surname><given-names>J</given-names></name><name><surname>Nielson</surname><given-names>DM</given-names></name><name><surname>Nilsonne</surname><given-names>G</given-names></name><name><surname>Notter</surname><given-names>MP</given-names></name><name><surname>Olivetti</surname><given-names>E</given-names></name><name><surname>Onicas</surname><given-names>AI</given-names></name><name><surname>Papale</surname><given-names>P</given-names></name><name><surname>Patil</surname><given-names>KR</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Pérez</surname><given-names>A</given-names></name><name><surname>Pischedda</surname><given-names>D</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name><name><surname>Prystauka</surname><given-names>Y</given-names></name><name><surname>Ray</surname><given-names>S</given-names></name><name><surname>Reuter-Lorenz</surname><given-names>PA</given-names></name><name><surname>Reynolds</surname><given-names>RC</given-names></name><name><surname>Ricciardi</surname><given-names>E</given-names></name><name><surname>Rieck</surname><given-names>JR</given-names></name><name><surname>Rodriguez-Thompson</surname><given-names>AM</given-names></name><name><surname>Romyn</surname><given-names>A</given-names></name><name><surname>Salo</surname><given-names>T</given-names></name><name><surname>Samanez-Larkin</surname><given-names>GR</given-names></name><name><surname>Sanz-Morales</surname><given-names>E</given-names></name><name><surname>Schlichting</surname><given-names>ML</given-names></name><name><surname>Schultz</surname><given-names>DH</given-names></name><name><surname>Shen</surname><given-names>Q</given-names></name><name><surname>Sheridan</surname><given-names>MA</given-names></name><name><surname>Silvers</surname><given-names>JA</given-names></name><name><surname>Skagerlund</surname><given-names>K</given-names></name><name><surname>Smith</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>DV</given-names></name><name><surname>Sokol-Hessner</surname><given-names>P</given-names></name><name><surname>Steinkamp</surname><given-names>SR</given-names></name><name><surname>Tashjian</surname><given-names>SM</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Thorp</surname><given-names>JN</given-names></name><name><surname>Tinghög</surname><given-names>G</given-names></name><name><surname>Tisdall</surname><given-names>L</given-names></name><name><surname>Tompson</surname><given-names>SH</given-names></name><name><surname>Toro-Serey</surname><given-names>C</given-names></name><name><surname>Torre Tresols</surname><given-names>JJ</given-names></name><name><surname>Tozzi</surname><given-names>L</given-names></name><name><surname>Truong</surname><given-names>V</given-names></name><name><surname>Turella</surname><given-names>L</given-names></name><name><surname>van ’t Veer</surname><given-names>AE</given-names></name><name><surname>Verguts</surname><given-names>T</given-names></name><name><surname>Vettel</surname><given-names>JM</given-names></name><name><surname>Vijayarajah</surname><given-names>S</given-names></name><name><surname>Vo</surname><given-names>K</given-names></name><name><surname>Wall</surname><given-names>MB</given-names></name><name><surname>Weeda</surname><given-names>WD</given-names></name><name><surname>Weis</surname><given-names>S</given-names></name><name><surname>White</surname><given-names>DJ</given-names></name><name><surname>Wisniewski</surname><given-names>D</given-names></name><name><surname>Xifra-Porxas</surname><given-names>A</given-names></name><name><surname>Yearling</surname><given-names>EA</given-names></name><name><surname>Yoon</surname><given-names>S</given-names></name><name><surname>Yuan</surname><given-names>R</given-names></name><name><surname>Yuen</surname><given-names>KSL</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Zosky</surname><given-names>JE</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Schonberg</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Variability in the analysis of a single neuroimaging dataset by many teams</article-title><source>Nature</source><volume>582</volume><fpage>84</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2314-9</pub-id><pub-id pub-id-type="pmid">32483374</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Botvinik-Nezer</surname><given-names>R</given-names></name><name><surname>Holzmeister</surname><given-names>F</given-names></name><name><surname>Camerer</surname><given-names>CF</given-names></name><name><surname>Dreber</surname><given-names>A</given-names></name><name><surname>Huber</surname><given-names>J</given-names></name><name><surname>Johannesson</surname><given-names>M</given-names></name><name><surname>Kirchler</surname><given-names>M</given-names></name><name><surname>Iwanir</surname><given-names>R</given-names></name><name><surname>Mumford</surname><given-names>JA</given-names></name><name><surname>Adcock</surname><given-names>RA</given-names></name><name><surname>Avesani</surname><given-names>P</given-names></name><name><surname>Baczkowski</surname><given-names>BM</given-names></name><name><surname>Bajracharya</surname><given-names>A</given-names></name><name><surname>Bakst</surname><given-names>L</given-names></name><name><surname>Ball</surname><given-names>S</given-names></name><name><surname>Barilari</surname><given-names>M</given-names></name><name><surname>Bault</surname><given-names>N</given-names></name><name><surname>Beaton</surname><given-names>D</given-names></name><name><surname>Beitner</surname><given-names>J</given-names></name><name><surname>Benoit</surname><given-names>RG</given-names></name><name><surname>Berkers</surname><given-names>R</given-names></name><name><surname>Bhanji</surname><given-names>JP</given-names></name><name><surname>Biswal</surname><given-names>BB</given-names></name><name><surname>Bobadilla-Suarez</surname><given-names>S</given-names></name><name><surname>Bortolini</surname><given-names>T</given-names></name><name><surname>Bottenhorn</surname><given-names>KL</given-names></name><name><surname>Bowring</surname><given-names>A</given-names></name><name><surname>Braem</surname><given-names>S</given-names></name><name><surname>Brooks</surname><given-names>HR</given-names></name><name><surname>Brudner</surname><given-names>EG</given-names></name><name><surname>Calderon</surname><given-names>CB</given-names></name><name><surname>Camilleri</surname><given-names>JA</given-names></name><name><surname>Castrellon</surname><given-names>JJ</given-names></name><name><surname>Cecchetti</surname><given-names>L</given-names></name><name><surname>Cieslik</surname><given-names>EC</given-names></name><name><surname>Cole</surname><given-names>ZJ</given-names></name><name><surname>Collignon</surname><given-names>O</given-names></name><name><surname>Cox</surname><given-names>RW</given-names></name><name><surname>Cunningham</surname><given-names>WA</given-names></name><name><surname>Czoschke</surname><given-names>S</given-names></name><name><surname>Dadi</surname><given-names>K</given-names></name><name><surname>Davis</surname><given-names>CP</given-names></name><name><surname>Luca</surname><given-names>AD</given-names></name><name><surname>Delgado</surname><given-names>MR</given-names></name><name><surname>Demetriou</surname><given-names>L</given-names></name><name><surname>Dennison</surname><given-names>JB</given-names></name><name><surname>Di</surname><given-names>X</given-names></name><name><surname>Dickie</surname><given-names>EW</given-names></name><name><surname>Dobryakova</surname><given-names>E</given-names></name><name><surname>Donnat</surname><given-names>CL</given-names></name><name><surname>Dukart</surname><given-names>J</given-names></name><name><surname>Duncan</surname><given-names>NW</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Eed</surname><given-names>A</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Erhart</surname><given-names>A</given-names></name><name><surname>Fontanesi</surname><given-names>L</given-names></name><name><surname>Fricke</surname><given-names>GM</given-names></name><name><surname>Fu</surname><given-names>S</given-names></name><name><surname>Galván</surname><given-names>A</given-names></name><name><surname>Gau</surname><given-names>R</given-names></name><name><surname>Genon</surname><given-names>S</given-names></name><name><surname>Glatard</surname><given-names>T</given-names></name><name><surname>Glerean</surname><given-names>E</given-names></name><name><surname>Goeman</surname><given-names>JJ</given-names></name><name><surname>Golowin</surname><given-names>SAE</given-names></name><name><surname>González-García</surname><given-names>C</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Grady</surname><given-names>CL</given-names></name><name><surname>Green</surname><given-names>MA</given-names></name><name><surname>Guassi Moreira</surname><given-names>JF</given-names></name><name><surname>Guest</surname><given-names>O</given-names></name><name><surname>Hakimi</surname><given-names>S</given-names></name><name><surname>Hamilton</surname><given-names>JP</given-names></name><name><surname>Hancock</surname><given-names>R</given-names></name><name><surname>Handjaras</surname><given-names>G</given-names></name><name><surname>Harry</surname><given-names>BB</given-names></name><name><surname>Hawco</surname><given-names>C</given-names></name><name><surname>Herholz</surname><given-names>P</given-names></name><name><surname>Herman</surname><given-names>G</given-names></name><name><surname>Heunis</surname><given-names>S</given-names></name><name><surname>Hoffstaedter</surname><given-names>F</given-names></name><name><surname>Hogeveen</surname><given-names>J</given-names></name><name><surname>Holmes</surname><given-names>S</given-names></name><name><surname>Hu</surname><given-names>CP</given-names></name><name><surname>Huettel</surname><given-names>SA</given-names></name><name><surname>Hughes</surname><given-names>ME</given-names></name><name><surname>Iacovella</surname><given-names>V</given-names></name><name><surname>Iordan</surname><given-names>AD</given-names></name><name><surname>Isager</surname><given-names>PM</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Jahn</surname><given-names>A</given-names></name><name><surname>Johnson</surname><given-names>MR</given-names></name><name><surname>Johnstone</surname><given-names>T</given-names></name><name><surname>Joseph</surname><given-names>MJE</given-names></name><name><surname>Juliano</surname><given-names>AC</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name><name><surname>Kassinopoulos</surname><given-names>M</given-names></name><name><surname>Koba</surname><given-names>C</given-names></name><name><surname>Kong</surname><given-names>XZ</given-names></name><name><surname>Koscik</surname><given-names>TR</given-names></name><name><surname>Kucukboyaci</surname><given-names>NE</given-names></name><name><surname>Kuhl</surname><given-names>BA</given-names></name><name><surname>Kupek</surname><given-names>S</given-names></name><name><surname>Laird</surname><given-names>AR</given-names></name><name><surname>Lamm</surname><given-names>C</given-names></name><name><surname>Langner</surname><given-names>R</given-names></name><name><surname>Lauharatanahirun</surname><given-names>N</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>S</given-names></name><name><surname>Leemans</surname><given-names>A</given-names></name><name><surname>Leo</surname><given-names>A</given-names></name><name><surname>Lesage</surname><given-names>E</given-names></name><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>MYC</given-names></name><name><surname>Lim</surname><given-names>PC</given-names></name><name><surname>Lintz</surname><given-names>EN</given-names></name><name><surname>Liphardt</surname><given-names>SW</given-names></name><name><surname>Losecaat Vermeer</surname><given-names>AB</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name><name><surname>Mack</surname><given-names>ML</given-names></name><name><surname>Malpica</surname><given-names>N</given-names></name><name><surname>Marins</surname><given-names>T</given-names></name><name><surname>Maumet</surname><given-names>C</given-names></name><name><surname>McDonald</surname><given-names>K</given-names></name><name><surname>McGuire</surname><given-names>JT</given-names></name><name><surname>Melero</surname><given-names>H</given-names></name><name><surname>Méndez Leal</surname><given-names>AS</given-names></name><name><surname>Meyer</surname><given-names>B</given-names></name><name><surname>Meyer</surname><given-names>KN</given-names></name><name><surname>Mihai</surname><given-names>G</given-names></name><name><surname>Mitsis</surname><given-names>GD</given-names></name><name><surname>Moll</surname><given-names>J</given-names></name><name><surname>Nielson</surname><given-names>DM</given-names></name><name><surname>Nilsonne</surname><given-names>G</given-names></name><name><surname>Notter</surname><given-names>MP</given-names></name><name><surname>Olivetti</surname><given-names>E</given-names></name><name><surname>Onicas</surname><given-names>AI</given-names></name><name><surname>Papale</surname><given-names>P</given-names></name><name><surname>Patil</surname><given-names>KR</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Pérez</surname><given-names>A</given-names></name><name><surname>Pischedda</surname><given-names>D</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name><name><surname>Prystauka</surname><given-names>Y</given-names></name><name><surname>Ray</surname><given-names>S</given-names></name><name><surname>Reuter-Lorenz</surname><given-names>PA</given-names></name><name><surname>Reynolds</surname><given-names>RC</given-names></name><name><surname>Ricciardi</surname><given-names>E</given-names></name><name><surname>Rieck</surname><given-names>JR</given-names></name><name><surname>Rodriguez-Thompson</surname><given-names>AM</given-names></name><name><surname>Romyn</surname><given-names>A</given-names></name><name><surname>Salo</surname><given-names>T</given-names></name><name><surname>Samanez-Larkin</surname><given-names>GR</given-names></name><name><surname>Sanz-Morales</surname><given-names>E</given-names></name><name><surname>Schlichting</surname><given-names>ML</given-names></name><name><surname>Schultz</surname><given-names>DH</given-names></name><name><surname>Shen</surname><given-names>Q</given-names></name><name><surname>Sheridan</surname><given-names>MA</given-names></name><name><surname>Silvers</surname><given-names>JA</given-names></name><name><surname>Skagerlund</surname><given-names>K</given-names></name><name><surname>Smith</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>DV</given-names></name><name><surname>Sokol-Hessner</surname><given-names>P</given-names></name><name><surname>Steinkamp</surname><given-names>SR</given-names></name><name><surname>Tashjian</surname><given-names>SM</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Thorp</surname><given-names>JN</given-names></name><name><surname>Tinghög</surname><given-names>G</given-names></name><name><surname>Tisdall</surname><given-names>L</given-names></name><name><surname>Tompson</surname><given-names>SH</given-names></name><name><surname>Toro-Serey</surname><given-names>C</given-names></name><name><surname>Torre Tresols</surname><given-names>JJ</given-names></name><name><surname>Tozzi</surname><given-names>L</given-names></name><name><surname>Truong</surname><given-names>V</given-names></name><name><surname>Turella</surname><given-names>L</given-names></name><name><surname>van ‘t Veer</surname><given-names>AE</given-names></name><name><surname>Verguts</surname><given-names>T</given-names></name><name><surname>Vettel</surname><given-names>JM</given-names></name><name><surname>Vijayarajah</surname><given-names>S</given-names></name><name><surname>Vo</surname><given-names>K</given-names></name><name><surname>Wall</surname><given-names>MB</given-names></name><name><surname>Weeda</surname><given-names>WD</given-names></name><name><surname>Weis</surname><given-names>S</given-names></name><name><surname>White</surname><given-names>DJ</given-names></name><name><surname>Wisniewski</surname><given-names>D</given-names></name><name><surname>Xifra-Porxas</surname><given-names>A</given-names></name><name><surname>Yearling</surname><given-names>EA</given-names></name><name><surname>Yoon</surname><given-names>S</given-names></name><name><surname>Yuan</surname><given-names>R</given-names></name><name><surname>Yuen</surname><given-names>KSL</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Zosky</surname><given-names>JE</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Schonberg</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Variability in the analysis of a single neuroimaging dataset by many teams</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/843193</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brenner</surname><given-names>N</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Adaptive rescaling maximizes information transmission</article-title><source>Neuron</source><volume>26</volume><fpage>695</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)81205-2</pub-id><pub-id pub-id-type="pmid">10896164</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burke</surname><given-names>CJ</given-names></name><name><surname>Baddeley</surname><given-names>M</given-names></name><name><surname>Tobler</surname><given-names>PN</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Partial adaptation of obtained and observed value signals preserves information about gains and losses</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>10016</fpage><lpage>10025</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0487-16.2016</pub-id><pub-id pub-id-type="pmid">27683899</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buschman</surname><given-names>TJ</given-names></name><name><surname>Siegel</surname><given-names>M</given-names></name><name><surname>Roy</surname><given-names>JE</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural substrates of cognitive capacity limitations</article-title><source>PNAS</source><volume>108</volume><fpage>11252</fpage><lpage>11255</lpage><pub-id pub-id-type="doi">10.1073/pnas.1104666108</pub-id><pub-id pub-id-type="pmid">21690375</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>X</given-names></name><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neuronal encoding of subjective value in dorsal and ventral anterior cingulate cortex</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>3791</fpage><lpage>3808</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3864-11.2012</pub-id><pub-id pub-id-type="pmid">22423100</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Normalization as a canonical neural computation</article-title><source>Nature Reviews. Neuroscience</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id><pub-id pub-id-type="pmid">22108672</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cicchini</surname><given-names>GM</given-names></name><name><surname>Arrighi</surname><given-names>R</given-names></name><name><surname>Cecchetti</surname><given-names>L</given-names></name><name><surname>Giusti</surname><given-names>M</given-names></name><name><surname>Burr</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Optimal encoding of interval timing in expert percussionists</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>1056</fpage><lpage>1060</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3411-11.2012</pub-id><pub-id pub-id-type="pmid">22262903</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clairis</surname><given-names>N</given-names></name><name><surname>Pessiglione</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Value, confidence, deliberation: a functional partition of the medial prefrontal cortex demonstrated across rating and choice tasks</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>5580</fpage><lpage>5592</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1795-21.2022</pub-id><pub-id pub-id-type="pmid">35654606</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conen</surname><given-names>KE</given-names></name><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Partial adaptation to the value range in the macaque orbitofrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>3498</fpage><lpage>3513</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2279-18.2019</pub-id><pub-id pub-id-type="pmid">30833513</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>KM</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>BOLD subjective value signals exhibit robust range adaptation</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>16533</fpage><lpage>16543</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3927-14.2014</pub-id><pub-id pub-id-type="pmid">25471589</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Optimal experimental design for event-related fMRI</article-title><source>Human Brain Mapping</source><volume>8</volume><fpage>109</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1999)8:2/3&lt;109::AID-HBM7&gt;3.0.CO;2-W</pub-id><pub-id pub-id-type="pmid">10524601</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Adam</surname><given-names>V</given-names></name><name><surname>Rigoux</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>VBA: a probabilistic treatment of nonlinear models for neurobiological and behavioural data</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003441</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003441</pub-id><pub-id pub-id-type="pmid">24465198</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Daunizeau</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>On parameters transformations for emulating sparse priors using variational laplace inference</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1703.07168">http://arxiv.org/abs/1703.07168</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diedrichsen</surname><given-names>J</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Representational models: A common framework for understanding encoding, pattern-component, and representational-similarity analysis</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005508</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005508</pub-id><pub-id pub-id-type="pmid">28437426</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Devauchelle</surname><given-names>AD</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Computational precision of mental inference as critical source of human choice suboptimality</article-title><source>Neuron</source><volume>92</volume><fpage>1398</fpage><lpage>1411</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.11.005</pub-id><pub-id pub-id-type="pmid">27916454</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ebitz</surname><given-names>RB</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The population doctrine in cognitive neuroscience</article-title><source>Neuron</source><volume>109</volume><fpage>3055</fpage><lpage>3068</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.07.011</pub-id><pub-id pub-id-type="pmid">34416170</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elliott</surname><given-names>R</given-names></name><name><surname>Agnew</surname><given-names>Z</given-names></name><name><surname>Deakin</surname><given-names>JFW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Medial orbitofrontal cortex codes relative rather than absolute value of financial rewards in humans</article-title><source>The European Journal of Neuroscience</source><volume>27</volume><fpage>2213</fpage><lpage>2218</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2008.06202.x</pub-id><pub-id pub-id-type="pmid">18445214</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Zhuo</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Chu</surname><given-names>C</given-names></name><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Laird</surname><given-names>AR</given-names></name><name><surname>Fox</surname><given-names>PT</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Yu</surname><given-names>C</given-names></name><name><surname>Jiang</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The human brainnetome atlas: a new brain atlas based on connectional architecture</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3508</fpage><lpage>3526</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw157</pub-id><pub-id pub-id-type="pmid">27230218</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiebig</surname><given-names>F</given-names></name><name><surname>Lansner</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A spiking working memory model based on hebbian short-term potentiation</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>83</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1989-16.2016</pub-id><pub-id pub-id-type="pmid">28053032</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>K</given-names></name><name><surname>Stryker</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Integrating Hebbian and homeostatic plasticity: introduction</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>372</volume><elocation-id>20160413</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0413</pub-id><pub-id pub-id-type="pmid">28093560</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Holmes</surname><given-names>AP</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name><name><surname>Grasby</surname><given-names>PJ</given-names></name><name><surname>Williams</surname><given-names>SCR</given-names></name><name><surname>Frackowiak</surname><given-names>RSJ</given-names></name><name><surname>Turner</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Analysis of fMRI time-series revisited</article-title><source>NeuroImage</source><volume>2</volume><fpage>45</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1006/nimg.1995.1007</pub-id><pub-id pub-id-type="pmid">9343589</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name><name><surname>Holmes</surname><given-names>E</given-names></name><name><surname>Zeidman</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Variational representational similarity analysis</article-title><source>NeuroImage</source><volume>201</volume><elocation-id>115986</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.06.064</pub-id><pub-id pub-id-type="pmid">31255808</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id><pub-id pub-id-type="pmid">26157000</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfinger</surname><given-names>JB</given-names></name><name><surname>Büchel</surname><given-names>C</given-names></name><name><surname>Holmes</surname><given-names>AP</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A study of analysis parameters that influence the sensitivity of event-related fMRI analyses</article-title><source>NeuroImage</source><volume>11</volume><fpage>326</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.1006/nimg.2000.0549</pub-id><pub-id pub-id-type="pmid">10725188</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kahneman</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>Thinking, Fast and Slow</source><publisher-name>Macmillan</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kahneman</surname><given-names>D</given-names></name><name><surname>Tversky</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>Prospect theory: an analysis of decision under risk</chapter-title><person-group person-group-type="editor"><name><surname>Kahneman</surname><given-names>D</given-names></name><name><surname>Tversky</surname><given-names>A</given-names></name></person-group><source>In Handbook of the Fundamentals of Financial Decision Making</source><publisher-name>WORLD SCIENTIFIC</publisher-name><fpage>99</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1142/9789814417358_0006</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaw</surname><given-names>MW</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Louie</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Normalized value coding explains dynamic adaptation in the human valuation process</article-title><source>PNAS</source><volume>114</volume><fpage>12696</fpage><lpage>12701</lpage><pub-id pub-id-type="doi">10.1073/pnas.1715293114</pub-id><pub-id pub-id-type="pmid">29133418</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>McClure</surname><given-names>P</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Deep neural networks in computational neuroscience</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/133504</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Sörensen</surname><given-names>LKA</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title><source>PNAS</source><volume>116</volume><fpage>21854</fpage><lpage>21863</lpage><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id><pub-id pub-id-type="pmid">31591217</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobayashi</surname><given-names>S</given-names></name><name><surname>Pinto de Carvalho</surname><given-names>O</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Adaptation of reward sensitivity in orbitofrontal neurons</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>534</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4009-09.2010</pub-id><pub-id pub-id-type="pmid">20071516</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Golan</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural network models and deep learning</article-title><source>Current Biology</source><volume>29</volume><fpage>R231</fpage><lpage>R236</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.02.034</pub-id><pub-id pub-id-type="pmid">30939301</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>A simple coding procedure enhances A neuron’s information capacity</article-title><source>Zeitschrift Fur Naturforschung. Section C, Biosciences</source><volume>36</volume><fpage>910</fpage><lpage>912</lpage><pub-id pub-id-type="pmid">7303823</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebreton</surname><given-names>M</given-names></name><name><surname>Jorge</surname><given-names>S</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Pessiglione</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>An automatic valuation system in the human brain: evidence from functional neuroimaging</article-title><source>Neuron</source><volume>64</volume><fpage>431</fpage><lpage>439</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.09.040</pub-id><pub-id pub-id-type="pmid">19914190</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>SL</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name><name><surname>Rangel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Stimulus value signals in ventromedial PFC reflect the integration of attribute value signals computed in fusiform gyrus and posterior superior temporal gyrus</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>8729</fpage><lpage>8741</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4809-12.2013</pub-id><pub-id pub-id-type="pmid">23678116</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopez-Persem</surname><given-names>A</given-names></name><name><surname>Bastin</surname><given-names>J</given-names></name><name><surname>Petton</surname><given-names>M</given-names></name><name><surname>Abitbol</surname><given-names>R</given-names></name><name><surname>Lehongre</surname><given-names>K</given-names></name><name><surname>Adam</surname><given-names>C</given-names></name><name><surname>Navarro</surname><given-names>V</given-names></name><name><surname>Rheims</surname><given-names>S</given-names></name><name><surname>Kahane</surname><given-names>P</given-names></name><name><surname>Domenech</surname><given-names>P</given-names></name><name><surname>Pessiglione</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Four core properties of the human brain valuation system demonstrated in intracranial signals</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>664</fpage><lpage>675</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0615-9</pub-id><pub-id pub-id-type="pmid">32284605</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Efficient coding and the neural representation of value</article-title><source>Annals of the New York Academy of Sciences</source><volume>1251</volume><fpage>13</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2012.06496.x</pub-id><pub-id pub-id-type="pmid">22694213</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Khaw</surname><given-names>MW</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Normalization is a general neural mechanism for context-dependent decision making</article-title><source>PNAS</source><volume>110</volume><fpage>6139</fpage><lpage>6144</lpage><pub-id pub-id-type="doi">10.1073/pnas.1217854110</pub-id><pub-id pub-id-type="pmid">23530203</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Webb</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Adaptive neural coding: from biological to behavioral decision-making</article-title><source>Current Opinion in Behavioral Sciences</source><volume>5</volume><fpage>91</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2015.08.008</pub-id><pub-id pub-id-type="pmid">26722666</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marois</surname><given-names>R</given-names></name><name><surname>Ivanoff</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Capacity limits of information processing in the brain</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>296</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.04.010</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>May</surname><given-names>KA</given-names></name><name><surname>Zhaoping</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Efficient coding theory predicts a tilt aftereffect from viewing untilted patterns</article-title><source>Current Biology</source><volume>26</volume><fpage>1571</fpage><lpage>1576</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.04.037</pub-id><pub-id pub-id-type="pmid">27291055</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClamrock</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Marr’s three levels: A re-evaluation</article-title><source>Minds and Machines</source><volume>1</volume><fpage>185</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1007/BF00361036</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Working memory capacity: limits on the bandwidth of cognition</article-title><source>Daedalus</source><volume>144</volume><fpage>112</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1162/DAED_a_00320</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nadal</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>Non Linear Neurons in the Low Noise Limit: A Factorial Code Maximizes Information transferJean</source><publisher-name>Semantic Scholar</publisher-name><pub-id pub-id-type="doi">10.1088/0954-898X/5/4/008</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Doherty</surname><given-names>JP</given-names></name><name><surname>Hampton</surname><given-names>A</given-names></name><name><surname>Kim</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Model-based fMRI and its application to reward learning and decision making</article-title><source>Annals of the New York Academy of Sciences</source><volume>1104</volume><fpage>35</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1196/annals.1390.022</pub-id><pub-id pub-id-type="pmid">17416921</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Doherty</surname><given-names>JP</given-names></name><name><surname>Rutishauser</surname><given-names>U</given-names></name><name><surname>Iigaya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The hierarchical construction of value</article-title><source>Current Opinion in Behavioral Sciences</source><volume>41</volume><fpage>71</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2021.03.027</pub-id><pub-id pub-id-type="pmid">35252481</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name><name><surname>Assad</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neurons in the orbitofrontal cortex encode economic value</article-title><source>Nature</source><volume>441</volume><fpage>223</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1038/nature04676</pub-id><pub-id pub-id-type="pmid">16633341</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name><name><surname>Assad</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The representation of economic value in the orbitofrontal cortex is invariant for changes of menu</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>95</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1038/nn2020</pub-id><pub-id pub-id-type="pmid">18066060</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Range-adapting representation of economic value in the orbitofrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>14004</fpage><lpage>14014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3751-09.2009</pub-id><pub-id pub-id-type="pmid">19890010</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name><name><surname>Rustichini</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Rational attention and adaptive coding: a puzzle and a solution</article-title><source>The American Economic Review</source><volume>104</volume><fpage>507</fpage><lpage>513</lpage><pub-id pub-id-type="doi">10.1257/aer.104.5.507</pub-id><pub-id pub-id-type="pmid">25484369</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pessiglione</surname><given-names>M</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Bridging across functional models: The OFC as a value-making neural network</article-title><source>Behavioral Neuroscience</source><volume>135</volume><fpage>277</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1037/bne0000464</pub-id><pub-id pub-id-type="pmid">34060880</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pezzulo</surname><given-names>G</given-names></name><name><surname>Rigoli</surname><given-names>F</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Active Inference, homeostatic regulation and adaptive behavioural control</article-title><source>Progress in Neurobiology</source><volume>134</volume><fpage>17</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2015.09.001</pub-id><pub-id pub-id-type="pmid">26365173</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polanía</surname><given-names>R</given-names></name><name><surname>Woodford</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Efficient coding of subjective value</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>134</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0292-0</pub-id><pub-id pub-id-type="pmid">30559477</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Barch</surname><given-names>DM</given-names></name><name><surname>Mitchell</surname><given-names>JP</given-names></name><name><surname>Wager</surname><given-names>TD</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name><name><surname>Devlin</surname><given-names>JT</given-names></name><name><surname>Cumba</surname><given-names>C</given-names></name><name><surname>Koyejo</surname><given-names>O</given-names></name><name><surname>Milham</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Toward open sharing of task-based fMRI data: the OpenfMRI project</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00012</pub-id><pub-id pub-id-type="pmid">23847528</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pooresmaeili</surname><given-names>A</given-names></name><name><surname>Arrighi</surname><given-names>R</given-names></name><name><surname>Biagi</surname><given-names>L</given-names></name><name><surname>Morrone</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Blood oxygen level-dependent activation of the primary visual cortex predicts size adaptation illusion</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>15999</fpage><lpage>16008</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1770-13.2013</pub-id><pub-id pub-id-type="pmid">24089504</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Porrill</surname><given-names>J</given-names></name><name><surname>Stone</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Undercomplete Independent Component Analysis for Signal Separation and Dimension Reduction</source><publisher-name>Semantic Scholar</publisher-name></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raghuraman</surname><given-names>AP</given-names></name><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Integration of multiple determinants in the neuronal computation of economic values</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>11583</fpage><lpage>11603</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1235-14.2014</pub-id><pub-id pub-id-type="pmid">25164656</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rakow</surname><given-names>T</given-names></name><name><surname>Cheung</surname><given-names>NY</given-names></name><name><surname>Restelli</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Losing my loss aversion: the effects of current and past environment on the relative sensitivity to losses and gains</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>27</volume><fpage>1333</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.3758/s13423-020-01775-y</pub-id><pub-id pub-id-type="pmid">32720085</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramsey</surname><given-names>NF</given-names></name><name><surname>Jansma</surname><given-names>JM</given-names></name><name><surname>Jager</surname><given-names>G</given-names></name><name><surname>Van Raalten</surname><given-names>T</given-names></name><name><surname>Kahn</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Neurophysiological factors in human information processing capacity</article-title><source>Brain</source><volume>127</volume><fpage>517</fpage><lpage>525</lpage><pub-id pub-id-type="doi">10.1093/brain/awh060</pub-id><pub-id pub-id-type="pmid">14691061</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rangel</surname><given-names>A</given-names></name><name><surname>Camerer</surname><given-names>C</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A framework for studying the neurobiology of value-based decision making</article-title><source>Nature Reviews. Neuroscience</source><volume>9</volume><fpage>545</fpage><lpage>556</lpage><pub-id pub-id-type="doi">10.1038/nrn2357</pub-id><pub-id pub-id-type="pmid">18545266</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rangel</surname><given-names>A</given-names></name><name><surname>Clithero</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Value normalization in decision making: theory and evidence</article-title><source>Current Opinion in Neurobiology</source><volume>22</volume><fpage>970</fpage><lpage>981</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2012.07.011</pub-id><pub-id pub-id-type="pmid">22939568</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigoli</surname><given-names>F</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural processes mediating contextual influences on human choice behaviour</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>12416</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms12416</pub-id><pub-id pub-id-type="pmid">27535770</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Rigoux</surname><given-names>L</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Adam</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>VBA-toolbox</data-title><version designator="5899497">5899497</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/MBB-team/VBA-toolbox/">https://github.com/MBB-team/VBA-toolbox/</ext-link></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rustichini</surname><given-names>A</given-names></name><name><surname>Conen</surname><given-names>KE</given-names></name><name><surname>Cai</surname><given-names>X</given-names></name><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Optimal coding and neuronal adaptation in economic decisions</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>1208</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-01373-y</pub-id><pub-id pub-id-type="pmid">29084949</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saez</surname><given-names>RA</given-names></name><name><surname>Saez</surname><given-names>A</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name><name><surname>Lau</surname><given-names>B</given-names></name><name><surname>Salzman</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Distinct roles for the amygdala and orbitofrontal cortex in representing the relative amount of expected reward</article-title><source>Neuron</source><volume>95</volume><fpage>70</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.012</pub-id><pub-id pub-id-type="pmid">28683271</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>McClure</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Anchors, scales and the relative coding of value in the brain</article-title><source>Current Opinion in Neurobiology</source><volume>18</volume><fpage>173</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2008.07.010</pub-id><pub-id pub-id-type="pmid">18692572</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soltani</surname><given-names>A</given-names></name><name><surname>De Martino</surname><given-names>B</given-names></name><name><surname>Camerer</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A range-normalization model of context-dependent choice: A new model and evidence</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002607</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002607</pub-id><pub-id pub-id-type="pmid">22829761</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N</given-names></name><name><surname>Schrater</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A value-relativistic decision theory predicts known biases in human preferences</article-title><conf-name>Proceedings of the Annual Meeting of the Cognitive Science Society</conf-name></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steverson</surname><given-names>K</given-names></name><name><surname>Brandenburger</surname><given-names>A</given-names></name><name><surname>Glimcher</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Choice-theoretic foundations of the divisive normalization model</article-title><source>Journal of Economic Behavior &amp; Organization</source><volume>164</volume><fpage>148</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/j.jebo.2019.05.026</pub-id><pub-id pub-id-type="pmid">32076358</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname><given-names>S</given-names></name><name><surname>Cross</surname><given-names>L</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Elucidating the underlying components of food valuation in the human orbitofrontal cortex</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1780</fpage><lpage>1786</lpage><pub-id pub-id-type="doi">10.1038/s41593-017-0008-x</pub-id><pub-id pub-id-type="pmid">29184201</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tom</surname><given-names>SM</given-names></name><name><surname>Fox</surname><given-names>CR</given-names></name><name><surname>Trepel</surname><given-names>C</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of loss aversion in decision-making under risk</article-title><source>Science</source><volume>315</volume><fpage>515</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1126/science.1134239</pub-id><pub-id pub-id-type="pmid">17255512</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toyoizumi</surname><given-names>T</given-names></name><name><surname>Kaneko</surname><given-names>M</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Modeling the dynamic interaction of Hebbian and homeostatic plasticity</article-title><source>Neuron</source><volume>84</volume><fpage>497</fpage><lpage>510</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.036</pub-id><pub-id pub-id-type="pmid">25374364</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tremblay</surname><given-names>L</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Relative reward preference in primate orbitofrontal cortex</article-title><source>Nature</source><volume>398</volume><fpage>704</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1038/19525</pub-id><pub-id pub-id-type="pmid">10227292</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troscianko</surname><given-names>J</given-names></name><name><surname>Osorio</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A model of colour appearance based on efficient coding of natural images</article-title><source>PLOS Computational Biology</source><volume>19</volume><elocation-id>e1011117</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011117</pub-id><pub-id pub-id-type="pmid">37319266</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turrigiano</surname><given-names>GG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The dialectic of Hebb and homeostasis</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>372</volume><elocation-id>20160258</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0258</pub-id><pub-id pub-id-type="pmid">28093556</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valerio</surname><given-names>R</given-names></name><name><surname>Navarro</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Optimal coding through divisive normalization models of V1 neurons</article-title><source>Network</source><volume>14</volume><fpage>579</fpage><lpage>593</lpage><pub-id pub-id-type="pmid">12938772</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>B</given-names></name><name><surname>Ke</surname><given-names>W</given-names></name><name><surname>Guang</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Yin</surname><given-names>L</given-names></name><name><surname>Deng</surname><given-names>S</given-names></name><name><surname>He</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>He</surname><given-names>T</given-names></name><name><surname>Zheng</surname><given-names>R</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>T</given-names></name><name><surname>Luan</surname><given-names>G</given-names></name><name><surname>Lu</surname><given-names>HD</given-names></name><name><surname>Zhang</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Shu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Firing frequency maxima of fast-spiking neurons in human, monkey, and mouse neocortex</article-title><source>Frontiers in Cellular Neuroscience</source><volume>10</volume><elocation-id>239</elocation-id><pub-id pub-id-type="doi">10.3389/fncel.2016.00239</pub-id><pub-id pub-id-type="pmid">27803650</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wark</surname><given-names>B</given-names></name><name><surname>Lundstrom</surname><given-names>BN</given-names></name><name><surname>Fairhall</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Sensory adaptation</article-title><source>Current Opinion in Neurobiology</source><volume>17</volume><fpage>423</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2007.07.001</pub-id><pub-id pub-id-type="pmid">17714934</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>TB</given-names></name><name><surname>Burke</surname><given-names>CJ</given-names></name><name><surname>Nebe</surname><given-names>S</given-names></name><name><surname>Preuschoff</surname><given-names>K</given-names></name><name><surname>Fehr</surname><given-names>E</given-names></name><name><surname>Tobler</surname><given-names>PN</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Testing models at the neural level reveals how the brain computes subjective value</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2106237118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2106237118</pub-id><pub-id pub-id-type="pmid">34686596</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Choice variability and suboptimality in uncertain environments</article-title><source>Current Opinion in Behavioral Sciences</source><volume>11</volume><fpage>109</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2016.07.003</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamada</surname><given-names>H</given-names></name><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Tymula</surname><given-names>A</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Free choice shapes normalized value signals in medial orbitofrontal cortex</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>162</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-02614-w</pub-id><pub-id pub-id-type="pmid">29323110</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zimmermann</surname><given-names>J</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Louie</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multiple timescales of normalized value coding underlie adaptive choice behavior</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>3206</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-05507-8</pub-id><pub-id pub-id-type="pmid">30097577</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Supplementary material</title><sec sec-type="appendix" id="s8-1"><title>How efficient is efficient value synthesis</title><p>In what follows, we unpack the impact of efficient value synthesis using an exemplar numerical simulation of an ANN operating value synthesis (as described in Equations 3 to 6). For the sake of clarity, we will focus on a toy example made of 8 input units (4 for the gain sublayer and 4 for the loss sublayer) and 4 integration units. We assume that the network has been set to construct a readout value that is close to the objective gamble’s EV, as defined by decision theory. We thus initialize the ANN parameters (including connectivity weights) using a random perturbation of ideal population codes, and then train the ANN to output EV over a unitary range of prospective gains and losses. .</p><p>As a reference point, we estimate the ensuing units’ receptive fields, in terms of the units’ response <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> output to any <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> pair of admissible prospective gains and losses. Note that we can associate a value to each point in that space <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>G</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. This enables us to characterize the relationship between EV and each integration unit in terms of their average response to the subset of pairs of prospective gain and loss that lie along iso-value lines. Note that this relationship is statistical in essence (as opposed to causal), since units respond to prospective gains and losses (as opposed to EV). Nevertheless, this relationship may be strong (in a statistical sense) because the network has been trained to signal value (which is constructed as a weighted sum of integration units’ responses). <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> below summarizes this analysis.</p><p>One can see that each integration unit has a receptive field that spans a specific range of prospective gains and losses. In turn, it exhibits an idiosyncratic statistical relationship to EV. Note that these relationships are slightly ambiguous (variations of response outputs within EV bins). This is because units typically respond nonlinearly to prospective gains and losses. Nevertheless, despite those multiple nonlinearities, the ANN’s readout value profile is clearly linear in the prospective gains and losses that compose gambles. <italic>Finally</italic> the ANN readout value’s sensitivities to prospective gains and losses match their theoretical values (namely: 0.5), which simply means that the network has been correctly trained. .</p><p>We then simulate self-organized plasticity according to <xref ref-type="disp-formula" rid="equ17">Equation 17</xref> over two restricted domains of prospective gains and losses: (i) the ‘narrow’ domain is symmetrical and such that the spanned range of gains is the same as that of losses, and (ii) the ‘wide’ domain is asymmetrical and such that the spanned range of gains is twice that of losses. Finally, we quantify the efficiency of value synthesis, both before and after self-organized plasticity has reshaped the system’s connectivity, in terms of the expected log steepness of units’ activation functions (<xref ref-type="disp-formula" rid="equ9">Equation 9</xref>) and in terms of the system’s resilience to neural noise. We measure the latter by reading out the noisy value response of the network to gain/loss pairs spanning the corresponding domain, having added Gaussian neural noise to integration units’ activity patterns with variances ranging from 1/256 to 2 (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>), and quantifying the rate of ‘stable choices,’ i.e., gamble decisions that are identical to those taken without neural noise.</p><p>The results of this analysis are summarized in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> below.</p><p>One can see that self-organized plasticity has increased the average steepness of units’ activation functions. This is reassuring, since it was derived to operate a gradient ascent on this metric. More importantly, self-organized plasticity tends to increase the system’s resilience to neural noise on integration units.</p></sec></sec><sec sec-type="appendix" id="s9"><title>What does self-organized plasticity do to the network?</title><p>To address this question, we first reproduce the analysis of <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, this time after self-organized plasticity has modified the network connectivity, while being exposed to the ‘wide.’ domain above (see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3:</xref> below).</p><p>Comparing <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> and <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3:</xref> shows that the receptive fields of integration units have been altered, even beyond the domain of prospective gains and losses that were spanned during efficient integration (except, maybe for unit #3, which underwent very small changes in its connectivity to the feature layer of the network). In particular, units #1 and #2 are now mostly sensitive to losses, whereas unit #4 has become mostly sensitive to gains. In turn, this eventually distorted the readout value profile. Importantly, the readout value profile now shows, within the spanned domain of prospective gains and losses, distorted sensitivities to gains and losses. More precisely, the sensitivity to losses <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is now about twice that of gains <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Self-organized plasticity over this domain of prospective gains and losses would thus eventually yield loss aversion.</p><p>One can also see that self-organized plasticity has changed the statistical relationship between integration units’ responses and gamble EVs. In brief, all integration units now show stronger response variations across the spanned range of EVs, i.e., inputs to integration units now tend to span the non-saturating range of their activation function (this is what is measured in the left panel of <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). In addition, this relationship also tends to become more ambiguous, i.e., there are stronger variations of response outputs within EV bins. This is due to the induced distortion of the units’ receptive fields. Nevertheless, self-organized plasticity results in an apparent phenomenon of (partial) adaptation to the range of EVs. Practically speaking, should one attempt to detect those integration units that show a significant relationship with EV, one would conclude that self-organized plasticity seems to have ‘recruited’ integration units (that would otherwise show no strong covariation with EV). But here again, this phenomenon is only apparent, because EV is not an input to the network.</p><p>Now, we ask whether and how self-organized plasticity changed the information content within the integration layer. To do this, we rely upon RSA, which can be reproduced in an empirical setting where neural populations are sampled over an arbitrary domain of prospective gains and losses. We first measure the neural dissimilarity <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of each possible pairwise combination of gains and losses, in terms of the correlation (across integration units) between the corresponding response patterns, i.e.,: <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. By construction, this measure of dissimilarity is bounded between 0 (when the response patterns are colinear) and 2 (when they are anti-correlated). We then construct the RDM for EV as follows. Recall that each pair of prospective gain and loss belongs to a given EV bin. The RDM element that corresponds to a pair <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> of EV bins is estimated as the average dissimilarity <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> over all combinations for which <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>∈</mml:mo><mml:mi>E</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>∈</mml:mo><mml:mi>E</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. We also construct RDMs for prospective gains and losses similarly. If the network contains information about a given variable, then the corresponding RDM should show a diagonal pattern, such that the neural dissimilarity increases with the absolute difference between elements of the pair. Accordingly, we define the neural encoding strength of EV (resp., prospective gains or losses) in terms of the gradient of neural dissimilarity per unit of absolute difference of EV (resp., prospective gains or losses). This analysis can be reproduced after self-organized plasticity has modified the integration layer’s receptive fields, while exposing the ANN to the wide (asymmetrical) domain above. <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref> below summarizes the results of these analyses.</p><p>In brief, self-organized plasticity strongly strengthened the encoding of all variables, i.e., the ANN’s integration layer contains more information about EV and prospective gains/losses than before self-organized plasticity. Interestingly, although this network has been trained to perform value synthesis, the encoding of EVs after self-organized plasticity is relatively weaker than before (smaller variations of dissimilarity along directions orthogonal to iso-EV lines), at least when compared to prospective gains and losses. This is because changes in integration units’ receptive fields eventually reduced the redundancy between integration units, which now tend to decompose the array of feature units’ outputs into their independent sources of variations (here: prospective gains and losses). Nevertheless, as one can check in <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>, applying the readout weights on the integration units’ response still yields a reasonable readout value profile (that exhibits loss aversion).</p></sec><sec sec-type="appendix" id="s10"><title>Logistic regression of behavioral data: Postdiction and out-of-sample predictions</title><p>The logistic regression analysis of peoples’ sensitivity to gains and losses provides a trial-by-trial prediction of gamble acceptance, for each pair of prospective gain and loss. But are hard decisions (i.e. when EV is close to zero) as well explained as easy decisions (e.g., when there is a strong incentive to gamble)? To address this question, we binned trials according to EV deciles (see Methods), and measured the rate of the logistic model’s postdiction error (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>).</p><p>Note: hereafter, we refer to ‘postdiction”’ as model predictions on data that was used to fit the model’s parameters. In contrast, ‘out-of-sample predictions’ are model-proper predictions on yet unseen data.</p><p>One can see that the logistic regression model achieves relatively similar postdiction error profiles in both groups. In particular, easy decisions (either low or high EV) exhibit much lower postdiction errors than difficult decisions (EV around zero).</p><p>We also performed counterfactual model simulations: for each subject, we simulated the trial-by-trial gamble acceptances that would have been observed, under the logistic model, had this subject/model been exposed to the sequence of prospective gains and losses that each subject of the other group was exposed to (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>). Those out-of-sample predictions are inaccurate: within the EV range that are common to both groups, it wrongly predicts that the gambling rate should be higher in the wide-range group than in the narrow-range group. This is not surprising, because predictions of the logistic model are entirely determined by the pair of prospective gain and loss (and cannot exhibit contextual effects). In other words, for a given gamble (defined in terms of its constituent gain and loss), the logistic model makes an out-of-sample prediction that corresponds to its equivalent postdiction (or is an extrapolation of it, if performed outside the EV range it was originally trained with).</p><p>Finally, we asked whether inter-individual differences in gambling rate (within the common EV range) were better explained in terms of inter-individual differences in loss aversion (i.e. the ratio <inline-formula><mml:math id="inf143"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf144"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf145"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are estimates of behavioral sensitivity to gains and losses, respectively) or in terms of gambling bias (<inline-formula><mml:math id="inf146"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>). This is summarized in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C, D</xref>. In brief, loss aversion negatively correlates with gambling rate within the common EV range (wide gain range group: <italic>r</italic>=0.59, p&lt;10<sup>–3</sup>, narrow gain range group: <italic>r</italic>=0.26, p=0.067, both groups together: <italic>r</italic>=0.53, p&lt;10<sup>–3</sup>).</p></sec><sec sec-type="appendix" id="s11"><title>Representational dissimilarity matrices within OFC subregions</title><p>In the main text, we summarize the information content within OFC subregions in terms of the gradients of neural dissimilarity per unit of prospective gains, losses, or EV. For the sake of completeness, we report below the underlying representational dissimilarity matrices, where trials have been binned according to prospective gains (gain-RDMs, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>), losses (loss-RDMs, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>), or EV (EV-RDMs, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>).</p><p>Note that a strong neural encoding of gains would correspond to strong variations in neural dissimilarity along directions orthogonal to iso-distance lines. Such RDMs would show a clear bandwise-diagonal structure, where neural dissimilarity would increase when moving away from the main diagonal. One can see that gain-RDMs within the lateral part of BA11 are the closest to this ideal situation.</p><p>Here again, loss-RDMs within the lateral part of BA11 show the strongest encoding of prospective losses. Interestingly, the loss-RDM of BA11 for the narrow gain range group seems to exhibit a block structure, such that trials seem to be partitioned into two classes according to whether they lie either below or above the median loss. This suggests that the encoding of losses is not, strictly speaking, linear. Nevertheless, this does not confound the analysis of encoding strength which we report in the main text.</p><p>Except in the lateral part of BA11, all EV-RDMs exhibit antidiagonal elements with weak neural dissimilarity. This means that multivariate fMRI patterns of trials that correspond to either very low or very high EV are similar to each other. This is the reason why the average neural dissimilarity tends to eventually <italic>decrease</italic> for extreme absolute EV distances (<xref ref-type="fig" rid="fig7">Figure 7</xref> of the main text).</p></sec><sec sec-type="appendix" id="s12"><title>Univariate analysis of fMRI data</title><p>In the main text, we report the results of multivariate (RSA) analyses of fMRI data, in five distinct subregions of the orbitofrontal cortex. For completeness, we also performed univariate analyses, having summarized the activity within OFC subregions in terms of the average trial-by-trial BOLD response over voxels (see Methods section for parametric estimation of trial-by-trial BOLD responses), and corrected the ensuing univariate responses for between-session confounding effects.</p><p>Here, we rely on two distinct general linear models or GLMs (<xref ref-type="bibr" rid="bib26">Friston et al., 1995</xref>). All models incorporated only one event per trial, which was the onset of the gamble presentation, convolved with a canonical hemodynamic response function, as well as with its temporal derivative (<xref ref-type="bibr" rid="bib29">Hopfinger et al., 2000</xref>). In the first GLM, we used two parametric modulations of the trial epoch regressor: namely prospective gains (G) and losses (L). We regressed the average trial-by-trial BOLD response against prospective gains and losses concurrently, using a GLM including an offset (constant term). The ensuing GLM parameter estimates measure the within-subjects gradient of BOLD responses per unit of prospective gains and losses. In the second GLM, we used only one parametric modulation, i.e. the gamble’s expected value EV=0.5*(G-L). Here, the ensuing GLM parameter estimates measure the within-subject’s gradient of BOLD response per unit of EV. Note that gain, loss and EV regressors were mean-centered but not rescaled to allow for a proper between-group comparison of neural sensitivity to these factors. For both regression analyses, we reported the ensuing parameter estimates at the group level and performed within-group and between-group statistical significance tests using standard one-sample and two-sample F-tests, respectively. The results of these analyses are summarized in <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref> below.</p><p>One can see that EV is significantly encoded in both groups of subjects in subregions BA14 and BA32. At the very least, the latter result survives the correction for multiple comparisons across OFC subregions. This reproduces established univariate results regarding the encoding of value in the fMRI literature. However, although some regions also show a significant encoding of prospective losses, nowhere in the OFC is there a significant encoding of both gains and losses, in both groups of subjects. This is strikingly different from the multivariate fMRI analyses results summarized in <xref ref-type="fig" rid="fig4">Figures 4</xref>—<xref ref-type="fig" rid="fig6">6</xref> of the main text, which clearly exhibit stronger statistical power. We note that, by construction, these two analyses are orthogonal to each other. This is because we chose to measure neural dissimilarity in terms of Pearson correlations across voxels, which are invariant under isotropic (within OFC subregions) affine transformations of voxel-wise trial series.</p></sec><sec sec-type="appendix" id="s13"><title>Bayesian priors on ANNs’ parameters</title><p>We fit each candidate ANNs to observed trial-by-trial gamble decision sequences using a dedicated Bayesian approach, which requires setting specific prior distributions on model parameters. These priors distributions are summarized on <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref> below.</p><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Parameters' priors for biologically constrained artificial neural networks (ANNs).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Parameter</th><th align="left" valign="top">Distributions</th><th align="left" valign="top">Rational</th></tr></thead><tbody><tr><td align="left" valign="top">Firing rate threshold</td><td align="left" valign="top"><inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>j</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top">Regular tiling of inputs</td></tr><tr><td align="left" valign="top">Activation function slope</td><td align="left" valign="top"><inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msup><mml:mtext>with</mml:mtext><mml:mspace width="thinmathspace"/><mml:mi>θ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top">Partially overlapping tiling of inputs</td></tr><tr><td align="left" valign="top">Initial connectivity</td><td align="left" valign="top"><inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top">EV population code</td></tr><tr><td align="left" valign="top">Value readout weights</td><td align="left" valign="top"><inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top">EV population code</td></tr><tr><td align="left" valign="top">Plasticity magnitude</td><td align="left" valign="top"><inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msup><mml:mtext>with</mml:mtext><mml:mspace width="thinmathspace"/><mml:mi>θ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top">Non informative prior</td></tr><tr><td align="left" valign="top">Plasticity rate</td><td align="left" valign="top"><inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mtext>with</mml:mtext><mml:mi>θ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top">Non informative prior</td></tr></tbody></table></table-wrap><p>All parameter notations are defined in the Methods section of the main text. Note that we performed all the analyses using the VBA academic freeware (<xref ref-type="bibr" rid="bib17">Daunizeau et al., 2014</xref>). Although this toolbox only handles Gaussian prior distributions, native (Gaussian) VBA parameters can be passed through arbitrary mappings prior to entering model computations. This enables VBA to enforce any required constraint (see e.g. <xref ref-type="bibr" rid="bib18">Daunizeau, 2017</xref>). This is the case here for the slopes of activation functions, as well as for the self-organized plasticity magnitude and rate parameters. In <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>, θ denotes VBA native parameters: they are given Gaussian prior distributions, and then passed through the appropriate nonlinear mapping to enforce positivity or bounding constraints.</p><p>In addition to the prior distributions for ANN parameters given in <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref> above, we set the number of attribute-specific and attribute-integration units to <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (per sublayer) and <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. We also rescaled the ANN’s inputs (i.e. prospective gains and losses) such that they lie within the unit interval. To ensure an unbiased comparison between decision attributes and/or between groups of participants, this rescaling is invariant across decision attributes and groups, and such that the upper bound of the unit interval corresponds to 150% of the maximum prospective gain or loss (here: 40$). This also prevents artifactual ceiling effects in the population code of the attribute layer (whose units with the highest firing rate thresholds always remain weakly activated).</p></sec><sec sec-type="appendix" id="s14"><title>ANNs’ behavioral postdiction and prediction accuracy</title><p>We fit the static and plastic ANN models to each subject’s sequence of gamble decisions. In what follows, we provide summary statistics of our ANN-based behavioral data analyses.</p><p>In addition to the balanced accuracy scores given in the main text, <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref> below gives the group average percentage of explained behavioral variance (R<sup>2</sup>) and its standard deviation (across participants) for each model (including the logistic regression model, for comparison purposes), and each group.</p><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Mean R<sup>2</sup> and its standard deviation for each model, for both groups.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top"/><th align="left" valign="top" colspan="2">Narrow range group</th><th align="left" valign="top" colspan="2">Wide range group</th></tr></thead><tbody><tr><td align="left" valign="top"/><td align="left" valign="top">Mean</td><td align="left" valign="top">Std</td><td align="left" valign="top">Mean</td><td align="left" valign="top">Std</td></tr><tr><td align="left" valign="top">Logistic</td><td align="char" char="." valign="top">0.74</td><td align="char" char="." valign="top">0.02</td><td align="char" char="." valign="top">0.66</td><td align="char" char="." valign="top">0.02</td></tr><tr><td align="left" valign="top">Static ANN</td><td align="char" char="." valign="top">0.73</td><td align="char" char="." valign="top">0.02</td><td align="char" char="." valign="top">0.62</td><td align="char" char="." valign="top">0.03</td></tr><tr><td align="left" valign="top">Plastic ANN</td><td align="char" char="." valign="top">0.75</td><td align="char" char="." valign="top">0.02</td><td align="char" char="." valign="top">0.66</td><td align="char" char="." valign="top">0.02</td></tr></tbody></table></table-wrap><p>In brief, all models achieve similar fit accuracies, in both groups, and no pairwise comparison between models reaches statistical significance.</p><p>But do the models yield similar types of postdiction errors? To address this question, we bin trials according to EV deciles, prior to averaging the rate of postdiction error across subjects. <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> below summarizes the accuracy of behavioral postdiction as a function of gambles’ expected value, for all ANNs.</p><p>As was the case for the logistic regression model, ANNs exhibit high fit accuracy for easy decisions (extreme EVs) and lower fit accuracy for hard decisions (EV around 0).</p></sec><sec sec-type="appendix" id="s15"><title>Parameter estimates of plastic ANNs</title><p>Of particular interest in our ANN-based analysis of behavioral data are the parameters that control the self-organized plasticity rule of efficient value synthesis (plasticity magnitude α and rate β parameters). <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref> below shows the empirical histograms of parameter estimates, for both groups of participants.</p><p>Overall, empirical distributions of parameter estimates are qualitatively similar in both groups of participants. When comparing groups of participants with respect to either α or β parameter estimates, nothing reaches statistical significance.</p><p>We also asked whether fitted plasticity parameters explained inter-individual differences in observed loss aversion, as measured using the logistic model (see <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2C, D</xref>). Nothing really stands out for plasticity rates, but the situation is quite different for plasticity magnitudes. Within the wide gain range group, there is a significant correlation between plasticity magnitudes and loss aversion across subjects (<italic>r</italic>=0.47, p&lt;10<sup>–3</sup>), while this correlation is negative within the narrow gain range group (<italic>r</italic>=−0.32, p=0.024). That the sign of this correlation is different in both groups makes sense, given that the effect of self-organized plasticity is to decrease loss aversion within the narrow gain range group, whereas, if anything, it tends to increase it within the wide range group (see <xref ref-type="fig" rid="fig4">Figure 4D</xref> in the main text).</p></sec><sec sec-type="appendix" id="s16"><title>On the diversity of response profiles in ANNs’ integration units</title><p>OFC neurons are notoriously diverse in their response profile, but a consistent finding is that, in the context of value-based decision-making, they can be classified in terms of so-called ‘choice cells,’ ‘chosen value cells’, and ‘offer value cells’ (<xref ref-type="bibr" rid="bib52">Padoa-Schioppa and Assad, 2006</xref>; <xref ref-type="bibr" rid="bib53">Padoa-Schioppa and Assad, 2008</xref>). Given that this can be considered a pre-requisite for any computational model of value integration in the OFC, we asked whether ANNs reproduce this known property of OFC neurons.</p><p>For each subject, we thus tested whether the response of integration units correlates (across trials) with choice, chosen value, and/or gamble value, where value is defined as the weighted sum of gains and losses (according to the static logistic model parameter estimates). Integration units are then classified in terms of which variable it correlate most with (but none of these labels is assigned if the best correlation is not significant). In brief, ‘choice units’ show response outputs that vary in a quasi-categorical manner with value (i.e. that discriminate trials in which the value of gambling is either positive or negative), ‘chosen value units’ exhibit a ReLU-like relationship with value (null when the value of gambling is negative, and increasing with value when it is positive), and ‘offer value units’ show a quasi-linear relationship with value. The results of this analysis are summarized in <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref> below.</p><p>In previous electrophysiological experiments, the detection rate of ‘offer value,’ ‘chosen value,’ and ‘choice’ cells within OFC neurons depends upon the delay to stimulus onset. This is because the detection analysis is typically performed at each time point within a given peristimulus time window. We thus extracted the frequency of ‘offer value,’ ‘chosen value,’ and ‘choice’ cells detected at OFC neurons’ response peak, i.e., about 300 msec after stimulus onset (see Figure 4 in <xref ref-type="bibr" rid="bib52">Padoa-Schioppa and Assad, 2006</xref>). For comparison purposes, we then normalized these estimated frequencies to remove non-responsive OFC neurons (see horizontal grey bars in <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>). We find that, although HP-ANN’s integration units were not at all designed to encode these quantities, they eventually reproduce the known response variability observed in OFC neurons (although few units are eventually classified as ‘choice cells’ in the narrow gain range group). This means that our ANN models reproduce the known diversity of response profiles within OFC neurons. Importantly, the response profiles are almost identical for both groups. In particular, we find that about 34% of integration units are classified as ‘chosen value’ cells. This is interesting, because the computational role of these units is in fact exactly the same as that of units that are classified as ‘offer value’ cells: together, they form a population code for the subjective value of gambling. In other words, this classification is not directly relevant for guessing the underlying computational role of integration units.</p></sec><sec sec-type="appendix" id="s17"><title>Value synthesis under efficient coding of gains and losses</title><p>In the main text, we assumed that neural noise would be acting on the output of integration units. But it may also be acting on its inputs, or equivalently on the outputs of the attribute units:<disp-formula id="equ18"><label>(A1)</label><mml:math id="m18"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>υ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msubsup><mml:mover><mml:mi>x</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mover><mml:mi>x</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf155"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> is some (uncontrollable) neural noise that competes with the ‘utile’ component <inline-formula><mml:math id="inf156"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math></inline-formula> of the responses of attributes units.</p><p>Similarly to <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>, this also induces an information loss <inline-formula><mml:math id="inf157"><mml:mi>I</mml:mi><mml:mi>L</mml:mi></mml:math></inline-formula>:<disp-formula id="equ19"><label>(A2)</label><mml:math id="m19"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>I</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>u</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In our framework, we do not consider how brain systems upstream of the attribute layers extract gain and loss information from visual stimuli and project it onto attribute layers. Rather, we assume that prospective gains and losses are encoded into population codes within attribute layers. Nevertheless, we can still model efficient coding at the level of attribute units. More precisely, efficient coding of gains and losses can then be achieved by modifying the response properties of attribute units to decrease the information loss <inline-formula><mml:math id="inf158"><mml:mi>I</mml:mi><mml:mi>L</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ19">Equation A2</xref> i.e.,:<disp-formula id="equ20"><label>(A3)</label><mml:math id="m20"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>I</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf159"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> is the 2 × 1 vector of location and scale parameters of attribute units’ activation functions.</p><p>This is but a proxy for the impact of self-organized plasticity upstream attribute layers. Note that there is no nonlocal term in <xref ref-type="disp-formula" rid="equ20">Equation A3</xref>, because the entropy term <inline-formula><mml:math id="inf160"><mml:mi>H</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is beyond the network’s control.</p><p>Let <inline-formula><mml:math id="inf161"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> be the change of location and scale parameters at trial or time <inline-formula><mml:math id="inf162"><mml:mi>t</mml:mi></mml:math></inline-formula>. Similarly to efficient value synthesis, an online implementation of <xref ref-type="disp-formula" rid="equ20">Equation A3</xref> is operated as follows:<disp-formula id="equ21"><label>(A4)</label><mml:math id="m21"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>≈</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mi>β</mml:mi><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf163"><mml:mi>β</mml:mi></mml:math></inline-formula> (note: <inline-formula><mml:math id="inf164"><mml:mn>0</mml:mn><mml:mo>§amp;lt;</mml:mo><mml:mi>β</mml:mi><mml:mo>§amp;lt;</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>) controls the exponential decay of past samples’ weights in the moving average operator (<xref ref-type="disp-formula" rid="equ13 equ14">Equations 13-14</xref> in the main text).</p><p>If the ANN is equipped with sigmoid activation functions, then simple analytical derivations show that <xref ref-type="disp-formula" rid="equ21">Equation A4</xref> reduces to the following update rule for location and scale parameters of attribute units:<disp-formula id="equ22"><label>(A5)</label><mml:math id="m22"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mi>β</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ22">Equation A5</xref> is the equivalent of <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> of the main text, i.e., it describes how the properties of attribute units within the network should modify their response properties to operate efficient coding of attribute inputs. In what follows, we simply refer to <xref ref-type="disp-formula" rid="equ22">Equation A5</xref> as <italic>efficient coding</italic> of attributes. Note that <xref ref-type="disp-formula" rid="equ22">Equation A5</xref> would also modify the receptive fields of integration units within the ANN. In principle, it could thus yield effects that are qualitatively similar to those of efficient value integration.</p><p>But what are the neural and behavioral impacts of this efficient coding mechanism? To address this question, we reproduced the same analyses as for efficient value synthesis.</p><p>First, we asked whether <xref ref-type="disp-formula" rid="equ22">Equation A5</xref> would produce apparent value range adaptation (<xref ref-type="fig" rid="fig2">Figure 2</xref> in the main text) in integration units. We randomized the trained connections of ANNs that operate value synthesis prior to exposing them to four different series of 256 decision trials made of prospective gains and losses with a predefined range. As before, we considered two ranges (either narrow or wide) for both prospective gains and losses, and exposed the ANNs to each of the 2 × 2 range combinations. We then averaged the activity of integration units, after having binned trials into EV deciles. We repeated this procedure 1000 times, and <xref ref-type="fig" rid="fig10">Figure 10</xref> in the main text summarizes the results of this analysis. One can see that the lower and upper limits of units’ mean responses are tied to the bounds of the spanned EV range, as is the case for static value synthesis (see <xref ref-type="fig" rid="fig2">Figure 2C and D</xref> in the main text). In turn, the slope of the relationship between EV and integration units’ mean responses does not seem to vary strongly with the range of spanned EVs. In other words, the efficient coding mechanism in <xref ref-type="disp-formula" rid="equ22">Equation A5</xref> does not induce apparent value range adaptation in the univariate response of integration units.</p><p>Second, we asked whether and how the shape of spanned the gain/loss domain modifies the neural and behavioral sensitivities to prospective gains and losses. We thus performed the same series of Monte-Carlo simulations as for efficient value synthesis. We simulated the response of the ANN, modified according to <xref ref-type="disp-formula" rid="equ22">Equation A5</xref> to operate efficient coding of attributes, to a series of 256 gambles, after having randomized the trained connectivity within the network. We systematically varied the spanned range of prospective gains and losses, and measured the behavioral sensitivities to gains and losses (in terms of the gradient of the readout value per unit of gain or loss) and the neural sensitivity to EV (in terms of the gradient of the integration layer’s neural dissimilarity per unit of EV). We repeated this procedure 1000 times, and <xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1</xref> below summarizes the results of this analysis.</p><p>In brief, the behavioral impact of efficient coding is qualitatively similar to that of efficient value synthesis. More precisely, the behavioral sensitivity to gains (resp., losses) decreases as the spanned range of gains (resp., losses) increases (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1D, E</xref>). Note that cross-attribute spillover effects are also present here, and to a greater extent than under efficient value synthesis. In addition, efficient coding of attributes induces a similar effect on behavioral loss aversion, which follows the ratio of the spanned range of gains relative to the spanned range of losses (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1F</xref>). This implies that behavioral observations alone would not disambiguate efficient value synthesis (as described in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> in the main text) from efficient coding of attributes (as described in <xref ref-type="disp-formula" rid="equ22">Equation A5</xref>).</p><p>Also, the neural encoding strength of EV in the integration layer is similarly impacted by the shape of the spanned gain/loss domain. That is, the neural encoding strength of EV decreases when the spanned range of either gains or losses increases. Thus, this effect does not disambiguate efficient value synthesis from efficient value coding of attributes. However, the neural sensitivity to gains and losses does not react to the shape of the spanned gain/loss domain as they do under the efficient value synthesis scenario (see <xref ref-type="fig" rid="fig10">Figure 10C, D</xref> in the main text). The difference here is twofold. First, the cross-attribute spillover effect dominates, i.e., the most salient effect is that the neural sensitivity to gains (resp. losses) increases when the spanned range of <italic>losses</italic> (resp., gains) increases. Second, if anything, the within-attribute effect seems to be opposite to that of efficient value synthesis. That is, the neural sensitivity to gains (resp. losses) <italic>increases</italic> when the spanned range of gains (resp., losses) increases. At the very least, this holds for small to intermediate ranges of gains and losses; this effect reverses for extreme ranges of gains and losses (most likely because of artefactual ceiling effects). This is interesting, because this is clearly at odds with the neural predictions of the efficient value synthesis scenario. Importantly, this directly contradicts the fMRI data sampled in the lateral part of Brodman area 11 (see <xref ref-type="fig" rid="fig5">Figure 5</xref> in the main text).</p><p>We then reproduced the same model-based analyses as with the efficient value synthesis scenario.</p><p>First, we fit the ANN model equipped with efficient coding of attributes to each participant’s trial-by-trial gamble decisions, and extract counterfactual out-of-sample behavioral predictions when exposing the fitted ANNs to the gamble series of the other group. We also performed the sliding window analysis to investigate the temporal dynamics of loss aversion, as captured by the scenario of efficient coding of attributes. The results of these analyses are summarized in <xref ref-type="fig" rid="fig10s2">Figure 10—figure supplement 2</xref> below.</p><p>In brief, behavioral postdictions under the scenario of efficient coding of attributes are as accurate as under the efficient value synthesis scenario. In particular, the postdicted dynamics of loss aversion exhibit the same qualitative properties (compared with <xref ref-type="fig" rid="fig7">Figure 7C</xref>). However, out-of-sample behavioral predictions are not as convincing: although the model does predict a behavioral change, predicted gamble rates are still higher for the wide gain range group (mean gambling rate=0.58±0.02) than for the narrow gain range group (mean gambling rate=0.48±0.02). This observation is partially confirmed when comparing the absolute out-of-sample prediction error of both plastic ANN models. In brief, we found that out-of-sample behavioral predictions under efficient coding of attributes were significantly less accurate than under efficient value synthesis for the wide range group (p=0.006, <italic>F</italic>=12.8), but not for the narrow range group (p=0.31, <italic>F</italic>=1.15).</p><p>Second, we reproduced the same RSA analyses as before, i.e., we evaluated the similarity between the ANN model and fMRI data sampled in the same five OFC subregions. The results of these analyses are summarized in <xref ref-type="fig" rid="fig10s3">Figure 10—figure supplement 3</xref> below.</p><p>No OFC subregion reaches statistical significance in both groups, for all types of RDMs. Importantly, this also holds for the lateral part of Brodman area 11 (only 5 out of 8 tests are significant). We note that, irrespective of the type of RDM considered, nowhere in the OFC is the comparison between both plastic models statistically significant.</p></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80979.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>National Institute on Drug Abuse Intramural Research Program</institution><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2020.09.08.287714" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2020.09.08.287714"/></front-stub><body><p>This valuable manuscript proposes a neural network mechanism for range adaptation for value-based decision making. The authors present solid evidence for the proposed mechanism.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80979.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>National Institute on Drug Abuse Intramural Research Program</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2020.09.08.287714">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2020.09.08.287714v3">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Synaptic plasticity in the orbitofrontal cortex explains how risk attitude adapts to the range of risk prospects&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Michael Frank as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) As you can see in the individual comments, all reviewers thought that your paper addresses an important topic. However, a central concern raised by all reviewers was that a substantial part of the model performance is driven by the activation function – yet, throughout the manuscript, you mainly discuss the role of Hebbian plasticity and largely ignore the effects of the activation function. There was agreement among reviewers that this is not warranted and that your manuscript requires substantial reframing, clarification, justification, and discussion. Reviewers would expect an adequately revised manuscript to look quite different from the current version, with an equal focus on all factors that allow the model to account for the observed changes across different ranges.</p><p>2) There are additional comments in the individual critiques that would be important to address, specifically regarding aspects of the analysis and interpretation.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1) Please explain on page 4 of the introduction why Hebbian plasticity should lead to spill-over effects. This is not very intuitive.</p><p>2) Interpretation of the null findings in the fMRI data (page 4 of results) is problematic because it is unclear whether they reflect a true null effect or a lack of sensitivity. Although this is true for all null results, it is particularly problematic for re-analyses, as the study was not designed or powered to test this question. It would be best to remove these results from the paper.</p><p>3) There is a fundamental difference between gaussian and sigmoidal activation functions. It would be important to include an adequate discussion of the assumptions and implications of these different functions in the main text.</p><p>4) The out-of-range predictions of the best model (Figure 3, lower-right panel, HP-ANN (gauss)) are not very convincing when considering the entire EV range. Model performance should be compared for the entire EV range, not just the common range. What does this mean for the proposed mechanism?</p><p>5) The paper focuses on Hebbian plasticity as a mechanism for context effects but judging from Figure 3, the choice of activation function has a comparable effect. Indeed, HP does almost nothing for models with sigmoidal activation functions, and HP only improves the out-of-range prediction for the common EV range but does very little for the uncommon range. A more balanced presentation that also discusses the type of activation function as a mechanism of adaptation would be important.</p><p>6) Are the RSA results in Figure 4 based on the ANNs with gaussian activation function? It would be important to show RSA results for all 4 ANNs in Figure 4, so it is possible to compare the results across models. Also, please add labels to the plot axes.</p><p>7) The proportion of offer and chosen value neurons shown in Figure 6 is opposite to what has been reported in the OFC of non-human primates. It would be good to discuss this discrepancy. Also, are the same proportions found for all 4 ANNs?</p><p>8) Figure 4: the spheres shown for BA11 are in the posterior medial rather than the lateral OFC. Please double-check the anatomical location of these ROIs. Are they really in the lateral OFC? Also, it would be good to provide center coordinates for the ROIs. In general, it would be important to better describe how the ROIs were generated. What were the search terms used in NeuroQuery? Also, NeuroQuery generates meta-analytic activation maps, not maps of anatomical structures. It would be better to use actual anatomical ROIs for fMRI data analysis.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1) Range adaptation is not shown directly in ANN units. Specific questions:</p><p>a) How do ANN units respond across different input values? How variable are these response patterns across units?</p><p>b) How do these response patterns vary with range?</p><p>c) Do these patterns (at the individual unit or mean level) resemble neuronal data from the orbitofrontal cortex (OFC)? Should it be expected to?</p><p>d) The example units that are shown (Figure 7) seem potentially different from previously reported neuronal data from OFC. For example, the dynamic range of the model covers only a narrow subset of the input space and varies substantially across conditions, whereas firing rates in OFC neurons tend to span the full range of possible values, and firing rates change only slightly between conditions [ref,ref]. Is this discrepancy just a side effect of showing results in terms of loss units rather than expected value? How should we interpret this apparent discrepancy?</p><p>e) ANN unit responses are compared to neuron classes observed in OFC (Figure 6). What does the mean ANN unit in each category look like, and how does this compare to the OFC responses referenced?</p><p>2) Modeling results focus on Hebbian networks with gaussian activation functions. Specific questions:</p><p>a) Is there a physiological motivation for the model, or is this primarily for mathematical convenience?</p><p>b) Do the main qualitative results (range effect on risk sensitivity) require a non-monotonic activation function?</p><p>c) How do the properties of the response functions (saturation and (non)-monotonicity) affect responses of ANN units?</p><p>d) There are visible differences between model behavior for Hebbian networks with sigmoidal and gaussian activation functions. How should these differences be interpreted? Does this lead to any predictions or constraints on what physiological implementation is consistent with this algorithm?</p><p>3) Out-of-sample predicted choices in the Hebbian model with gaussian activation function seem unintuitive for more extreme parts of the value range. E.g. for the out-of-sample wide range predictions, the model seems to over-predict risk aversion to the point that choice probabilities saturate and ~0.6 for increasingly high-value options. For the narrow range, out-of-sample prediction behavior even appears to be non-monotonic, leading to increased acceptance of extremely high loss options.</p><p>a) How should this be interpreted?</p><p>b) Does this depend on model parameters like update rate or covariance threshold?</p><p>c) In these parts of the range, how does the Hebbian model compare to alternate models, such as the other ANNs or logistic regression?</p><p>4) The relationship between neural responses and ANN activity relies on representational similarity analysis (RSA). However, significant RDM correlations could arise as a byproduct of the fact that both ANN output and BOLD activity in select regions correlate with behavioral choice patterns. Is there evidence that the correlation between Hebbian ANNs and BOLD activation reflects more than average choice patterns?</p><p>5) The authors make the strong claim that this model is the mechanistic explanation for adaptation in orbitofrontal cortex, but there is little comparison with previous models. Divisive normalization and other forms of adaptation to the value range are discarded based on a qualitative argument from the behavioral data. However, given that the Hebbian ANNs also produce some counterintuitive behavioral predictions, it is not obvious that they are better at accounting for observed patterns in neuronal adaptation or behavior. Addressing the following questions could clarify whether there is an argument for Hebbian ANNs over alternate mechanisms of adaptation:</p><p>a) How do behavioral predictions and RSA results from HP-ANNs compare quantitatively to results from other models of adaptation, including models of adaptation at the input stage?</p><p>b) Can Hebbian ANNs account for other previously observed patterns of behavior across different value ranges, such as stability of relative values across ranges in two-option choices and range-dependent decoy effects []?</p><p>c) Are there specific predictions that arise from Hebbian networks that could be tested in later work and used to differentiate between competing models?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>1) The paper assumes loss aversion as the primary behavioral factor, which is fine. However, it may be worth briefly mentioning the limitation that with the present experimental design it is impossible to dissociate loss aversion from risk aversion (see e.g. Williams et al., 2021, PNAS).</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Efficient value synthesis in the orbitofrontal cortex: how does peoples' risk attitude adapts to the range of risk prospects?&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Michael Frank (Senior Editor) and a Reviewing Editor.</p><p>All reviewers agreed that the revised manuscript has been improved. However, given the revised manuscript is essentially a fundamentally new manuscript, there is a new set of comments that would need to be addressed, as outlined below.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>I appreciate the authors' effort and dedication in re-working the manuscript. The revised manuscript is a fundamentally new and improved paper with new conclusions. I believe it could make an important contribution to the field and I remain enthusiastic. However, given most of the manuscript has changed, I have new comments that I think should be addressed</p><p>1) In general, the manuscript is quite long with extensive supplementary analyses. I believe the paper could be streamlined by highlighting the most important aspects and reducing the extent to which details are discussed in the main text.</p><p>2) All relevant information to understand the plots should be embedded within the figure rather than just the figure legends. It is cumbersome for readers to constantly have to consult the legends to understand what is shown in the figure. For instance, there is no label for the different colored plots (red/back, red/blue) or line styles in ANY of the figures. Also, some legends (Figure 4) refer to a color code, but this code is not provided. Moreover, there are no axis ticks and/or axis tick labels in some of the panels in Figures 3, 5, 6, 7, 9, 10 (top row), 12 (top row), 13 (top row), S4, S5, S6, S7, S10, and S12. Several figures don't include a color bar (e.g., Figure S5-7). Please carefully revise all figures. Note that this point was already raised in the previous round of reviews, but it was not addressed.</p><p>3) Figure 3D – Loss aversion over time: Instead of running a separate between group comparisons for each time-point, it would be more appropriate to run a single two-way ANOVA with within-subject factor time and between-subject factor group. A significant group-by-time interaction would support the conclusion that loss aversion diverges between the two groups across time.</p><p>4) Why was the lateral OFC (area 47/12) not included here, given that work by Suzuki et al. 2017 suggests that lateral OFC represents attribute-specific values?</p><p>5) It is not fully clear what is plotted in Figure 5-7. Are these the averages across all RDM cells with a certain δ G/L/EV in Figures S5-S7? Does this include the δ G/L/EV = 0? How is neural coding strength defined in the lower rows? Tick labels would have helped.</p><p>6) Figure 9 shows that subject-specific ANNs correlate with subject-specific RDMs. To claim that these models capture individual patterns of OFC activity, it would be important to show that these correlations exceed those with group-level ANNs. Moreover, to claim specificity for plastic ANNs, it would be necessary to show superiority of predictions from plastic vs static ANNs.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Efficient value synthesis in the orbitofrontal cortex explains how loss aversion adapts to the ranges of gain and loss prospects&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Michael Frank (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The authors have addressed my comments. I think the manuscript makes an interesting contribution to the field.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Overall the proposed model presents an interesting possible explanation for types of context-dependent loss aversion. The manuscript has improved over the course of revision and will be a worthwhile contribution to the literature. I have a few remaining comments that would help improve the clarity and accessibility of the results if they can be addressed before publication, but these are relatively minor.</p><p>1) While the figures have been substantially improved, several are still missing a description of the colors in the figure or legend, and instead have the placeholder phrase &quot;(color code)&quot; in the figure legend.</p><p>2) I appreciate your response to my previous comment about &quot;undoing&quot; adaptation (R2 Comment 3), but it is not clear to me in your response whether you are describing the computational role of &quot;offer value&quot; units in your model specifically, or just giving a hypothetical scenario. If I understand right, your model produces choice via a comparison of Vt for two options, and the&quot;offer value&quot; units are part of the integration layer (i.e. an input to Vt rather than the signal being compared directly). Is the idea that this would lead to stable preferences even without &quot;undoing&quot; adaptation downstream? Or would your model predict that preferences do shift in responses to &quot;offer value&quot; adaptation, and you suspect that past studies may not have been able to see it? Or are you just trying to say that there are several hypothetical possibilities, and in the specific task you are modeling it is not necessary to modify the weights? (As an aside, I also disagree with the argument that Rustichini et al. are interpreting a null result as evidence of absence – they start by predicting how preferences would change if choices arose from a simple comparison of offer value firing rates, then show that actual choice behavior does not match this prediction.)</p><p>3) In line 456 you discuss the spatial specificity of results, but unless I'm missing something this doesn't involve a direct comparison between regions. It may be worth reducing this claim.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Thank you for a responsive revision, I have no further points other than that the paper would still benefit from careful spell-checking.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80979.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) As you can see in the individual comments, all reviewers thought that your paper addresses an important topic. However, a central concern raised by all reviewers was that a substantial part of the model performance is driven by the activation function – yet, throughout the manuscript, you mainly discuss the role of Hebbian plasticity and largely ignore the effects of the activation function. There was agreement among reviewers that this is not warranted and that your manuscript requires substantial reframing, clarification, justification, and discussion. Reviewers would expect an adequately revised manuscript to look quite different from the current version, with an equal focus on all factors that allow the model to account for the observed changes across different ranges.</p><p>2) There are additional comments in the individual critiques that would be important to address, specifically regarding aspects of the analysis and interpretation.</p></disp-quote><p>First of all, we would like to thank you for your insightful and constructive criticism. Some of your comments helped us understand that our previous interpretations were unsatisfactory, and we eventually changed our mind on many important aspects of our work. In particular, extensive posthoc numerical simulations made us realize that Hebbian plasticity, as described in the previous version of the manuscript, <italic>does not</italic> induce value range adaptation in most contexts. This meant that our previous results were somehow illusory, in the sense that they would not generalize to other types of datasets. We thus started from scratch, and derived another -novel- computational framework, which is grounded on formal models of efficient coding. We will detail this below. The resulting manuscript is entirely different in its content, and we hope to have significantly improved the quality of our contribution.</p><p>Second, we would like to apologize for the time it took us to revise this manuscript. The reason for this delay is twofold. First, the first author (Jules Brochard) first moved to a new lab for his postdoc, and then eventually quit academia. This implied that we could only progress very slowly. Second, we made so many changes – on both computational modelling and data analysis sides – that this revision really required a lot of time to complete. Nevertheless, we hope that the resulting work will satisfy most of your concerns. We reiterate that this would not have been possible without you making us think again about our work.</p><p>Before we respond to your comments point by point below, we would like to summarize our revisions. We believe this is necessary to clarify some of our responses.</p><p>As we mentioned above, raw Hebbian plasticity turns out not to be a good model for range adaptation. We realized this when attempting to address one of your questions, about <italic>why</italic> Hebbian plasticity would induce range adaptation. We first tried to derive analytical results regarding the impact of Hebbian plasticity: this did not work. We then resorted to numerical simulations on a wide range of conditions, but this eventually demonstrated that Hebbian plasticity does not yield value range adaptation. More precisely, it only does so in very specific settings of the ANN internal connectivity, and these settings cannot be summarized in a simple manner and/or justified from first principles. So what does this imply for our previous analyses? When fitting the plastic ANNs to empirical data, we unknowingly identified idiosyncratic variants of these settings, eventually extracting “Hebbian explanations” for range adaptation. But we now know that these explanations were, at best, anecdotal: they would not generalize.</p><p>This was a rather disappointing realization. At this point, we wondered how to move forward. We thus reversed the logic of our reasoning and asked: what sort of change in the ANN structure would eventually yield range adaptation? We were looking for a computational principle that would suggest <italic>why</italic> neurons would range-adapt, i.e. why this would be adapted. We took inspiration from the theoretical literature on efficient coding, which highlighted that range adaptation is the mechanism by which neurons minimize the information loss that is induced by their limited firing range. However, existing efficient coding models had been derived for perceptual brain systems, where neurons <italic>transmit</italic> the information that they receive. Put simply, the idea here is that a neuron’s input is the physical quantity that is signaled to the brain (e.g., light intensity within a certain frequency band), whereas the neuron’s output is the percept (e.g., perceived amount of red). In turn, range adaptation (to a neuron’s input signal) directly induces perceptual context-dependent effects. This is not case in our context: value is the outcome of an integration mechanism, over multiple (and possibly conflicting) decision-relevant information. We thus extended existing efficient coding models to ANNs that operate such <italic>value synthesis.</italic> More precisely, we show that a simple form of self-organized plasticity between the ANN’s attribute-specific and attribute-integration layers does mitigate the information loss induced by the limited firing range of neural units. In what follows, we refer to this as <italic>efficient value synthesis</italic>.</p><p>As you will see, this type of self-organized plasticity shares with Hebbian plasticity its simple form, i.e. connections progressively change as a function of the output responses of pairwise connected units (see Equation 2 in the revised manuscript, as well as its mathematical derivation in the revised Methods section). However, it is not, strictly speaking, Hebbian (i.e. it does not reinforce connections that yield co-activation of source and target units). If anything, it is similar to -though simpler than- ANN training rules underlying infomax variants of independent component analysis or ICA, which are essentially anti-Hebbian (Bell &amp; Sejnowski, 1995; Nadal, 1994). Importantly, and for the same reason than in ICA network models, it turns out that this type of self-organized plasticity is incompatible with Gaussian activation functions. When revising this work, we thus focused on sigmoidal activation functions.</p><p>In the revised manuscript, we first summarize the mathematical derivation of efficient value synthesis and highlight its neural and behavioral consequences (in terms of the sensitivity to decision-relevant attributes, i.e. here: prospective gains and losses). Importantly, we show that efficient value synthesis induces value range adaptation in a wide range of contexts. We then test these neural and behavioral predictions using model-free data analyses in five subregions of the OFC. Finally, we reperform our previous model-based analyses using ANNs with and without self-organized plasticity, and show that only the former do yield accurate out-of-sample predictions of neural and behavioral data. Note that we also compare this model to a simpler range adaptation model, which operates at the level of attributes.</p><p>These changes eventually translated in significant modifications in all sections of the manuscript (Intro, Results, Methods, Discussion and Supplementary Materials). In fact, we had to replace most of the content of the previous version of the manuscript. This implies that some of the raised reviewers’ comments and questions may not be relevant anymore.</p><p>Nevertheless, we tried to address each one of them below, while referring to the novel computational framework that propose here. In any case, we believe our work has been significantly strengthened, and we thank once again the reviewers for their insightful comments and constructive criticism.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1) Please explain on page 4 of the introduction why Hebbian plasticity should lead to spill-over effects. This is not very intuitive.</p></disp-quote><p>This is a fair point. In brief, numerical simulations show that neither does Hebbian plasticity induce value range adaptation, nor does it lead to spillover effects. This is not the case for efficient value synthesis. However, we have toned down the importance of spillover effects. This is because we don’t think they are a reliable signature of range adaptation in integration units. We understood this by deriving an efficient coding model that at the level of attributes: this model also predicts spillover cross-attribute effects. More precisely, what discriminates the two models is the direction of the within-attribute effects, whereas the cross-attribute spillover effects are similar.</p><p>Note that the mathematical derivation of the model of efficient coding of attributes, the summary of its neural and behavioral predictions, as well as the results of its related model- based data analyses are reported in the revised Supplementary Materials.</p><disp-quote content-type="editor-comment"><p>2) Interpretation of the null findings in the fMRI data (page 4 of results) is problematic because it is unclear whether they reflect a true null effect or a lack of sensitivity. Although this is true for all null results, it is particularly problematic for re-analyses, as the study was not designed or powered to test this question. It would be best to remove these results from the paper.</p></disp-quote><p>You are referring here to the null findings of univariate fMRI data analyses. We have now moved these results to the Supplementary Materials.</p><disp-quote content-type="editor-comment"><p>3) There is a fundamental difference between gaussian and sigmoidal activation functions. It would be important to include an adequate discussion of the assumptions and implications of these different functions in the main text.</p></disp-quote><p>Sigmoidal activation functions are a simple summary of the known physiological neural response to an electrical input: the firing rate of neurons is known to be both lower and upper bounded. In our previous manuscript, we also considered Gaussian activation functions. In brief, they are a phenomenological modelling assumption that captures neural receptive fields that span a bounded subregion of stimulus’ properties (e.g. spatial field of view in V1 neurons). We note that Gaussian activation function can be understood as the physiological output of a neuron that is reciprocally coupled with an inhibitory unit (where both have sigmoidal activation functions).</p><p>In any case, we do not consider Gaussian activation functions anymore in the revised manuscript. This is because, under Gaussian activation functions, the self-organized plasticity mechanism that mitigate information loss yields unstable dynamics. We comment on this point in the revised Discussion section (lines 536-561).</p><disp-quote content-type="editor-comment"><p>4) The out-of-range predictions of the best model (Figure 3, lower-right panel, HP-ANN (gauss)) are not very convincing when considering the entire EV range. Model performance should be compared for the entire EV range, not just the common range. What does this mean for the proposed mechanism?</p></disp-quote><p>In the previous version of the manuscript, out-of-sample predictions of ANNs equipped with Hebbian plasticity were indeed inaccurate outside the common EV range. In particular they exhibited a non-monotonic relationship between EV and gambling rate. This is not the case anymore when relying on ANNs endowed with self-organized plasticity that operates efficient value synthesis under sigmoidal activation functions (see Figure 7 in the revised manuscript). Of course, the ensuing behavioral out-of-sample predictions are still not perfect. More precisely, they tend to slightly under-predict the observed context-dependency effect of peoples’ risk attitude. Nevertheless, the accuracy of our out-of-sample predictions has clearly improved. In addition, we now provide complementary analytical results and model-free evidence (see below) that strengthen our computational claims.</p><disp-quote content-type="editor-comment"><p>5) The paper focuses on Hebbian plasticity as a mechanism for context effects but judging from Figure 3, the choice of activation function has a comparable effect. Indeed, HP does almost nothing for models with sigmoidal activation functions, and HP only improves the out-of-range prediction for the common EV range but does very little for the uncommon range. A more balanced presentation that also discusses the type of activation function as a mechanism of adaptation would be important.</p></disp-quote><p>You are right: in our previous version of the manuscript, Hebbian plasticity could only capture temporal range adaptation in peoples’ risk attitude when combined with Gaussian activation functions. Let us reiterate that, given that Hebbian plasticity only yields range adaptation under specific ANN connectivity patterns, this should be considered as anecdotal evidence for this mechanism. Since we do not consider Gaussian activation functions anymore, this comment is now irrelevant. Nevertheless, for the sake of completeness, we would like to clarify our previous analyses.</p><p>In brief, neither gaussian nor sigmoidal activation functions can induce temporal range adaptation effects by themselves: those can only be the outcome of dynamical mechanisms that change the network’s response over time (as is the case for efficient coding of attributes or efficient value synthesis). This is why we did not consider the form of the units’ activation functions as a potential cause for temporal range adaptation. If anything, it should be considered a potential moderator of the impact of dynamical mechanisms such as plastic changes in the ANN’s internal connectivity.</p><disp-quote content-type="editor-comment"><p>6) Are the RSA results in Figure 4 based on the ANNs with gaussian activation function? It would be important to show RSA results for all 4 ANNs in Figure 4, so it is possible to compare the results across models. Also, please add labels to the plot axes.</p></disp-quote><p>Again, we have abandoned Gaussian activation functions in the revised manuscript. Nevertheless, we do now report the results of both out-of-sample behavioral and neural predictions for all models. For RSA, the results of the efficient value synthesis (resp., efficient coding of attributes) scenario are summarized in Figure 8 of the main text (resp., Figure S12 of the Supplementary Materials).</p><p>We note that we have extended our previous RSA analyses. In addition to the quantification of trial-by-trial similarity between ANNs’ integration layer and fMRI activity patterns, we now also measure their similarity on gain-, loss- and EV-dependent RDMs (the latter are derived by binning trials according to either gain, loss or EV). We did this because we could obtain quantitative predictions regarding the related information content within the integration layer of ANNs operating efficient value synthesis (or efficient coding of attributes). This eventually provides more opportunities for evaluating the evidence strength for or against our candidate computational models. In particular, this allows us to perform model-free data analyses, which are designed to test between-group differences in the neural encoding strength of gains, losses and EVs.</p><disp-quote content-type="editor-comment"><p>7) The proportion of offer and chosen value neurons shown in Figure 6 is opposite to what has been reported in the OFC of non-human primates. It would be good to discuss this discrepancy. Also, are the same proportions found for all 4 ANNs?</p></disp-quote><p>This comment refers to our previous posthoc analysis of ANN integration units’, which showed that ANNs equipped with Hebbian plasticity reproduced the known diversity of coding properties in OFC neurons. In the revised version of our manuscript, we have reperformed this analysis (this time with ANNs operating efficient value synthesis), and it yields qualitatively similar results. Nevertheless, we have decided to tone down this point, and have moved these results to the Supplementary Materials.</p><disp-quote content-type="editor-comment"><p>8) Figure 4: the spheres shown for BA11 are in the posterior medial rather than the lateral OFC. Please double-check the anatomical location of these ROIs. Are they really in the lateral OFC? Also, it would be good to provide center coordinates for the ROIs. In general, it would be important to better describe how the ROIs were generated. What were the search terms used in NeuroQuery? Also, NeuroQuery generates meta-analytic activation maps, not maps of anatomical structures. It would be better to use actual anatomical ROIs for fMRI data analysis.</p></disp-quote><p>We agree that our previous ROI partition of the OFC was somehow arbitrary. In particular, although the functional definition of the vmPFC is established in the fMRI community, its anatomical definition remains vague. We have now re-performed the RSA analyses based upon standard Broadman areas of the OFC, in particular: BA11 (splat into its lateral and medial parts), BA13, BA14 and BA32. This parcellation is based upon masks obtained from the BRAINNETOME atlas (<ext-link ext-link-type="uri" xlink:href="https://atlas.brainnetome.org/">https://atlas.brainnetome.org/</ext-link>): it tiles the entire OFC, except its most lateral part (which is BA12). We now describe in full details how the ROIs were obtained from BRAINNETOME anatomical masks. We also provide the ROI barycenter coordinates in a Table (see revised Methods section).</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1) Range adaptation is not shown directly in ANN units. Specific questions:</p></disp-quote><p>We now include detailed analyses of ANN numerical simulations with and without self- organized plasticity. We describe these analyses in the Methods and Results section of the revised manuscript. In brief, they allow us to address all the concerns below, and more. We now summarize the results of these analyses when answering your comments:</p><disp-quote content-type="editor-comment"><p>a) How do ANN units respond across different input values? How variable are these response patterns across units?</p></disp-quote><p>As a reminder of the overall structure of the ANN, the first layer is divided into two sublayers, which receive prospective gains (resp., prospective losses). Units of the first layer (so-called “attribute units”) send their outputs to units of a second layer (so-called “integration units”).</p><p>Subjective value is readout from the output activity of this second layer.</p><p>In brief, attribute sublayers form a population code of their respective inputs, i.e. their units are selective of either prospective gains or prospective losses (depending on the specific sublayer), which they belong to. In contrast, integration units exhibit mixed selectivity, with heterogeneous response profiles across units. We refer the reviewer to Figures 10 and 12 of the revised manuscript for representative examples of integration units’ receptive fields (over the gain/loss domain).</p><disp-quote content-type="editor-comment"><p>b) How do these response patterns vary with range?</p></disp-quote><p>Under the efficient value synthesis scenario, attribute units do not change their response patterns with the range of either gains or losses. However, self-organized plasticity eventually modifies the receptive field of integration units. This modification tends to blur the tiling of the domain that is spanned by prospective gains and losses inputs. More importantly, one can show that this eventually modifies the information content within the ANN’s integration layer in a systematic manner. In particular, the encoding strength of prospective gains (resp., losses) decreases when the spanned range of gains (resp., losses) increases.</p><disp-quote content-type="editor-comment"><p>c) Do these patterns (at the individual unit or mean level) resemble neuronal data from the orbitofrontal cortex (OFC)? Should it be expected to?</p></disp-quote><p>Qualitatively speaking, integration units do exhibit response profiles that are reminiscent of typical OFC neurons electrophysiological activity during value-based decision making. For example, they reproduce the diversity of coding that has been repeatedly observed in OFC neurons during value-based decision making (cf., “offer value cells”, “chosen value cells”, and “choice cells”, see Figure S8 of the revised Supplementary Materials). More importantly, integration units also exhibit the known properties of value range adaptation in these same neurons (see Figure 2 in the revised Results section).</p><p>Now, whether this sort of ANN model produces “realistic” electrophysiological activity profiles beyond this kind of statistical relationship is questionable. The reason is twofold. First, they are agnostic w.r.t. within-trial temporal dynamics. Second, there is some level of arbitrariness in the modelling assumptions (cf., e.g., structural constraints) that cannot be finessed using either behavioral or neuroimaging data. What we argue is robust in these ANN models is the information content that they carry, which is distributed over the activity profiles of their artificial neural unit layers. This is the main reason why we resort to variants of RSA for comparing their predictions to multivariate fMRI activity patterns.</p><p>We now comment on these issues in the revised Discussion section (lines 521-535).</p><disp-quote content-type="editor-comment"><p>d) The example units that are shown (Figure 7) seem potentially different from previously reported neuronal data from OFC. For example, the dynamic range of the model covers only a narrow subset of the input space and varies substantially across conditions, whereas firing rates in OFC neurons tend to span the full range of possible values, and firing rates change only slightly between conditions [ref,ref]. Is this discrepancy just a side effect of showing results in terms of loss units rather than expected value? How should we interpret this apparent discrepancy?</p></disp-quote><p>This point is directly related to the above comment Re: whether one should expect the ANN units to resemble neuronal data from OFC. We note that Figure 7 (in the previous version of the manuscript) was meant as a schematic depiction of the Hebbian plasticity mechanism under a strawman “population code” scenario. In our revised manuscript, we have now replaced it with graphical summaries of numerical simulations of ANNs. As you will see, representative ANN units exhibit receptive fields that have a rather arbitrary shape (i.e., with mixed gain/loss selectivity ; see Figures 10 and 12 of the revised manuscript). However, the induced statistical relationship between integration units’ output response and value is much simpler (in particular, it tends to be rather monotonic). In addition, this relationship typically spans the full range of values, with firing rates that show small changes across conditions (cf. Figure 2).</p><disp-quote content-type="editor-comment"><p>e) ANN unit responses are compared to neuron classes observed in OFC (Figure 6). What does the mean ANN unit in each category look like, and how does this compare to the OFC responses referenced?</p></disp-quote><p>In our context, “choice units” show response outputs that vary in a quasi-categorical manner with value (i.e. that discriminate trials in which the value of gambling is either positive or negative), “chosen value units” exhibit a ReLU-like relationship with value (null when the value of gambling is negative, and increasing with value when it is positive), and “offer value units” show a quasi-linear relationship with value.</p><disp-quote content-type="editor-comment"><p>2) Modeling results focus on Hebbian networks with gaussian activation functions. Specific questions:</p><p>a) Is there a physiological motivation for the model, or is this primarily for mathematical convenience?</p></disp-quote><p>In brief, the Gaussian activation functions that we used in the previous version of this manuscript were a mathematical convenience. Having said this, a Gaussian activation function can be understood as the physiological output of an excitatory unit that is reciprocally coupled with an inhibitory unit (where both have sigmoidal activation functions).</p><p>But we have abandoned this type of activation function anyway…</p><disp-quote content-type="editor-comment"><p>b) Do the main qualitative results (range effect on risk sensitivity) require a non-monotonic activation function?</p></disp-quote><p>We now provide extensive theoretical analyses of adaptation effects induced by self- organized plasticity within the network. In principle, these results would generalize to any monotonic activation function. Importantly, this would not hold for non-monotonic activation functions, because those would induce unstable plasticity dynamics. This is the reason why we now only focus on sigmoidal activation functions.</p><disp-quote content-type="editor-comment"><p>c) How do the properties of the response functions (saturation and (non)-monotonicity) affect responses of ANN units?</p></disp-quote><p>Intuitively, units with sigmoidal activation functions will be generally more active than units with Gaussian activation functions. This is because their effective receptive field is not upper-bounded. For the same reason, they will exhibit less functional specificity than units with Gaussian activation functions. But again, this is now irrelevant.</p><disp-quote content-type="editor-comment"><p>d) There are visible differences between model behavior for Hebbian networks with sigmoidal and gaussian activation functions. How should these differences be interpreted? Does this lead to any predictions or constraints on what physiological implementation is consistent with this algorithm?</p></disp-quote><p>Let us first respond in the frame of our previous computational model and analyses. In brief, we had a priori expected that both types of units, when equipped with Hebbian plasticity, would exhibit range adaptation effects. We had thus hoped to show that the ability of the Hebbian mechanism to yield accurate out-of-sample choice predictions would not depend upon the type of underlying activation functions. This turned out not to be the case. Note that we did not believe that there was a physiological lesson to be learned here. Rather, we thought that this difference may be an unavoidable artefact of our data analysis procedure.</p><p>Recall that fitted ANN models can generalize across groups (or contexts) if and only if they neither underfit nor overfit the choice data (and hence accurately discriminate temporal changes in gambling value that are due to adaptation from unsystematic choice variations). Although we had no a priori reason to expect that ANNs with sigmoidal activation functions would be more prone to underfitting or overfitting, it is reasonable to think that the tendency to underfit/overfit choice data may depend upon the type of units’ activation function.</p><p>Now, although we do not use Gaussian activation functions in the revised manuscript, this point still deserves a further comment. As we highlighted before, efficient value synthesis yields unstable plasticity dynamics under non-monotonic (e.g. Gaussian) activation functions. To understand this, recall that the self-organized plasticity rule derives from aligning the connectivity change with the gradient of information loss w.r.t. connection strengths. This gradient explodes when inputs fall within domains where the derivative of the activation function approaches zero. This unavoidably happens with non-monotonic activation functions because the plasticity mechanism eventually focuses the weighted inputs within the vicinity of their mode. In other terms, one may argue that only monotonic activation functions are compatible with the efficient value synthesis scenario. We note that this kind of issue was already highlighted in computational studies of network models of ICA (Bell &amp; Sejnowski, 1995; Nadal, 1994). We now comment on this in the revised Discussion section (cf. lines 536-551).</p><disp-quote content-type="editor-comment"><p>3) Out-of-sample predicted choices in the Hebbian model with gaussian activation function seem unintuitive for more extreme parts of the value range. E.g. for the out-of-sample wide range predictions, the model seems to over-predict risk aversion to the point that choice probabilities saturate and ~0.6 for increasingly high-value options. For the narrow range, out-of-sample prediction behavior even appears to be non-monotonic, leading to increased acceptance of extremely high loss options.</p><p>a) How should this be interpreted?</p></disp-quote><p>This was an artefact of our previous data analysis procedure. Recall that ANNs effectively are a nonlinear mapping between their inputs (here: prospective gains and losses) and their outputs (here: gambling choices). This mapping was made of a mixture of basis functions, whose receptive fields typically tile the gain/loss domain over which the ANN is trained. This means that, without additional constraints, this mapping poorly generalizes outside the range of gain and loss inputs with which it was trained on. Having said this, our modified model shows much better-behaved out-of-sample predictions…</p><disp-quote content-type="editor-comment"><p>b) Does this depend on model parameters like update rate or covariance threshold?</p></disp-quote><p>No, this artefact did not depend upon model parameters.</p><disp-quote content-type="editor-comment"><p>c) In these parts of the range, how does the Hebbian model compare to alternate models, such as the other ANNs or logistic regression?</p></disp-quote><p>In this part of the range, logistic regression or ANNs with sigmoidal activation functions did generally better than ANNs with Gaussian activation functions. But this is not the case with the efficient value scenario (which relies upon sigmoidal activation functions).</p><disp-quote content-type="editor-comment"><p>4) The relationship between neural responses and ANN activity relies on representational similarity analysis (RSA). However, significant RDM correlations could arise as a byproduct of the fact that both ANN output and BOLD activity in select regions correlate with behavioral choice patterns. Is there evidence that the correlation between Hebbian ANNs and BOLD activation reflects more than average choice patterns?</p></disp-quote><p>This is a fair point: you are asking whether the similarity between the model and neural data may not simply be driven by the similarity between the model and choice data (e.g., a model that would not fit choice data at all would be very unlikely to be similar to neural data). We do agree that this is in principle possible here. For example, if, for some reason, static ANNs underfit choice data (at least when compared to plastic ANNs), then they might have a better chance to look like trial-by-trial fMRI signals in the OFC. A first hint here comes from Figure S3 in our revised manuscript: one can see that plastic and static ANNs yield very similar choice postdiction error rates. Therefore, they are unlikely to differ in terms of their behavioral explanatory power.</p><p>But our model-free fMRI data analyses provide, in our opinion, a stronger counter argument here. In brief, only ANNs that operate efficient value synthesis do predict the observed change in the information content induced by the difference in spanned gain range. In particular, the information content about prospective gains and losses (as opposed to integrated value) is conditionally independent from choice data. Note that ANNs that operate efficient coding of attributes do not make accurate neural predictions (cf. Figures S10 and S12 in the Supplementary Marterials).</p><disp-quote content-type="editor-comment"><p>5) The authors make the strong claim that this model is the mechanistic explanation for adaptation in orbitofrontal cortex, but there is little comparison with previous models. Divisive normalization and other forms of adaptation to the value range are discarded based on a qualitative argument from the behavioral data. However, given that the Hebbian ANNs also produce some counterintuitive behavioral predictions, it is not obvious that they are better at accounting for observed patterns in neuronal adaptation or behavior. Addressing the following questions could clarify whether there is an argument for Hebbian ANNs over alternate mechanisms of adaptation:</p><p>a. How do behavioral predictions and RSA results from HP-ANNs compare quantitatively to results from other models of adaptation, including models of adaptation at the input stage?</p></disp-quote><p>This is an important point. In the revised version of the manuscript, we now consider a model of adaptation at the level of attributes. Its mathematical derivation, its neural and behavioral predictions, as well as the ensuing model-based data analyses are reported in the revised Supplementary Materials (cf. “value synthesis under efficient coding of gains and losses”). At the behavioral level, it makes predictions that are similar to the efficient value synthesis scenario. At the neural level however, it – somehow surprisingly- exhibits distinct properties. In particular, it <italic>does not</italic> induce value range adaptation in integration units (see Figure S9).</p><p>Moreover, it predicts that the neural encoding strength of gains (resp., losses) <italic>increases</italic> when the range of spanned gains (resp., losses) increases. This clearly contradicts our model-free analyses of fMRI data in the OFC.</p><p>Regarding divisive normalization, we do not see how it would provide a simple alternative explanation for this dataset. We acknowledge that divisive normalization provides an elegant explanation for specific forms of instantaneous context-dependency effects. For example, when deciding between three options, divisive normalization would predict irrational decoy effects (Louie et al., 2013; Steverson et al., 2019). But these kinds of models are not standard explanations for temporal range adaptation effects. To our knowledge, there exists only one variant of divisive normalization models that aims at capturing temporal range adaptation (Zimmermann et al., 2018). The model is very complex and relies on coupling slow and fast winner-take-all networks. Critically, it treats option values as inputs, and does not consider situations where subjective value has to be constructed from the integration of multiple decision attributes. We believe that it is beyond the scope of this work to extend this kind of model to networks that operate value synthesis.</p><disp-quote content-type="editor-comment"><p>b) Can Hebbian ANNs account for other previously observed patterns of behavior across different value ranges, such as stability of relative values across ranges in two-option choices and range-dependent decoy effects []?</p></disp-quote><p>Addressing this sort of issues would require extending the model to situations in which two (or three) option values are simultaneously represented and/or compared by the network. These additional mechanisms are beyond the scope of the current work (but we will be addressing these points in subsequent publications).</p><disp-quote content-type="editor-comment"><p>c) Are there specific predictions that arise from Hebbian networks that could be tested in later work and used to differentiate between competing models?</p></disp-quote><p>One possibility here is to target, using invasive neurophysiological approaches, the mechanisms that underlie plasticity (e.g., LTP and LTD). For example, specifically suppressing LTP and/or LTD (in value-coding OFC neurons) should prevent or distort value range adaptation. One could then compare choice behavior immediately after exposing subjects to (high or low) ranges of gains and/or losses, with and without LTP/LTD suppression.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>1) The paper assumes loss aversion as the primary behavioral factor, which is fine. However, it may be worth briefly mentioning the limitation that with the present experimental design it is impossible to dissociate loss aversion from risk aversion (see e.g. Williams et al., 2021, PNAS).</p></disp-quote><p>This is a fair point and we have included a comment on this in the revised Discussion section.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>All reviewers agreed that the revised manuscript has been improved. However, given the revised manuscript is essentially a fundamentally new manuscript, there is a new set of comments that would need to be addressed, as outlined below.</p></disp-quote><p>First of all, we thank the Editor and reviewers for providing us with another opportunity to improve our work. We have tried to address each comment (see our response below). In particular, we included a few additional results reports, modified most figures, moved some material from the Methods section into the Supplementary Material, and included novel discussion points in the Discussion section. We hope that you will agree with these changes.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>I appreciate the authors' effort and dedication in re-working the manuscript. The revised manuscript is a fundamentally new and improved paper with new conclusions. I believe it could make an important contribution to the field and I remain enthusiastic. However, given most of the manuscript has changed, I have new comments that I think should be addressed</p></disp-quote><p>Thank you very much for your positive appreciation of our work.</p><disp-quote content-type="editor-comment"><p>1) In general, the manuscript is quite long with extensive supplementary analyses. I believe the paper could be streamlined by highlighting the most important aspects and reducing the extent to which details are discussed in the main text.</p></disp-quote><p>We understand your comment. However, we found it difficult to remove material in the main text without harming the readability of the manuscript. In the hope of conciliating these two imperatives, we have moved some of the content of the Methods section in the Supplementary Materials (see also our response to point #12 of reviewer #2). We hope that you will find our revised manuscript concise enough.</p><disp-quote content-type="editor-comment"><p>2) All relevant information to understand the plots should be embedded within the figure rather than just the figure legends. It is cumbersome for readers to constantly have to consult the legends to understand what is shown in the figure. For instance, there is no label for the different colored plots (red/back, red/blue) or line styles in ANY of the figures. Also, some legends (Figure 4) refer to a color code, but this code is not provided. Moreover, there are no axis ticks and/or axis tick labels in some of the panels in Figures 3, 5, 6, 7, 9, 10 (top row), 12 (top row), 13 (top row), S4, S5, S6, S7, S10, and S12. Several figures don't include a color bar (e.g., Figure S5-7). Please carefully revise all figures. Note that this point was already raised in the previous round of reviews, but it was not addressed.</p></disp-quote><p>We have gone through all the manuscript’s Figures and modified them to improve readability as much as possible. In particular, we have now inserted self-contained axis ticks, colorbars and legends whenever they were missing. Note that most Figures number have changed, because we had to tie Figures in the supplementary material to Figures in the main text.</p><disp-quote content-type="editor-comment"><p>3) Figure 3D – Loss aversion over time: Instead of running a separate between group comparisons for each time-point, it would be more appropriate to run a single two-way ANOVA with within-subject factor time and between-subject factor group. A significant group-by-time interaction would support the conclusion that loss aversion diverges between the two groups across time.</p></disp-quote><p>This is a fair suggestion. We tried this but found no significant time-by-group interaction (p=0.43, F=0.61, R2=0.6%), which is why we report separate group comparisons. We now also report this null finding prior to reporting separate group comparisons.</p><disp-quote content-type="editor-comment"><p>4) Why was the lateral OFC (area 47/12) not included here, given that work by Suzuki et al. 2017 suggests that lateral OFC represents attribute-specific values?</p></disp-quote><p>In brief, we had no particular reason for not including area 47/12. We simply considered OFC subregions that were close neighbors to (the typical fMRI definition of) the ventromedial PFC. This effectively disqualified BA12/47, which is positioned on the most lateral part of the OFC. Having said this, we do not think that including BA47/12 is critical for our empirical demonstration. This is because fMRI activity patterns in BA11 already validate the model’s predictions (and we essentially rely on other OFC subregions as control ROIs). Given that inserting this additional ROI would mean adding more material to the main text and induce further delays in the revision of this work, we thus decided not to do it. We hope you understand and agree with us.</p><disp-quote content-type="editor-comment"><p>5) It is not fully clear what is plotted in Figure 5-7. Are these the averages across all RDM cells with a certain δ G/L/EV in Figures S5-S7? Does this include the δ G/L/EV = 0? How is neural coding strength defined in the lower rows? Tick labels would have helped.</p></disp-quote><p>Yes, upper panels in Figure 5 to 7 show the group mean of within-subject RDMs binned by differences in variables of interest (i.e.: G, L or EV). And yes, the group mean of G/L/EV-specific RDMs can be eyeballed in the Supplementary Material (Figure 5 —figure supplement 1, Figure 6 —figure supplement 1 and Figure 7 —figure supplement 1, respectively). In our previous manuscript, the y-axis limits were kept identical across OFC subregions, but this was not apparent because we had removed tick labels. We have included them all now.</p><disp-quote content-type="editor-comment"><p>6) Figure 9 shows that subject-specific ANNs correlate with subject-specific RDMs. To claim that these models capture individual patterns of OFC activity, it would be important to show that these correlations exceed those with group-level ANNs. Moreover, to claim specificity for plastic ANNs, it would be necessary to show superiority of predictions from plastic vs static ANNs.</p></disp-quote><p>These are fair comments. We tried both suggestions but failed to detect significant differences (both for within-subject versus group correlations and for plastic versus static ANN variants). We thus have modified our results report to tone down these claims. In brief, we simply use this analysis to afford evidence that ANNs that operate value synthesis (whether plastic or static) do yield reasonably realistic predictions regarding fMRI activity patterns within the OFC.</p><p>Note that, for the sake of completeness, we modified Figure 9 to enable readers to eyeball and compare the RSA results of both plastic and static models.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>Overall the proposed model presents an interesting possible explanation for types of context-dependent loss aversion. The manuscript has improved over the course of revision and will be a worthwhile contribution to the literature. I have a few remaining comments that would help improve the clarity and accessibility of the results if they can be addressed before publication, but these are relatively minor.</p><p>1) While the figures have been substantially improved, several are still missing a description of the colors in the figure or legend, and instead have the placeholder phrase &quot;(color code)&quot; in the figure legend.</p></disp-quote><p>We have now modified all relevant figure legends to provide an explicit description of the color code (e.g., for Figure 3: “[…] A: the neural encoding strength of gains (color code: from blue -minimal encoding strength- to yellow -maximal encoding strength-) is shown as a function of the spanned range of losses (x-axis, range increases from left to right) and gains (y-axis, range increases from bottom to top)). […]”.</p><disp-quote content-type="editor-comment"><p>2) I appreciate your response to my previous comment about &quot;undoing&quot; adaptation (R2 Comment 3), but it is not clear to me in your response whether you are describing the computational role of &quot;offer value&quot; units in your model specifically, or just giving a hypothetical scenario. If I understand right, your model produces choice via a comparison of Vt for two options, and the&quot;offer value&quot; units are part of the integration layer (i.e. an input to Vt rather than the signal being compared directly). Is the idea that this would lead to stable preferences even without &quot;undoing&quot; adaptation downstream? Or would your model predict that preferences do shift in responses to &quot;offer value&quot; adaptation, and you suspect that past studies may not have been able to see it? Or are you just trying to say that there are several hypothetical possibilities, and in the specific task you are modeling it is not necessary to modify the weights? (As an aside, I also disagree with the argument that Rustichini et al. are interpreting a null result as evidence of absence – they start by predicting how preferences would change if choices arose from a simple comparison of offer value firing rates, then show that actual choice behavior does not match this prediction.)</p></disp-quote><p>In brief, we were arguing (i) that Rustichini et al’s argument is statistically unsound, and (ii) that a variant of our model would actually predict no behavioral change despite apparent value range adaptation in “offer value cells”. We take the latter as a relevant counter-example for how puzzling the result was in the first place. Now, since we believe this point is the most important, we have dropped our former statistical criticism in the revised manuscript. In addition, we have modified this paragraph to clarify our reasoning as much as possible:</p><p>Intriguingly however, value range adaptation in “offer value cells” had been observed without any significant behavioral preference change. Under the assumption that preferences between offers derive from the direct comparison of output signals from “offer value cells”, this is surprising. To solve this puzzle, later theoretical work proposed that value range adaptation is somehow “undone” downstream value coding in the OFC (Padoa-Schioppa &amp; Rustichini, 2014). In our context, this would suggest that readout weights (<italic>w^(k)</italic> in Equation 2) would compensate for value-related adaptation, effectively thwarting the behavioral consequences of self-organized plasticity between attribute and integration layers. However, this reasoning critically relies upon the assumed computational role of “offer value cells”. In fact, this puzzle may effectively dissolve under other scenarios of how “offer value cells” contribute to decision making. Recall that this null result was obtained in a decision context where choice options were characterized in terms of the type of offer (i.e. juices that differ w.r.t. palatability), whose quantity was systematically varied. Here, value synthesis would effectively aggregate two attributes, namely palatability and quantity. Under this view, “offer value cells” simply are integration units that show a certain form of mixed selectivity, whereby units’ sensitivity to quantity strongly depends upon palatability. At this point, one needs to consider candidate scenarios of how the OFC may operate value synthesis for multiple options in a choice set. A possibility is that the OFC is automatically computing the value of the option that is currently under the attentional focus (Lebreton et al., 2009; Lopez-Persem et al., 2020), while storing the value of previously attended options within an orthogonal population code (Pessiglione &amp; Daunizeau, 2021). In principle, this implies that the OFC is wired such that it can handle arbitrary switches in attentional locus without compromising the integration of option-specific attributes. In this scenario, integration units (including those that look like “offer value cells”) would adapt to the range of all incoming attribute signals, irrespective of which option in the choice set is currently attended. In turn, “offer value cells” would look like they are only partially adapting to the value range of a given offer type (Burke et al., 2016; Conen &amp; Padoa-Schioppa, 2019). More importantly, to the extent that between-attribute spillover effects are negligible, changes in the range of offer quantities would distort the readout value profile along the quantity dimension without altering the palatability dimension. This would effectively leave the relative preference between offer types unchanged. Of course, this is only one candidate scenario among many. Nevertheless, we would still argue that the behavioral consequences of range adaptation in “offer value cells” actually depend upon their underlying computational role.</p><disp-quote content-type="editor-comment"><p>3) In line 456 you discuss the spatial specificity of results, but unless I'm missing something this doesn't involve a direct comparison between regions. It may be worth reducing this claim.</p></disp-quote><p>You are right. To make sure our claims regarding anatomical specificity are not over-interpreted, we have modified the relevant paragraph in the Discussion section as follows:</p><p>Although this clearly aligns with our model-free fMRI data analysis results, we do not claim that the evidence we provide regarding the anatomical location of value synthesis generalize beyond decision contexts that probe peoples’ loss aversion. In fact, our main claim is about whether and how efficient value synthesis operates within the OFC, as opposed to which specific subregion of the OFC drives the adaptation of loss aversion and/or related behavioral processes.</p></body></sub-article></article>