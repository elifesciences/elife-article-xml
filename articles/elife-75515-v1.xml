<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">75515</article-id><article-id pub-id-type="doi">10.7554/eLife.75515</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neural synchronization is strongest to the spectral flux of slow music and depends on familiarity and beat salience</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-260567"><name><surname>Weineck</surname><given-names>Kristin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3204-860X</contrib-id><email>kristin.weineck@ae.mpg.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-263950"><name><surname>Wen</surname><given-names>Olivia Xin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8845-1233</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-155060"><name><surname>Henry</surname><given-names>Molly J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2284-8884</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000rdbk18</institution-id><institution>Research Group “Neural and Environmental Rhythms”, Max Planck Institute for Empirical Aesthetics</institution></institution-wrap><addr-line><named-content content-type="city">Frankfurt am Main</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04cvxnb49</institution-id><institution>Goethe University Frankfurt, Institute for Cell Biology and Neuroscience</institution></institution-wrap><addr-line><named-content content-type="city">Frankfurt am Main</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05g13zd79</institution-id><institution>Department of Psychology, Toronto Metropolitan University</institution></institution-wrap><addr-line><named-content content-type="city">Toronto</named-content></addr-line><country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Jensen</surname><given-names>Ole</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03angcq70</institution-id><institution>University of Birmingham</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>12</day><month>09</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e75515</elocation-id><history><date date-type="received" iso-8601-date="2021-11-12"><day>12</day><month>11</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-07-25"><day>25</day><month>07</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-11-30"><day>30</day><month>11</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.11.29.470396"/></event></pub-history><permissions><copyright-statement>© 2022, Weineck et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Weineck et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-75515-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-75515-figures-v1.pdf"/><abstract><p>Neural activity in the auditory system synchronizes to sound rhythms, and brain–environment synchronization is thought to be fundamental to successful auditory perception. Sound rhythms are often operationalized in terms of the sound’s amplitude envelope. We hypothesized that – especially for music – the envelope might not best capture the complex spectro-temporal fluctuations that give rise to beat perception and synchronized neural activity. This study investigated (1) neural synchronization to different musical features, (2) tempo-dependence of neural synchronization, and (3) dependence of synchronization on familiarity, enjoyment, and ease of beat perception. In this electroencephalography study, 37 human participants listened to tempo-modulated music (1–4 Hz). Independent of whether the analysis approach was based on temporal response functions (TRFs) or reliable components analysis (RCA), the spectral flux of music – as opposed to the amplitude envelope – evoked strongest neural synchronization. Moreover, music with slower beat rates, high familiarity, and easy-to-perceive beats elicited the strongest neural response. Our results demonstrate the importance of spectro-temporal fluctuations in music for driving neural synchronization, and highlight its sensitivity to musical tempo, familiarity, and beat salience.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>When we listen to a melody, the activity of our neurons synchronizes to the music: in fact, it is likely that the closer the match, the better we can perceive the piece. However, it remains unclear exactly which musical features our brain cells synchronize to. Previous studies, which have often used ‘simplified’ music, have highlighted that the amplitude envelope (how the intensity of the sounds changes over time) could be involved in this phenomenon, alongside factors such as musical training, attention, familiarity with the piece or even enjoyment. Whether differences in neural synchronization could explain why musical tastes vary between people is also still a matter of debate.</p><p>In their study, Weineck et al. aim to better understand what drives neuronal synchronization to music. A technique known as electroencephalography was used to record brain activity in 37 volunteers listening to instrumental music whose tempo ranged from 60 to 240 beats per minute. The tunes varied across an array of features such as familiarity, enjoyment and how easy the beat was to perceive. Two different approaches were then used to calculate neural synchronization, which yielded converging results.</p><p>The analyses revealed that three types of factors were associated with a strong neural synchronization. First, amongst the various cadences, a tempo of 60-120 beats per minute elicited the strongest match with neuronal activity. Interestingly, this beat is commonly found in Western pop music, is usually preferred by listeners, and often matches spontaneous body rhythms such as walking pace. Second, synchronization was linked to variations in pitch and sound quality (known as ‘spectral flux’) rather than in the amplitude envelope. And finally, familiarity and perceived beat saliency – but not enjoyment or musical expertise – were connected to stronger synchronization.</p><p>These findings help to better understand how our brains allow us to perceive and connect with music. The work conducted by Weineck et al. should help other researchers to investigate this field; in particular, it shows how important it is to consider spectral flux rather than amplitude envelope in experiments that use actual music.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>Neural synchronization</kwd><kwd>music</kwd><kwd>temporal response function</kwd><kwd>tempo</kwd><kwd>reliable component analysis</kwd><kwd>spectral flux</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>ERC-STG-804029 BRAINSYNC</award-id><principal-award-recipient><name><surname>Henry</surname><given-names>Molly J</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004189</institution-id><institution>Max-Planck-Gesellschaft</institution></institution-wrap></funding-source><award-id>Max Planck Research Group Grant</award-id><principal-award-recipient><name><surname>Henry</surname><given-names>Molly J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Two different analysis approaches for measuring neural synchronization to natural music revealed strongest synchronization to musical spectral flux as opposed to the more commonly used amplitude envelope.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Neural activity synchronizes to different types of rhythmic sounds, such as speech and music (<xref ref-type="bibr" rid="bib21">Doelling and Poeppel, 2015</xref>; <xref ref-type="bibr" rid="bib44">Nicolaou et al., 2017</xref>; <xref ref-type="bibr" rid="bib18">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="bib31">Kösem et al., 2018</xref>) over a wide range of rates. In music, neural activity synchronizes with the beat, the most prominent isochronous pulse in music to which listeners sway their bodies or tap their feet (<xref ref-type="bibr" rid="bib60">Tierney and Kraus, 2015</xref>; <xref ref-type="bibr" rid="bib46">Nozaradan et al., 2012</xref>; <xref ref-type="bibr" rid="bib34">Large and Snyder, 2009</xref>; <xref ref-type="bibr" rid="bib21">Doelling and Poeppel, 2015</xref>). Listeners show a strong behavioral preference for music with beat rates around 2 Hz (here, we use the term <italic>tempo</italic> to refer to the beat rate). The preference for 2 Hz coincides with the modal tempo of Western pop music (<xref ref-type="bibr" rid="bib40">Moelants, 2002</xref>) and the most prominent frequency of natural adult body movements (<xref ref-type="bibr" rid="bib36">MacDougall and Moore, 2005</xref>). Indeed, previous research showed that listeners perceive rhythmic sequences at beat rates around 1–2 Hz especially accurately when they are able to track the beat by moving their bodies (<xref ref-type="bibr" rid="bib67">Zalta et al., 2020</xref>). Despite the perceptual and motor evidence, studies looking at tempo-dependence of neural synchronization are scarce (<xref ref-type="bibr" rid="bib21">Doelling and Poeppel, 2015</xref>; <xref ref-type="bibr" rid="bib44">Nicolaou et al., 2017</xref>) and we are not aware of any human EEG study using naturalistic polyphonic musical stimuli that were manipulated in the tempo domain.</p><p>In the current study, we aimed to test whether the preference for music with beat rates around 2 Hz is reflected in the strength of neural synchronization by examining neural synchronization across a relatively wide and finely spaced range of musical tempi (1–4 Hz, corresponding to the neural δ band). In addition, a number of different musical, behavioral, and perceptual measures have been shown to modulate neural synchronization and influence music perception, including complexity, familiarity, repetition of the music, musical training of the listener, and attention to the stimulus (<xref ref-type="bibr" rid="bib32">Kumagai et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Madsen et al., 2019</xref>; <xref ref-type="bibr" rid="bib21">Doelling and Poeppel, 2015</xref>). Thus, we investigated the effects of enjoyment, familiarity and the ease of beat perception on neural synchronization.</p><p>Most studies assessing neural synchronization to music have examined synchronization to either the stimulus amplitude envelope, which quantifies intensity fluctuations over time (<xref ref-type="bibr" rid="bib21">Doelling and Poeppel, 2015</xref>; <xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref>; <xref ref-type="bibr" rid="bib66">Wollman et al., 2020</xref>), or ‘higher order’ musical features such as surprise and expectation (<xref ref-type="bibr" rid="bib16">Di Liberto et al., 2020</xref>). This mimics approaches used for studying neural synchronization to speech, where neural activity has been shown to synchronize with the amplitude envelope (<xref ref-type="bibr" rid="bib52">Peelle and Davis, 2012</xref>), which roughly corresponds to syllabic fluctuations (<xref ref-type="bibr" rid="bib20">Doelling et al., 2014</xref>), as well as to ‘higher order’ semantic information (<xref ref-type="bibr" rid="bib5">Broderick et al., 2019</xref>). Notably, most studies that have examined neural synchronization to musical rhythm have used simplified musical stimuli, such as MIDI melodies (<xref ref-type="bibr" rid="bib32">Kumagai et al., 2018</xref>) and monophonic melodies (<xref ref-type="bibr" rid="bib16">Di Liberto et al., 2020</xref>), or rhythmic lines comprising clicks or sine tones (<xref ref-type="bibr" rid="bib46">Nozaradan et al., 2012</xref>; <xref ref-type="bibr" rid="bib45">Nozaradan et al., 2011</xref>; <xref ref-type="bibr" rid="bib66">Wollman et al., 2020</xref>); only a few studies have focused on naturalistic, polyphonic music (<xref ref-type="bibr" rid="bib60">Tierney and Kraus, 2015</xref>; <xref ref-type="bibr" rid="bib37">Madsen et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref>; <xref ref-type="bibr" rid="bib21">Doelling and Poeppel, 2015</xref>). ‘Higher order’ musical features are difficult to compute for naturalistic music, which is typically polyphonic and has complex spectro-temporal properties (<xref ref-type="bibr" rid="bib68">Zatorre et al., 2002</xref>). However, amplitude-envelope synchronization is well documented: neural activity synchronizes to amplitude fluctuations in music between 1 Hz and 8 Hz, and synchronization is especially strong for listeners with musical expertise (<xref ref-type="bibr" rid="bib21">Doelling and Poeppel, 2015</xref>).</p><p>Because of the complex nature of natural polyphonic music, we hypothesized that amplitude envelope might not be the only or most dominant feature to which neural activity could synchronize (<xref ref-type="bibr" rid="bib43">Müller, 2015</xref>). Thus, the current study investigated neural responses to different musical features that evolve over time and capture different aspects of the stimulus dynamics. Here, we use the term <italic>musical feature</italic> to refer to time-varying aspects of music that fluctuate on time scales corresponding roughly to the neural δ band, as opposed to elements of music such as key, harmony or syncopation. We examined amplitude envelope, the first derivative of the amplitude envelope (usually more sensitive to sound onsets than the amplitude envelope), beat times, and <italic>spectral flux</italic>, which describes spectral changes of the signal on a frame-to-frame basis by computing the difference between the spectral vectors of subsequent frames (<xref ref-type="bibr" rid="bib43">Müller, 2015</xref>). One potential advantage of spectral flux over the envelope or its derivative is that spectral flux is sensitive to rhythmic information that is communicated by changes in pitch even when they are not accompanied by changes in amplitude. Critically, temporal and spectral information jointly influence the perceived accent structure in music, which provides information about beat locations (<xref ref-type="bibr" rid="bib53">Pfordresher, 2003</xref>; <xref ref-type="bibr" rid="bib24">Ellis and Jones, 2009</xref>; <xref ref-type="bibr" rid="bib28">Jones, 1993</xref>).</p><p>The current study investigated neural synchronization to natural music by using two different analysis approaches: Reliable Components Analysis (RCA) (<xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref>) and temporal response functions (TRFs) (<xref ref-type="bibr" rid="bib16">Di Liberto et al., 2020</xref>). A theoretically important distinction here is whether neural synchronization observed using these techniques reflects phase-locked, unidirectional coupling between a stimulus rhythm and activity generated by a neural oscillator (<xref ref-type="bibr" rid="bib33">Lakatos et al., 2019</xref>) versus the convolution of a stimulus with the neural activity evoked by that stimulus (<xref ref-type="bibr" rid="bib69">Zuk et al., 2021</xref>). TRF analyses involve modeling neural activity as a linear convolution between a stimulus and relatively broad-band neural activity (e.g. 1–15 Hz or 1–30 Hz; <xref ref-type="bibr" rid="bib11">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="bib12">Crosse et al., 2021</xref>); as such, there is a natural tendency for papers applying TRFs to interpret neural synchronization through the lens of convolution (although there are plenty of exceptions to this e.g. <xref ref-type="bibr" rid="bib10">Crosse et al., 2015</xref>; <xref ref-type="bibr" rid="bib15">Di Liberto et al., 2015</xref>). RCA-based analyses usually calculate correlation or coherence between a stimulus and relatively narrow-band activity, and in turn interpret neural synchronization as reflecting entrainment of a narrow-band neural oscillation to a stimulus rhythm (<xref ref-type="bibr" rid="bib21">Doelling and Poeppel, 2015</xref>; <xref ref-type="bibr" rid="bib1">Assaneo et al., 2019</xref>). Ultimately, understanding under what circumstances and using what techniques the neural synchronization we observe arises from either of these physiological mechanisms is an important scientific question (<xref ref-type="bibr" rid="bib22">Doelling et al., 2019</xref>; <xref ref-type="bibr" rid="bib23">Doelling and Assaneo, 2021</xref>; <xref ref-type="bibr" rid="bib62">van Bree et al., 2022</xref>). However, doing so is not within the scope of the present study, and we prefer to remain agnostic to the potential generator of synchronized neural activity. Here, we refer to and discuss ‘entrainment in the broad sense’ (<xref ref-type="bibr" rid="bib47">Obleser and Kayser, 2019</xref>) without making assumptions about <italic>how</italic> neural synchronization arises, and we will moreover show that these two classes of analyses techniques strongly agree with each other.</p><p>We aimed to answer four questions. (1) Does neural synchronization to natural music depend on tempo? (2) Which musical feature shows the strongest neural synchronization during natural music listening? (3) How compatible are RCA- and TRF-based methods at quantifying neural synchronization to natural music? (4) How do enjoyment, familiarity, and ease of beat perception affect neural synchronization? To answer these research questions, we recorded electroencephalography (EEG) data while participants listened to instrumental music presented at different tempi (1–4 Hz). Strongest neural synchronization was observed in response to the spectral flux of music, for tempi between 1 and 2 Hz, to familiar songs, and to songs with an easy-to-perceive beat.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Scalp EEG activity of 37 human participants was measured while they listened to instrumental segments of natural music from different genres (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>). Music segments were presented at thirteen parametrically varied tempi (1–4 Hz in 0.25 Hz steps; see Materials and methods). We assessed neural synchronization to four different musical features: amplitude envelope, first derivative of the amplitude envelope, beat times, and spectral flux. Neural synchronization was quantified using two different analysis pipelines and compared: (1) RCA combined with time- and frequency-domain analyses (<xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref>), and (2) TRFs (<xref ref-type="bibr" rid="bib11">Crosse et al., 2016</xref>). As different behavioral and perceptual measures have been shown to influence neural synchronization to music (<xref ref-type="bibr" rid="bib37">Madsen et al., 2019</xref>; <xref ref-type="bibr" rid="bib7">Cameron et al., 2019</xref>), we investigated the effects of enjoyment, familiarity, and the ease with which a beat was perceived (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). To be able to use a large variety of musical stimuli on the group level, and to decrease any effects that may have arisen from individual stimuli occurring at certain tempi but not others, participants were divided into four subgroups that listened to different pools of stimuli (for more details please see Materials and methods). The subgroups’ stimulus pools overlapped, but the individual song stimuli were presented at different tempi for each subgroup.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental design and musical features.</title><p>(<bold>A</bold>) Schematic of the experimental procedure. Each trial consisted of the presentation of one music segment, during which participants were instructed to listen attentively without moving. After a 1 s silence, the last 5.5 s of the music segment was repeated while participants tapped their finger along with the beat. At the end of each trial, participants rated their enjoyment and familiarity of the music segment, as well as the ease with which they were able to tap to the beat (Translated English example in Figure: “How much did you like the song?” rated from “not at all” to “very much”). (<bold>B</bold>) Exemplary traces of the four musical features of one music segment. (<bold>C</bold>) Z-scored mean amplitude spectrum of all 4 musical features. Light orange dashed boxes highlight when the FFT Frequency corresponds to the stimulation tempo or first harmonic. (<bold>D</bold>) Mutual information (MI) for all possible feature combinations (green) compared to a surrogate distribution (yellow, three-way ANOVA, *p<sub>FDR</sub> &lt;0.001, rest: p<sub>FDR</sub> &lt;0.05). Boxplots indicate the median, the 25th and 75th percentiles (n=52). (<bold>E</bold>) MI scores between all possible feature combinations (*p<sub>FDR</sub> &lt;0.001, rest: p<sub>FDR</sub> &lt;0.05).</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Source data for visualizing and analyzing the musical features.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-75515-fig1-data1-v1.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Shared Mutual Information (MI) between musical features across tempo conditions.</title><p>MI scores for all possible feature combinations as a function of tempo. Significance was evaluated based on a three-way ANOVA (with tempo as second factor, see Materials and methods for further information) and a follow-up pairwise comparison. Red boxes indicate significant tempo-dependent shared MI between musical features with a significance level of p<sub>FDR</sub> &lt;0.05.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Tempo manipulations of original music segments.</title><p>(<bold>A</bold>) Histogram of the original tempo of the music segments prior to tempo manipulation binned between 1–4 Hz in steps of 0.25 Hz (n=72). Histograms of (<bold>B</bold>) the difference between the original tempo and the slowest possible tempo manipulation and (<bold>C</bold>) the difference between the fastest possible tempo manipulation and the original tempo per music segment.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig1-figsupp2-v1.tif"/></fig></fig-group><sec id="s2-1"><title>Musical features</title><p>We examined neural synchronization to the time courses of four different musical features (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). First, we quantified energy fluctuations over time as the gammatone-filtered amplitude envelope (we report analyses on the full-band envelope in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Second, we computed the half-wave-rectified first derivative of the amplitude envelope, which is typically considered to be sensitive to the presence of onsets in the stimulus (<xref ref-type="bibr" rid="bib3">Bello et al., 2005</xref>). Third, a percussionist drummed along with the musical segments to define beat times, which were here treated in a binary manner. Fourth, a spectral novelty function, referred to as spectral flux (<xref ref-type="bibr" rid="bib43">Müller, 2015</xref>), was computed to capture changes in frequency content (as opposed to amplitude fluctuations) over time. In contrast to the first derivative, the spectral flux is better able to identify note onsets that are characterized by changes in spectral content (pitch or timbre), even if the energy level remains the same. To ensure that each musical feature possessed acoustic cues to the stimulation-tempo manipulation, we computed a fast Fourier transform (FFT) on the musical-feature time courses separately for each stimulation-tempo condition; the mean amplitude spectra are plotted in <xref ref-type="fig" rid="fig1">Figure 1C</xref>.</p><p>Overall, amplitude peaks were observed at the intended stimulation tempo and at the harmonic rates for all stimulus features.</p><p>In order to assess the degree to which the different musical features might have been redundant, we calculated mutual information (MI) for all possible pairwise feature combinations and compared MI values to surrogate distributions calculated separately for each feature pair (<xref ref-type="fig" rid="fig1">Figure 1D and E</xref>). MI quantifies the amount of information gained about one random variable by observing a second variable (<xref ref-type="bibr" rid="bib9">Cover and Thomas, 2005</xref>). MI values were analyzed using separate three-way ANOVAs (MI data vs. MI surrogate ×Tempo × Subgroup) for each musical feature.</p><p>Spectral flux shared significant information with all other musical features; significant MI (relative to surrogate) was found between amplitude envelope and spectral flux (F(1,102)=24.68, p<sub>FDR</sub> = 1.01e-5, η<sup>2</sup>=0.18), derivative and spectral flux (F(1,102)=82.3, p<sub>FDR</sub> = 1.92e-13, η<sup>2</sup>=0.45) and beat times and spectral flux (F(1,102)=23.05, p<sub>FDR</sub> = 1.3e-5, η<sup>2</sup>=0.13). This demonstrates that spectral flux captures information from all three other musical features, and as such, we expected that spectral flux would be associated with strongest neural synchronization. Unsurprisingly, there was also significant shared information between the amplitude envelope and first derivative (F(1,102)=14.11, p<sub>FDR</sub> = 4.67e-4, η<sup>2</sup>=0.09); other comparisons: (F<sub>env-beat</sub>(1,102)=8.44, p<sub>FDR</sub> = 0.006, η<sup>2</sup>=0.07; F<sub>der-beat</sub>(1,102)=6.06, p<sub>FDR</sub> = 0.016, η<sup>2</sup>=0.05).</p><p>There was a main effect of Tempo on MI shared between the amplitude envelope and derivative (F(12,91)=4, p<sub>FDR</sub> = 2e-4, η<sup>2</sup>=0.32) and the spectral flux and beat times (F(12,91)=5.48, p<sub>FDR</sub> = 4.35e-6, η<sup>2</sup>=0.37) (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). This is likely due to the presence of slightly different songs in the different tempo conditions, as the effect of tempo on MI was unsystematic for both feature pairs (see Materials and methods and <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>). MI for the remaining feature pairs did not differ significantly across tempi.</p><p>No significant differences in MI were observed between subgroups, despite the subgroups hearing slightly different pools of musical stimuli: (F<sub>env-der</sub>(3,100)=0.71, p<sub>FDR</sub> = 0.94, η<sup>2</sup>=0.01; F<sub>env-beat</sub>(3,100)=2.63, p<sub>FDR</sub> = 0.33, η<sup>2</sup>=0.07; F<sub>env-spec</sub>(3,100)=0.3, p<sub>FDR</sub> = 0.94, η<sup>2</sup>=0.01; F<sub>der-beat</sub>(3,100)=0.43, p<sub>FDR</sub> = 0.94, η<sup>2</sup>=0.01; F<sub>der-spec</sub>(3,100)=0.46, p<sub>FDR</sub> = 0.94, η<sup>2</sup>=0.01; F<sub>beat-spec</sub>(3,100)=0.13, p<sub>FDR</sub> = 0.94, η<sup>2</sup>=0.002).</p></sec><sec id="s2-2"><title>Neural synchronization was strongest in response to slow music</title><p>Neural synchronization to music was investigated using two converging analysis pipelines based on (1) RCA followed by time- (stimulus-response correlation, SRCorr) and frequency- (stimulus-response coherence, SRCoh) domain analysis and (2) TRFs.</p><p>First, an RCA-based analysis approach was used to assess tempo effects on neural synchronization to music (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). RCA involves estimating a spatial filter that maximizes correlation across data sets from multiple participants (for more details see Materials and methods) (<xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref>; <xref ref-type="bibr" rid="bib50">Parra et al., 2018</xref>). The resulting time course data from a single reliable component can then be assessed in terms of its correlation in the time domain (SRCorr) or coherence in the frequency domain (SRCoh) with different musical feature time courses. Our analyses focused on the first reliable component, which exhibited an auditory topography (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). To control for inherent tempo-dependent effects that could influence our results (such as higher power or variance at lower frequencies, that is 1/f), SRCorr and SRCoh values were normalized by a surrogate distribution. This way the temporal alignment between the stimulus and neural time course was destroyed, but the spectrotemporal composition of each signal was preserved. The surrogate distribution was obtained by randomly circularly shifting the neural time course in relation to the musical features per tempo condition and stimulation subgroup for 50 iterations (<xref ref-type="bibr" rid="bib69">Zuk et al., 2021</xref>). Subsequently, the ‘raw’ SRCorr or SRCoh values were z-scored by subtracting the mean and dividing by the standard deviation of the surrogate distribution.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Stimulus–response correlation and stimulus–response coherence are tempo dependent for all musical features.</title><p>(<bold>A</bold>) Projected topography of the first reliable component (RC1). (<bold>B</bold>) Average SRCorr of the aligned neural response and surrogate distribution (grey) across tempi for each musical feature (left) and the z-scored SRCorr based on a surrogate distribution (right) (± SEM; shaded area). Highest correlations were found at slow tempi (repeated-measure ANOVA, Greenhouse-Geiser correction where applicable). The slopes of regression models were used to compare the tempo-specificity between musical features. (<bold>C</bold>) Mean SRCorr across musical features. Highest correlations were found in response to spectral flux with significant differences between all possible feature combinations, p<sub>FDR</sub> &lt;0.001, except between the envelope or derivative and beat onsets, p<sub>FDR</sub> &lt;0.01 (n=34, repeated-measure ANOVA, Tukey’s test, median, 25th and 75th percentiles). Z-scored SRCoh in response to the (<bold>D</bold>) amplitude envelope, (<bold>E</bold>) first derivative, (<bold>F</bold>) beat onsets and (<bold>G</bold>) spectral flux. Each panel depicts the SRCoh as colorplot (left) and the pooled SRCoh values at the stimulation tempo and first harmonic (right, n=34, median, 25th and 75th percentile). (<bold>H</bold>) Same as (<bold>C</bold>) for the SRCoh with significant differences between all possible feature combinations (p<sub>FDR</sub> &lt;0.001) apart between the envelope and beat onsets. Coherence values were averaged over the stimulus tempo and first harmonic. (<bold>I</bold>) Mean differences of SRCoh values at the stimulation tempo and first harmonic (n=34, negative values: higher SRCoh at harmonic, positive values: higher SRCoh at stimulation tempo, paired-sample t-test, p<sub>FDR</sub> &lt;0.05). (<bold>J</bold>) Same as (<bold>I</bold>) based on the FFT amplitudes (p<sub>FDR</sub> &lt;0.001).</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Source data for the RCA-based measures stimulus-response correlation (SRCorr) and stimulus-response coherence (SRCoh).</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-75515-fig2-data1-v1.zip"/></supplementary-material></p><p><supplementary-material id="fig2sdata2"><label>Figure 2—source data 2.</label><caption><title>Output of the RCA-based analysis of the first two stimulation subgroups (based on <xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref>).</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-75515-fig2-data2-v1.zip"/></supplementary-material></p><p><supplementary-material id="fig2sdata3"><label>Figure 2—source data 3.</label><caption><title>Output of the RCA-based analysis of the last two stimulation subgroups (based on <xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref>).</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-75515-fig2-data3-v1.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>SRCorr and SRCoh in response to the full-band amplitude envelope and derivative.</title><p>(<bold>A</bold>) Mean SRCorr across stimulation tempi and musical features (± SEM). Similar to <xref ref-type="fig" rid="fig2">Figure 2B</xref> of the main manuscript, the full-band amplitude envelope (Hilbert transform) and resultant first derivative were used. Significance between tempi was assessed using a repeated-measure ANOVA (with Greenhouse-Geiser correction if applicable). (<bold>B</bold>) SRCorr across musical features. Statistically significant differences were identified between all musical feature combinations except between the envelope and beat onsets using a repeated-measure ANOVA (p<sub>FDR</sub> &lt;0.001, median, 25th and 75th percentiles). (<bold>C</bold>) Colormap of the fast Fourier Transform (FFT) of the first reliable component (RC1) across stimulation tempi. Note that the colorbar is in a logarithmic scale.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Individual data examples for the SRCorr and SRCoh.</title><p>(<bold>A</bold>) Each plot shows the mean z-scored SRCorr of one participant across stimulation tempi. Each line represents the average of one musical feature (± SEM). (<bold>B</bold>) Illustrative color plots of the normalized SRCoh in response to the spectral flux across stimulation tempi (1–4 Hz) of the same four participants as in (<bold>A</bold>). Z-scoring was based on the surrogate distribution.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig2-figsupp2-v1.tif"/></fig></fig-group><p>The resulting z-scored SRCorrs were significantly tempo-dependent for the amplitude envelope and the spectral flux (repeated-measure ANOVAs with Greenhouse-Geiser correction where required: F<sub>env</sub>(12,429)=2.5, p<sub>GG</sub> = 0.015, η<sup>2</sup>=0.07; F<sub>der</sub>(12,429)=1.67, p=0.07, η<sup>2</sup>=0.05; F<sub>beat</sub>(12,429)=0.94, p=0.5, η<sup>2</sup>=0.03; F<sub>spec</sub>(12,429)=2.92, p<sub>GG</sub> = 6.88e-4, η<sup>2</sup>=0.08). Highest correlations were found at slower tempi (~1–2 Hz).</p><p>No significant differences were observed across subgroups (F<sub>env</sub>(3,30)=1.13, p<sub>FDR=</sub>0.55, η<sup>2</sup>=0.1; F<sub>der</sub>(3,30)=0.72, p<sub>FDR</sub> = 0.55, η<sup>2</sup>=0.07; F<sub>beat</sub>(3,30)=0.85, p<sub>FDR</sub> = 0.55, η<sup>2</sup>=0.08; F<sub>spec</sub>(3,30)=0.9, p<sub>FDR</sub> = 0.55, η<sup>2</sup>=0.08). The results for the z-scored SRCorr were qualitatively similar to the ‘raw’ SRCorr with biggest differences for the beat feature.</p><p>In the frequency domain, z-scored SRCoh (<xref ref-type="fig" rid="fig2">Figure 2D–G</xref>) showed clear peaks at the stimulation tempo and harmonics. Overall, SRCoh was stronger at the first harmonic of the stimulation tempo than at the stimulation tempo itself, regardless of the musical feature (<xref ref-type="fig" rid="fig2">Figure 2I</xref>, paired-sample t-test, envelope: t(12)=-5.16, p<sub>FDR</sub> = 0.001, r<sub>e</sub> = 0.73; derivative: t(12)=-5.11, p<sub>FDR</sub> = 0.001, r<sub>e</sub> = 0.72; beat: t(12)=-4.13, p<sub>FDR</sub> = 0.004, r<sub>e</sub> = 0.64; spectral flux: t(12)=-3.3, p<sub>FDR</sub> = 0.01, r<sub>e</sub> = 0.56). The stimuli themselves mostly also contained highest FFT amplitudes at the first harmonic (<xref ref-type="fig" rid="fig2">Figure 2J</xref>, envelope: t(12)=-6.81, p<sub>FDR</sub> = 5.23e-5, <italic>r</italic><sub>e</sub>=0.81; derivative: t(12)=-6.88, p<sub>FDR</sub> = 5.23e-5, <italic>r</italic><sub>e</sub> = 0.81; spectral flux: t(12)=-8.04, p<sub>FDR</sub> = 2.98e-5, <italic>r</italic><sub>e</sub> = 0.85), apart from the beat onsets (beat: t(12)=6.27, p<sub>FDR</sub> = 8.56–5. <italic>r</italic><sub>e</sub> = 0.79).</p><p>For evaluating tempo-dependent effects, we averaged z-scored SRCoh across the stimulation tempo and first harmonic and submitted the average z-SRCoh values to repeated-measure ANOVAs for each musical feature. Z-SRCoh was highest for slow music, but this tempo dependence was only significant for the spectral flux and beat onsets (F<sub>env</sub>(12,429)=1.31, p=0.21, η<sup>2</sup>=0.04; F<sub>der</sub>(12,429)=1.71, p=0.06, η<sup>2</sup>=0.05; F<sub>beat</sub>(12,429)=2.07, p<sub>GG</sub> = 0.04, η<sup>2</sup>=0.06; F<sub>spec</sub>(12,429)=2.82, p<sub>GG</sub> = 0.006, η<sup>2</sup>=0.08). No significant differences for the SRCoh were observed across subgroups (F<sub>env</sub>(3,30)=0.93, p<sub>FDR=</sub>0.58, η<sup>2</sup>=0.09; F<sub>der</sub>(3,30)=3.07, p<sub>FDR</sub> = 0.17, η<sup>2</sup>=0.24; F<sub>beat</sub>(3,30)=2.26, p<sub>FDR</sub> = 0.2, η<sup>2</sup>=0.18; F<sub>spec</sub>(3,30)=0.29, p<sub>FDR</sub> = 0.83, η<sup>2</sup>=0.03). Individual data examples of the SRCorr and SRCoh can be found in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>.</p><p>Second, TRFs were calculated for each stimulation tempo. A TRF-based approach is a linear-system identification technique that serves as a filter describing the mapping of stimulus features onto the neural response (forward model) (<xref ref-type="bibr" rid="bib11">Crosse et al., 2016</xref>). Using linear convolution and ridge regression to avoid overfitting, the TRF was computed based on mapping each musical feature to ‘training’ EEG data. Using a leave-one-trial-out approach, the EEG response for the left-out trial was predicted based on the TRF and the stimulus feature of the same trial. The predicted EEG data were then correlated with the actual, unseen EEG data (we refer to this correlation value throughout as <italic>TRF correlation</italic>). We analyzed the two outputs of the TRF analysis: the filter at different time lags, which typically resembles evoked potentials, and the TRF correlations (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>TRFs are tempo dependent.</title><p>(<bold>A</bold>) Mean TRF (± SEM) correlations as a function of stimulation tempo per stimulus feature (p-values next to the legend correspond to a repeated-measure ANOVA across tempi for every musical feature and the p-value below to the slope comparison of a linear regression model). TRF correlations were highest for spectral flux and combined musical features for slow tempi. The TRF correlations were z-scored based on a surrogate distribution (right panel). (<bold>B</bold>) Violin plots of the TRF correlations across musical features. Boxplots illustrate the median, 25th and 75th percentiles (n=34). Significant pairwise musical feature comparisons were calculated using a repeated-measure ANOVA with follow-up Tukey’s test, *p<sub>FDR</sub> &lt;0.001. (<bold>C</bold>) Top panel: Topographies of the TRF correlations and TRF time lags (0–400ms) in response to the amplitude envelope. Each line depicts one stimulation tempo (13 tempi between 1 Hz, blue and 4 Hz, green). Lower panel: Colormap of the normalized TRF weights of the envelope in the same time window across stimulation tempi. (<bold>D</bold>) Same as (<bold>C</bold>) for the first derivative, (<bold>E</bold>) beat onsets and (<bold>F</bold>) spectral flux. Cluster-based permutation testing was used to identify significant tempo-specific time windows (red dashed box, p&lt;0.05). Inset: Mean TRF weights in response to the spectral flux for time lags between 102 and 211ms (n=34, median, 25th and 75th percentile).</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Source data of the TRF correlations and weights.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-75515-fig3-data1-v1.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>TRFs in response to the full-band amplitude envelope and first derivative show similar patterns as the gammatone filtered musical features.</title><p>(<bold>A</bold>) Mean TRF correlations across stimulation tempi and musical features (± SEM). (<bold>B</bold>) TRF correlations across musical features. Violin plots indicate the median, 25th and 75th percentiles (n=34, repeated-measure ANOVA; *p<sub>FDR</sub> &lt;0.001). Similarly to the gammatone filtered features, the full-band envelope and derivative show significantly smaller TRF correlations in comparison to the spectral flux and the combination of all features. (<bold>C</bold>) Auditory topographies of the TRF correlations in response to the full-band features. (<bold>D</bold>) TRF weights for time lags between 0 and 400ms of the Hilbert envelope. Cluster-based permutation testing was used to identify significant time lag windows (envelope: 250–400ms, p=0.01; derivative: 281–400ms, p=0.02). Each line represents one stimulation tempo (n=13, blue, 1 Hz - green, 4 Hz). (<bold>E</bold>) Colormaps of the TRF weights over the same time lags for the envelope. Red dashed lines highlight significant time windows. (<bold>F</bold>) Average TRF weights from the significant time lag window for the envelope (n=34, median, 25th and 75th percentile). (<bold>G</bold>) Latency of the P3 peak located in the significant time lag window across stimulation tempi for the full-band envelope TRFs. The latencies were divided into two subgroups (1–2.5 Hz and 2.75–4 Hz) and a regression fit into the data. (<bold>H)-(K</bold>) Same as (<bold>D</bold>)-(<bold>G</bold>) for the first derivative.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Corrected TRF weights of the spectral flux after removing the effects of the other musical features.</title><p>(<bold>A</bold>) TRF weights in response to the spectral flux. To calculate those weights, a multivariate TRF approach based on the amplitude envelope, first derivative and beat onsets was used and the resulting TRF predictions were subtracted from the ‘actual’ EEG data. The residual EEG data was used to compute the spectral TRF model. (<bold>B</bold>) Similarly to the main <xref ref-type="fig" rid="fig3">Figure 3F</xref> the TRF weights at the previously calculated significant time window were plotted as a function of tempo. The boxplots indicate the median, 25th and 75th percentile (n=34).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>No differences in TRFs correlations between more vs. less modulated music.</title><p>(<bold>A</bold>) TRF correlations for up to three trials per participant when the original music tempo ≈ manipulated music tempo (labelled ‘Original’) vs. when the manipulated music tempo was faster than the original music segments (‘Slow’). For this analysis, different trials for the different stimulation subgroups from the same stimulation tempo condition (here: 2.25 Hz) were used (n<sub>ori</sub> = 91; n<sub>slow</sub> = 96 trials). In the right plot the TRF correlation were z-scored based on the surrogate distribution on a per trials basis. No significant differences were observed between groups (repeated-measures ANOVA). (<bold>B</bold>) Same as (<bold>A</bold>), but here the original tempo was at a slower tempo and was contrasted against music segments that were originally faster and were manipulated to be played at a 1.5 Hz (n<sub>ori</sub> = 57; n<sub>fast</sub> = 58 trials). The boxplots indicate the median, 25th and 75th percentile.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig3-figsupp3-v1.tif"/></fig></fig-group><p>Again, strongest neural synchronization (here quantified as Pearson correlation coefficient between the predicted and actual EEG data) was observed for slower music (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). After z-scoring the TRF correlations with respect to the surrogate distributions, as described for the SRcorr and SRcoh measures, repeated-measures ANOVAs showed that significant effects of Tempo were observed for all musical features with z-TRF correlations being strongest at slower tempi (~1–2 Hz) (F<sub>env</sub>(12,429)=2.47, p=0.004, η<sup>2</sup>=0.07; F<sub>der</sub>(12,429)=1.84, p<sub>GG</sub> = 0.04, η<sup>2</sup>=0.05; F<sub>beat</sub>(12,429)=3.81, p<sub>GG</sub> = 3.18e-4, η<sup>2</sup>=0.11; F<sub>spec</sub>(12,429)=12.87, p<sub>GG</sub> = 3.87e-13, η<sup>2</sup>=0.29).</p><p>The original tempi of the music segments prior to being tempo manipulated fell mostly into the range spanning 1.25–2.5 Hz (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref>). Thus, music that was presented at stimulation tempi in this range were shifted to a smaller degree than music presented at tempi outside of this range, and music presented at slow tempi tended to be shifted to a smaller degree than music presented at fast tempi (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2B</xref>,C). Thus, we conducted a control analysis to show that there was no significant effect on z-TRF correlations of how much music stimuli were tempo shifted (2.25 Hz: F(2,96)=0.45, <italic>P</italic>=0.43; 1.5 Hz: F(2,24)=0.49, p=0.49; <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>; for more details see Materials and methods).</p></sec><sec id="s2-3"><title>Spectral flux drives strongest neural synchronization</title><p>As natural music is a complex, multi-layered auditory stimulus, we sought to explore neural synchronization to different musical features and to identify the stimulus feature or features that would drive strongest neural synchronization. Regardless of the dependent measure (RCA-SRCorr, RCA-SRCoh, TRF correlation), strongest neural synchronization was found in response to spectral flux (<xref ref-type="fig" rid="fig2">Figures 2C, H</xref>, <xref ref-type="fig" rid="fig3">3B</xref>). In particular, significant differences (as quantified with a repeated-measure ANOVA followed by Tukey’s test) were observed between the spectral flux and all other musical features based on z-scored SRCorr (F<sub>SRCorr</sub>(3,132)=39.27, p<sub>GG</sub> = 1.2e-16, η<sup>2</sup>=0.55), z-SRCoh (F<sub>SRCoh</sub>(3,132)=26.27, p<sub>GG</sub> = 1.72e-12, η<sup>2</sup>=0.45) and z-TRF correlations (F<sub>TRF</sub>(4,165)=30.09, p<sub>GG</sub> = 1.21e-13, η<sup>2</sup>=0.48).</p><p>As the TRF approach offers the possibility of running a multivariate analysis, all musical features were combined and the resulting z-scored TRF correlations were compared to the single-feature TRF correlations (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Although there was a significant increase in z-TRF correlations in comparison to the amplitude envelope (repeated-measure ANOVA with follow-up Tukey’s test, p<sub>FDR</sub> = 1.66e-08), first derivative (p<sub>FDR</sub> = 1.66e-8), and beat onsets (p<sub>FDR</sub> = 1.66e-8), the spectral flux alone showed an advantage over the multi-featured TRF (p<sub>FDR</sub> = 6.74e-8). Next, we ran a multivariate TRF analysis combining amplitude envelope, first derivative, and beat onsets, and then subtracted the predicted EEG data from the actual EEG data (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). We calculated a TRF forward model using spectral flux to predict the EEG data residualized with respect to the multivariate predictor combining the remaining musical features. The resulting TRF weights were qualitatively similar to the model with spectral flux as the only predictor of the neural response. Thus, taking all stimulus features together is not a better descriptor of the neural response than the spectral flux alone, indicating together with the MI results from <xref ref-type="fig" rid="fig1">Figure 1</xref> that spectral flux is a more complete representation of the rhythmic structure of the music than the other musical features.</p><p>To test how strongly modulated TRF correlations were by tempo for each musical feature, a linear regression was fitted to single-participant z-TRF correlations as a function of tempo, and the slopes were compared across musical features (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Linear slopes were significantly higher for spectral flux and the multivariate model compared to the remaining three musical features (repeated-measure ANOVA with follow-up Tukey’s test, envelope-spectral flux: p<sub>FDR</sub> = 2.8e-6; envelope – all: p<sub>FDR</sub> = 2.88e-4; derivative-spectral flux: p<sub>FDR</sub> = 7.47e-8; derivative – all: p<sub>FDR</sub> = 2.8e-6; beat-spectral flux: p<sub>FDR</sub> = 2.47e-8; beat – all: p<sub>FDR</sub> = 2.09e-5; spectral flux – all: p<sub>FDR</sub> = 0.01). The results for z-SRCorr were qualitatively similar except for the comparison between the envelope and spectral flux (envelope-spectral flux: p<sub>FDR</sub> = 0.12; derivative-spectral flux: p<sub>FDR</sub> = 0.04; beat-spectral flux: p<sub>FDR</sub> = 6e-4; <xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>Finally, we also examined the time courses of TRF weights (<xref ref-type="fig" rid="fig3">Figure 3C–F</xref>) for time lags between 0 and 400ms, and how they depended on tempo. Cluster-based permutation testing (1,000 repetitions) was used to identify time windows in which TRF weights differed across tempi for each musical feature (see Materials and methods for more details). Significant effects of tempo on TRF weights were observed for spectral flux between 102–211ms (p=0.01; <xref ref-type="fig" rid="fig3">Figure 3F</xref>). The tempo specificity was observable in the amplitudes of the TRF weights, which were largest for slower music (<xref ref-type="fig" rid="fig3">Figure 3F</xref>). The TRFs for the amplitude envelope and first derivative demonstrated similar patterns to each other, with strong deflections in time windows consistent with a canonical auditory P1–N1–P2 complex, but did not differ significantly between stimulation tempi (<xref ref-type="fig" rid="fig3">Figure 3C–D</xref>). Similarly, the full-band (Hilbert) amplitude envelope and the corresponding first derivative (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) displayed tempo-specific effects at time lags of 250–400ms (envelope, p=0.01) and 281–400ms (derivative, p=0.02). Visual inspection suggested that TRF differences for these musical features were related to latency, as opposed to amplitude (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1E-F,I-J</xref>). Therefore, we identified the latencies of the TRF-weight time courses within the time window of P3 and fitted a piece-wise linear regression to those mean latency values per musical feature (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1G, K</xref>). In particular, TRF latency in the P3 time window decreased over the stimulation tempo conditions from 1 to 2.5 Hz and from 2.75 to 4 Hz for both stimulus features (derivative: T<sub>1-2.5Hz</sub>=-1.08, p=0.33, R<sup>2</sup>=0.03; T<sub>2.75-4Hz</sub>=-2.2, p=0.09, R<sup>2</sup>=0.43), but this was only significant for the envelope (T<sub>1-2.5Hz</sub>=-6.1, p=0.002, R<sup>2</sup>=0.86; T<sub>2.75-4Hz</sub>=-5.66, p=0.005, R<sup>2</sup>=0.86).</p></sec><sec id="s2-4"><title>Results of TRF and SRCorr/SRCoh converge</title><p>So far, we demonstrated that both RCA- and TRF-based measures of neural synchronization led to similar results at the group level, and reveal strongest neural synchronization to spectral flux and at slow tempi. Next, we wanted to quantify the relationship between the SRCorr/SRCoh and TRF correlations across individuals (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). This could have implications for the interpretation of studies focusing only on one method. To test this relationship, we predicted TRF correlations from SRCorr or SRCoh values (fixed effect) in separate linear mixed-effects models with Participant and Tempo as random effects (grouping variables). For all further analyses, we used the ‘raw’ (non-z-scored) values for all dependent measures, as they yielded in the previous analysis (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>) qualitatively similar results to the z-scored values. Each musical feature was modeled independently.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Significant relationships between SRCorr and TRF correlations for all musical features.</title><p>(<bold>A</bold>) Linear-mixed effects models of the SRCorr (predictor variable) and TRF correlations (response variable) in response to the amplitude envelope. Each dot represents the mean correlation of one participant (n=34) at one stimulation tempo (n=13) (=grouping variables; blue, 1 Hz-green, 4 Hz). Violin plots illustrate fixed effects coefficients (β). (<bold>B)-(D</bold>) same as (<bold>A</bold>) for the first derivative, beat onsets and spectral flux. For all musical features, the fixed effects were significant.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Source data for comparing the results of the TRF and RCA-based measures.</title></caption><media mimetype="application" mime-subtype="matlab-mat" xlink:href="elife-75515-fig4-data1-v1.mat"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Significant relationships between SRCoh and TRF correlations for all musical features at the stimulation tempo and first harmonic.</title><p>(<bold>A</bold>) Linear-mixed effects models of the SRCoh (predictor variable) and TRF correlations (response variable) in response to the amplitude envelope at the intended stimulation tempo (left, dark grey) and first harmonic (right, light grey). Each dot represents the mean correlation of one participant (n=34) at one stimulation tempo (n=13) (=grouping variables; blue, 1 Hz-green, 4 Hz). Violin plots illustrate fixed effects coefficients (β). (<bold>B)-(D</bold>) same as (<bold>A</bold>) for the first derivative, beat onsets and spectral flux. For all musical features, the fixed effects were significant (p<sub>FDR</sub> &lt;0.01). Model comparisons were implemented based in the Likelihood ratio test (LRStat, p&lt;0.001).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig4-figsupp1-v1.tif"/></fig></fig-group><p>For all four musical features, SRCorr significantly predicted TRF correlations (t<sub>env</sub>(440) = 9.77, β<sub>env</sub>=0.53, p<sub>FDR</sub> &lt;1e-15, R<sup>2</sup>=0.51; t<sub>der</sub>(440) = 8.09, β<sub>der</sub>=0.46, p<sub>FDR</sub> = 5.77e-14, R<sup>2</sup>=0.28; t<sub>beat</sub>(440) = 12.12, β<sub>beat</sub>=0.67, p<sub>FDR</sub> &lt;1e-15, R<sup>2</sup>=0.61; t<sub>spec</sub>(440) = 12.49, β<sub>spec</sub>=0.56, p<sub>FDR</sub> = 1e-15, R<sup>2</sup>=0.76). The strongest correlations between neural synchronization measures were found for the beat onsets and spectral flux of music (<xref ref-type="fig" rid="fig4">Figure 4C and D</xref>).</p><p>In the frequency domain, we examined the SRCoh values at the stimulation tempo and first harmonic separately (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). SRCoh values at both the intended stimulation tempo and the first harmonic significantly predicted TRF correlations for all musical features. For all musical features, the first harmonic was a better predictor of TRF correlations than the intended stimulation tempo except for the beat onsets (intended tempo: t<sub>env</sub>(440) = 4.78, β<sub>env</sub>=0.17, p<sub>FDR</sub> = 3.15e-6, R<sup>2</sup>=0.34; t<sub>der</sub>(440) = 3.06, β<sub>der</sub>=0.1, p<sub>FDR</sub> = 0.002, R<sup>2</sup>=0.13; t<sub>beat</sub>(440) = 8.12, β<sub>beat</sub>=0.28, p<sub>FDR</sub> = 1.95e-14, R<sup>2</sup>=0.5; t<sub>spec</sub>(440) = 3.42, β<sub>spec</sub>=0.09, p<sub>FDR</sub> = 7.9e-4, R<sup>2</sup>=0.64; first harmonic: t<sub>env</sub>(440) = 6.17, β<sub>env</sub>=0.09, p<sub>FDR</sub> = 3.07e-9, R<sup>2</sup>=0.33; t<sub>der</sub>(440) = 4.98, β<sub>der</sub>=0.09, p<sub>FDR</sub> = 1.43e-6, R<sup>2</sup>=0.16; t<sub>beat</sub>(440) = 8.79, β<sub>beat</sub>=0.2, p<sub>FDR</sub> &lt;1e-15, R<sup>2</sup>=0.51; t<sub>spec</sub>(440) = 6.87, β<sub>spec</sub>=0.09, p<sub>FDR</sub> = 5.82e-11, R<sup>2</sup>=0.64). Overall, these results suggest that, despite their analytical differences as well as common differences in interpretation, TRF and RCA–SRCorr/RCA-SRCoh seem to pick up on similar features of the neural response, but may potentially strengthen each other’s explanatory power when used together.</p></sec><sec id="s2-5"><title>Familiar songs and songs with an easy-to-tap beat drive strongest neural synchronization</title><p>Next, we tested whether neural synchronization to music depended on (1) how much the song was enjoyed, (2) the familiarity of the song, and (3) how easy it was to tap the beat of the song; each of these characteristics was rated on a scale ranging between –100 and +100. We hypothesized that difficulty to perceive and tap to the beat in particular would be associated with weaker neural synchronization. Ratings on all three dimensions are shown in <xref ref-type="fig" rid="fig5">Figure 5A</xref>. To evaluate the effects of tempo on the individuals’ ratings, separate repeated-measure ANOVAs were conducted for each behavioral rating. All behavioral ratings were unaffected by tempo (enjoyment: F(12,429)=0.58, p=0.85, η<sup>2</sup>=0.02; familiarity: F(12,429)=1.44, p<sub>GG</sub> = 0.18, η<sup>2</sup>=0.04; ease of beat tapping: F(12,429)=1.62, <italic>P</italic>=0.08, η<sup>2</sup>=0.05).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>TRF correlations are highest in response to familiar songs.</title><p>(<bold>A</bold>) Normalized (to the maximum value per rating/participant), averaged behavioral ratings of enjoyment, familiarity and easiness to tap to the beat (± SEM). No significant differences across tempo conditions were observed (repeated-measure ANOVA with Greenhouse-Geiser correction). (<bold>B</bold>) Mean TRF correlations topography across all ratings (based on the analysis of 15 trials with highest and lowest ratings per behavioral measure). (<bold>C</bold>) Violin plots of TRF correlations comparing low vs. highly enjoyed, low vs. highly familiar, and subjectively difficult vs. easy beat trials. Strongest TRF correlations were found in response to familiar music and music with an easy-to-perceive beat (n=34, paired-sample t-test, *p<sub>FDR</sub> &lt;0.05). Boxplots indicate median, 25th and 75th percentile. (<bold>D</bold>) Mean TRFs (± SEM) for time lags between 0–400ms of more and less enjoyable music songs. (<bold>E)-(F</bold>) Same as (<bold>D</bold>) for trials with low vs. high familiarity and difficult vs. easy beat ratings.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Source data of the behavioral ratings and TRF correlations.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-75515-fig5-data1-v1.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Significant differences of FFT amplitudes at stimulus-relevant frequencies between differently rated trials.</title><p>Z-scored average FFT amplitudes at the stimulation tempo and first harmonic of the 15 highest vs. lowest ratings per behavioral rating category (n=34, low vs. highly enjoyed, low vs. highly familiar, and subjectively difficult vs. easy beat trials). Significant differences were observed for all pairwise comparisons (paired-sample t-test, p<sub>FDR</sub>≈0.01).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Musical training did not have an effect on TRF correlations regardless of the musical feature.</title><p>Scatter plot between the general sophistication index (F7, Gold-MSI) and mean TRF correlations per participant (n=34). No significant correlations between the Gold-MSI and TRF correlations were observed in response to the (<bold>A</bold>) amplitude envelope, (<bold>B</bold>) first derivative, (<bold>C</bold>) beat onsets and (<bold>D</bold>) spectral flux.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig5-figsupp2-v1.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Music tapping rate across participants.</title><p>(<bold>A)-(B</bold>) Illustrative histograms of the relative number of trials per tapped music rate of two participants with a fitted skewed Gaussian. The modes indicate the preferred music tapping rate and the width the shape of the fitted Gaussian. (<bold>C</bold>) Mean music-tapping histogram across all participants (n=29, 5 participants were excluded from the music tapping analysis due to inconsistent taps). The mean preferred tapping frequency was 1.55 Hz.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-fig5-figsupp3-v1.tif"/></fig></fig-group><p>To assess the effects of familiarity, enjoyment, and beat-tapping ease on neural synchronization, TRFs in response to spectral flux were calculated for the 15 trials with the highest and the 15 trials with the lowest ratings per participant per behavioral rating (<xref ref-type="fig" rid="fig5">Figure 5B–F</xref>). TRF correlations were not significantly different for less enjoyed compared to more enjoyed music (paired-sample t-test, t(33)=1.91, p<sub>FDR</sub> = 0.06, <italic>r</italic><sub>e</sub> = 0.36; <xref ref-type="fig" rid="fig5">Figure 5C</xref>). In contrast, significantly higher TRF correlations were observed for familiar vs. unfamiliar songs (t(33)=-2.57, p<sub>FDR</sub> = 0.03, <italic>r</italic><sub>e</sub> = 0.46), and for songs with an easier-to-perceive beat (t(33)=-2.43, p<sub>FDR</sub> = 0.03, <italic>r</italic><sub>e</sub> = 0.44). These results were reflected in the TRFs at time lags between 0 and 400ms (<xref ref-type="fig" rid="fig5">Figure 5D–F</xref>). We wanted to test whether these TRF differences may have been attributable to acoustic features, such as the beat salience of the musical stimuli, which could have an effect on both behavioral ratings and TRFs. Thus, we computed single-trial FFTs on the spectral flux of the 15 highest vs. lowest rated trials (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Pairwise comparisons revealed higher stimulus-related FFT peaks for more enjoyed music (t-test, t(33)=-2.79, p<sub>FDR</sub> = 0.01, <italic>r</italic><sub>e</sub> = 0.49), less familiar music (t(33)=2.73, p<sub>FDR</sub> = 0.01, <italic>r</italic><sub>e</sub> = 0.49) and easier-to-perceive beats (t(33)=-3.33, p<sub>FDR</sub> = 0.01, <italic>r</italic><sub>e</sub> = 0.56).</p><p>Next, we wanted to entertain the possibility that musical expertise could modulate neural synchronization to music (<xref ref-type="bibr" rid="bib21">Doelling and Poeppel, 2015</xref>). We used the Goldsmith’s Musical Sophistication Index (Gold-MSI) to quantify musical ‘sophistication’ (referring not only to the years of musical training, but also e. g. musical engagement or self-reported perceptual abilities <xref ref-type="bibr" rid="bib42">Müllensiefen et al., 2014</xref>), which we then correlated with neural synchronization. No significant correlations were observed between musical sophistication and TRF correlations (Pearson correlation, envelope: <italic>R</italic>=−0.21, p<sub>FDR</sub> = 0.32; derivative: <italic>R</italic>=−0.24, p<sub>FDR</sub> = 0.31; beats: <italic>R</italic>=−0.04, p<sub>FDR</sub> = 0.81; spectral flux: <italic>R</italic>=−0.34, p<sub>FDR</sub> = 0.2; <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We investigated neural synchronization to naturalistic, polyphonic music presented at different tempi. The music stimuli varied along a number of dimensions in idiosyncratic ways, including the familiarity and enjoyment of the music, and the ease with which the beat was perceived. The current study demonstrates that neural synchronization is strongest to (1) music with beat rates between 1 and 2 Hz, (2) spectral flux of music, and (3) familiar music and music with an easy-to-perceive beat. In addition, (4) analysis approaches based on TRF and RCA revealed converging results.</p><sec id="s3-1"><title>Neural synchronization was strongest to music with beat rates in the 1–2 Hz range</title><p>Strongest neural synchronization was found in response to stimulation tempi between 1 and 2 Hz in terms of SRCorr (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), TRF correlations (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), and TRF weights (<xref ref-type="fig" rid="fig3">Figure 3C–F</xref>). Moreover, we observed a behavioral preference to tap to the beat in this frequency range, as the group preference for music tapping was at 1.55 Hz (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). Previous studies have shown a preference to listen to music with beat rates around 2 Hz (<xref ref-type="bibr" rid="bib2">Bauer et al., 2015</xref>), which is moreover the modal beat rate in Western pop music (<xref ref-type="bibr" rid="bib40">Moelants, 2002</xref>) and the rate at which the modulation spectrum of natural music peaks (<xref ref-type="bibr" rid="bib18">Ding et al., 2017</xref>). Even in nonmusical contexts, spontaneous adult human locomotion is characterized by strong energy around 2 Hz (<xref ref-type="bibr" rid="bib36">MacDougall and Moore, 2005</xref>). Moreover, when asked to rhythmically move their bodies at a comfortable rate, adults will spontaneously move at rates around 2 Hz (<xref ref-type="bibr" rid="bib38">McAuley et al., 2006</xref>) regardless whether they use their hands or feet (<xref ref-type="bibr" rid="bib56">Rose et al., 2020</xref>). Thus, there is a tight link between preferred rates of human body movement and preferred rates for the music we make and listen to that was moreover reflected in our neural data. This is perhaps not surprising, as musical rhythm perception activates motor areas of the brain, such as the basal ganglia and supplementary motor area (<xref ref-type="bibr" rid="bib26">Grahn and Brett, 2007</xref>), and is further associated with increased auditory–motor functional connectivity (<xref ref-type="bibr" rid="bib8">Chen et al., 2008</xref>). In turn, involving the motor system in rhythm perception tasks improves temporal acuity (<xref ref-type="bibr" rid="bib41">Morillon et al., 2014</xref>), but only for beat rates in the 1–2 Hz range (<xref ref-type="bibr" rid="bib67">Zalta et al., 2020</xref>).</p><p>The tempo range within which we observed strongest synchronization partially coincides with the original tempo range of the music stimuli (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). A control analysis revealed that the amount of tempo manipulation (difference between original music tempo and tempo at which the music segment was presented to the participant) did not affect TRF correlations. Thus, we interpret our data as reflecting a neural preference for specific musical tempi rather than an effect of naturalness or the amount that we had to tempo shift the stimuli. However, since our experiment was not designed to answer this question, we were only able to conduct this analysis for two tempi, 2.25 Hz and 1.5 Hz (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>), and thus are not able to rule out the influence of the magnitude of tempo manipulation on other tempo conditions.</p><p>In the frequency domain, SRCoh was strongest at the stimulation tempo and its harmonics (<xref ref-type="fig" rid="fig2">Figure 2D–G,I</xref>). In fact, highest coherence was observed at the first harmonic and not at the stimulation tempo itself (<xref ref-type="fig" rid="fig2">Figure 2I</xref>). This replicates previous work that also showed higher coherence (<xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref>) and spectral amplitude (<xref ref-type="bibr" rid="bib60">Tierney and Kraus, 2015</xref>) at the first harmonic than at the musical beat rate. There are several potential reasons for this finding. One reason could be that the stimulation tempo that we defined for each musical stimulus was based on beat rate, but natural music can be subdivided into smaller units (e.g. notes) that can occur at faster time scales. A recent MEG study demonstrated inter-trial phase coherence for note rates up to 8 Hz (<xref ref-type="bibr" rid="bib21">Doelling and Poeppel, 2015</xref>). Hence, the neural responses to the music stimuli in the current experiment were likely synchronized to not only the beat rate, but also faster elements such as notes. In line with this hypothesis, FFTs conducted on the stimulus features themselves showed higher amplitudes at the first harmonic than the stimulation tempo for all musical features except the beat onsets (<xref ref-type="fig" rid="fig2">Figure 2J</xref>). Moreover, there are other explanations for higher coherence at the first harmonic than at the beat rate. For example, the low-frequency beat-rate neural responses fall into a steeper part of the 1 /f slope, and as such may simply suffer from worse signal-to-noise ratio than their harmonics.</p><p>Regardless of the reason, since frequency-domain analyses separate the neural response into individual frequency-specific peaks, it is easy to interpret neural synchronization (SRCoh) or stimulus spectral amplitude at the beat rate and the note rate – or at the beat rate and its harmonics – as independent (<xref ref-type="bibr" rid="bib30">Keitel et al., 2021</xref>). However, music is characterized by a nested, hierarchical rhythmic structure, and it is unlikely that neural synchronization at different metrical levels goes on independently and in parallel. One potential advantage of TRF-based analyses is that they operate on relatively wide-band data compared to Fourier-based approaches, and as such are more likely to preserve nested neural activity and perhaps less likely to lead to over- or misinterpretation of frequency-specific effects.</p></sec><sec id="s3-2"><title>Neural synchronization is driven by spectral flux</title><p>Neural synchronization was strongest in response to the spectral flux of music, regardless whether the analysis was based on TRFs or RCA. Similar to studies using speech stimuli, music studies typically use the amplitude envelope of the sound to characterize the stimulus rhythm (<xref ref-type="bibr" rid="bib63">Vanden Bosch der Nederlanden et al., 2020</xref>; <xref ref-type="bibr" rid="bib32">Kumagai et al., 2018</xref>; <xref ref-type="bibr" rid="bib21">Doelling and Poeppel, 2015</xref>; <xref ref-type="bibr" rid="bib14">Decruy et al., 2019</xref>; <xref ref-type="bibr" rid="bib54">Reetzke et al., 2021</xref>). Although speech and music share features such as amplitude fluctuations over time and hierarchical grouping (<xref ref-type="bibr" rid="bib51">Patel, 2003</xref>), there are differences in their spectro-temporal composition that make spectral information especially important for music perception. For example, while successful speech recognition requires 4–8 spectral channels, successful recognition of musical melodies requires at least 16 spectral channels (<xref ref-type="bibr" rid="bib58">Shannon, 2005</xref>) – the flipside of this is that music is more difficult than speech to understand based only on amplitude-envelope information. Moreover, increasing spectral complexity of a music stimulus enhances neural synchronization (<xref ref-type="bibr" rid="bib66">Wollman et al., 2020</xref>). Previous work on joint accent structure indicates that spectral information is an important contributor to beat perception (<xref ref-type="bibr" rid="bib24">Ellis and Jones, 2009</xref>; <xref ref-type="bibr" rid="bib53">Pfordresher, 2003</xref>). Thus, it was our hypothesis in designing the current study that a feature that incorporates spectral changes over time, as opposed to amplitude differences only, would better capture how neural activity entrains to musical rhythm.</p><p>Using TRF analysis, we found that not only was neural synchronization to spectral flux stronger than to any other musical feature, it was also stronger than the response to a multivariate predictor that combined all musical features. For this reason, we calculated the shared information (MI) between each pair of musical features, and found that spectral flux shared significant information with all other musical features (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Hence, spectral flux seems to capture information contained in, for example, the amplitude envelope, but also to contain unique information about rhythmic structure that cannot be gleaned from the other acoustic features (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><p>One hurdle to performing any analysis of the coupling between neural activity and a stimulus time course is knowing ahead of time the feature or set of features that will well characterize the stimulus on a particular time scale given the nature of the research question. Indeed, there is no necessity that the feature that best drives neural synchronization will be the most obvious or prominent stimulus feature. Here, we treated feature comparison as an empirical question (<xref ref-type="bibr" rid="bib15">Di Liberto et al., 2015</xref>), and found that spectral flux is a better predictor of neural activity than the amplitude envelope of music. Beyond this comparison though, the issue of feature selection also has important implications for comparisons of neural synchronization across, for example, different modalities.</p><p>For example, a recent study found that neuronal activity synchronizes less strongly to music than to speech <xref ref-type="bibr" rid="bib69">Zuk et al., 2021</xref>; notably this paper focused on the amplitude envelope to characterize the rhythms of both stimulus types. However, our results show that neural synchronization is especially strong to the spectral content of music, and that spectral flux may be a better measure for capturing musical dynamics than the amplitude envelope (<xref ref-type="bibr" rid="bib43">Müller, 2015</xref>). Imagine listening to a melody played in a <italic>glissando</italic> fashion on a violin. There might never be a clear onset that would be represented by the amplitude envelope – all of the rhythmic structure is communicated by spectral changes. Indeed, many automated tools for extracting the beat in music used in the musical information retrieval (MIR) literature rely on spectral flux information (<xref ref-type="bibr" rid="bib48">Olivera et al., 2010</xref>). Also in the context of body movement, spectral flux has been associated with the type and temporal acuity of synchronization between the body and music at the beat rate (<xref ref-type="bibr" rid="bib6">Burger et al., 2018</xref>) to a greater extent than other acoustic characterizations of musical rhythmic structure. As such, we found that spectral flux synchronized brain activity better than the amplitude envelope.</p></sec><sec id="s3-3"><title>Neural synchronization was strongest to familiar songs and songs with an easy beat</title><p>We found that the strength of neural synchronization depended on the familiarity of music and the ease with which a beat could be perceived (<xref ref-type="fig" rid="fig5">Figure 5</xref>). This is in line with previous studies showing stronger neural synchronization to familiar music (<xref ref-type="bibr" rid="bib37">Madsen et al., 2019</xref>) and familiar sung utterances (<xref ref-type="bibr" rid="bib64">Vanden Bosch der Nederlanden et al., 2022</xref>). Moreover, stronger synchronization for musicians than for nonmusicians has been interpreted as reflecting musicians’ stronger expectations about musical structure. On the surface, these findings might appear to contradict work showing stronger responses to music that violated expectations in some way (<xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref>; <xref ref-type="bibr" rid="bib16">Di Liberto et al., 2020</xref>). However, we believe these findings are compatible: familiar music would give rise to stronger expectations and stronger neural synchronization, and stronger expectations would give rise to stronger ‘prediction error’ when violated. In the current study, the musical stimuli never contained violations of any expectations, and so we observed stronger neural synchronization to familiar compared to unfamiliar music. There was also higher neural synchronization to music with subjectively ‘easy-to-tap-to’ beats. Overall, we interpret our results as indicating that stronger neural synchronization is evoked in response to music that is more predictable: familiar music and with easy-to-track beat structure.</p><p>Musical training did not affect the degree of neural synchronization in response to tempo-modulated music (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). This contrasts with previous music research showing that musicians’ neural activity was entrained more strongly by music than non-musicians’ (<xref ref-type="bibr" rid="bib37">Madsen et al., 2019</xref>; <xref ref-type="bibr" rid="bib21">Doelling and Poeppel, 2015</xref>; <xref ref-type="bibr" rid="bib16">Di Liberto et al., 2020</xref>). There are several possible reasons for this discrepancy. One is that most studies that have observed differences between musicians and nonmusicians focused on classical music (<xref ref-type="bibr" rid="bib21">Doelling and Poeppel, 2015</xref>; <xref ref-type="bibr" rid="bib37">Madsen et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Di Liberto et al., 2020</xref>), whereas we incorporated music stimuli with different instruments and from different genres (e.g. Rock, Pop, Techno, Western, Hip Hop, or Jazz). We suspect that musicians are more likely to be familiar with, in particular, classical music, and as we have shown that familiarity with the individual piece increases neural synchronization, these studies may have inadvertently confounded musical training with familiarity. Another potential reason for the lack of effects of musical training on neural synchronization in the current study could originate from the choice of utilizing acoustic audio descriptors as opposed to ‘higher order’ musical features. However, ‘higher order’ features such as surprise or entropy that have been shown to be influenced by musical expertise (<xref ref-type="bibr" rid="bib16">Di Liberto et al., 2020</xref>) are difficult to compute for natural, polyphonic music.</p></sec><sec id="s3-4"><title>TRF- and RCA-based measures show converging results</title><p>RCA and TRF approaches share their ability to characterize neural responses to single-trial, ongoing, naturalistic stimuli. As such, both techniques afford something that is challenging or impossible to accomplish with ‘classic’ ERP analysis. However, we made use of two techniques in parallel in order to leverage their unique advantages. RCA allows for frequency-domain analysis such as SRCoh, which can be useful for identifying neural synchronization specifically at the beat rate, for example. The frequency-specificity could serve as an advantage of the SRCoh over the TRF measures, where an EEG broadband signal was used. However, the RCA-based approaches <xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref> have been criticized because of their potential susceptibility to autocorrelation, which is argued to be minimized in the TRF approach (<xref ref-type="bibr" rid="bib69">Zuk et al., 2021</xref>), which uses ridge regression to dampen fast oscillatory components (<xref ref-type="bibr" rid="bib12">Crosse et al., 2021</xref>). However, by minimizing the effects of auto-correlation one concern could be that this could remove neural oscillations of interest as well. TRFs also offer a univariate and multivariate analysis approach that allowed us to show that adding other musical features to the model did not improve the correspondence to the neural data over and above spectral flux alone.</p><p>Despite their differences, we found strong correspondence between the dependent variables from the two types of analyses. Specifically, TRF correlations were strongly correlated with stimulation-tempo SRCoh, and this correlation was higher than for SRCoh at the first harmonic of the stimulation tempo for the amplitude envelope, derivative and beat onsets (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Thus, despite being computed on a relatively broad range of frequencies, the TRF seems to be correlated with frequency-specific measures at the stimulation tempo. The strong correspondence between the two analysis approaches has implications for how users interpret their results. Although certainly not universally true, we have noticed a tendency for TRF users to interpret their results in terms of a convolution of an impulse response with a stimulus, whereas users of stimulus–response correlation or coherence tend to speak of entrainment of ongoing neural oscillations. The current results demonstrate that the two approaches produce similar results, even though the logic behind the techniques differs. Thus, whatever the underlying neural mechanism, using one or the other does not necessarily allow us privileged access to a specific mechanism.</p></sec><sec id="s3-5"><title>Conclusions</title><p>This study presented new insights into neural synchronization to natural music. We compared neural synchronization to different musical features and showed strongest neural responses to the spectral flux. This has important implications for research on neural synchronization to music, which has so far often quantified stimulus rhythm with what we would argue is a subpar acoustic feature – the amplitude envelope. Moreover, our findings demonstrate that neural synchronization is strongest for slower beat rates, and for predictable stimuli, namely familiar music with an easy-to-perceive beat.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Thirty-seven participants completed the study (26 female, 11 male, mean age = 25.7 years, SD = 4.33 years, age range = 19–36 years). Target sample size for this was estimated using G*Power3, assuming 80% power for a significant medium-sized effect. We estimate a target sample size of 24 (+4) for within-participant condition comparisons and 32 (+4) for correlations, and defaulted to the larger value since this experiment was designed to investigate both types of effects. The values in parentheses were padding to allow for discarding ~15% of the recorded data. The datasets of three participants were discarded because of large artefacts in the EEG signal (see section <italic>EEG data Preprocessing</italic>), technical problems or for not following the experimental instructions. The behavioral and neural data of the remaining 34 participants were included in the analysis.</p><p>Prior to the EEG experiment, all participants filled out an online survey about their demographic and musical background using LimeSurvey (LimeSurvey GmbH, Hamburg, Germany, <ext-link ext-link-type="uri" xlink:href="http://www.limesurvey.org">http://www.limesurvey.org</ext-link>). All participants self-identified as German speakers. Most participants self-reported normal hearing (seven participants reported occasional ringing in one or both ears). Thirty-four participants were right- and three were left-handed. Musical expertise was assessed using the Goldsmith Music Sophistication Index (Gold-MSI; <xref ref-type="bibr" rid="bib42">Müllensiefen et al., 2014</xref>). Participants received financial compensation for participating (Online: 2.50 €, EEG: 7€ per 30 min). All participants signed the informed consent before starting the experiment. The study was approved by the Ethics Council of the Max Planck Society Ethics Council in compliance with the Declaration of Helsinki (Application No: 2019_04).</p></sec><sec id="s4-2"><title>Stimuli</title><p>The stimulus set started from 39 instrumental versions of musical pieces from different genres, including techno, rock, blues, and hip-hop. The musical pieces were available in a <italic>*.wav</italic> format on Qobuz Downloadstore (<ext-link ext-link-type="uri" xlink:href="https://www.qobuz.com/de-de/shop">https://www.qobuz.com/de-de/shop</ext-link>). Each musical piece was segmented manually using Audacity (Version 2.3.3, Audacity Team, <ext-link ext-link-type="uri" xlink:href="https://www.audacityteam.org">https://www.audacityteam.org</ext-link>) at musical phrase boundaries (e.g. between chorus and verse), leading to a pool of 93 musical segments with varying lengths between 14.4 and 38 s. We did not use the beat count from any publicly available beat-tracking softwares, because they did not track beats reliably across genres. Due to the first Covid-19 lockdown, we assessed the original tempo of each musical segment using an online method. Eight tappers, including the authors, listened to and tapped to each segment on their computer keyboard for a minimum of 17 taps; the tempo was recorded using an online BPM estimation tool (<ext-link ext-link-type="uri" xlink:href="https://www.all8.com/tools/bpm.htm">https://www.all8.com/tools/bpm.htm</ext-link>). In order to select stimuli with unambiguous strong beats that are easy to tap to, we excluded 21 segments due to high variability in tapped metrical levels (if more than 2 tappers tapped different from the others) or bad sound quality.</p><p>The remaining 72 segments were then tempo-manipulated using a custom-written MAX patch (Max 8.1.0, Cycling ’74, San Francisco, CA, USA). Each segment was shifted to tempi between 1 and 4 Hz in steps of 0.25 Hz. All musical stimuli were generated using the MAX patch, even if the original tempo coincided with the stimulation tempo. Subsequently, the authors screened all of the tempo-shifted music and eliminated versions where the tempo manipulation led to acoustic distortions, made individual notes indistinguishable, or excessively repetitive. Overall, 703 music stimuli with durations of 8.3–56.6 s remained. All stimuli had a sampling rate of 44,100 Hz, were converted from stereo to mono, linearly ramped with 500 ms fade-in and fade-out and root-mean-square normalized using Matlab (R2018a; The MathWorks, Natick, MA, USA). A full overview of the stimulus segments, the original tempi and the modulated tempo range can be found in the Appendix (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p><p>Each participant was assigned to one of four pseudo-randomly generated stimulus lists. Each list comprised 4–4.6 min of musical stimulation per tempo condition (<xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref>), resulting in 7–17 different musical segments per tempo and a total of 159–162 segments (trials) per participant. Each segment was repeated only once per tempo but was allowed to occur up to three times at different tempi within one experimental session (tempo difference between two presentations of the same segment was 0.5 Hz minimum). The presentation order of the musical segments was randomly generated for each participant prior to the experiment. The music stimuli were played at 50 dB sensation level (SL), based on individual hearing thresholds that were determined using the method of limits (<xref ref-type="bibr" rid="bib35">Leek, 2001</xref>).</p></sec><sec id="s4-3"><title>Experimental design</title><p>After attaching the EEG electrodes and seating the participant in an acoustically and electrically shielded booth, the participant was asked to follow the instructions on the computer screen (BenQ Monitor XL2420Z, 144 Hz, 24”, 1920 × 1080, Windows 7 Pro (64-bit)). The auditory and visual stimulus presentation was achieved using custom-written Matlab scripts using Psychtoolbox (PTB-3, <xref ref-type="bibr" rid="bib4">Brainard, 1997</xref>) in Matlab (R2017a; The MathWorks, Natick, MA, USA). Upon publication the Source Code for stimulus presentation can be found on the projects OSF repository (<xref ref-type="bibr" rid="bib65">Weineck et al., 2022</xref>).</p><p>The overall experimental flow for each participant can be found in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. First, each participant conducted a self-paced spontaneous motor tempo task (SMT; <xref ref-type="bibr" rid="bib25">Fraisse, 1982</xref>), which is a commonly used technique to assess individual’s preferred tapping rate (<xref ref-type="bibr" rid="bib55">Rimoldi, 1951</xref>, <xref ref-type="bibr" rid="bib39">Mcauley, 2010</xref>). To obtain SMT, each participant tapped for thirty seconds (3 repetitions) at a comfortable rate with a finger on the table close to a contact microphone (Oyster S/P 1605, Schaller GmbH, Postbauer-Heng, Germany). Second, we estimated individual’s hearing threshold using the method of limits. All sounds in this study were delivered by a Fireface soundcard (RME Fireface UCX Audiointerface, Audio AG, Haimhausen, Germany) via on-ear headphones (Beyerdynamics DT-770 Pro, Beyerdynamic GmbH &amp; Co. KG, Heilbronn, Germany). After a short three-trial training, the main task was performed. The music stimuli in the main task were grouped into eight blocks with approximately 20 trials per block and the possibility to take a break in between.</p><p>Each trial comprised two parts: attentive listening (music stimulation without movement) and tapping (music stimulation +finger tapping; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). During attentive listening, one music stimulus was presented (8.3–56.6 s) while the participant looked at a fixation cross on the screen; the participant was instructed to mentally locate the beat without moving. Tapping began after a 1 s interval; the last 5.5 s of the previously listened musical segment were repeated, and participants were instructed to tap a finger to the beat of the musical segment (as indicated by the replacement of the fixation cross by a hand on the computer screen). Note that 5.5 s of tapping data is not sufficient to conduct standard analyses of sensorimotor synchronization; rather, our goal was to confirm that the participants tapped at the intended beat rate based on our tempo manipulation. After each trial, participants were asked to rate the segment based on <italic>enjoyment</italic>/<italic>pleasure</italic>, <italic>familiarity</italic> and <italic>ease of tapping to the beat</italic> with the computer mouse on a visual analogue scale ranging from –100 to +100. At the end of the experiment, the participant performed the SMT task again for three repetitions.</p></sec><sec id="s4-4"><title>EEG data acquisition</title><p>EEG data were acquired using BrainVision Recorder (v.1.21.0303, Brain Products GmbH, Gilching, Germany) and a Brain Products actiCap system with 32 active electrodes attached to an elastic cap based on the international 10–20 location system (actiCAP 64Ch Standard-2 Layout Ch1-32, Brain Products GmbH, Gilching, Germany). The signal was referenced to the FCz electrode and grounded at the AFz position. Electrode impedances were kept below 10 kOhm. The brain activity was acquired using a sampling rate of 1000 Hz via a BrainAmp DC amplifier (BrainAmp ExG, Brain Products GmbH, Gilching, Germany). To ensure correct timing between the recorded EEG data and the auditory stimulation, a TTL trigger pulse over a parallel port was sent at the onset and offset of each musical segment and the stimulus envelope was recorded to an additional channel using a StimTrak (StimTrak, Brain Products GmbH, Gilching, Germany).</p></sec><sec id="s4-5"><title>Data analysis</title><sec id="s4-5-1"><title>Behavioral data</title><p>Tapping data were processed offline with a custom-written Matlab script. To extract the taps, the <italic>*.wav</italic> files were imported and downsampled (from 44.1 kHz to 2205 Hz). The threshold for extracting the taps was adjusted for each trial manually (SMT and music tapping) and trials with irregular tap intervals were rejected. The SMT results were not analyzed as part of this study and will not be discussed further. For the music tapping, only trials with at least three taps (two intervals) were included for further analysis. Five participants were excluded from the music tapping analysis due to irregular and inconsistent taps within a trial (if &gt;40% of the trials were excluded).</p><p>On each trial, participants were asked to rate the musical segments based on <italic>enjoyment</italic>/<italic>pleasure</italic>, <italic>familiarity</italic> and <italic>ease to tap to the beat</italic>. The rating scores were normalized to the maximum absolute rating per participant and per category. For the group analysis the mean and standard error of the mean (SEM) were calculated. For assessing the effects of each subjective dimension on neural synchronization, the 15 trials with the highest and lowest ratings (regardless of the tempo) per participant were further analyzed (see <italic>EEG – Temporal Response Function</italic>).</p></sec><sec id="s4-5-2"><title>Audio analysis</title><p>We assessed neural synchronization to four different musical features (<xref ref-type="fig" rid="fig1">Figure 1B–C</xref>). Note that the term ‘musical feature’ is used to describe time-varying features of music that operate on a similar time-scale as neural synchronization as opposed to the classical musical elements such as syncopation or harmony; (1) Amplitude envelope – gammatone filtered amplitude envelope in the main manuscript and absolute value of the full-band Hilbert envelope in the figure supplement; the gammatone filterbank consisted of 128 channels linearly spaced between 60 and 6000 Hz. (2) Half-wave rectified, first derivative of the amplitude envelope, which detects energy changes over time and is typically more sensitive to onsets (<xref ref-type="bibr" rid="bib13">Daube et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Di Liberto et al., 2020</xref>). (3) Binary-coded beat onsets (0=no beat; 1=beat); a professionally trained percussionist tapped with a wooden drumstick on a MIDI drum pad to the beat of each musical segment at the original tempo (three trials per piece). After latency correction, the final beat times were taken as the average of the two takes with the smallest difference (<xref ref-type="bibr" rid="bib27">Harrison and Müllensiefen, 2018</xref>). (4) Spectral novelty (‘spectral flux’) (<xref ref-type="bibr" rid="bib43">Müller, 2015</xref>) was computed using a custom-written Python script (Python 3.6, Spyder 4.2.0) using the packages <italic>numpy</italic> and <italic>librosa</italic>. For computing the spectral flux of each sound, the spectrogram across frequencies of consecutive frames (frame length = 344 samples) was compared. The calculation of the spectral flux is based on the logarithmic amplitude spectrogram that results in a 1D vector (spectral information fluctuating over time). All stimulus features were z-scored and downsampled to 128 Hz for computing the stimulus-brain synchrony. To account for slightly different numbers of samples between stimulus features, they were cut to have matching sample sizes.</p><p>To validate that each musical feature contained acoustic cues to our tempo manipulation, we conducted a discrete Fourier transform using a Hamming window on each musical segment (resulting frequency resolution of 0.0025 Hz), averaged and z-scored the amplitude spectra per tempo and per musical feature (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><p>To assess how much information the different musical features share, a mutual information (MI) score was computed between each pair of musical features (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). MI (in bits) is a time-sensitive measure that quantifies the reduction of uncertainty for one variable after observing a second variable (<xref ref-type="bibr" rid="bib9">Cover and Thomas, 2005</xref>). MI was computed using <italic>quickMI</italic> from the Neuroscience Information Theory Toolbox with 4 bins, no delay, and a p-value cut-off of 0.001 (<xref ref-type="bibr" rid="bib61">Timme and Lapish, 2018</xref>). For each stimulus feature, all trials were concatenated in the same order for each tempo condition and stimulation subgroup (Time x 13 Tempi x 4 Subgroups). MI values for pairs of musical features were compared to surrogate datasets in which one musical feature was time reversed (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). To statistically assess the shared information between musical features, a three-way ANOVA test was performed (with first factor: data-surrogate comparison; second factor: tempo and third factor: stimulation subgroup).</p></sec><sec id="s4-5-3"><title>EEG data preprocessing</title><p>Unless stated otherwise, all EEG data were analyzed offline using custom-written Matlab code (R2019b; The MathWorks, Natick, MA, USA) combined with the Fieldtrip toolbox (<xref ref-type="bibr" rid="bib49">Oostenveld et al., 2011</xref>). The continuous EEG data were bandpass filtered between 0.5 and 30 Hz (Butterworth filter), re-referenced to the average reference, downsampled to 500 Hz, and epoched between 1 s after stimulus onset (to remove onset responses to the start of the music stimulus) until the end of the initial musical segment presentation (attentive listening part of the trial). Single trials and channels containing large artefacts were removed based on an initial visual inspection. Missing channels were interpolated based on neighbouring channels with a maximum distance of 3 (<italic>ft_prepare_neighbours</italic>). Subsequently, Independent Component Analysis (ICA) was applied to remove artefacts and eye movements semi-automatically. After transforming the data back from component to electrode space, electrodes that exceeded 4 standard deviations of the mean squared data for at least 10% of the recording time were excluded. If bad electrodes were identified, pre-processing for that recording was repeated after removing the identified electrode (<xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref>). For the RCA analysis, if an electrode was identified for which 10% of the trial data exceeded a threshold of mean +2 standard deviations of the single-trial, single-electrode mean squared amplitude, the electrode data of the entire trial was replaced by NaNs. Next, noisy transients of the single-trial, single-electrode recordings were rejected. Therefore, data points were replaced by NaNs when the data points exceeded a threshold of two standard deviations of the single-trial, single-electrode mean squared amplitude. This procedure was repeated four times to ensure that all artefacts were removed (<xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref>). For the TRF analysis, which does not operate on NaNs, noisy transients were replaced by estimates using shape-preserving piecewise cubic spline interpolation or by the interpolation of neighbouring channels for single-trial bad electrodes.</p><p>Next, the data were restructured to match the requirements of the RCA or TRF (see sections <italic>EEG – Temporal Response Function</italic> and <italic>EEG – Reliable Component Analysis</italic>), downsampled to 128 Hz and z-scored. If necessary, the neural data were cut to match the exact sample duration of the stimulus feature per trial. For the RCA analysis approach, the trials in each tempo condition were concatenated resulting in a time-by-electrode matrix (Time x 32 Electrodes; with Time varying across tempo condition). Subsequently the data of participants in the same subgroup were pooled together in a time-by-electrode-by-participant matrix (Time x 32 Electrodes x 9 or 10 Participants depending on the subgroup). In contrast to the RCA, for the TRF analysis, trials in the same stimulation condition were not concatenated in time, but grouped into cell arrays per participant according to the stimulus condition (Tempo x Trials x Electrodes x Time).</p></sec><sec id="s4-5-4"><title>EEG – reliable component analysis</title><p>To reduce data dimensionality and enhance the signal-to-noise ratio, we performed RCA (reliable components analysis, also correlated components analysis) (<xref ref-type="bibr" rid="bib19">Dmochowski et al., 2012</xref>). RCA is designed to capture the maximum correlation between datasets of different participants by combining electrodes linearly into a vector space. One important feature of this technique is that it maximizes the correlation between electrodes across participants (which differentiates it from the similar canonical correlation analysis) (<xref ref-type="bibr" rid="bib37">Madsen et al., 2019</xref>). Using the <italic>rcaRun</italic> Matlab function (<xref ref-type="bibr" rid="bib19">Dmochowski et al., 2012</xref>; <xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref>), the time-by-electrode matrix was transformed to a time-by-component matrix with the maximum across-trial correlation in the first reliable component (RC1), followed by components with correlation values in descending order. For each RCA calculation, for each tempo condition and subgroup, the first three RCs were retained, together with forward-model projections for visualizing the scalp topographies. The next analysis steps in the time and frequency-domain were conducted on the maximally correlated RC1 component.</p><p>To examine the correlation between the neural signal and stimulus over time, the stimulus-response correlation (SRCorr) was calculated for every musical feature. This analysis procedure was adopted from <xref ref-type="bibr" rid="bib29">Kaneshiro et al., 2020</xref>. In brief, every stimulus feature was concatenated in time with trials of the same tempo condition and subgroup to match the neural component-by-time matrix. The stimulus features were temporally filtered to account for the stimulus–brain time lag, and the stimulus features and neural time-courses were correlated. To create a temporal filter, every stimulus feature was transformed into a Toeplitz matrix, where every column repeats the stimulus-feature time course, shifted by one sample up to a maximum shift of 1 s, plus an additional intercept column. The Moore-Penrose pseudoinverse of the Toeplitz matrix and temporal filter was used to calculate the SRCorr. To report the SRCorr, the mean (± SEM) correlation coefficient across tempo conditions for every stimulus feature was calculated. For comparing tempo-specificity between musical features, a linear regression was fit to SRCorr values (and TRF correlations) as a function of tempo for every participant and for every musical feature (using <italic>fitlm</italic>). We compared the resulting slopes across musical features with a one-way ANOVA.</p><p>Stimulus-response coherence (SRCoh) is a measure that quantifies the consistency of phase and amplitude of two signals in a specific frequency band and ranges from 0 (no coherence) to 1 (perfect coherence) (<xref ref-type="bibr" rid="bib59">Srinivasan et al., 2007</xref>). Here, the magnitude-squared coherence between different stimulus features and neural data was computed using the function <italic>mscohere</italic> with a Hamming window of 5 s and 50% overlap, resulting in a frequency range 0–64 Hz with a 0.125 Hz resolution. As strong coherence was found at the stimulation tempo and the first harmonic, the SRCoh values of each frequency vector were compared between musical features.</p><p>In order to control for any frequency-specific differences in the overall power of the neural data that could have led to artificially inflated observed neural synchronization at lower frequencies, the SRCorr and SRCoh values were z-scored based on a surrogate distribution (<xref ref-type="bibr" rid="bib69">Zuk et al., 2021</xref>). Each surrogate distribution was generated by shifting the neural time course by a random amount relative to the musical feature time courses, keeping the time courses of the neural data and musical features intact. For each of 50 iterations, a surrogate distribution was created for each stimulation subgroup and tempo condition. The z-scoring was calculated by subtracting the mean and dividing by the standard deviation of the surrogate distribution.</p></sec><sec id="s4-5-5"><title>EEG – temporal response function</title><p>The TRF is a modeling technique, which computes a filter that optimally describes the relationship between the brain response and stimulus features (<xref ref-type="bibr" rid="bib17">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="bib11">Crosse et al., 2016</xref>). Via linear convolution, the filter delineates how the stimulus features map onto the neural response (forward model), using ridge regression to avoid overfitting (range of lambda values: 10<sup>-6</sup> - 10<sup>6</sup>). All computations of the TRF used the Matlab toolbox “The multivariate Temporal Response Function (mTRF) Toolbox” (<xref ref-type="bibr" rid="bib11">Crosse et al., 2016</xref>). The TRF was calculated in a leave-one-out cross-validation manner for all trials per stimulation tempo; this procedure was repeated for each musical feature separately, and additionally for all musical features together in a multivariate model (using <italic>mTRFcrossval</italic> and <italic>mTRFtrain</italic>) using time lags 0–400ms (<xref ref-type="bibr" rid="bib16">Di Liberto et al., 2020</xref>). For the multivariate TRF approach, the stimulus features were combined by replacing the single time-lag vector by several time-lag vectors for every musical feature (Time x 4 musical features at different time lags). Using <italic>mTRFpredict</italic>, the neural time course of the left-out trial was predicted based on the time course of the corresponding musical feature of that trial. The quality of the predicted neural data was assessed by computing Pearson correlations between the predicted and actual EEG data separately for each electrode (TRF correlations). We averaged over the seven to eight electrodes with the highest TRF correlations that also corresponded to a canonical auditory topography. To quantify differences in the TRFs, the mean TRF correlation across stimulation tempo and/or musical feature was calculated per participant. The TRF weights across time lags were Fisher-z-scored (<xref ref-type="fig" rid="fig3">Figure 3C–F</xref>; <xref ref-type="bibr" rid="bib11">Crosse et al., 2016</xref>). Analogous to the SRCorr and SRCoh, the TRF correlations were z-scored based on subtracting the mean and dividing the standard deviation of a surrogate distribution which was generated by shifting the neural data randomly relative to the musical features during the training and prediction of the TRF for 50 iterations per participant and stimulation tempo.</p><p>We tested the effects of more vs. less modulated music segments on the neural response by comparing TRF correlations within a stimulation tempo condition (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). Therefore, we took up to three trials per participant within the 2.25 Hz stimulation tempo condition where the original tempo ranged between (2.01–2.35 Hz) and compared them to up to three trials where the original tempo was slower (1.25–1.5 Hz). The same analysis was repeated in the 1.5 Hz stimulation tempo condition (original tempo ~1.25–1.6 Hz vs. originally faster music at ~2.1–2.5 Hz).</p><p>The assessment of TRF weights across time lags was accomplished by using a clustering approach for each musical feature and comparing significant data clusters to clusters from a random distribution (<xref ref-type="fig" rid="fig3">Figure 3C–F</xref>). To extract significant time windows in which the TRF weights were able to differentiate the different tempo conditions, a one-way ANOVA was performed at each time point. Clusters (consecutive time windows) were identified if the p-value was below a significance level of 0.01 and the size and F-statistic of those clusters were retained. Next, the clusters were compared to a surrogate dataset, which followed the same procedure, but had the labels of the tempo conditions randomly shuffled before entering it to the ANOVA. This step was repeated 1000 times (permutation testing). At the end, the significance of clusters was evaluated by subtracting the proportion of times the summed F-values of each clusters exceeded the summed F-values of the surrogate clusters from 1. A p-value below 0.05 was considered significant (<xref ref-type="fig" rid="fig3">Figure 3G</xref>). This approach yielded significant regions for the full-band (Hilbert) envelope and derivative (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). As these clusters did not show differences across amplitudes but rather in time, a latency analysis was conducted. Therefore, local minima around the grand average minimum or maximum within the significant time lag window were identified for every participant/tempo condition and the latencies retained. As there was no significant correlation between latencies and tempo conditions, the stimulation tempi were split upon visual inspection into two groups (1–2.5 Hz and 2.75–4 Hz). Subsequently, a piecewise linear regression was fitted to the data and the R<sup>2</sup> and p-values calculated (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>In order to test whether spectral flux predicted the neural signal over and above the information it shared with the amplitude envelope, first derivative and beat onsets, we calculated TRFs for spectral flux after ‘partialing out’ their effects (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). This was achieved by first calculating TRF predictions based on a multivariate model comprising the amplitude envelope, derivate and beat onsets, and second, subtracting those predictions from the ‘actual’ EEG data and using the residual EEG data to compute a spectral flux model.</p><p>TRFs were evaluated based on participant ratings of enjoyment, familiarity, and ease to tap to the beat. Two TRFs were calculated per participant based on the 15 highest and 15 lowest ratings on each measure (ignoring tempo condition and subgroup), and the TRF correlations and time lags were compared between the two groups of trials (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Significant differences between the groups were evaluated based on paired-sample t-tests.</p><p>The effect of musical sophistication was analyzed by computing the Pearson correlation coefficients between the maximum TRF correlation across tempi per participant and the general musical sophistication (Gold-MSI) per participant (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>).</p></sec><sec id="s4-5-6"><title>EEG – comparison of TRF and RCA measures</title><p>The relationship between the TRF analysis approach and the SRCorr was calculated using a linear-mixed effects model (using <italic>fitlme</italic>). Participant and tempo were random (grouping) effects; SRCorr the fixed (predictor) effect and TRF correlations the response variable. To examine the underlying model assumption, the residuals of the linear-mixed effects model were plotted and checked for consistency. The best predictors of the random effects and the fixed-effects coefficients (beta) were computed for every musical feature and illustrated as violin plot (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p></sec></sec><sec id="s4-6"><title>Statistical analysis</title><p>For each analysis, we assessed the overall difference between multiple subgroups using a one-way ANOVA. To test for significant differences across tempo conditions and musical features (TRF Correlation, SRCorr and SRCoh), repeated-measure ANOVAs were conducted coupled to Tukey’s test and Greenhouse-Geiser correction was applied when the assumption of sphericity was violated (as calculated with the Mauchly’s test). As effect size measures, we report partial η<sup>2</sup> for repeated-measures ANOVAs and <italic>r</italic><sub>equivalent</sub> for paired sample t-test (<xref ref-type="bibr" rid="bib57">Rosenthal and Rubin, 2003</xref>). Where applicable, the p-values were corrected using the False Discovery Rate (FDR).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Software, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Formal analysis, Supervision, Funding acquisition, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All participants signed the informed consent before starting the experiment. The study was approved by the Ethics Council of the Max Planck Society Ethics Council in compliance with the Declaration of Helsinki (Application No: 2019_04).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-75515-transrepform1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The source data and source code of all main results, the source code of the musical stimulus presentation and the raw EEG data are freely available on the OSF repository (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/Y5XHS">https://doi.org/10.17605/OSF.IO/Y5XHS</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Weineck</surname><given-names>K</given-names></name><name><surname>Wen</surname><given-names>O</given-names></name><name><surname>Henry</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Neural synchronization is strongest to the spectral flux of slow music and depends on familiarity and beat salience</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/Y5XHS</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank the lab staff of the Max Planck Institute for Empirical Aesthetics for technical support during data acquisition and Lauren Fink for valuable input during data analysis and stimulus feature design.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Assaneo</surname><given-names>MF</given-names></name><name><surname>Ripollés</surname><given-names>P</given-names></name><name><surname>Orpella</surname><given-names>J</given-names></name><name><surname>Lin</surname><given-names>WM</given-names></name><name><surname>de Diego-Balaguer</surname><given-names>R</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spontaneous synchronization to speech reveals neural mechanisms facilitating language learning</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>627</fpage><lpage>632</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0353-z</pub-id><pub-id pub-id-type="pmid">30833700</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bauer</surname><given-names>A-KR</given-names></name><name><surname>Kreutz</surname><given-names>G</given-names></name><name><surname>Herrmann</surname><given-names>CS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Individual musical tempo preference correlates with EEG beta rhythm</article-title><source>Psychophysiology</source><volume>52</volume><fpage>600</fpage><lpage>604</lpage><pub-id pub-id-type="doi">10.1111/psyp.12375</pub-id><pub-id pub-id-type="pmid">25353087</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bello</surname><given-names>JP</given-names></name><name><surname>Daudet</surname><given-names>L</given-names></name><name><surname>Abdallah</surname><given-names>S</given-names></name><name><surname>Duxbury</surname><given-names>C</given-names></name><name><surname>Davies</surname><given-names>M</given-names></name><name><surname>Sandler</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A tutorial on onset detection in music signals</article-title><source>IEEE Transactions on Speech and Audio Processing</source><volume>13</volume><fpage>1035</fpage><lpage>1047</lpage><pub-id pub-id-type="doi">10.1109/TSA.2005.851998</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Psychophysics Toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broderick</surname><given-names>MP</given-names></name><name><surname>Anderson</surname><given-names>AJ</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Semantic context enhances the early auditory encoding of natural speech</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>7564</fpage><lpage>7575</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0584-19.2019</pub-id><pub-id pub-id-type="pmid">31371424</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burger</surname><given-names>B</given-names></name><name><surname>London</surname><given-names>J</given-names></name><name><surname>Thompson</surname><given-names>MR</given-names></name><name><surname>Toiviainen</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Synchronization to metrical levels in music depends on low-frequency spectral components and tempo</article-title><source>Psychological Research</source><volume>82</volume><fpage>1195</fpage><lpage>1211</lpage><pub-id pub-id-type="doi">10.1007/s00426-017-0894-2</pub-id><pub-id pub-id-type="pmid">28712036</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cameron</surname><given-names>DJ</given-names></name><name><surname>Zioga</surname><given-names>I</given-names></name><name><surname>Lindsen</surname><given-names>JP</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Wiggins</surname><given-names>GA</given-names></name><name><surname>Potter</surname><given-names>K</given-names></name><name><surname>Bhattacharya</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural entrainment is associated with subjective groove and complexity for performed but not mechanical musical rhythms</article-title><source>Experimental Brain Research</source><volume>237</volume><fpage>1981</fpage><lpage>1991</lpage><pub-id pub-id-type="doi">10.1007/s00221-019-05557-4</pub-id><pub-id pub-id-type="pmid">31152188</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>JL</given-names></name><name><surname>Penhune</surname><given-names>VB</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Listening to musical rhythms recruits motor regions of the brain</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>2844</fpage><lpage>2854</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn042</pub-id><pub-id pub-id-type="pmid">18388350</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cover</surname><given-names>TM</given-names></name><name><surname>Thomas</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>Entropy, Relative Entropy, and Mutual Information</source><publisher-name>wiley</publisher-name><pub-id pub-id-type="doi">10.1002/047174882X</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Butler</surname><given-names>JS</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Congruent visual speech enhances cortical entrainment to continuous auditory speech in noise-free conditions</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>14195</fpage><lpage>14204</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1829-15.2015</pub-id><pub-id pub-id-type="pmid">26490860</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Bednar</surname><given-names>A</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The Multivariate Temporal Response Function (mTRF) toolbox: A MATLAB toolbox for relating neural signals to continuous stimuli</article-title><source>Frontiers in Human Neuroscience</source><volume>10</volume><elocation-id>604</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00604</pub-id><pub-id pub-id-type="pmid">27965557</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Zuk</surname><given-names>NJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Nidiffer</surname><given-names>AR</given-names></name><name><surname>Molholm</surname><given-names>S</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Linear modeling of neurophysiological responses to speech and other continuous stimuli: methodological considerations for applied research</article-title><source>Frontiers in Neuroscience</source><volume>15</volume><elocation-id>705621</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2021.705621</pub-id><pub-id pub-id-type="pmid">34880719</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daube</surname><given-names>C</given-names></name><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Simple acoustic features can explain phoneme-based predictions of cortical responses to speech</article-title><source>Current Biology</source><volume>29</volume><fpage>1924</fpage><lpage>1937</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.04.067</pub-id><pub-id pub-id-type="pmid">31130454</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Decruy</surname><given-names>L</given-names></name><name><surname>Vanthornhout</surname><given-names>J</given-names></name><name><surname>Francart</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Evidence for enhanced neural tracking of the speech envelope underlying age-related speech-in-noise difficulties</article-title><source>Journal of Neurophysiology</source><volume>122</volume><fpage>601</fpage><lpage>615</lpage><pub-id pub-id-type="doi">10.1152/jn.00687.2018</pub-id><pub-id pub-id-type="pmid">31141449</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>O’Sullivan</surname><given-names>JA</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Low-frequency cortical entrainment to speech reflects phoneme-level processing</article-title><source>Current Biology</source><volume>25</volume><fpage>2457</fpage><lpage>2465</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.030</pub-id><pub-id pub-id-type="pmid">26412129</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Pelofi</surname><given-names>C</given-names></name><name><surname>Bianco</surname><given-names>R</given-names></name><name><surname>Patel</surname><given-names>P</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Herrero</surname><given-names>JL</given-names></name><name><surname>de Cheveigné</surname><given-names>A</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical encoding of melodic expectations in human temporal cortex</article-title><source>eLife</source><volume>9</volume><elocation-id>e51784</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.51784</pub-id><pub-id pub-id-type="pmid">32122465</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural coding of continuous speech in auditory cortex during monaural and dichotic listening</article-title><source>Journal of Neurophysiology</source><volume>107</volume><fpage>78</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1152/jn.00297.2011</pub-id><pub-id pub-id-type="pmid">21975452</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Patel</surname><given-names>AD</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Butler</surname><given-names>H</given-names></name><name><surname>Luo</surname><given-names>C</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Temporal modulations in speech and music</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>81</volume><fpage>181</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.02.011</pub-id><pub-id pub-id-type="pmid">28212857</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dmochowski</surname><given-names>JP</given-names></name><name><surname>Sajda</surname><given-names>P</given-names></name><name><surname>Dias</surname><given-names>J</given-names></name><name><surname>Parra</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Correlated components of ongoing EEG point to emotionally laden attention - a possible marker of engagement?</article-title><source>Frontiers in Human Neuroscience</source><volume>6</volume><elocation-id>112</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2012.00112</pub-id><pub-id pub-id-type="pmid">22623915</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname><given-names>KB</given-names></name><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Ghitza</surname><given-names>O</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual parsing</article-title><source>NeuroImage</source><volume>85 Pt 2</volume><fpage>761</fpage><lpage>768</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.06.035</pub-id><pub-id pub-id-type="pmid">23791839</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname><given-names>KB</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical entrainment to music and its modulation by expertise</article-title><source>PNAS</source><volume>112</volume><fpage>E6233</fpage><lpage>E6242</lpage><pub-id pub-id-type="doi">10.1073/pnas.1508431112</pub-id><pub-id pub-id-type="pmid">26504238</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname><given-names>KB</given-names></name><name><surname>Assaneo</surname><given-names>MF</given-names></name><name><surname>Bevilacqua</surname><given-names>D</given-names></name><name><surname>Pesaran</surname><given-names>B</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An oscillator model better predicts cortical entrainment to music</article-title><source>PNAS</source><volume>116</volume><fpage>10113</fpage><lpage>10121</lpage><pub-id pub-id-type="doi">10.1073/pnas.1816414116</pub-id><pub-id pub-id-type="pmid">31019082</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname><given-names>KB</given-names></name><name><surname>Assaneo</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural oscillations are a start toward understanding brain activity rather than the end</article-title><source>PLOS Biology</source><volume>19</volume><elocation-id>e3001234</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001234</pub-id><pub-id pub-id-type="pmid">33945528</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>RJ</given-names></name><name><surname>Jones</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The role of accent salience and joint accent structure in meter perception</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>35</volume><fpage>264</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1037/a0013482</pub-id><pub-id pub-id-type="pmid">19170487</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fraisse</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>6 - Rhythm and Tempo, Psychology of Music</source><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grahn</surname><given-names>JA</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Rhythm and beat perception in motor areas of the brain</article-title><source>Journal of Cognitive Neuroscience</source><volume>19</volume><fpage>893</fpage><lpage>906</lpage><pub-id pub-id-type="doi">10.1162/jocn.2007.19.5.893</pub-id><pub-id pub-id-type="pmid">17488212</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>PMC</given-names></name><name><surname>Müllensiefen</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Development and validation of the computerised adaptive beat alignment test (CA-BAT)</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>12395</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-30318-8</pub-id><pub-id pub-id-type="pmid">30120265</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="1993">1993</year><source>Dynamics of Musical Patterns: How Do Melody and Rhythm Fit Together? Psychology and Music: The Understanding of Melody and Rhythm</source><publisher-name>Lawrence Erlbaum Associates, Inc</publisher-name></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaneshiro</surname><given-names>B</given-names></name><name><surname>Nguyen</surname><given-names>DT</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Dmochowski</surname><given-names>JP</given-names></name><name><surname>Berger</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Natural music evokes correlated EEG responses reflecting temporal structure and beat</article-title><source>NeuroImage</source><volume>214</volume><elocation-id>116559</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116559</pub-id><pub-id pub-id-type="pmid">31978543</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>C</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Jessen</surname><given-names>S</given-names></name><name><surname>Henry</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Frequency-specific effects in infant electroencephalograms do not require entrained neural oscillations: A commentary on Köster et al. (2019)</article-title><source>Psychological Science</source><volume>32</volume><fpage>966</fpage><lpage>971</lpage><pub-id pub-id-type="doi">10.1177/09567976211001317</pub-id><pub-id pub-id-type="pmid">33979246</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kösem</surname><given-names>A</given-names></name><name><surname>Bosker</surname><given-names>HR</given-names></name><name><surname>Takashima</surname><given-names>A</given-names></name><name><surname>Meyer</surname><given-names>A</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural entrainment determines the words we hear</article-title><source>Current Biology</source><volume>28</volume><fpage>2867</fpage><lpage>2875</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.07.023</pub-id><pub-id pub-id-type="pmid">30197083</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumagai</surname><given-names>Y</given-names></name><name><surname>Matsui</surname><given-names>R</given-names></name><name><surname>Tanaka</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Music familiarity affects EEG entrainment when little attention is paid</article-title><source>Frontiers in Human Neuroscience</source><volume>12</volume><elocation-id>444</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2018.00444</pub-id><pub-id pub-id-type="pmid">30459583</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A new unifying account of the roles of neuronal entrainment</article-title><source>Current Biology</source><volume>29</volume><fpage>R890</fpage><lpage>R905</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.07.075</pub-id><pub-id pub-id-type="pmid">31550478</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Large</surname><given-names>EW</given-names></name><name><surname>Snyder</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Pulse and meter as neural resonance</article-title><source>Annals of the New York Academy of Sciences</source><volume>1169</volume><fpage>46</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2009.04550.x</pub-id><pub-id pub-id-type="pmid">19673754</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leek</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Adaptive procedures in psychophysical research</article-title><source>Perception &amp; Psychophysics</source><volume>63</volume><fpage>1279</fpage><lpage>1292</lpage><pub-id pub-id-type="doi">10.3758/bf03194543</pub-id><pub-id pub-id-type="pmid">11800457</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacDougall</surname><given-names>HG</given-names></name><name><surname>Moore</surname><given-names>ST</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Marching to the beat of the same drummer: the spontaneous tempo of human locomotion</article-title><source>Journal of Applied Physiology</source><volume>99</volume><fpage>1164</fpage><lpage>1173</lpage><pub-id pub-id-type="doi">10.1152/japplphysiol.00138.2005</pub-id><pub-id pub-id-type="pmid">15890757</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madsen</surname><given-names>J</given-names></name><name><surname>Margulis</surname><given-names>EH</given-names></name><name><surname>Simchy-Gross</surname><given-names>R</given-names></name><name><surname>Parra</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity and training</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>3576</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-40254-w</pub-id><pub-id pub-id-type="pmid">30837633</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAuley</surname><given-names>JD</given-names></name><name><surname>Jones</surname><given-names>MR</given-names></name><name><surname>Holub</surname><given-names>S</given-names></name><name><surname>Johnston</surname><given-names>HM</given-names></name><name><surname>Miller</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The time of our lives: life span development of timing and event tracking</article-title><source>Journal of Experimental Psychology. General</source><volume>135</volume><fpage>348</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.135.3.348</pub-id><pub-id pub-id-type="pmid">16846269</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mcauley</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Tempo and Rhythm. Music Perception</source><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib40"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Moelants</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Preferred tempo reconsidered</article-title><conf-name>Proceedings of the 7th international conference on music perception and cognition Citeseer</conf-name><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Motor contributions to the temporal precision of auditory attention</article-title><source>Nature Communications</source><volume>5</volume><elocation-id>5255</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms6255</pub-id><pub-id pub-id-type="pmid">25314898</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müllensiefen</surname><given-names>D</given-names></name><name><surname>Gingras</surname><given-names>B</given-names></name><name><surname>Musil</surname><given-names>J</given-names></name><name><surname>Stewart</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The musicality of non-musicians: an index for assessing musical sophistication in the general population</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e89642</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0089642</pub-id><pub-id pub-id-type="pmid">24586929</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Müller</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Fundamentals of Music Processing: Audio, Analysis, Algorithms, Applications</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-319-21945-5</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicolaou</surname><given-names>N</given-names></name><name><surname>Malik</surname><given-names>A</given-names></name><name><surname>Daly</surname><given-names>I</given-names></name><name><surname>Weaver</surname><given-names>J</given-names></name><name><surname>Hwang</surname><given-names>F</given-names></name><name><surname>Kirke</surname><given-names>A</given-names></name><name><surname>Roesch</surname><given-names>EB</given-names></name><name><surname>Williams</surname><given-names>D</given-names></name><name><surname>Miranda</surname><given-names>ER</given-names></name><name><surname>Nasuto</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Directed motor-auditory EEG connectivity is modulated by music tempo</article-title><source>Frontiers in Human Neuroscience</source><volume>11</volume><elocation-id>502</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2017.00502</pub-id><pub-id pub-id-type="pmid">29093672</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Peretz</surname><given-names>I</given-names></name><name><surname>Missal</surname><given-names>M</given-names></name><name><surname>Mouraux</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Tagging the neuronal entrainment to beat and meter</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>10234</fpage><lpage>10240</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0411-11.2011</pub-id><pub-id pub-id-type="pmid">21753000</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Peretz</surname><given-names>I</given-names></name><name><surname>Mouraux</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Selective neuronal entrainment to the beat and meter embedded in a musical rhythm</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>17572</fpage><lpage>17581</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3203-12.2012</pub-id><pub-id pub-id-type="pmid">23223281</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural entrainment and attentional selection in the listening brain</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>913</fpage><lpage>926</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.08.004</pub-id><pub-id pub-id-type="pmid">31606386</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Olivera</surname><given-names>J</given-names></name><name><surname>Gouyon</surname><given-names>F</given-names></name><name><surname>Martins</surname><given-names>L</given-names></name><name><surname>Reis</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><data-title>IBT: A real-time tempo and beat tracking system</data-title><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1416470">https://doi.org/10.5281/zenodo.1416470</ext-link></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Fieldtrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Parra</surname><given-names>L</given-names></name><name><surname>Haufe</surname><given-names>S</given-names></name><name><surname>Haufe</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Correlated Components Analysis --- Extracting Reliable Dimensions in Multivariate Data</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1801.08881">https://arxiv.org/abs/1801.08881</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Rhythm in language and music: parallels and differences</article-title><source>Annals of the New York Academy of Sciences</source><volume>999</volume><fpage>140</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1196/annals.1284.015</pub-id><pub-id pub-id-type="pmid">14681127</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural oscillations carry speech rhythm through to comprehension</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>320</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00320</pub-id><pub-id pub-id-type="pmid">22973251</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfordresher</surname><given-names>PQ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The role of melodic and rhythmic accents in musical structure</article-title><source>Music Perception</source><volume>20</volume><fpage>431</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1525/mp.2003.20.4.431</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reetzke</surname><given-names>R</given-names></name><name><surname>Gnanateja</surname><given-names>GN</given-names></name><name><surname>Chandrasekaran</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural tracking of the speech envelope is differentially modulated by attention and language experience</article-title><source>Brain and Language</source><volume>213</volume><elocation-id>104891</elocation-id><pub-id pub-id-type="doi">10.1016/j.bandl.2020.104891</pub-id><pub-id pub-id-type="pmid">33290877</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rimoldi</surname><given-names>HJA</given-names></name></person-group><year iso-8601-date="1951">1951</year><article-title>Personal tempo</article-title><source>The Journal of Abnormal and Social Psychology</source><volume>46</volume><fpage>283</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1037/h0057479</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rose</surname><given-names>D</given-names></name><name><surname>Cameron</surname><given-names>DJ</given-names></name><name><surname>Lovatt</surname><given-names>PJ</given-names></name><name><surname>Grahn</surname><given-names>JA</given-names></name><name><surname>Annett</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Comparison of spontaneous motor tempo during finger tapping, toe tapping and stepping on the spot in people with and without parkinson’s disease</article-title><source>Journal of Movement Disorders</source><volume>13</volume><fpage>47</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.14802/jmd.19043</pub-id><pub-id pub-id-type="pmid">31986868</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenthal</surname><given-names>R</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>r equivalent: A simple effect size indicator</article-title><source>Psychological Methods</source><volume>8</volume><fpage>492</fpage><lpage>496</lpage><pub-id pub-id-type="doi">10.1037/1082-989X.8.4.492</pub-id><pub-id pub-id-type="pmid">14664684</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>RV</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>Speech and Music Have Different Requirements for Spectral Resolution</source><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>R</given-names></name><name><surname>Winter</surname><given-names>WR</given-names></name><name><surname>Ding</surname><given-names>J</given-names></name><name><surname>Nunez</surname><given-names>PL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>EEG and MEG coherence: measures of functional connectivity at distinct spatial scales of neocortical dynamics</article-title><source>Journal of Neuroscience Methods</source><volume>166</volume><fpage>41</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.06.026</pub-id><pub-id pub-id-type="pmid">17698205</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tierney</surname><given-names>A</given-names></name><name><surname>Kraus</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural entrainment to the rhythmic structure of music</article-title><source>Journal of Cognitive Neuroscience</source><volume>27</volume><fpage>400</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00704</pub-id><pub-id pub-id-type="pmid">25170794</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Timme</surname><given-names>NM</given-names></name><name><surname>Lapish</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A tutorial for information theory in neuroscience</article-title><source>ENeuro</source><volume>5</volume><elocation-id>ENEURO.0052-18.2018</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0052-18.2018</pub-id><pub-id pub-id-type="pmid">30211307</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Bree</surname><given-names>S</given-names></name><name><surname>Alamia</surname><given-names>A</given-names></name><name><surname>Zoefel</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Oscillation or not-Why we can and need to know (commentary on Doelling and Assaneo, 2021)</article-title><source>The European Journal of Neuroscience</source><volume>55</volume><fpage>201</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1111/ejn.15542</pub-id><pub-id pub-id-type="pmid">34817088</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanden Bosch der Nederlanden</surname><given-names>CM</given-names></name><name><surname>Joanisse</surname><given-names>MF</given-names></name><name><surname>Grahn</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Music as a scaffold for listening to speech: Better neural phase-locking to song than speech</article-title><source>NeuroImage</source><volume>214</volume><elocation-id>116767</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116767</pub-id><pub-id pub-id-type="pmid">32217165</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanden Bosch der Nederlanden</surname><given-names>CM</given-names></name><name><surname>Joanisse</surname><given-names>MF</given-names></name><name><surname>Grahn</surname><given-names>JA</given-names></name><name><surname>Snijders</surname><given-names>TM</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Familiarity modulates neural tracking of sung and spoken utterances</article-title><source>NeuroImage</source><volume>252</volume><elocation-id>119049</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119049</pub-id><pub-id pub-id-type="pmid">35248707</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Weineck</surname><given-names>K</given-names></name><name><surname>Wen</surname><given-names>OX</given-names></name><name><surname>Henry</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Neural synchronization is strongest to the spectral flux of slow music and depends on familiarity and beat salience, OSF</article-title><ext-link ext-link-type="uri" xlink:href="https://osf.io/y5xhs/?view_only=08032acf6ea540418147789d6d269187">https://osf.io/y5xhs/?view_only=08032acf6ea540418147789d6d269187</ext-link><date-in-citation iso-8601-date="2022-07-05">July 5, 2022</date-in-citation></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wollman</surname><given-names>I</given-names></name><name><surname>Arias</surname><given-names>P</given-names></name><name><surname>Aucouturier</surname><given-names>J-J</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural entrainment to music is sensitive to melodic spectral complexity</article-title><source>Journal of Neurophysiology</source><volume>123</volume><fpage>1063</fpage><lpage>1071</lpage><pub-id pub-id-type="doi">10.1152/jn.00758.2018</pub-id><pub-id pub-id-type="pmid">32023136</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zalta</surname><given-names>A</given-names></name><name><surname>Petkoski</surname><given-names>S</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Natural rhythms of periodic temporal attention</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>1051</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-14888-8</pub-id><pub-id pub-id-type="pmid">32103014</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Penhune</surname><given-names>VB</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Structure and function of auditory cortex: music and speech</article-title><source>Trends in Cognitive Sciences</source><volume>6</volume><fpage>37</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01816-7</pub-id><pub-id pub-id-type="pmid">11849614</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuk</surname><given-names>NJ</given-names></name><name><surname>Murphy</surname><given-names>JW</given-names></name><name><surname>Reilly</surname><given-names>RB</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Envelope reconstruction of speech and music highlights stronger tracking of speech at low frequencies</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1009358</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009358</pub-id><pub-id pub-id-type="pmid">34534211</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Overview over the music stimuli.</title><p>Parameters of stimulus creation for all 72 musical stimulus segments. The columns indicate 1. the stimulus number, 2. title of the musical piece, 3. the Artist of each musical piece, 4. the CD each piece was taken from (available at Qobuz Downloadstore), 5. timestamp of the music segment onset relative to the start of the recording [min.sec,ms], 6. duration of the music segment [sec] relative to the start of the music segment, 7. original tempo of excerpt [BPM; beats per minute] based on the taps of the authors and their colleagues and 8. frequency range [Hz] of the tempo-modulation (in 0.25 Hz steps) for each music piece.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">No.</th><th align="left" valign="bottom">Title</th><th align="left" valign="bottom">Artist</th><th align="left" valign="bottom">CD</th><th align="left" valign="bottom">Start [min]</th><th align="left" valign="bottom">Duration [sec]</th><th align="left" valign="bottom" colspan="2">Tempo [BPM/Hz]</th><th align="left" valign="bottom">Range [Hz]</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">Abba Medley</td><td align="left" valign="bottom">Super Troopers</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">7.30,71</td><td align="char" char="." valign="bottom">22,99</td><td align="char" char="." valign="bottom">136.09 /</td><td align="char" char="." valign="bottom">2.27</td><td align="char" char="ndash" valign="bottom">1.5–3.75</td></tr><tr><td align="char" char="." valign="bottom">2</td><td align="left" valign="bottom">Abba Medley</td><td align="left" valign="bottom">Super Troopers</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">8.59,57</td><td align="char" char="." valign="bottom">21,69</td><td align="char" char="." valign="bottom">135.92 /</td><td align="char" char="." valign="bottom">2.27</td><td align="char" char="ndash" valign="bottom">1.75–4</td></tr><tr><td align="char" char="." valign="bottom">3</td><td align="left" valign="bottom">All is Alive</td><td align="left" valign="bottom">Francesco P.</td><td align="left" valign="bottom">Instrumental Hits, Vol.1</td><td align="char" char="." valign="bottom">1.22,85</td><td align="char" char="." valign="bottom">21,41</td><td align="char" char="." valign="bottom">128.27 /</td><td align="char" char="." valign="bottom">2.14</td><td align="char" char="ndash" valign="bottom">1.5–4</td></tr><tr><td align="char" char="." valign="bottom">4</td><td align="left" valign="bottom">All is Alive</td><td align="left" valign="bottom">Francesco P.</td><td align="left" valign="bottom">Instrumental Hits, Vol.1</td><td align="char" char="." valign="bottom">2.06,72</td><td align="char" char="." valign="bottom">21,62</td><td align="char" char="." valign="bottom">127.98 /</td><td align="char" char="." valign="bottom">2.13</td><td align="char" char="ndash" valign="bottom">1.5–4</td></tr><tr><td align="char" char="." valign="bottom">5</td><td align="left" valign="bottom">Apache</td><td align="left" valign="bottom">The Shadows</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.39,59</td><td align="char" char="." valign="bottom">14,78</td><td align="char" char="." valign="bottom">135.22 /</td><td align="char" char="." valign="bottom">2.25</td><td align="char" char="ndash" valign="bottom">1.75–4</td></tr><tr><td align="char" char="." valign="bottom">6</td><td align="left" valign="bottom">Apache</td><td align="left" valign="bottom">The Shadows</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.54,25</td><td align="char" char="." valign="bottom">21,60</td><td align="char" char="." valign="bottom">133.86 /</td><td align="char" char="." valign="bottom">2.23</td><td align="char" char="ndash" valign="bottom">1.75–4</td></tr><tr><td align="char" char="." valign="bottom">7</td><td align="left" valign="bottom">La Bikina</td><td align="left" valign="bottom">Rubén Fuentes Gasson</td><td align="left" valign="bottom">Bachata</td><td align="char" char="." valign="bottom">0.47,49</td><td align="char" char="." valign="bottom">14,99</td><td align="char" char="." valign="bottom">124.83 /</td><td align="char" char="." valign="bottom">2.08</td><td align="char" char="ndash" valign="bottom">1.75–4</td></tr><tr><td align="char" char="." valign="bottom">8</td><td align="left" valign="bottom">Bulldog</td><td align="left" valign="bottom">The Ventures</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.06,15</td><td align="char" char="." valign="bottom">18,13</td><td align="char" char="." valign="bottom">151.33 /</td><td align="char" char="." valign="bottom">2.52</td><td align="char" char="ndash" valign="bottom">2–4</td></tr><tr><td align="char" char="." valign="bottom">9</td><td align="left" valign="bottom">Careless Whisper</td><td align="left" valign="bottom">Mads Haaber</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">2.41,93</td><td align="char" char="." valign="bottom">25,50</td><td align="char" char="." valign="bottom">76.93 /</td><td align="char" char="." valign="bottom">1.28</td><td align="char" char="ndash" valign="bottom">1–2.75</td></tr><tr><td align="char" char="." valign="bottom">10</td><td align="left" valign="bottom">Cocaine</td><td align="left" valign="bottom">Corben Cassavette</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">1.39,29</td><td align="char" char="." valign="bottom">25,74</td><td align="char" char="." valign="bottom">105.13 /</td><td align="char" char="." valign="bottom">1.75</td><td align="char" char="ndash" valign="bottom">1.5–4</td></tr><tr><td align="char" char="." valign="bottom">11</td><td align="left" valign="bottom">Dark Place</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">0.20,83</td><td align="char" char="." valign="bottom">22,22</td><td align="char" char="." valign="bottom">92.13 /</td><td align="char" char="." valign="bottom">1.54</td><td align="char" char="ndash" valign="bottom">1–3.75</td></tr><tr><td align="char" char="." valign="bottom">12</td><td align="left" valign="bottom">F.B. I.</td><td align="left" valign="bottom">The Shadows</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.20,59</td><td align="char" char="." valign="bottom">15,31</td><td align="char" char="." valign="bottom">140.05 /</td><td align="char" char="." valign="bottom">2.33</td><td align="char" char="ndash" valign="bottom">1.25–4</td></tr><tr><td align="char" char="." valign="bottom">13</td><td align="left" valign="bottom">Five Trips</td><td align="left" valign="bottom">Tr3ntatr3 Giri</td><td align="left" valign="bottom">Instrumental Hits, Vol.1</td><td align="char" char="." valign="bottom">1.48,79</td><td align="char" char="." valign="bottom">16,48</td><td align="char" char="." valign="bottom">123.24 /</td><td align="char" char="." valign="bottom">2.05</td><td align="char" char="ndash" valign="bottom">1.5–4</td></tr><tr><td align="char" char="." valign="bottom">14</td><td align="left" valign="bottom">Guybo</td><td align="left" valign="bottom">Eddie Cochran</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.18,99</td><td align="char" char="." valign="bottom">16,28</td><td align="char" char="." valign="bottom">110.00 /</td><td align="char" char="." valign="bottom">1.83</td><td align="char" char="ndash" valign="bottom">1.5–3</td></tr><tr><td align="char" char="." valign="bottom">15</td><td align="left" valign="bottom">Gypsy Salsa, Cha Cha Beat</td><td align="left" valign="bottom">Corp Latino Dance Group</td><td align="left" valign="bottom">Hot Latin Dance</td><td align="char" char="." valign="bottom">1.57,06</td><td align="char" char="." valign="bottom">24,26</td><td align="char" char="." valign="bottom">100.04 /</td><td align="char" char="." valign="bottom">1.67</td><td align="char" char="ndash" valign="bottom">1.5–3</td></tr><tr><td align="char" char="." valign="bottom">16</td><td align="left" valign="bottom">Highway Riderz</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">0.19,61</td><td align="char" char="." valign="bottom">30,30</td><td align="char" char="." valign="bottom">97.13 /</td><td align="char" char="." valign="bottom">1.62</td><td align="char" char="ndash" valign="bottom">1–4</td></tr><tr><td align="char" char="." valign="bottom">17</td><td align="left" valign="bottom">In Go</td><td align="left" valign="bottom">Chuck Berry</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.26,10</td><td align="char" char="." valign="bottom">24,00</td><td align="char" char="." valign="bottom">116.30 /</td><td align="char" char="." valign="bottom">1.94</td><td align="char" char="ndash" valign="bottom">1.5–4</td></tr><tr><td align="char" char="." valign="bottom">18</td><td align="left" valign="bottom">Oh by Jingo!</td><td align="left" valign="bottom">Chet Atkins</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.07,47</td><td align="char" char="." valign="bottom">23,59</td><td align="char" char="." valign="bottom">120.55 /</td><td align="char" char="." valign="bottom">2.01</td><td align="char" char="ndash" valign="bottom">1–3.5</td></tr><tr><td align="char" char="." valign="bottom">19</td><td align="left" valign="bottom">Keep It 1,000</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">1.13,87</td><td align="char" char="." valign="bottom">25,10</td><td align="char" char="." valign="bottom">78.06 /</td><td align="char" char="." valign="bottom">1.30</td><td align="char" char="ndash" valign="bottom">1–3</td></tr><tr><td align="char" char="." valign="bottom">20</td><td align="left" valign="bottom">The Last Day</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">1.27,40</td><td align="char" char="." valign="bottom">22,52</td><td align="char" char="." valign="bottom">88.13 /</td><td align="char" char="." valign="bottom">1.47</td><td align="char" char="ndash" valign="bottom">1–2.5</td></tr><tr><td align="char" char="." valign="bottom">21</td><td align="left" valign="bottom">For the Last Time</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">1.16,88</td><td align="char" char="." valign="bottom">26,21</td><td align="char" char="." valign="bottom">74.86 /</td><td align="char" char="." valign="bottom">1.25</td><td align="char" char="ndash" valign="bottom">1–3.25</td></tr><tr><td align="char" char="." valign="bottom">22</td><td align="left" valign="bottom">Lights Out</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">0.32,12</td><td align="char" char="." valign="bottom">21,60</td><td align="char" char="." valign="bottom">89.12 /</td><td align="char" char="." valign="bottom">1.49</td><td align="char" char="ndash" valign="bottom">1.25–3.5</td></tr><tr><td align="char" char="." valign="bottom">23</td><td align="left" valign="bottom">I like</td><td align="left" valign="bottom">Francesco P.</td><td align="left" valign="bottom">Instrumental Hits, Vol.1</td><td align="char" char="." valign="bottom">1.34,19</td><td align="char" char="." valign="bottom">27,95</td><td align="char" char="." valign="bottom">112.13 /</td><td align="char" char="." valign="bottom">1.87</td><td align="char" char="ndash" valign="bottom">1.25–3.75</td></tr><tr><td align="char" char="." valign="bottom">24</td><td align="left" valign="bottom">Dark Line</td><td align="left" valign="bottom">Alex Cundari</td><td align="left" valign="bottom">Instrumental Hits, Vol.1</td><td align="char" char="." valign="bottom">0.37,81</td><td align="char" char="." valign="bottom">19,75</td><td align="char" char="." valign="bottom">106.08 /</td><td align="char" char="." valign="bottom">1.77</td><td align="char" char="ndash" valign="bottom">1.25–3</td></tr><tr><td align="char" char="." valign="bottom">25</td><td align="left" valign="bottom">Live Forever</td><td align="left" valign="bottom">The Wonderwalls</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">1.16,28</td><td align="char" char="." valign="bottom">22,61</td><td align="char" char="." valign="bottom">90.11 /</td><td align="char" char="." valign="bottom">1.50</td><td align="char" char="ndash" valign="bottom">1.5–3.75</td></tr><tr><td align="char" char="." valign="bottom">26</td><td align="left" valign="bottom">Lucy in the Sky with Diamonds</td><td align="left" valign="bottom">Ricardo Caliente</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">2.43,55</td><td align="char" char="." valign="bottom">23,93</td><td align="char" char="." valign="bottom">81.11 /</td><td align="char" char="." valign="bottom">1.35</td><td align="char" char="ndash" valign="bottom">1–3.5</td></tr><tr><td align="char" char="." valign="bottom">27</td><td align="left" valign="bottom">Monalisa</td><td align="left" valign="bottom">Ken Laszlo</td><td align="left" valign="bottom">Instrumental Hits, Vol.1</td><td align="char" char="." valign="bottom">1.13,37</td><td align="char" char="." valign="bottom">29,79</td><td align="char" char="." valign="bottom">129.97 /</td><td align="char" char="." valign="bottom">2.17</td><td align="char" char="ndash" valign="bottom">1.5–4</td></tr><tr><td align="char" char="." valign="bottom">28</td><td align="left" valign="bottom">Monalisa</td><td align="left" valign="bottom">Ken Laszlo</td><td align="left" valign="bottom">Instrumental Hits, Vol.1</td><td align="char" char="." valign="bottom">2.13,01</td><td align="char" char="." valign="bottom">28,80</td><td align="char" char="." valign="bottom">130.02 /</td><td align="char" char="." valign="bottom">2.17</td><td align="char" char="ndash" valign="bottom">1.5–4</td></tr><tr><td align="char" char="." valign="bottom">29</td><td align="left" valign="bottom">Can't Fight the Moonlight</td><td align="left" valign="bottom">Jon Carran</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">1.10,02</td><td align="char" char="." valign="bottom">18,56</td><td align="char" char="." valign="bottom">97.95 /</td><td align="char" char="." valign="bottom">1.63</td><td align="char" char="ndash" valign="bottom">1.5–3.75</td></tr><tr><td align="char" char="." valign="bottom">30</td><td align="left" valign="bottom">Muy Tranquilo</td><td align="left" valign="bottom">Gramatik</td><td align="left" valign="bottom">Muy Tranquilo</td><td align="char" char="." valign="bottom">2.05,1</td><td align="char" char="." valign="bottom">26,59</td><td align="char" char="." valign="bottom">90.04 /</td><td align="char" char="." valign="bottom">1.50</td><td align="char" char="ndash" valign="bottom">1–3.25</td></tr><tr><td align="char" char="." valign="bottom">31</td><td align="left" valign="bottom">No Mercy</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">0.12,15</td><td align="char" char="." valign="bottom">26,41</td><td align="char" char="." valign="bottom">76.11 /</td><td align="char" char="." valign="bottom">1.27</td><td align="char" char="ndash" valign="bottom">1–3.5</td></tr><tr><td align="char" char="." valign="bottom">32</td><td align="left" valign="bottom">I'm A Pusha</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">0.11,07</td><td align="char" char="." valign="bottom">22,60</td><td align="char" char="." valign="bottom">85.15 /</td><td align="char" char="." valign="bottom">1.42</td><td align="char" char="ndash" valign="bottom">1–3.25</td></tr><tr><td align="char" char="." valign="bottom">33</td><td align="left" valign="bottom">Rockin' the Blues Away</td><td align="left" valign="bottom">Tiny Grimes Quintet</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.05,61</td><td align="char" char="." valign="bottom">20,84</td><td align="char" char="." valign="bottom">141.05 /</td><td align="char" char="." valign="bottom">2.35</td><td align="char" char="ndash" valign="bottom">1.5–4</td></tr><tr><td align="char" char="." valign="bottom">34</td><td align="left" valign="bottom">The Rocking Guitar</td><td align="left" valign="bottom">Ini Kamoze</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.17,39</td><td align="char" char="." valign="bottom">16,21</td><td align="char" char="." valign="bottom">118.66 /</td><td align="char" char="." valign="bottom">1.98</td><td align="char" char="ndash" valign="bottom">1.5–3.25</td></tr><tr><td align="char" char="." valign="bottom">35</td><td align="left" valign="bottom">Country Rodeo Song</td><td align="left" valign="bottom">Marco Rinaldo</td><td align="left" valign="bottom">Country Instrumental Mix</td><td align="char" char="." valign="bottom">1.46,35</td><td align="char" char="." valign="bottom">27,70</td><td align="char" char="." valign="bottom">112.94 /</td><td align="char" char="." valign="bottom">1.88</td><td align="char" char="ndash" valign="bottom">1.5–3.75</td></tr><tr><td align="char" char="." valign="bottom">36</td><td align="left" valign="bottom">I Shot the Sheriff</td><td align="left" valign="bottom">Corben Cassavette</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">0.19,52</td><td align="char" char="." valign="bottom">25,54</td><td align="char" char="." valign="bottom">94.12 /</td><td align="char" char="." valign="bottom">1.57</td><td align="char" char="ndash" valign="bottom">1.25–4</td></tr><tr><td align="char" char="." valign="bottom">37</td><td align="left" valign="bottom">Sing Sing Sing</td><td align="left" valign="bottom">Benny Goodman</td><td align="left" valign="bottom">Sing Sing Sing</td><td align="char" char="." valign="bottom">0.18,23</td><td align="char" char="." valign="bottom">36,01</td><td align="char" char="." valign="bottom">108.68 /</td><td align="char" char="." valign="bottom">1.81</td><td align="char" char="ndash" valign="bottom">1.5–3</td></tr><tr><td align="char" char="." valign="bottom">38</td><td align="left" valign="bottom">Si Una Vez</td><td align="left" valign="bottom">Pete Astudillo</td><td align="left" valign="bottom">Bachata</td><td align="char" char="." valign="bottom">0.46,54</td><td align="char" char="." valign="bottom">16,95</td><td align="char" char="." valign="bottom">124.54 /</td><td align="char" char="." valign="bottom">2.08</td><td align="char" char="ndash" valign="bottom">1.5–3</td></tr><tr><td align="char" char="." valign="bottom">39</td><td align="left" valign="bottom">I'm Still Standing</td><td align="left" valign="bottom">Ricardo Caliente</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">0.39,14</td><td align="char" char="." valign="bottom">21,72</td><td align="char" char="." valign="bottom">86.04 /</td><td align="char" char="." valign="bottom">1.43</td><td align="char" char="ndash" valign="bottom">1.25–2.75</td></tr><tr><td align="char" char="." valign="bottom">40</td><td align="left" valign="bottom">Streets On Fire</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">0.23,33</td><td align="char" char="." valign="bottom">24,99</td><td align="char" char="." valign="bottom">81.16 /</td><td align="char" char="." valign="bottom">1.35</td><td align="char" char="ndash" valign="bottom">1–3.75</td></tr><tr><td align="char" char="." valign="bottom">41</td><td align="left" valign="bottom">Tequila</td><td align="left" valign="bottom">The Champs</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">1.02,85</td><td align="char" char="." valign="bottom">21,45</td><td align="char" char="." valign="bottom">89.40 /</td><td align="char" char="." valign="bottom">1.49</td><td align="char" char="ndash" valign="bottom">1–3</td></tr><tr><td align="char" char="." valign="bottom">42</td><td align="left" valign="bottom">Vegas Dream</td><td align="left" valign="bottom">Vegas Project</td><td align="left" valign="bottom">Instrumental Hits, Vol.1</td><td align="char" char="." valign="bottom">1.13,73</td><td align="char" char="." valign="bottom">22,73</td><td align="char" char="." valign="bottom">128.08 /</td><td align="char" char="." valign="bottom">2.13</td><td align="char" char="ndash" valign="bottom">1.5–3.25</td></tr><tr><td align="char" char="." valign="bottom">43</td><td align="left" valign="bottom">I Can't Wait</td><td align="left" valign="bottom">Alex Cundari</td><td align="left" valign="bottom">Instrumental Hits, Vol.1</td><td align="char" char="." valign="bottom">0.23,73</td><td align="char" char="." valign="bottom">24,08</td><td align="char" char="." valign="bottom">83.59 /</td><td align="char" char="." valign="bottom">1.39</td><td align="char" char="ndash" valign="bottom">1–2.5</td></tr><tr><td align="char" char="." valign="bottom">44</td><td align="left" valign="bottom">Who Dat</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">1.15,58</td><td align="char" char="." valign="bottom">24,50</td><td align="char" char="." valign="bottom">87.10 /</td><td align="char" char="." valign="bottom">1.45</td><td align="char" char="ndash" valign="bottom">1–3.25</td></tr><tr><td align="char" char="." valign="bottom">45</td><td align="left" valign="bottom">Abba Medley</td><td align="left" valign="bottom">Super Troopers</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">0.28,38</td><td align="char" char="." valign="bottom">27,05</td><td align="char" char="." valign="bottom">136.09 /</td><td align="char" char="." valign="bottom">2.27</td><td align="char" char="ndash" valign="bottom">1.25–4</td></tr><tr><td align="char" char="." valign="bottom">46</td><td align="left" valign="bottom">Abba Medley</td><td align="left" valign="bottom">Super Troopers</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">5.09,89</td><td align="char" char="." valign="bottom">21,71</td><td align="char" char="." valign="bottom">136.09 /</td><td align="char" char="." valign="bottom">2.27</td><td align="char" char="ndash" valign="bottom">1–3</td></tr><tr><td align="char" char="." valign="bottom">47</td><td align="left" valign="bottom">Abba Medley</td><td align="left" valign="bottom">Super Troopers</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">6.03,59</td><td align="char" char="." valign="bottom">20,64</td><td align="char" char="." valign="bottom">136.09 /</td><td align="char" char="." valign="bottom">2.27</td><td align="char" char="ndash" valign="bottom">1.5–4</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom">La Bikina</td><td align="left" valign="bottom">Rubén Fuentes Gasson</td><td align="left" valign="bottom">Bachata</td><td align="char" char="." valign="bottom">1.18,31</td><td align="char" char="." valign="bottom">46,00</td><td align="char" char="." valign="bottom">124.83 /</td><td align="char" char="." valign="bottom">2.08</td><td align="char" char="ndash" valign="bottom">1.75–4</td></tr><tr><td align="char" char="." valign="bottom">49</td><td align="left" valign="bottom">Bulldog</td><td align="left" valign="bottom">The Ventures</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.44,98</td><td align="char" char="." valign="bottom">19,20</td><td align="char" char="." valign="bottom">151.33 /</td><td align="char" char="." valign="bottom">2.52</td><td align="char" char="ndash" valign="bottom">2–4</td></tr><tr><td align="char" char="." valign="bottom">50</td><td align="left" valign="bottom">Bulldog</td><td align="left" valign="bottom">The Ventures</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">1.21,25</td><td align="char" char="." valign="bottom">38,85</td><td align="char" char="." valign="bottom">151.33 /</td><td align="char" char="." valign="bottom">2.52</td><td align="char" char="ndash" valign="bottom">2–4</td></tr><tr><td align="char" char="." valign="bottom">51</td><td align="left" valign="bottom">Careless Whisper</td><td align="left" valign="bottom">Mads Haaber</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">3.09,44</td><td align="char" char="." valign="bottom">24,41</td><td align="char" char="." valign="bottom">76.93 /</td><td align="char" char="." valign="bottom">1.28</td><td align="char" char="ndash" valign="bottom">1–2.75</td></tr><tr><td align="char" char="." valign="bottom">52</td><td align="left" valign="bottom">Dark Place</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">1.45,73</td><td align="char" char="." valign="bottom">35,10</td><td align="char" char="." valign="bottom">92.13 /</td><td align="char" char="." valign="bottom">1.54</td><td align="char" char="ndash" valign="bottom">1–3.75</td></tr><tr><td align="char" char="." valign="bottom">53</td><td align="left" valign="bottom">F.B. I.</td><td align="left" valign="bottom">The Shadows</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.37,30</td><td align="char" char="." valign="bottom">20,91</td><td align="char" char="." valign="bottom">140.05 /</td><td align="char" char="." valign="bottom">2.33</td><td align="char" char="ndash" valign="bottom">1.5–4</td></tr><tr><td align="char" char="." valign="bottom">54</td><td align="left" valign="bottom">Guybo</td><td align="left" valign="bottom">Eddie Cochran</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.36,31</td><td align="char" char="." valign="bottom">17,53</td><td align="char" char="." valign="bottom">110.00 /</td><td align="char" char="." valign="bottom">1.83</td><td align="char" char="ndash" valign="bottom">1.5–3</td></tr><tr><td align="char" char="." valign="bottom">55</td><td align="left" valign="bottom">Highway Riderz</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">0.59,19</td><td align="char" char="." valign="bottom">20,40</td><td align="char" char="." valign="bottom">97.13 /</td><td align="char" char="." valign="bottom">1.62</td><td align="char" char="ndash" valign="bottom">1–4</td></tr><tr><td align="char" char="." valign="bottom">56</td><td align="left" valign="bottom">In Go</td><td align="left" valign="bottom">Chuck Berry</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.48,42</td><td align="char" char="." valign="bottom">26,80</td><td align="char" char="." valign="bottom">116.30 /</td><td align="char" char="." valign="bottom">1.94</td><td align="char" char="ndash" valign="bottom">1.5–4</td></tr><tr><td align="char" char="." valign="bottom">57</td><td align="left" valign="bottom">Oh by Jingo!</td><td align="left" valign="bottom">Chet Atkins</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.40,89</td><td align="char" char="." valign="bottom">23,00</td><td align="char" char="." valign="bottom">120.55 /</td><td align="char" char="." valign="bottom">2.01</td><td align="char" char="ndash" valign="bottom">1.25–3.5</td></tr><tr><td align="char" char="." valign="bottom">58</td><td align="left" valign="bottom">Live Forever</td><td align="left" valign="bottom">The Wonderwalls</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">1.41,02</td><td align="char" char="." valign="bottom">34,00</td><td align="char" char="." valign="bottom">90.11 /</td><td align="char" char="." valign="bottom">1.50</td><td align="char" char="ndash" valign="bottom">1.25–3.5</td></tr><tr><td align="char" char="." valign="bottom">59</td><td align="left" valign="bottom">Live Forever</td><td align="left" valign="bottom">The Wonderwalls</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">3.35,72</td><td align="char" char="." valign="bottom">25,80</td><td align="char" char="." valign="bottom">90.11 /</td><td align="char" char="." valign="bottom">1.50</td><td align="char" char="ndash" valign="bottom">1.25–3.5</td></tr><tr><td align="char" char="." valign="bottom">60</td><td align="left" valign="bottom">Lucy in the Sky with Diamonds</td><td align="left" valign="bottom">Ricardo Caliente</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">3.06,40</td><td align="char" char="." valign="bottom">23,00</td><td align="char" char="." valign="bottom">81.11 /</td><td align="char" char="." valign="bottom">1.35</td><td align="char" char="ndash" valign="bottom">1–3.5</td></tr><tr><td align="char" char="." valign="bottom">61</td><td align="left" valign="bottom">Can't Fight the Moonlight</td><td align="left" valign="bottom">Jon Carran</td><td align="left" valign="bottom">Instrumental Pop Hits</td><td align="char" char="." valign="bottom">1.42,26</td><td align="char" char="." valign="bottom">21,82</td><td align="char" char="." valign="bottom">97.95 /</td><td align="char" char="." valign="bottom">1.63</td><td align="char" char="ndash" valign="bottom">1.5–3.75</td></tr><tr><td align="char" char="." valign="bottom">62</td><td align="left" valign="bottom">No Mercy</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">0.50,16</td><td align="char" char="." valign="bottom">26,74</td><td align="char" char="." valign="bottom">76.11 /</td><td align="char" char="." valign="bottom">1.27</td><td align="char" char="ndash" valign="bottom">1–3.5</td></tr><tr><td align="char" char="." valign="bottom">63</td><td align="left" valign="bottom">No Mercy</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">2.33,53</td><td align="char" char="." valign="bottom">26,41</td><td align="char" char="." valign="bottom">76.11 /</td><td align="char" char="." valign="bottom">1.27</td><td align="char" char="ndash" valign="bottom">1–3.5</td></tr><tr><td align="char" char="." valign="bottom">64</td><td align="left" valign="bottom">Rockin' the Blues Away</td><td align="left" valign="bottom">Tiny Grimes Quintet</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.47,65</td><td align="char" char="." valign="bottom">25,50</td><td align="char" char="." valign="bottom">141.05 /</td><td align="char" char="." valign="bottom">2.35</td><td align="char" char="ndash" valign="bottom">1.5–4</td></tr><tr><td align="char" char="." valign="bottom">65</td><td align="left" valign="bottom">Rockin' the Blues Away</td><td align="left" valign="bottom">Tiny Grimes Quintet</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">1.12,78</td><td align="char" char="." valign="bottom">36,07</td><td align="char" char="." valign="bottom">141.05 /</td><td align="char" char="." valign="bottom">2.35</td><td align="char" char="ndash" valign="bottom">1.5–4</td></tr><tr><td align="char" char="." valign="bottom">66</td><td align="left" valign="bottom">The Rocking Guitar</td><td align="left" valign="bottom">Ini Kamoze</td><td align="left" valign="bottom">Rock Story &quot;Instrumental Versions&quot;</td><td align="char" char="." valign="bottom">0.33,58</td><td align="char" char="." valign="bottom">15,08</td><td align="char" char="." valign="bottom">118.66 /</td><td align="char" char="." valign="bottom">1.98</td><td align="char" char="ndash" valign="bottom">1.5–3.25</td></tr><tr><td align="char" char="." valign="bottom">67</td><td align="left" valign="bottom">Country Rodeo Song</td><td align="left" valign="bottom">Marco Rinaldo</td><td align="left" valign="bottom">Country Instrumental Mix</td><td align="char" char="." valign="bottom">2.13,99</td><td align="char" char="." valign="bottom">24,03</td><td align="char" char="." valign="bottom">112.94 /</td><td align="char" char="." valign="bottom">1.88</td><td align="char" char="ndash" valign="bottom">1.5–3.75</td></tr><tr><td align="char" char="." valign="bottom">68</td><td align="left" valign="bottom">Sing Sing Sing</td><td align="left" valign="bottom">Benny Goodman</td><td align="left" valign="bottom">Sing Sing Sing</td><td align="char" char="." valign="bottom">1.08,65</td><td align="char" char="." valign="bottom">18,87</td><td align="char" char="." valign="bottom">108.68 /</td><td align="char" char="." valign="bottom">1.81</td><td align="char" char="ndash" valign="bottom">1.5–3</td></tr><tr><td align="char" char="." valign="bottom">69</td><td align="left" valign="bottom">Sing Sing Sing</td><td align="left" valign="bottom">Benny Goodman</td><td align="left" valign="bottom">Sing Sing Sing</td><td align="char" char="." valign="bottom">2.46,03</td><td align="char" char="." valign="bottom">36,29</td><td align="char" char="." valign="bottom">108.68 /</td><td align="char" char="." valign="bottom">1.81</td><td align="char" char="ndash" valign="bottom">1.5–3</td></tr><tr><td align="char" char="." valign="bottom">70</td><td align="left" valign="bottom">Si Una Vez</td><td align="left" valign="bottom">Pete Astudillo</td><td align="left" valign="bottom">Bachata</td><td align="char" char="." valign="bottom">1.03,70</td><td align="char" char="." valign="bottom">29,27</td><td align="char" char="." valign="bottom">124.54 /</td><td align="char" char="." valign="bottom">2.08</td><td align="char" char="ndash" valign="bottom">1.5–3</td></tr><tr><td align="char" char="." valign="bottom">71</td><td align="left" valign="bottom">Streets on fire</td><td align="left" valign="bottom">Beataddictz</td><td align="left" valign="bottom">Street Beatz, Vol.2</td><td align="char" char="." valign="bottom">2.00,63</td><td align="char" char="." valign="bottom">38,00</td><td align="char" char="." valign="bottom">81.16 /</td><td align="char" char="." valign="bottom">1.35</td><td align="char" char="ndash" valign="bottom">1–3.75</td></tr><tr><td align="char" char="." valign="bottom">72</td><td align="left" valign="bottom">I Can't Wait</td><td align="left" valign="bottom">Alex Cundari</td><td align="left" valign="bottom">Instrumental Hits, Vol.1</td><td align="char" char="." valign="bottom">1.10,71</td><td align="char" char="." valign="bottom">34,48</td><td align="char" char="." valign="bottom">83.59 /</td><td align="char" char="." valign="bottom">1.39</td><td align="char" char="ndash" valign="bottom">1–2.5</td></tr></tbody></table></table-wrap></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75515.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jensen</surname><given-names>Ole</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03angcq70</institution-id><institution>University of Birmingham</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.11.29.470396" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.11.29.470396"/></front-stub><body><p>This study investigated the neural tracking of music using novel methodology. The core finding was stronger neuronal entrainment to &quot;spectral flux&quot; rather than other more commonly tested features such as amplitude envelope. The study is methodologically sophisticated and provides novel insight on the neuronal mechanisms of music perception.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75515.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Jensen</surname><given-names>Ole</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03angcq70</institution-id><institution>University of Birmingham</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Zoefel</surname><given-names>Benedikt</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.11.29.470396">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.11.29.470396v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Neural entrainment is strongest to the spectral flux of slow music and depends on familiarity and beat salience&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Barbara Shinn-Cunningham as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Benedikt Zoefel (Reviewer #1) and Nate Zuk (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>In general the reviewers were positive however more work needs to be done to validate that the results are not a consequence of the analyses or the specific choice of music before tempo manipulation as pointed out by the reviewers. Also, it would be important to better clarify the concepts of neural entertainment, synchronization and neural tracking and their meaning in this specific context.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>I believe the authors could emphasize their most important results more when they appear in the Results section. An example is the beginning of page 11 where one important sentence (&quot;SRCoh was highest…&quot;) is hidden in the text. This result is interesting – highlighting it would also make main results easier to extract.</p><p>RCA: &quot;These approaches have been criticized because of their potential susceptibility to autocorrelation&quot;. As the authors mention a possible involvement of neural oscillations (in their introduction), it could be useful to point out that a removal of autocorrelation (to calculate TRF) might actually remove oscillations as well.</p><p>Due to differences in x and y axes, I was initially confused by Figure 1c, wondering why stimulation tempo (Hz) does not correspond to FFT frequency (Hz). Maybe the authors could include lines to show where stimulation tempo = FFT frequency and the first harmonic? This would make the relevant information much easier to extract.</p><p>I leave it to the authors, but an additional point that might be worth discussing is the fact that humans seem to be most sensitive to amplitude modulations at higher frequencies (around 4 Hz) than those that seem to play an important role in the current study (1-2 Hz). This is for example summarized in a review by Edwards and Chang (2013, Hear Res). Other relevant work is that by Teng, Poeppel and colleagues showing theta activity to most reliably follow acoustic rhythms. Could the authors discuss whether this means that music is special or other reasons for relatively low preferred rates in their work?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The paper was very nice but sometimes hard to read because I am not so confident with the difference between engagement, entertainment, synchronization and tracking. In the literature, those terms are sometimes used interchangeably, and sometimes instead, they are used with a precise meaning. I suggest the authors think about rephrasing parts of the paper and clarifying those terms from the beginning to broaden the range of readers that can enjoy the paper in depth. I found very few typos, and the visualizations were nice. Sometimes I think that the plots are too small and hard to read, and the x/y axes proportion must be chosen carefully.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>p. 4, line 72: &quot;…and we are not aware…tempo-modulated&quot;. I think Kaneshiro et al., 2020 fits this description. They did not use a controlled spacing of tempos like was done in this study, but they used naturalistic polyphonic music with a variety of tempos. Rajendran et al., 2020 also fits this description, although they did extracellular recordings and not EEG.</p><p>p. 6, line 106-107: &quot;…spectral flux is sensitive…changes in amplitude.&quot; Can you provide a citation where rhythmic information is provided by changes in pitch but not in amplitude, perhaps including a behavioral measure of rhythm salience?</p><p>Figure 1C: I think the reason the z-scored amplitude decreases with decreasing stimulation tempo is because z-scoring reduces the magnitude of the components as more non-zero spectral components are introduced in the range of interest. This could be confusing to readers given that EEG may track these tempos better. The authors could alternatively z-score the original signals (before computing the spectra), which might result in more consistent amplitudes across stimulation tempo.</p><p>p. 20, lines 368-370: &quot;In this study…tapped frequency.&quot; I think this should go earlier in the results, around where Figure 1 is presented, in order to clarify the difference between &quot;stimulation tempo&quot; (which is referred to earlier) and &quot;tapped beat rate&quot;.</p><p>p. 25-26, lines 481-510: This discussion aims to highlight the importance of considering other stimulus features besides the envelope (like spectral flux) when comparing the neural tracking of speech and music, partly via criticizing Zuk et al. (2021) for focusing on the envelope. However, the current study does not fully back up some of the author's critiques, mainly because speech stimuli were not included in this study. We don't know how much spectral flux improves EEG prediction of speech over envelope-based measures, although it likely will (see the use of spectrotemporal changes and acoustic edges in Daube et al., 2019). But we can instead compare the prediction accuracies to Di Liberto et al., 2015, Current Biology.</p><p>Envelope-based forward modeling of speech produces the worst average prediction accuracies around 0.05, and the accuracies increase as features such as the spectrogram and phonetic features are added (Figure 2 of that paper). These prediction accuracies are above the prediction accuracies in the current study (0.04 on average with spectral flux, and 0.06 for the slowest tempos, see Figure 3 of this study), supporting the possibility that speech is generally tracked better than music. I agree it is possible that other musical features may predict EEG better and could produce comparable prediction accuracies to speech. But for the discussion here, the authors should instead focus on how similarities and differences in relevant features for speech and music could affect interpretations for studies comparing neural tracking, which would nicely follow the first paragraph of this section. More specifically, the authors should clarify what &quot;characterize stimuli as fairly as possible&quot; (line 509) means and suggest alternative ways of comparing speech and music. If they choose to criticize studies using the envelope alone (which might not be necessary to make their point), I recommend also acknowledging that their study does not directly address questions about speech and music comparisons and that more work needs to be done to understand the differences between speech and music tracking.</p><p>p. 25, lines 497-498: &quot;…we found…envelope.&quot; It is not clear if spectral flux causes stronger entrainment given the data. Rephrase to: &quot;…we found that spectral flux was tracked better than the amplitude envelope.&quot;</p><p>p. 26, lines 520-521: &quot;…and ease of beat tapping…stimulation tempi.&quot; The statistical analysis of the data in Figure 3 showed that ease of beat tapping did not significantly vary with stimulation tempo (lines 327-329), which contradicts this statement.</p><p>p. 28, lines 532-535: &quot;One is that our study…entrainment.&quot; I don't understand why this would matter, because the analysis was done after selecting &quot;musical&quot; and &quot;non-musical&quot; subjects. Perhaps cut this?</p><p>p. 29, line 594: &quot;correlate&quot; should be &quot;correlated&quot;.</p><p>p. 32, line 647-648: &quot;Each segment…0.25 Hz.&quot; The details for the range of tempi are in Supplementary Table 1, but it would be useful here to state the maximum amount of tempo shift relative to the original, both for increasing and decreasing the tempo. Additionally, I mentioned in the public review that the original tempos seem to span the range with the best neural tracking. You should also state the original range of tempos here.</p><p>p. 34, lines 703-704: &quot;This signal…AFz position.&quot; Why did you choose to reference this way? Is there a related citation?</p><p>p. 37, line 773: &quot;To statistically asses&quot; should be &quot;assess&quot;.</p><p>p. 40, section EEG – Reliable Components Analysis, Temporal Response Function: In the public review, I suggested normalizing the measures of correlation, coherence, and prediction accuracy by null distributions. When you do so, make sure the shuffling of trials is done using trials with the same tempi. Otherwise, there will be a mismatch in the spectral content of the music due to differences in tempi, which defeats the point of generating the null distributions.</p><p>p. 40, line 855: &quot;…a system identification technique…&quot; This isn't quite true, change to &quot;…a modeling technique…&quot;.</p><p>p. 41, lines 873-884, also Figure 3C: From my understanding, TRFs were fit to each stimulus feature separately. I am ok with this for comparing prediction accuracies because the models appear to have the same dimensionality (T x 1, where T is the number of lags). However, this is potentially an issue for interpreting TRF weights because the stimulus features in the separate models are correlated, so peaks and troughs found in the TRF for one model might be the effect of one of the other features. One possibility is to look at the TRF weights of the full model instead, but ridge unfortunately optimizes the model by making the weights correlated (an effect that produces smooth TRFs), so I don't think it alleviates the issue either. The best approach would be to iteratively partial out the contribution of each stimulus feature. Start with the envelope model, compute the EEG prediction with that model, and then subtract the prediction from the EEG data. Then fit the derivative model and then the beats model, removing each after they are fit. This way, the TRF weights found for the spectral flux model reflect the temporal response after removing the effects of the other stimulus features.</p><p>p. 42-43, lines 903-933: In the public review, I mentioned that the TRF-SVM modeling needed to be clearer. Specifically:</p><p>– How are the TRFs fit for each group? If you are using a leave-one-trial-out procedure like before, the resulting TRFs for each trial are the models fit to all trials except the testing trial. As a result, they are going to be highly similar to each other, which could explain why the SVM performs so well. Here, I recommend instead fitting the TRF to each trial separately (no cross-validation) using the same ridge parameter that you found for the model trained earlier (in the section EEG – Temporal Response Function). That way the TRFs will represent the stimulus-response mappings for each individual trial.</p><p>– Clarify which training step was referred to for calculating the surrogate data (line 920). I recommend shuffling prior to the TRF modeling step, although if you fit the TRF to each trial individually then the result of shuffling before vs after will be the same.</p><p>– Lines 913-916: There are 6 trials at tapped rate, and 6 trials at 2x tapped rate, which should result in n=12 not n=13.</p><p>Figure 5 – supplement 1: The distributions on the left don't look like they come from the points on the right in each plot. Can you check if there was a mistake with the points plotted or the y-axes?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75515.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>In general the reviewers were positive however more work needs to be done to validate that the results are not a consequence of the analyses or the specific choice of music before tempo manipulation as pointed out by the reviewers. Also, it would be important to better clarify the concepts of neural entertainment, synchronization and neural tracking and their meaning in this specific context.</p><p>Reviewer #1 (Recommendations for the authors):</p><p>I believe the authors could emphasize their most important results more when they appear in the Results section. An example is the beginning of page 11 where one important sentence (&quot;SRCoh was highest…&quot;) is hidden in the text. This result is interesting – highlighting it would also make main results easier to extract.</p></disp-quote><p>We tried to highlight our most important results by dividing the results into smaller paragraphs. This way, we hope that the main results will be easier to read.</p><disp-quote content-type="editor-comment"><p>RCA: &quot;These approaches have been criticized because of their potential susceptibility to autocorrelation&quot;. As the authors mention a possible involvement of neural oscillations (in their introduction), it could be useful to point out that a removal of autocorrelation (to calculate TRF) might actually remove oscillations as well.</p></disp-quote><p>This is true. We added this concern to the discussion of the manuscript.</p><p>p. 28 l. 556-560: “However, the RCA-based approaches (Kaneshiro et al., 2020) have been criticized because of their potential susceptibility to autocorrelation, which is argued to be minimized in the TRF approach (Zuk et al., 2021), which uses ridge regression to dampen fast oscillatory components (Crosse et al., 2021). However, by minimizing the effects of auto-correlation one concern could be that this could remove neural oscillations as well.”</p><disp-quote content-type="editor-comment"><p>Due to differences in x and y axes, I was initially confused by Figure 1c, wondering why stimulation tempo (Hz) does not correspond to FFT frequency (Hz). Maybe the authors could include lines to show where stimulation tempo = FFT frequency and the first harmonic? This would make the relevant information much easier to extract.</p></disp-quote><p>We added lines to Figure 1C to highlight when the first harmonic or the stimulation tempo equal the FFT Frequency.</p><disp-quote content-type="editor-comment"><p>I leave it to the authors, but an additional point that might be worth discussing is the fact that humans seem to be most sensitive to amplitude modulations at higher frequencies (around 4 Hz) than those that seem to play an important role in the current study (1-2 Hz). This is for example summarized in a review by Edwards and Chang (2013, Hear Res). Other relevant work is that by Teng, Poeppel and colleagues showing theta activity to most reliably follow acoustic rhythms. Could the authors discuss whether this means that music is special or other reasons for relatively low preferred rates in their work?</p></disp-quote><p>Thanks for this. It is true that Edwards and Chang, 2013 (among others) identify highest sensitivity to amplitude modulation around 4 Hz. This is faster than the rates at which we saw strongest neural synchronization to the amplitude envelope of music. It is possible then, that the rates at which we examined neural synchronization were “suboptimal” with respect to the system’s sensitivity to amplitude modulation, which may have handicapped amplitude envelope as a feature to describe music. However, Edwards and Chang also identify highest sensitivity to frequency modulation around the same rate, that is, 2–5 Hz. Again, this is faster than the rates at which we saw strongest neural synchronization to spectral flux. So we would argue that the difference between our amplitude and spectral features was not due to differences in the pre-existing sensitivities of the auditory system to these types of modulations. We actually have some data from another study (unpublished) showing that behavioral preferences for different rates are category-specific, meaning that you can tell the difference between amplitude- / frequency-modulated sounds, speech, and music based on the rates that listeners prefer to hear those sounds presented at. Although these are not neural data, they suggest to us that sensitivity to the modulations in technical sounds (AM, FM) might not be sufficient to predict sensitivity to fluctuations in categories of natural sounds, and music in particular. However, we would not necessarily propose that music is special in this way. Although we find this to be an extremely interesting topic, any discussion we would add would be fairly wild speculation – therefore, we hope to have your support by not adding this interesting point to our manuscript and lengthening the discussion further.</p><p>Nonetheless, we do include a discussion about why these low rates are important in natural music, which we reproduce here for your convenience.</p><p>p. 23, l. 409-426: “Strongest neural synchronization was found in response to stimulation tempi between 1 and 2 Hz in terms of SRCorr (Figure 2B), TRF correlations (Figure 3A), and TRF weights (Figure 3C-F). Moreover, we observed a behavioral preference to tap to the beat in this frequency range, as the group preference for music tapping was at 1.55 Hz (Figure 5 —figure supplement 3). Previous studies have shown a preference to listen to music with beat rates around 2 Hz (Bauer et al., 2015), which is moreover the modal beat rate in Western pop music (Moelants, 2002) and the rate at which the modulation spectrum of natural music peaks (Ding et al., 2017). Even in nonmusical contexts, spontaneous adult human locomotion is characterized by strong energy around 2 Hz (MacDougall and Moore, 2005). Moreover, when asked to rhythmically move their bodies at a comfortable rate, adults will spontaneously move at rates around 2 Hz (McAuley et al., 2006) regardless whether they use their hands or feet (Rose et al., 2020). Thus, there is a tight link between preferred rates of human body movement and preferred rates for the music we make and listen to that was moreover reflected in our neural data. This is perhaps not surprising, as musical rhythm perception activates motor areas of the brain, such as the basal ganglia and supplementary motor area (Grahn and Brett, 2007), and is further associated with increased auditory–motor functional connectivity (Chen et al., 2008). In turn, involving the motor system in rhythm perception tasks improves temporal acuity (Morillon et al., 2014), but only for beat rates in the 1–2 Hz range (Zalta et al., 2020).”</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>The paper was very nice but sometimes hard to read because I am not so confident with the difference between engagement, entertainment, synchronization and tracking. In the literature, those terms are sometimes used interchangeably, and sometimes instead, they are used with a precise meaning. I suggest the authors think about rephrasing parts of the paper and clarifying those terms from the beginning to broaden the range of readers that can enjoy the paper in depth. I found very few typos, and the visualizations were nice. Sometimes I think that the plots are too small and hard to read, and the x/y axes proportion must be chosen carefully.</p></disp-quote><p>Thank you for your helpful comments and your positive review. We rephrased parts of the paper and moved explanatory sections from the discussion to the introduction. We decided to use the term “neural synchronization” throughout, but do discuss how our study relates to the concept of neural entrainment, and how the analysis frameworks we tested relate to different theoretical backgrounds. We also rearranged some Figures (especially the Figures 2 and 3) that appeared to be quite small. We hope that this way the Figures are better readable and easier to understand.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>p. 4, line 72: &quot;…and we are not aware…tempo-modulated&quot;. I think Kaneshiro et al., 2020 fits this description. They did not use a controlled spacing of tempos like was done in this study, but they used naturalistic polyphonic music with a variety of tempos. Rajendran et al., 2020 also fits this description, although they did extracellular recordings and not EEG.</p></disp-quote><p>Kaneshiro et al., 2020 used four natural (unmanipulated) songs, which had tempi of 156 BPM, 94 BPM, 90 BPM and 86 BPM. Although it is true that this covers a range of tempi, the experimental manipulation was not to parametrically vary tempo as we did here. In contrast, Rajendran et al., 2020 used a larger musical stimulus set with tempi ranging from 0.7 to 3.7 Hz and investigated the neural response to those musical stimuli in rats.</p><p>Despite these differences, we can see why those studies could fit this description in our Introduction, and we have changed the sentence.</p><p>p. 3 l. 54-57: “Despite the perceptual and motor evidence, studies looking at tempo-dependence of neural synchronization are scarce (Doelling and Poeppel, 2015, Nicolaou et al., 2017) and we are not aware of any human EEG study using naturalistic polyphonic musical stimuli that were manipulated in the tempo domain.“</p><disp-quote content-type="editor-comment"><p>p. 6, line 106-107: &quot;…spectral flux is sensitive…changes in amplitude.&quot; Can you provide a citation where rhythmic information is provided by changes in pitch but not in amplitude, perhaps including a behavioral measure of rhythm salience?</p></disp-quote><p>A number of past studies (such as Jones, 1987; Jones and Pfordresher, 1997; Ellis and Jones, 2009) describe the “joint accent structure” of music, where the position of perceived accents is determined by changes in pitch (deviations) in relation to the pre-existing melody (melodic accents) in addition to timing changes in relation of the temporal flow of an auditory stimulus (temporal accents, Jones and Pfordresher, 1997).</p><p>For a more concrete example, you could imagine classical music played in a glissando style by a violin or cello. There may never be a clear sound on-/offset, and the perceived rhythm can be based entirely on spectral changes of the music piece.</p><p>Finally, related to the behavioral consequences, a study by Burger et al., 2013, demonstrated a positive correlation between the spectral flux at lower frequencies (50-100 Hz) in music with perceived beat strength and urge to move.</p><p>We briefly mentioned parts of this in the discussion, but partially moved it up to the introduction and added citations:</p><p>p. 5 l. 95-100: “One potential advantage of spectral flux over the envelope or its derivative is that spectral flux is sensitive to rhythmic information that is communicated by changes in pitch even when they are not accompanied by changes in amplitude. Critically, temporal and spectral information jointly influence the perceived accent structure in music, which provides information about beat locations (Pfordresher, 2003, Ellis and Jones, 2009, Jones, 1993).”</p><p>p. 25 l. 476-477: “Previous work on joint accent structure indicates that spectral information is an important contributor to beat perception (Ellis and Jones, 2009, Pfordresher, 2003).”</p><disp-quote content-type="editor-comment"><p>Figure 1C: I think the reason the z-scored amplitude decreases with decreasing stimulation tempo is because z-scoring reduces the magnitude of the components as more non-zero spectral components are introduced in the range of interest. This could be confusing to readers given that EEG may track these tempos better. The authors could alternatively z-score the original signals (before computing the spectra), which might result in more consistent amplitudes across stimulation tempo.</p></disp-quote><p>We z-scored the original signal before computing the FFT. For plotting, we z-scored the FFT again. However, we compared the z-scored FFTs to the FFTs that were computed without subsequent z-scoring and similar trends were found (please see plots in <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>). Therefore, we decided to keep the z-scored FFT (as shown in the original version of the manuscript).</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-sa2-fig1-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>p. 20, lines 368-370: &quot;In this study…tapped frequency.&quot; I think this should go earlier in the results, around where Figure 1 is presented, in order to clarify the difference between &quot;stimulation tempo&quot; (which is referred to earlier) and &quot;tapped beat rate&quot;.</p></disp-quote><p>Thank you. As we removed most of the tapping-related analysis from the manuscript, we also deleted this sentence.</p><disp-quote content-type="editor-comment"><p>p. 25-26, lines 481-510: This discussion aims to highlight the importance of considering other stimulus features besides the envelope (like spectral flux) when comparing the neural tracking of speech and music, partly via criticizing Zuk et al. (2021) for focusing on the envelope. However, the current study does not fully back up some of the author's critiques, mainly because speech stimuli were not included in this study. We don't know how much spectral flux improves EEG prediction of speech over envelope-based measures, although it likely will (see the use of spectrotemporal changes and acoustic edges in Daube et al., 2019). But we can instead compare the prediction accuracies to Di Liberto et al., 2015, Current Biology.</p><p>Envelope-based forward modeling of speech produces the worst average prediction accuracies around 0.05, and the accuracies increase as features such as the spectrogram and phonetic features are added (Figure 2 of that paper). These prediction accuracies are above the prediction accuracies in the current study (0.04 on average with spectral flux, and 0.06 for the slowest tempos, see Figure 3 of this study), supporting the possibility that speech is generally tracked better than music. I agree it is possible that other musical features may predict EEG better and could produce comparable prediction accuracies to speech. But for the discussion here, the authors should instead focus on how similarities and differences in relevant features for speech and music could affect interpretations for studies comparing neural tracking, which would nicely follow the first paragraph of this section. More specifically, the authors should clarify what &quot;characterize stimuli as fairly as possible&quot; (line 509) means and suggest alternative ways of comparing speech and music. If they choose to criticize studies using the envelope alone (which might not be necessary to make their point), I recommend also acknowledging that their study does not directly address questions about speech and music comparisons and that more work needs to be done to understand the differences between speech and music tracking.</p></disp-quote><p>Thank you for this important comment and sorry for the potential misunderstanding that originated from this part of the discussion. In this section of the discussion, we did not mean to write negatively about studies that solely focus on the stimulus amplitude envelope. And we certainly did not mean to imply that our paper has anything definitive to say about direct comparisons of neural synchronization to music and speech. Every paper follows a different agenda and focuses on different aspects of the research in the field. In the current study, we wanted to test the effects of different acoustic features on neural synchronization (to music only). The point we were trying to make is not that the amplitude envelope is a “bad” acoustic feature, but rather that one could also consider using different acoustic features (going beyond the often-used amplitude envelope) to arrive at a more nuanced understanding of neural synchronization to music. As you say, this approach has been taken in speech work in the past and has improved forward-model predictions of neural data. Here, we are attemping a similar approach in the musical domain, where our choice of musical features was grounded in some intuition or understanding of the kinds of acoustic fluctuations that might give rise to a sense of temporal regularity or pulse.</p><p>Our aim with this discussion was not to create a conflict or a debate in the literature, but rather just to provide a little food for thought for future work. For that reason, we have substantially cut down this part of the discussion, and no longer speak of “characterizing stimuli as fairly as possible”. We reproduce this revised bit of the discussion here for your convenience:</p><p>p. 26-27 l. 498-511: “For example, a recent study found that neuronal activity synchronizes less strongly to music than to speech (Zuk et al., 2021); notably this paper focused on the amplitude envelope to characterize the rhythms of both stimulus types. However, our results show that neural synchronization is especially strong to the spectral content of music, and that spectral flux may be a better measure for capturing musical dynamics than the amplitude envelope (Müller, 2015). Imagine listening to a melody played in a glissando fashion on a violin. There might never be a clear onset that would be represented by the amplitude envelope – all of the rhythmic structure is communicated by spectral changes. Indeed, many automated tools for extracting the beat in music used in the musical information retrieval (MIR) literature rely on spectral flux information (Oliveira et al., 2010). Also, in the context of body movement, spectral flux has been associated with the type and temporal acuity of synchronization between the body and music at the beat rate (Burger et al., 2018) to a greater extent than other acoustic characterizations of musical rhythmic structure. As such, we found that spectral flux synchronized brain activity better than the amplitude envelope.”</p><disp-quote content-type="editor-comment"><p>p. 25, lines 497-498: &quot;…we found…envelope.&quot; It is not clear if spectral flux causes stronger entrainment given the data. Rephrase to: &quot;…we found that spectral flux was tracked better than the amplitude envelope.&quot;</p></disp-quote><p>Thank you. We changed the word “entrainment” to “synchronization” throughout the manuscript. For more consistency, we therefore rephrased the sentence to:</p><p>p. 27 l. 510-511: “As such, we found that spectral flux synchronized brain activity better than the amplitude envelope.”</p><disp-quote content-type="editor-comment"><p>p. 26, lines 520-521: &quot;…and ease of beat tapping…stimulation tempi.&quot; The statistical analysis of the data in Figure 3 showed that ease of beat tapping did not significantly vary with stimulation tempo (lines 327-329), which contradicts this statement.</p></disp-quote><p>Yes, it is correct that we did not see any significant differences of the behavioral ratings across tempo conditions. We removed the sentence from the manuscript.</p><disp-quote content-type="editor-comment"><p>p. 28, lines 532-535: &quot;One is that our study…entrainment.&quot; I don't understand why this would matter, because the analysis was done after selecting &quot;musical&quot; and &quot;non-musical&quot; subjects. Perhaps cut this?</p></disp-quote><p>We removed the “musicians” vs. “non-musicians” part from the manuscript due to the suggestions of Reviewer 2. Therefore, we cut this sentence.</p><disp-quote content-type="editor-comment"><p>p. 29, line 594: &quot;correlate&quot; should be &quot;correlated&quot;.</p></disp-quote><p>Thank you. Done.</p><disp-quote content-type="editor-comment"><p>p. 32, line 647-648: &quot;Each segment…0.25 Hz.&quot; The details for the range of tempi are in Supplementary Table 1, but it would be useful here to state the maximum amount of tempo shift relative to the original, both for increasing and decreasing the tempo. Additionally, I mentioned in the public review that the original tempos seem to span the range with the best neural tracking. You should also state the original range of tempos here.</p></disp-quote><p>We added the original music tempo range and maximum amount of tempo change as histograms to Figure 1 —figure supplement 2. We also added statements of the original music tempo in the Results section (p. 13 l. 265-273) and mention that it coincides with the tempo range of highest neural synchronization in the discussion (p. 23-24 l. 427-436).</p><disp-quote content-type="editor-comment"><p>p. 34, lines 703-704: &quot;This signal…AFz position.&quot; Why did you choose to reference this way? Is there a related citation?</p></disp-quote><p>We chose to reference this way because the FCz and AFz are the standard positions for grounding and referencing the electrodes when using the actiCAP 64Ch Standard-2 system and layout from Brain Products. As these central positions are not ideal when conducting auditory experiments, we re-referenced our data to the average reference. This procedure has been used previously such as in Falk et al., 2017 and Cabral-Calderin and Henry, 2022.</p><disp-quote content-type="editor-comment"><p>p. 37, line 773: &quot;To statistically asses&quot; should be &quot;assess&quot;.</p></disp-quote><p>Done. (Oops.)</p><disp-quote content-type="editor-comment"><p>p. 40, section EEG – Reliable Components Analysis, Temporal Response Function: In the public review, I suggested normalizing the measures of correlation, coherence, and prediction accuracy by null distributions. When you do so, make sure the shuffling of trials is done using trials with the same tempi. Otherwise, there will be a mismatch in the spectral content of the music due to differences in tempi, which defeats the point of generating the null distributions.</p></disp-quote><p>When calculating the z-score of the data, we took care that we calculated the surrogate distribution per tempo and participant. Details of the implementation can be found in the Materials and methods section:</p><p>p. 39 l. 823-831: “In order to control for any frequency-specific differences in the overall power of the neural data that could have led to artificially inflated observed neural synchronization at lower frequencies, the SRCorr and SRCoh values were z-scored based on a surrogate distribution (Zuk et al., 2021). Each surrogate distribution was generated by shifting the neural time course by a random amount relative to the musical feature time courses, keeping the time courses of the neural data and musical features intact. For each of 50 iterations, a surrogate distribution was created for each stimulation subgroup and tempo condition. The z-scoring was calculated by subtracting the mean and dividing by the standard deviation of the surrogate distribution.”</p><disp-quote content-type="editor-comment"><p>p. 40, line 855: &quot;…a system identification technique…&quot; This isn't quite true, change to &quot;…a modeling technique…&quot;.</p></disp-quote><p>Thank you, we changed it.</p><disp-quote content-type="editor-comment"><p>p. 41, lines 873-884, also Figure 3C: From my understanding, TRFs were fit to each stimulus feature separately. I am ok with this for comparing prediction accuracies because the models appear to have the same dimensionality (T x 1, where T is the number of lags). However, this is potentially an issue for interpreting TRF weights because the stimulus features in the separate models are correlated, so peaks and troughs found in the TRF for one model might be the effect of one of the other features. One possibility is to look at the TRF weights of the full model instead, but ridge unfortunately optimizes the model by making the weights correlated (an effect that produces smooth TRFs), so I don't think it alleviates the issue either. The best approach would be to iteratively partial out the contribution of each stimulus feature. Start with the envelope model, compute the EEG prediction with that model, and then subtract the prediction from the EEG data. Then fit the derivative model and then the beats model, removing each after they are fit. This way, the TRF weights found for the spectral flux model reflect the temporal response after removing the effects of the other stimulus features.</p></disp-quote><p>Our mutual-information analysis showed that the musical features are indeed correlated, and in particular that spectral flux significantly shares mutual information with all other music features (Figure 1). Thus, the TRF weights for each feature will be necessarily nonindependent (though it’s not clear to us that they might be “an effect of one of the other features” alone). Therefore, we took you up on your analysis suggestion. However, we decided to not do a step-wise regression analysis, as this would lead to multiple orthogonalizations and there is no obvious reason in which order the TRFs should be computed. Instead, we calculated a multivariate TRF based on the amplitude envelope, first derivative of the envelope, and beat onsets (everything but spectral flux). Then, as you suggested, we subtracted the resulting predictions from the EEG data. The residual data were used to compute the TRF weights in response to spectral flux. The consequent TRF weights look qualitatively similar to the originals (compare to Figure 3F) and we added them to the analysis (Figure 3 —figure supplement 2):</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-sa2-fig2-v1.tif"/></fig><p>We also plotted the TRF amplitude in the previously computed significant time lag window (102-211ms):</p><fig id="sa2fig3" position="float"><label>Author response image 3.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75515-sa2-fig3-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>p. 42-43, lines 903-933: In the public review, I mentioned that the TRF-SVM modeling needed to be clearer. Specifically:</p><p>– How are the TRFs fit for each group? If you are using a leave-one-trial-out procedure like before, the resulting TRFs for each trial are the models fit to all trials except the testing trial. As a result, they are going to be highly similar to each other, which could explain why the SVM performs so well. Here, I recommend instead fitting the TRF to each trial separately (no cross-validation) using the same ridge parameter that you found for the model trained earlier (in the section EEG – Temporal Response Function). That way the TRFs will represent the stimulus-response mappings for each individual trial.</p><p>– Clarify which training step was referred to for calculating the surrogate data (line 920). I recommend shuffling prior to the TRF modeling step, although if you fit the TRF to each trial individually then the result of shuffling before vs after will be the same.</p></disp-quote><p>Thank you for making this important comment. We used a leave-one-trial-out procedure as before, and therefore agree that this way of analyzing the data is not the most suitable. In response to your comments we, as you suggest, either calculate the TRFs based on individual trials or calculate the TRF surrogate dataset by shuffling the labels prior to the TRF analysis (instead of implementing it at the training step of the SVM classifier). However, in neither of those cases the SVM accuracies of the actual data were significantly better than the SVM accuracies to a surrogate dataset. Therefore, we removed this part from the manuscript.</p><disp-quote content-type="editor-comment"><p>– Lines 913-916: There are 6 trials at tapped rate, and 6 trials at 2x tapped rate, which should result in n=12 not n=13.</p></disp-quote><p>By “n” we did not mean the number of trials, but rather how many participants and tempo conditions fulfilled this requirement. However, as we removed all SVM-related things from the manuscript, we also removed this part from the manuscript.</p><disp-quote content-type="editor-comment"><p>Figure 5 – supplement 1: The distributions on the left don't look like they come from the points on the right in each plot. Can you check if there was a mistake with the points plotted or the y-axes?</p></disp-quote><p>Thank you for this important comment. The left plot depicts the mean TRF correlations per participant whereas the right plot displayed the maximum TRF correlations per participant. Based on the comments of Reviewer 2, we removed the left plots (contrasting musicians vs. non-musicians) in each panel and only plotted the general sophistication index (from the Gold-MSI) against the TRF correlations. To make it more consistent with the Figures of the main manuscript we changed the maximum TRF correlations into the mean TRF correlations per participant (this way the right plot corresponds to the previous “musician vs. non-musician plot”, Figure 5 —figure supplement 2).</p><p>References</p><p>Burger, B., Ahokas, R., Keipi, A. and Toiviainen, P. (Year) Relationships between spectral flux, perceived rhythmic strength, and the propensity to move. City.</p><p>Cabral-Calderin, Y. and Henry, M.J. (2022) Reliability of Neural Entrainment in the Human Auditory System. J Neurosci, <bold>42</bold>, 894-908.</p><p>Crosse, M.J., Di Liberto, G.M., Bednar, A. and Lalor, E.C. (2016) The Multivariate Temporal Response Function (mTRF) Toolbox: A MATLAB Toolbox for Relating Neural Signals to Continuous Stimuli. Frontiers in Human Neuroscience, <bold>10</bold>.</p><p>Crosse, M.J., Zuk, N.J., Di Liberto, G.M., Nidiffer, A.R., Molholm, S. and Lalor, E.C. (2021) Linear Modeling of Neurophysiological Responses to Speech and Other Continuous Stimuli: Methodological Considerations for Applied Research. Frontiers in Neuroscience, <bold>15</bold>.</p><p>Di Liberto, G.M., Pelofi, C., Bianco, R., Patel, P., Mehta, A.D., Herrero, J.L., de Cheveigné, A., Shamma, S. and Mesgarani, N. (2020) Cortical encoding of melodic expectations in human temporal cortex. <italic>eLife</italic>, <bold>9</bold>, e51784.</p><p>Di Liberto, Giovanni M., O’Sullivan, James A. and Lalor, Edmund C. (2015) Low-Frequency Cortical Entrainment to Speech Reflects Phoneme-Level Processing. Current Biology, <bold>25</bold>, 2457-2465.</p><p>Ding, N., Chatterjee, M. and Simon, J.Z. (2014) Robust cortical entrainment to the speech envelope relies on the spectro-temporal fine structure. NeuroImage, <bold>88</bold>, 41-46.</p><p>Edwards, E. and Chang, E.F. (2013) Syllabic (∼2–5 Hz) and fluctuation (∼1–10 Hz) ranges in speech and auditory processing. Hearing Research, <bold>305</bold>, 113-134.</p><p>Ellis, R.J. and Jones, M.R. (2009) The role of accent salience and joint accent structure in meter perception. Journal of experimental psychology. Human perception and performance, <bold>35 1</bold>, 264-280.</p><p>Falk, S., Lanzilotti, C. and Schön, D. (2017) Tuning Neural Phase Entrainment to Speech. Journal of Cognitive Neuroscience, <bold>29</bold>, 1378-1389.</p><p>Jones, M.R. (1987) Dynamic pattern structure in music: Recent theory and research. Perception and Psychophysics, <bold>41</bold>, 621-634.</p><p>Jones, M.R. and Pfordresher, P.Q. (1997) Tracking musical patterns using joint accent structure. Canadian Psychological Association, Canada, pp. 271-291.</p><p>Kaneshiro, B., Nguyen, D.T., Norcia, A.M., Dmochowski, J.P. and Berger, J. (2020) Natural music evokes correlated EEG responses reflecting temporal structure and beat. NeuroImage, <bold>214</bold>, 116559.</p><p>Madsen, J., Margulis, E.H., Simchy-Gross, R. and Parra, L.C. (2019) Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity and training. Scientific Reports, <bold>9</bold>, 3576.</p><p>Rajendran, V., Harper, N. and Schnupp, J. (2020) Auditory cortical representation of music favours the perceived beat. Royal Society Open Science, <bold>7</bold>, 191194.</p><p>Teng, X., Meng, Q. and Poeppel, D. (2021) Modulation Spectra Capture EEG Responses to Speech Signals and Drive Distinct Temporal Response Functions. eneuro, <bold>8</bold>, ENEURO.0399-0320.2020.</p><p>Vanden Bosch der Nederlanden, C.M., Joanisse, M.F., Grahn, J.A., Snijders, T.M. and Schoffelen, J.-M. (2022) Familiarity modulates neural tracking of sung and spoken utterances. NeuroImage, <bold>252</bold>, 119049.</p><p>Zuk, N.J., Murphy, J.W., Reilly, R.B. and Lalor, E.C. (2021) Envelope reconstruction of speech and music highlights stronger tracking of speech at low frequencies. PLOS Computational Biology, <bold>17</bold>, e1009358.</p></body></sub-article></article>