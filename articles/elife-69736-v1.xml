<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">69736</article-id><article-id pub-id-type="doi">10.7554/eLife.69736</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Causal neural mechanisms of context-based object recognition</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-236488"><name><surname>Wischnewski</surname><given-names>Miles</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-76860"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4026-7303</contrib-id><email>m.peelen@donders.ru.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution>Donders Institute for Brain, Cognition and Behaviour, Radboud University</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff2"><label>2</label><institution>Department of Biomedical Engineering, University of Minnesota</institution><addr-line><named-content content-type="city">Minneapolis</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>O'Connell</surname><given-names>Redmond G</given-names></name><role>Reviewing Editor</role><aff><institution>Trinity College Dublin</institution><country>Ireland</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>10</day><month>08</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e69736</elocation-id><history><date date-type="received" iso-8601-date="2021-04-24"><day>24</day><month>04</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-07-26"><day>26</day><month>07</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-04-26"><day>26</day><month>04</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.04.23.441170"/></event></pub-history><permissions><copyright-statement>© 2021, Wischnewski and Peelen</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Wischnewski and Peelen</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-69736-v1.pdf"/><abstract><p>Objects can be recognized based on their intrinsic features, including shape, color, and texture. In daily life, however, such features are often not clearly visible, for example when objects appear in the periphery, in clutter, or at a distance. Interestingly, object recognition can still be highly accurate under these conditions when objects are seen within their typical scene context. What are the neural mechanisms of context-based object recognition? According to parallel processing accounts, context-based object recognition is supported by the parallel processing of object and scene information in separate pathways. Output of these pathways is then combined in downstream regions, leading to contextual benefits in object recognition. Alternatively, according to feedback accounts, context-based object recognition is supported by (direct or indirect) feedback from scene-selective to object-selective regions. Here, in three pre-registered transcranial magnetic stimulation (TMS) experiments, we tested a key prediction of the feedback hypothesis: that scene-selective cortex causally and selectively supports context-based object recognition before object-selective cortex does. Early visual cortex (EVC), object-selective lateral occipital cortex (LOC), and scene-selective occipital place area (OPA) were stimulated at three time points relative to stimulus onset while participants categorized degraded objects in scenes and intact objects in isolation, in different trials. Results confirmed our predictions: relative to isolated object recognition, context-based object recognition was selectively and causally supported by OPA at 160–200 ms after onset, followed by LOC at 260–300 ms after onset. These results indicate that context-based expectations facilitate object recognition by disambiguating object representations in the visual cortex.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>object</kwd><kwd>scene</kwd><kwd>expectation</kwd><kwd>perception</kwd><kwd>categorization</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>725970</award-id><principal-award-recipient><name><surname>Peelen</surname><given-names>Marius V</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Context-based object recognition causally relies on both scene- and object-selective cortex, with scene-selective cortex generating expectations (at 160-200 ms after onset) that disambiguate object representations in object-selective cortex (at 260-300 ms after onset).</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Objects are typically seen within a rich, structured, and familiar context, such as cars on a road and chairs in a living room. Decades of behavioral work have shown that context facilitates the recognition of objects (<xref ref-type="bibr" rid="bib1">Bar, 2004</xref>; <xref ref-type="bibr" rid="bib2">Biederman et al., 1982</xref>; <xref ref-type="bibr" rid="bib29">Oliva and Torralba, 2007</xref>). This contextual facilitation is crucial for everyday behavior, allowing us to recognize objects under poor viewing conditions (<xref ref-type="fig" rid="fig1">Figure 1</xref>), at a distance, in clutter, and in the periphery where visual resolution is low. Yet despite the pervasive influence of context on object recognition, our knowledge of the neural mechanisms of object recognition almost exclusively comes from studies in which participants view clearly visible isolated objects without context. These studies have shown that isolated object recognition results from the transformation of local, low-level features into view-invariant object representations along the ventral stream (<xref ref-type="bibr" rid="bib8">DiCarlo et al., 2012</xref>; <xref ref-type="bibr" rid="bib24">Liu et al., 2009</xref>; <xref ref-type="bibr" rid="bib38">Riesenhuber and Poggio, 1999</xref>; <xref ref-type="bibr" rid="bib40">Serre et al., 2007</xref>). Does a similar local-to-global hierarchy support context-based object recognition?</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Example of context-based object recognition.</title><p>At night (top panels), the truck is easily recognized by participants when placed in context (left) but not when taken out of context (right). With sufficient light (bottom panels), the truck is easily recognized also when presented in isolation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69736-fig1-v1.tif"/></fig><p>One possibility is that context-based object recognition is supported by the parallel feedforward processing of local object information in the ventral stream object pathway and global scene processing in a separate scene pathway (<xref ref-type="bibr" rid="bib16">Henderson and Hollingworth, 1999</xref>; <xref ref-type="bibr" rid="bib31">Park et al., 2011</xref>). Output of these pathways may then be combined in downstream decision-making regions, leading to contextual benefits in object recognition. Alternatively, context-based object recognition may be supported by feedback processing, with scene context providing a prior that is integrated with ambiguous object representations in the visual cortex (<xref ref-type="bibr" rid="bib1">Bar, 2004</xref>; <xref ref-type="bibr" rid="bib3">Brandman and Peelen, 2017</xref>; <xref ref-type="bibr" rid="bib7">de Lange et al., 2018</xref>). Neuroimaging studies have not been able to distinguish between these possibilities because the contextual modulation of neural activity in object-selective cortex (<xref ref-type="bibr" rid="bib3">Brandman and Peelen, 2017</xref>; <xref ref-type="bibr" rid="bib11">Faivre et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Gronau et al., 2008</xref>; <xref ref-type="bibr" rid="bib37">Rémy et al., 2014</xref>) could precede but also follow object recognition, for example reflecting post-recognition imagery (<xref ref-type="bibr" rid="bib9">Dijkstra et al., 2018</xref>; <xref ref-type="bibr" rid="bib36">Reddy et al., 2010</xref>).</p><p>To distinguish between these accounts, we used transcranial magnetic stimulation (TMS) to interfere with processing in right object-selective lateral occipital cortex (LOC; <xref ref-type="bibr" rid="bib13">Grill-Spector, 2003</xref>; <xref ref-type="bibr" rid="bib25">Malach et al., 1995</xref>) and right scene-selective occipital place area (OPA; <xref ref-type="bibr" rid="bib10">Dilks et al., 2013</xref>; <xref ref-type="bibr" rid="bib13">Grill-Spector, 2003</xref>) at three time points relative to stimulus onset. We additionally stimulated the early visual cortex (EVC) to investigate the causal contribution of feedback processing in this region during both isolated object recognition and context-based object recognition (<xref ref-type="bibr" rid="bib4">Camprodon et al., 2010</xref>; <xref ref-type="bibr" rid="bib20">Koivisto et al., 2011</xref>; <xref ref-type="bibr" rid="bib32">Pascual-Leone and Walsh, 2001</xref>; <xref ref-type="bibr" rid="bib45">Wokke et al., 2013</xref>). EVC stimulation was targeted around 2 cm above the inion, with the coil positioned such that TMS induced static phosphenes centrally in the visual field, where the stimuli were presented. This region corresponds primarily to V1 (<xref ref-type="bibr" rid="bib19">Koivisto et al., 2010</xref>; <xref ref-type="bibr" rid="bib32">Pascual-Leone and Walsh, 2001</xref>). The three regions were stimulated in separate pre-registered experiments (N=24 in each experiment; see Materials and methods).</p><p>Because TMS effects are variable across individuals, for example, due to individual differences in functional coordinates but also skull thickness and subject-specific gyral folding patterns (<xref ref-type="bibr" rid="bib30">Opitz et al., 2013</xref>), we used a TMS-based assignment procedure to ensure the effectiveness of TMS over each of the three stimulated regions at the individual participant level (<xref ref-type="bibr" rid="bib42">van Koningsbruggen et al., 2013</xref>). To achieve this, all 72 participants in the current study first underwent a separate TMS session in which the effectiveness of TMS over the three regions was established using object and scene recognition tasks (for the full procedure and results of this screening experiment, see <xref ref-type="bibr" rid="bib44">Wischnewski and Peelen, 2021</xref>). Only participants who showed reduced scene recognition performance after OPA stimulation were assigned to the OPA experiment (N=24), only participants who showed reduced object recognition performance after LOC stimulation were assigned to the LOC experiment (N=24), and only participants who experienced TMS-induced phosphenes after EVC stimulation were assigned to the EVC experiment (N=24). All 72 participants satisfied at least one of these criteria such that no participants had to be excluded.</p><p>In all experiments, participants performed an unspeeded eight-alternative forced-choice object recognition task, indicating whether a briefly presented stimulus belonged to one of the eight categories (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Participants performed this task for clearly visible isolated objects (<italic>isolated object recognition</italic>) as well as for degraded objects presented within a congruent scene context (<italic>context-based object recognition</italic>). In addition to the object recognition tasks, participants also performed a scene-alone task in which the object was cropped out and replaced with background. In this condition, participants had to guess the object category of the cropped-out object.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Overview of task and stimulation methods.</title><p>(<bold>a</bold>) Schematic overview of a trial. Two TMS pulses (40 ms apart) were delivered on each trial at one of three time windows relative to stimulus onset (60–100 ms, 160–200 ms, and 260–300 ms). The three TMS timings occurred in random order within each block. (<bold>b</bold>) Examples of each of the eight categories shown in the experiment, in the isolated object condition (left) and the context-based object condition (right). Note that the local degradation of the objects in the context-based object condition is not clearly visible from these small example images. This degradation strongly reduces object recognition when the degraded objects are presented out of scene context (see <xref ref-type="bibr" rid="bib3">Brandman and Peelen, 2017</xref>). These conditions were presented in random order and participants performed the same categorization task on all stimuli. (<bold>c</bold>) Overview of the three TMS sites and the three time windows of stimulation. Shaded background colors indicate presumed time windows of inhibition for double-pulse TMS. TMS, transcranial magnetic stimulation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69736-fig2-v1.tif"/></fig><p>Predictions (<xref ref-type="fig" rid="fig3">Figure 3a</xref>) were based on the findings of recent functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) experiments investigating context-based object recognition (<xref ref-type="bibr" rid="bib3">Brandman and Peelen, 2017</xref>). In those experiments, participants viewed degraded objects in scene context, degraded objects alone, and scenes alone. Behavioral results showed that the degraded objects were easy to recognize when presented in scene context (&gt;70% correct in a nine-category task) but hard to recognize when presented alone (37% correct). fMRI results showed that the multivariate representation of the category of the degraded objects in LOC was strongly enhanced when the objects were viewed in scene context relative to when they were viewed alone. Importantly, the corresponding scenes presented alone did not evoke discriminable object category responses in LOC, providing evidence for supra-additive contextual facilitation. Interestingly, the contextual facilitation of object processing in LOC was correlated with concurrently evoked activity in scene-selective regions, suggesting an interaction between scene- and object-selective regions. MEG results showed that the information about the category of the degraded objects in scenes (derived from multivariate sensor patterns) peaked at two time points: at 160–180 ms and at 280–300 ms after stimulus onset. Crucially, only the later peak showed a significant contextual facilitation effect, with more information about the degraded objects in scenes than the degraded objects alone. Similar to the LOC results, at this time point, the scenes alone did not evoke discriminable object category responses, such that the contextual facilitation of object processing could not reflect the additive processing of scenes and objects. Taken together, these results indicate that scenes—processed in scene-selective cortex—disambiguate object representations in LOC at around 300 ms after stimulus onset.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Predictions and results.</title><p>(<bold>a</bold>) We hypothesized that isolated object recognition (top row) would be causally supported by EVC at 60–100 ms (early time point in right plot), followed by LOC at 160–200 ms (middle time point in central plot), reflecting feedforward processing of intact object features (<xref ref-type="bibr" rid="bib5">Cichy et al., 2014</xref>). Scene-selective OPA (left plot) was not expected to contribute to isolated object recognition at any time point (<xref ref-type="bibr" rid="bib10">Dilks et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">Wischnewski and Peelen, 2021</xref>). Similar to isolated object recognition, we hypothesized that context-based object recognition (middle row) would be causally supported by EVC at 60–100 ms and by LOC at 160–200 ms, reflecting feedforward processing. In contrast to isolated object recognition, we hypothesized that OPA would causally support context-based object recognition at 160–200 ms (middle time point in left plot), reflecting scene processing. Crucially, scene-based expectations were hypothesized to reach LOC later in time, disambiguating object representations at 260–300 ms (late time point in central plot; <xref ref-type="bibr" rid="bib3">Brandman and Peelen, 2017</xref>). TMS over LOC at this time point should thus selectively disrupt context-based object recognition. EVC was hypothesized to receive feedback from LOC at 160–200 ms (<xref ref-type="bibr" rid="bib4">Camprodon et al., 2010</xref>; <xref ref-type="bibr" rid="bib20">Koivisto et al., 2011</xref>; <xref ref-type="bibr" rid="bib27">Murray et al., 2002</xref>; <xref ref-type="bibr" rid="bib45">Wokke et al., 2013</xref>), which we expected to be most important for context-based object recognition, in which the object needs to be segregated from the background scene (<xref ref-type="bibr" rid="bib21">Korjoukov et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Lamme and Roelfsema, 2000</xref>; <xref ref-type="bibr" rid="bib39">Scholte et al., 2008</xref>). Finally, OPA was predicted to causally support scene-alone recognition at 160–200 ms (bottom row). (<bold><bold>b</bold></bold>) Results of three TMS experiments. Predictions were largely confirmed, except for feedback effects in EVC (at 160–200 ms), which were specific to isolated object recognition rather than context-based object recognition. *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001, with error bars reflecting the SEM. EVC, early visual cortex; LOC, lateral occipital cortex; OPA, occipital place area; TMS, transcranial magnetic stimulation.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Individual participant means (accuracy and RT).</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-69736-fig3-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69736-fig3-v1.tif"/></fig><p>The current TMS study was designed to provide causal evidence for this account. Differently from the neuroimaging studies, here we compared the recognition of degraded objects in scenes with the recognition of intact objects alone, rather than degraded objects alone. This was because the large accuracy difference between the recognition of degraded objects in scenes and degraded objects alone prevents a direct comparison of TMS effects between these conditions. Furthermore, this design allowed us to compare the causal neural mechanisms underlying object recognition based on scene context and local features, with the possibility to match the tasks in terms of recognition accuracy.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Across all TMS conditions, objects were equally recognizable when presented in isolation (without degradation; 75.8%) and when presented degraded within a scene (76.7%; main effect of Task: F(1,71)=1.22, p=0.27), showing that scene context can compensate for the loss of object visibility induced by the local degradation (<xref ref-type="bibr" rid="bib3">Brandman and Peelen, 2017</xref>). Importantly, despite the equal performance, recognition in the two object recognition tasks was supported by different neural mechanisms in a time-specific manner (three-way interaction between Task [intact object recognition, context-based object recognition], Region [OPA, LOC, and EVC], and Time [60–100 ms, 160–200 ms, and 260–300 ms]; F(4,138)=14.37, p&lt;0.001, η<sub>p</sub>²=0.294). This interaction was followed up by separate analyses for each of the stimulated regions.</p><p>TMS did not significantly affect response time (RT), with no interactions involving either TMS time or TMS region: Time×Region, F(4,138)=0.163, p=0.957; Task×Region, F(2,69)=0.81, p=0.450; Time×Task, F(2,142)=0.37, p=0.689; Time×Region×Task, F(4,138)=1.153, p=0.334. There were also no significant main effects of Time (F(2,142)=2.88, p=0.060) or Region (F(2,69)=0.82, p=0.447).</p><sec id="s2-1"><title>OPA experiment</title><p>Stimulation of scene-selective OPA differentially affected performance in the two tasks (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, left panel; Task×Time interaction F(2,46)=8.21, p&lt;0.001, η<sub>p</sub>²=0.263). For <italic>isolated object recognition</italic>, there was no effect of TMS time (F(2,46)=0.07, p=0.935, η<sub>p</sub>²=0.003), indicating that isolated object recognition was not influenced by TMS over OPA. By contrast, <italic>context-based object recognition</italic> was strongly modulated by TMS time (F(2,46)=19.54, p&lt;0.001, η<sub>p</sub>²=0.459). As predicted, TMS selectively impaired context-based object recognition performance when OPA was stimulated 160–200 ms after scene onset, both relative to earlier stimulation (t(23)=5.39, p&lt;0.001, d=1.099) and relative to later stimulation (t(23)=5.36, p&lt;0.001, d=1.095), with no significant difference between early and late stimulation (t(23)=0.26, p=0.795). These results show that OPA, a scene-selective region, is causally and selectively involved in (context-based) object recognition.</p><p>The pre-registration of the OPA experiment additionally included predictions for a third task, the scene-alone task (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, bottom row). Similar to the context-based recognition task, we expected that OPA stimulation at 160–200 ms after scene onset would impair accuracy in the scene-alone task. The Task×Time interaction reported above was also significant when including this condition as a third task in the ANOVA (F(4,92)=4.64, p=0.002, η<sub>p</sub>²=0.168). For the scene-alone task, accuracy was significantly affected by TMS time (F(2,46)=4.77, p=0.013, η<sub>p</sub>²=0.172). TMS impaired scene-alone accuracy when OPA was stimulated at 160–200 ms after scene onset relative to later stimulation (t(23)=3.02, p=0.006, d=0.616), though not relative to earlier stimulation (t(23)=1.62, p=0.118). There was no significant difference between early and late stimulation (t(23)=−1.50, p=0.145). Together with the context-based object recognition results, these findings provide information about the causal time course of OPA’s involvement in scene recognition, showing a selective OPA effect at 160–200 ms after stimulus onset.</p></sec><sec id="s2-2"><title>LOC experiment</title><p>Stimulation of object-selective LOC differentially affected performance in the two tasks (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, middle panel; Task×Time interaction F(2,46)=12.99, p&lt;0.001, η<sub>p</sub>²=0.361). For <italic>isolated object recognition</italic>, there was a main effect of TMS time (F(2,46)=15.50, p&lt;0.001, η<sub>p</sub>²=0.403; <xref ref-type="fig" rid="fig3">Figure 3b</xref>). As predicted, TMS selectively impaired isolated object recognition performance when LOC was stimulated at 160–200 ms after stimulus onset, both relative to earlier stimulation (t(23)=4.58, p&lt;0.001, d=0.936) and relative to later stimulation (t(23)=5.39, p&lt;0.001, d=1.101), with no significant difference between early and late stimulation (t(23)=−1.17, p=0.255). A different temporal profile was observed for <italic>context-based object recognition</italic>. For this task, TMS time also had a significant effect (F(2,46)=9.03, p&lt;0.001, η<sub>p</sub>²=0.282; <xref ref-type="fig" rid="fig3">Figure 3b</xref>). In contrast to the isolated object condition, performance strongly decreased when TMS was applied later in time, at 260–300 ms after stimulus onset, both relative to early stimulation (t(23)=4.01, p&lt;0.001, d=0.818) and relative to middle stimulation (t(23)=2.26, p=0.034, d=0.461). Context-based object recognition accuracy was moderately reduced when TMS was applied at 160–200 ms relative to earlier stimulation (t(23)=2.17, p=0.041, d=0.442). These findings confirm that LOC is causally involved in both isolated object recognition and context-based object recognition at 160–200 ms after stimulus onset. Crucially, LOC was causally involved in context-based object recognition at 260–300 ms, confirming our hypothesis that contextual feedback to LOC supports context-based object recognition.</p></sec><sec id="s2-3"><title>EVC experiment</title><p>Finally, stimulation of EVC allowed us to test whether similar feedback effects could be observed earlier in the visual hierarchy. Results showed that the time of EVC stimulation differentially affected performance in the two tasks (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, right panel; Task×Time interaction F(2,46)=14.42, p&lt;0.001, η<sub>p</sub>²=0.385). For <italic>isolated object recognition</italic>, there was a main effect of TMS time (F(2,46)=13.27, p&lt;0.001, η<sub>p</sub>²=0.366; <xref ref-type="fig" rid="fig3">Figure 3b</xref>). As predicted, TMS applied early in time impaired recognition performance relative to TMS late in time (t(23)=3.44, p=0.002, d=0.701). Interestingly, and contrary to our prediction, isolated object recognition was also impaired when TMS was applied at 160–200 ms compared to late stimulation (t(23)=5.19, p&lt;0.001, d=1.06). There was no difference in performance between TMS at early and intermediate time windows (t(23)=1.16, p=0.257). For <italic>context-based object recognition</italic>, there was a main effect of TMS time (F(2,46)=19.01, p&lt;0.001, η<sub>p</sub>²=0.452; <xref ref-type="fig" rid="fig3">Figure 3b</xref>). As predicted, TMS applied early in time impaired recognition performance relative to TMS late in time (t(23)=5.41, p&lt;0.001, d=1.105). Contrary to our prediction, context-based object recognition performance was not significantly reduced when TMS was applied at the middle time window relative to later stimulation (t(23)=0.99, p=0.334). These findings confirm that EVC is causally involved in initial visual processing, supporting both isolated object recognition and context-based recognition. In the 160–200 ms time window, EVC was causally involved in isolated object recognition but not context-based object recognition.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Altogether, these results reveal distinct neural mechanisms underlying object recognition based on local features (isolated object recognition) and scene context (context-based object recognition). During feedforward processing, EVC and object-selective cortex supported both the recognition of objects in scenes and in isolation, while scene-selective cortex was uniquely required for context-based object recognition. Results additionally showed that feedback to EVC causally supported isolated object recognition, while feedback to object-selective cortex causally supported context-based object recognition. These results provide evidence for two routes to object recognition, each characterized by feedforward and feedback processing but involving different brain regions at different time points (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Schematic summarizing results.</title><p>Distinct cortical routes causally support isolated object recognition and context-based object recognition. Isolated object recognition (top row) was supported by EVC early in time (60–100 ms), reflecting initial visual encoding. This was followed by LOC at 160–200 ms, reflecting higher-level object processing. At this time window, EVC was still required for isolated object recognition, presumably reflecting feedback processing. Similar to isolated object recognition, context-based object recognition (bottom row) was supported by EVC at 60–100 ms, followed by LOC at 160–200 ms. However, context-based object recognition additionally required OPA at 160–200 ms, reflecting scene processing. Finally, context-based object recognition causally depended on late processing (260–300 ms) in LOC, reflecting contextual disambiguation (<xref ref-type="bibr" rid="bib3">Brandman and Peelen, 2017</xref>). Note that the arrows do not necessarily reflect direct connections between brain regions. EVC, early visual cortex; LOC, lateral occipital cortex.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69736-fig4-v1.tif"/></fig><p>The finding that EVC (for isolated object recognition) and LOC (for context-based object recognition) causally supported object recognition well beyond the feedforward sweep suggests that feedback processing is required for accurate object recognition. Feedback processing in EVC and LOC may be explained under a common hierarchical perceptual inference framework (<xref ref-type="bibr" rid="bib12">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib15">Haefner et al., 2016</xref>; <xref ref-type="bibr" rid="bib23">Lee and Mumford, 2003</xref>; <xref ref-type="bibr" rid="bib35">Rao and Ballard, 1999</xref>), in which a global representation provides a prior that allows for disambiguating relatively more local information. For context-based object recognition, the scene (represented in OPA) would be the global element, providing a prior for processing the relatively more local shape of the object (represented in LOC). For isolated object recognition, object shape would be the global element, providing a prior for processing the relatively more local inner object features (e.g., the eyes of a squirrel; represented in EVC). Feedback based on the more global representations thus serves to disambiguate the representation of more local representations. While feedback processing was hypothesized for LOC based on previous neuroimaging findings, we did not hypothesize that feedback to EVC would be required for recognizing isolated objects. Future studies are needed to test under what conditions feedback to EVC causally contributes to object recognition (<xref ref-type="bibr" rid="bib4">Camprodon et al., 2010</xref>; <xref ref-type="bibr" rid="bib20">Koivisto et al., 2011</xref>; <xref ref-type="bibr" rid="bib45">Wokke et al., 2013</xref>). In line with the reverse hierarchy theory, we expect that the specific feedback that is useful for a given task—and the brain regions involved—depend on the available information in the image together with specific task demands (<xref ref-type="bibr" rid="bib17">Hochstein and Ahissar, 2002</xref>).</p><p>An alternative interpretation of the relatively late causal involvement of EVC in isolated object recognition, and LOC in context-based object recognition, is that these effects reflect local recurrence rather than feedback. This interpretation cannot be ruled out based on the current results alone. However, based on previous findings, we think this is unlikely, at least for LOC. In the fMRI study that used a similar stimulus set as used here (<xref ref-type="bibr" rid="bib3">Brandman and Peelen, 2017</xref>), representations of degraded objects in LOC were facilitated (relative to degraded objects alone) by the presence of scene context, indicating input from outside of LOC considering that LOC did not represent object information from scenes presented alone. Furthermore, the corresponding MEG study showed two peaks for degraded objects in scenes, one at 160–180 ms and one at 280–300 ms. The later peak showed a significant contextual facilitation effect in the MEG study, with better decoding of degraded objects in scenes than degraded objects alone. The present finding that TMS over LOC at 260–300 ms selectively impaired context-based object recognition is fully in line with these fMRI and MEG findings, pointing to feedback processing rather than local recurrence.</p><p>Taken together with previous findings, the current results are thus best explained by an account in which information from scenes (processed in scene-selective cortex) feeds back to LOC to disambiguate object representations. This mechanism may underlie the behavioral benefits previously observed for object recognition in semantically and syntactically congruent (vs. incongruent) scene context (<xref ref-type="bibr" rid="bib2">Biederman et al., 1982</xref>; <xref ref-type="bibr" rid="bib6">Davenport and Potter, 2004</xref>; <xref ref-type="bibr" rid="bib26">Munneke et al., 2013</xref>; <xref ref-type="bibr" rid="bib43">Võ and Wolfe, 2013</xref>), as predicted by interactive accounts that propose that contextual facilitation is supported by contextual expectations (<xref ref-type="bibr" rid="bib1">Bar, 2004</xref>; <xref ref-type="bibr" rid="bib6">Davenport and Potter, 2004</xref>), with quickly extracted global scene ‘gist’ priming the representation of candidate objects in the visual cortex (<xref ref-type="bibr" rid="bib1">Bar, 2004</xref>; <xref ref-type="bibr" rid="bib29">Oliva and Torralba, 2007</xref>; <xref ref-type="bibr" rid="bib41">Torralba, 2003</xref>). The current TMS results suggest that OPA is crucial for extracting this global scene information at around 160–200 ms after scene onset, and that this information is integrated with local object information in LOC around 100 ms later. The current results do not speak to whether OPA-LOC connectivity is direct or indirect, for example involving additional brain regions such as other scene-selective regions or the orbitofrontal cortex (<xref ref-type="bibr" rid="bib1">Bar, 2004</xref>).</p><p>Our study raises the interesting question of what type of context-based expectations help to disambiguate object representations in LOC. The scenes in the current study provided multiple cues that may help to recognize the degraded objects. For example, the scenes provided information about the approximate real-world size of the objects as well as the objects’ likely semantic category. Both of these cues may help to recognize objects (<xref ref-type="bibr" rid="bib2">Biederman et al., 1982</xref>; <xref ref-type="bibr" rid="bib6">Davenport and Potter, 2004</xref>; <xref ref-type="bibr" rid="bib26">Munneke et al., 2013</xref>; <xref ref-type="bibr" rid="bib43">Võ and Wolfe, 2013</xref>). Future experiments could test whether feedback to LOC is specifically related to one of these cues. For example, one could test whether similar effects are found when objects are presented in semantically uninformative scenes, with the scene only providing information about the approximate real-world size of the object.</p><p>To conclude, the current study provides causal evidence that context-based expectations facilitate object recognition by disambiguating object representations in the visual cortex. More generally, results reveal that distinct neural mechanisms support object recognition based on local features and global scene context. Future experiments may extend our approach to include other contextual features such as co-occurring objects, temporal context, and input from other modalities.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Prior to experimentation, we decided to test 24 participants in all three experiments. Preregistrations can be found at <ext-link ext-link-type="uri" xlink:href="https://aspredicted.org/cs4wz.pdf">https://aspredicted.org/cs4wz.pdf</ext-link> (OPA), <ext-link ext-link-type="uri" xlink:href="https://aspredicted.org/yc969.pdf">https://aspredicted.org/yc969.pdf</ext-link> (LOC), and <ext-link ext-link-type="uri" xlink:href="https://aspredicted.org/cy9fq.pdf">https://aspredicted.org/cy9fq.pdf</ext-link> (EVC). In total, 72 right-handed volunteers (43 females, mean age ± SD = 23.33 ± 3.59, age range = 18–33) with normal or corrected-to-normal vision took part in the experiment, after participating in a TMS localization experiment (<xref ref-type="bibr" rid="bib44">Wischnewski and Peelen, 2021</xref>). Participants were excluded if they reported to have one of the following: CNS-acting medication, previous neurosurgical treatments, metal implants in the head or neck area, migraine, epilepsy or previous cerebral seizures (also within their family), pacemaker, intracranial metal clips, cochlea implants, or pregnancy. Additionally, participants were asked to refrain from consuming alcohol and recreational drugs 72 hr before the experiment and refrain from consuming coffee 2 hr before the experiment. Participants were divided over three experiments, targeting three cortical areas, based on a previous experiment. All experiments included 24 participants (OPA experiment, 12 females, mean age ± SD = 23.67 ± 3.92; LOC experiment, 14 females, mean age ± SD = 23.50 ± 3.09; EVC experiment, 17 females, mean age ± SD = 22.83 ± 3.81). Prior to the experimental session, participants were informed about the experimental procedures and gave written informed consent. The study procedures were approved by the ‘Centrale Commissie voor Mensgebonden Onderzoek (CCMO)’ and conducted in accordance with the Declaration of Helsinki.</p></sec><sec id="s4-2"><title>Transcranial magnetic stimulation</title><p>TMS was applied via a Cool-B65 figure-of-8 coil with an outer diameter of 75 mm, which received input from a Magpro-X-100 magnetic stimulator (MagVenture, Farum, Denmark). Two TMS pulses (biphasic, wavelength: 280 µs) separated by 40 ms (25 Hz) were applied to disrupt visual cortex activity. Given that latency of visual cortex activation varies across participants, a two-pulse TMS design was chosen since it allows for a broader time window of disruption while maintaining relatively good temporal resolution (<xref ref-type="bibr" rid="bib28">O'Shea et al., 2004</xref>; <xref ref-type="bibr" rid="bib33">Pitcher et al., 2007</xref>; <xref ref-type="bibr" rid="bib45">Wokke et al., 2013</xref>). The intensity of stimulation was adjusted to 85% of the individual phosphene threshold (PT). PT was established by increasing stimulator output targeting EVC until 50% of the pulses resulted in the perception of a phosphene while participants fixated on a black screen in a dimly lit room. The TMS coil was placed with the help of an infrared-based neuronavigation system (Localite, Bonn, Germany) using an individually adapted standard brain model over the right LOC, right OPA, or EVC. Each stimulation location was identified through Talairach coordinates set in the Localite neuronavigation system. The coordinates were 45, –74, 0 for LOC (<xref ref-type="bibr" rid="bib34">Pitcher et al., 2009</xref>) and 34, –77, 21 for OPA (<xref ref-type="bibr" rid="bib18">Julian et al., 2016</xref>). TMS was placed on EVC based on its anatomical location, 2 cm above the inion (<xref ref-type="bibr" rid="bib19">Koivisto et al., 2010</xref>; <xref ref-type="bibr" rid="bib32">Pascual-Leone and Walsh, 2001</xref>). We then established the optimal coil position in such a way that phosphenes were reported centrally in the visual field, where the stimuli were presented.</p></sec><sec id="s4-3"><title>Experimental stimuli</title><p>Stimuli consisted of 128 scene photographs with a single object belonging to one of the following eight categories: airplane, bird, car, fish, human, mammal, ship, and train. For the isolated object recognition task, the object was cropped out of the scene and presented at its original location on a gray background. For the context-based object recognition task, the object was pixelated to remove local features. The experiment additionally included a scene-alone condition, in which the object was cropped out and replaced with background using a content-aware fill tool. In this condition, participants had to guess the object category of the cropped-out object.</p><p>To avoid that participants could recognize the degraded objects in scenes based on having seen their intact version, the stimulus set was divided into two halves: for each participant, half of the stimuli were used in the context-based object condition, and the other half of the stimuli were used both in the isolated object condition and the scene-alone condition. This assignment was counterbalanced across participants. The scenes spanned a visual angle of 6°×4.5°.</p></sec><sec id="s4-4"><title>Main task</title><p>Before the experiment, participants received instructions and were presented with an example stimulus (which was not used in the main experiment). This example displayed how each stimulus variation (context-based, isolated object, and scene alone) was derived from an original photograph. For the main task, each trial started with a fixation cross (500 ms), followed by a stimulus presented for 33 ms. Next, a blank screen was shown for 500 ms. After this, participants were asked to respond by pressing one out of eight possible keys according to the object category presented (<xref ref-type="fig" rid="fig2">Figure 2</xref>). No limit on RT was given. However, participants were encouraged during the instructions to respond within 3 s. The response screen was presented until the participant responded. The next trial started after a 2 s inter-trial interval. This relatively long interval was chosen to prevent repetitive TMS effects. TMS was applied at one of three different time points, with randomized order. TMS pulses could be applied at 60 ms and 100 ms after stimulus onset, 160 ms and 200 ms after stimulus onset, or 260 ms and 300 ms after stimulus onset. In 2 participants out of the 72 (1 in the LOC experiment and 1 in the EVC experiment), each pulse was accidentally delivered 16 ms earlier than described above.</p><p>Each stimulus was repeated three times, once for each TMS timing (60–100 ms, 160–200 ms, and 260–300 ms). This resulted in a total of 576 trials, which were presented in a random order. To avoid fatigue, the task was divided into 12 blocks of 48 trials, each lasting approximately 4 min, with short breaks in between of approximately 1 min. Thus, completing the task took about 60 min. The total duration of the experiment, including preparation and PT determination, was approximately 90 min.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 Research and Innovation Program (Grant agreement no 725970). The authors would like to thank Talia Brandman for help in stimulus creation, Andrea Ghiani for discussing results, and Marco Gandolfo, Floris de Lange, and Surya Gayet for feedback on an earlier version of the manuscript.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Formal analysis, Investigation, Methodology, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Prior to the experimental session, participants were informed about the experimental procedures and gave written informed consent. The study procedures were approved by the 'Centrale Commissie voor Mensgebonden Onderzoek (CCMO)' under project number 2019-5311 (NL69407.091.19), and conducted in accordance with the Declaration of Helsinki.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-69736-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data generated or analysed during this study are included in the manuscript and supporting files. Source data files have been provided for Figure 3.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Visual objects in context</article-title><source>Nature Reviews Neuroscience</source><volume>5</volume><fpage>617</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.1038/nrn1476</pub-id><pub-id pub-id-type="pmid">15263892</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname> <given-names>I</given-names></name><name><surname>Mezzanotte</surname> <given-names>RJ</given-names></name><name><surname>Rabinowitz</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Scene perception: detecting and judging objects undergoing relational violations</article-title><source>Cognitive Psychology</source><volume>14</volume><fpage>143</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(82)90007-X</pub-id><pub-id pub-id-type="pmid">7083801</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandman</surname> <given-names>T</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Interaction between scene and object processing revealed by human fMRI and MEG decoding</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>7700</fpage><lpage>7710</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0582-17.2017</pub-id><pub-id pub-id-type="pmid">28687603</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camprodon</surname> <given-names>JA</given-names></name><name><surname>Zohary</surname> <given-names>E</given-names></name><name><surname>Brodbeck</surname> <given-names>V</given-names></name><name><surname>Pascual-Leone</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Two phases of V1 activity for visual recognition of natural images</article-title><source>Journal of Cognitive Neuroscience</source><volume>22</volume><fpage>1262</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21253</pub-id><pub-id pub-id-type="pmid">19413482</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id><pub-id pub-id-type="pmid">24464044</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davenport</surname> <given-names>JL</given-names></name><name><surname>Potter</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Scene consistency in object and background perception</article-title><source>Psychological Science</source><volume>15</volume><fpage>559</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1111/j.0956-7976.2004.00719.x</pub-id><pub-id pub-id-type="pmid">15271002</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname> <given-names>FP</given-names></name><name><surname>Heilbron</surname> <given-names>M</given-names></name><name><surname>Kok</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How do expectations shape perception?</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>764</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.06.002</pub-id><pub-id pub-id-type="pmid">30122170</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name><name><surname>Zoccolan</surname> <given-names>D</given-names></name><name><surname>Rust</surname> <given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname> <given-names>N</given-names></name><name><surname>Mostert</surname> <given-names>P</given-names></name><name><surname>Lange</surname> <given-names>FP</given-names></name><name><surname>Bosch</surname> <given-names>S</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Differential temporal dynamics during visual imagery and perception</article-title><source>eLife</source><volume>7</volume><elocation-id>e33904</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.33904</pub-id><pub-id pub-id-type="pmid">29807570</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dilks</surname> <given-names>DD</given-names></name><name><surname>Julian</surname> <given-names>JB</given-names></name><name><surname>Paunov</surname> <given-names>AM</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The occipital place area is causally and selectively involved in scene perception</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>1331</fpage><lpage>1336</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4081-12.2013</pub-id><pub-id pub-id-type="pmid">23345209</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faivre</surname> <given-names>N</given-names></name><name><surname>Dubois</surname> <given-names>J</given-names></name><name><surname>Schwartz</surname> <given-names>N</given-names></name><name><surname>Mudrik</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Imaging object-scene relations processing in visible and invisible natural scenes</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>4567</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-38654-z</pub-id><pub-id pub-id-type="pmid">30872607</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The neural basis of object perception</article-title><source>Current Opinion in Neurobiology</source><volume>13</volume><fpage>159</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(03)00040-0</pub-id><pub-id pub-id-type="pmid">12744968</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gronau</surname> <given-names>N</given-names></name><name><surname>Neta</surname> <given-names>M</given-names></name><name><surname>Bar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Integrated contextual representation for objects' identities and their locations</article-title><source>Journal of Cognitive Neuroscience</source><volume>20</volume><fpage>371</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1162/jocn.2008.20027</pub-id><pub-id pub-id-type="pmid">18004950</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haefner</surname> <given-names>RM</given-names></name><name><surname>Berkes</surname> <given-names>P</given-names></name><name><surname>Fiser</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perceptual Decision-Making as probabilistic inference by neural sampling</article-title><source>Neuron</source><volume>90</volume><fpage>649</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.03.020</pub-id><pub-id pub-id-type="pmid">27146267</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname> <given-names>JM</given-names></name><name><surname>Hollingworth</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>High-level scene perception</article-title><source>Annual Review of Psychology</source><volume>50</volume><fpage>243</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.50.1.243</pub-id><pub-id pub-id-type="pmid">10074679</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochstein</surname> <given-names>S</given-names></name><name><surname>Ahissar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>View from the top: hierarchies and reverse hierarchies in the visual system</article-title><source>Neuron</source><volume>36</volume><fpage>791</fpage><lpage>804</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)01091-7</pub-id><pub-id pub-id-type="pmid">12467584</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Julian</surname> <given-names>JB</given-names></name><name><surname>Ryan</surname> <given-names>J</given-names></name><name><surname>Hamilton</surname> <given-names>RH</given-names></name><name><surname>Epstein</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The occipital place area is causally involved in representing environmental boundaries during navigation</article-title><source>Current Biology</source><volume>26</volume><fpage>1104</fpage><lpage>1109</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.02.066</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koivisto</surname> <given-names>M</given-names></name><name><surname>Mäntylä</surname> <given-names>T</given-names></name><name><surname>Silvanto</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The role of early visual cortex (V1/V2) in conscious and unconscious visual perception</article-title><source>NeuroImage</source><volume>51</volume><fpage>828</fpage><lpage>834</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.02.042</pub-id><pub-id pub-id-type="pmid">20188199</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koivisto</surname> <given-names>M</given-names></name><name><surname>Railo</surname> <given-names>H</given-names></name><name><surname>Revonsuo</surname> <given-names>A</given-names></name><name><surname>Vanni</surname> <given-names>S</given-names></name><name><surname>Salminen-Vaparanta</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Recurrent processing in V1/V2 contributes to categorization of natural scenes</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>2488</fpage><lpage>2492</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3074-10.2011</pub-id><pub-id pub-id-type="pmid">21325516</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Korjoukov</surname> <given-names>I</given-names></name><name><surname>Jeurissen</surname> <given-names>D</given-names></name><name><surname>Kloosterman</surname> <given-names>NA</given-names></name><name><surname>Verhoeven</surname> <given-names>JE</given-names></name><name><surname>Scholte</surname> <given-names>HS</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The time course of perceptual grouping in natural scenes</article-title><source>Psychological Science</source><volume>23</volume><fpage>1482</fpage><lpage>1489</lpage><pub-id pub-id-type="doi">10.1177/0956797612443832</pub-id><pub-id pub-id-type="pmid">23137967</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname> <given-names>VA</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The distinct modes of vision offered by feedforward and recurrent processing</article-title><source>Trends in Neurosciences</source><volume>23</volume><fpage>571</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01657-X</pub-id><pub-id pub-id-type="pmid">11074267</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>TS</given-names></name><name><surname>Mumford</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Hierarchical bayesian inference in the visual cortex</article-title><source>Journal of the Optical Society of America A</source><volume>20</volume><elocation-id>1434</elocation-id><pub-id pub-id-type="doi">10.1364/JOSAA.20.001434</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>H</given-names></name><name><surname>Agam</surname> <given-names>Y</given-names></name><name><surname>Madsen</surname> <given-names>JR</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Timing, timing, timing: fast decoding of object information from intracranial field potentials in human visual cortex</article-title><source>Neuron</source><volume>62</volume><fpage>281</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.025</pub-id><pub-id pub-id-type="pmid">19409272</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malach</surname> <given-names>R</given-names></name><name><surname>Reppas</surname> <given-names>JB</given-names></name><name><surname>Benson</surname> <given-names>RR</given-names></name><name><surname>Kwong</surname> <given-names>KK</given-names></name><name><surname>Jiang</surname> <given-names>H</given-names></name><name><surname>Kennedy</surname> <given-names>WA</given-names></name><name><surname>Ledden</surname> <given-names>PJ</given-names></name><name><surname>Brady</surname> <given-names>TJ</given-names></name><name><surname>Rosen</surname> <given-names>BR</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex</article-title><source>PNAS</source><volume>92</volume><fpage>8135</fpage><lpage>8139</lpage><pub-id pub-id-type="doi">10.1073/pnas.92.18.8135</pub-id><pub-id pub-id-type="pmid">7667258</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munneke</surname> <given-names>J</given-names></name><name><surname>Brentari</surname> <given-names>V</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The influence of scene context on object recognition is independent of attentional focus</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>552</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00552</pub-id><pub-id pub-id-type="pmid">23970878</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname> <given-names>SO</given-names></name><name><surname>Kersten</surname> <given-names>D</given-names></name><name><surname>Olshausen</surname> <given-names>BA</given-names></name><name><surname>Schrater</surname> <given-names>P</given-names></name><name><surname>Woods</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Shape perception reduces activity in human primary visual cortex</article-title><source>PNAS</source><volume>99</volume><fpage>15164</fpage><lpage>15169</lpage><pub-id pub-id-type="doi">10.1073/pnas.192579399</pub-id><pub-id pub-id-type="pmid">12417754</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Shea</surname> <given-names>J</given-names></name><name><surname>Muggleton</surname> <given-names>NG</given-names></name><name><surname>Cowey</surname> <given-names>A</given-names></name><name><surname>Walsh</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Timing of target discrimination in human frontal eye fields</article-title><source>Journal of Cognitive Neuroscience</source><volume>16</volume><fpage>1060</fpage><lpage>1067</lpage><pub-id pub-id-type="doi">10.1162/0898929041502634</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliva</surname> <given-names>A</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The role of context in object recognition</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>520</fpage><lpage>527</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.09.009</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Opitz</surname> <given-names>A</given-names></name><name><surname>Legon</surname> <given-names>W</given-names></name><name><surname>Rowlands</surname> <given-names>A</given-names></name><name><surname>Bickel</surname> <given-names>WK</given-names></name><name><surname>Paulus</surname> <given-names>W</given-names></name><name><surname>Tyler</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Physiological observations validate finite element models for estimating subject-specific electric field distributions induced by transcranial magnetic stimulation of the human motor cortex</article-title><source>NeuroImage</source><volume>81</volume><fpage>253</fpage><lpage>264</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.067</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname> <given-names>S</given-names></name><name><surname>Brady</surname> <given-names>TF</given-names></name><name><surname>Greene</surname> <given-names>MR</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Disentangling scene content from spatial boundary: complementary roles for the parahippocampal place area and lateral occipital complex in representing real-world scenes</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>1333</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3885-10.2011</pub-id><pub-id pub-id-type="pmid">21273418</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pascual-Leone</surname> <given-names>A</given-names></name><name><surname>Walsh</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Fast backprojections from the motion to the primary visual area necessary for visual awareness</article-title><source>Science</source><volume>292</volume><fpage>510</fpage><lpage>512</lpage><pub-id pub-id-type="doi">10.1126/science.1057099</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname> <given-names>D</given-names></name><name><surname>Walsh</surname> <given-names>V</given-names></name><name><surname>Yovel</surname> <given-names>G</given-names></name><name><surname>Duchaine</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>TMS evidence for the involvement of the right occipital face area in early face processing</article-title><source>Current Biology</source><volume>17</volume><fpage>1568</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.07.063</pub-id><pub-id pub-id-type="pmid">17764942</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname> <given-names>D</given-names></name><name><surname>Charles</surname> <given-names>L</given-names></name><name><surname>Devlin</surname> <given-names>JT</given-names></name><name><surname>Walsh</surname> <given-names>V</given-names></name><name><surname>Duchaine</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Triple dissociation of faces, bodies, and objects in extrastriate cortex</article-title><source>Current Biology</source><volume>19</volume><fpage>319</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.01.007</pub-id><pub-id pub-id-type="pmid">19200723</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname> <given-names>RPN</given-names></name><name><surname>Ballard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddy</surname> <given-names>L</given-names></name><name><surname>Tsuchiya</surname> <given-names>N</given-names></name><name><surname>Serre</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Reading the mind's eye: Decoding category information during mental imagery</article-title><source>NeuroImage</source><volume>50</volume><fpage>818</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.11.084</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rémy</surname> <given-names>F</given-names></name><name><surname>Vayssière</surname> <given-names>N</given-names></name><name><surname>Pins</surname> <given-names>D</given-names></name><name><surname>Boucart</surname> <given-names>M</given-names></name><name><surname>Fabre-Thorpe</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Incongruent object/context relationships in visual scenes: Where are they processed in the brain?</article-title><source>Brain and Cognition</source><volume>84</volume><fpage>34</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/j.bandc.2013.10.008</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Hierarchical models of object recognition in cortex</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>1019</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1038/14819</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scholte</surname> <given-names>HS</given-names></name><name><surname>Jolij</surname> <given-names>J</given-names></name><name><surname>Fahrenfort</surname> <given-names>JJ</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Feedforward and recurrent processing in scene segmentation: electroencephalography and functional magnetic resonance imaging</article-title><source>Journal of Cognitive Neuroscience</source><volume>20</volume><fpage>2097</fpage><lpage>2109</lpage><pub-id pub-id-type="doi">10.1162/jocn.2008.20142</pub-id><pub-id pub-id-type="pmid">18416684</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname> <given-names>T</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A feedforward architecture accounts for rapid categorization</article-title><source>PNAS</source><volume>104</volume><fpage>6424</fpage><lpage>6429</lpage><pub-id pub-id-type="doi">10.1073/pnas.0700622104</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torralba</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Contextual Priming for Object Detection</article-title><source>International Journal of Computer Vision</source><volume>53</volume><fpage>169</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1023/A:1023052124951</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Koningsbruggen</surname> <given-names>MG</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A causal role for the extrastriate body area in detecting people in Real-World scenes</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>7003</fpage><lpage>7010</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2853-12.2013</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Võ</surname> <given-names>ML</given-names></name><name><surname>Wolfe</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Differential electrophysiological signatures of semantic and syntactic scene processing</article-title><source>Psychological Science</source><volume>24</volume><fpage>1816</fpage><lpage>1823</lpage><pub-id pub-id-type="doi">10.1177/0956797613476955</pub-id><pub-id pub-id-type="pmid">23842954</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wischnewski</surname> <given-names>M</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Causal evidence for a double dissociation between object- and Scene-Selective regions of visual cortex: a preregistered TMS replication study</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>751</fpage><lpage>756</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2162-20.2020</pub-id><pub-id pub-id-type="pmid">33262244</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wokke</surname> <given-names>ME</given-names></name><name><surname>Vandenbroucke</surname> <given-names>AR</given-names></name><name><surname>Scholte</surname> <given-names>HS</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Confuse your illusion: feedback to early visual cortex contributes to perceptual completion</article-title><source>Psychological Science</source><volume>24</volume><fpage>63</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1177/0956797612449175</pub-id><pub-id pub-id-type="pmid">23228938</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.69736.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>O'Connell</surname><given-names>Redmond G</given-names></name><role>Reviewing Editor</role><aff><institution>Trinity College Dublin</institution><country>Ireland</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>O'Connell</surname><given-names>Redmond G</given-names></name><role>Reviewer</role><aff><institution>Trinity College Dublin</institution><country>Ireland</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Kok</surname><given-names>Peter</given-names> </name><role>Reviewer</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.04.23.441170">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.04.23.441170v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This study will be of interest to scientists involved in high-level vision. The data provide a compelling demonstration of the causal role of three key visual areas in context-based object recognition. The key claims of the manuscript are supported by the data, and are strengthened by the pre-registration of each of the three experiments.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Causal neural mechanisms of context-based object recognition&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Redmond G O’Connell as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Joshua Gold as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Peter Kok (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) The preregistration for this study commits to including the scene-only condition in the statistical analyses for the main experiment however only the object-based and context-based conditions were considered. The authors should either provide a strong justification for this deviation or else run and report the statistics as originally planned.</p><p>2) More detail on the precise participant screening procedure is required. Exactly what tasks and stimuli were participants exposed to? What TMS SOAs were used? How many participants were excluded based on this procedure? A statement should be added to the main text flagging this procedure to the reader so that they are clear that the basic main effect of TMS to LOC object-based task performance was pre-ordained.</p><p>3) The authors should provide greater discussion of alternative interpretations of their results. Currently the authors only entertain the possibility that the results reflect feedback effects but they should also consider the possibility of persistent/recurrent activity within visual areas reflecting extended processing of stimuli held in iconic memory.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The study hypotheses could be articulated more clearly. Citations should be provided to back up the hypothesised TMS SOA effects in each region. The legend of Figure 3 would seem a good place to lay out the predictions in more detail.</p><p>The area identified as EVA should be clarified in the Introduction. What visual regions does this area encompass? Just V1? Others?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. The authors argue that feedback based on more global representations (of the scene) serve to disambiguate more local representations (of the object).</p><p>The scene-only condition seems important to this interpretation, and I would suggest discussing this manipulation further in the main text. An alternative interpretation is that scene recognition simply reduces the epistemic priors of what the object could be (e.g., if the scene is grass, the object is unlikely to be a fish, ship, airplane, or train). Indeed, Figure S2 suggests that performance in the scene-only condition, although relatively poor (40-50%), is much higher than chance (12.5%). So scene recognition seems to filter the range of possible correct responses in the first place, and the effects that are observed may be separate to the capacity of scene recognition to disambiguate specific features of the object itself.</p><p>I take the authors' point that the absent effect of TMS on accuracy in the scene-only condition argues against this as a possibility. Nevertheless, an informative condition may have been one in which the image was simply occluded by a shape (e.g., circle) that provided only information about the relative size of the object, but not about its internal features. Such a manipulation would have maintained a similarity with the context-based condition while eliminating the intrinsic perceptual features of the object that could be disambiguated by the scene (other than its approximate size, which provides information about the possible object category, but not the object itself). This may have allowed the authors to more clearly determine whether context-based object recognition is specifically driven by disambiguation of the perceptual features intrinsic to each object.</p><p>I am not necessarily suggesting running an additional experiment, but further clarification/discussion on the above issues would be worthwhile.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>– Why where these exact time windows chosen for stimulation (and hypotheses)? Can you provide a more concrete (neurophysiological) rationale?</p><p>– The EVC results are opposite to the hypothesised ones, with later involvement for objects in isolation than those in context. The authors acknowledge this, but do not discuss in much depth why this might be. Alternatively, they might simply acknowledge that we do not know why this is, and more research is needed on this point.</p><p>– Could you also draw the hypotheses that the parallel account yields, so that it can be clearly seen which data points distinguish the hypotheses? Is the late LOC stimulation effect in context vs. isolation the only crucial point?</p><p>– Regarding this datapoint, could it be that late LOC stimulation interferes with object recognition in the context condition because the objects were degraded, rather than because they were presented in a scene context? In other words, because there is more (local) recurrence in LOC required to resolve degraded objects? This seems important to rule out (or acknowledge).</p><p>– From Figure S1, it seems that participants were generally faster in the EVC experiment. Any idea why this might be?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.69736.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The preregistration for this study commits to including the scene-only condition in the statistical analyses for the main experiment however only the object-based and context-based conditions were considered. The authors should either provide a strong justification for this deviation or else run and report the statistics as originally planned.</p></disp-quote><p>The pre-registered statistics for the scene-alone condition in the OPA experiment are now included in the manuscript (p.9-10). The relevant figure (Figure 3) has also been updated such that the scene-alone condition results are now in the main text rather than the Supplement. Results confirmed our predictions, showing a reduction of scene-alone performance when OPA was stimulated 160-200 ms after stimulus onset. Note that the scene-alone condition was only included in the pre-registration of the OPA experiment, which is why we had not reported the corresponding statistics previously. (This condition was not relevant for the LOC and EVC experiments.)</p><disp-quote content-type="editor-comment"><p>2) More detail on the precise participant screening procedure is required. Exactly what tasks and stimuli were participants exposed to? What TMS SOAs were used? How many participants were excluded based on this procedure? A statement should be added to the main text flagging this procedure to the reader so that they are clear that the basic main effect of TMS to LOC object-based task performance was pre-ordained.</p></disp-quote><p>We now introduce the screening procedure in the main text and point the reader to a recent publication that documents the methods and results of this experiment (Wischnewski and Peelen, J Neurosci 2021). The screening experiment followed the design of Dilks et al. (J Neurosci 2013), stimulating OPA and LOC using 5 TMS pulses at a rate of 10Hz (i.e., no SOAs were used). No participants were excluded – all participants were assigned to one of the three conditions (OPA, LOC, EVC). This is now more clearly explained in the manuscript.</p><disp-quote content-type="editor-comment"><p>3) The authors should provide greater discussion of alternative interpretations of their results. Currently the authors only entertain the possibility that the results reflect feedback effects but they should also consider the possibility of persistent/recurrent activity within visual areas reflecting extended processing of stimuli held in iconic memory.</p></disp-quote><p>We have added a paragraph to the Discussion section in which we discuss the alternative interpretation of local recurrence (p.13-14).</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>The study hypotheses could be articulated more clearly. Citations should be provided to back up the hypothesised TMS SOA effects in each region. The legend of Figure 3 would seem a good place to lay out the predictions in more detail.</p></disp-quote><p>Thanks for these suggestions. We have added citations to back up the hypothesized SOAs. We have also explained the previous fMRI/MEG study that led to these predictions in more detail. We now also included a more detailed explanation in the Figure 3 legend.</p><disp-quote content-type="editor-comment"><p>The area identified as EVA should be clarified in the Introduction. What visual regions does this area encompass? Just V1? Others?</p></disp-quote><p>We have added this clarification to the Introduction, citing previous work using the same procedures. EVC here primarily corresponds to V1.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1. The authors argue that feedback based on more global representations (of the scene) serve to disambiguate more local representations (of the object).</p><p>The scene-only condition seems important to this interpretation, and I would suggest discussing this manipulation further in the main text. An alternative interpretation is that scene recognition simply reduces the epistemic priors of what the object could be (e.g., if the scene is grass, the object is unlikely to be a fish, ship, airplane, or train). Indeed, Figure S2 suggests that performance in the scene-only condition, although relatively poor (40-50%), is much higher than chance (12.5%). So scene recognition seems to filter the range of possible correct responses in the first place, and the effects that are observed may be separate to the capacity of scene recognition to disambiguate specific features of the object itself.</p><p>I take the authors' point that the absent effect of TMS on accuracy in the scene-only condition argues against this as a possibility. Nevertheless, an informative condition may have been one in which the image was simply occluded by a shape (e.g., circle) that provided only information about the relative size of the object, but not about its internal features. Such a manipulation would have maintained a similarity with the context-based condition while eliminating the intrinsic perceptual features of the object that could be disambiguated by the scene (other than its approximate size, which provides information about the possible object category, but not the object itself). This may have allowed the authors to more clearly determine whether context-based object recognition is specifically driven by disambiguation of the perceptual features intrinsic to each object.</p><p>I am not necessarily suggesting running an additional experiment, but further clarification/discussion on the above issues would be worthwhile.</p></disp-quote><p>The scene-alone condition is now included in the main text, as well as in Figure 3. We have also included a more extensive summary of previous fMRI and MEG studies that formed the basis for the current study, where we focused on the question of whether scene and object information are additive or super-additive. We have explained this study in more detail in the Introduction to make the predictions clearer.</p><p>The reviewer also raises an interesting point about what aspects the scene helps to disambiguate. Object size is certainly one candidate. We have included a paragraph to the Discussion raising this possibility (p.14-15).</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>– Why where these exact time windows chosen for stimulation (and hypotheses)? Can you provide a more concrete (neurophysiological) rationale?</p></disp-quote><p>We have added citations to back up the hypothesized SOAs. We have also explained our previous fMRI and MEG work in more detail in the Introduction, as this led to the current predictions.</p><disp-quote content-type="editor-comment"><p>– The EVC results are opposite to the hypothesised ones, with later involvement for objects in isolation than those in context. The authors acknowledge this, but do not discuss in much depth why this might be. Alternatively, they might simply acknowledge that we do not know why this is, and more research is needed on this point.</p></disp-quote><p>We have extended the discussion of these results and mention that more work is needed to follow up on these findings (p.13).</p><disp-quote content-type="editor-comment"><p>– Could you also draw the hypotheses that the parallel account yields, so that it can be clearly seen which data points distinguish the hypotheses? Is the late LOC stimulation effect in context vs. isolation the only crucial point?</p></disp-quote><p>Yes, the late LOC effect is the most informative point to distinguish between the hypotheses. We have now made this clear in the Introduction, also based on a more detailed description of previous fMRI/MEG work.</p><disp-quote content-type="editor-comment"><p>– Regarding this datapoint, could it be that late LOC stimulation interferes with object recognition in the context condition because the objects were degraded, rather than because they were presented in a scene context? In other words, because there is more (local) recurrence in LOC required to resolve degraded objects? This seems important to rule out (or acknowledge).</p></disp-quote><p>We have added a paragraph to the Discussion section in which we discuss the alternative interpretation of local recurrence (p.13-14).</p><disp-quote content-type="editor-comment"><p>– From Figure S1, it seems that participants were generally faster in the EVC experiment. Any idea why this might be?</p></disp-quote><p>There were no significant differences between regions (main effect of Region: (F(2,69) = 0.82, p = 0.447)), which is now reported in the main text (p.8). Thus, while participants were numerically faster in the EVC experiment, this was not reliably different from the other regions (note that Region was manipulated across participants, unlike the other variables).</p></body></sub-article></article>