<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">77216</article-id><article-id pub-id-type="doi">10.7554/eLife.77216</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Self-configuring feedback loops for sensorimotor control</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-267564"><name><surname>Verduzco-Flores</surname><given-names>Sergio Oscar</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0712-145X</contrib-id><email>sergio.verduzco@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-7802"><name><surname>De Schutter</surname><given-names>Erik</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8618-5138</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02qg15b79</institution-id><institution>Computational Neuroscience Unit, Okinawa Institute of Science and Technology</institution></institution-wrap><addr-line><named-content content-type="city">Okinawa</named-content></addr-line><country>Japan</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gallego</surname><given-names>Juan Álvaro</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Imperial College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Makin</surname><given-names>Tamar R</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>14</day><month>11</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e77216</elocation-id><history><date date-type="received" iso-8601-date="2022-01-20"><day>20</day><month>01</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-10-26"><day>26</day><month>10</month><year>2022</year></date></history><permissions><copyright-statement>© 2022, Verduzco-Flores and De Schutter</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Verduzco-Flores and De Schutter</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-77216-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-77216-figures-v2.pdf"/><abstract><p>How dynamic interactions between nervous system regions in mammals performs online motor control remains an unsolved problem. In this paper, we show that feedback control is a simple, yet powerful way to understand the neural dynamics of sensorimotor control. We make our case using a minimal model comprising spinal cord, sensory and motor cortex, coupled by long connections that are plastic. It succeeds in learning how to perform reaching movements of a planar arm with 6 muscles in several directions from scratch. The model satisfies biological plausibility constraints, like neural implementation, transmission delays, local synaptic learning and continuous online learning. Using differential Hebbian plasticity the model can go from motor babbling to reaching arbitrary targets in less than 10 min of in silico time. Moreover, independently of the learning mechanism, properly configured feedback control has many emergent properties: neural populations in motor cortex show directional tuning and oscillatory dynamics, the spinal cord creates convergent force fields that add linearly, and movements are ataxic (as in a motor system without a cerebellum).</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>synaptic plasticity</kwd><kwd>motor control</kwd><kwd>spinal cord</kwd><kwd>synergy</kwd><kwd>motor cortex</kwd><kwd>directional tuning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><funding-statement>No external funding was received for this work.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Learning to reach in the sensorimotor loop, and the required neural dynamics, can be potentially explained by simple principles.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><sec id="s1-1"><title>The challenge</title><p>Neuroscience has made great progress in decoding how cortical regions perform specific brain functions like primate vision (<xref ref-type="bibr" rid="bib51">Kaas and Collins, 2003</xref>; <xref ref-type="bibr" rid="bib6">Ballard and Zhang, 2021</xref> and rodent navigation <xref ref-type="bibr" rid="bib23">Chersi and Burgess, 2015</xref>; <xref ref-type="bibr" rid="bib73">Moser et al., 2017</xref>). Conversely, the evolutionary much older motor control system still poses fundamental questions, despite a large body of experimental work. This is because, in mammals, in addition to areas in cortex like premotor and motor areas and to some degree sensory and parietal ones, many <italic>extracortical regions</italic> have important and unique functions: basal ganglia, thalamus, cerebellum, pons, brain stem nuclei like the red nucleus and spinal cord (<xref ref-type="bibr" rid="bib33">Eccles, 1981</xref>; <xref ref-type="bibr" rid="bib67">Loeb and Tsianos, 2015</xref>). These structures are highly interconnected by fast conducting axons and all show strong dynamic activity changes, related to the ongoing dynamics of the performed motor act. Clinical and lesion studies have confirmed the necessity of each of these regions for normal smooth motor control of arm reaching (<xref ref-type="bibr" rid="bib95">Shadmehr and Wise, 2005</xref>; <xref ref-type="bibr" rid="bib4">Arber and Costa, 2018</xref>).</p><p>Fully understanding motor control will thus entail understanding the simultaneous function and interplay of all brain regions involved. Little by little, new experimental techniques will allow us to monitor more neurons, in more regions, and for longer periods (<xref ref-type="bibr" rid="bib108">Tanaka et al., 2018</xref>, e.g.). But to make sense of these data computational models must step up to the task of integrating all those regions to create a functional neuronal machine.</p><p>Finally, relatively little is known about the neural basis of motor development <italic>in infants</italic> (<xref ref-type="bibr" rid="bib46">Hadders-Algra, 2018</xref>). Nevertheless, a full understanding of primate motor control will not only require explanation of how these brain regions complement and interact with each other but also how this can be learned during childhood.</p><p>With these challenges in mind we recently developed a motor control framework based on differential Hebbian learning (<xref ref-type="bibr" rid="bib116">Verduzco-Flores et al., 2022</xref>). A common theme in physiology is the control of <italic>homeostatic variables</italic> (e.g. blood glucose levels, body temperature, etc.) using negative feedback mechanisms (<xref ref-type="bibr" rid="bib120">Woods and Ramsay, 2007</xref>). From a broad perspective, our approach considers the musculoskeletal system as an extension of this homeostatic control system: movement aims to make the external environment conducive to the internal control of homeostatic variables (e.g. by finding food, or shelter from the sun).</p><p>Our working hypothesis (see <xref ref-type="bibr" rid="bib116">Verduzco-Flores et al., 2022</xref>) is that control of homeostatic variables requires a feedback controller that uses the muscles to produce a desired set of sensory perceptions. The motosensory loop, minimally containing motor cortex, spinal cord, and sensory cortex may implement that feedback controller. To test this hypothesis we implemented a relatively complete model of the sensorimotor loop (<xref ref-type="fig" rid="fig1">Figure 1</xref>), using the learning rules in <xref ref-type="bibr" rid="bib116">Verduzco-Flores et al., 2022</xref> to produce 2D arm reaching. The activity of the neural populations and the movements they produced showed remarkable consistency with the experimental observations that we describe next.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Main components of the model.</title><p>In the left panel, each box stands for a neural population, except for P, which represents the arm and the muscles. Arrows indicate static connections, open circles show <italic>input correlation</italic> synapses, and the two colored circles show possible locations of synapses with the learning rule in <xref ref-type="bibr" rid="bib116">Verduzco-Flores et al., 2022</xref>. In the <italic>spinal learning</italic> model the green circle connections are plastic, and the red circle connections are static. In the <italic>cortical learning</italic> model the red circle connections are plastic, whereas the green circle connections are static. In the <italic>static network</italic> all connections are static. A : afferent population. <inline-formula><mml:math id="inf1"><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mi mathvariant="bold">A</mml:mi></mml:msub></mml:math></inline-formula> : Somatosensory cortex, modulated by afferent input. <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> : somatosensory cortex, prescribed pattern. <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> : population signaling the difference between <inline-formula><mml:math id="inf4"><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mi mathvariant="bold">P</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> : primary motor cortex. <inline-formula><mml:math id="inf6"><mml:mi mathvariant="bold">C</mml:mi></mml:math></inline-formula> : spinal cord. Inside the <inline-formula><mml:math id="inf7"><mml:mi mathvariant="bold">C</mml:mi></mml:math></inline-formula> box the circles represent the excitatory (<inline-formula><mml:math id="inf8"><mml:mi mathvariant="bold">E</mml:mi></mml:math></inline-formula>) and inhibitory (<inline-formula><mml:math id="inf9"><mml:mi mathvariant="bold">I</mml:mi></mml:math></inline-formula>) interneurons, organized into six pairs. The interneurons in each pair innervate an alpha motoneuron (<inline-formula><mml:math id="inf10"><mml:mi>α</mml:mi></mml:math></inline-formula>), each of which stimulates one of the six muscles in the arm, numbered from 0 to 5. The trios consisting of <inline-formula><mml:math id="inf11"><mml:mi mathvariant="bold">E</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf12"><mml:mi mathvariant="bold">I</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf13"><mml:mi>α</mml:mi></mml:math></inline-formula> units are organized into agonists and antagonists, depending on whether their <inline-formula><mml:math id="inf14"><mml:mi>α</mml:mi></mml:math></inline-formula> motoneurons cause torques in similar or opposite directions. These relations are shown in the right-side panel.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig1-v2.tif"/></fig></sec><sec id="s1-2"><title>Relevant findings in motor control</title><p>Before describing our modeling approach, we summarize some of the relevant experimental data that will be important to understanding the results. We focus on three related issues: (1) the role of the spinal cord in movement, (2) the nature of representations in motor cortex, and (3) muscle synergies, and how the right pattern of muscle activity is produced.</p><p>For animals to move, spinal motoneurons must activate the skeletal muscles. In general, descending signals from the corticospinal tract do not activate the motoneurons directly, but instead provide input to a network of excitatory and inhibitory interneurons (<xref ref-type="bibr" rid="bib14">Bizzi et al., 2000</xref>; <xref ref-type="bibr" rid="bib61">Lemon, 2008</xref>; <xref ref-type="bibr" rid="bib3">Arber, 2012</xref>; <xref ref-type="bibr" rid="bib5">Asante and Martin, 2013</xref>; <xref ref-type="bibr" rid="bib2">Alstermark and Isa, 2012</xref>; <xref ref-type="bibr" rid="bib50">Jankowska, 2013</xref>; <xref ref-type="bibr" rid="bib117">Wang et al., 2017</xref>; <xref ref-type="bibr" rid="bib113">Ueno et al., 2018</xref>). Learning even simple behaviors involves long-term plasticity, both at the spinal cord (SC) circuit, and at higher regions of the motor hierarchy (<xref ref-type="bibr" rid="bib118">Wolpaw et al., 1983</xref>; <xref ref-type="bibr" rid="bib45">Grau, 2014</xref>; <xref ref-type="bibr" rid="bib69">Meyer-Lohmann et al., 1986</xref>; <xref ref-type="bibr" rid="bib119">Wolpaw, 1997</xref>; <xref ref-type="bibr" rid="bib79">Norton and Wolpaw, 2018</xref>). Despite its obvious importance, there are comparatively few attempts to elucidate the nature of the SC computations, and the role of synaptic plasticity.</p><p>The role ascribed to SC is closely related to the role assumed from motor cortex, particularly M1. One classic result is that M1 pyramidal neurons of macaques activate preferentially when the hand is moving in a particular direction. When the preferred directions of a large population of neurons are added as vectors, a population vector appears, which points close to the hand’s direction of motion (<xref ref-type="bibr" rid="bib38">Georgopoulos et al., 1982</xref>; <xref ref-type="bibr" rid="bib39">Georgopoulos et al., 1986</xref>). This launched the hypothesis that M1 represents kinematic, or other high-level parameters of the movement, which are transformed into movements in concert with the SC. This hypothesis mainly competes with the view that M1 represents muscle forces. Much research has been devoted to this issue (<xref ref-type="bibr" rid="bib52">Kakei et al., 1999</xref>; <xref ref-type="bibr" rid="bib111">Truccolo et al., 2008</xref>; <xref ref-type="bibr" rid="bib53">Kalaska, 2009</xref>; <xref ref-type="bibr" rid="bib40">Georgopoulos and Stefanis, 2007</xref>; <xref ref-type="bibr" rid="bib47">Harrison and Murphy, 2012</xref>; <xref ref-type="bibr" rid="bib107">Tanaka, 2016</xref>; <xref ref-type="bibr" rid="bib72">Morrow and Miller, 2003</xref>; <xref ref-type="bibr" rid="bib109">Todorov, 2000</xref>, e.g.).</p><p>Another important observation is that the preferred directions of motor neurons cluster around one main axis. As shown in <xref ref-type="bibr" rid="bib94">Scott et al., 2001</xref>, this suggests that M1 is mainly concerned with dynamical aspects of the movement, rather than representing its kinematics.</p><p>A related observation is that the preferred directions in M1 neurons experience random drifts that overlap learned changes (<xref ref-type="bibr" rid="bib90">Rokni et al., 2007</xref>; <xref ref-type="bibr" rid="bib81">Padoa-Schioppa et al., 2004</xref>). This leads to the hypothesis that M1 is a redundant network that is constantly using feedback error signals to capture the <italic>task-relevant dimensions</italic>, placing the configuration of synaptic weights in an <italic>optimal manifold</italic>.</p><p>A different perspective for studying motor cortex is to focus on how it can produce movements, rather than describing its activity (<xref ref-type="bibr" rid="bib97">Shenoy et al., 2013</xref>). One specific proposal is that motor cortex has a collection of pattern generators, and specific movements can be created by combining their activity (<xref ref-type="bibr" rid="bib97">Shenoy et al., 2013</xref>; <xref ref-type="bibr" rid="bib104">Sussillo et al., 2015</xref>). Experimental support for this hypothesis came through the surprising finding of rotational dynamics in motor cortex activity (<xref ref-type="bibr" rid="bib24">Churchland et al., 2012</xref>), suggesting that oscillators with different frequencies are used to produce desired patterns. This begs the question of how the animal chooses its desired patterns of motion.</p><p>Selecting a given pattern of muscle activation requires <italic>planning</italic>. Motor units are the final actuators in the motor system, but they number in the tens of thousands, so planning movements in this space is unfeasible. A low-dimensional representation of desired limb configurations (such as the location of the hand in Euclidean coordinates) is better. Movement generation likely involves a coordinate transformation, from the endpoint coordinates (e.g. hand coordinates) into actuator coordinates (e.g. muscle lengths), from which motor unit activation follows directly. Even using pure engineering methods, as for robot control, computing this coordinate transformation is very challenging. For example, this must overcome kinematic redundancies, as when many configurations of muscle lengths put the hand in the same location.</p><p>The issue of coordinate transformation is central for motor control (<xref ref-type="bibr" rid="bib95">Shadmehr and Wise, 2005</xref>; <xref ref-type="bibr" rid="bib93">Schöner et al., 2018</xref>; <xref ref-type="bibr" rid="bib114">Valero-Cuevas, 2009</xref>; <italic>motor primitives and muscle synergies</italic> are key concepts in this discussion). Representing things as combinations of elementary components is a fundamental theme in applied mathematics. For example, linear combinations of basis vectors can represent any vector, and linear combinations of wavelets can approximate any smooth function (<xref ref-type="bibr" rid="bib58">Keener, 1995</xref>). In motor control, this idea arises in the form of motor primitives. Motor primitives constitute a set of basic motions, such that that any movement can be decomposed into them (<xref ref-type="bibr" rid="bib43">Giszter, 2015</xref>; <xref ref-type="bibr" rid="bib76">Mussa–Ivaldi and Bizzi, 2000</xref>; <xref ref-type="bibr" rid="bib13">Bizzi et al., 1991</xref>). This is closely related to the concept of synergies. The term ‘synergy’ may mean several things (<xref ref-type="bibr" rid="bib59">Kelso, 2009</xref>; <xref ref-type="bibr" rid="bib18">Bruton and O’Dwyer, 2018</xref>), but in this paper, we use it to denote a pattern of muscle activity arising as a coherent unit. Synergies may be composed of motor primitives, or they may be the motor primitives themselves.</p><p>A promising candidate for motor primitives comes in the form of convergent force fields, which have been observed for the hindlimbs of frogs and rats (<xref ref-type="bibr" rid="bib42">Giszter et al., 1993</xref>; <xref ref-type="bibr" rid="bib75">Mussa-Ivaldi et al., 1994</xref>, or in the forelimbs of monkeys <xref ref-type="bibr" rid="bib121">Yaron et al., 2020</xref>). In experiments where the limb is held at a particular location, local stimulation of the spinal cord will cause a force to the limb’s endpoint. The collection of these force vectors for all of the limb endpoint’s positions forms a force field, and these force fields have two important characteristics: (1) they have a unique fixed point and (2) simultaneous stimulation of two spinal cord locations produces a force field which is the sum of the force fields from stimulating the two locations independently. It is argued that movement planning may be done in terms of force fields, since they can produce movements that are resistant to perturbations, and also permit a solution to the problem of coordinate transformation with redundant actuators (<xref ref-type="bibr" rid="bib76">Mussa–Ivaldi and Bizzi, 2000</xref>).</p><p>The neural origin of synergies, and whether they are used by the motor system is a matter of ongoing debate (<xref ref-type="bibr" rid="bib110">Tresch and Jarc, 2009</xref>; <xref ref-type="bibr" rid="bib30">de Rugy et al., 2013</xref>; <xref ref-type="bibr" rid="bib15">Bizzi and Cheung, 2013</xref>). To us, it is of interest that single spinal units found in the mouse (<xref ref-type="bibr" rid="bib62">Levine et al., 2014</xref> and monkey <xref ref-type="bibr" rid="bib106">Takei et al., 2017</xref>) spinal cord (sometimes called Motor Synergy Encoders, or MSEs) can reliably produce specific patterns of motoneuron activation.</p></sec><sec id="s1-3"><title>Model concepts</title><p>We believe that it is impossible to understand the complex dynamical system in biological motor control without the help of computational modeling. Therefore, we set out to build a minimal model that could eventually control an autonomous agent, while still satisfying biological plausibility constraints.</p><p>Design principles and biological-plausibility constraints for neural network modeling have been proposed before (<xref ref-type="bibr" rid="bib87">Pulvermüller et al., 2021</xref>; <xref ref-type="bibr" rid="bib80">O’Reilly, 1998</xref>; <xref ref-type="bibr" rid="bib88">Richards et al., 2019</xref>). Placing emphasis on the motor system, we compiled a set of characteristics that cover the majority of these constraints. Namely:</p><list list-type="bullet"><list-item><p>Spanning the whole sensorimotor loop.</p></list-item><list-item><p>Using only neural elements. Learning their connection strengths is part of the model.</p></list-item><list-item><p>Learning does not rely on a training dataset. It is instead done by synaptic elements using local information.</p></list-item><list-item><p>Learning arises from continuous-time interaction with a continuous-space environment.</p></list-item><list-item><p>There is a clear vision on how the model integrates with the rest of the brain in order to enact more general behavior.</p></list-item></list><p>Our aim is hierarchical control of homeostatic variables, with the spinal cord and motor cortex at the bottom of this hierarchy. At first glance, spinal plasticity poses a conundrum, because it changes the effect of corticospinal inputs. Cortex is playing a piano that keeps changing its tuning. A solution comes when we consider the corticospinal loop (e.g. the long-loop reflex) as a negative control system, where the spinal cord activates the effectors to reduce an error. The role of cortex is to produce perceptual variables that are controllable, and can eventually improve homeostatic regulation. In this regard, our model is a variation of Perceptual Control Theory (<xref ref-type="bibr" rid="bib85">Powers, 1973</xref>; <xref ref-type="bibr" rid="bib86">Powers, 2005</xref>), but if the desired value of the controller is viewed as a prediction, then this approach resembles active inference models (<xref ref-type="bibr" rid="bib1">Adams et al., 2013</xref>). Either way, the goal of the system is to reduce the difference between the desired and the perceived value of some variable.</p><p>If cortex creates representations for perceptual variables, the sensorimotor loop must be configured so those variables can be controlled. This happens when the error in those variables activates the muscles in a way that brings the perceived value closer to the desired value. In other words, we must find the input-output structure of the feedback controller implicit in the long-loop reflex. We have found that this important problem can be solved by the differential Hebbian learning rules introduced in <xref ref-type="bibr" rid="bib116">Verduzco-Flores et al., 2022</xref>. We favor the hypothesis that this learning takes place is in the connections from motor cortex to interneurons and brainstem. Nevertheless, we show that all our results are valid if learning happens in the connections from sensory to motor cortex.</p><p>In the Results section we will describe our model, its variations, and how it can learn to reach. Next we will show that many phenomena described above are present in this model. These phenomena emerge from having a properly configured neural feedback controller with a sufficient degree of biological realism. This means that even if the synaptic weights of the connections are set by hand and are static, the phenomena still emerge, as long as the system is configured to reduce errors. In short, we show that a wealth of phenomena in motor control can be explained simply by feedback control in the sensorimotor loop, and that this feedback control can be configured in a flexible manner by the learning rules presented in <xref ref-type="bibr" rid="bib116">Verduzco-Flores et al., 2022</xref>.</p></sec></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A neural architecture for motor control</title><p>The model in this paper contains the main elements of the long-loop reflex, applied to the control of a planar arm using six muscles. The left panel of <xref ref-type="fig" rid="fig1">Figure 1</xref> shows the architecture of the model, which contains 74 firing rate neurons organized in six populations. This architecture resembles a feedback controller that makes the activity in a neural population <inline-formula><mml:math id="inf15"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> approach the activity in a different population <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>The six firing-rate neurons (called <italic>units</italic> in this paper) in <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> represent a region of somatosensory cortex, and its inputs consist of the static gamma (II) afferents. In steady state, activity of the II afferents is monotonically related to muscle length (<xref ref-type="bibr" rid="bib71">Mileusnic et al., 2006</xref>), which in turn can be used to prescribe hand location. Other afferent signals are not provided to <inline-formula><mml:math id="inf18"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> in the interest of simplicity.</p><p><inline-formula><mml:math id="inf19"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> represents a different cortical layer of the same somatosensory region as <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula>, where a ‘desired’ or ‘predicted’ activity has been caused by brain regions not represented in the model. Each firing rate neuron in <inline-formula><mml:math id="inf21"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> has a corresponding unit in <inline-formula><mml:math id="inf22"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula>, and they represent the mean activity at different levels of the same microcolumn (<xref ref-type="bibr" rid="bib74">Mountcastle, 1997</xref>). <inline-formula><mml:math id="inf23"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a region (either in sensory or motor cortex) that conveys the difference between activities in <inline-formula><mml:math id="inf24"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula>, which is the error signal to be minimized by negative feedback control.</p><p>Population <inline-formula><mml:math id="inf26"><mml:mi>A</mml:mi></mml:math></inline-formula> represents sensory thalamus and dorsal parts of the spinal cord. It contains 18 units with logarithmic activation functions, each receiving an input from a muscle afferent. Each muscle provides proprioceptive feedback from models of the Ia, Ib, and II afferents. In rough terms, Ia afferents provide information about contraction velocity, and Ib afferents signal the amount of tension in the muscle and tendons.</p><p>Population <inline-formula><mml:math id="inf27"><mml:mi>M</mml:mi></mml:math></inline-formula> represents motor cortex. Ascending inputs to <inline-formula><mml:math id="inf28"><mml:mi>M</mml:mi></mml:math></inline-formula> arise from population <inline-formula><mml:math id="inf29"><mml:mi>A</mml:mi></mml:math></inline-formula>, and use a variation of the <italic>input correlation</italic> learning rule (<xref ref-type="bibr" rid="bib84">Porr and Wörgötter, 2006</xref>), where the <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> inputs act as a learning signal. The input correlation rule enhances the stability of the controller. More details are presented in Methods. The <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> inputs to <inline-formula><mml:math id="inf32"><mml:mi>M</mml:mi></mml:math></inline-formula> can either be static, or use a learning rule to be described below.</p><p>To represent positive and negative values, both <inline-formula><mml:math id="inf33"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf34"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> use a ‘dual representation’, where each error signal is represented by two units. Let <inline-formula><mml:math id="inf35"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> be the error associated with the <inline-formula><mml:math id="inf36"><mml:mi>i</mml:mi></mml:math></inline-formula>-th muscle. One of the two <inline-formula><mml:math id="inf37"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> units representing <italic>e</italic><sub><italic>i</italic></sub> is a monotonic function of <inline-formula><mml:math id="inf38"><mml:mrow><mml:mtext>max</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, whereas the other unit increases according to <inline-formula><mml:math id="inf39"><mml:mrow><mml:mtext>max</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. These opposing inputs, along with mutual inhibition between the two units creates dynamics where sensorimotor events cause both excitatory and inhibitory responses, which agrees with experimental observations (<xref ref-type="bibr" rid="bib96">Shafi et al., 2007</xref>; <xref ref-type="bibr" rid="bib101">Steinmetz et al., 2019</xref>; <xref ref-type="bibr" rid="bib77">Najafi et al., 2020</xref>), and allows transmitting ‘negative’ values using excitatory projections. Dual units in <inline-formula><mml:math id="inf40"><mml:mi>M</mml:mi></mml:math></inline-formula> receive the same inputs, but with the opposite sign.</p><p>Plasticity mechanisms within the sensorimotor loop should specify which muscles contract in order to reduce an error signaled by <inline-formula><mml:math id="inf41"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. We suggest that this plasticity could take place in the spinal cord and/or motor cortex. To show that our learning mechanisms work regardless of where the learning takes place, we created two main configurations of the model. In the first configuration, called the ‘spinal learning’ model, a ‘spinal’ network <inline-formula><mml:math id="inf42"><mml:mi>C</mml:mi></mml:math></inline-formula> transforms the <inline-formula><mml:math id="inf43"><mml:mi>M</mml:mi></mml:math></inline-formula> outputs into muscle stimulation. <inline-formula><mml:math id="inf44"><mml:mi>C</mml:mi></mml:math></inline-formula> learns to transform sensory errors into appropriate motor commands using a differential Hebbian learning rule (<xref ref-type="bibr" rid="bib116">Verduzco-Flores et al., 2022</xref>). In this configuration, the error input to each <inline-formula><mml:math id="inf45"><mml:mi>M</mml:mi></mml:math></inline-formula> unit comes from one of the <inline-formula><mml:math id="inf46"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> activities. A second configuration, called the ‘cortical learning’ model, has ‘all-to-all’ connections from <inline-formula><mml:math id="inf47"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf48"><mml:mi>M</mml:mi></mml:math></inline-formula> using the differential Hebbian rule, whereas the connections from <inline-formula><mml:math id="inf49"><mml:mi>M</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf50"><mml:mi>C</mml:mi></mml:math></inline-formula> use appropriately patterned static connections. Both configurations are basically the same model; the difference is that one configuration has our learning rule on the inputs to <inline-formula><mml:math id="inf51"><mml:mi>C</mml:mi></mml:math></inline-formula>, whereas the other has it on the inputs to <inline-formula><mml:math id="inf52"><mml:mi>M</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>While analyzing our model we reproduced several experimental phenomena (described below). Interestingly, these phenomena did not arise because of the learning rules. To make this explicit, we created a third configuration of our model, called the ‘static network’. This configuration does not change the weight of any synaptic connection during the simulation. The initial weights were hand-set to approximate the optimal solution everywhere (see Methods). We will show that all emergent phenomena in the paper are also present in the static network.</p><p>We explain the idea behind the differential Hebbian rule as applied in the connections from <inline-formula><mml:math id="inf53"><mml:mi>M</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> contains <inline-formula><mml:math id="inf55"><mml:mi>N</mml:mi></mml:math></inline-formula> interneurons, whose activity vector we denote as <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The input to each of these units is an <inline-formula><mml:math id="inf57"><mml:mi>M</mml:mi></mml:math></inline-formula> dimensional vector <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Each unit in <inline-formula><mml:math id="inf59"><mml:mi>C</mml:mi></mml:math></inline-formula> has an output <inline-formula><mml:math id="inf60"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a positive sigmoidal function. The inputs are assumed to be errors, and to reduce them we want <italic>e</italic><sub><italic>j</italic></sub> to activate <italic>c</italic><sub><italic>i</italic></sub> when <italic>c</italic><sub><italic>i</italic></sub> can reduce <italic>e</italic><sub><italic>j</italic></sub>. One way this could happen is when the weight <inline-formula><mml:math id="inf62"><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> from <italic>e</italic><sub><italic>j</italic></sub> to <italic>c</italic><sub><italic>i</italic></sub> is proportional to the negative of their sensitivity derivative:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Assuming a monotonic relation between the motor commands and the errors, relation 1 entails that errors will trigger an action to cancel them, with some caveats considered in <xref ref-type="bibr" rid="bib116">Verduzco-Flores et al., 2022</xref>. Synaptic weights akin to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> can be obtained using a learning rule that extracts correlations between the derivatives of <italic>c</italic><sub><italic>i</italic></sub> and <italic>e</italic><sub><italic>j</italic></sub> (see Methods). Using this rule, the commands coming from population <inline-formula><mml:math id="inf63"><mml:mi>C</mml:mi></mml:math></inline-formula> can eventually move the arm so that <inline-formula><mml:math id="inf64"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> activity resembles <inline-formula><mml:math id="inf65"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> activity.</p><p><inline-formula><mml:math id="inf66"><mml:mi>C</mml:mi></mml:math></inline-formula> is organized to capture the most basic motifs of spinal cord connectivity using a network where balance between excitation and inhibition is crucial (<xref ref-type="bibr" rid="bib11">Berg et al., 2007</xref>; <xref ref-type="bibr" rid="bib12">Berg et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Goulding et al., 2014</xref>). Each one of six <inline-formula><mml:math id="inf67"><mml:mi>α</mml:mi></mml:math></inline-formula> motoneurons stimulate one muscle, and is stimulated by one excitatory (<inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula>), and one inhibitory (<inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula>) interneuron. <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> stimulate one another, resembling the classic Wilson-Cowan model (<xref ref-type="bibr" rid="bib26">Cowan et al., 2016</xref>). The trios composed of <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf73"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> neurons compose a group that controls the activation of one muscle, with <inline-formula><mml:math id="inf74"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> receiving convergent inputs from <inline-formula><mml:math id="inf76"><mml:mi>M</mml:mi></mml:math></inline-formula>. This resembles the premotor network model in <xref ref-type="bibr" rid="bib82">Petersen et al., 2014</xref>. (<inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) trios are connected to other trios following the agonist-antagonist motif that is common in the spinal cord (<xref ref-type="bibr" rid="bib83">Pierrot-Deseilligny and Burke, 2005</xref>). This means that <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> units project to the <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> units of agonists, and to the <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> units of antagonists (<xref ref-type="fig" rid="fig1">Figure 1</xref>, right panel). When the agonist/antagonist relation is not strongly defined, muscles can be ‘partial’aASaS agonists/antagonists, or unrelated.</p><p>Connections from <inline-formula><mml:math id="inf81"><mml:mi>A</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf82"><mml:mi>C</mml:mi></mml:math></inline-formula> (the ‘short-loop reflex’) use the input correlation learning rule, analogous to the connections from <inline-formula><mml:math id="inf83"><mml:mi>A</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf84"><mml:mi>M</mml:mi></mml:math></inline-formula>.</p><p>Direct connections from <inline-formula><mml:math id="inf85"><mml:mi>M</mml:mi></mml:math></inline-formula> to alpha motoneurons are not necessary for the model to reach, but they were introduced in new versions because in higher primates these connections are present for distal joints (<xref ref-type="bibr" rid="bib61">Lemon, 2008</xref>). Considering that bidirectional plasticity has been observed in corticomotoneural connections (<xref ref-type="bibr" rid="bib78">Nishimura et al., 2013</xref>), we chose to endow them with the differential Hebbian rule of <xref ref-type="bibr" rid="bib116">Verduzco-Flores et al., 2022</xref>.</p><p>Because timing is essential to support the conclusions of this paper, every connection has a transmission delay, and all firing rate neurons are modeled with ordinary differential equations.</p><p>All the results in this paper apply to the three configurations described above (spinal learning, cortical learning, and static network). To emphasize the robustness and potential of the learning mechanisms, in the Appendix we introduce two variations of the spinal learning model (in the <italic>Variations of the spinal learning model</italic> section). All results in the paper also apply to those two variations. In one of the variations (the ‘synergistic’ network), each spinal motoneuron stimulates two muscles rather than one. In the second variation (the ‘mixed errors’ network), the inputs from <inline-formula><mml:math id="inf86"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf87"><mml:mi>M</mml:mi></mml:math></inline-formula> are not one-to-one, but instead come from a matrix that combines multiple error signals as the input to each <inline-formula><mml:math id="inf88"><mml:mi>M</mml:mi></mml:math></inline-formula> unit.</p><p>Since most results apply to all configurations, and since results could depend on the random initial weights, we report simulation results using three means and three standard deviations <inline-formula><mml:math id="inf89"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>±</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>±</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>±</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, with the understanding that these three value pairs correspond respectively to the spinal learning, motor learning, and static network models. The statistics come from 20 independent simulations with different initial conditions.</p><p>A reference section in the Appendix (the <italic>Comparison of the 5 configurations</italic> section) summarizes the basic traits of all different model configurations (including the two variations of the spinal learning model), and compiles all their numerical results.</p><p>For each configuration, a single simulation was used to produce all the representative plots in different sections of the paper.</p></sec><sec id="s2-2"><title>The model can reach by matching perceived and desired sensory activity</title><p>Reaches are performed by specifying an <inline-formula><mml:math id="inf90"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> pattern equal to the <inline-formula><mml:math id="inf91"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> activity when the hand is at the target. The acquisition of these <inline-formula><mml:math id="inf92"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> patterns is not in the scope of this paper (but see <xref ref-type="bibr" rid="bib116">Verduzco-Flores et al., 2022</xref>).</p><p>We created a set of random targets by sampling uniformly from the space of joint angles. Using this to set a different pattern in <inline-formula><mml:math id="inf93"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> every 40 s, we allowed the arm to move freely during 16 <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> target presentations. To encourage exploratory movements we used noise and two additional units described in the Methods.</p><p>All model configurations were capable of reaching. To decide if reaching was learned in a trial we took the average distance between the hand and the target (the <italic>average error</italic>) during the last four target presentations. Learning was achieved when this error was smaller than 10 cm.</p><p>The system learned to reach in 99 out of 100 trials (20 for each configuration). One simulation with the spinal learning model had an average error of 14 cm during the last 4 reaches of training. To assess the speed of learning we recorded the average number of target presentations required before the error became less than 10 cm for the first time. This average number of failed reaches before the first success was: <inline-formula><mml:math id="inf95"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1.8</mml:mn><mml:mo>±</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mn>1.2</mml:mn><mml:mo>±</mml:mo><mml:mn>.9</mml:mn></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p><xref ref-type="fig" rid="fig2">Figure 2A</xref> shows the error through 16 successive reaches (640 s of in silico time) in a typical case for the spinal learning model. A supplementary video (<xref ref-type="video" rid="app1video1">Appendix 1—Video 1</xref>) shows the arm’s movements during this simulation. Figures similar to <xref ref-type="fig" rid="fig2">Figure 2</xref> can be seen for all configurations as figure supplements (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Representative training phase of a simulation for the spinal learning model.</title><p>(<bold>A</bold>) Distance between the target and the hand through 640 s of simulation, corresponding to 16 reaches to different targets. The horizontal dotted line corresponds to 10 cm. The times when <inline-formula><mml:math id="inf96"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> changes are indicated with a vertical, dotted yellow line. Notice that the horizontal time axis is the same for all panels of this figure. The average error can be seen to decrease through the first two reaches. (<bold>B</bold>) Desired versus actual hand coordinates through the training phase. The straight lines denote the desired X (green) and Y (red) coordinates of the hand. The noisy orange and blue lines show the actual coordinates of the hand. (<bold>C</bold>) Activity of units 0 and 1 in <inline-formula><mml:math id="inf97"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf98"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula>. This panel shows that the desired values in the <inline-formula><mml:math id="inf99"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> units (straight dotted lines) start to become tracked by the perceived values. (<bold>D</bold>) Activity of <inline-formula><mml:math id="inf100"><mml:mi>M</mml:mi></mml:math></inline-formula> units 1, 2, and their duals. Notice that even when the error is close to zero the activity in the <inline-formula><mml:math id="inf101"><mml:mi>M</mml:mi></mml:math></inline-formula> units does not disappear. E: Activity of the <inline-formula><mml:math id="inf102"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:math></inline-formula> trio for muscle 0. The intrinsic noise in the units causes ongoing activity. Moreover, the inhibitory activity (orange line) dominates the excitatory activity (blue line).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Representative training phase of the simulation for the cortical learning configuration.</title><p>Panels are as in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Representative training phase of the simulation for the static network.</title><p>Panels are as in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Representative training phase of the simulation for the synergistic network.</title><p>Panels are as in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Representative training phase of the simulation for the mixed errors network.</title><p>Panels are as in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig2-figsupp4-v2.tif"/></fig></fig-group><p>In <xref ref-type="fig" rid="fig2">Figure 2A</xref>, the error increases each time a new target was presented (yellow vertical lines), but as learning continues it was consistently reduced below 10 cm.</p><p>Panel B also shows the effect of learning, as the hand’s Cartesian coordinates eventually track the target coordinates whenever they change. This is also reflected as the activity in <inline-formula><mml:math id="inf103"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> becoming similar to the activity in <inline-formula><mml:math id="inf104"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> (panel C).</p><p>Panels D and E of <xref ref-type="fig" rid="fig2">Figure 2</xref> show the activity of a few units in population <inline-formula><mml:math id="inf105"><mml:mi>M</mml:mi></mml:math></inline-formula> and population <inline-formula><mml:math id="inf106"><mml:mi>C</mml:mi></mml:math></inline-formula> during the 640 s of this training phase. During the first few reaches, <inline-formula><mml:math id="inf107"><mml:mi>M</mml:mi></mml:math></inline-formula> shows a large imbalance between the activity of units and their duals, reflecting larger errors. Eventually these activities balance out, leading to a more homogeneous activity that may increase when a new target appears. M1 activation patterns that produce no movement are called the <italic>null-space activity</italic> (<xref ref-type="bibr" rid="bib56">Kaufman et al., 2014</xref>). In our case, this includes patterns where <inline-formula><mml:math id="inf108"><mml:mi>M</mml:mi></mml:math></inline-formula> units have the same activity as their duals. This, together with the noise and oscillations intrinsic to the system cause the activity in <inline-formula><mml:math id="inf109"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf110"><mml:mi>C</mml:mi></mml:math></inline-formula> to never disappear.</p><p>In panel E, the noise in the <inline-formula><mml:math id="inf111"><mml:mi>C</mml:mi></mml:math></inline-formula> units becomes evident. It can also be seen that inhibition dominates excitation (due to <inline-formula><mml:math id="inf112"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf113"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> connections), which promotes stability in the circuit.</p><p>We tested whether any of the novel elements in the model were superfluous. To this end, we removed each of the elements individually and checked if the model could still learn to reach. In conclusion, removing individual elements generally deteriorated performance, but the factor that proved essential for all configurations with plasticity was the differential Hebbian learning in the connections from <inline-formula><mml:math id="inf114"><mml:mi>M</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf115"><mml:mi>C</mml:mi></mml:math></inline-formula> or from <inline-formula><mml:math id="inf116"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf117"><mml:mi>M</mml:mi></mml:math></inline-formula>. For details, see the the Appendix section titled <italic>The model fails when elements are removed</italic>.</p></sec><sec id="s2-3"><title>Center-out reaching 1: The reach trajectories present traits of cerebellar ataxia</title><p>In order to compare our model with experimental data, after the training phase we began a standard center-out reaching task. Switching to this task merely consisted of presenting the targets in a different way, but for the sake of smoother trajectories we removed the noise from the units in <inline-formula><mml:math id="inf118"><mml:mi>C</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf119"><mml:mi>M</mml:mi></mml:math></inline-formula>.</p><p><xref ref-type="fig" rid="fig3">Figure 3A</xref> shows the eight peripheral targets around a hand rest position. Before reaching a peripheral target, a reach to the center target was performed, so the whole experiment was a single continuous simulation controlled by the <inline-formula><mml:math id="inf120"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> pattern.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Center-out reaching.</title><p>(<bold>A</bold>) The arm at its resting position, with hand coordinates (0.3, 0.3) meters, where a center target is located. Eight peripheral targets (cyan dots) were located on a circle around the center target, with a 10 cm radius. The muscle lines, connecting the muscle insertion points, are shown in red. The shoulder is at the origin, whereas the elbow has coordinates (0.3, 0). Shoulder insertion points remain fixed. (<bold>B-F</bold>) Hand trajectories for all reaches in the three configurations. The trajectory’s color indicates the target. Dotted lines show individual reaches, whereas thick lines indicate the average of the 6 reaches.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Center-out reaching before the control loop gain was adjusted.</title><p>Panels are as in <xref ref-type="fig" rid="fig3">Figure 3</xref>, with the two extra configurations presented in Appendix 1. The numbers in in panels B-F represent a particular configuration: Config. 1=spinal learning, Config. 2=cortical learning, Config. 3=static network, Config 4=synergistic network, Config. 5=mixed errors network.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Training phase of the simulation for the static network before gain was reduced.</title><p>Panels are as in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Training phase of the simulation for the synergistic network before gain was reduced.</title><p>Panels are as in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig3-figsupp3-v2.tif"/></fig></fig-group><p>Peripheral targets were selected at random, each appearing six times. This produced 48 reaches (without counting reaches to the center), each one lasting 5 s. Panels B through D of <xref ref-type="fig" rid="fig3">Figure 3</xref> show the trajectories followed by the hand in the three configurations. During these 48 reaches the average distance between the hand and the target was <inline-formula><mml:math id="inf121"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>3.3</mml:mn><mml:mo>±</mml:mo><mml:mrow><mml:mn>.01</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mn>2.9</mml:mn><mml:mo>±</mml:mo><mml:mn>.001</mml:mn></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mn>2.9</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>.0003</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> centimeters.</p><p>Currently our system has neither cerebellum nor visual information. Lacking a ‘healthy’ model to make quantitative comparisons, we analyzed and compared them to data from cerebellar patients.</p><p>For the sake of stability and simplicity, our system is configured to perform slow movements. Fast and slow reaches are different in cerebellar patients (<xref ref-type="bibr" rid="bib8">Bastian et al., 1996</xref>). Slow reaches undershoot the target, follow longer hand paths, and show movement decomposition (joints appear to move one at a time). In <xref ref-type="fig" rid="fig3">Figure 3</xref> the trajectories begin close to the 135 degree axis, indicating a slower response at the elbow joint. With the parameters used, the spinal learning and cortical learning models tend to undershoot the target, whereas in the static network the hand can oscillate around the target.</p><p>The traits of the trajectories can be affected by many hyperparameters in the model, but the dominant factor seems to be the gain in the control loop. Our model involves delays, activation latencies, momentum, and interaction torques. Unsurprisingly, increasing the gain leads to oscillations along with faster reaching. On the other hand, low gain leads to slow, stable reaching that often undershoots the target. Since we do not have a cerebellum to overcome this trade off, the gain was the only hyperparameter that was manually adjusted for all configurations (See Methods). In particular, we adjusted the slope of the <inline-formula><mml:math id="inf122"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf123"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> units so the system was stable, but close to the onset of oscillations. Gain was allowed to be a bit larger in the static network so oscillations could be observed. The figure supplements for <xref ref-type="fig" rid="fig3">Figure 3</xref> shows more examples of configurations with higher gain (See <italic>Gain and oscillations</italic> in Appendix 1 for details).</p><p>The shape of the trajectory also depends on the target. Different reach directions cause different interaction forces, and encounter different levels of viscoelastic resistance from the muscles.</p><p><xref ref-type="fig" rid="fig4">Figure 4</xref> reveals that the approach to the target is initially fast, but gradually slows down. Healthy subjects usually present a bell-shaped velocity profile, with some symmetry between acceleration and deceleration. This symmetry is lost with cerebellar ataxia (<xref ref-type="bibr" rid="bib9">Becker et al., 1991</xref>; <xref ref-type="bibr" rid="bib41">Gilman et al., 1976</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Distance to target and reach velocity through time for the three configurations.</title><p>Thick lines show the average over 48 reaches (8 targets, 6 repetitions). Filled stripes show standard deviation. For the spinal and cortical learning configurations (left and center plots) the hand initially moves quickly to the target, but the direction is biased, so it needs to gradually correct the error from this initial fast approach; most of the variance in error and velocity appears when these corrections cause small-amplitude oscillations. In the case of the static network (right plots) oscillations are ongoing, leading to a large variance in velocity.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig4-v2.tif"/></fig><p>We are not aware of center-out reaching studies for cerebellar patients in the dark, but (<xref ref-type="bibr" rid="bib29">Day et al., 1998</xref>) does examine reaching in these conditions. Summarizing its findings:</p><list list-type="order"><list-item><p>Movements were slow.</p></list-item><list-item><p>The endpoints had small variation, but they had constant errors.</p></list-item><list-item><p>Longer, more circuitous trajectories, with most changes in direction during the last quarter.</p></list-item><list-item><p>Trajectories to the same target showed variations.</p></list-item></list><p>From <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref> we can observe constant endpoint errors when the gain is low, in the spinal and cortical learning models. Circuitous trajectories with a pronounced turn around the end of the third quarter are also observed. Individual trajectories can present variations. A higher gain, as in the static network on the right plots, can increase these variations, as illustrated in the figure supplements for Appendix 1.</p></sec><sec id="s2-4"><title>Center-out reaching 2: Directional tuning and preferred directions</title><p>To find whether directional tuning could arise during learning, we analyzed the <inline-formula><mml:math id="inf124"><mml:mi>M</mml:mi></mml:math></inline-formula> population activity for the 48 radial reaches described in the previous subsection.</p><p>For each of the 12 units in <inline-formula><mml:math id="inf125"><mml:mi>M</mml:mi></mml:math></inline-formula>, <xref ref-type="fig" rid="fig5">Figure 5A</xref> shows the mean firing rate of the unit when reaching each of the 8 targets. The red arrows show the Preferred Direction (PD) vectors that arise from these distributions of firing rates. For the sake of exposition, <xref ref-type="fig" rid="fig5">Figure 5</xref> shows data for the simpler case of one-to-one connectivity between <inline-formula><mml:math id="inf126"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf127"><mml:mi>M</mml:mi></mml:math></inline-formula> in the spinal learning model, but these results generalize to the case when each <inline-formula><mml:math id="inf128"><mml:mi>M</mml:mi></mml:math></inline-formula> unit receives a linear combination of the <inline-formula><mml:math id="inf129"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> activities (the ‘mixed errors’ variation presented in the <italic>Variations of the spinal learning model</italic> section of the Appendix.)</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Directional tuning of the units in <inline-formula><mml:math id="inf130"><mml:mi>M</mml:mi></mml:math></inline-formula> for a simulation with the spinal learning model.</title><p>(<bold>A</bold>) Average firing rate per target, and preferred direction (see Methods) for each of the 12 units in <inline-formula><mml:math id="inf131"><mml:mi>M</mml:mi></mml:math></inline-formula>. Each polar plot corresponds to a single unit, and each of the 8 purple wedges corresponds to one of the 8 targets. The length of a wedge indicates the mean firing rate when the hand was reaching the corresponding target. The red arrow indicates the direction and relative magnitude of the PD vector. The black arrow shows the predicted PD vector, in this case just the corresponding arrows from panel B. (<bold>B</bold>) For each muscle and target, a wedge shows the muscle’s length at rest position minus the length at the target, divided by the rest position length. The red arrow comes from the sum of the wedges taken as vectors, and represents the muscle’s direction of maximum contraction. Plots corresponding to antagonist muscles are connected by red lines. (<bold>C</bold>) Average activity of the 6 <inline-formula><mml:math id="inf132"><mml:mi>A</mml:mi></mml:math></inline-formula> units indicating muscle tension. The black arrows come from the sum of wedges taken as vectors, showing the relation between muscle tension and preferred direction.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig5-v2.tif"/></fig><p>We found that <inline-formula><mml:math id="inf133"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>11.8</mml:mn><mml:mo>±</mml:mo><mml:mrow><mml:mn>.4</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mn>12</mml:mn><mml:mo>±</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mn>12</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> units were significantly tuned to reach direction (<inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, bootstrap test), with PD vectors of various lengths. The direction of the PD vectors is not mysterious. Each <inline-formula><mml:math id="inf135"><mml:mi>M</mml:mi></mml:math></inline-formula> unit controls the length error of one muscle. <xref ref-type="fig" rid="fig5">Figure 5B</xref> shows that the required contraction length depends on both the target and the muscle. The PD vectors of units 0–5 point to the targets that require the most contraction of their muscle. Units 6–11 are the duals of 0–5, and their PD is in the opposite direction. <xref ref-type="fig" rid="fig5">Figure 5C</xref> shows that the PD may also be inferred from the muscle activity, reflected as average tension.</p><p>In the case when each <inline-formula><mml:math id="inf136"><mml:mi>M</mml:mi></mml:math></inline-formula> unit receives a linear combination of <inline-formula><mml:math id="inf137"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> errors, its PD can be predicted using a linear combination of the ‘directions of maximum contraction’ shown in <xref ref-type="fig" rid="fig5">Figure 5B</xref>, using the same weights as the <inline-formula><mml:math id="inf138"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> inputs. When accounting for the length of the PD vectors, this can predict the PD angle with a coefficient of determination <inline-formula><mml:math id="inf139"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>≈</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>.74</mml:mn><mml:mo>±</mml:mo><mml:mrow><mml:mn>.18</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mn>.88</mml:mn><mml:mo>±</mml:mo><mml:mn>.14</mml:mn></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mn>.86</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>.01</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>As mentioned in the Introduction, the PDs of motor cortex neurons tend to align in particular directions <xref ref-type="bibr" rid="bib94">Scott et al., 2001</xref>. This is almost trivially true for this model, since the PD vectors are mainly produced by linear combinations of the vectors in <xref ref-type="fig" rid="fig5">Figure 5B</xref>.</p><p><xref ref-type="fig" rid="fig6">Figure 6</xref> shows the PD for all the <inline-formula><mml:math id="inf140"><mml:mi>M</mml:mi></mml:math></inline-formula> units in a representative simulation for each of the configurations. In every simulation, the PD distribution showed significant bimodality (<inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). The main axis of the PD distribution (see Methods) was <inline-formula><mml:math id="inf142"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>59</mml:mn><mml:mo>±</mml:mo><mml:mrow><mml:mn>7</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mn>52</mml:mn><mml:mo>±</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mn>54</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>.5</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> degrees.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Preferred direction vectors for the 12 <inline-formula><mml:math id="inf143"><mml:mi>M</mml:mi></mml:math></inline-formula> units.</title><p>In all three plots the arrows denote the direction and magnitude of the preferred direction (PD) for an individual unit. The gray dotted lines shows the main axis of the distribution. The red dotted lines are a 45 degree rotation of the gray line, for comparison with <xref ref-type="bibr" rid="bib94">Scott et al., 2001</xref>. It can be seen that all configurations display a strong bimodality, especially when considering the units with a larger PD vector. The axis where the PD vectors tend to aggregate is in roughly the same position for the three configurations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig6-v2.tif"/></fig><p>To compare with (<xref ref-type="bibr" rid="bib94">Scott et al., 2001</xref>) we rotate this line 45 degrees so the targets are in the same position relative to the shoulder (e.g. <xref ref-type="bibr" rid="bib64">Lillicrap and Scott, 2013</xref> <xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="bibr" rid="bib60">Kurtzer et al., 2006</xref> <xref ref-type="fig" rid="fig1">Figure 1</xref>). This places the average main axes above in a range between 99 and 104 degrees, comparable to the 117 degrees in <xref ref-type="bibr" rid="bib94">Scott et al., 2001</xref>.</p><p>The study in <xref ref-type="bibr" rid="bib64">Lillicrap and Scott, 2013</xref> suggested that a rudimentary spinal cord feedback system should be used to understand <italic>why</italic> the PD distribution arises. Our model is the first to achieve this.</p><p>The PD vectors are not stationary, but experience random fluctuations that become more pronounced in new environments (<xref ref-type="bibr" rid="bib90">Rokni et al., 2007</xref>; <xref ref-type="bibr" rid="bib81">Padoa-Schioppa et al., 2004</xref>). The brain is constantly remodeling itself, without losing the ability to perform its critical operations (<xref ref-type="bibr" rid="bib21">Chambers and Rumpel, 2017</xref>). Our model is continuously learning, so we tested the change in the PDs by setting 40 additional center-out reaches (no intrinsic noise) after the previous experiment, once for each configuration.</p><p>To encourage changes we set 10 different targets instead of 8. After a single trial for each configuration the change in angle for the 12 PD vectors had means and standard deviations of <inline-formula><mml:math id="inf144"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>3.3</mml:mn><mml:mo>±</mml:mo><mml:mrow><mml:mn>2.4</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mn>4.9</mml:mn><mml:mo>±</mml:mo><mml:mn>2.1</mml:mn></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mn>.3</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>.2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> degrees. Larger changes (around 7 degrees) could be observed in the ‘mixed errors’ variation of the model, presented in the Appendix (<italic>Variations of the spinal learning model</italic> section). We also measured the change in the preferred directions of the muscles, obtained as in <xref ref-type="fig" rid="fig5">Figure 5C</xref>. This yielded differences and standard deviations <inline-formula><mml:math id="inf145"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>3.8</mml:mn><mml:mo>±</mml:mo><mml:mrow><mml:mn>2.1</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mn>6.4</mml:mn><mml:mo>±</mml:mo><mml:mn>2.9</mml:mn></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mn>.2</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>.2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> degrees.</p><p>The average distance between hand and target during the 40 reaches was <inline-formula><mml:math id="inf146"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mn>3.6</mml:mn><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mn>2.9</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> cm, showing that the hand was still moving towards the targets, although with different errors due to their new locations.</p></sec><sec id="s2-5"><title>Center-out reaching 3: Rotational dynamics</title><p>Using a dynamical systems perspective, (<xref ref-type="bibr" rid="bib97">Shenoy et al., 2013</xref>) considers that the muscle activity <inline-formula><mml:math id="inf147"><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (a vector function of time) arises from the cortical activity vector <inline-formula><mml:math id="inf148"><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> after it is transformed by the downstream circuitry:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>It is considered that the mapping <inline-formula><mml:math id="inf149"><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> may consist of sophisticated controllers, but for the sake of simplicity this mapping is considered static, omitting spinal cord plasticity. The cortical activity arises from a dynamical system:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf150"><mml:mrow><mml:mi mathvariant="bold">u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents inputs to motor cortex from other areas, and <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a function that describes how the state of the system evolves.</p><p>A difficulty associated with <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> is explaining how <inline-formula><mml:math id="inf152"><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> generates a desired muscle pattern <inline-formula><mml:math id="inf153"><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> when the function <inline-formula><mml:math id="inf154"><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the dynamics of a recurrent neural network. One possibility is that M1 has intrinsic oscillators of various frequencies, and they combine their outputs to shape the desired pattern. This prompted the search for oscillatory activity in M1 while macaques performed center-out reaching motions. A brief oscillation (in the order of 200ms, or 5 Hz) was indeed found in the population activity (<xref ref-type="bibr" rid="bib24">Churchland et al., 2012</xref>, and the model in <xref ref-type="bibr" rid="bib104">Sussillo et al., 2015</xref>) was able to reproduce this result, although this was done in the open-loop version of <xref ref-type="disp-formula" rid="equ2 equ3">Equations 2 and 3</xref>, where <inline-formula><mml:math id="inf155"><mml:mrow><mml:mi mathvariant="bold">u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> contains no afferent feedback (this is further commented in the Supplemental Discussion).</p><p>Recently it was shown that the oscillations in motor cortex can arise when considering the full sensorimotor loop, without the need of recurrent connections in motor cortex (<xref ref-type="bibr" rid="bib55">Kalidindi et al., 2021</xref>). A natural question is whether our model can also reproduce the oscillations in <xref ref-type="bibr" rid="bib24">Churchland et al., 2012</xref> without requiring M1 oscillators or recurrent connections.</p><p>The analysis in <xref ref-type="bibr" rid="bib24">Churchland et al., 2012</xref> is centered around measuring the amount of rotation in the M1 population activity. The first step is to project the M1 activity vectors onto their first six principal components. These six components are then rotated so the evolution of the activity maximally resembles a pure rotation. These rotated components are called the ‘jPCA vectors’. The amount of variance in the M1 activity explained by the first two jPCA vectors is a measure of rotation. The Methods section provides more details of this procedure.</p><p>Considering that we have a low-dimensional, non-spiking, slow-reaching model, we can only expect to qualitatively replicate the essential result in <xref ref-type="bibr" rid="bib24">Churchland et al., 2012</xref>, which is most of the variance being contained in the first jPCA plane.</p><p>We replicated the jPCA analysis, with adjustments to account for the smaller number of neurons, the slower dynamics, and the fact that there is no delay period before the reach (See Methods). The result can be observed in <xref ref-type="fig" rid="fig7">Figure 7</xref>, where 8 trajectories are seen in the plots. Each trajectory is the average activity of the 12 <inline-formula><mml:math id="inf156"><mml:mi>M</mml:mi></mml:math></inline-formula> units when reaching to one of the 8 targets, projected onto the jPCA plane. The signature of a rotational structure in these plots is that most trajectories circulate in a counterclockwise direction. Quantitatively, the first jPCA plane (out of six) captures <inline-formula><mml:math id="inf157"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>.42</mml:mn><mml:mo>±</mml:mo><mml:mrow><mml:mn>.04</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mn>.42</mml:mn><mml:mo>±</mml:mo><mml:mn>.04</mml:mn></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mn>.46</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>.03</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of the variance.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Rotational dynamics in the M population in a representative simulation for all configurations.</title><p>Each plot shows the first two jPCA components during 0.25 s, for each of the 8 conditions/targets. Traces are colored according to the magnitude of their initial <inline-formula><mml:math id="inf158"><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> component, from smallest (green) to largest (red).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig7-v2.tif"/></fig><p>With this analysis we show that our model does not require intrinsic oscillations in motor cortex to produce rotational dynamics, in agreement with (<xref ref-type="bibr" rid="bib55">Kalidindi et al., 2021</xref> and <xref ref-type="bibr" rid="bib31">DeWolf et al., 2016</xref>).</p></sec><sec id="s2-6"><title>The effect of changing the mass</title><p>Physical properties of the arm can change, not only as the arm grows, but also when tools or new environments come into play. As a quick test of whether the properties in this paper are robust to moderate changes, we changed the mass of the arm and forearm from 1 to 0.8 kg and ran one simulation for each of the five configurations.</p><p>With a lighter arm the average errors during center-out reaching were <inline-formula><mml:math id="inf159"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2.5</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mn>3.2</mml:mn><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> cm. The hand trajectories with a reduced mass can be seen in the top 3 plots of <xref ref-type="fig" rid="fig8">Figure 8</xref>. We can observe that the spinal learning model slightly reduced its mean error, whereas the cortical learning model increased it. This can be understood by noticing that a reduction in mass is akin to an increase in gain. The spinal learning model with its original gain was below the threshold of oscillations at the endpoint, and a slight mass decrease did not change this. The cortical learning model with the original gain was already oscillating slightly, and an increase in gain increased the oscillations.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Hand trajectories with low mass (0.8 kg, top 3 plots) and high mass (1.2 kg, bottom 3 plots) for the 3 configurations.</title><p>Plots are as in <xref ref-type="fig" rid="fig3">Figure 3</xref>. The spinal learning model and the static network show qualitatively similar trajectories compared to those in <xref ref-type="fig" rid="fig3">Figure 3</xref>. In contrast, the cortical learning model began to display considerable endpoint oscillations for several targets after its mass was reduced. These oscillations persist after the mass has been increased.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig8-v2.tif"/></fig><p>In the same simulation, after the center-out reaching was completed, we once more modified the mass of the arm and forearm, from 0.8 to 1.2 kg, after which we began the center-out reaching again. This time the center-out reaching errors were <inline-formula><mml:math id="inf160"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2.4</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mn>3.3</mml:mn><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mn>2.9</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> cm. The hand trajectories for this high mass condition are in the bottom 3 plots in <xref ref-type="fig" rid="fig8">Figure 8</xref>. It can be seen that the spinal learning and cortical learning models retained their respectively improved and decreased performance, whereas the static network performed roughly the same for all mass conditions. A tentative explanation is that with reduced mass the synaptic learning rules tried to compensate for faster movements with weights that effectively increased the gain in the loop. After the mass was increased these weights did not immediately revert, leading to similar trajectories after the increase in mass.</p><p>The results of the paper still held after our mass manipulations. For all configurations, PD vectors could be predicted with a coefficient of determination between.74 and.92; All units in <inline-formula><mml:math id="inf161"><mml:mi>M</mml:mi></mml:math></inline-formula> were significantly tuned to direction; the main axis of the PD distribution ranged between 56 and 61 degrees, and the first jPCA plane captured between 33% and 58% of the variance.</p></sec><sec id="s2-7"><title>Spinal stimulation produces convergent direction fields</title><p>Due to the viscoelastic properties of the muscles, the mechanical system without active muscle contraction will have a fixed point with lowest potential energy at the arm’s rest position. Limited amounts of muscle contraction shift the position of that fixed point. This led us to question whether this could produce convergent force fields, which as discussed before are candidate motor primitives, and have been found experimentally.</p><p>To simulate local stimulation of an isolated spinal cord we removed all neuronal populations except for those in <inline-formula><mml:math id="inf162"><mml:mi>C</mml:mi></mml:math></inline-formula>, and applied inputs to the individual pairs of <inline-formula><mml:math id="inf163"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> units projecting to the same motoneuron. Doing this for different starting positions of the hand, and recording its initial direction of motion, produces a <italic>direction field</italic>. A direction field maps each initial hand location to a vector pointing in the average direction of the force that initially moves the hand.</p><p>The first two panels of <xref ref-type="fig" rid="fig9">Figure 9</xref> show the result of stimulating individual E-I pairs in <inline-formula><mml:math id="inf164"><mml:mi>C</mml:mi></mml:math></inline-formula>, which will indeed produce direction fields with different fixed points.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Two sample direction fields and their linear addition for circuit <italic>C</italic><sub>1</sub>.</title><p>(<bold>A</bold>) Direction Field (DF) from stimulation of the interneurons for muscle 0 (biarticular biceps). The approximate location of the fixed point is shown with a blue dot. (<bold>B</bold>) DF from stimulation of muscle 3 (biarticular triceps) interneurons. A red dot shows the fixed point. (<bold>C</bold>) Panels A and B overlapped. (<bold>D</bold>) In green, the DF from stimulating the interneurons for muscles 0 and 3 together. In purple, the sum of the DFs from panels A and B. Dots show the fixed points. The average angle between the green and purple vectors is 4 degrees.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-fig9-v2.tif"/></fig><p>We found that these direction fields add approximately linearly (<xref ref-type="fig" rid="fig9">Figure 9D</xref>). More precisely, let <inline-formula><mml:math id="inf165"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be the direction field from stimulating spinal locations <inline-formula><mml:math id="inf166"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf167"><mml:mi>b</mml:mi></mml:math></inline-formula> simultaneously, and <inline-formula><mml:math id="inf168"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be the angle of <inline-formula><mml:math id="inf169"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at hand coordinates <inline-formula><mml:math id="inf170"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Using similar definitions for <inline-formula><mml:math id="inf171"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, we say the direction fields add linearly if <inline-formula><mml:math id="inf172"><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mo>∀</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>We define the mean angle difference between <inline-formula><mml:math id="inf173"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf174"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> as<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf175"><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> is the number of <inline-formula><mml:math id="inf176"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> sample points. We found that when averaged over the 15 (<italic>C</italic><sub>1</sub>) or 144 (<italic>C</italic><sub>2</sub>) possible <inline-formula><mml:math id="inf177"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> pairs, the mean of <inline-formula><mml:math id="inf178"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was 13.5 degrees.</p><p>Randomly choosing two possibly different pairs <inline-formula><mml:math id="inf179"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf180"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for the stimulation locations leads to a mean angle difference of 37.6 degrees between the fields <inline-formula><mml:math id="inf181"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf182"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. A bootstrap test showed that these angles are significantly larger (<inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) than in the previous case where <inline-formula><mml:math id="inf184"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The resting field is defined as the direction field when no units are stimulated. Removing the resting field from <inline-formula><mml:math id="inf185"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf186"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> does not alter these results.</p><p>Recent macaque forelimb experiments (<xref ref-type="bibr" rid="bib121">Yaron et al., 2020</xref>) show that the magnitude of the vectors in the <inline-formula><mml:math id="inf187"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> fields is larger than expected from <inline-formula><mml:math id="inf188"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (supralinear summation). We found no evidence for this effect, suggesting that it depends on mechanisms beyond those present in our model.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Summary of findings and predictions</title><p>We have presented a model of the long loop reflex with a main assumption: negative feedback configured with two differential Hebbian learning rules. One novel rule sets the loop’s input-output structure, and the other rule (input correlation) promotes stability. We showed that this model can make arm reaches by trying to perceive a given afferent pattern.</p><p>Our study made two main points:</p><list list-type="order"><list-item><p>Many experimental phenomena emerge from a feedback controller with minimally-complete musculoskeletal and neural models (emphasis is placed on the balance between excitation and inhibition).</p></list-item><list-item><p>Even if the feedback controller has multiple inputs and outputs, its input-output structure can be flexibly configured by a differential Hebbian learning rule, as long as errors are monotonic.</p></list-item></list><p>The first main point above was made using a feedback control network with no learning (called the static network in the Results). We showed that in this static network: (1) reaching trajectories are similar to models of cerebellar ataxia, (2) motor cortex units are tuned to preferred directions, (3) those preferred directions follow a bimodal distribution, (4) motor cortex units present rotational dynamics, (5) reaching is still possible when mass is altered, and (6) spinal stimulation produces convergent direction fields.</p><p>The second main point was made using two separate models, both using the same differential Hebbian learning rules, but applied at different locations. The spinal learning model presents the hypothesis that the spinal cord learns to adaptively configure the input-output structure of the feedback controller. The cortical learning model posits that configuring this structure could instead be a function of motor cortex; this would not disrupt our central claims. These two models should not be considered as incompatible hypotheses. Different elements performing overlapping functions are common in biological systems (<xref ref-type="bibr" rid="bib34">Edelman and Gally, 2001</xref>).</p><p>Two variations of the spinal learning model in the Appendix show that this learning mechanism is quite flexible, opening the doors for certain types of synergies, and for more complex errors (that still maintian the constraint of monotonicity).</p><p>We list some properties of the model, and possible implications:</p><list list-type="bullet"><list-item><p>Basic arm reaching happens through negative feedback, trying to perceive a target value set in cortex. Learning the input-output structure of the feedback controller may require spinal cord plasticity.</p><list list-type="bullet"><list-item><p>Cerebellar patients should not be able to adapt to tasks that require fast reactions, as negative feedback alone cannot compensate for delays in the system (<xref ref-type="bibr" rid="bib91">Sanguineti et al., 2003</xref>). On the other hand, they should be able to learn tasks that require remapping afferent inputs to movements. One example is <xref ref-type="bibr" rid="bib89">Richter et al., 2004</xref>, where cerebellar patients learned to move in a novel dynamic environment, but their movements were less precise than those of controls.</p></list-item></list></list-item><list-item><p>The shape of reaches is dominated by mechanical and viscoelastic properties of the arm and muscles.</p><list list-type="bullet"><list-item><p>Unfamiliar viscous forces as in <xref ref-type="bibr" rid="bib89">Richter et al., 2004</xref> should predictably alter the trajectory (<xref ref-type="fig" rid="fig3">Figure 3</xref>) for cerebellar patients, who should not be able to adapt unless they move slowly and are explicitly compensating.</p></list-item></list></list-item><list-item><p>Preferred Directions (PDs) in motor cortex happen because muscles need to contract more when reaching in certain directions.</p><list list-type="bullet"><list-item><p>The PD distribution should align with the directions where the muscles need to contract to reduce the error. These directions depend on which error is encoding. If the error is not related to reaching (e.g. related to haptic feedback), a different PD distribution may arise after overtraining.</p></list-item><list-item><p>Drift in the PD vectors comes from the ongoing adaptation, and it should not disrupt performance.</p></list-item></list></list-item><list-item><p>The oscillations intrinsic to delayed feedback control after the onset of a target are sufficient to explain the quasi-oscillations observed in motor cortex (<xref ref-type="bibr" rid="bib24">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib55">Kalidindi et al., 2021</xref>).</p></list-item><list-item><p>Convergent force fields happen naturally in musculoskeletal systems when there is balance in the stimulation between agonists and antagonists. Linear addition of force fields is a result of forces/torques adding linearly.</p></list-item></list><p>Since our relatively simple model reproduces these phenomena, we believe it constitutes a good null hypothesis for them. But beyond explaining experimental observations, this model makes inroads into the hard problem of how the central nervous system (CNS) can generate effective control signals, recently dubbed the ‘supraspinal pattern formation’ problem (<xref ref-type="bibr" rid="bib16">Bizzi and Ajemian, 2020</xref>). From our perspective, the CNS does not need to generate precise activation patterns for muscles and synergies; it needs to figure out which perceptions need to change. It is subcortical structures that learn the movement details. The key to make such a model work is the differential Hebbian learning framework in <xref ref-type="bibr" rid="bib116">Verduzco-Flores et al., 2022</xref>, which handles the final credit assignment problem.</p><p>We chose not to include a model of the cerebellum at this stage. Our model reflects the brain structure of an infant baby who can make clumsy reaching movements. At birth the cerebellum is incomplete and presumably not functional. It requires structured input from spinal cord and cortex to establish correct synaptic connections during postnatal development and will contribute to smooth reaching movements at a later age.</p><p>Encompassing function, learning, and experimental phenomena in a single simple model is a promising start towards a more integrated computational neuroscience. We consider that such models have the potential to steer complex large-scale models so they can also achieve learning and functionality from scratch.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><p>Simulations were run in the Draculab simulator (<xref ref-type="bibr" rid="bib115">Verduzco-Flores and De Schutter, 2019</xref>). All the parameters from the equations in this paper are presented in the Appendix. Parameters not shown can be obtained from Python dictionaries in the source code. This code can be downloaded from: <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/sergio.verduzco/public_materials/-/tree/master/adaptive_plasticity">https://gitlab.com/sergio.verduzco/public_materials/-/tree/master/adaptive_plasticity</ext-link>.</p><sec id="s4-1"><title>Unit equations</title><p>With the exception of the <inline-formula><mml:math id="inf189"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf190"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> populations, the activity <italic>u</italic><sub><italic>i</italic></sub> of any unit in <xref ref-type="fig" rid="fig1">Figure 1</xref> has dynamics:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf191"><mml:mi>τ</mml:mi></mml:math></inline-formula> is a time constant, <inline-formula><mml:math id="inf192"><mml:mi>β</mml:mi></mml:math></inline-formula> is the slope of the sigmoidal function, <inline-formula><mml:math id="inf193"><mml:mi>η</mml:mi></mml:math></inline-formula> is its threshold, and <inline-formula><mml:math id="inf194"><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the sum of delayed inputs times their synaptic weights.</p><p>Units in the <inline-formula><mml:math id="inf195"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> populations (in the spinal learning model) or in <inline-formula><mml:math id="inf196"><mml:mi>M</mml:mi></mml:math></inline-formula> (in the cortical learning model) had an additional noise term, which turned <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> into this Langevin equation:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>d</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ς</mml:mi><mml:mi>d</mml:mi><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf197"><mml:mrow><mml:mi>W</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a Wiener process with unit variance, and <inline-formula><mml:math id="inf198"><mml:mi>ς</mml:mi></mml:math></inline-formula> is a parameter to control the noise amplitude. This equation was solved using the Euler-Maruyama method. All other unit equations were integrated using the forward Euler method. The equations for the plant and the muscles were integrated with SciPy’s (<ext-link ext-link-type="uri" xlink:href="https://scipy.org/">https://scipy.org/</ext-link>) explicit Runge-Kutta 5(4) method.</p><p>Units in the <inline-formula><mml:math id="inf199"><mml:mi>A</mml:mi></mml:math></inline-formula> population use a rectified logarithm activation function, leading to these dynamics for their activity:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf200"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> is a time constant, <inline-formula><mml:math id="inf201"><mml:mi>I</mml:mi></mml:math></inline-formula> is the scaled sum of inputs, <inline-formula><mml:math id="inf202"><mml:mi>T</mml:mi></mml:math></inline-formula> is a threshold, and <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the &quot;positive part&quot; function.</p></sec><sec id="s4-2"><title>Learning rules</title><p>The learning rule for the connections from <inline-formula><mml:math id="inf204"><mml:mi>M</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf205"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> units in the spinal learning model was first described in <xref ref-type="bibr" rid="bib116">Verduzco-Flores et al., 2022</xref>. It has an equation:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="160%" minsize="160%">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo>¨</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo>¨</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow><mml:mo maxsize="160%" minsize="160%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="160%" minsize="160%">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow><mml:mo maxsize="160%" minsize="160%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In this equation, <inline-formula><mml:math id="inf206"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the activity of the <inline-formula><mml:math id="inf207"><mml:mi>j</mml:mi></mml:math></inline-formula>-th unit in <inline-formula><mml:math id="inf208"><mml:mi>M</mml:mi></mml:math></inline-formula> at time <inline-formula><mml:math id="inf209"><mml:mi>t</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf210"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo>¨</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is its second derivative. Angle brackets denote averages, so that <inline-formula><mml:math id="inf211"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo>¨</mml:mo></mml:mover><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>≡</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo>¨</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf212"><mml:msub><mml:mi>N</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:math></inline-formula> is the number of <inline-formula><mml:math id="inf213"><mml:mi>M</mml:mi></mml:math></inline-formula> units. <inline-formula><mml:math id="inf214"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the derivative of the activity for the postsynaptic unit, and <inline-formula><mml:math id="inf215"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> is a time delay ensuring that the rule captures the proper temporal causality. In the Supplementary Discussion of the Appendix we elaborate on how such a learning rule could be present in the spinal cord.</p><p>The learning rule in 9 was also fitted with soft weight-bounding to prevent connections from changing sign, and multiplicative normalization was used to control the magnitude of the weights by ensuring two requirements: (1) all weights from projections of the same <inline-formula><mml:math id="inf216"><mml:mi>M</mml:mi></mml:math></inline-formula> unit should add to <inline-formula><mml:math id="inf217"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, (2) all weights ending at the same <inline-formula><mml:math id="inf218"><mml:mi>C</mml:mi></mml:math></inline-formula> unit should add to <inline-formula><mml:math id="inf219"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. With this, the learning rule adopted the form:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>ω</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ζ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ζ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In this equation <inline-formula><mml:math id="inf220"><mml:mi>α</mml:mi></mml:math></inline-formula> is a constant learning rate, <inline-formula><mml:math id="inf221"><mml:mi mathvariant="normal">Δ</mml:mi></mml:math></inline-formula> is the right-hand side expression of <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>, and <inline-formula><mml:math id="inf222"><mml:mi>λ</mml:mi></mml:math></inline-formula> is a scalar parameter. The value <inline-formula><mml:math id="inf223"><mml:msub><mml:mi>ζ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is <inline-formula><mml:math id="inf224"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> divided by the sum of outgoing weights from the <inline-formula><mml:math id="inf225"><mml:mi>j</mml:mi></mml:math></inline-formula>-th <inline-formula><mml:math id="inf226"><mml:mi>M</mml:mi></mml:math></inline-formula> unit, and <inline-formula><mml:math id="inf227"><mml:msub><mml:mi>ζ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is <inline-formula><mml:math id="inf228"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> divided by the sum of incoming <inline-formula><mml:math id="inf229"><mml:mi>M</mml:mi></mml:math></inline-formula> weights on <italic>c</italic><sub><italic>i</italic></sub>. This type of normalization is meant to reflect the competition for resources among synapses, both at the presynaptic and postsynaptic level.</p><p>The synapses in the connections from <inline-formula><mml:math id="inf230"><mml:mi>A</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf231"><mml:mi>M</mml:mi></mml:math></inline-formula> and from <inline-formula><mml:math id="inf232"><mml:mi>A</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf233"><mml:mi>C</mml:mi></mml:math></inline-formula> used the input correlation rule (<xref ref-type="bibr" rid="bib84">Porr and Wörgötter, 2006</xref>):<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mi>w</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>I</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf234"><mml:msub><mml:mi>I</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> is the scaled sum of inputs from the <inline-formula><mml:math id="inf235"><mml:mi>A</mml:mi></mml:math></inline-formula> population, <inline-formula><mml:math id="inf236"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the learning rate, <inline-formula><mml:math id="inf237"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the scaled sum of inputs from <inline-formula><mml:math id="inf238"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf239"><mml:mi>M</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf240"><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is its derivative. Unlike the original input correlation rule, this rule uses soft weight bounding to avoid weights changing signs. Moreover, the sum of the weights was kept close to a <inline-formula><mml:math id="inf241"><mml:msub><mml:mi>ω</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> value. In practice this meant dividing the each individual <inline-formula><mml:math id="inf242"><mml:mi>w</mml:mi></mml:math></inline-formula> value by the sum of weights from <inline-formula><mml:math id="inf243"><mml:mi>A</mml:mi></mml:math></inline-formula>-to-<inline-formula><mml:math id="inf244"><mml:mi>M</mml:mi></mml:math></inline-formula> (or <inline-formula><mml:math id="inf245"><mml:mi>A</mml:mi></mml:math></inline-formula>-to-<inline-formula><mml:math id="inf246"><mml:mi>C</mml:mi></mml:math></inline-formula>) connections, and multiplying times <inline-formula><mml:math id="inf247"><mml:msub><mml:mi>ω</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> at each update. In addition, weight clipping was used to keep individual weights below a value <inline-formula><mml:math id="inf248"><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p>The learning rule in the cortical learning model was the same, but the presynaptic units were in <inline-formula><mml:math id="inf249"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and the postsynaptic units in <inline-formula><mml:math id="inf250"><mml:mi>M</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s4-3"><title>Exploratory mechanism</title><p>Without any additional mechanisms the model risked getting stuck in a fixed arm position before it could learn. We included two mechanisms to permit exploration in the system. We describe these two mechanisms as they were applied to the spinal learning model and its two variations. The description below also applies to the case of the cortical learning model, with the <inline-formula><mml:math id="inf251"><mml:mi>M</mml:mi></mml:math></inline-formula> units (instead of the <inline-formula><mml:math id="inf252"><mml:mi>C</mml:mi></mml:math></inline-formula> units) receiving the noise and extra connections.</p><p>The first exploratory mechanism consists of intrinsic noise in the <inline-formula><mml:math id="inf253"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf254"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> interneurons, which causes low-amplitude oscillations in the arm. We have observed that intrinsic oscillations in the <inline-formula><mml:math id="inf255"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> units are also effective to allow learning (data not shown), but the option of intrinsic noise permits the use of simple sigmoidal units in <inline-formula><mml:math id="inf256"><mml:mi>C</mml:mi></mml:math></inline-formula>, and contributes to the discussion regarding the role of noise in neural computation.</p><p>The second mechanism for exploration consists of an additional unit, called <inline-formula><mml:math id="inf257"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>. This unit acted similarly to a leaky integrator of the total activity in <inline-formula><mml:math id="inf258"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, reflecting the total error. If the leaky integral of the <inline-formula><mml:math id="inf259"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> activity crossed a threshold, then <inline-formula><mml:math id="inf260"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> would send a signal to all the <inline-formula><mml:math id="inf261"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf262"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> units, causing adaptation. The adaptation consisted of an inhibitory current that grew depending on the accumulated previous activity.</p><p>To model this, <inline-formula><mml:math id="inf263"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf264"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> units received an extra input <inline-formula><mml:math id="inf265"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. When the input from the <inline-formula><mml:math id="inf266"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> unit was larger than 0.8, and <inline-formula><mml:math id="inf267"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the value of <inline-formula><mml:math id="inf268"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> would be set to <inline-formula><mml:math id="inf269"><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>. This is the square of a low-passed filtered version of <italic>u</italic><sub><italic>i</italic></sub>. More explicitly,<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>If the input from <inline-formula><mml:math id="inf270"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> was smaller than 0.8, or <inline-formula><mml:math id="inf271"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> became larger than 0.2, then <inline-formula><mml:math id="inf272"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> would decay towards zero:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>With this mechanism, if the arm got stuck then error would accumulate, leading to adaptation in the spinal interneurons. This would cause the most active interneurons to receive the most inhibition, shifting the ‘dominant’ activities, and producing larger amplitude exploratory oscillations.</p><p>When a new target is presented, <inline-formula><mml:math id="inf273"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> must reset its own activity back to a low value. Given our requirement to fully implement the controller using neural elements, we needed a way to detect changes in <inline-formula><mml:math id="inf274"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula>. A unit denominated <inline-formula><mml:math id="inf275"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula> can detect these changes using synapses that react to the derivative of the activity in <inline-formula><mml:math id="inf276"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> units. <inline-formula><mml:math id="inf277"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula> was connected to <inline-formula><mml:math id="inf278"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> in order to reset its activity.</p><p>More precisely, when inputs from <inline-formula><mml:math id="inf279"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula> were larger than 0.1, the activity of <inline-formula><mml:math id="inf280"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> had dynamics:<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>40</mml:mn><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Otherwise it had these dynamics:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> if </mml:mtext><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">[</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="120%" minsize="120%">]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mtext> otherwise.</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>As before, <inline-formula><mml:math id="inf281"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a sigmoidal function, and <inline-formula><mml:math id="inf282"><mml:mi>I</mml:mi></mml:math></inline-formula> is the scaled sum of inputs other than <inline-formula><mml:math id="inf283"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula>. When <inline-formula><mml:math id="inf284"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is smaller than a threshold <inline-formula><mml:math id="inf285"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the value of <inline-formula><mml:math id="inf286"><mml:mi>a</mml:mi></mml:math></inline-formula> actually decreases, as this error is deemed small enough. When <inline-formula><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> the activity increases, but the rate of increase is modulated by a rate of increase <inline-formula><mml:math id="inf288"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≡</mml:mo><mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf289"><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> is a low-pass filtered version of <inline-formula><mml:math id="inf290"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a constant parameter.</p><p><inline-formula><mml:math id="inf291"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula> was a standard sigmoidal unit receiving inputs from <inline-formula><mml:math id="inf292"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula>, with each synaptic weight obeying this equation:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <italic>s</italic><sub><italic>j</italic></sub> represents the synapse’s presynaptic input.</p></sec><sec id="s4-4"><title>Plant, muscles, afferents</title><p>The planar arm was modeled as a compound double pendulum, where both the arm and forearm were cylinders with 1 kg. of mass. No gravity was present, and a moderate amount of viscous friction was added at each joint (3 <inline-formula><mml:math id="inf293"><mml:mfrac><mml:mrow><mml:mpadded width="+5pt"><mml:mi>N</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mpadded width="+5pt"><mml:mi>m</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>). The derivation and validation of the double pendulum’s equations can be consulted in a Jupyter notebook included with Draculab’s source code (in the tests folder).</p><p>The muscles used a standard Hill-type model, as described in <xref ref-type="bibr" rid="bib95">Shadmehr and Wise, 2005</xref>, Pg. 99. The muscle’s tension <inline-formula><mml:math id="inf294"><mml:mi>T</mml:mi></mml:math></inline-formula> obeys:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mi>b</mml:mi></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⋅</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf295"><mml:mi>I</mml:mi></mml:math></inline-formula> is the input, <inline-formula><mml:math id="inf296"><mml:mi>g</mml:mi></mml:math></inline-formula> an input gain, <inline-formula><mml:math id="inf297"><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the parallel elasticity constant, <inline-formula><mml:math id="inf298"><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the series elasticity constant, <inline-formula><mml:math id="inf299"><mml:mi>b</mml:mi></mml:math></inline-formula> is the damping constant for the parallel element, <inline-formula><mml:math id="inf300"><mml:mi>x</mml:mi></mml:math></inline-formula> is the length of the muscle, and <inline-formula><mml:math id="inf301"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>. In here, <inline-formula><mml:math id="inf302"><mml:msubsup><mml:mi>x</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> is the resting length of the series element, whereas <inline-formula><mml:math id="inf303"><mml:msubsup><mml:mi>x</mml:mi><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> is the resting length of the parallel element. All resting lengths were calculated from the steady state when the hand was located at coordinates (0.3, 0.3).</p><p>We created a model of the Ia and II afferents using simple structural elements. This model includes, for each muscle one dynamic nuclear bag fiber, and one static bag fiber. Both of these fibers use the same tension equation as the muscle, but with different parameters. For the static bag fiber:<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The dynamic bag fiber uses the same equation, with the <inline-formula><mml:math id="inf304"><mml:mi>s</mml:mi></mml:math></inline-formula> superscript replaced by <inline-formula><mml:math id="inf305"><mml:mi>d</mml:mi></mml:math></inline-formula>. No inputs were applied to the static or dynamic bag fibers, so they were removed from these equations. The rest lengths of the static and dynamic bag fibers where those of their corresponding muscles times factors <inline-formula><mml:math id="inf306"><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mn>0</mml:mn><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>l</mml:mi><mml:mn>0</mml:mn><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, respectively.</p><p>The Ia afferent output is proportional to a linear combination of the lengths for the serial elements in both dynamic and static bag fibers. The II output has two components, one proportional to the length of the serial element, and one approximately proportional to the length of the parallel element, both in the static bag fiber. In practice this was implemented through the following equations:<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>I</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In here, <inline-formula><mml:math id="inf307"><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf308"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are gain factors. <inline-formula><mml:math id="inf309"><mml:msubsup><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf310"><mml:msubsup><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> are constants determining the fraction of <inline-formula><mml:math id="inf311"><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf312"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> output that comes from the serial element.</p><p>The model of the Golgi tendon organ producing the Ib outputs was taken from <xref ref-type="bibr" rid="bib66">Lin and Crago, 2002</xref>. First, a rectified tension was obtained as:<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf313"><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:msub></mml:math></inline-formula> is a gain factor, <italic>T</italic><sub>0</sub> is a constant that can further alter the slope of the tension, and <inline-formula><mml:math id="inf314"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the tension, half-rectified. The <inline-formula><mml:math id="inf315"><mml:msub><mml:mi>I</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:math></inline-formula> afferent output followed dynamics:<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>I</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-5"><title>Static connections</title><p>In all cases, the connections to <inline-formula><mml:math id="inf316"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> used one-to-one connectivity with the <inline-formula><mml:math id="inf317"><mml:mi>A</mml:mi></mml:math></inline-formula> units driven by the II afferents, whereas connections from <inline-formula><mml:math id="inf318"><mml:mi>A</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf319"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf320"><mml:mi>C</mml:mi></mml:math></inline-formula> used all-to-all projections from the units driven by the Ia and Ib afferents. Projections from <inline-formula><mml:math id="inf321"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf322"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> used one-to-one excitatory connections to the first 6 units, and inhibitory projections to the next six units. Projections from <inline-formula><mml:math id="inf323"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf324"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> used the opposite sign from this.</p><p>Connections from <inline-formula><mml:math id="inf325"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf326"><mml:mi>M</mml:mi></mml:math></inline-formula> were one-to-one, so the <inline-formula><mml:math id="inf327"><mml:mi>j</mml:mi></mml:math></inline-formula>-th unit in <inline-formula><mml:math id="inf328"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> only sent a projection to unit <inline-formula><mml:math id="inf329"><mml:mi>j</mml:mi></mml:math></inline-formula> in <inline-formula><mml:math id="inf330"><mml:mi>M</mml:mi></mml:math></inline-formula>. A variation of this connectivity is presented in the Appendix (See <italic>Variations of the spinal learning model</italic>).</p><p>We now explain how we adjusted the synaptic weights of the static network. To understand the projections from <inline-formula><mml:math id="inf331"><mml:mi>M</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf332"><mml:mi>C</mml:mi></mml:math></inline-formula> and to the alpha motoneurons it is useful to remember that each <inline-formula><mml:math id="inf333"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:math></inline-formula> trio is associated with one muscle, and the <inline-formula><mml:math id="inf334"><mml:mi>M</mml:mi></mml:math></inline-formula> units also control the error of a single muscle. This error indicates that the muscle is longer than desired. Thus, the <inline-formula><mml:math id="inf335"><mml:mi>M</mml:mi></mml:math></inline-formula> unit associated with muscle <inline-formula><mml:math id="inf336"><mml:mi>i</mml:mi></mml:math></inline-formula> sent excitatory projections to the <inline-formula><mml:math id="inf337"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf338"><mml:mi>α</mml:mi></mml:math></inline-formula> units associated with muscle <inline-formula><mml:math id="inf339"><mml:mi>i</mml:mi></mml:math></inline-formula>, and to the <inline-formula><mml:math id="inf340"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> units of the antagonists of <inline-formula><mml:math id="inf341"><mml:mi>i</mml:mi></mml:math></inline-formula>. Additionally, weaker projections were sent to the <inline-formula><mml:math id="inf342"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:math></inline-formula> units of muscle <inline-formula><mml:math id="inf343"><mml:mi>i</mml:mi></mml:math></inline-formula>’s agonists. Notice that only excitatory connections were used.</p><p>The reverse logic was used to set the connections from <inline-formula><mml:math id="inf344"><mml:mi>A</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf345"><mml:mi>C</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf346"><mml:mi>M</mml:mi></mml:math></inline-formula>. If muscle <inline-formula><mml:math id="inf347"><mml:mi>i</mml:mi></mml:math></inline-formula> is tensing or elongating, this can predict an increase in the error for its antagonists, which is the kind of signal that the input correlation rule is meant to detect. Therefore, the <inline-formula><mml:math id="inf348"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></inline-formula> afferent (signaling tension) of muscle <inline-formula><mml:math id="inf349"><mml:mi>i</mml:mi></mml:math></inline-formula> sent an excitatory signal to the <inline-formula><mml:math id="inf350"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> unit associated with muscle <inline-formula><mml:math id="inf351"><mml:mi>i</mml:mi></mml:math></inline-formula>, and to the <inline-formula><mml:math id="inf352"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:math></inline-formula> units associated with <inline-formula><mml:math id="inf353"><mml:mi>i</mml:mi></mml:math></inline-formula>’s antagonists. Moreover, this <inline-formula><mml:math id="inf354"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></inline-formula> afferent also sent an excitatory projection to the dual of the <inline-formula><mml:math id="inf355"><mml:mi>M</mml:mi></mml:math></inline-formula> unit associated with muscle <inline-formula><mml:math id="inf356"><mml:mi>i</mml:mi></mml:math></inline-formula>. Connections from <inline-formula><mml:math id="inf357"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula> afferents (roughly signaling elongation speed) followed the same pattern, but with slightly smaller connection strengths.</p></sec><sec id="s4-6"><title>Rotational dynamics</title><p>We explain the method to project the activity of <inline-formula><mml:math id="inf358"><mml:mi>M</mml:mi></mml:math></inline-formula> onto the jPCA plane. For all units in <inline-formula><mml:math id="inf359"><mml:mi>M</mml:mi></mml:math></inline-formula> we considered the activity during a 0.5 s sample beginning 50 ms after the target onset. Unlike (<xref ref-type="bibr" rid="bib24">Churchland et al., 2012</xref>), we did not apply PCA preprocessing, since we only have 12 units in <inline-formula><mml:math id="inf360"><mml:mi>M</mml:mi></mml:math></inline-formula>. Let <inline-formula><mml:math id="inf361"><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> be the activity at time <inline-formula><mml:math id="inf362"><mml:mi>t</mml:mi></mml:math></inline-formula> of the unit <inline-formula><mml:math id="inf363"><mml:mi>i</mml:mi></mml:math></inline-formula> in <inline-formula><mml:math id="inf364"><mml:mi>M</mml:mi></mml:math></inline-formula>, when reaching at target <inline-formula><mml:math id="inf365"><mml:mi>j</mml:mi></mml:math></inline-formula> for the <inline-formula><mml:math id="inf366"><mml:mi>k</mml:mi></mml:math></inline-formula>-th repetition. By <inline-formula><mml:math id="inf367"><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> we denote the average over all repeated reaches to the same target, and by <inline-formula><mml:math id="inf368"><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> we indicate averaging over both targets and repetitions. The normalized average trace per condition is defined as: <inline-formula><mml:math id="inf369"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≡</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="inf370"><mml:mi>I</mml:mi></mml:math></inline-formula> stand for the number of units in <inline-formula><mml:math id="inf371"><mml:mi>M</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf372"><mml:mi>T</mml:mi></mml:math></inline-formula> for the number of time points, and <inline-formula><mml:math id="inf373"><mml:mi>J</mml:mi></mml:math></inline-formula> for the number of targets. Following (<xref ref-type="bibr" rid="bib24">Churchland et al., 2012</xref>), we unroll the set of <inline-formula><mml:math id="inf374"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> values into a matrix <inline-formula><mml:math id="inf375"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi>J</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, so we may represent the data through a matrix <inline-formula><mml:math id="inf376"><mml:mi>M</mml:mi></mml:math></inline-formula> that provides the least-squares solution to the problem <inline-formula><mml:math id="inf377"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This solution comes from the equation <inline-formula><mml:math id="inf378"><mml:mrow><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>. Furthermore, this matrix can be decomposed into symmetric and anti-symmetric components <inline-formula><mml:math id="inf379"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The jPCA plane comes from the complex conjugate eigenvalues of <inline-formula><mml:math id="inf380"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p>In practice, our source code follows the detailed explanation provided in the Supplementary Information of <xref ref-type="bibr" rid="bib24">Churchland et al., 2012</xref>, which reformulates this matrix problem as a vector problem.</p></sec><sec id="s4-7"><title>Parameter search</title><p>We kept all parameter values in a range where they still made biological sense. Parameter values that were not constrained by biological data were adjusted using a genetic algorithm, and particle swarm optimization (PSO). We used a separate optimization run for each one of the configurations, consisting of roughly 30 iterations of the genetic and PSO algorithms, with populations sizes of 90 and 45 individuals respectively. After this we manually adjusted the gain of the control loop by increasing or decreasing the slope of the sigmoidal units in the <inline-formula><mml:math id="inf381"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf382"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> populations. This is further described in the Appendix (<italic>Gain and oscillations</italic> section).</p><p>The parameters used can affect the results in the paper. We chose parameters that minimized either the error during the second half of the learning phase, or the error during center-out reaching. Both of these measures are agnostic to the other results.</p></sec><sec id="s4-8"><title>Preferred direction vectors</title><p>Next we describe how PD vectors were obtained for the <inline-formula><mml:math id="inf383"><mml:mi>M</mml:mi></mml:math></inline-formula> units.</p><p>Let <inline-formula><mml:math id="inf384"><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denote the firing rate of the <inline-formula><mml:math id="inf385"><mml:mi>j</mml:mi></mml:math></inline-formula>-th <inline-formula><mml:math id="inf386"><mml:mi>M</mml:mi></mml:math></inline-formula> unit when reaching for the <inline-formula><mml:math id="inf387"><mml:mi>k</mml:mi></mml:math></inline-formula>-th target, averaged over 4 s, and across reaches to the same target. We created a function <inline-formula><mml:math id="inf388"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>→</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> that mapped the X,Y coordinates of each target to its corresponding <inline-formula><mml:math id="inf389"><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> value, but in the domain of <italic>h</italic><sub><italic>j</italic></sub> the coordinates were shifted so the center location was at the origin.</p><p>Next we approximated <italic>h</italic><sub><italic>j</italic></sub> with a plane, using the least squares method, and obtained a unit vector <italic>u</italic><sub><italic>j</italic></sub> normal to that plane, starting at the intersection of the <inline-formula><mml:math id="inf390"><mml:mi>z</mml:mi></mml:math></inline-formula>-axis and the plane, and pointing towards the XY plane. The PD vector was defined as the projection of <italic>u</italic><sub><italic>j</italic></sub> on the XY plane.</p><p>In order to predict the PD vectors, we first obtained for each muscle the ‘direction of maximum contraction’, verbally described in panel B of <xref ref-type="fig" rid="fig5">Figure 5</xref>. More formally, let <inline-formula><mml:math id="inf391"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denote the length of the <inline-formula><mml:math id="inf392"><mml:mi>i</mml:mi></mml:math></inline-formula>-th muscle when the hand is at target <inline-formula><mml:math id="inf393"><mml:mi>k</mml:mi></mml:math></inline-formula>, and let <inline-formula><mml:math id="inf394"><mml:msubsup><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:msubsup></mml:math></inline-formula> denote its length when the hand is at the center location. With <inline-formula><mml:math id="inf395"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> we denote the unit vector with base at the center location, pointing in the direction of the <inline-formula><mml:math id="inf396"><mml:mi>k</mml:mi></mml:math></inline-formula>-th target. The direction of maximum length change for the <inline-formula><mml:math id="inf397"><mml:mi>i</mml:mi></mml:math></inline-formula>-th muscle comes from the following vector sum:<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf398"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>For the <inline-formula><mml:math id="inf399"><mml:mi>j</mml:mi></mml:math></inline-formula>-th unit in <inline-formula><mml:math id="inf400"><mml:mi>M</mml:mi></mml:math></inline-formula>, its predicted PD vector comes from a linear combination of the <inline-formula><mml:math id="inf401"><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> vectors. Let the input to this unit be <inline-formula><mml:math id="inf402"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic>e</italic><sub><italic>i</italic></sub> is the output of the <inline-formula><mml:math id="inf403"><mml:mi>i</mml:mi></mml:math></inline-formula>-th SPF unit (representing the error in the <inline-formula><mml:math id="inf404"><mml:mi>i</mml:mi></mml:math></inline-formula>-th muscle). The predicted PD vector is:<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>d</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>To obtain the main axis of the PD distribution, the <inline-formula><mml:math id="inf405"><mml:mi>i</mml:mi></mml:math></inline-formula>-th PD vector was obtained in the polar form <inline-formula><mml:math id="inf406"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf407"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. We reflected vectors in the lower half using the rule: <inline-formula><mml:math id="inf408"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> if <inline-formula><mml:math id="inf409"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> otherwise. The angle of the main axis was the angle of the average PD vector using these modified angles: <inline-formula><mml:math id="inf410"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>arctan</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:mstyle><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:mstyle><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-9"><title>Statistical tests</title><p>To find whether <inline-formula><mml:math id="inf411"><mml:mi>M</mml:mi></mml:math></inline-formula> units were significantly tuned to the reach direction we used a bootstrap procedure. For each unit we obtained the length of its PD vector 10,000 times when the identity of the target for each reach was randomly shuffled. We considered there was significant tuning when the length of the true PD vector was longer than 99.9% of these random samples.</p><p>To obtain the coefficient of determination for the predicted PD angles, let <inline-formula><mml:math id="inf412"><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup></mml:math></inline-formula> denote the angle of the true PD for the <inline-formula><mml:math id="inf413"><mml:mi>j</mml:mi></mml:math></inline-formula>-th <inline-formula><mml:math id="inf414"><mml:mi>M</mml:mi></mml:math></inline-formula> unit, and <inline-formula><mml:math id="inf415"><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup></mml:math></inline-formula> be the angle of its predicted PD. We obtained residuals for the angles as <inline-formula><mml:math id="inf416"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, where this difference is actually the angle of the smallest rotation that turns one angle into the other. Each residual was then scaled by the norm of its corresponding PD vector, to account for the fact that these were not homogeneous. Denoting these scaled residuals as <inline-formula><mml:math id="inf417"><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mi>j</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> the residual sum of squares is <inline-formula><mml:math id="inf418"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mi>j</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The total sum of squares was: <inline-formula><mml:math id="inf419"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf420"><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the mean of the <inline-formula><mml:math id="inf421"><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup></mml:math></inline-formula> angles. The coefficient of determination comes from the usual formula <inline-formula><mml:math id="inf422"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To assess bimodality of the PD distribution we used a version of the Rayleigh statistic adapted to look for bimodal distributions where the two modes are oriented at 180 degrees from each other, introduced in <xref ref-type="bibr" rid="bib64">Lillicrap and Scott, 2013</xref>. This test consists of finding an modified Rayleigh <inline-formula><mml:math id="inf423"><mml:mi>r</mml:mi></mml:math></inline-formula> statistic defined as:<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the <inline-formula><mml:math id="inf424"><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> angles are the angles for the PDs. A bootstrap procedure is then used, where this <inline-formula><mml:math id="inf425"><mml:mi>r</mml:mi></mml:math></inline-formula> statistic is produced 100,000 times by sampling from the uniform distribution on the <inline-formula><mml:math id="inf426"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> interval. The PD distribution was deemed significantly bimodal if its <inline-formula><mml:math id="inf427"><mml:mi>r</mml:mi></mml:math></inline-formula> value was larger than 99.9% of the random <inline-formula><mml:math id="inf428"><mml:mi>r</mml:mi></mml:math></inline-formula> values.</p><p>We used a bootstrap test to find whether there was statistical significance to the linear addition of direction fields. To make this independent of the individual pair of locations stimulated, we obtained the direction fields for all 15 possible pairs of locations, and for each pair calculated the mean angle difference between <inline-formula><mml:math id="inf429"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf430"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> as described in the main text. We next obtained the mean of these 15 average angle deviations, to obtain a global average angle deviation <inline-formula><mml:math id="inf431"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p>We then repeated this procedure 400 times when the identities of the stimulation sites <inline-formula><mml:math id="inf432"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></inline-formula> were shuffled, to obtain 400 global average angle deviations <inline-formula><mml:math id="inf433"><mml:msubsup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup></mml:math></inline-formula>. We declared statistical significance if <inline-formula><mml:math id="inf434"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was smaller than 99% of the <inline-formula><mml:math id="inf435"><mml:msubsup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup></mml:math></inline-formula> values.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Validation, Investigation, Visualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Supervision, Methodology, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-77216-transrepform1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. The source code to generate all figures is available as two commented Jupyter notebooks. They can be downloaded from the following repository: <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/sergio.verduzco/public_materials/-/tree/master/adaptive_plasticity">https://gitlab.com/sergio.verduzco/public_materials/-/tree/master/adaptive_plasticity</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:809d100d2fa7d06106eb32dcd04664f074050aa5;origin=https://gitlab.com/sergio.verduzco/public_materials.git;visit=swh:1:snp:5ee5cf66f15a94b5d855ea381800a8ed427075a8;anchor=swh:1:rev:482c0659d6e90b30a4a1acb4ab3e3a03dfd902c4">swh:1:rev:482c0659d6e90b30a4a1acb4ab3e3a03dfd902c4</ext-link>). Instructions are in the &quot;readme.md&quot; file. Briefly: Prerequisites for running the notebooks are: Python 3.5 or above (<ext-link ext-link-type="uri" xlink:href="https://www.python.org">https://www.python.org</ext-link>); Jupyter (<ext-link ext-link-type="uri" xlink:href="https://jupyter.org">https://jupyter.org</ext-link>); Draculab (<ext-link ext-link-type="uri" xlink:href="https://gitlab.com/sergio.verduzco/draculab">https://gitlab.com/sergio.verduzco/draculab</ext-link>). Please see the links above for detailed installation instructions.</p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors wish to thank Prof. Kenji Doya for helping revise early versions of this manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adams</surname><given-names>RA</given-names></name><name><surname>Shipp</surname><given-names>S</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Predictions not commands: active inference in the motor system</article-title><source>Brain Structure &amp; Function</source><volume>218</volume><fpage>611</fpage><lpage>643</lpage><pub-id pub-id-type="doi">10.1007/s00429-012-0475-5</pub-id><pub-id pub-id-type="pmid">23129312</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alstermark</surname><given-names>B</given-names></name><name><surname>Isa</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Circuits for skilled reaching and grasping</article-title><source>Annual Review of Neuroscience</source><volume>35</volume><fpage>559</fpage><lpage>578</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150527</pub-id><pub-id pub-id-type="pmid">22524789</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arber</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Motor circuits in action: specification, connectivity, and function</article-title><source>Neuron</source><volume>74</volume><fpage>975</fpage><lpage>989</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.05.011</pub-id><pub-id pub-id-type="pmid">22726829</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arber</surname><given-names>S</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Connecting neuronal circuits for movement</article-title><source>Science</source><volume>360</volume><fpage>1403</fpage><lpage>1404</lpage><pub-id pub-id-type="doi">10.1126/science.aat5994</pub-id><pub-id pub-id-type="pmid">29954969</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asante</surname><given-names>CO</given-names></name><name><surname>Martin</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Differential joint-specific corticospinal tract projections within the cervical enlargement</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e74454</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0074454</pub-id><pub-id pub-id-type="pmid">24058570</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ballard</surname><given-names>DH</given-names></name><name><surname>Zhang</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The hierarchical evolution in human vision modeling</article-title><source>Topics in Cognitive Science</source><volume>13</volume><fpage>309</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1111/tops.12527</pub-id><pub-id pub-id-type="pmid">33838010</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bashor</surname><given-names>DP</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A large-scale model of some spinal reflex circuits</article-title><source>Biol Cybern</source><volume>78</volume><fpage>147</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1007/s004220050421</pub-id><pub-id pub-id-type="pmid">9525039</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastian</surname><given-names>AJ</given-names></name><name><surname>Martin</surname><given-names>TA</given-names></name><name><surname>Keating</surname><given-names>JG</given-names></name><name><surname>Thach</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Cerebellar ataxia: abnormal control of interaction torques across multiple joints</article-title><source>Journal of Neurophysiology</source><volume>76</volume><fpage>492</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1152/jn.1996.76.1.492</pub-id><pub-id pub-id-type="pmid">8836239</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Becker</surname><given-names>WJ</given-names></name><name><surname>Morrice</surname><given-names>BL</given-names></name><name><surname>Clark</surname><given-names>AW</given-names></name><name><surname>Lee</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Multi-joint reaching movements and eye-hand tracking in cerebellar incoordination: investigation of a patient with complete loss of Purkinje cells</article-title><source>Canadian Journal of Neurological Sciences / Journal Canadien Des Sciences Neurologiques</source><volume>18</volume><fpage>476</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1017/S0317167100032194</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bekolay</surname><given-names>T</given-names></name><name><surname>Bergstra</surname><given-names>J</given-names></name><name><surname>Hunsberger</surname><given-names>E</given-names></name><name><surname>Dewolf</surname><given-names>T</given-names></name><name><surname>Stewart</surname><given-names>TC</given-names></name><name><surname>Rasmussen</surname><given-names>D</given-names></name><name><surname>Choo</surname><given-names>X</given-names></name><name><surname>Voelker</surname><given-names>AR</given-names></name><name><surname>Eliasmith</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Nengo: a python tool for building large-scale functional brain models</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>48</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00048</pub-id><pub-id pub-id-type="pmid">24431999</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>RW</given-names></name><name><surname>Alaburda</surname><given-names>A</given-names></name><name><surname>Hounsgaard</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Balanced inhibition and excitation drive spike activity in spinal half-centers</article-title><source>Science</source><volume>315</volume><fpage>390</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1126/science.1134960</pub-id><pub-id pub-id-type="pmid">17234950</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>RW</given-names></name><name><surname>Willumsen</surname><given-names>A</given-names></name><name><surname>Lindén</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>When networks walk a fine line: balance of excitation and inhibition in spinal motor circuits</article-title><source>Current Opinion in Physiology</source><volume>8</volume><fpage>76</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1016/j.cophys.2019.01.006</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizzi</surname><given-names>E</given-names></name><name><surname>Mussa-Ivaldi</surname><given-names>FA</given-names></name><name><surname>Giszter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Computations underlying the execution of movement: a biological perspective</article-title><source>Science</source><volume>253</volume><fpage>287</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.1126/science.1857964</pub-id><pub-id pub-id-type="pmid">1857964</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizzi</surname><given-names>E</given-names></name><name><surname>Tresch</surname><given-names>MC</given-names></name><name><surname>Saltiel</surname><given-names>P</given-names></name><name><surname>d’Avella</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>New perspectives on spinal motor systems</article-title><source>Nature Reviews. Neuroscience</source><volume>1</volume><fpage>101</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1038/35039000</pub-id><pub-id pub-id-type="pmid">11252772</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizzi</surname><given-names>E</given-names></name><name><surname>Cheung</surname><given-names>VCK</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The neural origin of muscle synergies</article-title><source>Frontiers in Computational Neuroscience</source><volume>7</volume><elocation-id>51</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2013.00051</pub-id><pub-id pub-id-type="pmid">23641212</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizzi</surname><given-names>E</given-names></name><name><surname>Ajemian</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>From motor planning to execution: a sensorimotor loop perspective</article-title><source>Journal of Neurophysiology</source><volume>124</volume><fpage>1815</fpage><lpage>1823</lpage><pub-id pub-id-type="doi">10.1152/jn.00715.2019</pub-id><pub-id pub-id-type="pmid">33052779</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borisyuk</surname><given-names>R</given-names></name><name><surname>Al Azad</surname><given-names>AK</given-names></name><name><surname>Conte</surname><given-names>D</given-names></name><name><surname>Roberts</surname><given-names>A</given-names></name><name><surname>Soffe</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Modeling the connectome of a simple spinal cord</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00020</pub-id><pub-id pub-id-type="pmid">21977016</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruton</surname><given-names>M</given-names></name><name><surname>O’Dwyer</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Synergies in coordination: a comprehensive overview of neural, computational, and behavioral approaches</article-title><source>Journal of Neurophysiology</source><volume>120</volume><fpage>2761</fpage><lpage>2774</lpage><pub-id pub-id-type="doi">10.1152/jn.00052.2018</pub-id><pub-id pub-id-type="pmid">30281388</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caligiore</surname><given-names>D</given-names></name><name><surname>Parisi</surname><given-names>D</given-names></name><name><surname>Baldassarre</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Integrating reinforcement learning, equilibrium points, and minimum variance to understand the development of reaching: a computational model</article-title><source>Psychological Review</source><volume>121</volume><fpage>389</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1037/a0037016</pub-id><pub-id pub-id-type="pmid">25090425</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cangiano</surname><given-names>L</given-names></name><name><surname>Grillner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Mechanisms of rhythm generation in a spinal locomotor network deprived of crossed connections: the lamprey hemicord</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>923</fpage><lpage>935</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2301-04.2005</pub-id><pub-id pub-id-type="pmid">15673673</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname><given-names>AR</given-names></name><name><surname>Rumpel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A stable brain from unstable components: emerging concepts and implications for neural computation</article-title><source>Neuroscience</source><volume>357</volume><fpage>172</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2017.06.005</pub-id><pub-id pub-id-type="pmid">28602920</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheah</surname><given-names>CC</given-names></name><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Slotine</surname><given-names>JJE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Adaptive tracking control for robots with unknown kinematic and dynamic properties</article-title><source>The International Journal of Robotics Research</source><volume>25</volume><fpage>283</fpage><lpage>296</lpage><pub-id pub-id-type="doi">10.1177/0278364906063830</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chersi</surname><given-names>F</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The cognitive architecture of spatial navigation: hippocampal and striatal contributions</article-title><source>Neuron</source><volume>88</volume><fpage>64</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.021</pub-id><pub-id pub-id-type="pmid">26447573</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Foster</surname><given-names>JD</given-names></name><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural population dynamics during reaching</article-title><source>Nature</source><volume>487</volume><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/nature11129</pub-id><pub-id pub-id-type="pmid">22722855</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisi</surname><given-names>RRL</given-names></name><name><surname>Kohn</surname><given-names>AF</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Simulation system of spinal cord motor nuclei and associated nerves and muscles, in a web-based architecture</article-title><source>Journal of Computational Neuroscience</source><volume>25</volume><fpage>520</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1007/s10827-008-0092-8</pub-id><pub-id pub-id-type="pmid">18506610</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowan</surname><given-names>JD</given-names></name><name><surname>Neuman</surname><given-names>J</given-names></name><name><surname>van Drongelen</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Wilson-cowan equations for neocortical dynamics</article-title><source>Journal of Mathematical Neuroscience</source><volume>6</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1186/s13408-015-0034-5</pub-id><pub-id pub-id-type="pmid">26728012</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cutsuridis</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Does abnormal spinal reciprocal inhibition lead to co-contraction of antagonist motor units? A modeling study</article-title><source>International Journal of Neural Systems</source><volume>17</volume><fpage>319</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1142/S0129065707001160</pub-id><pub-id pub-id-type="pmid">17696295</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Danner</surname><given-names>SM</given-names></name><name><surname>Shevtsova</surname><given-names>NA</given-names></name><name><surname>Frigon</surname><given-names>A</given-names></name><name><surname>Rybak</surname><given-names>IA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Computational modeling of spinal circuits controlling limb coordination and gaits in quadrupeds</article-title><source>eLife</source><volume>6</volume><elocation-id>e31050</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31050</pub-id><pub-id pub-id-type="pmid">29165245</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Day</surname><given-names>BL</given-names></name><name><surname>Thompson</surname><given-names>PD</given-names></name><name><surname>Harding</surname><given-names>AE</given-names></name><name><surname>Marsden</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Influence of vision on upper limb reaching movements in patients with cerebellar ataxia</article-title><source>Brain</source><volume>121 (Pt 2)</volume><fpage>357</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1093/brain/121.2.357</pub-id><pub-id pub-id-type="pmid">9549511</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Rugy</surname><given-names>A</given-names></name><name><surname>Loeb</surname><given-names>GE</given-names></name><name><surname>Carroll</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Are muscle synergies useful for neural control?</article-title><source>Frontiers in Computational Neuroscience</source><volume>7</volume><elocation-id>00019</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2013.00019</pub-id><pub-id pub-id-type="pmid">23519326</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeWolf</surname><given-names>T</given-names></name><name><surname>Stewart</surname><given-names>TC</given-names></name><name><surname>Slotine</surname><given-names>JJ</given-names></name><name><surname>Eliasmith</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A spiking neural model of adaptive arm control</article-title><source>Proceedings. Biological Sciences</source><volume>283</volume><elocation-id>20162134</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2016.2134</pub-id><pub-id pub-id-type="pmid">27903878</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dura-Bernal</surname><given-names>S</given-names></name><name><surname>Zhou</surname><given-names>X</given-names></name><name><surname>Neymotin</surname><given-names>SA</given-names></name><name><surname>Przekwas</surname><given-names>A</given-names></name><name><surname>Francis</surname><given-names>JT</given-names></name><name><surname>Lytton</surname><given-names>WW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical spiking network interfaced with virtual musculoskeletal arm and robotic arm</article-title><source>Frontiers in Neurorobotics</source><volume>9</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.3389/fnbot.2015.00013</pub-id><pub-id pub-id-type="pmid">26635598</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eccles</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Physiology of motor control in man</article-title><source>Applied Neurophysiology</source><volume>44</volume><fpage>5</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1159/000102178</pub-id><pub-id pub-id-type="pmid">7294779</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edelman</surname><given-names>GM</given-names></name><name><surname>Gally</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Degeneracy and complexity in biological systems</article-title><source>PNAS</source><volume>98</volume><fpage>13763</fpage><lpage>13768</lpage><pub-id pub-id-type="doi">10.1073/pnas.231499798</pub-id><pub-id pub-id-type="pmid">11698650</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farina</surname><given-names>D</given-names></name><name><surname>Negro</surname><given-names>F</given-names></name><name><surname>Dideriksen</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The effective neural drive to muscles is the common synaptic input to motor neurons</article-title><source>The Journal of Physiology</source><volume>592</volume><fpage>3427</fpage><lpage>3441</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2014.273581</pub-id><pub-id pub-id-type="pmid">24860172</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Once more on the equilibrium-point hypothesis (lambda model) for motor control</article-title><source>Journal of Motor Behavior</source><volume>18</volume><fpage>17</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1080/00222895.1986.10735369</pub-id><pub-id pub-id-type="pmid">15136283</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiete</surname><given-names>IR</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>CZH</given-names></name><name><surname>Hahnloser</surname><given-names>RHR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Spike-time-dependent plasticity and heterosynaptic competition organize networks to produce long scale-free sequences of neural activity</article-title><source>Neuron</source><volume>65</volume><fpage>563</fpage><lpage>576</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.02.003</pub-id><pub-id pub-id-type="pmid">20188660</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Georgopoulos</surname><given-names>AP</given-names></name><name><surname>Kalaska</surname><given-names>JF</given-names></name><name><surname>Caminiti</surname><given-names>R</given-names></name><name><surname>Massey</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>On the relations between the direction of two-dimensional arm movements and cell discharge in primate motor cortex</article-title><source>The Journal of Neuroscience</source><volume>2</volume><fpage>1527</fpage><lpage>1537</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.02-11-01527.1982</pub-id><pub-id pub-id-type="pmid">7143039</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Georgopoulos</surname><given-names>AP</given-names></name><name><surname>Schwartz</surname><given-names>AB</given-names></name><name><surname>Kettner</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Neuronal population coding of movement direction</article-title><source>Science</source><volume>233</volume><fpage>1416</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1126/science.3749885</pub-id><pub-id pub-id-type="pmid">3749885</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Georgopoulos</surname><given-names>AP</given-names></name><name><surname>Stefanis</surname><given-names>CN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Local shaping of function in the motor cortex: motor contrast, directional tuning</article-title><source>Brain Research Reviews</source><volume>55</volume><fpage>383</fpage><lpage>389</lpage><pub-id pub-id-type="doi">10.1016/j.brainresrev.2007.05.001</pub-id><pub-id pub-id-type="pmid">17543390</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilman</surname><given-names>S</given-names></name><name><surname>Carr</surname><given-names>D</given-names></name><name><surname>Hollenberg</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Kinematic effects of deafferentation and cerebellar ablation</article-title><source>Brain</source><volume>99</volume><fpage>311</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.1093/brain/99.2.311</pub-id><pub-id pub-id-type="pmid">825186</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giszter</surname><given-names>SF</given-names></name><name><surname>Mussa-Ivaldi</surname><given-names>FA</given-names></name><name><surname>Bizzi</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Convergent force fields organized in the frog’s spinal cord</article-title><source>The Journal of Neuroscience</source><volume>13</volume><fpage>467</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.13-02-00467.1993</pub-id><pub-id pub-id-type="pmid">8426224</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giszter</surname><given-names>SF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Motor primitives--new data and future questions</article-title><source>Current Opinion in Neurobiology</source><volume>33</volume><fpage>156</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.04.004</pub-id><pub-id pub-id-type="pmid">25912883</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goulding</surname><given-names>M</given-names></name><name><surname>Bourane</surname><given-names>S</given-names></name><name><surname>Garcia-Campmany</surname><given-names>L</given-names></name><name><surname>Dalet</surname><given-names>A</given-names></name><name><surname>Koch</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Inhibition downunder: an update from the spinal cord</article-title><source>Current Opinion in Neurobiology</source><volume>26</volume><fpage>161</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.03.006</pub-id><pub-id pub-id-type="pmid">24743058</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grau</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning from the spinal cord: how the study of spinal cord plasticity informs our view of learning</article-title><source>Neurobiology of Learning and Memory</source><volume>108</volume><fpage>155</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2013.08.003</pub-id><pub-id pub-id-type="pmid">23973905</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hadders-Algra</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Early human motor development: from variation to the ability to vary and adapt</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>90</volume><fpage>411</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2018.05.009</pub-id><pub-id pub-id-type="pmid">29752957</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>TC</given-names></name><name><surname>Murphy</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Towards a circuit mechanism for movement tuning in motor cortex</article-title><source>Frontiers in Neural Circuits</source><volume>6</volume><elocation-id>127</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2012.00127</pub-id><pub-id pub-id-type="pmid">23346050</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Helmchen</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1999">1999</year><source>Dendrites as Biochemical Compartments</source><publisher-loc>Oxford, England</publisher-loc><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izawa</surname><given-names>J</given-names></name><name><surname>Kondo</surname><given-names>T</given-names></name><name><surname>Ito</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Biological arm motion through reinforcement learning</article-title><source>Biol Cybern</source><volume>91</volume><fpage>10</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1007/s00422-004-0485-3</pub-id><pub-id pub-id-type="pmid">15309543</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jankowska</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Neuroscience in the 21st Century: From Basic to Clinical</source><publisher-loc>New York, NY</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib51"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kaas</surname><given-names>JH</given-names></name><name><surname>Collins</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>The Primate Visual System</source><publisher-loc>New York, United States</publisher-loc><publisher-name>John Wiley &amp; Sons, Ltd</publisher-name><pub-id pub-id-type="doi">10.1201/9780203507599</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kakei</surname><given-names>S</given-names></name><name><surname>Hoffman</surname><given-names>DS</given-names></name><name><surname>Strick</surname><given-names>PL</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Muscle and movement representations in the primary motor cortex</article-title><source>Science</source><volume>285</volume><fpage>2136</fpage><lpage>2139</lpage><pub-id pub-id-type="doi">10.1126/science.285.5436.2136</pub-id><pub-id pub-id-type="pmid">10497133</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalaska</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>From intention to action: motor cortex and the control of reaching movements</article-title><source>Advances in Experimental Medicine and Biology</source><volume>629</volume><fpage>139</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1007/978-0-387-77064-2_8</pub-id><pub-id pub-id-type="pmid">19227499</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaleb</surname><given-names>K</given-names></name><name><surname>Pedrosa</surname><given-names>V</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Network-centered homeostasis through inhibition maintains hippocampal spatial map and cortical circuit function</article-title><source>Cell Reports</source><volume>36</volume><elocation-id>109577</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.109577</pub-id><pub-id pub-id-type="pmid">34433026</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalidindi</surname><given-names>HT</given-names></name><name><surname>Cross</surname><given-names>KP</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Omrani</surname><given-names>M</given-names></name><name><surname>Falotico</surname><given-names>E</given-names></name><name><surname>Sabes</surname><given-names>PN</given-names></name><name><surname>Scott</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Rotational dynamics in motor cortex are consistent with a feedback controller</article-title><source>eLife</source><volume>10</volume><elocation-id>e67256</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67256</pub-id><pub-id pub-id-type="pmid">34730516</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cortical activity in the null space: permitting preparation without movement</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>440</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1038/nn.3643</pub-id><pub-id pub-id-type="pmid">24487233</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawato</surname><given-names>M</given-names></name><name><surname>Ohmae</surname><given-names>S</given-names></name><name><surname>Hoang</surname><given-names>H</given-names></name><name><surname>Sanger</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>50 years since the MarR, Ito, and albus models of the cerebellum</article-title><source>Neuroscience</source><volume>462</volume><fpage>151</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2020.06.019</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Keener</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="1995">1995</year><source>Principles Of Applied Mathematics: Transformation And Approximation</source><publisher-loc>New York, United States</publisher-loc><publisher-name>Avalon Publishing</publisher-name></element-citation></ref><ref id="bib59"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kelso</surname><given-names>JAS</given-names></name></person-group><year iso-8601-date="2009">2009</year><person-group person-group-type="editor"><name><surname>Sternad</surname><given-names>D</given-names></name></person-group><source>Progress in Motor Control: A Multidisciplinary Perspective, Advances in Experimental Medicine and Biology</source><publisher-loc>Boston, MA</publisher-loc><publisher-name>Springer US</publisher-name></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurtzer</surname><given-names>I</given-names></name><name><surname>Pruszynski</surname><given-names>JA</given-names></name><name><surname>Herter</surname><given-names>TM</given-names></name><name><surname>Scott</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Primate upper limb muscles exhibit activity patterns that differ from their anatomical action during a postural task</article-title><source>Journal of Neurophysiology</source><volume>95</volume><fpage>493</fpage><lpage>504</lpage><pub-id pub-id-type="doi">10.1152/jn.00706.2005</pub-id><pub-id pub-id-type="pmid">16251262</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lemon</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Descending pathways in motor control</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>195</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.31.060407.125547</pub-id><pub-id pub-id-type="pmid">18558853</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levine</surname><given-names>AJ</given-names></name><name><surname>Hinckley</surname><given-names>CA</given-names></name><name><surname>Hilde</surname><given-names>KL</given-names></name><name><surname>Driscoll</surname><given-names>SP</given-names></name><name><surname>Poon</surname><given-names>TH</given-names></name><name><surname>Montgomery</surname><given-names>JM</given-names></name><name><surname>Pfaff</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Identification of a cellular node for motor control pathways</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>586</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1038/nn.3675</pub-id><pub-id pub-id-type="pmid">24609464</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Todorov</surname><given-names>E</given-names></name><name><surname>Pan</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Hierarchical Feedback and Learning for Multi-joint Arm Movement Control</article-title><conf-name>In 2005 IEEE Engineering in Medicine and Biology 27th Annual Conference</conf-name><fpage>4400</fpage><lpage>4403</lpage><pub-id pub-id-type="doi">10.1109/IEMBS.2005.1615441</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Scott</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Preference distributions of primary motor cortex neurons reflect control solutions optimized for limb biomechanics</article-title><source>Neuron</source><volume>77</volume><fpage>168</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.041</pub-id><pub-id pub-id-type="pmid">23312524</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>S</given-names></name><name><surname>Goldman</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Balanced cortical microcircuitry for maintaining information in working memory</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1306</fpage><lpage>1314</lpage><pub-id pub-id-type="doi">10.1038/nn.3492</pub-id><pub-id pub-id-type="pmid">23955560</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>CCK</given-names></name><name><surname>Crago</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Neural and mechanical contributions to the stretch reflex: a model synthesis</article-title><source>Annals of Biomedical Engineering</source><volume>30</volume><fpage>54</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1114/1.1432692</pub-id><pub-id pub-id-type="pmid">11874142</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loeb</surname><given-names>GE</given-names></name><name><surname>Tsianos</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Major remaining gaps in models of sensorimotor systems</article-title><source>Frontiers in Computational Neuroscience</source><volume>9</volume><elocation-id>70</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2015.00070</pub-id><pub-id pub-id-type="pmid">26089795</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>V</given-names></name><name><surname>Scholz</surname><given-names>JP</given-names></name><name><surname>Schöner</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Redundancy, self-motion, and motor control</article-title><source>Neural Computation</source><volume>21</volume><fpage>1371</fpage><lpage>1414</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.01-08-698</pub-id><pub-id pub-id-type="pmid">19718817</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer-Lohmann</surname><given-names>J</given-names></name><name><surname>Christakos</surname><given-names>CN</given-names></name><name><surname>Wolf</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Dominance of the short-latency component in perturbation induced electromyographic responses of long-trained monkeys</article-title><source>Experimental Brain Research</source><volume>64</volume><fpage>393</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1007/BF00340475</pub-id><pub-id pub-id-type="pmid">2948829</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mici</surname><given-names>L</given-names></name><name><surname>Parisi</surname><given-names>GI</given-names></name><name><surname>Wermter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An incremental self-organizing architecture for sensorimotor learning and prediction</article-title><source>IEEE Transactions on Cognitive and Developmental Systems</source><volume>10</volume><fpage>918</fpage><lpage>928</lpage><pub-id pub-id-type="doi">10.1109/TCDS.2018.2832844</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mileusnic</surname><given-names>MP</given-names></name><name><surname>Brown</surname><given-names>IE</given-names></name><name><surname>Lan</surname><given-names>N</given-names></name><name><surname>Loeb</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Mathematical models of proprioceptors. I. control and transduction in the muscle spindle</article-title><source>Journal of Neurophysiology</source><volume>96</volume><fpage>1772</fpage><lpage>1788</lpage><pub-id pub-id-type="doi">10.1152/jn.00868.2005</pub-id><pub-id pub-id-type="pmid">16672301</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrow</surname><given-names>MM</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Prediction of muscle activity by populations of sequentially recorded primary motor cortex neurons</article-title><source>Journal of Neurophysiology</source><volume>89</volume><fpage>2279</fpage><lpage>2288</lpage><pub-id pub-id-type="doi">10.1152/jn.00632.2002</pub-id><pub-id pub-id-type="pmid">12612022</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spatial representation in the hippocampal formation: a history</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1448</fpage><lpage>1464</lpage><pub-id pub-id-type="doi">10.1038/nn.4653</pub-id><pub-id pub-id-type="pmid">29073644</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mountcastle</surname><given-names>VB</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The columnar organization of the neocortex</article-title><source>Brain</source><volume>120</volume><fpage>701</fpage><lpage>722</lpage><pub-id pub-id-type="doi">10.1093/brain/120.4.701</pub-id><pub-id pub-id-type="pmid">9153131</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mussa-Ivaldi</surname><given-names>FA</given-names></name><name><surname>Giszter</surname><given-names>SF</given-names></name><name><surname>Bizzi</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Linear combinations of primitives in vertebrate motor control</article-title><source>PNAS</source><volume>91</volume><fpage>7534</fpage><lpage>7538</lpage><pub-id pub-id-type="doi">10.1073/pnas.91.16.7534</pub-id><pub-id pub-id-type="pmid">8052615</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mussa–Ivaldi</surname><given-names>FA</given-names></name><name><surname>Bizzi</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Motor learning through the combination of primitives</article-title><source>Philosophical Transactions of the Royal Society of London. Series B</source><volume>355</volume><fpage>1755</fpage><lpage>1769</lpage><pub-id pub-id-type="doi">10.1098/rstb.2000.0733</pub-id><pub-id pub-id-type="pmid">11205339</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Najafi</surname><given-names>F</given-names></name><name><surname>Elsayed</surname><given-names>GF</given-names></name><name><surname>Cao</surname><given-names>R</given-names></name><name><surname>Pnevmatikakis</surname><given-names>E</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Excitatory and inhibitory subnetworks are equally selective during decision-making and emerge simultaneously during learning</article-title><source>Neuron</source><volume>105</volume><fpage>165</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.045</pub-id><pub-id pub-id-type="pmid">31753580</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nishimura</surname><given-names>Y</given-names></name><name><surname>Perlmutter</surname><given-names>SI</given-names></name><name><surname>Eaton</surname><given-names>RW</given-names></name><name><surname>Fetz</surname><given-names>EE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Spike-timing-dependent plasticity in primate corticospinal connections induced during free behavior</article-title><source>Neuron</source><volume>80</volume><fpage>1301</fpage><lpage>1309</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.08.028</pub-id><pub-id pub-id-type="pmid">24210907</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norton</surname><given-names>JJS</given-names></name><name><surname>Wolpaw</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Acquisition, maintenance, and therapeutic use of a simple motor skill</article-title><source>Current Opinion in Behavioral Sciences</source><volume>20</volume><fpage>138</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2017.12.021</pub-id><pub-id pub-id-type="pmid">30480059</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Six principles for biologically based computational models of cortical cognition</article-title><source>Trends in Cognitive Sciences</source><volume>2</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(98)01241-8</pub-id><pub-id pub-id-type="pmid">21227277</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>CSR</given-names></name><name><surname>Bizzi</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Neuronal activity in the supplementary motor area of monkeys adapting to a new dynamic environment</article-title><source>Journal of Neurophysiology</source><volume>91</volume><fpage>449</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1152/jn.00876.2002</pub-id><pub-id pub-id-type="pmid">12968016</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petersen</surname><given-names>PC</given-names></name><name><surname>Vestergaard</surname><given-names>M</given-names></name><name><surname>Jensen</surname><given-names>KHR</given-names></name><name><surname>Berg</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Premotor spinal network with balanced excitation and inhibition during motor patterns has high resilience to structural division</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>2774</fpage><lpage>2784</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3349-13.2014</pub-id><pub-id pub-id-type="pmid">24553920</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pierrot-Deseilligny</surname><given-names>E</given-names></name><name><surname>Burke</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>The Circuitry of the Human Spinal Cord</source><publisher-loc>Cambridge</publisher-loc><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511545047</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Porr</surname><given-names>B</given-names></name><name><surname>Wörgötter</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Strongly improved stability and faster convergence of temporal sequence learning by using input correlations only</article-title><source>Neural Computation</source><volume>18</volume><fpage>1380</fpage><lpage>1412</lpage><pub-id pub-id-type="doi">10.1162/neco.2006.18.6.1380</pub-id><pub-id pub-id-type="pmid">16764508</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Powers</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Feedback: beyond behaviorism stimulus-response laws are wholly predictable within a control-system model of behavioral organization</article-title><source>Science</source><volume>179</volume><fpage>351</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1126/science.179.4071.351</pub-id><pub-id pub-id-type="pmid">4682961</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Powers</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>Behavior: The Control of Perception</source><publisher-loc>New Canaan, CT, US</publisher-loc><publisher-name>Benchmark Press</publisher-name></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulvermüller</surname><given-names>F</given-names></name><name><surname>Tomasello</surname><given-names>R</given-names></name><name><surname>Henningsen-Schomers</surname><given-names>MR</given-names></name><name><surname>Wennekers</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Biological constraints on neural network models of cognitive function</article-title><source>Nature Reviews. Neuroscience</source><volume>22</volume><fpage>488</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1038/s41583-021-00473-5</pub-id><pub-id pub-id-type="pmid">34183826</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Beaudoin</surname><given-names>P</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Christensen</surname><given-names>A</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>de Berker</surname><given-names>A</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Gillon</surname><given-names>CJ</given-names></name><name><surname>Hafner</surname><given-names>D</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Pack</surname><given-names>CC</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Roelfsema</surname><given-names>P</given-names></name><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Scellier</surname><given-names>B</given-names></name><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Yamins</surname><given-names>D</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Therien</surname><given-names>D</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A deep learning framework for neuroscience</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1761</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0520-2</pub-id><pub-id pub-id-type="pmid">31659335</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richter</surname><given-names>S</given-names></name><name><surname>Maschke</surname><given-names>M</given-names></name><name><surname>Timmann</surname><given-names>D</given-names></name><name><surname>Konczak</surname><given-names>J</given-names></name><name><surname>Kalenscher</surname><given-names>T</given-names></name><name><surname>Illenberger</surname><given-names>AR</given-names></name><name><surname>Kalveram</surname><given-names>KT</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Adaptive motor behavior of cerebellar patients during exposure to unfamiliar external forces</article-title><source>Journal of Motor Behavior</source><volume>36</volume><fpage>28</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.3200/JMBR.36.1.28-38</pub-id><pub-id pub-id-type="pmid">14766486</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rokni</surname><given-names>U</given-names></name><name><surname>Richardson</surname><given-names>AG</given-names></name><name><surname>Bizzi</surname><given-names>E</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Motor learning with unstable neural representations</article-title><source>Neuron</source><volume>54</volume><fpage>653</fpage><lpage>666</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.04.030</pub-id><pub-id pub-id-type="pmid">17521576</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanguineti</surname><given-names>V</given-names></name><name><surname>Morasso</surname><given-names>PG</given-names></name><name><surname>Baratto</surname><given-names>L</given-names></name><name><surname>Brichetto</surname><given-names>G</given-names></name><name><surname>Luigi Mancardi</surname><given-names>G</given-names></name><name><surname>Solaro</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cerebellar ataxia: quantitative assessment and cybernetic interpretation</article-title><source>Human Movement Science</source><volume>22</volume><fpage>189</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1016/s0167-9457(02)00159-8</pub-id><pub-id pub-id-type="pmid">12667749</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanner</surname><given-names>RM</given-names></name><name><surname>Slotine</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Gaussian networks for direct adaptive control</article-title><source>IEEE Transactions on Neural Networks</source><volume>3</volume><fpage>837</fpage><lpage>863</lpage><pub-id pub-id-type="doi">10.1109/72.165588</pub-id><pub-id pub-id-type="pmid">18276483</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schöner</surname><given-names>G</given-names></name><name><surname>Tekülve</surname><given-names>J</given-names></name><name><surname>Zibner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reaching for Objects: A Neural Process Account in A Developmental Perspective</source><publisher-loc>Oxfordshire, United Kingdom</publisher-loc><publisher-name>Taylor &amp; Francis Group Logo</publisher-name></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>SH</given-names></name><name><surname>Gribble</surname><given-names>PL</given-names></name><name><surname>Graham</surname><given-names>KM</given-names></name><name><surname>Cabel</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Dissociation between hand motion and population vectors from neural activity in motor cortex</article-title><source>Nature</source><volume>413</volume><fpage>161</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1038/35093102</pub-id><pub-id pub-id-type="pmid">11557980</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shadmehr</surname><given-names>R</given-names></name><name><surname>Wise</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>The Computational Neurobiology of Reaching and Pointing: A Foundation for Motor Learning</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shafi</surname><given-names>M</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Quintana</surname><given-names>J</given-names></name><name><surname>Chow</surname><given-names>C</given-names></name><name><surname>Fuster</surname><given-names>J</given-names></name><name><surname>Bodner</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Variability in neuronal activity in primate cortex during working memory tasks</article-title><source>Neuroscience</source><volume>146</volume><fpage>1082</fpage><lpage>1108</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2006.12.072</pub-id><pub-id pub-id-type="pmid">17418956</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical control of arm movements: a dynamical systems perspective</article-title><source>Annual Review of Neuroscience</source><volume>36</volume><fpage>337</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150509</pub-id><pub-id pub-id-type="pmid">23725001</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shevtsova</surname><given-names>NA</given-names></name><name><surname>Talpalar</surname><given-names>AE</given-names></name><name><surname>Markin</surname><given-names>SN</given-names></name><name><surname>Harris-Warrick</surname><given-names>RM</given-names></name><name><surname>Kiehn</surname><given-names>O</given-names></name><name><surname>Rybak</surname><given-names>IA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Organization of left-right coordination of neuronal activity in the mammalian spinal cord: insights from computational modelling</article-title><source>The Journal of Physiology</source><volume>593</volume><fpage>2403</fpage><lpage>2426</lpage><pub-id pub-id-type="doi">10.1113/JP270121</pub-id><pub-id pub-id-type="pmid">25820677</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shevtsova</surname><given-names>NA</given-names></name><name><surname>Rybak</surname><given-names>IA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Organization of flexor-extensor interactions in the mammalian spinal cord: insights from computational modelling</article-title><source>The Journal of Physiology</source><volume>594</volume><fpage>6117</fpage><lpage>6131</lpage><pub-id pub-id-type="doi">10.1113/JP272437</pub-id><pub-id pub-id-type="pmid">27292055</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachowski</surname><given-names>NJ</given-names></name><name><surname>Dougherty</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Spinal inhibitory interneurons: Gatekeepers of sensorimotor pathways</article-title><source>International Journal of Molecular Sciences</source><volume>22</volume><elocation-id>2667</elocation-id><pub-id pub-id-type="doi">10.3390/ijms22052667</pub-id><pub-id pub-id-type="pmid">33800863</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Zatka-Haas</surname><given-names>P</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Distributed coding of choice, action and engagement across the mouse brain</article-title><source>Nature</source><volume>576</volume><fpage>266</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1787-x</pub-id><pub-id pub-id-type="pmid">31776518</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stienen</surname><given-names>AHA</given-names></name><name><surname>Schouten</surname><given-names>AC</given-names></name><name><surname>Schuurmans</surname><given-names>J</given-names></name><name><surname>van der Helm</surname><given-names>FCT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Analysis of reflex modulation with a biologically realistic neural network</article-title><source>Journal of Computational Neuroscience</source><volume>23</volume><fpage>333</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.1007/s10827-007-0037-7</pub-id><pub-id pub-id-type="pmid">17503169</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Generating coherent patterns of activity from chaotic neural networks</article-title><source>Neuron</source><volume>63</volume><fpage>544</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.018</pub-id><pub-id pub-id-type="pmid">19709635</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neural network that finds a naturalistic solution for the production of muscle activity</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1025</fpage><lpage>1033</lpage><pub-id pub-id-type="doi">10.1038/nn.4042</pub-id><pub-id pub-id-type="pmid">26075643</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning: An Introduction</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takei</surname><given-names>T</given-names></name><name><surname>Confais</surname><given-names>J</given-names></name><name><surname>Tomatsu</surname><given-names>S</given-names></name><name><surname>Oya</surname><given-names>T</given-names></name><name><surname>Seki</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural basis for hand muscle synergies in the primate spinal cord</article-title><source>PNAS</source><volume>114</volume><fpage>8643</fpage><lpage>8648</lpage><pub-id pub-id-type="doi">10.1073/pnas.1704328114</pub-id><pub-id pub-id-type="pmid">28739958</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Modeling the motor cortex: optimality, recurrent neural networks, and spatial dynamics</article-title><source>Neuroscience Research</source><volume>104</volume><fpage>64</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1016/j.neures.2015.10.012</pub-id><pub-id pub-id-type="pmid">26562334</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>YH</given-names></name><name><surname>Tanaka</surname><given-names>YR</given-names></name><name><surname>Kondo</surname><given-names>M</given-names></name><name><surname>Terada</surname><given-names>SI</given-names></name><name><surname>Kawaguchi</surname><given-names>Y</given-names></name><name><surname>Matsuzaki</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Thalamocortical axonal activity in motor cortex exhibits layer-specific dynamics during motor learning</article-title><source>Neuron</source><volume>100</volume><fpage>244</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.08.016</pub-id><pub-id pub-id-type="pmid">30174116</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorov</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Direct cortical control of muscle activation in voluntary arm movements: a model</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>391</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1038/73964</pub-id><pub-id pub-id-type="pmid">10725930</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tresch</surname><given-names>MC</given-names></name><name><surname>Jarc</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The case for and against muscle synergies</article-title><source>Current Opinion in Neurobiology</source><volume>19</volume><fpage>601</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2009.09.002</pub-id><pub-id pub-id-type="pmid">19828310</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Truccolo</surname><given-names>W</given-names></name><name><surname>Friehs</surname><given-names>GM</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Primary motor cortex tuning to intended movement kinematics in humans with tetraplegia</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>1163</fpage><lpage>1178</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4415-07.2008</pub-id><pub-id pub-id-type="pmid">18234894</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsianos</surname><given-names>GA</given-names></name><name><surname>Goodner</surname><given-names>J</given-names></name><name><surname>Loeb</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Useful properties of spinal circuits for learning and performing planar reaches</article-title><source>Journal of Neural Engineering</source><volume>11</volume><elocation-id>056006</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/11/5/056006</pub-id><pub-id pub-id-type="pmid">25082652</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ueno</surname><given-names>M</given-names></name><name><surname>Nakamura</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Gu</surname><given-names>Z</given-names></name><name><surname>Niehaus</surname><given-names>J</given-names></name><name><surname>Maezawa</surname><given-names>M</given-names></name><name><surname>Crone</surname><given-names>SA</given-names></name><name><surname>Goulding</surname><given-names>M</given-names></name><name><surname>Baccei</surname><given-names>ML</given-names></name><name><surname>Yoshida</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Corticospinal circuits from the sensory and motor cortices differentially regulate skilled movements through distinct spinal interneurons</article-title><source>Cell Reports</source><volume>23</volume><fpage>1286</fpage><lpage>1300</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2018.03.137</pub-id><pub-id pub-id-type="pmid">29719245</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valero-Cuevas</surname><given-names>FJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A mathematical approach to the mechanical capabilities of limbs and fingers</article-title><source>Advances in Experimental Medicine and Biology</source><volume>629</volume><fpage>619</fpage><lpage>633</lpage><pub-id pub-id-type="doi">10.1007/978-0-387-77064-2_33</pub-id><pub-id pub-id-type="pmid">19227524</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verduzco-Flores</surname><given-names>S</given-names></name><name><surname>De Schutter</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Draculab: a python simulator for firing rate neural networks with delayed adaptive connections</article-title><source>Frontiers in Neuroinformatics</source><volume>13</volume><elocation-id>18</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2019.00018</pub-id><pub-id pub-id-type="pmid">31001101</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verduzco-Flores</surname><given-names>S</given-names></name><name><surname>Dorrell</surname><given-names>W</given-names></name><name><surname>De Schutter</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A differential Hebbian framework for biologically-plausible motor control</article-title><source>Neural Networks</source><volume>150</volume><fpage>237</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2022.03.002</pub-id><pub-id pub-id-type="pmid">35325677</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>H</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Williams</surname><given-names>PR</given-names></name><name><surname>Alwahab</surname><given-names>NSA</given-names></name><name><surname>Kapur</surname><given-names>K</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Ding</surname><given-names>H</given-names></name><name><surname>Gerfen</surname><given-names>CR</given-names></name><name><surname>Wang</surname><given-names>KH</given-names></name><name><surname>He</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Deconstruction of corticospinal circuits for goal-directed motor skills</article-title><source>Cell</source><volume>171</volume><fpage>440</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2017.08.014</pub-id><pub-id pub-id-type="pmid">28942925</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpaw</surname><given-names>JR</given-names></name><name><surname>Kieffer</surname><given-names>VA</given-names></name><name><surname>Seegal</surname><given-names>RF</given-names></name><name><surname>Braitman</surname><given-names>DJ</given-names></name><name><surname>Sanders</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Adaptive plasticity in the spinal stretch reflex</article-title><source>Brain Research</source><volume>267</volume><fpage>196</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(83)91059-4</pub-id><pub-id pub-id-type="pmid">6860948</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpaw</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The complex structure of a simple memory</article-title><source>Trends in Neurosciences</source><volume>20</volume><fpage>588</fpage><lpage>594</lpage><pub-id pub-id-type="doi">10.1016/s0166-2236(97)01133-8</pub-id><pub-id pub-id-type="pmid">9416673</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woods</surname><given-names>SC</given-names></name><name><surname>Ramsay</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Homeostasis: beyond curt Richter</article-title><source>Appetite</source><volume>49</volume><fpage>388</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1016/j.appet.2006.09.015</pub-id><pub-id pub-id-type="pmid">17524521</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yaron</surname><given-names>A</given-names></name><name><surname>Kowalski</surname><given-names>D</given-names></name><name><surname>Yaguchi</surname><given-names>H</given-names></name><name><surname>Takei</surname><given-names>T</given-names></name><name><surname>Seki</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Forelimb force direction and magnitude independently controlled by spinal modules in the macaque</article-title><source>PNAS</source><volume>117</volume><fpage>27655</fpage><lpage>27666</lpage><pub-id pub-id-type="doi">10.1073/pnas.1919253117</pub-id><pub-id pub-id-type="pmid">33060294</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zappacosta</surname><given-names>S</given-names></name><name><surname>Mannella</surname><given-names>F</given-names></name><name><surname>Mirolli</surname><given-names>M</given-names></name><name><surname>Baldassarre</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>General differential Hebbian learning: capturing temporal relations between events in neural networks and the brain</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006227</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006227</pub-id><pub-id pub-id-type="pmid">30153263</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zelenin</surname><given-names>PV</given-names></name><name><surname>Vemula</surname><given-names>MG</given-names></name><name><surname>Lyalka</surname><given-names>VF</given-names></name><name><surname>Kiehn</surname><given-names>O</given-names></name><name><surname>Talpalar</surname><given-names>AE</given-names></name><name><surname>Deliagina</surname><given-names>TG</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Differential contribution of V0 interneurons to execution of rhythmic and nonrhythmic motor behaviors</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>3432</fpage><lpage>3445</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1979-20.2021</pub-id><pub-id pub-id-type="pmid">33637562</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Supplementary discussion</title><sec sec-type="appendix" id="s8-1"><title>Comparison with previous models</title><p>There are many other models of reaching and motor control. Most of them have one or more of the following limitations:</p><list list-type="order"><list-item><p>They use non-neural systems to produce motor commands.</p></list-item><list-item><p>They control a single degree of freedom, sidestepping the problem of controller configuration, since the error is one-dimensional.</p></list-item><list-item><p>They do not model a biologically plausible form of synaptic learning.</p></list-item></list><p>We will contrast our model with some of the work that does not strongly present these limitations, and with a few others. Due to space constraints the contributions of many models will not be addressed, and for those mentioned we will limit ourselves to explain some of their limitations.</p><p>The model in <xref ref-type="bibr" rid="bib31">DeWolf et al., 2016</xref> is similar has similar goals to our model, but with very different assumptions. In their model, motor cortex receives a location error vector <inline-formula><mml:math id="inf436"><mml:mi>x</mml:mi></mml:math></inline-formula>, and transforms it into a vector of joint torques. So that this transformation implements adaptive kinematics, it must approximate a Jacobian matrix that includes the effects of the arm’s inertia matrix, using location errors and joint velocities as training signals. This is accomplished by adapting an algorithm taken from the robotics literature (<xref ref-type="bibr" rid="bib22">Cheah et al., 2006</xref>), implementing it in a spiking neural network. Additionally a second algorithm from robotics (<xref ref-type="bibr" rid="bib92">Sanner and Slotine, 1992</xref>) is used to provide an adaptive dynamics component, which is interpreted as the cerebellar contributions.</p><p>In order to implement vector functions in spiking neural networks, <xref ref-type="bibr" rid="bib31">DeWolf et al., 2016</xref> uses the Neural Engineering Framework (<xref ref-type="bibr" rid="bib10">Bekolay et al., 2014</xref>). The essence of this approach is to represent values in populations of neurons with cosine-like tuning functions. These populations implement expansive recoding, becoming a massively overcomplete basis of the input space. Implementing a function using this population as the input is akin to using a linear decoder to extract the desired function values from the population activity. This can be done through standard methods, such as least-squares minimization, or random gradient descent. The parameters of the linear decoder then become weights of a feedforward neural layer implementing the function.</p><p>The model in <xref ref-type="bibr" rid="bib31">DeWolf et al., 2016</xref> has therefore a rather different approach. They use engineering techniques to create a powerful motor control system, using algorithms from robotics, and 30,000 neurons to approximate their computations, which are then ascribed to sensory, motor, and premotor cortices, as well as the cerebellum. In contrast, we use 74 firing rate units, and unlike (<xref ref-type="bibr" rid="bib31">DeWolf et al., 2016</xref>) we include muscles, muscle afferents, transmission delays, and a spinal cord.</p><p>There is nothing intrinsically wrong with using an engineering approach to try to understand a biological function. The crucial part is which model will be experimentally validated. Some differences between the models that may be able to separate them experimentally are: (1) In <xref ref-type="bibr" rid="bib31">DeWolf et al., 2016</xref> premotor cortex is required to produce the error signal, whereas we ascribed this to sensory cortex. (2) In <xref ref-type="bibr" rid="bib31">DeWolf et al., 2016</xref> direct afferent connections to motor cortex are not considered, whereas in our model they are important to maintain stability during learning (in the absence of a cerebellum). (3) In <xref ref-type="bibr" rid="bib31">DeWolf et al., 2016</xref> spinal cord adaptation is not necessary to implement adaptive kinematics. In contrast, spinal cord adaptation is important in one of the interpretations of our model.</p><p>The model in <xref ref-type="bibr" rid="bib32">Dura-Bernal et al., 2015</xref> uses spiking neurons, and a realistic neuromechanical model in order to perform 2D reaching. The feedback is in term of muscle lengths, rather than muscle afferent signals. There is no mechanism to stop the arm, or hold it on target. Most importantly, learning relies on a critic, sending rewarding or punishing signals depending on whether the hand was approaching or getting away from the target. This is implicitly reducing the error dimension using a hidden mechanism. Furthermore, each single target must be trained individually, and it is not discussed how this can lead to a flexible reaching mechanism without suffering from catastrophic interference.</p><p>The model in <xref ref-type="bibr" rid="bib109">Todorov, 2000</xref> is used to obtain characteristics of M1 activity given the required muscle forces to produce a movement. It is an open-loop, locally-linear model, where all connections from M1 directly stimulate a linear motoneuron. Among other things, it showed that representations of kinematic parameters can appear when the viscoelastic properties of the muscles are taken into account, giving credence to the hypothesis that M1 directly activates muscle groups. Outside of its scope are neural implementation, learning, or the role of spinal cord.</p><p><xref ref-type="bibr" rid="bib63">Li et al., 2005</xref> proposes a 2-level hierarchical controller for a 2-DOF arm. Since this model is based on optimal control theory, it is given a cost function, and proceeds by iteratively approaching a solution of the associated Jacobi-Bellman equation. There is no neural implementation of these computations, nor a description of learning.</p><p><xref ref-type="bibr" rid="bib68">Martin et al., 2009</xref> is not properly a neural model, but it is rooted in Dynamic Field Theory, which assumes that the population of neuronal activities encodes &quot;activation variables&quot;. For this model activation variables represent kinematic parameters as a virtual joint location vector <inline-formula><mml:math id="inf437"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which is an internal representation of the arm’s configuration.</p><p>The innovative part of <xref ref-type="bibr" rid="bib68">Martin et al., 2009</xref> is in describing how <inline-formula><mml:math id="inf438"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is updated and used to control a redundant manipulator. In particular, the kinematic Jacobian, its null-space matrix, their derivatives, and the Moore-Penrose pseudoinverse are all computed analytically in order to obtain differential equations where the joint motions that move the end effector decouple from those which don’t.</p><p>Encapsulating the muscle commands into a virtual joint location whose dynamics are decoupled for motions that don’t affect the end-effector location is a very interesting concept. Still, this is far from a neural implementation, and learning is not considered.</p><p>The model in <xref ref-type="bibr" rid="bib19">Caligiore et al., 2014</xref> studies the long-term development of infant reaching using a PD controller, and an actor critic mechanism implementing a neural version of TD-learning (<xref ref-type="bibr" rid="bib105">Sutton and Barto, 2018</xref>). The 2-dimensional PD controller receives two desired joint angles (interpreted as an equilibrium position), producing appropriate torques. Since it uses the Equilibrium Point (EP) Hypothesis (<xref ref-type="bibr" rid="bib36">Feldman, 1986</xref>), the reaching portion of this model is tantamount to associating states with equilibrium positions. This model thus performs at a higher level of analysis. Our model could operate at the same level if we added a reinforcement learning component to learn <inline-formula><mml:math id="inf439"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> values allowing the hand to <italic>touch</italic> a target whose distance is known. (<xref ref-type="bibr" rid="bib19">Caligiore et al., 2014</xref>) does not consider separate neuronal populations (e.g. spinal cord, sensory cortex), propagation delays, or low-level learning.</p><p>The model in <xref ref-type="bibr" rid="bib49">Izawa et al., 2004</xref> shows how reinforcement learning can be applied to redundant actuators, such as biological arms. It is, however, not a neural model.</p><p>In <xref ref-type="bibr" rid="bib70">Mici et al., 2018</xref>, a neural network produces predictions of visual input in order to deal with temporal delays in a sensorimotor system. The network used for this study uses non-local learning, and adds or destroys nodes as required during its operation. It is thus not biologically-plausible.</p><p><xref ref-type="bibr" rid="bib112">Tsianos et al., 2014</xref> is a reaching model that also considers the spinal cord as a highly-configurable controller. Corticospinal inputs are assumed to be step commands, which means that motor cortex operates in an open-loop configuration. In order to produce reaching based on these constant commands, a biologically-implausible gradient descent mechanism is required, where the same reach is performed for various values of a synaptic weight, keeping the value that led to the best performance. Furthermore, the model learns to reach one target at a time, which would require learning anew when the new target is more than 45 degrees apart.</p><p>As mentioned in the main text, in the context of rotational dynamics, the model in <xref ref-type="bibr" rid="bib104">Sussillo et al., 2015</xref> was used to produce desired muscle forces using a recurrent neural network. This model uses the FORCE algorithm (<xref ref-type="bibr" rid="bib103">Sussillo and Abbott, 2009</xref>) to adjust the weights of a neural network with activity vector <inline-formula><mml:math id="inf440"><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> so it can produce experimentally observed muscle activity <inline-formula><mml:math id="inf441"><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Oscillatory dynamics arise when the model is constrained to be simple.</p><p>Although very insightful, this model is limited by the fact that <xref ref-type="disp-formula" rid="equ2 equ3">Equations 2 and 3</xref> represent an open-loop configuration, where only the M1 dynamics are considered. In essence, the model is doing a function approximation with the FORCE algorithm. The question of how the training data <inline-formula><mml:math id="inf442"><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is produced is not addressed, nor is the role of spinal cord or sensory cortex (but see their Supplementary Figure 1).</p><p>Other than the aforementioned model in <xref ref-type="bibr" rid="bib112">Tsianos et al., 2014</xref>, we are unaware of spinal cord models addressing arm reaching. When these models are coupled with a musculoskeletal system, it is usually for the control of one degree of freedom using antagonistic neural populations. We mention some examples next.</p><p>The spinal cord model in <xref ref-type="bibr" rid="bib7">Bashor, 1998</xref> inspired much of the subsequent work, organizing the spinal circuit into six pairs of populations controlling two antagonistic muscle groups at one joint. With this model, the effect of Ia and Ib afferent input was studied in various neuronal populations.</p><p><xref ref-type="bibr" rid="bib102">Stienen et al., 2007</xref> used a model similar to that in <xref ref-type="bibr" rid="bib7">Bashor, 1998</xref> in conjunction with a one DOF musculoskeletal model to study the effect of Ia afferents in the modulation of reflexes. <xref ref-type="bibr" rid="bib27">Cutsuridis, 2007</xref> also adapted a similar model, and used it to inquire whether Parkinsonian rigidity arose from alterations in reciprocal inhibition mediated by reduced dopamine.</p><p>The model in <xref ref-type="bibr" rid="bib25">Cisi and Kohn, 2008</xref> has several features not found in <xref ref-type="bibr" rid="bib7">Bashor, 1998</xref>, including a heterogeneous motoneuron population, and a mechanism to generate electromyograms. This was used to study the generation of the H-reflex, and response properties of the motor neuron pool (<xref ref-type="bibr" rid="bib35">Farina et al., 2014</xref>).</p><p>The model in <xref ref-type="bibr" rid="bib98">Shevtsova et al., 2015</xref> goes beyond models such as <xref ref-type="bibr" rid="bib7">Bashor, 1998</xref> by using populations of genetically-identified interneurons. This model is used to study rhythm generating abilities of the spinal circuit, as well as coordination between flexors and extensors (<xref ref-type="bibr" rid="bib99">Shevtsova and Rybak, 2016</xref>; <xref ref-type="bibr" rid="bib28">Danner et al., 2017</xref>). Knowledge regarding the role of genetically identified spinal interneurons in movement generation is still evolving (<xref ref-type="bibr" rid="bib123">Zelenin et al., 2021</xref>; <xref ref-type="bibr" rid="bib100">Stachowski and Dougherty, 2021</xref>, e.g.).</p><p>This paper focuses on mammals, but the spinal cord of simpler organisms is better characterized (<xref ref-type="bibr" rid="bib17">Borisyuk et al., 2011</xref>; <xref ref-type="bibr" rid="bib20">Cangiano and Grillner, 2005</xref>, e.g.), and may lead to the first realistic models producing ethological behavior.</p></sec><sec sec-type="appendix" id="s8-2"><title>Possible implementations of the learning rule</title><p>The learning rule in <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> is a Hebbian rule that also presents derivatives, heterosynaptic competition, and normalization (e.g. removing the mean) of its terms. None of these is new in a model claiming biological plausibility (<xref ref-type="bibr" rid="bib84">Porr and Wörgötter, 2006</xref>; <xref ref-type="bibr" rid="bib122">Zappacosta et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Fiete et al., 2010</xref>; <xref ref-type="bibr" rid="bib54">Kaleb et al., 2021</xref>, e.g.). We nevertheless mention possible ways for derivatives and normalized terms to appear.</p><p>Formally, the time derivative of a function <inline-formula><mml:math id="inf443"><mml:mrow><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mrow><mml:mi>ℝ</mml:mi><mml:mo>→</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> evaluated at time <inline-formula><mml:math id="inf444"><mml:mi>t</mml:mi></mml:math></inline-formula> is the limit <inline-formula><mml:math id="inf445"><mml:mfrac><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> as <inline-formula><mml:math id="inf446"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. If <inline-formula><mml:math id="inf447"><mml:mi>f</mml:mi></mml:math></inline-formula> represents the firing rate of a cell, a measure of change roughly proportional to the derivative can come from <inline-formula><mml:math id="inf448"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for some small value <inline-formula><mml:math id="inf449"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>. The most obvious way that such a difference may arise is through feedforward inhibition (for the <italic>e</italic><sub><italic>j</italic></sub> signal), and feedback inhibition (for the <italic>c</italic><sub><italic>i</italic></sub> signal). Feedforward and feedback inhibition are common motifs in spinal circuits (<xref ref-type="bibr" rid="bib83">Pierrot-Deseilligny and Burke, 2005</xref>).</p><p>A somewhat different way to approach a time derivative is by using two low-pass filters with different time constants:<disp-formula id="equ27"><mml:math id="m27"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>≈</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ28"><mml:math id="m28"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mrow><mml:mi/><mml:mo>≫</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>These principles are illustrated in <xref ref-type="bibr" rid="bib65">Lim and Goldman, 2013</xref>, where they are used to explain negative-derivative feedback.</p><p>Low-pass filtering can also arise in the biochemical cascades following synaptic depolarization. The most salient case is intracellular calcium concentration, which has been described as an indicator of firing rate with leaky integrator dynamics (<xref ref-type="bibr" rid="bib48">Helmchen, 1999</xref>). Although the physiology of spinal interneurons has not been characterized with sufficient detail to make specific hypotheses, it is clearly possible that feedback inhibition and low-pass filtering are enough to approximate a second-order derivative.</p><p>The <inline-formula><mml:math id="inf450"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> terms in our learning rule are mean-centered. The most straightforward way to subtract a mean is to have inhibitory units with converging inputs (e.g. receiving all the <italic>e</italic><sub><italic>j</italic></sub> signals) providing input to the <italic>c</italic><sub><italic>i</italic></sub> units. The Ib interneurons (<xref ref-type="bibr" rid="bib83">Pierrot-Deseilligny and Burke, 2005</xref>) are one possibility for mediating this. Another possibility is that the mean-subtraction happens at the single unit level when the input (<italic>e</italic><sub><italic>j</italic></sub>) and lateral (<italic>c</italic><sub><italic>i</italic></sub>) connections are located at different parts of the dendritic tree. In particular, a larger level of overall input activation <inline-formula><mml:math id="inf451"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo>¨</mml:mo></mml:mover><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> could produce a scarcity of postsynaptic resources flowing from the main branches of the dendritic tree into the individual dendritic spines, resulting in reduced plasticity.</p></sec><sec sec-type="appendix" id="s8-3"><title>Limitations of the model</title><p>A model as simple as ours will undoubtedly be wrong in many details. The main simplifying assumptions of our model are:</p><list list-type="bullet"><list-item><p>Firing rate encoding. Each unit in this model captures the mean-field activity of a neuronal population. This may occlude computations depending on spike timing, as well as fine-grained computations at the neuronal and dendritic level.</p></list-item><list-item><p>Trivial sensory cortex. We assumed that sensory cortex conveyed errors directly based on static gamma afferent activity. Sensory cortex may instead build complex representations of afferent activity. It would be interesting to test how the requirement of controllability could guide learning of these representations.</p></list-item><list-item><p>No visual information. Should a visual error be available, it could be treated by <inline-formula><mml:math id="inf452"><mml:mi>M</mml:mi></mml:math></inline-formula> in a similar way to somatosensory errors. If the visual error holds a monotonic relation with the afferent signals, then it should be possible to adjust the long-loop reflex in order to reduce it. When the relation between the visual error and the afferent signals is not monotonic (e.g. in some context the afferent signals correlate positively, and in other negatively), an alternative approach involving reinforcement learning can be pursued (<xref ref-type="bibr" rid="bib116">Verduzco-Flores et al., 2022</xref>).</p></list-item><list-item><p>Very gross anatomical detail. The detailed anatomical organization of cortex and spinal cord is not considered. Moreover, the proportions for different types of cells are not considered.</p></list-item><list-item><p>Errors must be monotonic. Muscle activation may not monotonically reduce visual errors. Moreover, the haptic perception of contact is dependent on the environment, so it would not make an appropriate error signal.</p></list-item><list-item><p>No cerebellum, basal ganglia, brainstem, or premotor cortex.</p></list-item></list><p>Each of these omissions is also a possible route for improving the model. We aim to grow a more sophisticated model, but each new component must integrate with the rest, improve functionality and agree with biological literature.</p><p>A final limitation concerns proofs of convergence. Many factors complicate them for this model: transmission delays, noise, synaptic learning, fully neural implementation, as well as complex muscle and afferent models. We tested our results for many initial conditions, but this of course is no guarantee.</p><p>50 years ago Marr’s model of the cerebellum became a stepping stone to further theoretical and experimental progress, despite all its shortcomings (<xref ref-type="bibr" rid="bib57">Kawato et al., 2021</xref>). We aspire our model to be the next step towards a complete model of motor control.</p></sec></sec><sec sec-type="appendix" id="s9"><title>Variations of the spinal learning model</title><p>The main text mentions two variations of the spinal learning network that emphasize the robustness and potential of the learning mechanism. We will explain the rationale behind those two variations.</p><p>There is evidence for interneurons that drive a set of muscles, possibly setting the circuit foundation for motor synergies (<xref ref-type="bibr" rid="bib43">Giszter, 2015</xref>; <xref ref-type="bibr" rid="bib62">Levine et al., 2014</xref>; <xref ref-type="bibr" rid="bib15">Bizzi and Cheung, 2013</xref>). To explore whether our ideas were compatible with interneurons activating multiple muscles, we explored whether reaching can be learned when the <inline-formula><mml:math id="inf453"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf454"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> units send projections to more than one motoneuron. To achieve this we modified the architecture of <xref ref-type="fig" rid="fig1">Figure 1</xref> so that for every combination of two different muscles there was a pair of <inline-formula><mml:math id="inf455"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> units that stimulated both of them.</p><p>As illustrated in the <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>, from the set of 6 muscles there are 15 combinations of 2 muscles, but 3 of them consist of antagonist pairs. Removing these we are left with 12 pairs of muscles, and for each muscle pair we had a Wilson-Cowan-like <inline-formula><mml:math id="inf456"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> pair sending projections to the alpha motoneurons of both muscles. Furthermore, for each pair of muscles, there is another pair that contains both their antagonists, and we can use this fact to generalize the concept of antagonists when interneurons project to several motoneurons. The <inline-formula><mml:math id="inf457"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> units sent projections to the <inline-formula><mml:math id="inf458"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> units of their antagonists. This organization allowed us to maintain the balance between excitation and inhibition in the network, along with the connectivity motifs used previously.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Modified architecture of the <inline-formula><mml:math id="inf459"><mml:mi>C</mml:mi></mml:math></inline-formula> population.</title><p>Each pair of <inline-formula><mml:math id="inf460"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> units projects to two <inline-formula><mml:math id="inf461"><mml:mi>α</mml:mi></mml:math></inline-formula> motoneurons (top). There are 15 possible pairs of muscles, corresponding to the blue lines for each arm in the figure. Three of the pairs (marked with red crosses) contain antagonist muscles, and are not included. The remaining 12 pairs can be arranged into 2 groups of 6 units each. The units in the group marked with green circles are the antagonists of the units with the same number, marked with pink circles.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-app1-fig1-v2.tif"/></fig><p>Because this model could be considered a proof-of-concept for the compatibility of our learning mechanisms with this particular type of synergies, we refer to this model as the “synergistic” network.</p><p>To introduce the second variation of the spinal learning network, we may notice that in all configurations so far the projections from <inline-formula><mml:math id="inf462"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf463"><mml:mi>M</mml:mi></mml:math></inline-formula> use one-to-one connectivity (each <inline-formula><mml:math id="inf464"><mml:mi>M</mml:mi></mml:math></inline-formula> unit controls the length error of one muscle). Interestingly, this is not necessary. In a second variation of the spinal learning network, dubbed the “mixed errors” network, each unit in <inline-formula><mml:math id="inf465"><mml:mi>M</mml:mi></mml:math></inline-formula> can be driven by a linear combination of <inline-formula><mml:math id="inf466"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> errors.</p><p>To ensure that the information about the <inline-formula><mml:math id="inf467"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> activity was transmitted to <inline-formula><mml:math id="inf468"><mml:mi>M</mml:mi></mml:math></inline-formula>, we based our <inline-formula><mml:math id="inf469"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf470"><mml:mi>M</mml:mi></mml:math></inline-formula> connections on the following 6-dimensional orthogonal matrix:<disp-formula id="equ29"><label>(A1)</label><mml:math id="m29"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>2</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>2</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>2</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The rows of this matrix form an orthogonal basis in <inline-formula><mml:math id="inf471"><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>6</mml:mn></mml:msup></mml:math></inline-formula>, and normalizing each row we obtain a matrix <inline-formula><mml:math id="inf472"><mml:msup><mml:mi>R</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> whose rows form an orthonormal basis. Connections from the 12 <inline-formula><mml:math id="inf473"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> units to the 12 <inline-formula><mml:math id="inf474"><mml:mi>M</mml:mi></mml:math></inline-formula> units used this 12 × 12 matrix:<disp-formula id="equ30"><label>(A2)</label><mml:math id="m30"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mi>R</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:msup><mml:mi>R</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The one-to-one connections from <inline-formula><mml:math id="inf475"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf476"><mml:mi>M</mml:mi></mml:math></inline-formula> used in our models are unrealistic, but the mixed errors network shows that this simplification can be overcome, since all the results of the paper also apply to this variation.</p><p>All numerical tests applied to the 3 configurations in the main text of the paper were also applied to the two variations of the spinal learning model. Results can be seen in this Appendix, in the Comparison of the 5 configurations. <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplements 3</xref> and <xref ref-type="fig" rid="fig2s4">4</xref> illustrate the training phase for the synergistic and mixed error networks.</p><p>We also tested whether the stimulation of an isolated spinal cord produced convergent direction fields in the synergistic network, as was done in the main text for the spinal network <inline-formula><mml:math id="inf477"><mml:mi>C</mml:mi></mml:math></inline-formula> common to the other four configurations. We found that the mean angle difference <inline-formula><mml:math id="inf478"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> between the direction fields <inline-formula><mml:math id="inf479"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf480"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, averaged over the 144 possible <inline-formula><mml:math id="inf481"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> pairs, was 19.8 degrees.</p><p>Randomly choosing pairs <inline-formula><mml:math id="inf482"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf483"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for the stimulation locations lead to a <inline-formula><mml:math id="inf484"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> angle of 72.3 degrees. As before, a bootstrap test showed that this <inline-formula><mml:math id="inf485"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> value is significantly different (<inline-formula><mml:math id="inf486"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Removing the resting field does not alter this result. Moreover, we found no evidence for supralinear summation of force fields in the synergistic network.</p></sec><sec sec-type="appendix" id="s10"><title>The model fails when elements are removed</title><p>Due to the larger number of tests, we only used 5 trials for each configuration in this section. The p values reported in this section come from the one-sided t-test. For brevity, the different configurations of the model will be denoted by numbers in the rest of this Appendix: 1=spinal learning, 2=cortical learning, 3=static network, 4=synergistic network, 5=mixed errors network.</p><p>A model with fully random connectivity and no plasticity has an exceedingly low probability of having an input-output structure leading it to reduce errors. The configurations of the model with plasticity (configurations 1,2,4,5), however, only have random connections at one of the projections in the sensorimotor loop (either from <inline-formula><mml:math id="inf487"><mml:mi>M</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf488"><mml:mi>C</mml:mi></mml:math></inline-formula>, or from <inline-formula><mml:math id="inf489"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf490"><mml:mi>M</mml:mi></mml:math></inline-formula>). This may increase the chance of randomly obtaining a good input-output structure, which could throw the usefulness of the learning rules into question.</p><p>Removing both types of plasticity in configurations 1,2,4,5 impaired reaching in all 5 tests for each configuration, as reflected by the inability to reduce the average error below 10 centimeters in any of the last 4 reaches of the learning phase. This was also true when removing only the plasticity in the connections from <inline-formula><mml:math id="inf491"><mml:mi>M</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf492"><mml:mi>C</mml:mi></mml:math></inline-formula> (configurations 1, 4, 5) or from <inline-formula><mml:math id="inf493"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf494"><mml:mi>M</mml:mi></mml:math></inline-formula> (configuration 2). In each one of the plastic configurations (1,2,4,5) the average error for the last 4 training targets was <inline-formula><mml:math id="inf495"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>22</mml:mn><mml:mo>±</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mn>22</mml:mn><mml:mo>±</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mn>24</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mn>9</mml:mn><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mn>22</mml:mn><mml:mo>±</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> centimeters, which was significantly higher than the case with normal plasticity (<inline-formula><mml:math id="inf496"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for configurations 2,4,5, <inline-formula><mml:math id="inf497"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.028</mml:mn></mml:mrow></mml:math></inline-formula> for configuration 1, where the failed trials were not discarded).</p><p>Removing plasticity in the connections from <inline-formula><mml:math id="inf498"><mml:mi>A</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf499"><mml:mi>C</mml:mi></mml:math></inline-formula> or from <inline-formula><mml:math id="inf500"><mml:mi>A</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf501"><mml:mi>M</mml:mi></mml:math></inline-formula> individually had for the most part no statistically significant effects. Removing plasticity in both connections simultaneously, however, roughly duplicated the error in configurations 2 and 4 during center-out reaching (one-sided t-test, <inline-formula><mml:math id="inf502"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Error may be slightly increased for configuration 1 (the small sample size allowed no strong conclusions), whereas configuration 5 was seemingly unaffected.</p><p>Configurations (1,2,4,5) could still learn to reach after removing the <inline-formula><mml:math id="inf503"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> unit. Configuration 4 roughly duplicated its center-out reaching error (<inline-formula><mml:math id="inf504"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) and its time to initially reach (<inline-formula><mml:math id="inf505"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>.016</mml:mn></mml:mrow></mml:math></inline-formula>). Configuration 1 increased its time to initially reach about 3 times (<inline-formula><mml:math id="inf506"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), and the other two configurations were seemingly unaffected. The <inline-formula><mml:math id="inf507"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> unit was essential for previous, less robust versions of the model.</p><p>Removing noise made learning too slow, to the point where mean error in the last 4 training reaches could not be reduced below 10 cm in any trial for configurations 4, and 5. It was reduced below 10 cm in a single trial for configuration 2. Configuration 1 managed t learn normally. Center-out reaches were not possible in configurations 2, 4, and 5 with mean errors of <inline-formula><mml:math id="inf508"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>15</mml:mn><mml:mo>±</mml:mo><mml:mrow><mml:mn>8</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mn>17</mml:mn><mml:mo>±</mml:mo><mml:mn>11</mml:mn></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mn>9</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> centimeters respectively, at least 3 times larger than the models with noise (<inline-formula><mml:math id="inf509"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Center-out reaches were normal in configuration 1, but the first reach with mean error below 10 centimeters took significantly longer to happen (from 2.5 to 6.4 attempts in average, <inline-formula><mml:math id="inf510"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>).</p><p>Removing Ia and Ib afferents, and instead sending the output of II afferents to <inline-formula><mml:math id="inf511"><mml:mi>C</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf512"><mml:mi>M</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf513"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> prevented reaching in configurations 1, 2, 3, and 4 (except for a single trial in configuration 1). Configuration 5 could still learn to reach, but the mean error in the last 4 training reaches and during the center-out reaching was significantly higher (<inline-formula><mml:math id="inf514"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>Removing the agonist-antagonist connections in <inline-formula><mml:math id="inf515"><mml:mi>C</mml:mi></mml:math></inline-formula> prevented reaching in (5| 0| 0| 3| 2) trials for configurations (1,2,3,4,5) respectively. Error in center-out reaching was significantly increased for configurations 4 and 5, and it did not increase significantly for configurations 2 and 3.</p><p><xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref> is the same as <xref ref-type="fig" rid="fig2">Figure 2</xref> of the main text, but in this case the noise and the ACT units were both removed. The average distance from the hand to the target was roughly 18 cm. A video illustrating this failure to learn is included with the supplementary videos.</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>A failure to learn for a simulation of configuration 1 (spinal learning) with no noise and no ACT unit.</title><p>Captions are as in <xref ref-type="fig" rid="fig2">Figure 2</xref> of the main text.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77216-app1-fig2-v2.tif"/></fig><p>Configuration 1 was resilient to removal of noise and the ACT unit individually. The simulation of <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref> suggests that removing more than one element will have larger consequences on the performance of the model.</p></sec><sec sec-type="appendix" id="s11"><title>Comparison of the 5 configurations</title><p>Once again, the 5 configurations in this paper are represented by a number: 1=spinal learning, 2=cortical learning, 3=static network, 4=synergistic network, 5=mixed errors network. The following table shows the connectivity in each one. Abbreviations: A2A: all-to-all, O2O: one-to-one, DH: the differential Hebbian learning rule from <xref ref-type="bibr" rid="bib116">Verduzco-Flores et al., 2022</xref>, IC: the Input Correlation learning rule, S: static connections (see section for details on the weights of static connections).</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Configuration</th><th align="left" valign="bottom"><inline-formula><mml:math id="inf516"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf517"><mml:mi>M</mml:mi></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf518"><mml:mi>M</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf519"><mml:mi>C</mml:mi></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf520"><mml:mi>A</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf521"><mml:mi>C</mml:mi></mml:math></inline-formula>,<inline-formula><mml:math id="inf522"><mml:mi>M</mml:mi></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">O2O, S</td><td align="left" valign="bottom">A2A, DH</td><td align="left" valign="bottom">A2A, IC</td></tr><tr><td align="char" char="." valign="bottom">2</td><td align="left" valign="bottom">A2A, DH</td><td align="left" valign="bottom">S</td><td align="left" valign="bottom">A2A, IC</td></tr><tr><td align="char" char="." valign="bottom">3</td><td align="left" valign="bottom">O2O, S</td><td align="left" valign="bottom">S</td><td align="left" valign="bottom">S</td></tr><tr><td align="char" char="." valign="bottom">4</td><td align="left" valign="bottom">O2O, S</td><td align="left" valign="bottom">A2A, DH</td><td align="left" valign="bottom">A2A, IC</td></tr><tr><td align="char" char="." valign="bottom">5</td><td align="left" valign="bottom">S</td><td align="left" valign="bottom">A2A, DH</td><td align="left" valign="bottom">A2A, IC</td></tr></tbody></table></table-wrap><list list-type="bullet"><list-item><p>The spinal learning model (configuration 1) is a “basic” network where the input-output structure of the control loop happens in the spinal cord, in the connections from M to C.</p></list-item><list-item><p>The cortical learning model (configuration 2) is also a “basic” network, but the input-output structure is resolved in the intracortical connections from <inline-formula><mml:math id="inf523"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></list-item><list-item><p>The static network (configuration 3) uses only static connections, and is meant to show that the results in the paper appear in a close to optimal configuration of feedback control, rather than being some sophisticated product of the plasticity rules.</p></list-item><list-item><p>The synergistic network (configuration 4) is an extension of configuration 1, where the spinal cord has 12 CE, CI pairs rather than 6, and each pair stimulates 2α motoneurons.</p></list-item><list-item><p>The mixed errors network (configuration 5) is a different variation of configuration 1, where the connections from <inline-formula><mml:math id="inf524"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are not one-to-one, but instead come from an orthogonal matrix.</p></list-item></list><p>The following table summarizes the numerical results for the 5 configurations.</p><table-wrap id="inlinetable2" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Measurement</th><th align="left" valign="bottom">1</th><th align="left" valign="bottom">2</th><th align="left" valign="bottom">3</th><th align="left" valign="bottom">4</th><th align="left" valign="bottom">5</th></tr></thead><tbody><tr><td align="left" valign="bottom">Failed reaches<sup>1</sup></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf525"><mml:mrow><mml:mn>1.8</mml:mn><mml:mo>±</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf526"><mml:mrow><mml:mn>1.2</mml:mn><mml:mo>±</mml:mo><mml:mn>.9</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf527"><mml:mrow><mml:mn>0</mml:mn><mml:mo>±</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf528"><mml:mrow><mml:mn>1.6</mml:mn><mml:mo>±</mml:mo><mml:mn>1.3</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf529"><mml:mrow><mml:mn>4</mml:mn><mml:mo>±</mml:mo><mml:mn>2.5</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Center-out error<sup>2</sup></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf530"><mml:mrow><mml:mn>3.3</mml:mn><mml:mo>±</mml:mo><mml:mn>.01</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf531"><mml:mrow><mml:mn>2.9</mml:mn><mml:mo>±</mml:mo><mml:mn>.001</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf532"><mml:mrow><mml:mn>2.9</mml:mn><mml:mo>±</mml:mo><mml:mn>.0003</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf533"><mml:mrow><mml:mn>3</mml:mn><mml:mo>±</mml:mo><mml:mn>.008</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf534"><mml:mrow><mml:mn>2.8</mml:mn><mml:mo>±</mml:mo><mml:mn>.0007</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf535"><mml:mi>M</mml:mi></mml:math></inline-formula> units tuned</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">to direction</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf536"><mml:mrow><mml:mn>11.8</mml:mn><mml:mo>±</mml:mo><mml:mn>.4</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf537"><mml:mrow><mml:mn>12</mml:mn><mml:mo>±</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf538"><mml:mrow><mml:mn>12</mml:mn><mml:mo>±</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf539"><mml:mrow><mml:mn>12</mml:mn><mml:mo>±</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf540"><mml:mrow><mml:mn>12</mml:mn><mml:mo>±</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf541"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> forfor</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">predicted PD</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf542"><mml:mrow><mml:mn>.74</mml:mn><mml:mo>±</mml:mo><mml:mn>.18</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf543"><mml:mrow><mml:mn>.88</mml:mn><mml:mo>±</mml:mo><mml:mn>.14</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf544"><mml:mrow><mml:mn>.86</mml:mn><mml:mo>±</mml:mo><mml:mn>.01</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf545"><mml:mrow><mml:mn>.89</mml:mn><mml:mo>±</mml:mo><mml:mn>.06</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf546"><mml:mrow><mml:mn>.82</mml:mn><mml:mo>±</mml:mo><mml:mn>.03</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">PD distribution</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">main axis (deg)</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf547"><mml:mrow><mml:mn>59</mml:mn><mml:mo>±</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf548"><mml:mrow><mml:mn>52</mml:mn><mml:mo>±</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf549"><mml:mrow><mml:mn>54</mml:mn><mml:mo>±</mml:mo><mml:mn>.5</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf550"><mml:mrow><mml:mn>60</mml:mn><mml:mo>±</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf551"><mml:mrow><mml:mn>58</mml:mn><mml:mo>±</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">PD drift</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">angle (deg)</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf552"><mml:mrow><mml:mn>3.3</mml:mn><mml:mo>±</mml:mo><mml:mn>2.4</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf553"><mml:mrow><mml:mn>4.9</mml:mn><mml:mo>±</mml:mo><mml:mn>2.1</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf554"><mml:mrow><mml:mn>.3</mml:mn><mml:mo>±</mml:mo><mml:mn>.2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf555"><mml:mrow><mml:mn>1.8</mml:mn><mml:mo>±</mml:mo><mml:mn>1.3</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf556"><mml:mrow><mml:mn>7</mml:mn><mml:mo>±</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Muscle PD</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">drift angle (deg)</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf557"><mml:mrow><mml:mn>3.8</mml:mn><mml:mo>±</mml:mo><mml:mn>2.1</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf558"><mml:mrow><mml:mn>6.4</mml:mn><mml:mo>±</mml:mo><mml:mn>2.9</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf559"><mml:mrow><mml:mn>.2</mml:mn><mml:mo>±</mml:mo><mml:mn>.2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf560"><mml:mrow><mml:mn>11.4</mml:mn><mml:mo>±</mml:mo><mml:mn>15.2</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf561"><mml:mrow><mml:mn>27.7</mml:mn><mml:mo>±</mml:mo><mml:mn>34.5</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Center-out error</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="char" char="." valign="bottom">(10 targets)</td><td align="char" char="." valign="bottom">3</td><td align="left" valign="bottom">3.6</td><td align="char" char="." valign="bottom">2.9</td><td align="char" char="." valign="bottom">2.6</td><td align="left" valign="bottom">4.5</td></tr><tr><td align="left" valign="bottom">Variance in</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">first jPCA</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf562"><mml:mrow><mml:mn>.42</mml:mn><mml:mo>±</mml:mo><mml:mn>.04</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf563"><mml:mrow><mml:mn>.42</mml:mn><mml:mo>±</mml:mo><mml:mn>.04</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf564"><mml:mrow><mml:mn>.46</mml:mn><mml:mo>±</mml:mo><mml:mn>.03</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf565"><mml:mrow><mml:mn>.45</mml:mn><mml:mo>±</mml:mo><mml:mn>.04</mml:mn></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf566"><mml:mrow><mml:mn>.47</mml:mn><mml:mo>±</mml:mo><mml:mn>.07</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Center-out error</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">(light arm)</td><td align="char" char="." valign="bottom">2.5</td><td align="left" valign="bottom">3.2</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">6.1</td><td align="left" valign="bottom">2.9</td></tr><tr><td align="left" valign="bottom">Center-out error</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">(heavy arm)</td><td align="char" char="." valign="bottom">2.4</td><td align="left" valign="bottom">3.3</td><td align="char" char="." valign="bottom">2.9</td><td align="char" char="." valign="bottom">5.6</td><td align="left" valign="bottom">3.2</td></tr></tbody></table></table-wrap><p>Average number of reaches before the first reach when the mean error was below 10 cm. <sup>2</sup> Average distance (in centimeters) between the hand and the target during center-out reaching.</p></sec><sec sec-type="appendix" id="s12"><title>Gain and oscillations</title><p>The gain of a feedback loop describes how the amplitude of the error signal increases as it gets transformed into a control signal sent to the plant. As described in the Methods (section), we used a relatively low number of iterations of an optimization algorithm to find suitable parameters for each configuration of the model. This led to configurations with gains that were coarsely tuned. <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> is analogous to <xref ref-type="fig" rid="fig3">Figure 3</xref>, and shows the hand trajectories right after the optimization algorithm was finished. It can be observed that configurations 2 and 3 were particularly prone to oscillations, and configuration 1 would undershoot many targets.</p><p>To improve performance, as well as to facilitate comparison of the 5 configurations, we adjusted their gains. This involved manually adjusting the slope of the sigmoidal units in populations <inline-formula><mml:math id="inf567"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf568"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula>, until they appeared stable, but on the verge of oscillating (so reaching would be faster). This required from 1 to 3 attempts. The gain of configuration 1 was slightly increased, whereas the gain of configurations 2,3,4 was reduced. Configuration 5 was left with the same parameters.</p><p>The trajectories in panels C and D of <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> are reminiscent of terminal tremors in cerebellar ataxia. An animation showing the movement of the arm for the 5 configurations before gain adjustment is included among the Supplementary Videos. In addition, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplements 2</xref> and <xref ref-type="fig" rid="fig3s3">3</xref> show the error and activity of several units during the training reaches for configurations 3 and 4, analogous to <xref ref-type="fig" rid="fig2">Figure 2</xref>. It can be observed that the oscillations are present in the whole network, suggesting that the control signals are trying to catch up with an error that keeps reversing direction.</p></sec><sec sec-type="appendix" id="s13"><title>Supplementary videos</title><p>To help visualization of the arm’s learning and performance under different conditions, 4 videos were produced. The videos indicate the model configuration using the enumeration from this Appendix: 1=spinal learning, 2=cortical learning, 3=static network, 4=synergistic network, 5=mixed errors network.</p><p>To download these videos, please visit <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/sergio.verduzco/public_materials/-/tree/master/adaptive_plasticity">https://gitlab.com/sergio.verduzco/public_materials/-/tree/master/adaptive_plasticity</ext-link></p><p>The videos’ content is as follows:</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-77216-app1-video1.mp4" id="app1video1"><label>Appendix 1—video 1.</label><caption><title>Visualization of the arm and the muscles during the learning phase for configuration 1.</title><p>Data comes from the simulation shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Speed is roughly 4 X.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-77216-app1-video2.mp4" id="app1video2"><label>Appendix 1—video 2.</label><caption><title>Arm animation of the first 180 seconds of center-out reaching for the 5 configurations.</title><p>Speed is roughly 4 X.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-77216-app1-video3.mp4" id="app1video3"><label>Appendix 1—video 3.</label><caption><title>The learning phase for a simulation with configuration 1.</title><p>Both noise and the unit were removed, reducing exploration and disrupting learning. Data comes from the same simulation in <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>. Speed is 4 X.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-77216-app1-video4.mp4" id="app1video4"><label>Appendix 1—video 4.</label><caption><title>The first 180 seconds of center-out reaching for the 5 configurations before the gains were adjusted.</title><p>Speed is roughly 4 X. Configuration 2 shows target-dependent oscillations after 70 seconds.</p></caption></media></sec><sec sec-type="appendix" id="s14"><title>Parameter values</title><p>Values appear in order for configurations 1–5. A single number means that all configurations use that same value.</p><sec sec-type="appendix" id="s14-1"><title>Unit parameters</title><p>The superscript <inline-formula><mml:math id="inf569"><mml:msup><mml:mi/><mml:mi>x</mml:mi></mml:msup></mml:math></inline-formula> on a population name indicates that a parameter has heterogeneous values. This means that a random value was added to the parameter for each individual unit. This random value comes from a uniform distribution centered at zero, with a width equal to 1% of the reported parameter value.</p><table-wrap id="inlinetable3" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Parameter</th><th align="left" valign="bottom">Equation</th><th align="left" valign="bottom">Population</th><th align="left" valign="bottom">Value</th></tr></thead><tbody><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf570"><mml:msub><mml:mi>τ</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">5</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf571"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">10 [ms]</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf572"><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">20 [ms]</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf573"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">140, 70, 150, 180, 110 [ms]</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf574"><mml:msup><mml:mi>M</mml:mi><mml:mi>x</mml:mi></mml:msup></mml:math></inline-formula></td><td align="left" valign="bottom">50 [ms]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf575"><mml:mi>β</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">6</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf576"><mml:mrow><mml:msup><mml:mi>α</mml:mi><mml:mi>x</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">2</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf577"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">1.63, 1.72, 1.70, 3.38, 1.44</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf578"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">4.0, 3.38, 3.44, 2.46, 3.63</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf579"><mml:msup><mml:mi>M</mml:mi><mml:mi>x</mml:mi></mml:msup></mml:math></inline-formula></td><td align="left" valign="bottom">1.5, 1.5, 2.0, 2.0, 1.17</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf580"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">3.0, 2.2, 2.0, 2.3, 3.0</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf581"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">9</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf582"><mml:mi>η</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">6</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf583"><mml:msup><mml:mi>α</mml:mi><mml:mi>x</mml:mi></mml:msup></mml:math></inline-formula></td><td align="left" valign="bottom">1.1</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf584"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">2.0, 1.93, 2.13, 2.31, 1.67</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf585"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">1.5, 1.41, 1.63, 1.72, 1.7</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf586"><mml:msub><mml:mi>M</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">1.3, 1.96, 0.68, 1.19, 1.38</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf587"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf588"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">0.75</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">units 0,3</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf589"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">.4</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">units 1,2,5</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf590"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">.3</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">unit 4</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf591"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">.25</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf592"><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">.1</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf593"><mml:mi>ς</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">7</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf594"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">0.63, 0, 0, 0.69, 0.72</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf595"><mml:mi>M</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">0, 0.62, 0, 0, 0</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf596"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">8</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf597"><mml:mi>A</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">10 [ms]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf598"><mml:mi>T</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">8</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf599"><mml:mi>A</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">.2</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf600"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> afferents</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf601"><mml:mi>A</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">0</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf602"><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> afferents</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf603"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">12, 13</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf604"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">11 [s]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf605"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">15, 16</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf606"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">.31</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf607"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">16</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf608"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">10 [ms]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf609"><mml:mi>γ</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">16</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf610"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">8</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf611"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">17</td><td align="char" char="." valign="bottom"><inline-formula><mml:math id="inf612"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula> (synapse)</td><td align="left" valign="bottom">20</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s14-2"><title>Learning rules</title><table-wrap id="inlinetable4" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Parameter</th><th align="left" valign="bottom">Equation</th><th align="left" valign="bottom">Value</th></tr></thead><tbody><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf613"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">9</td><td align="left" valign="bottom">0.33, 0.37, 0.15, 0.36, 0.32 [s]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf614"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">500, 0, 0, 500, 500</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf615"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">0, 527, 0, 0, 0</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf616"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">300, 0, 0, 300, 300</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf617"><mml:mi>λ</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">.03</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf618"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">2.52</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf619"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">3.23, 3.19, 2.98, 3.23, 3.23</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf620"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">3.29, 2.14, 1.50, 3.69, 0.57</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf621"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">2.86, 2.86, 1.50, 2.86, 2.86</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf622"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">11</td><td align="left" valign="bottom">26.17</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf623"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">11</td><td align="left" valign="bottom">22.5</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf624"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">11</td><td align="left" valign="bottom">0.85, 1.14, 1, 0.53, 0.53</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf625"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">11</td><td align="left" valign="bottom">1.68, 2, 2, 1.55, 2.88</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf626"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">11</td><td align="left" valign="bottom">.48,.22,.2,.25,.33</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf627"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">11</td><td align="left" valign="bottom">.3,.64,.64,.28,.59</td></tr><tr><td align="left" valign="bottom" colspan="3">1 Constraints in the sum of weights are also used with static connections</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s14-3"><title>Muscles and afferents</title><table-wrap id="inlinetable5" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Parameter</th><th align="left" valign="bottom">Equation</th><th align="left" valign="bottom">Value</th></tr></thead><tbody><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf628"><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">18</td><td align="left" valign="bottom">20 [N/m]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf629"><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">18</td><td align="left" valign="bottom">20 [N/m]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf630"><mml:mi>b</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">18</td><td align="left" valign="bottom">1 [N.s / m]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf631"><mml:mi>g</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">18</td><td align="left" valign="bottom">67.11 [N]</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">muscles 0,3</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf632"><mml:mi>g</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">18</td><td align="left" valign="bottom">.75 [N]</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">muscles 1,2,4,5</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf633"><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">19,20,21</td><td align="left" valign="bottom">2 [N/m]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf634"><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">19,20,21</td><td align="left" valign="bottom">2 [N/m]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf635"><mml:msup><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:math></inline-formula></td><td align="left" valign="bottom">19, 21</td><td align="left" valign="bottom">.5[Ns / m]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf636"><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">19,20,21</td><td align="left" valign="bottom">1 [N/m]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf637"><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">19</td><td align="left" valign="bottom">.2 [N/m]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf638"><mml:msup><mml:mi>b</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula></td><td align="left" valign="bottom">19</td><td align="left" valign="bottom">2[N. s / m]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf639"><mml:msubsup><mml:mi>l</mml:mi><mml:mn>0</mml:mn><mml:mi>s</mml:mi></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">19</td><td align="left" valign="bottom">.7</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf640"><mml:msubsup><mml:mi>l</mml:mi><mml:mn>0</mml:mn><mml:mi>d</mml:mi></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">19</td><td align="left" valign="bottom">.8</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf641"><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">20</td><td align="left" valign="bottom">[7.5, 25, 25, 7.5, 25, 25]<inline-formula><mml:math id="inf642"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">muscles 0–5</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf643"><mml:msubsup><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">20</td><td align="left" valign="bottom">0.1</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf644"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">21</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf645"><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>5.46</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>5.46</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>,</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">muscles 0–5</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf646"><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>5.8</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>5.8</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi/><mml:mo>*</mml:mo><mml:mo>*</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula><break/><inline-formula><mml:math id="inf647"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf648"><mml:msubsup><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">21</td><td align="left" valign="bottom">0.5</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf649"><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">22</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom"><italic>T</italic><sub>0</sub></td><td align="left" valign="bottom">22</td><td align="left" valign="bottom">10 [N]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf650"><mml:msub><mml:mi>τ</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">23</td><td align="left" valign="bottom">50 [ms]</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s14-4"><title>Connection delays and weights</title><p>Connections considered &quot;local&quot; used a delay of 10 [ms], unless those those connections implied a not-modeled disynaptic inhibition. All other connections had a delay of 20 [ms].</p><table-wrap id="inlinetable6" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Source</th><th align="left" valign="bottom">Target</th><th align="left" valign="bottom">Delay</th></tr></thead><tbody><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf651"><mml:mi>A</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf652"><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="char" char="." valign="bottom">20 [ms]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf653"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf654"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf655"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">muscle</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf656"><mml:mi>M</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf657"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf658"><mml:mi>M</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf659"><mml:mi>M</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">afferents</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf660"><mml:mi>A</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf661"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf662"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf663"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf664"><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf665"><mml:mi>A</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf666"><mml:mi>M</mml:mi></mml:math></inline-formula></td><td align="char" char="." valign="bottom">10 [ms]</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf667"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf668"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf669"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf670"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf671"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf672"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf673"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf674"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf675"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf676"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr></tbody></table></table-wrap><p>The next table shows the value of fixed synaptic weights not specified in section. Columns indicate the source of the connection, rows indicate the target. &quot;Aff&quot; stands for the muscle afferents. Potentially plastic connections are marked as &quot;+&quot;. If a connection marked “+” is static in one of the configurations, its weight is determined by the <inline-formula><mml:math id="inf677"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> parameters of <xref ref-type="disp-formula" rid="equ10 equ1">Equations 10, 11</xref>.</p><table-wrap id="inlinetable7" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom"><inline-formula><mml:math id="inf678"><mml:mi>α</mml:mi></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf679"><mml:mi>A</mml:mi></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf680"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf681"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf682"><mml:mi>M</mml:mi></mml:math></inline-formula></th><th align="left" valign="bottom">Aff.</th><th align="left" valign="bottom"><inline-formula><mml:math id="inf683"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf684"><mml:msub><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf685"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf686"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">-1</td><td align="char" char="plus" valign="bottom">+</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf687"><mml:mi>A</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">2 (<inline-formula><mml:math id="inf688"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>),</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">4 (<inline-formula><mml:math id="inf689"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula>)</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf690"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">1</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf691"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="char" char="plus" valign="bottom">+</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf692"><mml:mrow><mml:msup><mml:mn>.5</mml:mn><mml:mi>a</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>.18</mml:mn><mml:mi>b</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf693"><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mn>1.8</mml:mn><mml:mi>c</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula></td><td align="char" char="plus" valign="bottom">+</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf694"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="plus" valign="bottom">+</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf695"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="char" char="plus" valign="bottom">+</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf696"><mml:msup><mml:mn>.5</mml:mn><mml:mi>c</mml:mi></mml:msup></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="char" char="plus" valign="bottom">+</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf697"><mml:mrow><mml:msup><mml:mn>1.83</mml:mn><mml:mi>d</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>.16</mml:mn><mml:mi>e</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf698"><mml:mi>M</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="char" char="plus" valign="bottom">+</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">*</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="plus" valign="bottom">+</td></tr><tr><td align="left" valign="bottom">muscles</td><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf699"><mml:msub><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf700"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">1 or –1</td><td align="char" char="." valign="bottom">1 or –1</td><td align="char" char="." valign="bottom">–1.77</td></tr><tr><td align="left" valign="bottom" colspan="10">a Agonist connections, b Partial agonist connections. c Withing the same triplet. d Antagonist connections. e Partial antagonist connections.<break/>units inhibited their duals with weights that depended on the configuration: −0.93,–0.74, −1.00,–1.14, 0.0.</td></tr></tbody></table></table-wrap><p>All connections whose source is <inline-formula><mml:math id="inf701"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf702"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> have a weight of 1.</p></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.77216.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Gallego</surname><given-names>Juan Álvaro</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Imperial College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>This solid modelling study presents a valuable contribution toward understanding the neural control of movement. The authors show that a minimal model comprising key sensorimotor cortical areas as well as a spinal circuits controlling a limb readily replicates landmark observations from behavioural and electrophysiological studies. This work will be of broad interest to motor control researchers, as well as to neurophysiologists interested in testing the predictions derived from this model.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.77216.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gallego</surname><given-names>Juan Álvaro</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Imperial College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Capogrosso</surname><given-names>Marco</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01an3r305</institution-id><institution>University of Pittsburgh</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Berg</surname><given-names>Rune W</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035b05819</institution-id><institution>University of Copenhagen</institution></institution-wrap><country>Denmark</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Adaptive plasticity in the spinal cord can produce reaching from scratch and reproduces motor cortex directional tuning&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Tamar Makin as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Marco Capogrosso (Reviewer #2); Rune W Berg (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission. The general consensus was that this is an interesting model, which will be a valuable contribution to the literature provided that additional controls and clarifications are provided.</p><p>Below you will find the individual reviewers comments. In addition to these, the editor prepared for the authors a summary of the essential revisions required. This is based on both the individual comments and the extensive following discussion between the reviewers and editor.</p><p>Essential revisions:</p><p>1. The paper focuses on an intriguing new hypothesis: that the spinal cord minimizes an error signal generated in motor cortex. Given the novelty of this hypothesis and its implications for the present manuscript, it is critical that the authors provide additional controls/analyses that specifically address it. These include:</p><p>a) What happens if error minimisation happened in the motor cortex rather than in the spinal cord; would the model still produce reaching movements? Do the cortical neurons look like actual neurons (distribution of directional tuning, rotational dynamics)? Do &quot;synergies&quot; still emerge in the spinal cord?</p><p>b) Does a model with the same hierarchical structure that doesn't perform error minimisation and is still capable of reaching exhibit the properties that the current model does (in terms of motor cortical activity, synergies, etc)?</p><p>2. The authors posit that motor cortex reports and error that is being minimised during learning. Under this assumption, the motor cortical signals should be reduced to zero, which is what indeed seems to happen, e.g., in Figure 2E. Nevertheless, it is known that the activity of motor cortical neurons is not zero during the execution of a correct movement, even in highly trained individuals: e.g., trained monkeys have rotation dynamics when reaching (Churchland Cunningham et al. Nature 2012) or hand cycling (Russo et al. Neuron 2018). How can these observations be reconciled with the current model?</p><p>3. The cerebellum is mentioned a couple of times but not included in the model. Based on the known fact that the cerebellum receives and integrates both massive corticopontine input and spinocerebellar proprioceptive input, its role is hence often assumed to be exactly what the motor cortex does in this model. The rationale behind not letting Cerebellum assume this role, but instead making the motor cortex assume this role, is not clear.</p><p>4. The authors claim that previously known phenomena (convergent force fields and rotational dynamics) &quot;emerge&quot; with their learning model. However, this is not supported by the results. Firstly, when they tested the convergent force field, the spinal cord was separated from the descending and ascending connections with cortex. This means that this observed property is independent of having learned the cortico-spinal connections. In addition, rotational dynamics were almost exclusively observed in the one-to-one cortical coupling, which the authors call a &quot;less-plausible model. Therefore, these phenomena are not an emergent property that results from their learning model. It is necessary to specify what are the essential factors for each observed phenomenon.</p><p>5. This model has attributed all of the learning effects to the synaptic connection between the cortex and spinal cord (descending and ascending connections). However, it has been repeatedly demonstrated that plastic changes within the cortex occur during motor learning (Roth et al. Neuron 2020). How is the physiological relevance of attributing all the learning to descending and ascending pathways justified?</p><p>6. In relation to the previous comment: an equivalent circuit could be achieved by changing the intracortical synaptic connections. Why is the corticospinal connection the sole target of learning, and how do the results change (or remain unchanged) if learning happens in other parts of the circuit?</p><p>7. No model can reproduce the whole CNS. However, the reviewers think that a few key features should be added to the model:</p><p>a) The authors have not included any form of feedback in the spinal cord. Indeed, short-latency stretch reflexes are critical for movement: coupled with motoneurons, they are the basic units to generate movement (Xu, Tianqui et al. PNAS 2018); models of human walking show that gait can emerge only considering sensory inputs to the cord (Song and Geyer J Physiol 2015); and their absence prevents recovery from spinal cord injury (Takeoka, Aya et al. Cell 2014). Ia afferents have perhaps the strongest monosynaptic glutamatergic synaptic input to spinal motoneurons and constitute a primary driver of spinal motoneuron excitability which this model is currently missing. Does adding short-latency feedback change the conclusion of the paper?</p><p>b) As the authors discussed in the manuscript, the model omits many components that are known to be involved in learning and, as a result, the model reproduces the impaired motor control rather than the intact state. However, this prevents us from verifying whether the model properly reproduced the biological motor control. Specifically, they claim that the model reproduced ataxic reaching which resemble to the cerebellum patients. However, it is not surprising that a poorly tuned feedback controller (i.e. PID controller) shows an oscillation. Therefore, it is necessary for the authors to show that their model significantly resembles the biological motor control by comparing to the model which share the hierarchical structures but the error is corrected within the cortex and only motor commands descend to the spinal cord.</p><p>c) 20 % of corticospinal projections to motoneurons innervating the arm and particularly the hand are thought to be monosynaptic. Does their addition change the present results?</p><p>d) The proportions and the connectivity of the network make the model seem less biologically plausible, which should at least be addressed in the text. First, the number of neurons in the populations at various levels seems rather small compared to the immense size of these structures. Why not have more neurons in the networks? This applies to all parts of the model, also to the spinal network. In many species, the ratio of motor neurons to interneurons is about 1:10, yet in this model, it is more like 1:2 (the trio). Also, the connectivity has a bias towards the feedforward network, yet local recurrent connectivity seems to be widely present in the nervous system. The proportions in the connectivity from M to C could also be problematic. All the spinal units (c = [c1, …. ,cN]) receive commands/error signal from M (e = [e1, …. ,eM]). Is M&gt;N? Presuming M is &gt; N this would represent a converging input from M to C, and that is problematic since the corticospinal projections are rather sparse. More likely there is diverging connectivity from M to C. It is unclear to me if this is an important problem, it may not be, but it would be appropriate to address such proportions in the text.</p><p>e) The model does not include any form of &quot;pattern generator&quot;, a limitation the authors are aware of, and denote as the &quot;supraspinal pattern formation&quot; problem. They seem to export this to another area of the brain while the motor cortex-spinal cord loop takes care of an appropriate execution. This needs to be clarified and/or addressed with the model.</p><p>f) It is well-known that the spinal cord autonomously can produce motor behaviors. It is unclear whether C in the model is capable of this.</p><p>g) Regarding the synergistic model, their model has spinal interneurons recruiting two motoneuron pools, which is very different from the muscle synergies observed in animals (d'Avella et al., J Neurosci 2006). It is necessary to justify why this type of synergies should be used here.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The authors aimed to study corticospinal control of reach movements. More specifically they aim to understand how an unsupervised system could learn to reach and whether that system would then show properties observed in experiments. For this, the authors conceived a neural network model with defined hierarchy and architecture coupled to a simple biomechanical arm model in a closed-loop that is capable of learning a reach-out task. Interestingly, the model shows several properties found in experimental data such as directional tuning of motor cortical units as well as discrete force fields in the spinal cord.</p><p>Strengths</p><p>The paper has many strengths. It is rigorous and well written, and while not well articulated in the introduction I completely agree with the authors that computational models are necessary if we want to aim at any variation of the concept of &quot;understanding&quot; in relation to the central nervous system. Therefore their goal is absolutely significant and relevant.</p><p>I also particularly liked the approach to start from specific hypotheses on the structure and function of the different modeled layers while leaving completely free the organization and strength of synapses in the system. This approach is elegant because starting from clear hypotheses it allows to observe what properties emerge &quot;spontaneously&quot; and compare these properties with experimental data.</p><p>The results obtained are reasonable in terms of kinematics and it's interesting to observe the emergence of these properties from an error minimization rule imposed within the spinal cord.</p><p>Weaknesses</p><p>The conclusions that the authors make in the paper are very strong. Given the emergence of certain interesting properties within the nervous system, it is implicit the implication that this model can have some sort of general meaning, and most importantly, that it validates from the theoretical point of view the homeostatic control idea. More specifically, the paper starts with a very strong hypothesis, which is that the spinal cord is minimizing an error signal generated in motor cortex. For an experimental neuroscientist (as myself) this is intriguing but also strikes as a very strong interpretation of the role of what we call motor cortex that for decades has been imagined as an actuator of movements. Because of the importance of the implication I don't think that the properties shown in the paper are sufficient to make such a claim, even if implicit. Therefore I think that some form of control is required. I understand completely that models are hard to build, but I can't escape wondering what would happen if the motor cortex would not be producing an error signal. What if that error minimization happened instead in the motor cortex and not in the spinal cord? Would your model still produce reach movements? Would those cortical neurons show directionality? Would &quot;synergies&quot; still emerge in the cord? I think this is an important point if we're claiming that those properties emerge as a consequence of the fact that the motor cortex is calculating an error and the spinal cord is minimizing that error. Therefore, to validate the main hypothesis of the paper, which is that the motor cortex is calculating an error and the spinal cord is minimizing that error, then I would like to see that a model that does not do that, but it's still capable of reaching while maintaining the same hierarchical architecture would not show the properties that the authors found in their model.</p><p>The second weakness is somehow related to the first. There are certain assumptions on the structure that are not true experimentally and it would be important to incorporate these because they may change the observed properties and could thus be used as further validation.</p><p>The first missing property is direct cortico-spinal control of spinal motoneurons. The authors say in the introduction that the motor cortex does not activate spinal motoneurons directly and only connects to interneurons. Instead, the major difference between primates (including humans) and all the other animals is precisely the existence of direct monosynaptic connections between the motor cortex and muscles of the upper limb (particularly the hand). These monosynaptic components are thought to represent about 20% of the cortico-spinal tract. It would be important to see whether their presence changes the results that they obtained.</p><p>The second more important functional inaccuracy concerns the spinal cord. The authors send their sensory signals to the &quot;thalamus&quot; which then relays them to the &quot;sensory cortex&quot; and &quot;motor cortex&quot;. In the supplementary material they described the fact that they decided not to include what they call &quot;short reflex&quot; that would connect &quot;the thalamus&quot; to the spinal cord, or provide a feedback in some way. Therefore in their model, the spinal cord is not receiving a feedback at all. Unfortunately, this is in stark contrast with the actual structure of the spinal cord. Proprioceptive afferents are pivotal members of the spinal motor infrastructure. They are so important than in simple animals, coupled with motoneurons, they constitute the units that generate movement (Xu, Tianqi, et al. &quot;Descending pathway facilitates undulatory wave propagation in <italic>Caenorhabditis elegans</italic> through gap junctions.&quot; Proceedings of the National Academy of Sciences 115.19 (2018): E4493-E4502.); models of human locomotion show that it is possible to build walking models without pattern generators that only consider sensory inputs int he spinal cord (Song, S., and Geyer, H. (2015). A neural circuitry that emphasizes spinal feedback generates diverse behaviours of human locomotion. J. Physiol. 593, 3493-3511.) and they seem to be critical to direct motor learning and recovery after spinal cord injury (Takeoka, Aya, et al. &quot;Muscle spindle feedback directs locomotor recovery and circuit reorganization after spinal cord injury.&quot; Cell 159.7 (2014): 1626-1639.) Most importantly Ia afferents have perhaps the strongest monosynaptic glutamatergic synaptic input to the spinal motoneurons and constitute a primary driver of spinal motoneuron excitability which this model is currently missing. Additionally, some of the agonist-antagonist relationship that the authors find could be significantly changed by the presence of these afferents because Ia afferents connect not only to homologous motoneurons but up to 60% of motoneurons of synergistic muscles and they inhibit antagonists via Ia inhibitory interneurons (See Moraud E. (2016) Mechanisms Underlying the Neuromodulation of Spinal Circuits for Correcting Gait and Balance Deficits after Spinal Cord Injury, Neuron).</p><p>In consequence, the presence of this &quot;short-feedback&quot; seems paramount to me to have an accurate representation of the spinal cord, and it's important to verify that this structure would not change the main conclusions of the paper.</p><p>Indeed, as the authors have correctly implied, the main difference between the nervous system and an artificial neural network is the underlying defined architecture and hierarchy that must be represented at least for the most important elements. And spinal sensory feedback is a building block of movement and learning. Indeed, completely paralyzed rats are capable of learning different walking patterns when totally disconnected from the brain if excitation is provided by means of electrical stimulation and likely sensory afferents are the driver of this learning (Courtine, Grégoire, et al., &quot;Transformation of nonfunctional spinal circuits into functional states after the loss of brain input.&quot; Nature neuroscience 12.10 (2009): 1333-1342.).</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>In this study, the authors investigate how the motor cortex and spinal cord interact in a long-loop reflex organization, from a computational modeling perspective. The motor cortex represents kinematic aspects, i.e. the actual movement of the hand and arm, as opposed to dynamics, which would involve the forces exerted by the muscles or higher-level parameters of the movement. Nevertheless, to achieve the end goal of a certain movement of the hand, a transformation from actuator (muscle) coordinates has to take place, and this is a challenging process. The authors propose that the motor cortex report an error between the desired and the actual movement, use computational modeling to explain how a flexible error reduction mechanism can be achieved and explained by computational modeling of the interplay between cortex and the spinal cord.</p><p>Strengths: A theoretical analysis of the interplay between the spinal cord and motor cortex has rarely been studied previously especially using the long-loop reflex, dynamics, pattern formation, and plasticity. This theoretical contribution is important, especially in the light of the vast new experimental data that is being generated by e.g. calcium imaging and electrophysiology and the new observations of rotational cortical dynamics. It is also rare that the role of the spinal cord is being incorporated into the executing of movement in parallel with the motor cortex.</p><p>Weaknesses: Some of the assumptions hinges on a flexible error reduction mechanism, which is described in a previous (unpublished) study, which seems somewhat fragile. The model does not really get to the root of the problem of how the nervous system generates the desired movement, which the authors are aware of and call the &quot;supra spinal pattern formation&quot; problem. They seem to export this problem to another area of the brain and describe how the motor cortex-spinal cord loop takes care of an appropriate execution.</p><p>The study evaluates multiple aspects of the model in comparison with experimental observation such as direction tuning curve, rotational dynamics, and learning.</p><p>Overall, this is an interesting and well-executed study.</p><p>Specific comments:</p><p>There are a couple of issues regarding the model that is not so clear (or perhaps well-hidden somewhere in the supplement). It is stated that the role of the motor cortex is to report the &quot;error signal&quot; of the movement, i.e. the signal coming from SPA, which is the difference between the desired movement (SP) and the actual movement (SA). First, if the motor cortex is reporting the error, and this error is being minimized while learning, then the signal in the motor cortex should ideally be reduced to zero, which is also what seems to happen in e.g. Figure 2E. Nevertheless, it is known that activity in the motor cortex is not zero during the execution of correct movement, especially in highly trained individuals, e.g. trained monkeys have rotational dynamics when hand-cycling (Churchland 2012). This also seems to be the case in the current study with directional tuning (Figure 5) and rotation, which appear contradictory to me. This is a bit unclear how this is possible. Second, the cerebellum is mentioned a couple of times but not included in the model. It is known that the cerebellum receives and integrates both massive corticopontine input and spinocerebellar proprioceptive input and the role of the cerebellum is hence often assumed to do exactly what the motor cortex does in this model. The rationale behind not letting Cerebellum assume this role, but instead making the motor cortex assume this role, is not clear to me.</p><p>The current model does not explain what the authors call the &quot;supra spinal pattern formation&quot; problem, i.e. how the motor commands arise in the neuronal networks in the cortex and elsewhere. The problem of motor commands is conveniently exported to the external parameter &quot;SP&quot; module that is generating the intended movement. This is for a good reason since it is indeed a hard problem to explain how motor programs arise. I do not see a solution to this issue, only that it is a limitation to be aware of, which the authors seem to be. Nevertheless, the model proposed by the authors makes an effort to explain some of the dynamics of the different regions involved in properly learning and executing the motor commands.</p><p>I have some concerns regarding the network, the proportions, and the connectivity that make the model seem less biologically plausible, which should at least be addressed in the text. First, the number of neurons in the populations at various levels seems rather small compared to the immense size of these structures. Why not have more neurons in the networks? This applies to all parts of the model, also to the spinal network. In many species, the ratio of motor neurons to interneurons is about 1:10, yet in this model, it is more like 1:2 (the trio). Also, the connectivity has a bias towards the feedforward network, yet local recurrent connectivity seems to be widely present in the nervous system. The proportions in the connectivity from M to C could also be problematic. All the spinal units (c = [c1, …. ,cN]) receive commands/error signal from M (e = [e1, …. ,eM]). Is M&gt;N? Presuming M is &gt; N this would represent a converging input from M to C, and that is problematic since the corticospinal projections are rather sparse. More likely there is diverging connectivity from M to C. It is unclear to me if this is an important problem, it may not be, but it would be appropriate to address such proportions in the text.<italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>This study proposes a new computational model for learning upper limb reaching control. In particular, this study develops a model that satisfies biological plausibility, which has not always been satisfied by previous models: (1) a network of the whole sensorimotor loop, (2) a model using neural elements, (3) learning using local information, and (4) continuous online learning. Specifically, their model showed that learning in the synaptic weights of descending and ascending projections between the cortex and spinal cord can reproduce ataxic reaching often observed in cerebellar patients. Furthermore, various phenomena that have been demonstrated in the motor control field emerge with this model. Based on these results, the authors argue that this is a biologically plausible learning model for animals' reaching control.</p><p>Strength:</p><p>1) This study examines a number of previous motor learning models and defines the constraints to develop a biologically plausible model. Given that there are too many parameters to be considered to build the motor control model, I truly appreciate the authors defining the constraints for a biological motor learning model and clarifying the research goal.</p><p>Weakness:</p><p>1) As the authors discussed in the manuscript, the model omits many components that are known to be involved in learning and, as a result, the model reproduces the impaired motor control rather than the intact state. However, this prevents us from verifying whether the model properly reproduced the biological motor control. Specifically, they claim that the model reproduced ataxic reaching which resemble to the cerebellum patients. However, it is not surprising that a poorly tuned feedback controller (i.e. PID controller) shows an oscillation. Therefore, it is necessary for the authors to show that their model significantly resembles the biological motor control by comparing to the model which shares the hierarchical structures but the error is corrected within the cortex and only motor commands descend to the spinal cord.</p><p>2) This model has attributed all of the learning effects to the synaptic connection between the cortex and spinal cord (descending and ascending connections). However, it has been repeatedly demonstrated that plastic changes within the cortex occur during motor learning [1 and others]. Therefore, the physiological relevance of attributing everything to descending and ascending pathways is questionable.</p><p>[1] Roth, R. H. et al. Cortical Synaptic AMPA Receptor Plasticity during Motor Learning. Neuron 105, 895-908.e5 (2020).</p><p>3) Relating to the previous comment, this model only changed the connection with the spinal cord, but the equivalent circuit can be achieved by changing the intracortical synaptic connection. It is necessary to justify why the connection with the spinal cord should be the sole target of learning.</p><p>4) The authors claim that previously known phenomena (convergent force field and rotational dynamics) &quot;emerge&quot; with this learning model. However, this is not supported by the results. Firstly, when they tested the convergent force field, the spinal cord is separated from the descending and ascending connections with cortex. This means that this observed property is independent of the learning of the cortico-spinal connections. In addition, rotational dynamics is almost exclusively observed in the one-to-one cortical coupling, which the authors call a &quot;less-plausible model. Therefore, these phenomena are not emergent properties as a result of their learning model. It is necessary to specify what are the essential factors for each observed phenomenon.</p><p>5) Regarding the synergistic model, their model has spinal interneurons recruiting two motoneuron pools, which is very different from the muscle synergies observed in animals</p><p>[2]. It is necessary to justify why this type of synergies should be used here.</p><p>[2] d'Avella A, Portone A, Fernandez L, Lacquaniti F (2006) Control of fast-reaching movements by muscle synergy combinations. The Journal of neuroscience: the official journal of the Society for Neuroscience 26:7791-7810</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Self-configuring feedback loops for sensorimotor control&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Tamar Makin (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Although the reviewers were satisfied by the new analyses, they were also unanimous in thinking that the manuscript is hard to follow and that this complexity could make it less broadly appealing. Our recommendation is thus for the authors to try to simplify the text, minimising the mental burden to the reader. The main concerns were:</p><p>1) The five different &quot;configurations&quot; are not well explained/motivated throughout the paper.</p><p>2) Figure captions are too short, which makes it hard to understand the results (e.g., what is Figure 8 about?).</p><p>3) There are too many abbreviations, which again does not help comprehend the text.</p><p>4) The Discussion is hard to follow, foremost because it requires that the readers remember all five configurations, which were presented in the much earlier -and very long- Results section.</p><p>5) The overall logic could be potentially streamlined so the study becomes easier to understand.</p><p>In addition, you may want to consider addressing the comment by Reviewer #4, although it is not mandatory.</p><p><italic>Reviewer #4 (Recommendations for the authors):</italic></p><p>I really appreciate the authors revising the manuscript. The argument is now much clearer. I found that the authors are very open about what neural implementation this motor learning is occurring in, and it seems too early to suggest the neural implementation since the same learning can occur either in the spinal cord or in the cerebral cortex, or perhaps in other structures such as the cerebellum which is omitted by this model.</p><p>As a neurophysiologist, I am curious about what insights this computational model will provide into our understanding of the neural mechanism of motor learning. I understand that the key assumptions of this model are that (1) errors vary monotonically with motor command and (2) differential Hebbian learning at the output connections reduces the errors. The second assumption has already been discussed for possible implementation in the spinal cord. However, the validity of the first assumption is not yet clear. I would like to suggest the authors elaborate on what neural mechanism could transform errors in the multiple sensory systems (e.g visual systems) into the monotonic error such as muscle length. It is also important to mention where the desired somatosensory activity (Sp) comes from.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.77216.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1. The paper focuses on an intriguing new hypothesis: that the spinal cord minimizes an error signal generated in motor cortex. Given the novelty of this hypothesis and its implications for the present manuscript, it is critical that the authors provide additional controls/analyses that specifically address it. These include:</p><p>a) What happens if error minimisation happened in the motor cortex rather than in the spinal cord; would the model still produce reaching movements? Do the cortical neurons look like actual neurons (distribution of directional tuning, rotational dynamics)? Do &quot;synergies&quot; still emerge in the spinal cord?</p><p>b) Does a model with the same hierarchical structure that doesn't perform error minimisation and is still capable of reaching exhibit the properties that the current model does (in terms of motor cortical activity, synergies, etc)?</p></disp-quote><p>a) We have explicitly modeled the scenario where the input-output structure of the controller is resolved in motor cortex. We now emphasize that the results of the paper do not depend on whether this happens in motor cortex or spinal cord, or even on the learning rule. The main ingredients are properly configured feedback control, and a minimally realistic model of the sensorimotor loop. To leave no ambiguity about this point, we include a version of our model with no plasticity (configuration 3).</p><p>b) All the model configurations that we present in the paper show all experimental phenomena. Hopefully this will clarify this point.</p><disp-quote content-type="editor-comment"><p>2. The authors posit that motor cortex reports and error that is being minimised during learning. Under this assumption, the motor cortical signals should be reduced to zero, which is what indeed seems to happen, e.g., in Figure 2E. Nevertheless, it is known that the activity of motor cortical neurons is not zero during the execution of a correct movement, even in highly trained individuals: e.g., trained monkeys have rotation dynamics when reaching (Churchland Cunningham et al. Nature 2012) or hand cycling (Russo et al. Neuron 2018). How can these observations be reconciled with the current model?</p></disp-quote><p>The error being minimized does not entail that motor cortex activity disappears. We expand on this point on our reply to reviewer 2.</p><disp-quote content-type="editor-comment"><p>3. The cerebellum is mentioned a couple of times but not included in the model. Based on the known fact that the cerebellum receives and integrates both massive corticopontine input and spinocerebellar proprioceptive input, its role is hence often assumed to be exactly what the motor cortex does in this model. The rationale behind not letting Cerebellum assume this role, but instead making the motor cortex assume this role, is not clear.</p></disp-quote><p>The role assigned to learning in this model, and the role typically ascribed to the cerebellum are not actually the same.</p><p>Please see our reply to reviewer 2.</p><disp-quote content-type="editor-comment"><p>4. The authors claim that previously known phenomena (convergent force fields and rotational dynamics) &quot;emerge&quot; with their learning model. However, this is not supported by the results. Firstly, when they tested the convergent force field, the spinal cord was separated from the descending and ascending connections with cortex. This means that this observed property is independent of having learned the cortico-spinal connections. In addition, rotational dynamics were almost exclusively observed in the one-to-one cortical coupling, which the authors call a &quot;less-plausible model. Therefore, these phenomena are not an emergent property that results from their learning model. It is necessary to specify what are the essential factors for each observed phenomenon.</p></disp-quote><p>We did not intend to claim that the phenomena in the paper arise from learning, and we hope that the current manuscript is clear about this point. In the case of an isolated spinal cord the synapses that learn are not present, so the phenomena is clearly an effect of the musculoskeletal model, and of the structure in the spinal cord circuit. We also try to make clearer that the rotational dynamics emerge for all the included configurations of the model.</p><disp-quote content-type="editor-comment"><p>5. This model has attributed all of the learning effects to the synaptic connection between the cortex and spinal cord (descending and ascending connections). However, it has been repeatedly demonstrated that plastic changes within the cortex occur during motor learning (Roth et al. Neuron 2020). How is the physiological relevance of attributing all the learning to descending and ascending pathways justified?</p></disp-quote><p>As clarified in our responses to reviewers 2 and 3, we do not claim that all of the learning is being done in spinal cord. We expect many types of learning to happen along the sensorimotor loop; our model is only addressing how the structure of the feedback controller is obtained. Moreover, we remain open to this learning happening in the motor cortex.</p><disp-quote content-type="editor-comment"><p>6. In relation to the previous comment: an equivalent circuit could be achieved by changing the intracortical synaptic connections. Why is the corticospinal connection the sole target of learning, and how do the results change (or remain unchanged) if learning happens in other parts of the circuit?</p></disp-quote><p>We have addressed this by introducing the version of our model where learning happens in the connections from SPA to M (configuration 2).</p><disp-quote content-type="editor-comment"><p>7. No model can reproduce the whole CNS. However, the reviewers think that a few key features should be added to the model:</p><p>a) The authors have not included any form of feedback in the spinal cord. Indeed, short-latency stretch reflexes are critical for movement: coupled with motoneurons, they are the basic units to generate movement (Xu, Tianqui et al. PNAS 2018); models of human walking show that gait can emerge only considering sensory inputs to the cord (Song and Geyer J Physiol 2015); and their absence prevents recovery from spinal cord injury (Takeoka, Aya et al. Cell 2014). Ia afferents have perhaps the strongest monosynaptic glutamatergic synaptic input to spinal motoneurons and constitute a primary driver of spinal motoneuron excitability which this model is currently missing. Does adding short-latency feedback change the conclusion of the paper?</p></disp-quote><p>We have now included feedback from the muscle afferents to the spinal cord.</p><disp-quote content-type="editor-comment"><p>b) As the authors discussed in the manuscript, the model omits many components that are known to be involved in learning and, as a result, the model reproduces the impaired motor control rather than the intact state. However, this prevents us from verifying whether the model properly reproduced the biological motor control. Specifically, they claim that the model reproduced ataxic reaching which resemble to the cerebellum patients. However, it is not surprising that a poorly tuned feedback controller (i.e. PID controller) shows an oscillation. Therefore, it is necessary for the authors to show that their model significantly resembles the biological motor control by comparing to the model which share the hierarchical structures but the error is corrected within the cortex and only motor commands descend to the spinal cord.</p></disp-quote><p>As commented to reviewer 3, we hopefully have gone beyond showing that a feedback controller shows an oscillation when explaining common points between the model and qualitative traits of ataxic movements.</p><disp-quote content-type="editor-comment"><p>c) 20 % of corticospinal projections to motoneurons innervating the arm and particularly the hand are thought to be monosynaptic. Does their addition change the present results?</p></disp-quote><p>We have included projections with plastic synapses from motor cortex to the α motoneurons to illustrate that this does not change the results.</p><disp-quote content-type="editor-comment"><p>d) The proportions and the connectivity of the network make the model seem less biologically plausible, which should at least be addressed in the text. First, the number of neurons in the populations at various levels seems rather small compared to the immense size of these structures. Why not have more neurons in the networks? This applies to all parts of the model, also to the spinal network. In many species, the ratio of motor neurons to interneurons is about 1:10, yet in this model, it is more like 1:2 (the trio). Also, the connectivity has a bias towards the feedforward network, yet local recurrent connectivity seems to be widely present in the nervous system. The proportions in the connectivity from M to C could also be problematic. All the spinal units (c = [c1, …. ,cN]) receive commands/error signal from M (e = [e1, …. ,eM]). Is M&gt;N? Presuming M is &gt; N this would represent a converging input from M to C, and that is problematic since the corticospinal projections are rather sparse. More likely there is diverging connectivity from M to C. It is unclear to me if this is an important problem, it may not be, but it would be appropriate to address such proportions in the text.</p></disp-quote><p>We address these limitations in the paper, and in our response to reviewer 2.</p><disp-quote content-type="editor-comment"><p>e) The model does not include any form of &quot;pattern generator&quot;, a limitation the authors are aware of, and denote as the &quot;supraspinal pattern formation&quot; problem. They seem to export this to another area of the brain while the motor cortex-spinal cord loop takes care of an appropriate execution. This needs to be clarified and/or addressed with the model.</p></disp-quote><p>As clarified in the paper, generation of desired sensory perceptions is beyond the scope of this paper, but an approach is outlined in our previous work (reference 11).</p><disp-quote content-type="editor-comment"><p>f) It is well-known that the spinal cord autonomously can produce motor behaviors. It is unclear whether C in the model is capable of this.</p></disp-quote><p>We show that an isolated spinal cord has convergent force fields, which would produce a rudimentary type of reaching. Beyond this we don’t believe our spinal cord model can do much.</p><disp-quote content-type="editor-comment"><p>g) Regarding the synergistic model, their model has spinal interneurons recruiting two motoneuron pools, which is very different from the muscle synergies observed in animals (d'Avella et al., J Neurosci 2006). It is necessary to justify why this type of synergies should be used here.</p></disp-quote><p>As explained to reviewer 3, we used a natural proof of principle for the viability of Motor Synergy Encoders (reference 56). Whether the model can express other type of synergies is an interesting avenue for future research.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>The authors aimed to study corticospinal control of reach movements. More specifically they aim to understand how an unsupervised system could learn to reach and whether that system would then show properties observed in experiments. For this, the authors conceived a neural network model with defined hierarchy and architecture coupled to a simple biomechanical arm model in a closed-loop that is capable of learning a reach-out task. Interestingly, the model shows several properties found in experimental data such as directional tuning of motor cortical units as well as discrete force fields in the spinal cord.</p><p>Strengths</p><p>The paper has many strengths. It is rigorous and well written, and while not well articulated in the introduction I completely agree with the authors that computational models are necessary if we want to aim at any variation of the concept of &quot;understanding&quot; in relation to the central nervous system. Therefore their goal is absolutely significant and relevant.</p><p>I also particularly liked the approach to start from specific hypotheses on the structure and function of the different modeled layers while leaving completely free the organization and strength of synapses in the system. This approach is elegant because starting from clear hypotheses it allows to observe what properties emerge &quot;spontaneously&quot; and compare these properties with experimental data.</p><p>The results obtained are reasonable in terms of kinematics and it's interesting to observe the emergence of these properties from an error minimization rule imposed within the spinal cord.</p><p>Weaknesses</p><p>The conclusions that the authors make in the paper are very strong. Given the emergence of certain interesting properties within the nervous system, it is implicit the implication that this model can have some sort of general meaning, and most importantly, that it validates from the theoretical point of view the homeostatic control idea. More specifically, the paper starts with a very strong hypothesis, which is that the spinal cord is minimizing an error signal generated in motor cortex. For an experimental neuroscientist (as myself) this is intriguing but also strikes as a very strong interpretation of the role of what we call motor cortex that for decades has been imagined as an actuator of movements. Because of the importance of the implication I don't think that the properties shown in the paper are sufficient to make such a claim, even if implicit. Therefore I think that some form of control is required. I understand completely that models are hard to build, but I can't escape wondering what would happen if the motor cortex would not be producing an error signal. What if that error minimization happened instead in the motor cortex and not in the spinal cord? Would your model still produce reach movements? Would those cortical neurons show directionality? Would &quot;synergies&quot; still emerge in the cord? I think this is an important point if we're claiming that those properties emerge as a consequence of the fact that the motor cortex is calculating an error and the spinal cord is minimizing that error. Therefore, to validate the main hypothesis of the paper, which is that the motor cortex is calculating an error and the spinal cord is minimizing that error, then I would like to see that a model that does not do that, but it's still capable of reaching while maintaining the same hierarchical architecture would not show the properties that the authors found in their model.</p></disp-quote><p>Thank you for the various observations up to this point. The question of motor cortex as a generator of errors or as an actuator is a subtle one. We want to raise the possibility that the spinal cord could be an actuator, but as we make now clear, this is not the central theme of the paper. We have restructured the paper to clarify this.</p><p>The paper makes two main points: (1) feedback control with a realistic enough musculoskeletal/neural model can explain many experimental phenomena, and (2) This feedback control can be adaptively configured through differential Hebbian learning rules. To leave no room for misunderstanding we added two new configurations. One of these has no learning at all, it is just a feedback controller (configuration 3). The other one (configuration 2) has the learning in motor cortex rather than the spinal cord. All the results hold for both of these configurations.</p><disp-quote content-type="editor-comment"><p>The second weakness is somehow related to the first. There are certain assumptions on the structure that are not true experimentally and it would be important to incorporate these because they may change the observed properties and could thus be used as further validation.</p><p>The first missing property is direct cortico-spinal control of spinal motoneurons. The authors say in the introduction that the motor cortex does not activate spinal motoneurons directly and only connects to interneurons. Instead, the major difference between primates (including humans) and all the other animals is precisely the existence of direct monosynaptic connections between the motor cortex and muscles of the upper limb (particularly the hand). These monosynaptic components are thought to represent about 20% of the cortico-spinal tract. It would be important to see whether their presence changes the results that they obtained.</p></disp-quote><p>We did not mean to convey that direct motoneuron connections are entirely absent. The point that we want to convey is that the most dominant form of connectivity is through interneuron connections. As far as we know, direct connections are found in advanced primates, and mostly for distal joints, accounting more for manual dexterity than for reaching.</p><p>The good news is that our learning rule works well after introducing these direct connections, which are now part of the model.</p><disp-quote content-type="editor-comment"><p>The second more important functional inaccuracy concerns the spinal cord. The authors send their sensory signals to the &quot;thalamus&quot; which then relays them to the &quot;sensory cortex&quot; and &quot;motor cortex&quot;. In the supplementary material they described the fact that they decided not to include what they call &quot;short reflex&quot; that would connect &quot;the thalamus&quot; to the spinal cord, or provide a feedback in some way. Therefore in their model, the spinal cord is not receiving a feedback at all. Unfortunately, this is in stark contrast with the actual structure of the spinal cord. Proprioceptive afferents are pivotal members of the spinal motor infrastructure. They are so important than in simple animals, coupled with motoneurons, they constitute the units that generate movement (Xu, Tianqi, et al. &quot;Descending pathway facilitates undulatory wave propagation in <italic>Caenorhabditis elegans</italic> through gap junctions.&quot; Proceedings of the National Academy of Sciences 115.19 (2018): E4493-E4502.); models of human locomotion show that it is possible to build walking models without pattern generators that only consider sensory inputs int he spinal cord (Song, S., and Geyer, H. (2015). A neural circuitry that emphasizes spinal feedback generates diverse behaviours of human locomotion. J. Physiol. 593, 3493-3511.) and they seem to be critical to direct motor learning and recovery after spinal cord injury (Takeoka, Aya, et al. &quot;Muscle spindle feedback directs locomotor recovery and circuit reorganization after spinal cord injury.&quot; Cell 159.7 (2014): 1626-1639.) Most importantly Ia afferents have perhaps the strongest monosynaptic glutamatergic synaptic input to the spinal motoneurons and constitute a primary driver of spinal motoneuron excitability which this model is currently missing. Additionally, some of the agonist-antagonist relationship that the authors find could be significantly changed by the presence of these afferents because Ia afferents connect not only to homologous motoneurons but up to 60% of motoneurons of synergistic muscles and they inhibit antagonists via Ia inhibitory interneurons (See Moraud E. (2016) Mechanisms Underlying the Neuromodulation of Spinal Circuits for Correcting Gait and Balance Deficits after Spinal Cord Injury, Neuron).</p><p>In consequence, the presence of this &quot;short-feedback&quot; seems paramount to me to have an accurate representation of the spinal cord, and it's important to verify that this structure would not change the main conclusions of the paper.</p><p>Indeed, as the authors have correctly implied, the main difference between the nervous system and an artificial neural network is the underlying defined architecture and hierarchy that must be represented at least for the most important elements. And spinal sensory feedback is a building block of movement and learning. Indeed, completely paralyzed rats are capable of learning different walking patterns when totally disconnected from the brain if excitation is provided by means of electrical stimulation and likely sensory afferents are the driver of this learning (Courtine, Grégoire, et al., &quot;Transformation of nonfunctional spinal circuits into functional states after the loss of brain input.&quot; Nature neuroscience 12.10 (2009): 1333-1342.).</p></disp-quote><p>This is a very important observation. We agree that there is an important role to be played by the connections from muscle afferents to interneurons. To address this omission we have included these connections, under the hypothesis that they play a role similar to connections from afferents to motor cortex, namely, predictive error reduction.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>In this study, the authors investigate how the motor cortex and spinal cord interact in a long-loop reflex organization, from a computational modeling perspective. The motor cortex represents kinematic aspects, i.e. the actual movement of the hand and arm, as opposed to dynamics, which would involve the forces exerted by the muscles or higher-level parameters of the movement. Nevertheless, to achieve the end goal of a certain movement of the hand, a transformation from actuator (muscle) coordinates has to take place, and this is a challenging process. The authors propose that the motor cortex report an error between the desired and the actual movement, use computational modeling to explain how a flexible error reduction mechanism can be achieved and explained by computational modeling of the interplay between cortex and the spinal cord.</p><p>Strengths: A theoretical analysis of the interplay between the spinal cord and motor cortex has rarely been studied previously especially using the long-loop reflex, dynamics, pattern formation, and plasticity. This theoretical contribution is important, especially in the light of the vast new experimental data that is being generated by e.g. calcium imaging and electrophysiology and the new observations of rotational cortical dynamics. It is also rare that the role of the spinal cord is being incorporated into the executing of movement in parallel with the motor cortex.</p><p>Weaknesses: Some of the assumptions hinges on a flexible error reduction mechanism, which is described in a previous (unpublished) study, which seems somewhat fragile. The model does not really get to the root of the problem of how the nervous system generates the desired movement, which the authors are aware of and call the &quot;supra spinal pattern formation&quot; problem. They seem to export this problem to another area of the brain and describe how the motor cortex-spinal cord loop takes care of an appropriate execution.</p><p>The study evaluates multiple aspects of the model in comparison with experimental observation such as direction tuning curve, rotational dynamics, and learning.</p><p>Overall, this is an interesting and well-executed study.</p></disp-quote><p>Thank you for your kind comments. Regarding the weaknesses of the study, I would like to reiterate my initial reply to the reviewer 1: a central theme of this is study is the explanatory power arising from feedback control and a full model of the sensorimotor loop. This is independent of the learning mechanism. Also, our previous study is now published (reference 11 in the manuscript).</p><disp-quote content-type="editor-comment"><p>Specific comments:</p><p>There are a couple of issues regarding the model that is not so clear (or perhaps well-hidden somewhere in the supplement). It is stated that the role of the motor cortex is to report the &quot;error signal&quot; of the movement, i.e. the signal coming from SPA, which is the difference between the desired movement (SP) and the actual movement (SA). First, if the motor cortex is reporting the error, and this error is being minimized while learning, then the signal in the motor cortex should ideally be reduced to zero, which is also what seems to happen in e.g. Figure 2E. Nevertheless, it is known that activity in the motor cortex is not zero during the execution of correct movement, especially in highly trained individuals, e.g. trained monkeys have rotational dynamics when hand-cycling (Churchland 2012). This also seems to be the case in the current study with directional tuning (Figure 5) and rotation, which appear contradictory to me. This is a bit unclear how this is possible. Second, the cerebellum is mentioned a couple of times but not included in the model. It is known that the cerebellum receives and integrates both massive corticopontine input and spinocerebellar proprioceptive input and the role of the cerebellum is hence often assumed to do exactly what the motor cortex does in this model. The rationale behind not letting Cerebellum assume this role, but instead making the motor cortex assume this role, is not clear to me.</p></disp-quote><p>We did not wish to convey the idea that the role of motor cortex is to convey an error, serving as some sort of rely station, and we apologize if that’s the case. Surely, there is more interesting learning being done in motor cortex, and we try to remain open to that possibility. This is one of the reasons for having a configuration of the model where motor cortex neurons are driven by more than one error signal. This can also observed in the fact that in our model motor cortex mixes error and afferent signals to produce a simple form of predictive control.</p><p>In the current manuscript we try to clarify that even if the error is reduced to zero the activity in motor cortex does not disappear. Activity in motor cortex can be very high, but if excitation and inhibition are balanced in the spinal cord movement will not be generated. Since there are “dual” subpopulations in M, high homogeneous activity may equally activate excitatory and inhibitory interneurons. A careful look at figures 2, and S1-S4, S8, S9 reveals that the M activity is far from ever being zero, and considering that the firing rate units represent averages over a population, some individual neurons could have very high activity in an equivalent spiking model. For all configurations the lowest stationary activity in M oscillates around 0.2 (figure S4).</p><p>The results with directional tuning and rotation come from the fact that large errors, such as those happening after the presentation of a new target (panel D, figures 2, S1-S4), cause a sudden increase in activity from certain M units. This activity quickly returns to baseline level, as the error is reduced.</p><p>We believe that the role being ascribed to learning in this system, and the role typically assumed from cerebellum have some subtle distinctions. In our model learning is meant to determine the input-output structure of the system: which muscle contracts in response to an error. For illustration purposes we may think of the classic vestibulo-ocular reflex. Rotation of the head in one direction causes rotation of the eyeball in the opposite direction to stabilize gaze. The inputoutput structure of this system is determined: the head and the eyeball rotate in opposite directions. The degree of this reflex rotation, however, is adaptive, and the cerebellum plays a role here. It is generally thought that the cerebellum creates an internal model of the system being controlled, permitting model-predictive control, and thus increasing performance of the controller. Most cerebellar models following this paradigm assume that a feedback control system with a good-enough configuration is already in place.</p><disp-quote content-type="editor-comment"><p>The current model does not explain what the authors call the &quot;supra spinal pattern formation&quot; problem, i.e. how the motor commands arise in the neuronal networks in the cortex and elsewhere. The problem of motor commands is conveniently exported to the external parameter &quot;SP&quot; module that is generating the intended movement. This is for a good reason since it is indeed a hard problem to explain how motor programs arise. I do not see a solution to this issue, only that it is a limitation to be aware of, which the authors seem to be. Nevertheless, the model proposed by the authors makes an effort to explain some of the dynamics of the different regions involved in properly learning and executing the motor commands.</p></disp-quote><p>This is indeed a difficult problem. We believe that “exporting” the problem to finding a desired set of perceptions “SP” offers an advantage. First, it partially solves the part of creating the spatiotemporal pattern of neuronal activity producing meaningful movement: this arises from the feedback control. Secondly, we believe that finding appropriate “SP” patterns is a task that could be solved with the right hierarchical architecture. More details are in our previous paper (reference 11).</p><disp-quote content-type="editor-comment"><p>I have some concerns regarding the network, the proportions, and the connectivity that make the model seem less biologically plausible, which should at least be addressed in the text. First, the number of neurons in the populations at various levels seems rather small compared to the immense size of these structures. Why not have more neurons in the networks? This applies to all parts of the model, also to the spinal network. In many species, the ratio of motor neurons to interneurons is about 1:10, yet in this model, it is more like 1:2 (the trio). Also, the connectivity has a bias towards the feedforward network, yet local recurrent connectivity seems to be widely present in the nervous system. The proportions in the connectivity from M to C could also be problematic. All the spinal units (c = [c1, …. ,cN]) receive commands/error signal from M (e = [e1, …. ,eM]). Is M&gt;N? Presuming M is &gt; N this would represent a converging input from M to C, and that is problematic since the corticospinal projections are rather sparse. More likely there is diverging connectivity from M to C. It is unclear to me if this is an important problem, it may not be, but it would be appropriate to address such proportions in the text.</p></disp-quote><p>The reviewer is correct that there is a vast gap between the numbers (and proportions) in the real brain, and those in our model. Despite being a “minimal” model, our codebase is rather large, and simulation times are borderline problematic. Simulations including time delays, multiple regions (in a closed loop with a physical plant), and multiple forms of plasticity tend to require care. We don’t include more neurons because we don’t want to increase complexity without a strong motivation. Given that our firing rate units represent the average activity of a population, for a large number of units the net effect of different population sizes can be reflected by the strength of the connectivity, unless there are significant dynamics beyond those provided by the E-I circuit. Still this is a limitation of the model, and we expose it in the paper (line 1634).</p><p>This observation also applies to our M to C connections. Interestingly, our simulations with plasticity in M to C begin with random full connectivity, but only have a few strong connections at the end. This result is not presented to avoid further bloating of the paper.</p><p>There is indeed a bias towards the feedforward network. We also include ascending connections from A to C and M (creating three separate loops), and lateral connections in M, SPA, and C. Our lateral connections in cortex have the role of supporting the emergence of “dual” representations. Surely there are more subtle roles for recurrent and lateral connections, but in this paper we remain agnostic about them. Once more, we don’t introduce elements unrelated to the goals of the model.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>This study proposes a new computational model for learning upper limb reaching control. In particular, this study develops a model that satisfies biological plausibility, which has not always been satisfied by previous models: (1) a network of the whole sensorimotor loop, (2) a model using neural elements, (3) learning using local information, and (4) continuous online learning. Specifically, their model showed that learning in the synaptic weights of descending and ascending projections between the cortex and spinal cord can reproduce ataxic reaching often observed in cerebellar patients. Furthermore, various phenomena that have been demonstrated in the motor control field emerge with this model. Based on these results, the authors argue that this is a biologically plausible learning model for animals' reaching control.</p><p>Strength:</p><p>1) This study examines a number of previous motor learning models and defines the constraints to develop a biologically plausible model. Given that there are too many parameters to be considered to build the motor control model, I truly appreciate the authors defining the constraints for a biological motor learning model and clarifying the research goal.</p><p>Weakness:</p><p>1) As the authors discussed in the manuscript, the model omits many components that are known to be involved in learning and, as a result, the model reproduces the impaired motor control rather than the intact state. However, this prevents us from verifying whether the model properly reproduced the biological motor control. Specifically, they claim that the model reproduced ataxic reaching which resemble to the cerebellum patients. However, it is not surprising that a poorly tuned feedback controller (i.e. PID controller) shows an oscillation. Therefore, it is necessary for the authors to show that their model significantly resembles the biological motor control by comparing to the model which shares the hierarchical structures but the error is corrected within the cortex and only motor commands descend to the spinal cord.</p></disp-quote><p>The qualitative characteristics of reaching in cerebellar patients have been studied for many years. It is indeed natural to think of tremors as faulty feedback control; Norbert Wiener (of Cybernetics fame) and many others were already thinking this way decades ago. We tried to go beyond this by: (1) the use of a closed-loop neuromusculoskeletal model, and (2) compiling and comparing the (somewhat sparse) ataxia literature relevant to our testing conditions.</p><p>A main point of the paper (emphasized in this new submission) is that feedback control is a powerful hypothesis, often forgotten by people explaining phenomena in motor control. We now include a model where the error is corrected within cortex, but the point still remains.</p><disp-quote content-type="editor-comment"><p>2) This model has attributed all of the learning effects to the synaptic connection between the cortex and spinal cord (descending and ascending connections). However, it has been repeatedly demonstrated that plastic changes within the cortex occur during motor learning [1 and others]. Therefore, the physiological relevance of attributing everything to descending and ascending pathways is questionable.</p><p>[1] Roth, R. H. et al. Cortical Synaptic AMPA Receptor Plasticity during Motor Learning. Neuron 105, 895-908.e5 (2020).</p></disp-quote><p>We did not intent to attribute everything to ascending and descending pathways. As mentioned to reviewer 2, we remain open to other type of learning in motor cortex, and as should be evident in this new version of the paper, we also remain open to learning configuration of the feedback control in motor cortex.</p><disp-quote content-type="editor-comment"><p>3) Relating to the previous comment, this model only changed the connection with the spinal cord, but the equivalent circuit can be achieved by changing the intracortical synaptic connection. It is necessary to justify why the connection with the spinal cord should be the sole target of learning.</p></disp-quote><p>To address this issue we have introduced the configuration 2 of the model, which does precisely this.</p><disp-quote content-type="editor-comment"><p>4) The authors claim that previously known phenomena (convergent force field and rotational dynamics) &quot;emerge&quot; with this learning model. However, this is not supported by the results. Firstly, when they tested the convergent force field, the spinal cord is separated from the descending and ascending connections with cortex. This means that this observed property is independent of the learning of the cortico-spinal connections. In addition, rotational dynamics is almost exclusively observed in the one-to-one cortical coupling, which the authors call a &quot;less-plausible model. Therefore, these phenomena are not emergent properties as a result of their learning model. It is necessary to specify what are the essential factors for each observed phenomenon.</p></disp-quote><p>We did not mean to attribute all results to learning in the cortico-spinal connections, and certainly not the result of convergent force fields, where those connections are not even present. We tested the convergent force fields with an isolated spinal cord because that is the experimental paradigm, and attributed the result to viscoelastic properties of the arm, forces/torques adding linearly, and the balance between agonists and antagonists.</p><p>Giving the impression that we attribute every result in the paper to synaptic learning is a fault of our manuscript. We have tried to be very clear about this with the inclusion of configuration 3, where no plasticity is present, but the results appear nonetheless.</p><p>The appearance of the plots showing rotational dynamics can change a lot depending on the hyperparamters of the model, and on the detail of the algorithm, such as the sampling window. These plots are different from those in Churchland et al. 2012: we have few conditions, few neurons, no overtrained model, and a different way to produce the plots. To assess the presence of quasi-oscillations I would advice to also pay attention to the numerical measure (percentage of variance in the first jPCA plane). Notice that we have twice as many jPCA planes as the original study, but close to the same variances.</p><disp-quote content-type="editor-comment"><p>5) Regarding the synergistic model, their model has spinal interneurons recruiting two motoneuron pools, which is very different from the muscle synergies observed in animals</p><p>[2]. It is necessary to justify why this type of synergies should be used here.</p><p>[2] d'Avella A, Portone A, Fernandez L, Lacquaniti F (2006) Control of fast-reaching movements by muscle synergy combinations. The Journal of neuroscience: the official journal of the Society for Neuroscience 26:7791-7810</p></disp-quote><p>The synergistic model is a proof of principle, stating that it is not incompatible with Motor Synergy Encoders (reference 56), and this could be an interesting avenue of future research.</p><p>The study in d’Avella et al., 2006 takes the electromyograph waveforms and applies an optimization algorithm to find about five time-dependent patterns (the “synergies”) that explain most of the variance (which turned out to be around 80%). This is indeed a different interpretation of synergies from the one we use. In the introduction we clarified that “synergy” has multiple meanings, but we focus on Motor Synergy Encoders. This is because they fit naturally with the model.</p><p>This, however, got me thinking (SVF). Who is to say that applying the optimization procedure in d’Avella et al. 2006 would not lead to explain most variance through a reduced number of waveforms? I think this would be a very interesting future experiment. Thank you for providing the reference, and leading me in this path of thought.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Although the reviewers were satisfied by the new analyses, they were also unanimous in thinking that the manuscript is hard to follow and that this complexity could make it less broadly appealing. Our recommendation is thus for the authors to try to simplify the text, minimising the mental burden to the reader. The main concerns were:</p><p>1) The five different &quot;configurations&quot; are not well explained/motivated throughout the paper.</p></disp-quote><p>To reduce the confusion caused by the multiple configurations we have taken 3 steps. First, we have found that two of the configurations are not necessary to the main argument in the paper, so they have been relegated to the Appendix. Second, rather than using numbers, each configuration now has a descriptive name. Third, we have further clarified the rationale for each configuration.</p><disp-quote content-type="editor-comment"><p>2) Figure captions are too short, which makes it hard to understand the results (e.g., what is Figure 8 about?).</p></disp-quote><p>We wrote more descriptive figure captions.</p><disp-quote content-type="editor-comment"><p>3) There are too many abbreviations, which again does not help comprehend the text.</p></disp-quote><p>We removed some abbreviations, and sometimes repeated their definitions.</p><disp-quote content-type="editor-comment"><p>4) The Discussion is hard to follow, foremost because it requires that the readers remember all five configurations, which were presented in the much earlier -and very long- Results section.</p></disp-quote><p>The Discussion has been redacted, once more stating the idea behind each configuration.</p><disp-quote content-type="editor-comment"><p>5) The overall logic could be potentially streamlined so the study becomes easier to understand.</p></disp-quote><p>We have tried to improve the most tedious paragraphs.</p><disp-quote content-type="editor-comment"><p>In addition, you may want to consider addressing the comment by Reviewer #4, although it is not mandatory.</p><p>Reviewer #4 (Recommendations for the authors):</p><p>I really appreciate the authors revising the manuscript. The argument is now much clearer. I found that the authors are very open about what neural implementation this motor learning is occurring in, and it seems too early to suggest the neural implementation since the same learning can occur either in the spinal cord or in the cerebral cortex, or perhaps in other structures such as the cerebellum which is omitted by this model.</p><p>As a neurophysiologist, I am curious about what insights this computational model will provide into our understanding of the neural mechanism of motor learning. I understand that the key assumptions of this model are that (1) errors vary monotonically with motor command and (2) differential Hebbian learning at the output connections reduces the errors. The second assumption has already been discussed for possible implementation in the spinal cord. However, the validity of the first assumption is not yet clear. I would like to suggest the authors elaborate on what neural mechanism could transform errors in the multiple sensory systems (e.g visual systems) into the monotonic error such as muscle length. It is also important to mention where the desired somatosensory activity (Sp) comes from.</p></disp-quote><p>We found the comment by Reviewer #4 to be very insightful. Dealing with monotonicity of the errors and with the origin of the S<sub>P</sub> patterns is crucial to eventually extend this work into a more general system (for example, to include vision). Considering the current complexity of the manuscript, we believe it is best to refer the interested readers to our previous work (enclosed with the submission). In particular, our previous paper (first referenced in line 42) develops a proof-of concept for how an actor-critic system can be used to deal with non-monotonicity. We have also suggested (line 264) to consult that paper when considering the origin of the S<sub>P</sub> patterns.</p></body></sub-article></article>