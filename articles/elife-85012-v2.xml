<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">85012</article-id><article-id pub-id-type="doi">10.7554/eLife.85012</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Eelbrain, a Python toolkit for time-continuous analysis with temporal response functions</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-177109"><name><surname>Brodbeck</surname><given-names>Christian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8380-639X</contrib-id><email>brodbecc@mcmaster.ca</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-299377"><name><surname>Das</surname><given-names>Proloy</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8807-042X</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-231817"><name><surname>Gillis</surname><given-names>Marlies</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3967-2950</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-299378"><name><surname>Kulasingham</surname><given-names>Joshua P</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-246285"><name><surname>Bhattasali</surname><given-names>Shohini</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6767-6529</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-252880"><name><surname>Gaston</surname><given-names>Phoebe</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-299379"><name><surname>Resnik</surname><given-names>Philip</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-23965"><name><surname>Simon</surname><given-names>Jonathan Z</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0858-0698</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02fa3aq29</institution-id><institution>McMaster University</institution></institution-wrap><addr-line><named-content content-type="city">Hamilton</named-content></addr-line><country>Canada</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05f950310</institution-id><institution>Katholieke Universiteit Leuven</institution></institution-wrap><addr-line><named-content content-type="city">Leuven</named-content></addr-line><country>Belgium</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ynxx418</institution-id><institution>Linköping University</institution></institution-wrap><addr-line><named-content content-type="city">Linköping</named-content></addr-line><country>Sweden</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>University of Toronto</institution></institution-wrap><addr-line><named-content content-type="city">Toronto</named-content></addr-line><country>Canada</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/047s2c258</institution-id><institution>University of Maryland, College Park</institution></institution-wrap><addr-line><named-content content-type="city">College Park</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>29</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e85012</elocation-id><history><date date-type="received" iso-8601-date="2022-11-18"><day>18</day><month>11</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-11-24"><day>24</day><month>11</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-08-03"><day>03</day><month>08</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.08.01.454687"/></event></pub-history><permissions><copyright-statement>© 2023, Brodbeck et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Brodbeck et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-85012-v2.pdf"/><abstract><p>Even though human experience unfolds continuously in time, it is not strictly linear; instead, it entails cascading processes building hierarchical cognitive structures. For instance, during speech perception, humans transform a continuously varying acoustic signal into phonemes, words, and meaning, and these levels all have distinct but interdependent temporal structures. Time-lagged regression using <italic>temporal response functions (TRFs</italic>) has recently emerged as a promising tool for disentangling electrophysiological brain responses related to such complex models of perception. Here, we introduce the Eelbrain Python toolkit, which makes this kind of analysis easy and accessible. We demonstrate its use, using continuous speech as a sample paradigm, with a freely available EEG dataset of audiobook listening. A companion GitHub repository provides the complete source code for the analysis, from raw data to group-level statistics. More generally, we advocate a hypothesis-driven approach in which the experimenter specifies a hierarchy of time-continuous representations that are hypothesized to have contributed to brain responses, and uses those as predictor variables for the electrophysiological signal. This is analogous to a multiple regression problem, but with the addition of a time dimension. TRF analysis decomposes the brain signal into distinct responses associated with the different predictor variables by estimating a multivariate TRF (mTRF), quantifying the influence of each predictor on brain responses as a function of time(-lags). This allows asking two questions about the predictor variables: (1) Is there a significant neural representation corresponding to this predictor variable? And if so, (2) what are the temporal characteristics of the neural response associated with it? Thus, different predictor variables can be systematically combined and evaluated to jointly model neural processing at multiple hierarchical levels. We discuss applications of this approach, including the potential for linking algorithmic/representational theories at different cognitive levels to brain responses through computational models with appropriate linking hypotheses.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>reverse correlation</kwd><kwd>STRF</kwd><kwd>open-source</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>BCS 1754284</award-id><principal-award-recipient><name><surname>Brodbeck</surname><given-names>Christian</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>BCS 2043903</award-id><principal-award-recipient><name><surname>Brodbeck</surname><given-names>Christian</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>IIS 2207770</award-id><principal-award-recipient><name><surname>Brodbeck</surname><given-names>Christian</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>SMA 1734892</award-id><principal-award-recipient><name><surname>Kulasingham</surname><given-names>Joshua P</given-names></name><name><surname>Simon</surname><given-names>Jonathan Z</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 DC014085</award-id><principal-award-recipient><name><surname>Kulasingham</surname><given-names>Joshua P</given-names></name><name><surname>Simon</surname><given-names>Jonathan Z</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 DC019394</award-id><principal-award-recipient><name><surname>Simon</surname><given-names>Jonathan Z</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003130</institution-id><institution>Fonds Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>SB 1SA0620N</award-id><principal-award-recipient><name><surname>Gillis</surname><given-names>Marlies</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>MURI N00014-18-1-2670</award-id><principal-award-recipient><name><surname>Bhattasali</surname><given-names>Shohini</given-names></name><name><surname>Resnik</surname><given-names>Philip</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>T32 DC017703</award-id><principal-award-recipient><name><surname>Gaston</surname><given-names>Phoebe</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Brain activity is modelled as a function of stimuli that evolve continuously in time, in experimental designs without repeated trials.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>This paper introduces Eelbrain, a Python toolkit that makes it straightforward to express cognitive hypotheses as predictive computational models and evaluate those predictions against electrophysiological brain responses. The toolkit is based on the idea of decomposing brain signals into distinct responses associated with different predictor variables by estimating a multivariate temporal response function (mTRF), which maps those predictors to brain responses elicited by time-continuous stimulation (<xref ref-type="bibr" rid="bib57">Theunissen et al., 2001</xref>; <xref ref-type="bibr" rid="bib38">Lalor et al., 2006</xref>; <xref ref-type="bibr" rid="bib18">David et al., 2007</xref>). This form of analysis has yielded valuable insights into the way that perception and cognitive processes unfold over time (e.g. <xref ref-type="bibr" rid="bib21">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="bib12">Broderick et al., 2018</xref>; <xref ref-type="bibr" rid="bib6">Brodbeck et al., 2018a</xref>; <xref ref-type="bibr" rid="bib17">Daube et al., 2019</xref>; <xref ref-type="bibr" rid="bib41">Liberto et al., 2021</xref>; <xref ref-type="bibr" rid="bib56">Sohoglu and Davis, 2020</xref>).</p><sec id="s1-1"><title>How to read this Paper</title><p>Time-lagged regression using TRFs is a mathematical method for analyzing the stimulus-response relationship between two signals that are evolving as a function of time, i.e., time series, like speech and brain activity measurements. The goal of this paper is to introduce several categories of cognitive neuroscience questions that can be asked using TRFs, and provide recipes for answering them. As such, the paper is not necessarily meant to be read in a linear fashion. The Introduction provides a general motivation for the approach and explains the underlying concepts in an accessible way. The <italic>Results</italic> section demonstrates how the technique can be applied to answer specific questions. The <italic>Discussion</italic> section highlights some more advanced considerations and caveats that should be kept in mind. The <italic>Materials and methods</italic> section explains the technical details and implementation in Eelbrain. The accompanying <ext-link ext-link-type="uri" xlink:href="https://github.com/Eelbrain/Alice">GitHub repository</ext-link> (<xref ref-type="bibr" rid="bib11">Brodbeck et al., 2023</xref>) provides the source code for everything discussed in the paper (<ext-link ext-link-type="uri" xlink:href="https://github.com/Eelbrain/Alice/blob/main/README.md">README.md</ext-link> contains instructions on how to get started). In addition, the<ext-link ext-link-type="uri" xlink:href="https://eelbrain.readthedocs.io/en/stable/auto_examples/index.html"> Examples section</ext-link> on the Eelbrain website provides source code examples for many other basic tasks.</p><p>Depending on the background of the reader, these resources can be approached differently – for example, for readers new to mTRFs, we recommend reading the <italic>Introduction</italic> and <italic>Results</italic> sections first to get an idea of the questions that can be answered, and then referring to the Materials and methods for more detailed background information. On the other hand, readers specifically interested in the Eelbrain toolbox may want to skip ahead to the <italic>Materials and methods</italic> section early on.</p></sec><sec id="s1-2"><title>The convolution model for brain responses</title><p>The mTRF approach is built on the assumption that the brain response is continuously evolving in time as a function of the recently encountered stimulus (<xref ref-type="bibr" rid="bib38">Lalor et al., 2006</xref>). Brain responses do not directly mirror a physical stimulus, but rather reflect a variety of transformations of that stimulus. For example, while speech is transmitted through air pressure variations in the kHz range, this signal is transformed by the auditory periphery, and macroscopic cortical responses are better described as responses to the slowly varying envelope of the original broadband signal. Thus, instead of directly predicting brain responses from the stimulus, the experimenter commonly selects one or several appropriate predictor variables to <italic>represent</italic> the stimulus, for example the low-frequency speech envelope (<xref ref-type="bibr" rid="bib39">Lalor and Foxe, 2010</xref>).</p><p>The convolution model is a formal specification of how the stimulus, as characterized by the predictor variables, leads to the response. The stimulus-response relationship is modeled as a linear convolution in time, as illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref>. In contrast to classical analysis approaches that require averaging, the convolution model applies to single trial data and does not require any repetition of identical stimuli. A convolution kernel, or impulse response, characterizes the influence of each elementary unit in a predictor on the response. This kernel is also called the TRF, to distinguish it from the measured response to the stimulus as a whole. In addition to modeling an individual predictor variable (<xref ref-type="fig" rid="fig1">Figure 1B, C</xref>), the convolution model can also incorporate multiple predictor variables through the assumption that responses are additive (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Each predictor variable is associated with its own TRF, and thus predicts a separable response component. The ultimate response is the sum of those response components at each time point. This additive model is consistent with the fact that macroscopic measurements of electrical brain signals reflect an additive superposition of signals from different brain regions, potentially reflecting separable neural processes (<xref ref-type="bibr" rid="bib47">Nunez and Srinivasan, 2006</xref>). When multiple predictor variables are jointly predicting a response, the collection of their TRFs is called an mTRF. As such, predictor variables can be thought of as hypotheses about how stimuli are represented by the brain, and multiple concurrent predictors can embody distinct hypotheses about how the stimulus is transformed across different brain regions (<xref ref-type="bibr" rid="bib9">Brodbeck and Simon, 2020</xref>). The additive nature of the convolution model allows it to be applied to comparatively natural stimulus conditions, such as audiobook listening (<xref ref-type="bibr" rid="bib32">Hamilton and Huth, 2020</xref>; <xref ref-type="bibr" rid="bib1">Alday, 2019</xref>), while modeling natural variability through different predictor variables rather than minimizing it through experimental design.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The convolution model for brain responses as a generalization of the averaging-based paradigm.</title><p>(<bold>A</bold>) The traditional event-related analysis method assumes that each stimulus (blue arrows) evokes an identical, discrete response, and this response can be recovered by averaging. This model assumes that responses are clearly separated in time. (<bold>B</bold>) In contrast, the convolution model assumes that each time point in the stimulus could potentially evoke a response. This is implemented with time series predictor variables, illustrated here with a time series containing several impulses. These impulses represent discrete events in the stimulus that are associated with a response, for example, the occurrence of words. This predictor time series is convolved with some kernel characterizing the general shape of responses to this event type – the <italic>temporal response function</italic> (TRF), depicted on the right. Gray arrows illustrate the convolution, with each impulse producing a TRF-shaped contribution to the response. As can be seen, the size of the impulse determines the magnitude of the contribution to the response. This allows testing hypotheses about stimulus events that systematically differ in the magnitude of the responses they elicit, for example, that responses increase in magnitude the more surprising a word is. A major advantage over the traditional averaging model is that responses can overlap in time. (<bold>C</bold>) Rather than discrete impulses, the predictor variable in this example is a continuously varying time series. Such continuously varying predictor variables can represent dynamic properties of sensory input, for example: the acoustic envelope of the speech signal. The response is dependent on the stimulus in the same manner as in (<bold>B</bold>), but now every time point of the stimulus evokes its own response shaped like the TRF and scaled by the magnitude of the predictor. Responses are, therefore, heavily overlapping. (<bold>D</bold>) The multivariate TRF (mTRF) model is a generalization of the TRF model with multiple predictors: like in a multiple regression model, each time series predictor variable is convolved with its own corresponding TRF, resulting in multiple partial responses. These partial responses are summed to generate the actual complete response. Source code: <ext-link ext-link-type="uri" xlink:href="https://github.com/Eelbrain/Alice/blob/main/figures/Convolution.py">figures/Convolution.py</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85012-fig1-v2.tif"/></fig><p>In most practical data analysis scenarios, the true TRFs are unknown, but the stimulus and the brain responses are known. A TRF estimation algorithm addresses this, by estimating the mTRF that is optimal to predict the brain response from the predictor variables representing the stimulus. <xref ref-type="fig" rid="fig2">Figure 2</xref> illustrates this with EEG responses being predicted from the speech envelope. Typically, this is a very high-dimensional problem – including several predictor variables, each of which can influence the brain response at a range of latencies. Due to the large number of parameters, mTRFs are prone to overfitting, meaning that the mTRFs learn properties of the noise in the specific dataset rather than the underlying, generalizable responses. TRF estimation methods deal with this problem by employing different regularization schemes, i.e., by bringing additional assumptions to the problem that are designed to limit overfitting (see <italic>Sparsity prior</italic> below). A further step to avoid spurious results due to overfitting is evaluating model quality with cross-validation, i.e., evaluating the model on data that was never used during training. This step allows evaluating whether the mTRF model can generalize to <italic>unseen</italic> data and <italic>predict</italic> novel responses, as opposed to merely <italic>explaining</italic> the responses it was trained on.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Temporal response function (TRF) analysis of EEG speech tracking.</title><p>The left half illustrates the estimation of a TRF model, the right half the evaluation of this model with cross-validation. First, the stimulus is used to generate a predictor variable, here the acoustic envelope (<bold>A</bold>). The predictor and corresponding EEG data (here only one sensor is shown) are then used to estimate a TRF (<bold>B</bold>). This TRF is then convolved with the predictor for the held-out testing data to predict the neural response in the testing data (<bold>C</bold>; measured: black; predicted: red). This predicted response is compared with the actual, measured EEG response to evaluate the predictive power of the model (<bold>D</bold>). A topographic map shows the % of the variability in the EEG response that is explained by the TRF model, estimated independently at each sensor. This head-map illustrates how the predictive power of a predictor differs across the scalp, depending on which neural sources a specific site is sensitive to. The sensor whose data and TRF are shown is marked in green. Source code: <ext-link ext-link-type="uri" xlink:href="https://github.com/Eelbrain/Alice/blob/main/figures/TRF.py">figures/TRF.py</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85012-fig2-v2.tif"/></fig></sec><sec id="s1-3"><title>Nonlinear responses</title><p>Convolution can only model linear responses to a given input, whereas true neural responses are known to be nonlinear. Indeed, nonlinear transformations of the stimulus are arguably the most interesting, because they can show how the brain transforms and abstracts away from the stimulus, rather than merely mirroring it. We advocate a model-driven approach to study such nonlinear responses. A non-linear response can be modeled by generating a predictor variable that applies a non-linear transformation to the original stimulus, and then predicting brain responses as a linear response to this new predictor variable. For instance, it is known that the auditory cortex is disproportionately sensitive to acoustic onsets. This sensitivity has been described with a neural model of auditory edge detection, implemented as a signal processing routine (<xref ref-type="bibr" rid="bib27">Fishbach et al., 2001</xref>). When this edge detection model is applied to the acoustic spectrogram, this results in a spectrogram of acoustic onsets, effectively mapping regions in the signal to which specific neuron populations should respond. This transformed spectrogram as a predictor variable thus operationalizes the hypothesis that neurons perform this non-linear transformation. Indeed, such acoustic onset spectrograms are highly significant predictors of auditory magnetoencephalography (MEG) responses (<xref ref-type="bibr" rid="bib17">Daube et al., 2019</xref>; <xref ref-type="bibr" rid="bib8">Brodbeck et al., 2020</xref>). Because mTRF models can only use linear transformations of the predictor variables to predict brain responses, a significant contribution from this predictor variable suggests that this non-linear transformation captures the non-linear nature of the neural processes giving rise to the brain responses.</p><p>This logic for studying nonlinear responses is taken to an even further level of abstraction when language models are used to predict brain responses. For instance, linguistic theory suggests that during speech comprehension, the continuous acoustic signal is transformed into discrete representations such as phonemes and words. However, we do not yet have an explicit, computational model of this transformation that could be used to generate an appropriate predictor. Instead, experimenters can estimate the result of an implicitly specified transformation based on extraneous knowledge, such as linguistic labels and corpus data. For example, responses to phonemes or phonetic features have been modeled through predictors reflecting discrete categories (<xref ref-type="bibr" rid="bib20">Di Liberto et al., 2015</xref>). Furthermore, a series of such investigations suggests that brain responses to speech reflect linguistic representations at different hierarchical levels (<xref ref-type="bibr" rid="bib6">Brodbeck et al., 2018a</xref>; <xref ref-type="bibr" rid="bib12">Broderick et al., 2018</xref>; <xref ref-type="bibr" rid="bib59">Weissbart et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Gillis et al., 2021</xref>; <xref ref-type="bibr" rid="bib10">Brodbeck et al., 2022</xref>). Such linguistic properties are commonly modeled as impulses corresponding to the onsets of words or phonemes. This does not necessarily entail the hypothesis that responses occur <italic>at word onsets</italic>. Rather, since the mTRFs allow responses at various latencies relative to the stimulus, such predictor variables can predict any time-locked responses that occur in an approximately fixed temporal relationship with the stimulus (within the pre-specified latency window).</p><p>During all this, it is important to keep in mind that even predictor variables that implement highly non-linear transformations are still likely to be correlated with the original stimulus (or linear transformations of it). For example, words and phonemes are associated with specific spectro-temporal acoustic patterns which systematically relate to their linguistic significance. Before drawing conclusions about non-linear transformations implemented by the brain, it is thus always important to control for more basic stimulus representations. In the domain of speech processing, this includes at least an acoustic spectrogram and an acoustic onset spectrogram (see Auditory response functions below). The latter in particular has been found to account for many responses that might otherwise be attributed to phonetic feature representations (<xref ref-type="bibr" rid="bib17">Daube et al., 2019</xref>).</p></sec><sec id="s1-4"><title>This tutorial</title><p>For this tutorial, we use the openly available Alice dataset (<xref ref-type="bibr" rid="bib3">Bhattasali et al., 2020</xref>) which contains EEG data from 33 participants who listened to the first chapter of <italic>Alice in Wonderland</italic> (12.4 min; 2129 words). The dataset also includes several word-level regressors derived from different syntactic language models, described in more detail in the original publication (<xref ref-type="bibr" rid="bib5">Brennan et al., 2019</xref>). Here, we implement a complete group-level analysis for different levels of representation using Eelbrain.</p><p>Eelbrain implements mTRF estimation using boosting (<xref ref-type="bibr" rid="bib18">David et al., 2007</xref>), as well as a variety of statistical tests, and ways to extract results for further analysis with other tools. The overall implementation of Eelbrain has been guided by the goal of facilitating mTRF estimation, group-level analysis, and visualization of results, for a general audience. The choice of boosting is significant as it encourages TRFs with a small number of non-zero coefficients, i.e., the boosting algorithm prefers a simpler explanation over a complex one (<xref ref-type="bibr" rid="bib37">Kulasingham and Simon, 2023</xref>). This makes boosting suitable for estimating mTRFs for models consisting of structured, highly correlated, and possibly redundant predictor variables (<xref ref-type="bibr" rid="bib19">David and Shamma, 2013</xref>), as is typical for models in cognitive neuroscience problems.</p></sec></sec><sec id="s2" sec-type="results"><title>Results</title><p>A critical goal of mTRF analysis for studying perception is evaluating brain responses that are nonlinear functions of the stimulus. In this section, we illustrate this by addressing increasingly complex nonlinear responses.</p><sec id="s2-1"><title>Response scale</title><p>A basic nonlinearity is that of scale: brain responses can grow nonlinearly with the scale of the input. For instance, in the auditory system, neural responses tend to scale logarithmically with acoustic power encoded in a spectrogram (<xref ref-type="bibr" rid="bib51">Rahman et al., 2020</xref>). This means that the same amount of power increase in the acoustic signal will cause a different increase in the brain response depending on the initial acoustic power value (see <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Such nonlinear responses are typically modeled by scaling the predictor variable and assuming a linear relationship between the scaled predictor and response (e.g. <xref ref-type="bibr" rid="bib28">Fox, 2008</xref>). Here, we determine the nonlinear relationship between acoustic power and brain response by comparing linear, logarithmic, and power law (<xref ref-type="bibr" rid="bib4">Biesmans et al., 2017</xref>) scales.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Nonlinear response scales.</title><p>(<bold>A</bold>) Illustration of logarithmic and power-law scales, compared to linear scale. (<bold>B</bold>) Gammatone spectrograms were ransformed to correspond to linear, power-law, and logarithmic response scales. Topographic maps show the predictive power of the three different spectrogram models. The color represents the percent of the variability in the EEG data that is explained by the respective multivariate TRF (mTRF) model. (<bold>C</bold>) Statistical comparison of the predictive power, averaged across all sensors. Error bars indicate the within-subject standard error of the mean, and significance is indicated for pairwise <italic>t</italic>-tests (<italic>df</italic> = 32). ***p≤0.001; Source code: <ext-link ext-link-type="uri" xlink:href="https://github.com/Eelbrain/Alice/blob/main/figures/Auditory-scale.py">figures/Auditory-scale.py</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85012-fig3-v2.tif"/></fig><p><xref ref-type="fig" rid="fig3">Figure 3B</xref> shows the gammatone spectrogram, transformed to linear, power-law, and logarithmic response scales, along with the predictive power for EEG data resulting from the different transformations. When considering the average predictive power at all electrodes, the power-law scale spectrogram was a better predictor than the linear scale (<italic>t</italic>(32) = 4.59, p&lt;0.001), and the log scale further improved predictions compared to the power-law scale (<italic>t</italic>(32) = 4.67, p&lt;0.001). Furthermore, nonlinear responses may mix properties of different scales. We thus tested whether the response may exhibit a linear component in addition to the logarithmic component by fitting a model including both the linear and the logarithmic spectrogram (i.e. twice as many predictors; similarly, polynomial regression combines multiple nonlinear transformations [<xref ref-type="bibr" rid="bib28">Fox, 2008</xref>]). The linear + log combined model did not improve predictions over the logarithmic model (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), suggesting that the EEG responses to the acoustic power are sufficiently described by the logarithmic response scale and do not contain an additional linear component.</p></sec><sec id="s2-2"><title>Auditory response functions</title><p>One goal of perception is to detect complex patterns present in the input signal, and we thus expect brain responses to represent features beyond simple acoustic intensity. Such features can be described as increasingly complex nonlinear transformations. Here, we illustrate this using acoustic onsets, a nonlinear transformation that is associated with strong responses and is a critical potential confound for higher-level features (<xref ref-type="bibr" rid="bib17">Daube et al., 2019</xref>). Acoustic features of speech input are shown in <xref ref-type="fig" rid="fig4">Figure 4A</xref>: The upper panel shows the log-transformed gammatone spectrogram, quantifying acoustic energy as a function of time in different frequency bands. The gammatone filters simulate response characteristics of the peripheral auditory system. A simplified, one-dimensional representation of this spectrogram is the envelope, which is the summed energy across all frequency bands (blue line). The lower panel of <xref ref-type="fig" rid="fig4">Figure 4A</xref> shows an acoustic onset spectrogram based on a neurally inspired acoustic edge detection model (<xref ref-type="bibr" rid="bib27">Fishbach et al., 2001</xref>), as described in <xref ref-type="bibr" rid="bib8">Brodbeck et al., 2020</xref>. Again, a simplified one-dimensional version of this predictor, summing across all bands, signifies the presence of onsets across frequency bands (blue line).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Auditory temporal response functions.</title><p>(<bold>A</bold>) Common representations of auditory speech features: an auditory spectrogram (upper panel), and the acoustic envelope, tracking total acoustic energy across frequencies over time (blue line); and an acoustic onset spectrogram (lower panel), also with a one-dimensional summary characterizing the presence of acoustic onsets over time (blue line). (<bold>B</bold>) Brain responses are predicted from the acoustic envelope of speech alone. Left: Cross-validated predictive power is highly significant (p&lt;0.001) at a large cluster covering all sensors. Right: the envelope temporal response function (TRF) – the y-axis represents the different EEG channels (in an arbitrary order), and the x-axis represents predictor-response time lags. The green vertical lines indicate specific (manually selected) time points of interest, for which head map topographies are shown. The black outlines mark significant clusters (p≤0.05, corrected for the whole TRF). For the boosting algorithm, predictors and responses are typically normalized, and the TRF is analyzed and displayed in this normalized scale. (<bold>C</bold>) Results for an multivariate TRF (mTRF) model including the acoustic envelope and acoustic onsets (blue lines in A). The left-most head map shows the percentage <italic>increase</italic> in predictive power over the TRF model using just the envelope (p&lt;0.001; color represents the change in the percent variability explained; black outlines mark significant clusters, p≤0.05, family-wise error corrected for the whole head map). Details are analogous to (<bold>B</bold>). (<bold>D</bold>) Results for an mTRF model including spectrogram and onset spectrogram, further increasing predictive power over the one-dimensional envelope and onset model (p&lt;0.001; color represents the change in percent variability explained). Since the resulting mTRFs distinguish between different frequencies in the stimulus, they are called spectro-temporal response functions (STRFs). In (<bold>D</bold>), these STRFs are visualized by summing across the different frequency bands. (<bold>E</bold>) To visualize the sensitivity of the STRFs to the different frequency bands, STRFs are instead averaged across sensors sensitive to the acoustic features. The relevant sensors are marked in the head map on the left, which also shows the predictive power of the full spectro-temporal model (color represents the percent variability explained). Because boosting generates sparse STRFs, especially when predictors are correlated, as are adjacent frequency bands in a spectrogram, STRFs were smoothed across frequency bands for visualization. a.u.: arbitrary units. Source code: <ext-link ext-link-type="uri" xlink:href="https://github.com/Eelbrain/Alice/blob/main/figures/Auditory-TRFs.py">figures/Auditory-TRFs.py</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85012-fig4-v2.tif"/></fig><p>To illustrate auditory features of increasing complexity we analyzed the following (m)TRF models:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="fig" rid="fig4">Figure 4B</xref> shows response characteristics to the acoustic envelope alone. A topographic head-map shows the envelope’s predictive power. The envelope alone is already a very good predictor of held-out EEG responses, with variability explained reaching 82% of that of the full spectro-temporal model (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>) at anterior electrodes. The TRF to the envelope exhibits features characteristic of auditory evoked responses to simple acoustic stimuli such as tones or isolated syllables, including a P1-N1-P2 sequence.</p><p><xref ref-type="fig" rid="fig4">Figure 4C</xref> shows the result of adding the one-dimensional acoustic onset predictor to the model. Together, onset and envelope significantly improve the prediction of held-out responses compared to just the envelope (p&lt;0.001), indicating that the onset representation is able to predict some aspects of the EEG responses that the envelope alone cannot. The typical TRF to the onsets is of shorter duration than that to the envelope, and is characterized by two prominent peaks around 60 and 180 ms. The envelope TRF here is not much affected by adding the onset to the model (compare with <xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p><p><xref ref-type="fig" rid="fig4">Figure 4D</xref> shows the additional benefit of representing the envelope and onsets in different frequency bands, i.e., predicting EEG from an auditory spectrogram and an onset spectrogram (here eight bands were used in each, for a total of 16 predictors). As the predictors are two-dimensional (frequency × time), the resulting mTRFs are three-dimensional (frequency × lag × EEG sensor), posing a challenge for visualization on a two-dimensional page. One approach, assuming that response functions are similar across frequency bands, is to sum responses across frequency bands. As shown in <xref ref-type="fig" rid="fig4">Figure 4D</xref>, this indeed results in response functions that look very similar to the one-dimensional versions of the same predictors (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). To visualize how the response functions differ for different frequency bands in the spectrograms, <xref ref-type="fig" rid="fig4">Figure 4E</xref> shows the full spectro-temporal response functions (STRFs), averaged across the electrodes that are most sensitive to the auditory stimulus features.</p><p>In sum, while the acoustic envelope is a powerful predictor of EEG responses to speech, additional acoustic features can improve predictions further, suggesting that they characterize neural representations that are not exhaustively described by the envelope. While the increase in prediction accuracy might seem small, more importantly, it allows the critical inference that the different predictors are characterizing separable neural representations. For instance, a significant response to acoustic onsets (after controlling for the envelope/spectrogram) provides evidence for a nonlinear component of the brain response to speech that represents acoustic onsets. Separating the influence of different predictors is also important because different neural representations can have different response characteristics under different situations. For example, acoustic onsets might be especially important in segregating multiple auditory streams (<xref ref-type="bibr" rid="bib8">Brodbeck et al., 2020</xref>; <xref ref-type="bibr" rid="bib26">Fiedler et al., 2019</xref>).</p></sec><sec id="s2-3"><title>Word onsets as discrete events: Comparing ERPs and TRFs</title><p>While speech is continuous, perception is often characterized through discrete events. For example, in speech, words may be perceived as perceptual units. Such events have been analyzed using ERPs, but they can also be incorporated into an mTRF model, by using predictors that contain impulses at relevant time points (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). In contrast to an ERP analysis, the mTRF analysis allows controlling for brain responses related to acoustic processing, and overlapping responses to events (words) that are close in time. Here, we directly compare these two analysis approaches.</p><p>Word onset TRFs controlling for acoustic processing were estimated using model <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></disp-formula></p><p>These word onset TRFs are shown in <xref ref-type="fig" rid="fig5">Figure 5</xref> alongside the classical ERP. As expected from the earlier discussion, the TRF and ERP patterns are similar. The time and scalp regions where the two estimates of the brain response to words differ significantly are marked with gray bars (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) and contours (<xref ref-type="fig" rid="fig5">Figure 5B, C</xref>; assessed via related-measures <italic>t</italic>-tests controlling for multiple comparisons). A prominent difference is that the ERP contains stronger activity in the baseline period, and a larger deflection at late lags (starting at 600 ms), with a topography similar to acoustic activity (compare with <xref ref-type="fig" rid="fig4">Figure 4</xref>), indicating artifactual leakage from acoustic responses. Additionally, there is a significant difference at around 200–450 ms which could be attributed to the temporal spread of the observed P2 peak in the ERPs, as the P2 peak in the TRF is temporally better defined (i.e. sharper) than in the ERPs.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Temporal response functions (TRFs) to discrete events are similar to event-related potentials (ERPs), but control for acoustic processing and overlapping responses.</title><p>To visualize ERP and TRF on a similar scale, both ERP and TRF were normalized (parametric normalization). (<bold>A</bold>) Visualization of the ERP and TRF response over time for a frontocentral channel, as indicated on the inset. The gray bars on the time axis indicate the temporal clusters in which the ERP and TRF differ significantly. Shading indicates within-subject standard errors (<italic>n</italic> = 33). (<bold>B</bold>) Visualization of ERP and TRF responses across all channels: the ERP responses to word onsets (left), the TRF to word onsets while controlling for acoustic processing (middle), and the difference between the ERP and the TRF (right). The black outline marks clusters in which the ERP and the TRF differ significantly in time and across sensors, assessed by a mass-univariate related-measures <italic>t</italic>-test. (<bold>C</bold>) Visualization of the topographies at selected time points, indicated by the vertical, dashed lines in (<bold>B</bold>), for respectively the ERP (top row), TRF (middle row), and their difference (bottom row). The contours mark regions where the ERP differs significantly from the TRF, as determined by the same mass-univariate related-measures <italic>t</italic>-test as in (<bold>B</bold>). a.u.: arbitrary units. Source code: <ext-link ext-link-type="uri" xlink:href="https://github.com/Eelbrain/Alice/blob/main/figures/Comparison-ERP-TRF.py">figures/Comparison-ERP-TRF.py</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85012-fig5-v2.tif"/></fig><p>Consistent with expectations, the ERPs overestimate the responses to words. The elevated baseline activity in particular suggests that the ERP is more prone to including brain activity that is not strictly a response to the stimulus feature, because the baseline period precedes the onset of the word. This is not surprising considering that the ERP is just an average of neural signals before and after word onsets, which includes, besides the responses to the words, neural responses to the acoustic signal, as well as activities that are not time-locked (including overlapping responses to the previous and next word, given that the average interval between word onsets in this stimulus set is only 334 ms). This highlights an advantage of the TRF paradigm over classical ERPs: the TRF enables characterization of only the neural response time-locked to the particular feature of interest, while explicitly controlling for signal variability due to other features of the stimuli.</p></sec><sec id="s2-4"><title>Categories of events: Function and content words</title><p>Given the powerful response to acoustic features of speech, it is important to take these responses into account when investigating linguistic representations. To illustrate the advantage of an mTRF analysis that can take into account the acoustic stimulus features, we revisit an old question: do brain responses differentiate between content words (roughly, words conveying a meaning) and function words (words necessary for a grammatical construction)? In a first, naive approach, we ask literally whether brain responses differ between function and content words, while ignoring any potential confounding influences from acoustic differences. For this, we compare the predictive power of models <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> and <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>, all based on predictors with unit magnitude impulses at word onsets:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mi>A</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mi>A</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></disp-formula></p><p>Note that <italic>all words</italic> and the two word-class-based predictors are mathematically redundant because the <italic>all words</italic> vector equals the sum of the <italic>function words</italic> and <italic>content words</italic> vectors. However, because of the sparsity prior employed through the boosting algorithm and cross-validation, such redundant predictors can still improve a model – for example, if there is a shared response among all words, then attributing this response once as a TRF to <italic>all words</italic> is sparser than attributing it twice, once for <italic>function words</italic> and once for <italic>content words</italic> separately (see also <italic>Sparsity prior</italic> below).</p><p>The predictive power of the model with the word class distinction (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>) is significantly higher compared to the model without it (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>) at a right anterior electrode cluster (p=0.002, <xref ref-type="fig" rid="fig6">Figure 6A</xref>, top). To investigate why the word class distinction improves the model’s predictive power, we compare the TRFs to function and content words (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Both TRFs are reconstructed from the relevant model components: Whenever a function word occurs in the stimulus, there will be both an impulse in the <italic>all words</italic>, and one in the <italic>function words</italic> predictor (and vice versa for content words). Accordingly, the full EEG response occurring at the function word is split into two components, one in the <italic>all words</italic>, and one in the <italic>function words</italic> TRF. In order to visualize these full EEG responses, the displayed TRF for function words consists of the sum of the TRFs of the <italic>all words</italic> and the <italic>function words</italic> predictors, and the TRF for content words consists of the sum of the TRFs of <italic>all words</italic> and <italic>content words</italic>. This comparison suggests that function words are associated with a larger positive response at anterior sensors than content words.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>The difference in response to content and function words explained by acoustic differences.</title><p>(<bold>A</bold>) Model comparisons of predictive power across the scalp. Each plot shows a head-map of the change in predictive power between different pairs of models. Top: <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> &gt; <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>; middle: <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> &gt; <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>; bottom: <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> &gt; <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>. Color represents the difference in percent variability explained. (<bold>B</bold>) Brain responses occurring after function words differ from brain responses after content words. Responses were estimated from the temporal response functions (TRFs) of model <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> by adding the word-class-specific TRF to the <italic>all words</italic> TRF. The contours mark the regions that are significantly different between function and content words based on a mass-univariate related-measures <italic>t</italic>-test. (<bold>C</bold>) Function words are associated with a sharper acoustic onset than content words. The average spectrograms associated with function and content words were estimated with time-lagged regression, using the same algorithm also used for TRF estimation, but predicting the acoustic spectrogram from the function and content word predictors. A dotted line is plotted at 70 ms to help visual comparison. Color scale is normalized. Source code: <ext-link ext-link-type="uri" xlink:href="https://github.com/Eelbrain/Alice/blob/main/figures/Word-class-acoustics.py">figures/Word-class-acoustic.py</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85012-fig6-v2.tif"/></fig><p>To investigate whether this difference in responses might be due to acoustic confounds, we control for brain responses to acoustic features in both models and compare <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> with <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>, repeated here for convenience:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></disp-formula></p><p>In contrast to the comparison without acoustics, the comparison of the predictive power of <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> with <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> no longer indicates a significant difference (p=0.065, <xref ref-type="fig" rid="fig6">Figure 6A</xref>, middle). This suggests that the information in the acoustic predictors can explain the difference between brain responses to function and content words in model <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>. To directly characterize the influence of the acoustic features we plot the predictive power of the acoustic features by comparing models <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> and <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>. This comparison suggests that acoustic features are highly predictive of brain signals at anterior sensors (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, bottom), encompassing the region in which the word class distinction originally showed an effect.</p><p>If the difference between brain responses to function and content words disappears when controlling for acoustic features, that suggests that acoustic features should differ between function and content words. We can assess this directly with another time-lagged regression model. First, we estimated filter kernels (analogous to the mTRFs) to predict the auditory spectrogram from models <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> and <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> (script: <ext-link ext-link-type="uri" xlink:href="https://github.com/Eelbrain/Alice/blob/main/analysis/estimate_word_acoustics.py">analysis/estimate_word_acoustics.py</ext-link>). To be able to statistically evaluate these results, we used 15-fold cross-validation and treated the predictive accuracy from each test fold as an independent estimate of predictive power (for a similar approach see <xref ref-type="bibr" rid="bib24">Etard et al., 2019</xref>). With predictive power averaged across frequency bands, the model distinguishing function and content words is significantly better at predicting the spectrogram than the model treating all words equally (<italic>t</italic>(14) = 13.49, p&lt;0.001), suggesting that function and content words indeed differ acoustically. Finally, the filter kernels to function and content words from this analysis can be interpreted as average acoustic patterns corresponding to the two word-classes (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). This comparison suggests that function words, on average, have a sharper acoustic onset. Since auditory cortex responses are known to be sensitive to acoustic onsets, a reasonable explanation for the difference in neural responses by word class, when not controlling for acoustic features, is that it reflects these sharper acoustic onsets of function words.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>While the sections above provide recipes for various aspects of mTRF analysis, we use this section to discuss a number of advanced considerations and caveats, and possible extensions.</p><sec id="s3-1"><title>TRF analysis vs. predictive power</title><p>In the various <italic>Results</italic> sections, we always tested the predictive power of a predictor variable before analyzing the corresponding TRF. This has a good reason: model comparisons based on predictive power are generally more conservative than comparisons based on TRF estimates. First, TRFs are directly estimated on the training data, and are still prone to some overfitting (although the boosting algorithm aims to minimize that using the validation step and early stopping). Second, as in conventional regression models, if two predictors are correlated, that means that they might share some of their predictive power. Model comparisons address this by testing for the <italic>unique</italic> predictive power of a variable, after controlling for all other variables, i.e., by testing for variability in the dependent measure that can <italic>only</italic> be explained by the predictor under investigation. The mTRFs (i.e. regression coefficients) cannot properly be disentangled in this way (<xref ref-type="bibr" rid="bib29">Freckleton, 2002</xref>), and will usually divide the shared explanatory power amongst themselves. This means that mTRF estimates may always be contaminated by responses to correlated variables, especially when the correlations among predictor variables are high. This consideration highlights the importance of testing the predictive power of individual model components before interpreting the corresponding TRFs to avoid spurious conclusions due to correlated predictors. In sum, a significant result in a model comparison provides strong evidence that a given predictor contributes unique predictive power when compared to the other predictors included in the model. In contrast, a significant effect in a TRF should always be interpreted with care, as it may also reflect the influence of other, correlated variables, even if those are included in the model.</p><p>TRF analysis may have an additional utility in diagnosing a special relationship between predictors. Assume that the brain represents a signal, say <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and this signal can be decomposed into <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> such that <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. In a model using the predictors <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, both will contribute significantly. Thus, one might conclude that the brain represents two quantities separately, when in fact it represents only the average of them. However, because <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, TRFs to <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in the model including only these two predictors should look identical, thus providing a diagnostic for such a case. If the TRF shapes do not look identical, then that means that the properties in <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are represented at different latencies, i.e., that they are separable.</p></sec><sec id="s3-2"><title>Sparsity prior</title><p>In its general formulation, mTRF analysis is a regression problem, albeit a high-dimensional one. Such high-dimensional analysis methods are almost always marred by overfitting. The presence of a large number of free parameters and correlated predictor variables makes the mTRFs prone to discovering noise patterns that are idiosyncratic to the particular dataset, and which do not generalize well to other datasets. Most regression analysis methods deal with this problem through regularization using a well-informed prior and some form of cross-validation. These regularization schemes vary considerably; some employ an explicitly formulated prior, e.g., the regularization in ridge regression (<xref ref-type="bibr" rid="bib13">Crosse et al., 2016a</xref>); while others are defined on the algorithmic level, e.g., the early stopping criterion for boosting (see <italic>Background: The boosting algorithm</italic>). The implicit prior in the boosting algorithm promotes sparsity, i.e., it forces unimportant filter kernel coefficients to exactly 0. For problems with a large number of correlated predictors, the boosting algorithm may be preferable to other sparsity-enforcing algorithms such as LASSO (<xref ref-type="bibr" rid="bib34">Hastie et al., 2007</xref>).</p><p>This sparsity prior has some consequences that might be counterintuitive at first. For example, in regression models, it is common to center predictors. This does not affect the explanatory power of individual regressors, because a shift in the mean of one predictor will simply lead to a corresponding shift in the coefficient for the intercept term. In contrast to this, a sparsity prior will favor a model with smaller coefficients. Consequently, an uncentered predictor that can explain responses with a small coefficient in the intercept term will be preferable over a centered predictor that requires a larger intercept coefficient.</p><p>Another consequence of the same preference for sparser models is that sometimes mathematically redundant predictors can improve the predictive power of a model. An example is that when splitting an impulse predictor into two categories, such as when dividing words into function and content words, the original <italic>all words</italic> predictor does not necessarily become redundant (see <italic>Categories of events: Function and content words</italic>). This can make model construction more complex, but it can also be informative, for example by showing whether there is a common response component to all words that can be learned better by including an <italic>all words</italic> predictor, in addition to word-class-specific responses.</p></sec><sec id="s3-3"><title>Discrete events and coding</title><p>In practice, the assumption of millisecond-precise time-locking to force-aligned features might seem hard to defend. For example, identification of clear word and phoneme boundaries is an artificial imposition, because the actual acoustic-phonetic features blend into each other due to co-articulation. An mTRF analysis only requires time-locking on average to produce some consistent responses. Nevertheless, more precise time estimates enhance the model’s ability to isolate the response. In order to model cognitive events more precisely, one might thus want to consider alternative event locations, for example the words’ uniqueness points instead of word onsets.</p><p>Impulse coding is not the only coding that is available. Impulses are based on the linking hypothesis of a fixed amount of response per impulse. An alternative linking hypothesis assumes that the brain response is increased for the duration of an event. This could be modeled using a step function instead of impulses. Yet another type of hypothesis might concern a modulation of another predictor. For example, the hypothesis that the magnitude of the neural representation of the speech envelope is modulated by how linguistically informative a given segment is can be implemented by scaling the speech envelope according to each phoneme’s linguistic surprisal (<xref ref-type="bibr" rid="bib23">Donhauser and Baillet, 2020</xref>). A related set of questions concerns whether the same predictor is more or less powerful in different experimental conditions (<xref ref-type="bibr" rid="bib56">Sohoglu and Davis, 2020</xref>).</p></sec><sec id="s3-4"><title>Source localization</title><p>EEG data is often analyzed in sensor space. However, because each sensor records a mixture of underlying neural (and artifactual) signals from different sources, sensor space data is inherently noisy. When data is recorded from a dense enough sensor array, neural source localization can be used to estimate a reconstruction of the sources contributing to the mixture signal, and assign the different brain responses to their cortical locations (<xref ref-type="bibr" rid="bib47">Nunez and Srinivasan, 2006</xref>). Approaches combining source localization with mTRF analysis can differentiate responses related to processing of continuous stimuli anatomically as well as temporally, and thus provide a way to investigate hierarchical models of sensory and cognitive processing involving multiple anatomical regions. In many cases, source localization can also improve the signal-to-noise ratio of a specific response, because it acts as a spatial filter, aggregating information that is relevant for a given brain region across sensors, and suppressing signals whose likely origin is a different location.</p><p>A straight-forward extension of the approach described here is to apply a linear inverse solution to the continuous data, and apply mTRF analyses to the virtual current dipoles (<xref ref-type="bibr" rid="bib7">Brodbeck et al., 2018b</xref>). For this purpose, Eelbrain contains functions that directly convert MNE-Python source estimate objects to <monospace>NDVar</monospace>s. A more advanced approach is the Neuro-Current Response Function technique, which performs mTRF estimation and source localization jointly, in a unified estimation problem. This approach allows an mTRF model at each virtual current dipole, and estimates those mTRFs collectively to optimize the prediction of MEG measurements in sensor space (<xref ref-type="bibr" rid="bib16">Das et al., 2020</xref>).</p></sec><sec id="s3-5"><title>Choosing the reference strategy</title><p>Since EEG data consists of voltage measurements, but only voltage differences are physically meaningful, the choice of referencing strategy matters. There are several common choices for a voltage reference for EEG data, including the average signal recorded at the mastoid electrodes, the central electrode Cz, and the average across all channels (also called the common average reference). Although the reference has minimal influence on the magnitude of the prediction accuracies averaged across channels or the latencies of the TRF peaks, it does have a substantial impact on the distribution across the EEG channels (<xref ref-type="fig" rid="fig7">Figure 7</xref>). This is because, depending on the chosen referencing strategy, the neural sources are projected differently across the scalp. Therefore, the activity of the neural dipoles of interest might be more (or less) prominent depending on the chosen referencing strategy.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Comparison of EEG reference strategies.</title><p>Predictive power topographies and temporal response functions (TRFs) to the acoustic envelope, according to three different referencing strategies: the average of the left and right mastoids (top), the central electrode Cz (middle), and the common average (bottom). (<bold>A</bold>) Visualization of the predictive power obtained with the different referencing strategies (color represents the percent variability explained). (<bold>B</bold>) The envelope TRFs for the different referencing strategies. The insets indicate the topographies corresponding to the vertical red dashed lines at latencies of 40, 140, and 240 ms. Source code: <ext-link ext-link-type="uri" xlink:href="https://github.com/Eelbrain/Alice/blob/main/figures/Reference-strategy.py">figures/Reference-strategy.py</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85012-fig7-v2.tif"/></fig></sec><sec id="s3-6"><title>Further applications</title><p>While cortical processing of speech has been a primary application for mTRF analysis, the technique has potential applications in any domain where stimuli unfold in time, and has already been successfully applied to music perception (<xref ref-type="bibr" rid="bib41">Liberto et al., 2021</xref>; <xref ref-type="bibr" rid="bib40">Leahy et al., 2021</xref>), audiovisual speech perception (<xref ref-type="bibr" rid="bib14">Crosse et al., 2016b</xref>), and subcortical auditory processing (<xref ref-type="bibr" rid="bib42">Maddox and Lee, 2018</xref>). Furthermore, mTRF analysis as discussed here assumes that TRFs are static across time. However, this is not always a valid assumption. For example, in multi-talker speech, TRFs to speech features change as a function of whether the listener attends to the given speech stream or not (<xref ref-type="bibr" rid="bib21">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="bib6">Brodbeck et al., 2018a</xref>; <xref ref-type="bibr" rid="bib12">Broderick et al., 2018</xref>). Thus, moment-to-moment fluctuations in attention might be associated with corresponding changes in the TRFs. While modeling this is a highly complex problem, with even more degrees of freedom, some initial progress has been made toward estimating mTRF models with dynamic TRFs that can change over time (<xref ref-type="bibr" rid="bib2">Babadi et al., 2010</xref>; <xref ref-type="bibr" rid="bib45">Miran et al., 2018</xref>; <xref ref-type="bibr" rid="bib50">Presacco et al., 2019</xref>).</p></sec><sec id="s3-7"><title>Conclusions</title><p>TRF analysis has several advantages over ERP-based methods. In the <italic>Results</italic> section we demonstrated, with several examples, how TRFs can be used to estimate the brain response associated with a given feature while controlling for (1) responses associated with other features that are correlated with the feature of interest, including correlations across different time lags (e.g. controlling for acoustics while analyzing responses to words), and (2) overlapping responses to events close in time (e.g. disentangling early responses to the current word from late responses to the previous word). In addition, analysis of the prediction accuracy allows for determining whether a given feature makes a unique contribution over and beyond other features of the same stimulus. This is especially important in naturalistic stimuli, where features are correlated in complex ways, because neural response estimates may be reliably different from zero even if the corresponding predictor does not provide unique predictive power. Finally, using cross-validation, models are assessed based on their <italic>predictive</italic> power, not just their <italic>explanatory</italic> power, thus providing an overall stronger test than conventional models.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>This section describes each step towards a group-level analysis, starting from the data that is included in the open Alice EEG dataset (<xref ref-type="bibr" rid="bib3">Bhattasali et al., 2020</xref>): EEG recordings, stimulus Wave (audio) files, and a comma-separated values (CSV) table with information about the words contained in the audiobook stimuli.</p><sec id="s4-1"><title>Overall architecture of the Eelbrain toolbox</title><p>Eelbrain provides high-level functionality to represent time series data (such as EEG) and functions to work with this data. The <monospace>Dataset</monospace> class provides a data-table in which each row represents a measurement <italic>case</italic> (e.g. a trial), similar to a <monospace>dataframe</monospace> in R. The <monospace>Var</monospace> and <monospace>Factor</monospace> classes represent continuous and categorical columns in such a <monospace>Dataset</monospace>. In addition, the <monospace>NDVar</monospace> class (<italic>n</italic>-dimensional variable) provides a container for <italic>n</italic>-dimensional data. An <monospace>NDVar</monospace> instance also carries meta-information, for example the sampling rate of a time series, and sensor locations in EEG data. This allows other functions to access that information without user intervention. For example, a plotting function can directly generate topographic plots of an <monospace>NDVar</monospace> representing event-related potential data, without the user specifying which data point corresponds to which sensor, which dimension corresponds to the time axis, what the sampling rate is, etc. More detailed introductions can be found in the <ext-link ext-link-type="uri" xlink:href="https://eelbrain.readthedocs.io/en/stable/auto_examples/index.html">Examples</ext-link> section of the Eelbrain online documentation.</p></sec><sec id="s4-2"><title>Time series data</title><p>In the mTRF paradigm, a time series (for example, voltage at an EEG channel) is modeled as a linear function of one or several other time series (for example, the acoustic envelope of speech). The first step for an mTRF model is thus bringing different time-dependent variables into a common representational format. This is illustrated in <xref ref-type="fig" rid="fig8">Figure 8</xref>, which shows an excerpt from the first stimulus in the Alice dataset aligned to the EEG data from the first subject, along with different representations of the stimulus, which can model different neural representations.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Time series representations of different commonly used speech representations, aligned with the EEG data.</title><p>EEG: band-pass filtered EEG responses. Sound Wave: acoustic wave form, time-aligned to the EEG data. Gammatone Spectrogram: spectrogram representation modeling processing at the auditory periphery. Gammatone 8 Bands: the gammatone spectrogram binned into 8 equal-width frequency bins for computational efficiency. Gammatone Envelope: sum of the gammatone spectrogram across all frequency bands, reflecting the broadband acoustic envelope. Acoustic Onsets: acoustic onset spectrogram, a transformation of the gammatone spectrogram using a neurally inspired model of auditory edge detection. Words: a uniform impulse at each word onsets, predicting a constant response to all words. N-Gram: an impulse at each word onset, scaled with that word’s surprisal, estimated from an <italic>n</italic>-gram language model. This predictor will predict brain responses to words that scale with how surprising each word is in its context. N-Gram Lexical: N-Gram surprisal only at content words, predicting a response that scales with surprisal and occurs at content words only. Source code: <ext-link ext-link-type="uri" xlink:href="https://github.com/Eelbrain/Alice/blob/main/figures/Time-series.py">figures/Time-series.py</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85012-fig8-v2.tif"/></fig><p>As <xref ref-type="fig" rid="fig8">Figure 8</xref> suggests, time series will often have different dimensions. For example, EEG data might be two-dimensional with a time and a sensor dimension; a predictor might be one-dimensional, such as the acoustic envelope, or also have multiple dimensions, such as a spectrogram with time and frequency dimensions. To simplify working with different arbitrary dimensions, Eelbrain uses the <monospace>NDVar</monospace> (<inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-dimensional variable) class. An <monospace>NDVar</monospace> instance associates an <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-dimensional numpy array (<xref ref-type="bibr" rid="bib33">Harris et al., 2020</xref>) with <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> dimension descriptors. For example, the EEG measurements can be represented by a two-dimensional <monospace>NDVar</monospace> with two dimensions characterizing the EEG sensor layout (<monospace>Sensor</monospace>) and the time axis, as a uniform time series (<monospace>UTS</monospace>).</p><p>The first step for the mTRF analysis is thus to import the EEG measurements and predictor variables as <monospace>NDVar</monospace> objects, and align them on the time axis, i.e., make sure they are described by identical <monospace>UTS</monospace> dimensions. The <monospace>eelbrain.load</monospace> module provides functions for importing different formats directly, such as MNE-Python objects and Wave files (<monospace>NDVar</monospace> objects can also be constructed directly from numpy arrays). Keeping information about dimensions on the <monospace>NDVar</monospace> objects allows for concise and readable code for tasks such as aligning, plotting, etc. The code for <xref ref-type="fig" rid="fig2">Figure 2</xref> includes an example of loading a Wave file and aligning its time axis to the EEG data through filtering and resampling.</p><sec id="s4-2-1"><title>EEG data</title><p>EEG data should be pre-processed according to common standards. In the Python ecosystem, MNE-Python offers a unified interface to a variety of EEG file formats and preprocessing routines (<xref ref-type="bibr" rid="bib31">Gramfort et al., 2014</xref>). Here, we rely on the preprocessed data provided with the Alice EEG dataset, referenced to the averaged mastoids, and processed for artifact reduction with independent component analysis (see <xref ref-type="bibr" rid="bib3">Bhattasali et al., 2020</xref>). However, a crucial additional step is filtering and downsampling the EEG data. When analyzing continuous electrophysiological recordings, removing low frequencies (i.e. high-pass filtering) takes the place of baseline correction, by removing high-amplitude slow drifts in the EEG signal which would otherwise overshadow effects of interest. Removing high frequencies, beyond the signals of interest (low-pass filtering), reduces noise and, more crucially, allows conducting the analysis at a lower sampling rate. This makes the analysis faster, because TRF estimation is computationally demanding, and processing times scale with the sampling rate. The major cortical phase-locked responses decrease quickly above 10 Hz (<xref ref-type="bibr" rid="bib22">Ding et al., 2014</xref>), although there can be exceptions, such as a pitch-following response up to 100 Hz (<xref ref-type="bibr" rid="bib36">Kulasingham et al., 2020</xref>). For the purpose of this tutorial, we are interested in the range of common cortical responses and apply a 0.5–20 Hz band-pass filter. Theoretically, a sampling rate exceeding two times the highest frequency (also known as Nyquist frequency) is necessary for a faithful representation of the signal. However, <xref ref-type="bibr" rid="bib47">Nunez and Srinivasan, 2006</xref> recommend a sampling rate of 2.5 times the highest frequency due to various empirical considerations, such as the presence of random jitters, finite roll-off of the band-pass filter, etc. An even higher rate could be desirable for some secondary analysis and visualization, since it leads to smoother results. Here, we conduct the analysis with a sampling rate of 100 Hz.</p><p>EEG data usually contain markers that indicate the start of the stimulus presentation. These can be used to quickly extract EEG data time-locked to the stimuli in the required time series format, i.e., as a two-dimensional <monospace>NDVar</monospace> with <monospace>Sensor</monospace> and <monospace>UTS</monospace> dimensions (see source code to <xref ref-type="fig" rid="fig2">Figure 2</xref>).</p></sec><sec id="s4-2-2"><title>Predictor variables</title><p>Any hypothesis about time-locked neural processing that can be quantified can be represented by a time series derived from the stimulus. Here, we will illustrate two approaches: The first approach implements hypotheses about spectro-temporal transformations of the acoustic signal by directly applying those transformations to the speech waveform. The second approach implements hypotheses about linguistic processing based on experimenter-determined, discrete linguistic events.</p><sec id="s4-2-2-1"><title>Time-continuous predictor variables: Gammatone spectrogram and derivatives</title><p>A common starting point for modeling acoustic responses is through a model of the cochlear transformation of the sound. Here, we use the gammatone spectrogram method to estimate cochlear transformations (<xref ref-type="bibr" rid="bib49">Patterson et al., 1992</xref>; <xref ref-type="bibr" rid="bib35">Heeris, 2018</xref>). A gammatone spectrogram is initially a high-dimensional representation, with more than a hundred time series representing the acoustic power at different frequency bands. For computational efficacy, the number of bands can be reduced by summarizing several contiguous bands into one. Here, we use eight bands as a compromise that leaves the global acoustic structure intact (<xref ref-type="fig" rid="fig8">Figure 8</xref>). An extreme form of this dimension reduction is using the acoustic envelope, which summarizes the entire spectrogram with a single band.</p><p>In addition to representing raw acoustic features, the auditory cortex is known to prominently represent acoustic onsets (<xref ref-type="bibr" rid="bib17">Daube et al., 2019</xref>). Here, we model such representations by applying the neurally inspired auditory edge detection transformation to the gammatone spectrogram (<xref ref-type="bibr" rid="bib8">Brodbeck et al., 2020</xref>). It is also common to approximate such a transformation through the half-wave rectified derivative of the acoustic envelope (<xref ref-type="bibr" rid="bib25">Fiedler et al., 2017</xref>; <xref ref-type="bibr" rid="bib17">Daube et al., 2019</xref>).</p><p>Because these predictor variables will be used repeatedly, it is convenient to generate them once and save them for future use. The script <monospace>predictors/make_gammatone.py</monospace> loops through all stimuli, computes a high-dimensional gammatone spectrogram (a two-dimensional <monospace>NDVar</monospace> with <monospace>frequency</monospace> and <monospace>UTS</monospace> dimensions), and saves it as a Python pickle file via the <monospace>eelbrain.save</monospace> module. The script <monospace>predictors/make_gammatone_predictors.py</monospace> loads these high-dimensional spectrograms via the <monospace>eelbrain.load</monospace> module and resamples them to serve as predictors, and it also applies the onset transformation and saves the resulting predictors.</p><p>Eelbrain provides functions to quickly generate this auditory model from audio files (<monospace>gammatone_bank</monospace> and <monospace>edge_detector</monospace>), as well as some basic signal processing routines for NDVars (e.g. <monospace>filter_data</monospace>; see the <ext-link ext-link-type="uri" xlink:href="https://eelbrain.readthedocs.io/en/stable/reference.html">Reference</ext-link> section of the online documentation for a list). Different models can be constructed using different tools and converted to <monospace>NDVar</monospace>s (for an example of importing data from a <monospace>*.mat</monospace> file, see the <ext-link ext-link-type="uri" xlink:href="https://eelbrain.readthedocs.io/en/stable/auto_examples/temporal-response-functions/mtrf.html">EEG speech envelope TRF</ext-link> example of the online documentation).</p></sec><sec id="s4-2-2-2"><title>Discrete predictor variables</title><p>The analysis of linguistic representations commonly relies on forced alignment, a method that infers time-stamps for phoneme and word boundaries by comparing the sound waveform with a transcript. An example of an open-source forced aligner with extensive documentation is the Montreal Forced Aligner (<xref ref-type="bibr" rid="bib44">McAuliffe et al., 2017</xref>). Because the forced alignment procedure requires some additional steps that are well documented by the respective aligners we skip it here, and instead use the word-onset time-stamps provided with the Alice dataset.</p><p>Discrete predictors come in two varieties: constant magnitude impulses and variable magnitude impulses (see <xref ref-type="fig" rid="fig8">Figure 8</xref>, lower half). Constant magnitude impulses always have the same magnitude, for example an impulse of magnitude 1 at each word onset. Such a predictor implements the hypothesis that all words are associated with a shared characteristic brain response, similar to an event-related potential (ERP). The TRF estimation algorithm will then determine the latencies relative to those impulses at which the brain exhibits a consistent response. Variable magnitude impulses implement the hypothesis that the brain response to each word varies systematically in amplitude with some quantity. For example, the N400 amplitude is assumed to co-vary with how surprising the word is in its context. A predictor with an impulse at each word onset, whose magnitude is equal to the surprisal of that word, will enable predicting a stereotyped response to words whose amplitude linearly varies with word surprisal. The linking hypothesis here is that for each event, the brain responds with population activity that scales in amplitude with how surprising that event is, or how much new information it provides (see e.g. <xref ref-type="bibr" rid="bib10">Brodbeck et al., 2022</xref>).</p><p>The Alice dataset comes with a table including all time-stamps and several linguistic properties (<monospace>stimuli/AliceChapterOne-EEG.csv</monospace>). Each row of this table represents one word, and contains the time point at which the word starts in the audio file, as well the surprisal values that were used to relate the EEG signal to several language models in the original analysis (<xref ref-type="bibr" rid="bib5">Brennan et al., 2019</xref>). Such a table listing event times and corresponding feature values is sufficient for constructing appropriate regressors on the fly, and has a much lower memory footprint than a complete time series. The script <monospace>predictors/make_word_predictors.py</monospace> converts the table into the Eelbrain Dataset format that can be directly used to construct the regressor as an <monospace>NDVar</monospace>. To keep a common file structure scheme with the continuous regressors, such a table is generated for each stimulus.</p><p>Eelbrain provides a function for turning any time/value combination reflecting the occurrence of discrete events into a continuous predictor variable: <monospace>event_impulse_predictor</monospace>. The time/value pairs can either be constructed directly in Python, or can be imported, for example from a text file, using the <monospace>load.tsv</monospace> function.</p></sec></sec></sec><sec id="s4-3"><title>TRF estimation</title><sec id="s4-3-1"><title>Background: The convolution model</title><p>The key assumption behind the mTRF approach is that the dependent variable, <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, is the result of a convolution (linear filtering) of one or several predictor variables, <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, with a corresponding filter kernel <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For a single predictor variable, the model is formulated as the convolution of the predictor variable with a one-dimensional filter kernel. For example, if <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the value of an EEG channel at time <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> the value of the acoustic envelope at time <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ8"><mml:math id="m8"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow/></mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the filter kernel, also known as TRF, and <inline-formula><mml:math id="inf23"><mml:mi>τ</mml:mi></mml:math></inline-formula> enumerates the time delays or lags between <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> at which <inline-formula><mml:math id="inf26"><mml:mi>x</mml:mi></mml:math></inline-formula> can influence <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. To extend this approach to multiple predictor variables, it is assumed that the individual filter responses are additive. In that case, <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> consists of <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> predictor time series and thus has two dimensions, one being the time axis and the other reflecting <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> different predictor variables. The corresponding mTRF is also two-dimensional, consisting of one TRF for each predictor variable:<disp-formula id="equ9"><mml:math id="m9"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow/></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow/></mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>This model allows predicting a dependent variable <inline-formula><mml:math id="inf31"><mml:mi>y</mml:mi></mml:math></inline-formula>, given predictors <inline-formula><mml:math id="inf32"><mml:mi>x</mml:mi></mml:math></inline-formula> and mTRF <inline-formula><mml:math id="inf33"><mml:mi>h</mml:mi></mml:math></inline-formula>.</p><p>In neural data analysis scenarios, typically the measured brain response and the stimuli are known, whereas the filter kernel <inline-formula><mml:math id="inf34"><mml:mi>h</mml:mi></mml:math></inline-formula> is unknown. This leads to two reformulations of the general problem in which <inline-formula><mml:math id="inf35"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf36"><mml:mi>y</mml:mi></mml:math></inline-formula> are known, and <inline-formula><mml:math id="inf37"><mml:mi>h</mml:mi></mml:math></inline-formula> is to be estimated; these represent alternative approaches for analysis. In the first, the so-called forward or encoding model, <inline-formula><mml:math id="inf38"><mml:mi>h</mml:mi></mml:math></inline-formula> is optimized to predict brain responses from stimulus representations. In the second, the so-called backward, or decoding model, <inline-formula><mml:math id="inf39"><mml:mi>h</mml:mi></mml:math></inline-formula> is optimized to reconstruct a stimulus representation from the neural measurements. The problems can both be expressed in the same general form and solved with the same algorithms. Eelbrain provides an implementation of the boosting algorithm (<xref ref-type="bibr" rid="bib18">David et al., 2007</xref>), further described in Background: Boosting implementation below.</p></sec><sec id="s4-3-2"><title>Forward model (encoding)</title><p>Given a continuous measurement and one or more temporally aligned predictor variables, the reverse correlation problem consists of finding the filter kernel, or mTRF, that optimally predicts the response from the stimuli. The result of the convolution now becomes the predicted response:<disp-formula id="equ10"><label>(8)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The goal of the algorithm estimating the mTRF <inline-formula><mml:math id="inf40"><mml:mi>h</mml:mi></mml:math></inline-formula> is to minimize the difference between the measured response <inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and the predicted response <inline-formula><mml:math id="inf42"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>. <xref ref-type="fig" rid="fig2">Figure 2</xref> illustrates the estimation of a forward model for EEG data.</p><p>The <monospace>eelbrain.boosting</monospace> function provides a high-level interface for estimating mTRFs and returns a <monospace>BoostingResult</monospace> object with different attributes containing the mTRF and several model fit metrics for further analysis. Usually, for a forward model, the brain response is predicted from the predictors using positive (i.e. causal) lags. For example,</p><p><code xml:space="preserve">trf = boosting(eeg, envelope, 0, 0.500)</code></p><p>would estimate a TRF to predict EEG data from the acoustic envelope, with stimulus lags ranging from 0 to 500 ms (Eelbrain uses seconds as the default time unit). This means that an event at a specific time in the acoustic envelope could influence the EEG response in a window between 0 and 500 ms later. If the dependent variable has multiple measurements, for example multiple EEG channels, Eelbrain automatically assumes a mass-univariate approach and estimates a TRF for each channel. Negative lags, as in</p><p><code xml:space="preserve">trf = boosting(eeg, envelope, –0.100, 0.500)</code></p><p>are non-causal in the sense that they assume a brain response that precedes the stimulus event. Such estimates can nevertheless be useful for at least two reasons. First, if the stimulus genuinely represents information in time, then non-causal lags can be used as an estimate of the noise floor. As such they are analogous to the baseline in ERP analyses, i.e., they indicate how variable TRFs are at time points at which no true response is expected. Second, when predictor variables are experimenter-determined, the temporal precision of the predictor time series might often be reduced, and information in the acoustic speech signal might in fact precede the predictor variable. In such cases, the negative lags might be diagnostic. For example, force-aligned word and phoneme onsets assume the existence of strict boundaries in the speech signal, when in fact the speech signal can be highly predictive of future phonemes due to coarticulation (e.g. <xref ref-type="bibr" rid="bib54">Salverda et al., 2003</xref>).</p><p>An advantage of the forward model is that it can combine information from multiple predictor variables. The boosting algorithm in particular is robust with a large number of (possibly correlated) predictor variables (e.g. <xref ref-type="bibr" rid="bib19">David and Shamma, 2013</xref>). Eelbrain supports two ways to specify multiple predictor variables. The first is using a multi-dimensional predictor, for example a two-dimensional <monospace>NDVar</monospace>, representing a <monospace>spectrogram</monospace> with multiple <monospace>frequency</monospace> bands. These are used with the boosting function just like one-dimensional time series, and will be treated as multi-dimensional predictor variables, i.e., the different frequency bands will be jointly used for the optimal prediction of the dependent variable:</p><p><code xml:space="preserve">mtrf = boosting(eeg, spectrogram, 0, 0.500)</code></p><p>The second option for using multiple predictor variables is specifying them as a list of <monospace>NDVar</monospace> (one and/or two-dimensional), for example:</p><p><code xml:space="preserve">mtrf = boosting(eeg, [envelope, spectrogram], 0, 0.500)</code></p></sec><sec id="s4-3-3"><title>Backward model (decoding)</title><p>Instead of predicting the EEG measurement from the stimulus, the same algorithm can attempt to reconstruct the stimulus from the EEG response. The filter kernel is then also called a decoder. This can be expressed with <xref ref-type="disp-formula" rid="equ10">Equation 8</xref>, but now <inline-formula><mml:math id="inf43"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> refers to a stimulus variable, for example the speech envelope, and <inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> refers to the EEG measurement at sensor <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and time <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Accordingly, a backward model can be estimated with the same boosting function. For example,</p><p><code xml:space="preserve">decoder = boosting(envelope, eeg, –0.500, 0)</code></p><p>estimates a decoder model to reconstruct the envelope from the EEG data using a 500 ms window. Note the specification of delay values <inline-formula><mml:math id="inf47"><mml:mi>τ</mml:mi></mml:math></inline-formula> in the boosting function, from the point of view of the predictor variable: because each point in the EEG response reflects the stimulus preceding it, the delay values are negative, i.e., a given point in the EEG response should be used to reconstruct the envelope in the 500 ms window preceding the EEG response.</p><p>Because the EEG channels now function as the multivariate predictor variable, all EEG channels are used jointly to reconstruct the envelope. An advantage of the backward model is that it combines data from all EEG sensors to reconstruct a stimulus feature. It provides a powerful measure of how much information about the stimulus is contained in the brain responses, taken as a whole. A downside is that it does not provide a straight-forward way for distinguishing responses that are due to several, correlated predictor variables. For this reason, we will not further discuss it here. However, backward models have applications in other domains, where questions are not about specific representations, for instance attention decoding in auditory scenes (<xref ref-type="bibr" rid="bib48">O’Sullivan et al., 2015</xref>).</p></sec></sec><sec id="s4-4"><title>Background: The boosting algorithm</title><p>The general TRF estimation problem, i.e., finding the optimal filter kernels in forward and backward models, can be solved with different approaches. The Eelbrain toolkit implements the boosting algorithm, which is resilient to over-fitting and performs well with correlated predictor variables (<xref ref-type="bibr" rid="bib18">David et al., 2007</xref>; <xref ref-type="bibr" rid="bib19">David and Shamma, 2013</xref>). The boosting algorithm is intentionally designed with a bias to produce sparse mTRFs, i.e., mTRFs in which many elements are exactly zero. The algorithm thus incorporates a so-called sparsity prior, i.e., a prior to favor sparser models over alternatives (see <italic>Sparsity prior</italic> above).</p><p>Boosting starts by dividing the data into training and validation folds, and initializing an empty mTRF (all values <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are initially set to 0). It then iteratively uses the training data to find the element in the filter kernel which, when changed by a fixed small amount <monospace>delta</monospace>, leads to the largest error reduction. The following pseudo-code shows the general boosting algorithm:</p><p><code xml:space="preserve">mTRF[:] = 0
n_validation_error_increased = 0

while n_validation_error_increased &lt; 2:
  current_training_error = error of mTRF in training set
  current_validation_error = error of mTRF in validation set

  # Find training error for all possible coordinate steps
  for element in mTRF:
    error_add[element] = error of mTRF with element = element + ∆
    error_sub[element] = error of mTRF with element = element - ∆

  # If no error reduction is possible, reduce ∆ or stop
  if smallest error in error_add, error_sub &gt; current_training_error:
   ∆ *= 0.5
   if ∆ ≥ mindelta:
     continue
   else:
     break

  # Update the mTRF and check validation set error
  mTRF = mTRF corresponding to smallest error in error_add, error_sub
  new_validation_error = error of mTRF in validation set
 if new_validation_error &gt; current_validation_error:
  n_validation_error_increased += 
else:
  n_validation_error_increased = 0

mTRF = the mTRF in the history with the smallest validation error</code></p><p>For multiple predictors, the search is performed over all the predictors as well as time lags, essentially letting the different predictors compete to explain the dependent variable. At each step, only a single element in the mTRF is modified. In that respect, boosting is a coordinate descent algorithm because it moves only at right angles in the parameter space. This promotes sparsity, as most elements stay at zero in every step. After each such delta change, the validation data is consulted to verify that the error is also reduced in the validation data. Once the error starts increasing in the validation data, the training stops. This early stopping strategy prevents the model from overfitting to the training data. In other words, the early stopping strategy in the boosting algorithm acts as an implicit prior to promote sparsity, i.e., it forces unimportant filter kernel coefficients to remain exactly 0.</p><p>The default implementation of the boosting algorithm constructs the kernel from impulses (each element <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is modified independently), which can lead to temporally discontinuous TRFs. In order to derive smoother TRFs, TRFs can be constructed from a basis of smooth window functions instead. In Eelbrain, the basis shape and window size are controlled by the <monospace>basis</monospace> and <monospace>basis_window</monospace> parameters in the boosting function. Their use is described in more detail in <italic>Meta-parameter: Basis function</italic> below.</p><p>Additionally, when using multiple predictors, it may be undesirable to stop the entire model training when a single predictor starts overfitting. In that case, the <monospace>selective_stopping</monospace> option allows freezing only the predictor which caused the overfitting, while training of the TRF components corresponding to the remaining predictors continues, until all predictors are frozen.</p><p>Finally, the default error metric for evaluating model quality is the widely used <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> error. Due to squaring, however, the ﻿﻿<inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> error is disproportionately sensitive to time points with large errors. In electrophysiology, large errors are typically produced by artifacts, and it is undesirable to give such artifacts a large influence on the model estimation. Since it is not trivial to exclude time intervals containing such artifacts from the analysis in continuous data, Eelbrain also allows the use of the <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> error norm through the <monospace>error='l1'</monospace> argument, which improves robustness against such outlier data.</p><p>Usually, both the dependent variable and the predictor are centered around zero, and either standardized against the standard deviation (for <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> error) or normalized by the <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> norm (for <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> error) before starting the boosting algorithm. The centering step is crucial to eliminate the intercept, since the boosting algorithm prefers sparse solutions, i.e., solutions with exact zero coefficients. When multiple predictors are used, this preprocessing step is applied separately to each of them, to ensure none of them affects the boosting procedure disproportionately. If required, this preprocessing step can be skipped through an argument to the boosting function: <monospace>scale_data=False</monospace>.</p><sec id="s4-4-1"><title>Cross-validation</title><p>By default, the boosting function trains an mTRF model on all available data. Reserving some data for cross-validation, which allows model evaluation without overfitting, can be enabled by setting the test parameter to <monospace>test=True</monospace>. Since the boosting algorithm already divides the data into training and validation sets, enabling cross-validation entails splitting the data into three sets for each run: training, validation, and test sets. While the training and validation segments are consulted for estimating the TRFs (as described above), the test segment does not influence the estimation of the TRFs at all. Only once the TRF estimation is finalized, the final TRF is used to predict the responses in the test segment. To use the data optimally, Eelbrain automatically implements <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-fold cross-validation, whereby the data is divided into <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> partitions, and each partition serves as the test set once (see the <ext-link ext-link-type="uri" xlink:href="https://eelbrain.readthedocs.io/en/latest/auto_examples/temporal-response-functions/partitions.html#sphx-glr-auto-examples-temporal-response-functions-partitions-py">Data partitions</ext-link> Eelbrain example). Thus, through <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-fold cross-validation, the whole response time series can be predicted from unseen data. The proportion of the explained variability in this response constitutes an unbiased estimate of the predictive power of the given predictors. The <monospace>BoostingResult</monospace> object returned by the boosting function contains all these metrics as attributes for further analysis.</p></sec><sec id="s4-4-2"><title>Comparison to ridge regression</title><p>An alternative approach to boosting is ridge regression (<xref ref-type="bibr" rid="bib15">Crosse et al., 2021</xref>), which uses Tikonov regularization. Tikonov regularization biases the mTRFs to suppress mTRFs in the stimulus subspace with a low signal-to-noise ratio, effectively imposing a smooth prior on the mTRF. In other words, Tikonov regularization tends to distribute the TRF power over all time lags. In contrast, boosting minimizes nonzero parameters in mTRFs, effectively imposing a sparseness prior on the mTRF, i.e., boosting concentrates the TRF power within a few time-lags. For a detailed discussion on the differences between boosting and ridge regression see <xref ref-type="bibr" rid="bib18">David et al., 2007</xref>. <xref ref-type="fig" rid="fig9">Figure 9</xref> shows a side-by-side comparison between boosting and ridge regression in a simulation study where a gammatone spectrogram was used to simulate the time-locked EEG response.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Simulation study comparing boosting with ridge regression when modeling multiple correlated variables.</title><p>(<bold>A</bold>) A pre-defined multivariate temporal response functions (mTRF) was used to generate simulated EEG data (left panel), and then reconstructed by boosting and ridge regression (middle and right panel). (<bold>B</bold>) Overlays of boosting, ridge regression, and ground truth TRFs at different center frequencies for comparison. Note that the ridge TRF follows the ground truth closely, but produces many false positives. On the contrary, the boosting TRF enjoys an excellent true negative rate, at the expense of biasing TRF peaks and troughs toward 0. a.u.: arbitrary units. Source code: <ext-link ext-link-type="uri" xlink:href="https://github.com/Eelbrain/Alice/blob/main/figures/Collinearity.py">figures/Collinearity.py</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85012-fig9-v2.tif"/></fig><p>Briefly, two adjacent gammatone bands (structured, highly correlated variables) were assumed to drive the auditory response with a spatiotemporally alternating pattern (<xref ref-type="fig" rid="fig9">Figure 9A</xref>, left panel). The simulated auditory response was then corrupted with additive pink noise to simulate EEG activity. We used the boosting function from Eelbrain and the ridge regression implementation from the pyEEG toolbox (<xref ref-type="bibr" rid="bib60">Weissbart et al., 2023</xref>) to recover the mTRFs from the simulated EEG. For both algorithms, we used 10-fold cross-validation to select the optimum amount of regularization. With boosting we further tuned the <monospace>selective_stopping</monospace> hyper-parameter according to the explained variances in unseen data (with the <monospace>test = True</monospace> setting), i.e., we chose the model with the maximal <monospace>selective_stopping</monospace>, after which the explained variance started decreasing. For boosting, the TRFs are constructed from a basis of hamming windows with a window length of 50 ms, resulting in smoother TRFs. Both the mTRFs recovered by boosting and ridge regression capture the dominant features of the ground truth mTRF. However, the boosting mTRF has only a few peaks and troughs, giving a simpler representation with peaks and troughs closely corresponding to those of the ground truth, while the ridge regression mTRF includes many additional noisy peaks and troughs (<xref ref-type="fig" rid="fig9">Figure 9A</xref>, middle, and right panel; see also <xref ref-type="bibr" rid="bib18">David et al., 2007</xref>; <xref ref-type="bibr" rid="bib19">David and Shamma, 2013</xref>). In order to emphasize the differences between the estimates, the mTRF components are shown at the two Gammatone frequency bands driving the response (<xref ref-type="fig" rid="fig9">Figure 9B</xref>, left and middle panel), and at another frequency band with no response (rightmost panel). The ridge regression follows the dominant peaks and troughs of the ground truth mTRF closely, but, in doing so, acquires numerous false positives, i.e., small non-zero values when the true mTRF is actually zero. On the other hand, the boosting mTRF displays a high true negative rate, while biasing the peaks and troughs towards zero. The two algorithms are further compared using multiple criteria in <xref ref-type="bibr" rid="bib37">Kulasingham and Simon, 2023</xref>.</p></sec></sec><sec id="s4-5"><title>Evaluating predictive power</title><p>In practice, a research question can often be operationalized by asking whether a specific predictor variable is neurally represented, i.e., whether it is a significant predictor of brain activity. However, different predictor variables of interest are very often correlated in naturalistic stimuli such as speech. It is thus important to test the explanatory power of a given variable while controlling for the effect of other, correlated variables. A powerful method for this is comparing the predictive power of minimally differing sets of predictor variables using cross-validation.</p><p>In this context, we use the term ‘model’ to refer to a set of predictor variables, and ‘model comparison’ refers to the practice of comparing the predictive power of two models on held-out data. It is important to re-estimate the mTRFs for each model under investigation to determine the effective predictive power of that model, because mTRFs are sensitive to correlation between predictors and can thus change depending on what other predictors are included during estimation.</p><p>When building models for a specific model comparison, we recommend a hierarchical approach: both models should include lower-level properties that the experimenter wants to control for, while the models should differ only in the feature of interest. For example, to investigate whether words are associated with a significant response after controlling for acoustic representations, one could compare the explained variability of <xref ref-type="disp-formula" rid="equ11">Equation 9</xref> and <xref ref-type="disp-formula" rid="equ12">Equation 10</xref>:<disp-formula id="equ11"><label>(9)</label><mml:math id="m11"><mml:mrow><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="equ12"><label>(10)</label><mml:math id="m12"><mml:mrow><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mtext> </mml:mtext><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></disp-formula></p><p>If the model in <xref ref-type="disp-formula" rid="equ12">Equation 10</xref> is able to predict held-out test data better than that in <xref ref-type="disp-formula" rid="equ11">Equation 9</xref>, then this difference can be attributed to a predictive power of word onset impulses over and above the spectrogram-based representations.</p><p>Estimating such mTRF models is the computationally most demanding part of this analysis. For this reason, it usually makes sense to store the result of the individual estimates. The script <monospace>analysis/estimate_trfs.py</monospace> implements this, by looping through all subjects, fitting mTRFs for multiple models, and saving the results for future analysis.</p></sec><sec id="s4-6"><title>Group analysis</title><p>In order to statistically answer questions about the predictive power of different models we will need to combine the data from different measurements, usually from different subjects. To combine data from multiple subjects along with meta-information such as subject and condition labels, Eelbrain provides the <monospace>Dataset</monospace> class, analogous to data-table representations in other statistics programs, such as a <monospace>dataframe</monospace> in R or Pandas, but with the added capability of handling data from <monospace>NDVar</monospace>s with arbitrary dimensionality.</p><p>A standard way of constructing a <monospace>Dataset</monospace> is collecting the individual cases, or rows, of the desired data table, and then combining them. The following short script provides a template for assembling a table of model predictive power for several subjects and two models (assuming the mTRF models have been estimated and saved accordingly):</p><p><code xml:space="preserve">cases = []
for subject in ['1', '2', '3']:
   for model in ['sgrams', 'sgrams+words']:
     mtrf = load.unpickle(f&quot;path/to/{subject}_{model}.pickle&quot;)     cases.append([subject, model, mtrf.proportion_explained])

column_names = ['subject', 'model', 'explained']
data = Dataset.from_caselist(column_names, cases)</code></p><p>Thus, even though the <monospace>proportion_explained</monospace> attribute might contain 64 EEG channels (i.e. an <monospace>NDVar</monospace> with <monospace>Sensor</monospace> dimension) it can be handled as a single entry in this data table.</p><sec id="s4-6-1"><title>Statistical tests</title><p>Statistical analysis of mTRFs faces the issue of multiple comparisons common in neuroimaging (<xref ref-type="bibr" rid="bib43">Maris and Oostenveld, 2007</xref>). One way around this is to derive a univariate outcome measure. The (average) predictive power across all sensors is a measure of the overall predictive power of a model. If a prior hypothesis about the location of an effect is available, then the average predictive power at a pre-specified group of sensors can serve as a more targeted measure, for example:</p><p><code xml:space="preserve">sensors = ['45', '34', '35']
data['explained_average'] = data['explained'].mean(sensor = sensors)</code></p><p>Eelbrain implements a limited number of basic univariate statistical tests in the <monospace>test</monospace> module, but more advanced statistical analysis can be performed after exporting the data into other libraries. All univariate entries in a <monospace>Dataset</monospace> can be transferred to a <monospace>pandas.DataFrame</monospace> (<xref ref-type="bibr" rid="bib53">Reback et al., 2021</xref>) with</p><p><code xml:space="preserve">dataframe = data.as_dataframe()</code></p><p>for analysis with other Python libraries like Pingouin (<xref ref-type="bibr" rid="bib58">Vallat, 2018</xref>), or saved as a text file with</p><p><code xml:space="preserve">data.save_txt('data.txt')</code></p><p>to be transferred to another statistics environment like R (<xref ref-type="bibr" rid="bib52">R Development Core Team, 2021</xref>).</p><p>Instead of restricting the analysis to a priori sensor groups, Eelbrain implements several mass-univariate tests (<xref ref-type="bibr" rid="bib46">Nichols and Holmes, 2002</xref>; <xref ref-type="bibr" rid="bib43">Maris and Oostenveld, 2007</xref>; <xref ref-type="bibr" rid="bib55">Smith and Nichols, 2009</xref>). These tests, implemented in the <monospace>eelbrain.testnd</monospace> module (for <italic>n</italic>-dimensional tests), are generally based on calculating a univariate statistic at each outcome measure (for example, a <italic>t</italic> value corresponding to a repeated-measures <italic>t</italic>-test comparing the predictive power of two models at each EEG sensor), and then using a permutation-based approach for estimating a null distribution for calculating <italic>p</italic>-values that control for family-wise error. In Eelbrain, these tests can be applied to <monospace>NDVar</monospace>s similarly to univariate tests, with additional arguments for controlling multiple comparison corrections. The code corresponding to <xref ref-type="fig" rid="fig4">Figure 4</xref> demonstrates a complete group analysis pipeline, from loading pickled mTRF models into a <monospace>Dataset</monospace> to plotting statistical results.</p><p>Mass-univariate tests can provide a detailed characterization of a given comparison, but calculating effect sizes for such results is not straightforward. For power analyses, carefully selected univariate outcome variables are usually more effective.</p></sec></sec><sec id="s4-7"><title>TRF analysis</title><p>While predictive power is the primary measure to assess the quality of a model, the TRFs themselves also provide information about the temporal relationship between the brain response and the stimuli. A TRF is an estimate of the brain’s response to an impulse stimulus of magnitude 1 (i.e. the impulse response). It thus characterizes the brain response as a function of time, relative to stimulus events, analogous to an ERP to a simple stimulus. For example, the TRF estimated to an acoustic envelope representation of speech commonly resembles the ERP to simple tone stimuli. The mTRFs can thus be analyzed in ways that are analogous to ERPs, for example using mass-univariate tests or based on component latencies. <xref ref-type="fig" rid="fig4">Figure 4</xref> demonstrates a representative analysis of TRFs and mTRFs corresponding to different auditory features.</p></sec><sec id="s4-8"><title>Meta-parameter: Basis function</title><p>By default, the fitting algorithm controls each element of a TRF independently of other elements. When a basis function is chosen, the algorithm instead generates the TRF by combining copies of that basis function, one centered on each element of the TRF. There are at least two reasons for selecting a basis function as part of the boosting algorithm for TRF estimation. First, constraining the TRFs to neurally more realistic shapes may increase the predictive power of the TRF model. Second, the resulting TRFs are smoother and thus easier to interpret and compare across subjects. <xref ref-type="fig" rid="fig9">Figure 9</xref> illustrates the effect of using a basis function, comparing a 50 or 100 ms wide Hamming window basis with the default impulse basis (‘0’). In this case, a 50 ms Hamming window basis leads to significantly better predictive power than the default impulse basis (<xref ref-type="fig" rid="fig10">Figure 10A</xref>). TRFs are illustrated using a single sensor which exhibits a strong auditory response (<xref ref-type="fig" rid="fig10">Figure 10B</xref>). <xref ref-type="fig" rid="fig10">Figure 10C</xref> shows that using a wider basis window leads to markedly smoother TRFs, and increases the overlap of the TRFs between subjects. The basis windows themselves are shown in inset D.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>The effect of using a basis function for temporal response function (TRF) estimation.</title><p>(<bold>A</bold>) Predictive power of the TRFs estimated with different basis windows (expressed as a percent of the variability in the EEG data that is explained by the respective TRF model). Error bars indicate the within-subject standard error of the mean, and significance is indicated for pairwise <italic>t</italic>-tests (<italic>df</italic> = 32; *p≤0.05). (<bold>B</bold>) The sensor selected for illustrating the TRFs. (<bold>C</bold>) TRFs for two subject (upper and middle plots) and the average across all subjects (bottom). (<bold>D</bold>) Basis windows. Notice that the impulse basis (‘0’) is only a single sample wide, but it appears as a triangle in a line plot with the apparent width determined by the sampling rate. Source code: <ext-link ext-link-type="uri" xlink:href="https://github.com/Eelbrain/Alice/blob/main/figures/TRF-Basis.py">figures/TRF-Basis.py</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85012-fig10-v2.tif"/></fig></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Validation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Software, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-85012-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The data analyzed here was originally released and can be retrieved from <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7302/Z29C6VNH">here</ext-link>. For the purpose of this tutorial, the data were restructured and rereleased <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.13016/pulf-lndn">here</ext-link>. The companion <ext-link ext-link-type="uri" xlink:href="https://github.com/Eelbrain/Alice">GitHub repository</ext-link> contains code and instructions for replicating all analyses presented in the paper (<xref ref-type="bibr" rid="bib11">Brodbeck et al., 2023</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Bhattasali</surname><given-names>S</given-names></name><name><surname>Das</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Data for: Eelbrain: A Python toolkit for time-continuous analysis with temporal response functions</data-title><source>Digital Repository at the University of Maryland</source><pub-id pub-id-type="doi">10.13016/pulf-lndn</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Brennan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>EEG Datasets for Naturalistic Listening to &quot;Alice in Wonderland&quot;</data-title><source>Deep Blue Data</source><pub-id pub-id-type="doi">10.7302/Z29C6VNH</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We are grateful for support from the following grants: NSF BCS 1754284, NSF BCS 2043903 &amp; NSF IIS 2207770 (CB); NSF SMA 1734892 (JPK &amp; JZS); NIH R01 DC014085 (JPK &amp; JZS); NIH R01 DC019394 (JZS); FWO SB 1SA0620N (MG); ONR MURI N00014-18-1-2670 (SB &amp; PR); and NIH T32 DC017703 (PG).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alday</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>M/EEG analysis of naturalistic stories: a review from speech to language processing</article-title><source>Language, Cognition and Neuroscience</source><volume>34</volume><fpage>457</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1080/23273798.2018.1546882</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Babadi</surname><given-names>B</given-names></name><name><surname>Kalouptsidis</surname><given-names>N</given-names></name><name><surname>Tarokh</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>SPARLS: the sparse RLS algorithm</article-title><source>IEEE Transactions on Signal Processing</source><volume>58</volume><fpage>4013</fpage><lpage>4025</lpage><pub-id pub-id-type="doi">10.1109/TSP.2010.2048103</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bhattasali</surname><given-names>S</given-names></name><name><surname>Brennan</surname><given-names>J</given-names></name><name><surname>Luh</surname><given-names>WM</given-names></name><name><surname>Franzluebbers</surname><given-names>B</given-names></name><name><surname>Hale</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The Alice Datasets</article-title><conf-name>The Alice Datasets: fMRI &amp; EEG Observations of Natural Language ComprehensionProceedings of the 12th Conference on Language Resources and Evaluation</conf-name><fpage>120</fpage><lpage>125</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biesmans</surname><given-names>W</given-names></name><name><surname>Das</surname><given-names>N</given-names></name><name><surname>Francart</surname><given-names>T</given-names></name><name><surname>Bertrand</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Auditory-inspired speech envelope extraction methods for improved EEG-based auditory attention detection in a cocktail party scenario</article-title><source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source><volume>25</volume><fpage>402</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2016.2571900</pub-id><pub-id pub-id-type="pmid">27244743</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brennan</surname><given-names>JR</given-names></name><name><surname>Hale</surname><given-names>JT</given-names></name><name><surname>Bolhuis</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hierarchical structure guides rapid linguistic predictions during naturalistic listening</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0207741</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0207741</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Hong</surname><given-names>LE</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Rapid transformation from auditory to linguistic representations of continuous speech</article-title><source>Current Biology</source><volume>28</volume><fpage>3976</fpage><lpage>3983</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.10.042</pub-id><pub-id pub-id-type="pmid">30503620</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Presacco</surname><given-names>A</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Neural source dynamics of brain responses to continuous stimuli: Speech processing from acoustics to comprehension</article-title><source>NeuroImage</source><volume>172</volume><fpage>162</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.01.042</pub-id><pub-id pub-id-type="pmid">29366698</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Jiao</surname><given-names>A</given-names></name><name><surname>Hong</surname><given-names>LE</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural speech restoration at the cocktail party: Auditory cortex recovers masked speech of both attended and ignored speakers</article-title><source>PLOS Biology</source><volume>18</volume><elocation-id>e3000883</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000883</pub-id><pub-id pub-id-type="pmid">33091003</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Continuous speech processing</article-title><source>Current Opinion in Physiology</source><volume>18</volume><fpage>25</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.cophys.2020.07.014</pub-id><pub-id pub-id-type="pmid">33225119</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Bhattasali</surname><given-names>S</given-names></name><name><surname>Cruz Heredia</surname><given-names>AAL</given-names></name><name><surname>Resnik</surname><given-names>P</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Lau</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Parallel processing in speech perception with local and global representations of linguistic context</article-title><source>eLife</source><volume>11</volume><elocation-id>e72056</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.72056</pub-id><pub-id pub-id-type="pmid">35060904</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Das</surname><given-names>P</given-names></name><name><surname>Gillis</surname><given-names>M</given-names></name><name><surname>Kulasingham</surname><given-names>JP</given-names></name><name><surname>Bhattasali</surname><given-names>S</given-names></name><name><surname>Gaston</surname><given-names>P</given-names></name><name><surname>Resnik</surname><given-names>P</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Alice Dataset for Eelbrain</data-title><version designator="swh:1:rev:54838821d2d5e8bba74661bcf670ff317dbdc429">swh:1:rev:54838821d2d5e8bba74661bcf670ff317dbdc429</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:544ab2cc03831f322751712ff3d1f918e97368e6;origin=https://github.com/Eelbrain/Alice;visit=swh:1:snp:ac9e05c0c98ab96986a065d4932e99c9ae7d4de9;anchor=swh:1:rev:54838821d2d5e8bba74661bcf670ff317dbdc429">https://archive.softwareheritage.org/swh:1:dir:544ab2cc03831f322751712ff3d1f918e97368e6;origin=https://github.com/Eelbrain/Alice;visit=swh:1:snp:ac9e05c0c98ab96986a065d4932e99c9ae7d4de9;anchor=swh:1:rev:54838821d2d5e8bba74661bcf670ff317dbdc429</ext-link></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broderick</surname><given-names>MP</given-names></name><name><surname>Anderson</surname><given-names>AJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Electrophysiological correlates of semantic dissimilarity reflect the comprehension of natural, narrative speech</article-title><source>Current Biology</source><volume>28</volume><fpage>803</fpage><lpage>809</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.01.080</pub-id><pub-id pub-id-type="pmid">29478856</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Bednar</surname><given-names>A</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>The multivariate temporal response function (mTRF) Toolbox: A MATLAB toolbox for relating neural signals to continuous stimuli</article-title><source>Frontiers in Human Neuroscience</source><volume>10</volume><elocation-id>604</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00604</pub-id><pub-id pub-id-type="pmid">27965557</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Eye can hear clearly now: inverse effectiveness in natural audiovisual speech processing relies on long-term crossmodal temporal integration</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>9888</fpage><lpage>9895</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1396-16.2016</pub-id><pub-id pub-id-type="pmid">27656026</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Zuk</surname><given-names>NJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Nidiffer</surname><given-names>AR</given-names></name><name><surname>Molholm</surname><given-names>S</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Linear modeling of neurophysiological responses to speech and other continuous stimuli: methodological considerations for applied research</article-title><source>Frontiers in Neuroscience</source><volume>15</volume><elocation-id>705621</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2021.705621</pub-id><pub-id pub-id-type="pmid">34880719</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Das</surname><given-names>P</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Babadi</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neuro-current response functions: A unified approach to MEG source analysis under the continuous stimuli paradigm</article-title><source>NeuroImage</source><volume>211</volume><elocation-id>116528</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116528</pub-id><pub-id pub-id-type="pmid">31945510</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daube</surname><given-names>C</given-names></name><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Simple acoustic features can explain phoneme-based predictions of cortical responses to speech</article-title><source>Current Biology</source><volume>29</volume><fpage>1924</fpage><lpage>1937</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.04.067</pub-id><pub-id pub-id-type="pmid">31130454</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Estimating sparse spectro-temporal receptive fields with natural stimuli</article-title><source>Network</source><volume>18</volume><fpage>191</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1080/09548980701609235</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Integration over multiple timescales in primary auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>19154</fpage><lpage>19166</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2270-13.2013</pub-id><pub-id pub-id-type="pmid">24305812</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>O’Sullivan</surname><given-names>JA</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Low-frequency cortical entrainment to speech reflects phoneme-level processing</article-title><source>Current Biology</source><volume>25</volume><fpage>2457</fpage><lpage>2465</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.030</pub-id><pub-id pub-id-type="pmid">26412129</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title><source>PNAS</source><volume>109</volume><fpage>11854</fpage><lpage>11859</lpage><pub-id pub-id-type="doi">10.1073/pnas.1205381109</pub-id><pub-id pub-id-type="pmid">22753470</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Chatterjee</surname><given-names>M</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Robust cortical entrainment to the speech envelope relies on the spectro-temporal fine structure</article-title><source>NeuroImage</source><volume>88</volume><fpage>41</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.054</pub-id><pub-id pub-id-type="pmid">24188816</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donhauser</surname><given-names>PW</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Two distinct neural timescales for predictive speech processing</article-title><source>Neuron</source><volume>105</volume><fpage>385</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.019</pub-id><pub-id pub-id-type="pmid">31806493</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Etard</surname><given-names>O</given-names></name><name><surname>Kegler</surname><given-names>M</given-names></name><name><surname>Braiman</surname><given-names>C</given-names></name><name><surname>Forte</surname><given-names>AE</given-names></name><name><surname>Reichenbach</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Decoding of selective attention to continuous speech from the human auditory brainstem response</article-title><source>NeuroImage</source><volume>200</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.06.029</pub-id><pub-id pub-id-type="pmid">31212098</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiedler</surname><given-names>L</given-names></name><name><surname>Wöstmann</surname><given-names>M</given-names></name><name><surname>Graversen</surname><given-names>C</given-names></name><name><surname>Brandmeyer</surname><given-names>A</given-names></name><name><surname>Lunner</surname><given-names>T</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Single-channel in-ear-EEG detects the focus of auditory attention to concurrent tone streams and mixed speech</article-title><source>Journal of Neural Engineering</source><volume>14</volume><elocation-id>036020</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aa66dd</pub-id><pub-id pub-id-type="pmid">28384124</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiedler</surname><given-names>L</given-names></name><name><surname>Wöstmann</surname><given-names>M</given-names></name><name><surname>Herbst</surname><given-names>SK</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Late cortical tracking of ignored speech facilitates neural selectivity in acoustically challenging conditions</article-title><source>NeuroImage</source><volume>186</volume><fpage>33</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.10.057</pub-id><pub-id pub-id-type="pmid">30367953</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fishbach</surname><given-names>A</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name><name><surname>Yeshurun</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Auditory edge detection: A neural model for physiological and psychoacoustical responses to amplitude transients</article-title><source>Journal of Neurophysiology</source><volume>85</volume><fpage>2303</fpage><lpage>2323</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.85.6.2303</pub-id><pub-id pub-id-type="pmid">11387378</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>Applied Regression Analysis and Generalized Linear Models</source><publisher-name>Sage</publisher-name></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freckleton</surname><given-names>RP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>On the misuse of residuals in ecology: regression of residuals vs. multiple regression</article-title><source>Journal of Animal Ecology</source><volume>71</volume><fpage>542</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1046/j.1365-2656.2002.00618.x</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillis</surname><given-names>M</given-names></name><name><surname>Vanthornhout</surname><given-names>J</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Francart</surname><given-names>T</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural markers of speech comprehension: measuring EEG tracking of linguistic speech representations, controlling the speech acoustics</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>10316</fpage><lpage>10329</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0812-21.2021</pub-id><pub-id pub-id-type="pmid">34732519</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Luessi</surname><given-names>M</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Strohmeier</surname><given-names>D</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Hämäläinen</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>MNE software for processing MEG and EEG data</article-title><source>NeuroImage</source><volume>86</volume><fpage>446</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.027</pub-id><pub-id pub-id-type="pmid">24161808</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamilton</surname><given-names>LS</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The revolution will not be controlled: natural stimuli in speech neuroscience</article-title><source>Language, Cognition and Neuroscience</source><volume>35</volume><fpage>573</fpage><lpage>582</lpage><pub-id pub-id-type="doi">10.1080/23273798.2018.1499946</pub-id><pub-id pub-id-type="pmid">32656294</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Picus</surname><given-names>M</given-names></name><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>van Kerkwijk</surname><given-names>MH</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Haldane</surname><given-names>A</given-names></name><name><surname>Del Río</surname><given-names>JF</given-names></name><name><surname>Wiebe</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Gérard-Marchant</surname><given-names>P</given-names></name><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Abbasi</surname><given-names>H</given-names></name><name><surname>Gohlke</surname><given-names>C</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Array programming with NumPy</article-title><source>Nature</source><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Walther</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Forward stagewise regression and the monotone lasso</article-title><source>Electronic Journal of Statistics</source><volume>1</volume><fpage>1</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1214/07-EJS004</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Heeris</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Gammatone Filterbank Toolkit</data-title><source>Filterbank Toolkit</source></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kulasingham</surname><given-names>JP</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Presacco</surname><given-names>A</given-names></name><name><surname>Kuchinsky</surname><given-names>SE</given-names></name><name><surname>Anderson</surname><given-names>S</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>High gamma cortical processing of continuous speech in younger and older listeners</article-title><source>NeuroImage</source><volume>222</volume><elocation-id>117291</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117291</pub-id><pub-id pub-id-type="pmid">32835821</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kulasingham</surname><given-names>JP</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Algorithms for estimating time-locked neural response components in cortical processing of continuous speech</article-title><source>IEEE Transactions on Bio-Medical Engineering</source><volume>70</volume><fpage>88</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1109/TBME.2022.3185005</pub-id><pub-id pub-id-type="pmid">35727788</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lalor</surname><given-names>EC</given-names></name><name><surname>Pearlmutter</surname><given-names>BA</given-names></name><name><surname>Reilly</surname><given-names>RB</given-names></name><name><surname>McDarby</surname><given-names>G</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The VESPA: A method for the rapid estimation of A visual evoked potential</article-title><source>NeuroImage</source><volume>32</volume><fpage>1549</fpage><lpage>1561</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.05.054</pub-id><pub-id pub-id-type="pmid">16875844</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lalor</surname><given-names>EC</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural responses to uninterrupted natural speech can be extracted with precise temporal resolution</article-title><source>The European Journal of Neuroscience</source><volume>31</volume><fpage>189</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2009.07055.x</pub-id><pub-id pub-id-type="pmid">20092565</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leahy</surname><given-names>J</given-names></name><name><surname>Kim</surname><given-names>SG</given-names></name><name><surname>Wan</surname><given-names>J</given-names></name><name><surname>Overath</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>An analytical framework of tonal and rhythmic hierarchy in natural music using the multivariate temporal response function</article-title><source>Frontiers in Neuroscience</source><volume>15</volume><elocation-id>665767</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2021.665767</pub-id><pub-id pub-id-type="pmid">34335154</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberto</surname><given-names>GMD</given-names></name><name><surname>Nie</surname><given-names>J</given-names></name><name><surname>Yeaton</surname><given-names>J</given-names></name><name><surname>Khalighinejad</surname><given-names>B</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural representation of linguistic feature hierarchy reflects second-language proficiency</article-title><source>NeuroImage</source><volume>227</volume><elocation-id>117586</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117586</pub-id><pub-id pub-id-type="pmid">33346131</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maddox</surname><given-names>RK</given-names></name><name><surname>Lee</surname><given-names>AKC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Auditory brainstem responses to continuous natural speech in human listeners</article-title><source>eNeuro</source><volume>5</volume><elocation-id>ENEURO.0441-17.2018</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0441-17.2018</pub-id><pub-id pub-id-type="pmid">29435487</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McAuliffe</surname><given-names>M</given-names></name><name><surname>Socolof</surname><given-names>M</given-names></name><name><surname>Mihuc</surname><given-names>S</given-names></name><name><surname>Wagner</surname><given-names>M</given-names></name><name><surname>Sonderegger</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Montreal forced aligner: trainable text-speech alignment using kaldi</article-title><conf-name>Interspeech 2017</conf-name><fpage>498</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.21437/Interspeech.2017-1386</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miran</surname><given-names>S</given-names></name><name><surname>Akram</surname><given-names>S</given-names></name><name><surname>Sheikhattar</surname><given-names>A</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Babadi</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Real-time tracking of selective auditory attention from M/EEG: a bayesian filtering approach</article-title><source>Frontiers in Neuroscience</source><volume>12</volume><elocation-id>262</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00262</pub-id><pub-id pub-id-type="pmid">29765298</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Holmes</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Nonparametric permutation tests for functional neuroimaging: A primer with examples</article-title><source>Human Brain Mapping</source><volume>15</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1002/hbm.1058</pub-id><pub-id pub-id-type="pmid">11747097</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nunez</surname><given-names>PL</given-names></name><name><surname>Srinivasan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Electric Fields of the Brain: The Neurophysics of EEG</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195050387.001.0001</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Sullivan</surname><given-names>JA</given-names></name><name><surname>Power</surname><given-names>AJ</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Rajaram</surname><given-names>S</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name><name><surname>Slaney</surname><given-names>M</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attentional selection in a cocktail party environment can be decoded from single-trial EEG</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>1697</fpage><lpage>1706</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht355</pub-id><pub-id pub-id-type="pmid">24429136</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Patterson</surname><given-names>RD</given-names></name><name><surname>Robinson</surname><given-names>K</given-names></name><name><surname>Holdsworth</surname><given-names>J</given-names></name><name><surname>McKeown</surname><given-names>D</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Allerhand</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1992">1992</year><source>Complex Sounds and Auditory ImagesAuditory Physiology and Perception</source><publisher-name>Elsevier</publisher-name><pub-id pub-id-type="doi">10.1016/B978-0-08-041847-6.50054-X</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Presacco</surname><given-names>A</given-names></name><name><surname>Miran</surname><given-names>S</given-names></name><name><surname>Babadi</surname><given-names>B</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><article-title>Real-time tracking of magnetoencephalographic neuromarkers during a dynamic attention-switching task</article-title><conf-name>2019 41st Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</conf-name><year iso-8601-date="2019">2019</year><conf-loc>Berlin, Germany</conf-loc><fpage>4148</fpage><lpage>4151</lpage><pub-id pub-id-type="doi">10.1109/EMBC.2019.8857953</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahman</surname><given-names>M</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Simple transformations capture auditory input to cortex</article-title><source>PNAS</source><volume>117</volume><fpage>28442</fpage><lpage>28451</lpage><pub-id pub-id-type="doi">10.1073/pnas.1922033117</pub-id><pub-id pub-id-type="pmid">33097665</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="software"><person-group person-group-type="author"><collab>R Development Core Team</collab></person-group><year iso-8601-date="2021">2021</year><data-title>R: A language and environment for statistical computing</data-title><publisher-loc>Vienna, Austria</publisher-loc><publisher-name>R Foundation for Statistical Computing</publisher-name><ext-link ext-link-type="uri" xlink:href="http://www.r-project.org">http://www.r-project.org</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Reback</surname><given-names>J</given-names></name><name><surname>Jbrockmendel</surname><given-names>M</given-names></name><name><surname>Van Den Bossche</surname><given-names>W</given-names></name><name><surname>Augspurger</surname><given-names>J</given-names></name><name><surname>Cloud</surname><given-names>T</given-names></name><name><surname>Hawkins</surname><given-names>P</given-names></name><name><surname>Gfyoung</surname><given-names>S</given-names></name><name><surname>Sinhrks</surname><given-names>R</given-names></name><name><surname>Klein</surname><given-names>M</given-names></name><name><surname>Terji Petersen</surname><given-names>A</given-names></name><name><surname>Tratner</surname><given-names>J</given-names></name><name><surname>She</surname><given-names>C</given-names></name><name><surname>Ayd</surname><given-names>W</given-names></name><name><surname>Hoefler</surname><given-names>P</given-names></name><name><surname>Naveh</surname><given-names>S</given-names></name><name><surname>Garcia</surname><given-names>M</given-names></name><name><surname>Schendel</surname><given-names>J</given-names></name><name><surname>Hayden</surname><given-names>A</given-names></name><name><surname>Saxton</surname><given-names>D</given-names></name><name><surname>Shadrach</surname><given-names>R</given-names></name><name><surname>Gorelli</surname><given-names>ME</given-names></name><name><surname>Jancauskas</surname><given-names>V</given-names></name><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Attack</surname><given-names>M</given-names></name><name><surname>Battiston</surname><given-names>A</given-names></name><name><surname>Skipper Seabold</surname><given-names>P</given-names></name><name><surname>Dong</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Pandas</data-title><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3509134">https://doi.org/10.5281/zenodo.3509134</ext-link></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salverda</surname><given-names>AP</given-names></name><name><surname>Dahan</surname><given-names>D</given-names></name><name><surname>McQueen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The role of prosodic boundaries in the resolution of lexical embedding in speech comprehension</article-title><source>Cognition</source><volume>90</volume><fpage>51</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1016/s0010-0277(03)00139-2</pub-id><pub-id pub-id-type="pmid">14597270</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Threshold-free cluster enhancement: Addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title><source>NeuroImage</source><volume>44</volume><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.061</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohoglu</surname><given-names>E</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Rapid computations of spectrotemporal prediction error support perception of degraded speech</article-title><source>eLife</source><volume>9</volume><elocation-id>e58077</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.58077</pub-id><pub-id pub-id-type="pmid">33147138</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theunissen</surname><given-names>FE</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Singh</surname><given-names>NC</given-names></name><name><surname>Hsu</surname><given-names>A</given-names></name><name><surname>Vinje</surname><given-names>WE</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to natural stimuli</article-title><source>Network</source><volume>12</volume><fpage>289</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1080/net.12.3.289.316</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vallat</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Pingouin: statistics in Python</article-title><source>Journal of Open Source Software</source><volume>3</volume><elocation-id>1026</elocation-id><pub-id pub-id-type="doi">10.21105/joss.01026</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weissbart</surname><given-names>H</given-names></name><name><surname>Kandylaki</surname><given-names>KD</given-names></name><name><surname>Reichenbach</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical tracking of surprisal during continuous speech comprehension</article-title><source>Journal of Cognitive Neuroscience</source><volume>32</volume><fpage>155</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01467</pub-id><pub-id pub-id-type="pmid">31479349</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Weissbart</surname><given-names>H</given-names></name><name><surname>Wendt</surname><given-names>K</given-names></name><name><surname>Etard</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>pyEEG</data-title><version designator="1.2">1.2</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/Hugo-W/pyEEG">https://github.com/Hugo-W/pyEEG</ext-link></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85012.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.08.01.454687" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.08.01.454687"/></front-stub><body><p>Brodbeck et al. offer a timely and important contribution to how neural signals in response to continuous temporal modulations (as seen in speech and language processing) can be modelled effectively using temporal response functions. They offer a compelling new approach that includes a novel application of a boosting algorithm in addition to an accessible and didactically useful toolbox for analysis. A comparison of boosting and ridge regression via simulation shows the important impact on methods in speech and language neuroscience, as well as in cognitive neuroscience more broadly.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85012.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Slaats</surname><given-names>Sophie</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.08.01.454687">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.08.01.454687v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Eelbrain: A Python toolkit for time-continuous analysis with temporal response functions&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Barbara Shinn-Cunningham as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Sophie Slaats (Reviewer #1).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Both Reviewers helpfully point that more clarity is needed regarding the boosting algorithm; its efficacy needs to be directly compared with existing toolboxes (e.g., Crosse et al., 2016, Weissbart/Reichenbach);</p><p>2) Reviewer 2's comments about ridge regression need to be addressed – perhaps comparing boosting and ridge regression with regard to collinearity via simulation could better support the conclusions;</p><p>3) Reviewer 2 rightly comments on the importance of accessibility and clarity in the documentation. This needs to be fully addressed in order for the work to have the impact it deserves.</p><p>4) Please respond to each of the Reviewer's main concerns, and also take care that important details that impact future users of the toolbox are double-checked (e.g., the broken links need to be fixed).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>This was a great read!</p><p>A few questions remain regarding the Boosting algorithm. Firstly, it is not exactly clear how the Sparsity prior is different from the Boosting algorithm, since both appear to favor sparse TRFs (a large number of zeros). Can the boosting algorithm be implemented without a sparsity prior, or is one a logical consequence of the other? It seems like early stopping and assuming an empty mTRF together determine the Sparsity prior. It would be helpful to include this explicitly in section 3.3 (Background: The boosting algorithm), to facilitate the interpretation of (the title of) section 5.2 (Sparsity prior).</p><p>Secondly, since the Boosting algorithm sets Eelbrain apart from other TRF implementations, such as the mTRF Toolbox (Crosse et al., 2016) or pyEEG from the Reichenbach lab (https://github.com/Hugo-W/pyEEG), a short comparison of results between two methods would be very informative. Which of the two resulting TRFs is closest to the actual neural response? Why is this the case? Would it be possible that boosting works better in some cases, but not in others?</p><p>On page 28 under header 5.4 (Source localization) there is a mention of source localization improving the signal-to-noise ratio of a specific response. While this is undoubtedly true, does this not also lead to the artificial decrease of signal-to-noise ratio for other responses (which might also be relevant, and even modeled)? In other words, is it possible that source localization might lead to an artificial overestimation of the contribution of a given predictor?</p><p>The code is easy to use thanks to the wonderful documentation. All the scripts run on the first try. There was only a single error in the scripts as I ran them: in Auditory-TRFs.py, under the header &quot;TRFs&quot;, the second cell does not run. The error was the following: [TypeError: 'TTestOneSample' object is not iterable]. All the other cells above it ran to completion. There was a tiny typo under the header &quot;Generate figure&quot; in the same script: the first comment says 'preditive' instead of 'predictive'. Finally, the implementation of the plots yields many warnings if the chosen fonts are not installed (as was the case for me), but this is easy to fix as a user.</p><p>The TRFs ran to completion for subjects 1 to 20 – the other ones were omitted due to time constraints. The visual comparison between the newly calculated TRFs and the downloaded ones (for the same subjects, of course) showed minuscule differences in the scalp maps – one or two electrodes were flipped for significance. Unfortunately, the reason for this difference is not clear to me at this point. The other figures were fully identical to the paper.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1) The main issue I have is that the current toolbox heavily relies on a single way to solve the TRF estimation problem. This is the boosting algorithm. While there are certainly benefits and downsides to any of the solutions, the toolbox forces a method on the user. As the authors explain, the boosting algorithm could come up with a solution in which fully colinear predictors still end up in the model. This would not happen using other algorithms. Interpreting the added benefit of individual predictors to the model could therefore lead to very different conclusions depending on the toolbox you use. It becomes very difficult for any lay audience to compare results from this algorithm to methods implemented in other toolboxes, for example in the mTRF toolbox introduced by Crosse et al. (2016). I am very positive that the authors are open about this and even provide counterintuitive examples of this problem. However, to make the differences clearer for a lay audience I have two improvements here that could be made:</p><p>A) Ideally, the toolbox should provide an easy way to implement different ways to solve the TRF estimation. Now, the boosting methods seem difficult to get around in the current implementation (to me). If the authors could provide at least options to change the solution by provide also other solutions themselves. Personally, adding ridge regression would be beneficial as that has been used e.g. in the Crosse et al., (2016) toolbox lot. But if the toolbox is made in a way that custom-made solutions or future solutions can easily be added and compared that would be a great benefit.</p><p>B) In the manuscript these methods should be quantitatively compared rather than only describing some counter-intuitive examples of the boosting algorithm. What would ridge regression do for this collinearity problem? I think the manuscript could benefit from a thorough comparison between the different methods.</p><p>2) The provided github code together with the manuscript is not very straightforward. While the installation of the toolbox was very smooth, to actually get to the figures provided in the manuscript required going back and forth within all the folders a couple of times. It was not clear to me that first all the code in the predictor/analysis folders must be run before ending up with the results figures etc. Some more instructions on this could have been helpful.</p><p>3) If the code in the github should provide the 'easy' way to do the TRF analysis, to me this was not necessarily extremely straightforward. Besides the order issues as described above, getting to the TRF required many steps that remain unexplained. In the code, the main bulk of work is going into making the predictors themselves. This to me makes sense as that is complicated work, however the manuscript itself they give illustrators how to make predictors and provide the code, but it doesn't seem very integral to eelbrain itself to make the predictors in a straightforward manner. So this is left up to the user. If I am a layperson using the toolbox, I, therefore, need to figure out myself outside of the toolbox (potentially based on the code the authors provide, but this does not seem to be an integral part of eelbrain) to make the predictors before I can continue using the code. Of course, a toolbox that helps with the TRF itself is useful, but a user that never has used a TRF before also needs help potentially making the predictors. I guess this is a choice for the authors, which potentially is not clear in the current version of the manuscript. Is the toolbox for solving the TRF problem using the boosting algorithm or is it also intended to provide a means to generate reasonable predictors? If the former is not solving some of the major issues that a lay audience might have if the latter then the manuscript should have way more explanation on how to generate the predictors using eelbrain. This involves not only showing it in the figure but also providing the relevant code, the current descriptions seem insufficient. It seems that the authors focus on making an easy-to-use TRF tool, but provide tools to make predictors but these are not integral to eelbrain. Thus, if the authors aim to provide a tool to go from data to the TRF (including making the predictors), then the manuscript should explain better how this is done and link this to the code. If they do not intend to do this, then they should more clearly separate what eelbrain can and cannot do.</p><p>4) Related to issue 3. The use of trftools. In the code for making the predictors, trftools is used a lot. However, when going to the github page of trftools it seems that the authors are not confident about the stability of the code of trftools (&quot;Tools for data analysis with multivariate temporal response functions (mTRFs). This repository mostly contains tools that extend Eelbrain but are not yet stable enough to be included in the main release.&quot;). If the code they provide for a publication refers to a toolbox they themselves don't deem stable I find it difficult to judge the value of the toolbox. A layperson might just go along and use the code provided with a published paper.</p><p>5) In general, the paper does not provide any tools or directions to use the toolbox. What is the benefit of this toolbox to what is already available? The overall logic of the toolbox and implementation would have been helpful. Now very often the authors refer to function and class types within the toolbox (e.g. NDVar objects etc.), but without any context, this is very difficult to grasp. I understand the manuscript is not the place to provide a full tutorial, but now the manuscript fails to provide an overall logic of the toolbox and how to approach a problem.</p><p>6) Regarding this piece of text: &quot;Because the default implementation of the boosting algorithm constructs 1 the kernel from impulses (each element hi,τ is modified independently), this can lead to temporally discontinuous TRFs. In order to derive smoother TRFs, TRFs can be constructed from a basis of smooth window functions instead. In Eelbrain, the basis shape and window size are controlled by the basis and basis_window parameters in the boosting function.&quot; Would it be helpful to demonstrate this and also to show what are the default options in the bases and basis_window and why these are chosen in this way? I think it would be useful for a user to know that these are critical choices that are made for them by the toolbox.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85012.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Both Reviewers helpfully point that more clarity is needed regarding the boosting algorithm; its efficacy needs to be directly compared with existing toolboxes (e.g., Crosse et al., 2016, Weissbart/Reichenbach);</p></disp-quote><p>We have added more details in the Methods section regarding the boosting algorithm, and we have added a simulation study to demonstrate an essential difference between boosting and ridge regression. For more details see also the responses to the relevant reviewer comments below.</p><disp-quote content-type="editor-comment"><p>2) Reviewer 2's comments about ridge regression need to be addressed – perhaps comparing boosting and ridge regression with regard to collinearity via simulation could better support the conclusions;</p></disp-quote><p>We have added a simulation study that simulates EEG responses to collinear predictor variables, and compares mTRFs reconstructed using boosting and ridge regression. While it is impossible to anticipate all manners of collinearity that users may encounter, the source code associated with the simulation study will provide a template for users to test their own stimuli for issues regarding collinearity.</p><disp-quote content-type="editor-comment"><p>3) Reviewer 2 rightly comments on the importance of accessibility and clarity in the documentation. This needs to be fully addressed in order for the work to have the impact it deserves.</p></disp-quote><p>We have addressed all issues raised regarding the documentation below.</p><disp-quote content-type="editor-comment"><p>4) Please respond to each of the Reviewer's main concerns, and also take care that important details that impact future users of the toolbox are double-checked (e.g., the broken links need to be fixed).</p></disp-quote><p>Please see below for responses.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>This was a great read!</p></disp-quote><p>Thank you for the positive evaluation!</p><disp-quote content-type="editor-comment"><p>A few questions remain regarding the Boosting algorithm. Firstly, it is not exactly clear how the Sparsity prior is different from the Boosting algorithm, since both appear to favor sparse TRFs (a large number of zeros). Can the boosting algorithm be implemented without a sparsity prior, or is one a logical consequence of the other? It seems like early stopping and assuming an empty mTRF together determine the Sparsity prior. It would be helpful to include this explicitly in section 3.3 (Background: The boosting algorithm), to facilitate the interpretation of (the title of) section 5.2 (Sparsity prior).</p></disp-quote><p>Thank you for pointing out these questions. We have clarified this in section 3.3 (Background: The boosting algorithm) along with other additions to better explain the algorithm.</p><disp-quote content-type="editor-comment"><p>Secondly, since the Boosting algorithm sets Eelbrain apart from other TRF implementations, such as the mTRF Toolbox (Crosse et al., 2016) or pyEEG from the Reichenbach lab (https://github.com/Hugo-W/pyEEG), a short comparison of results between two methods would be very informative. Which of the two resulting TRFs is closest to the actual neural response? Why is this the case? Would it be possible that boosting works better in some cases, but not in others?</p></disp-quote><p>We have added a simulation study to highlight the key difference between the two algorithms, and the resulting mTRFs’ relationship to the actual neural response (section 3.4.2 Comparison to ridge regression). Which of the two resulting TRFs will be closest to the actual neural response will depend on the metric that measures the ‘closeness’. As we see from the simulation example, boosting mTRFs achieve an excellent true negative rate in expense of biasing the peaks and troughs towards 0 while ridge mTRF closely follows the peaks and troughs, but admits lots of false positives. Now, depending on the hypothesis one is trying to test, in some cases it will be more crucial to have tight control over false positives than other cases. So, ultimately the particular use case and the underlying hypothesis will determine which one of the methods work better.</p><p>While a more systematic evaluation and comparison of the two algorithms is beyond the scope of this tutorial, we have also added references to existing work that provides more detailed comparisons between boosting and other reverse correlation algorithms <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?NQK9px">(David et al., 2007; Kulasingham and Simon, 2022)</ext-link>. In addition, the source code corresponding to the new simulation example also provides a template for a more in-depth comparison for interested users.</p><disp-quote content-type="editor-comment"><p>On page 28 under header 5.4 (Source localization) there is a mention of source localization improving the signal-to-noise ratio of a specific response. While this is undoubtedly true, does this not also lead to the artificial decrease of signal-to-noise ratio for other responses (which might also be relevant, and even modeled)? In other words, is it possible that source localization might lead to an artificial overestimation of the contribution of a given predictor?</p></disp-quote><p>This is certainly true in individual brain regions. Ideally, the goal of source localization is to isolate the signal originating from the given region while suppressing signals from other regions. In practice, if a certain source region is not modeled (e.g., due to using an inaccurate anatomical model, or only looking at a specific ROI), signals that originated from a region that is not considered might be missed. We have added this consideration to the Source localization section.</p><disp-quote content-type="editor-comment"><p>The code is easy to use thanks to the wonderful documentation. All the scripts run on the first try. There was only a single error in the scripts as I ran them: in Auditory-TRFs.py, under the header &quot;TRFs&quot;, the second cell does not run. The error was the following: [TypeError: 'TTestOneSample' object is not iterable]. All the other cells above it ran to completion. There was a tiny typo under the header &quot;Generate figure&quot; in the same script: the first comment says 'preditive' instead of 'predictive'. Finally, the implementation of the plots yields many warnings if the chosen fonts are not installed (as was the case for me), but this is easy to fix as a user.</p></disp-quote><p>Thank you for noting those. The error is fixed with Eelbrain release 0.38.4. We have also changed the font to Arial which is more commonly pre-installed and standard for scientific publications.</p><disp-quote content-type="editor-comment"><p>The TRFs ran to completion for subjects 1 to 20 – the other ones were omitted due to time constraints. The visual comparison between the newly calculated TRFs and the downloaded ones (for the same subjects, of course) showed minuscule differences in the scalp maps – one or two electrodes were flipped for significance. Unfortunately, the reason for this difference is not clear to me at this point. The other figures were fully identical to the paper.</p></disp-quote><p>The miniscule differences in the TRFs are likely due to the optimization of the boosting implementation in Eelbrain 0.38. This implementation change may have led to slight changes of results due to numerical inaccuracy issues (performing operations in a different sequence, etc.). We have now re-estimate all TRFs with the optimized algorithm and added them to the DRUM repository that hosts the EEG data.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1) The main issue I have is that the current toolbox heavily relies on a single way to solve the TRF estimation problem. This is the boosting algorithm. While there are certainly benefits and downsides to any of the solutions, the toolbox forces a method on the user. As the authors explain, the boosting algorithm could come up with a solution in which fully colinear predictors still end up in the model. This would not happen using other algorithms. Interpreting the added benefit of individual predictors to the model could therefore lead to very different conclusions depending on the toolbox you use. It becomes very difficult for any lay audience to compare results from this algorithm to methods implemented in other toolboxes, for example in the mTRF toolbox introduced by Crosse et al. (2016). I am very positive that the authors are open about this and even provide counterintuitive examples of this problem. However, to make the differences clearer for a lay audience I have two improvements here that could be made:</p><p>A) Ideally, the toolbox should provide an easy way to implement different ways to solve the TRF estimation. Now, the boosting methods seem difficult to get around in the current implementation (to me). If the authors could provide at least options to change the solution by provide also other solutions themselves. Personally, adding ridge regression would be beneficial as that has been used e.g. in the Crosse et al., (2016) toolbox lot. But if the toolbox is made in a way that custom-made solutions or future solutions can easily be added and compared that would be a great benefit.</p></disp-quote><p>We completely agree that it would be desirable to have an option of different estimation algorithms in the same package. Eelbrain’s boosting implementation is open and highly modular, and we would welcome community contributions for other algorithms. While we do not want to delay publication of this tutorial for such implementations, we have now added a simulation study that uses boosting and ridge regression alongside each other, which provides a template for users that want to compare the two algorithms further (section 3.4.2 Comparison to ridge regression).</p><disp-quote content-type="editor-comment"><p>B) In the manuscript these methods should be quantitatively compared rather than only describing some counter-intuitive examples of the boosting algorithm. What would ridge regression do for this collinearity problem? I think the manuscript could benefit from a thorough comparison between the different methods.</p></disp-quote><p>We have now added a simulation study to compare the boosting algorithm and ridge regression in presence of multiple collinear predictor variables. In the spirit of open source we chose to use the Python pyEEG toolbox from the Reichenbach lab (https://github.com/Hugo-W/pyEEG), over the MATLAB mTRF toolbox. As evident from the simulation example, both the boosting mTRF and ridge mTRF capture the dominant components of the ground truth mTRFs. But, the distinguishing factor between them is that the ridge mTRF follows the dominant portions of the true mTRF at the expense of numerous false positive discoveries while the boosting mTRF allows a small bias towards 0 in the peaks and troughs of the mTRFs to maintain a tight control over false positives. A detailed, systematic comparison between these two methods is out of scope of this work. However, we pointed the readers to a previous work that performed an extensive comparison between boosting and other reverse correlation algorithms <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?JjByLt">(David et al., 2007; Kulasingham and Simon, 2022)</ext-link>. In addition, we have also added the source code for this simulation as a platform for enthusiastic readers to carry out more in-depth comparisons.</p><disp-quote content-type="editor-comment"><p>2) The provided github code together with the manuscript is not very straightforward. While the installation of the toolbox was very smooth, to actually get to the figures provided in the manuscript required going back and forth within all the folders a couple of times. It was not clear to me that first all the code in the predictor/analysis folders must be run before ending up with the results figures etc. Some more instructions on this could have been helpful.</p></disp-quote><p>Thank you for pointing out this lack of clarity. We have added corresponding instructions to the README.md file of the Alice repository.</p><disp-quote content-type="editor-comment"><p>3) If the code in the github should provide the 'easy' way to do the TRF analysis, to me this was not necessarily extremely straightforward. Besides the order issues as described above, getting to the TRF required many steps that remain unexplained. In the code, the main bulk of work is going into making the predictors themselves. This to me makes sense as that is complicated work, however the manuscript itself they give illustrators how to make predictors and provide the code, but it doesn't seem very integral to eelbrain itself to make the predictors in a straightforward manner. So this is left up to the user. If I am a layperson using the toolbox, I, therefore, need to figure out myself outside of the toolbox (potentially based on the code the authors provide, but this does not seem to be an integral part of eelbrain) to make the predictors before I can continue using the code. Of course, a toolbox that helps with the TRF itself is useful, but a user that never has used a TRF before also needs help potentially making the predictors. I guess this is a choice for the authors, which potentially is not clear in the current version of the manuscript. Is the toolbox for solving the TRF problem using the boosting algorithm or is it also intended to provide a means to generate reasonable predictors? If the former is not solving some of the major issues that a lay audience might have if the latter then the manuscript should have way more explanation on how to generate the predictors using eelbrain. This involves not only showing it in the figure but also providing the relevant code, the current descriptions seem insufficient. It seems that the authors focus on making an easy-to-use TRF tool, but provide tools to make predictors but these are not integral to eelbrain. Thus, if the authors aim to provide a tool to go from data to the TRF (including making the predictors), then the manuscript should explain better how this is done and link this to the code. If they do not intend to do this, then they should more clearly separate what eelbrain can and cannot do.</p></disp-quote><p>Thank you for drawing attention to this shortcoming. We have added line-by-line comments to the scripts for generating the predictor variables, so that there should be no more unexplained steps required. In order to show users where to start to learn more about functionality allowing them to create new predictors, we have also added explicit pointers to the relevant functionality in Eelbrain in the Predictor variables subsection of the Methods (from there, users will be able to use the online documentation of those functions to find further help).</p><disp-quote content-type="editor-comment"><p>4) Related to issue 3. The use of trftools. In the code for making the predictors, trftools is used a lot. However, when going to the github page of trftools it seems that the authors are not confident about the stability of the code of trftools (&quot;Tools for data analysis with multivariate temporal response functions (mTRFs). This repository mostly contains tools that extend Eelbrain but are not yet stable enough to be included in the main release.&quot;). If the code they provide for a publication refers to a toolbox they themselves don't deem stable I find it difficult to judge the value of the toolbox. A layperson might just go along and use the code provided with a published paper.</p></disp-quote><p>The functionality from TRF-Tools that was required for this analysis has been moved to Eelbrain and TRF-Tools is no longer referenced from the Alice repository.</p><disp-quote content-type="editor-comment"><p>5) In general, the paper does not provide any tools or directions to use the toolbox. What is the benefit of this toolbox to what is already available? The overall logic of the toolbox and implementation would have been helpful. Now very often the authors refer to function and class types within the toolbox (e.g. NDVar objects etc.), but without any context, this is very difficult to grasp. I understand the manuscript is not the place to provide a full tutorial, but now the manuscript fails to provide an overall logic of the toolbox and how to approach a problem.</p></disp-quote><p>Thank you for pointing out this omission. We added a section describing the general structure of the toolbox at the beginning of the Methods section, under the heading “Overall architecture of the Eelbrain toolbox”</p><disp-quote content-type="editor-comment"><p>6) Regarding this piece of text: &quot;Because the default implementation of the boosting algorithm constructs 1 the kernel from impulses (each element hi,τ is modified independently), this can lead to temporally discontinuous TRFs. In order to derive smoother TRFs, TRFs can be constructed from a basis of smooth window functions instead. In Eelbrain, the basis shape and window size are controlled by the basis and basis_window parameters in the boosting function.&quot; Would it be helpful to demonstrate this and also to show what are the default options in the bases and basis_window and why these are chosen in this way? I think it would be useful for a user to know that these are critical choices that are made for them by the toolbox.</p></disp-quote><p>We have added a discussion of the effect of using different basis windows at the end of the Methods section.</p></body></sub-article></article>