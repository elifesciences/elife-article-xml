<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">78205</article-id><article-id pub-id-type="doi">10.7554/eLife.78205</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Monkeys exhibit human-like gaze biases in economic decisions</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-271602"><name><surname>Lupkin</surname><given-names>Shira M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3792-5571</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-96845"><name><surname>McGinty</surname><given-names>Vincent B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0883-4301</contrib-id><email>VINCE.MCGINTY@RUTGERS.EDU</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05vt9qd57</institution-id><institution>Center for Molecular and Behavioral Neuroscience, Rutgers University</institution></institution-wrap><addr-line><named-content content-type="city">Newark</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05vt9qd57</institution-id><institution>Behavioral and Neural Sciences Graduate Program, Rutgers University</institution></institution-wrap><addr-line><named-content content-type="city">Newark</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rich</surname><given-names>Erin L</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04a9tmd77</institution-id><institution>Icahn School of Medicine at Mount Sinai</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>27</day><month>07</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e78205</elocation-id><history><date date-type="received" iso-8601-date="2022-02-26"><day>26</day><month>02</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-07-25"><day>25</day><month>07</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-02-26"><day>26</day><month>02</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.02.24.481847"/></event></pub-history><permissions><copyright-statement>© 2023, Lupkin and McGinty</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Lupkin and McGinty</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-78205-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-78205-figures-v2.pdf"/><abstract><p>In economic decision-making individuals choose between items based on their perceived value. For both humans and nonhuman primates, these decisions are often carried out while shifting gaze between the available options. Recent studies in humans suggest that these shifts in gaze actively influence choice, manifesting as a bias in favor of the items that are viewed first, viewed last, or viewed for the overall longest duration in a given trial. This suggests a mechanism that links gaze behavior to the neural computations underlying value-based choices. In order to identify this mechanism, it is first necessary to develop and validate a suitable animal model of this behavior. To this end, we have created a novel value-based choice task for macaque monkeys that captures the essential features of the human paradigms in which gaze biases have been observed. Using this task, we identified gaze biases in the monkeys that were both qualitatively and quantitatively similar to those in humans. In addition, the monkeys’ gaze biases were well-explained using a sequential sampling model framework previously used to describe gaze biases in humans—the first time this framework has been used to assess value-based decision mechanisms in nonhuman primates. Together, these findings suggest a common mechanism that can explain gaze-related choice biases across species, and open the way for mechanistic studies to identify the neural origins of this behavior.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>When we choose between two items, we might expect to spend more time looking at the one we have a pre-existing preference for. For example, at the grocery store, you might assume that someone who likes grapes better than bananas would spend a longer time looking at the grapes. Surprisingly, a series of studies on human decision-making have shown that the opposite relationship is also true: the more time we spend looking at an item, the more likely we are to pick it. This ‘gaze bias’ occurs in many real-life and laboratory decision settings, and it is especially evident for choices between two equally preferred options. However, examining the brain circuits that underpin this behavior has so far been difficult due to a lack of animal models in which to study them.</p><p>In response, Lupkin and McGinty proposed that rhesus macaques may be the ideal species in which to study gaze biases, as these animals likely rely on the same brain regions as humans when gazing and making decisions. To test this hypothesis, a computer-based decision game similar to the ones used for humans was designed for the monkeys. It involved the animals having to choose between two icons that were associated with different amounts of a juice reward. Analysing how long the macaques had spent looking at each icon before making their choice revealed that they indeed tended to select the icon they had looked at for longer – including when the two icons indicated equal rewards. Other types of gaze biases present in humans were also detected, such as choosing the icon that was viewed first or last in a trial.</p><p>Additional analyses using computer simulations confirmed that the gaze biases of humans and monkeys were comparable and, critically, that they could be explained by similar underlying brain processes.</p><p>These strong similarities suggest that rhesus macaques could be used to study the neural basis for decision-making in both humans and nonhuman primates, potentially making it easier to examine the harmful changes in decision-making that occur in conditions like substance abuse or depression.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>decision making</kwd><kwd>free-viewing</kwd><kwd>economic choice</kwd><kwd>animal model</kwd><kwd>sequential sampling models</kwd><kwd>nonhuman primates</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100011132</institution-id><institution>Rutgers, The State University of New Jersey</institution></institution-wrap></funding-source><award-id>Deans Dissertation Fellowship</award-id><principal-award-recipient><name><surname>Lupkin</surname><given-names>Shira M</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100011132</institution-id><institution>Rutgers, The State University of New Jersey</institution></institution-wrap></funding-source><award-id>Academic Advancement Fund</award-id><principal-award-recipient><name><surname>Lupkin</surname><given-names>Shira M</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100011132</institution-id><institution>Rutgers, The State University of New Jersey</institution></institution-wrap></funding-source><award-id>Graduate Assistantship through the Behavioral and Neural Sciences Graduate Program</award-id><principal-award-recipient><name><surname>Lupkin</surname><given-names>Shira M</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001391</institution-id><institution>Whitehall Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>McGinty</surname><given-names>Vincent B</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Busch Biomedical Research Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>McGinty</surname><given-names>Vincent B</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000026</institution-id><institution>National Institute on Drug Abuse</institution></institution-wrap></funding-source><award-id>K01-DA-036659-01</award-id><principal-award-recipient><name><surname>McGinty</surname><given-names>Vincent B</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A novel animal model of economic decision-making captures complex patterns of choice behavior similar to those of humans, opening the way for mechanistic studies to probe the neural basis for this important form of executive function.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Economic decisions are ubiquitous in the natural world, and in natural settings humans and other primates often evaluate the available options by moving their eyes, rapidly shifting the focus of their gaze between the options as they deliberate. Over the past decade, studies in humans have documented an association between gaze and value-based choices. In particular, when offered a choice between multiple items, people are more likely to select the first item that they view, the item they look at just prior to indicating their decision, and the item that they spend the longest time viewing over the course of their deliberation. Further, choice biases can be induced through manipulations of gaze (<xref ref-type="bibr" rid="bib16">Gwinn et al., 2019</xref>; <xref ref-type="bibr" rid="bib30">Liu et al., 2020</xref>; <xref ref-type="bibr" rid="bib31">Liu et al., 2021</xref>; <xref ref-type="bibr" rid="bib44">Pärnamets et al., 2015</xref>; <xref ref-type="bibr" rid="bib59">Sui et al., 2020</xref>; <xref ref-type="bibr" rid="bib60">Tavares et al., 2017</xref>). Together, these studies suggest that gaze plays an active role in the decision process, such that simply viewing a given option makes us more likely to choose it (see <xref ref-type="bibr" rid="bib24">Krajbich, 2019</xref>, for a review).</p><p>However, despite increased interest in the role of gaze in decision-making, knowledge of the underlying neural mechanisms is still limited (c.f., <xref ref-type="bibr" rid="bib25">Krajbich et al., 2021</xref>; <xref ref-type="bibr" rid="bib29">Lim et al., 2011</xref>). One critical barrier to identifying these mechanisms is the lack of a suitable animal model of this behavior, and consequently a lack of studies addressing neural mechanisms at the cellular level. To this end, the aim of the present study was twofold: first, to develop a novel behavioral paradigm for macaque monkeys that captures the essential features of tasks previously used in humans; and second, to determine whether monkeys exhibit gaze-based choice biases that are similar to those of humans.</p><p>While the neural mechanisms linking gaze and economic choice are not fully known, the underlying computations have been explored using sequential sampling models, a highly successful class of models used to explain simple decision behaviors (<xref ref-type="bibr" rid="bib47">Ratcliff and McKoon, 2008</xref>). In these models, a stream of noisy evidence for each option is accumulated until one item accrues enough evidence to reach a pre-defined threshold, and a decision is rendered in favor of that item. For value-based tasks, the evidence accumulation rate can be formulated as a function of the relative value of the possible outcomes—the larger the difference in relative value, the faster that evidence accumulates for the higher valued item. Over the last decade, the basic sequential sampling model framework has been extended to account for the influence of gaze behavior (i.e. overt attention) on the decision process. These models, collectively referred to as attentional sequential sampling models (aSSMs), assume that evidence for a given item accumulates at a faster rate while that item is being viewed, and accumulates slower when it is not being viewed (<xref ref-type="bibr" rid="bib22">Krajbich et al., 2010</xref>; <xref ref-type="bibr" rid="bib23">Krajbich and Rangel, 2011</xref>; <xref ref-type="bibr" rid="bib56">Smith and Krajbich, 2018</xref>; <xref ref-type="bibr" rid="bib60">Tavares et al., 2017</xref>; <xref ref-type="bibr" rid="bib62">Thomas et al., 2019</xref>). Overall, these models have been successful in accounting for human gaze biases across several decision-making domains (<xref ref-type="bibr" rid="bib24">Krajbich, 2019</xref>; <xref ref-type="bibr" rid="bib32">Manohar and Husain, 2013</xref>; <xref ref-type="bibr" rid="bib56">Smith and Krajbich, 2018</xref>; <xref ref-type="bibr" rid="bib62">Thomas et al., 2019</xref>; <xref ref-type="bibr" rid="bib66">Westbrook et al., 2020</xref>), supporting the idea that gaze modulates the decision process by biasing an evidence accumulation process.</p><p>To identify candidate neural substrates, intracranial recordings in nonhuman primates (NHPs) are an ideal approach, as they combine an animal model with human-like oculomotor behavior with the high temporal resolution necessary to accommodate the rapid pace of natural eye movements (<xref ref-type="bibr" rid="bib27">Kravitz et al., 2013</xref>; <xref ref-type="bibr" rid="bib42">Ongür and Price, 2000</xref>; <xref ref-type="bibr" rid="bib68">Wise, 2008</xref>). Moreover, NHPs are commonly used as animal models in neuroeconomics. However, existing paradigms contain elements that make them unsuitable for testing the relationship between gaze and choice behavior. More specifically, many NHP studies restrict eye movements and/or use eye movements as a means of reporting choice, rather than for inspecting the choice options, (<xref ref-type="bibr" rid="bib6">Cavanagh et al., 2019</xref>; <xref ref-type="bibr" rid="bib18">Hunt et al., 2018</xref>; <xref ref-type="bibr" rid="bib50">Rich and Wallis, 2016</xref>). These common restrictions can interfere with natural gaze patterns and therefore limit the ability to identify the relationship between gaze behavior and choice.</p><p>To address the need for an animal model of gaze biases, we have developed a novel behavioral paradigm in which monkeys are able to freely deploy their gaze to the targets on the screen and to indicate their choice by a manual response. Using this paradigm, we replicated the three core behavioral signatures of gaze-related choice biases: We found that the monkeys were more likely to choose the first item that they viewed, the last item that they viewed before deciding, and the items that received more overall gaze time. In addition, we use a computational model previously used to assess human gaze biases (<xref ref-type="bibr" rid="bib62">Thomas et al., 2019</xref>) to determine whether similar computational mechanisms can explain choices in NHPs.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Basic psychometrics</title><p>We trained two monkeys to perform a value-based decision task in which they used eye movements to view the choice options and used a manual response lever to indicate their decision (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). To begin a trial, the monkeys were required to fixate on the center of the task display while holding down the center of three response levers. After the required eye fixation and lever hold period was satisfied, two targets were presented on the left and right of the task display, each associated with a reward ranging from 1 to 5 drops of juice. Following the initial fixation period, gaze was unrestricted, allowing the monkeys to look at each target as many times and for as long as they liked before indicating a choice. To encourage the monkeys to look directly at the targets, and to limit their ability to perceive the targets using peripheral vision, the targets were initially masked until the first saccade away from the central fixation point was detected (see <italic>Methods</italic>). To indicate their choice, the monkeys first lifted their hand off the centrally-located response lever, and then pressed the right or left lever to indicate which of the two targets they chose. The reaction time (RT) in each trial was defined as the period between the onset of the targets and the lift of the center lever. We collected data from a total of 54 sessions: 29 from Monkey C (N = 15,613) and 25 for Monkey K (N = 14,433).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Decision-making task and performance in two monkeys.</title><p>(<bold>A</bold>) Abbreviated task sequence; see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for full task sequence. The interval between target onset and center lever lift defines the decision reaction time (RT). The yellow and blue glyphs are choice targets, and the gray ‘+’ shapes indicate the location of visual crowders designed to obscure the targets until they are viewed (fixated) directly. For clarity, in this panel crowders are shown in gray and at a reduced scale; on the actual task display, crowders were multicolored and the same size as the targets, as in panel C. (<bold>B</bold>) An example set of 12 choice targets, organized into 5 groups corresponding to the 5 levels of juice reward. (<bold>C</bold>) Close-up view of a single target array, consisting of a central yellow choice target surrounded by six non-task-relevant visual crowders. Two such arrays appear on the display (panel A), each located 7.5° from the display center. (<bold>D–F</bold>). Task performance. (<bold>D</bold>) Fraction of left choices as a function of the left minus right target value (in units of juice drops). (<bold>E</bold>) Reaction Time (RT) decreases as a function of difficulty, defined as the absolute difference between target values; (<bold>F</bold>) Number of fixations per trial decreases as a function of difficulty. Filled circles show the mean values and smooth lines show the logistic (<bold>D</bold>) or linear (<bold>E–F</bold>) regression fits derived from mixed effects models (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, rows 1–3, respectively). Error bars are too small to be plotted. The data in blue are from Monkey C (N = 15,613), and in red, from Monkey K (N = 14,433).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78205-fig1-v2.tif"/><permissions><copyright-statement>© 2021, McGinty and Lupkin</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>McGinty and Lupkin</copyright-holder><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>Figure 1A-C are reproduced from Figure 1A-C of <xref ref-type="bibr" rid="bib34">McGinty and Lupkin, 2021</xref>, bioRxiv, published under the CC-BY-NC 4.0 International license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>).</license-p></license></permissions></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Full task sequence.</title><p>(<bold>A</bold>) Illustration of the complete task sequence. Note that for illustration purposes, in this panel the background is white, and the surrounding crowders are shown at reduced saturation relative to the centrally located targets. However, on the actual task display, the background was dark, and target luminance was reduced relative to the crowders (see panel B). (<bold>B</bold>) Close-up view of a single target array, consisting of a central yellow choice target surrounded by six non-task-relevant crowders. Two such arrays appear on the display (panel A), each centered 7.5° from the display center. (<bold>C</bold>) To generate stimulus sets, four shapes are selected without replacement from 7 possible; then each is rotated randomly by 0, 90, 180, 270 degrees. The four rotated shapes are then combined with four colors selected randomly from equally-spaced points on a color wheel defined within the RGB gamut in the CIELUV color space. The grid on the left shows the color and shape primitives as rows and columns, respectively. Each primitive was assigned a value from 0 to 4, corresponding to the number of drops of juice each would signal on its own. The 12 targets that made up a given stimulus set were taken from the off-diagonals of this grid; their value was determined according to their shape and color. The result is a set of targets ranging in value from 1 to 5 drops of juice.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78205-fig1-figsupp1-v2.tif"/></fig></fig-group><p>Overall choice performance is illustrated in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. Choices were nearly optimal for the easiest trials (value difference of +/-3 or 4 drops of juice,&gt;98% of choices in favor of higher value target) and occasionally suboptimal for more difficult trials (value difference of +/-1 or 2 drops of juice, 86% higher-value choices for both Monkey C and Monkey K). When the targets were of equal value, the probability of the monkeys choosing the left or right target was near chance (50% left choices for Monkey C, 46% for Monkey K). This pattern of choices indicates that the monkeys were behaving according to the incentive structure of the task, with a very low rate of guesses or lapses (<xref ref-type="bibr" rid="bib12">Fetsch, 2016</xref>). Further, choice behavior was well-described by a logistic function parameterized by the difference between the values of the left and right items (fit lines in <xref ref-type="fig" rid="fig1">Figure 1D</xref> are from a logit mixed effects regression: β<sub>value-difference</sub> = 1.37, CI = [1.34, 1.40]; F (1,30044)=8411.60, p&lt;1e-10). Additionally, we found that both the average RT and the average number of fixations per trial were modulated by choice difficulty: the monkeys responded slower and made more fixations when the two targets were near in value, compared to when the value difference between the targets was large (RT: <xref ref-type="fig" rid="fig1">Figure 1E</xref>, linear mixed effects regression: β<sub>difficulty</sub> = -0.032, CI: [-0.039,–0.026]; F (1, 30044)=106.09, p&lt;1e-10. Number of Fixations: <xref ref-type="fig" rid="fig1">Figure 1F</xref>, Linear mixed effects regression: β<sub>difficulty</sub> = -0.10, CI = [-0.12,–0.09]; F(1, 30044)=126.7, p&lt;1e-10). Together these results are consistent with behavioral patterns seen in both humans and NHPs performing 2-alternative forced choice tasks (e.g. <xref ref-type="bibr" rid="bib15">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib62">Thomas et al., 2019</xref>).</p></sec><sec id="s2-2"><title>Effects of difficulty and chosen value on reaction times and the number of fixations</title><p>In the preceding section, we show that both RT and the number of fixations increase as a function of trial difficulty. However, in high-performing subjects trial difficulty is correlated with chosen value, because the trials with large value differences (low difficulty) are also trials where higher-value offers are chosen. Therefore, the RT and fixation effects originally attributed to difficulty may be explained in part by chosen value. To determine the unique contributions of chosen value and difficulty to RT and the total number of fixations, we used the approach of <xref ref-type="bibr" rid="bib2">Balewski et al., 2022</xref>. Briefly, we modeled each behavioral metric as a function of one predictor, and then regressed the other predictor against the residuals of the first model. For example, to measure the unique contribution of chosen value to RT, we first ran a linear mixed effects model explaining RT as a function of difficulty. We then took the residuals from this model, which represent the RT after accounting for difficulty, and ran a second mixed effects model explaining these residuals as a function of chosen value.</p><p>Consistent with the findings of Balewski and colleagues, RT was modulated more by chosen value than by difficulty (β<sub>chosen-value</sub> = -0.026, CI = [-0.034,–0.019]; F (1,30044)=46.35, p&lt;1e-10; CPD = 8%; β<sub>difficulty</sub> = -0.011, CI = [-0.012,–0.010]; F (1,30044)=318.44, p&lt;1e-10; CPD = 1.4% <xref ref-type="fig" rid="fig2">Figure 2A/C</xref>, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> rows 4–5). Interestingly, we found the opposite pattern for the total number of fixations per trial: this metric was strongly modulated by difficulty (β<sub>difficulty</sub> = -0.065, CI = [-0.086,–0.043]; F (1,30044)=34.46, p=4.41e-9; CPD = 1.8%; <xref ref-type="fig" rid="fig2">Figure 2C</xref>), and showed no effect of chosen value (β<sub>chosen-value</sub> = -0.028, CI = [–0.095, 0.39]; F (1,30044)=0.66, p=0.42; CPD = 0.69%; <xref ref-type="fig" rid="fig2">Figure 2D</xref>, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> rows 4–5). In sum RTs appear to reflect the size of the upcoming reward (faster responses for higher rewards), whereas the number of fixations reflect choice difficulty, with more fixations associated with more difficult trials.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Differential contributions of difficulty and chosen value to decision reaction times and the number of fixations per trial.</title><p>(<bold>A</bold>) The effect of trial difficulty on reaction time after accounting for the effect of chosen value. (<bold>B</bold>) The effect of trial difficulty on number of fixations after accounting for chosen value. (<bold>C</bold>) The effect of chosen value on reaction time after accounting for difficulty. (<bold>D</bold>) The effect of chosen value on number of fixations after accounting for difficulty. Y-axes give the dependent measures (RT or number of fixations) after being residualized by either difficulty or chosen value, as indicated (see Methods). Dots indicate the mean of each dependent measure for each session. Black lines show linear model fits to the plotted dots. The betas and coefficients of partial determination (CPD) of these models are in the top right corner. Blue dots show the session means from Monkey C (N=25 sessions, 15,613 trials), red dots are from Monkey K (N=29 Sessions,14,433 trials).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78205-fig2-v2.tif"/></fig></sec><sec id="s2-3"><title>Gaze behavior and basic fixation properties</title><p>The following sections characterize the number and durations of the monkeys’ fixations onto the choice targets throughout each trial. A fixation was defined as a period of stable gaze upon one of the two targets. Consistent with fixation analyses in human studies, consecutive fixations upon the same target were merged into a single fixation epoch, and fixations onto non-target locations were not included in the analysis (See <italic>Behavior Analysis</italic> for details). Following conventions from human studies, final fixation durations are defined as the interval between fixation onset and the center lever lift; in other words, final fixation durations are delimited by the reaction time in each trial (e.g. <xref ref-type="bibr" rid="bib22">Krajbich et al., 2010</xref>). For non-final fixations, the duration is defined in the conventional way, as the interval between fixation onset and the initiation of the next saccade.</p></sec><sec id="s2-4"><title>Number of fixations per trial</title><p>In our task, information about the choice targets was obscured unless the monkey was directly fixating on them. Therefore, the optimal strategy involves a minimum of two fixations, one to each target. Consistent with this, the monkeys almost always looked at both targets on each trial (97.2% of trials for Monkey C, 91.6% for Monkey K). The fraction of trials with 1, 2, 3, or ≥4 total fixations before the RT for Monkey C was 2.8%, 41.2%, 49.2%, and 6.8%; and for Monkey K was 8.4%, 43.0%, 43.8%, 4.8%. Thus, in the vast majority of trials the monkeys either looked at each target exactly once (2 total fixations) or looked at one target once and the other twice (3 total fixations). The average number of fixations per trial for Monkey K was 2.45 SEM 0.02 and for Monkey C was 2.57 SEM 0.04. This average is smaller than in human binary choice tasks, in which the average number of fixations per trial is usually 3 or more. As we discuss below, the difference between human and monkey total fixation numbers has implications for comparing fixation durations between species.</p></sec><sec id="s2-5"><title>Fixation durations depend on relative and absolute order in the trial</title><p>Fixations can be defined by both their absolute and relative order in a trial. Absolute order refers to the serial position in the trial: first, second, third, etc. Relative order refers to when the fixation occurs with regards to the trial start or end: initial, middle, final. Note that these definitions are not independent or mutually exclusive. Instead, they depend on the total number of fixations in the trial. For example, in a trial with a total of two fixations, the second fixation is also the ‘final’ fixation (there are no middle fixations). However, in a trial with three total fixations, the second fixation is considered a ‘middle’ fixation. Because the monkeys usually make either two or three total fixations in a trial, relative and absolute order are conflated. (In contrast, humans typically make three or more fixations per trial, so that second fixations are almost always ‘middle’ fixations.) Consequently, it is necessary to independently analyze the effects of both absolute and relative fixation order.</p><p>To assess the effects of absolute order (first, second, third, etc.) independent of relative order (final/non-final), we first fit a linear regression explaining fixation duration as a function of relative order. We then regressed absolute order against the residuals from the first model. Consistent with human fixation patterns, we found that fixation durations decreased as a function of absolute order, after controlling for relative order (β<sub>relativeOrder</sub> = -0.049, CI = [-0.065,–0.033]; F(1, 7762)=36.06, p&lt;1.92e-9; <xref ref-type="fig" rid="fig3">Figure 3</xref>). The decrease is especially notable at the third fixation, which in a binary task is the first ‘re-fixation’ onto a previously sampled target, and is consistent with human fixation patterns (<xref ref-type="bibr" rid="bib32">Manohar and Husain, 2013</xref>). Next, to assess the effects of relative order independent of absolute order, we fit a linear mixed-effects model comparing fixation durations across absolute positions, treating absolute position as a categorical variable. We then fit a second model explaining the residuals as a function of whether or not the fixation was a final fixation (i.e., relative order). After accounting for serial position, we found that non-final fixations were shorter than final fixations (β<sub>relativeOrder</sub> = -0.047, CI = [-0.057,–0.037]; F(1, 7762)=86.34, p&lt;1e-10; <xref ref-type="fig" rid="fig3">Figure 3</xref>). In sum, we identified two independent order-based effects on duration: a decrease in duration with successive fixations in a trial, as well as a tendency to fixate longer on final fixations.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Distribution of fixation durations across absolute and relative position in the trial.</title><p>Violins show distribution of fixation durations across absolute position in the trial (x-axis) and split according to whether they were final (dark gray) or non-final (light gray) fixations in the trial. 1st fixations: N=30,046 non-final and 1,652 final; 2<sup>nd</sup> fixations: N=15,753 non-final and 12,641 final; 3rd fixations: N=1,749 non-final and 14,004 final; 4th fixations or greater: N=80 non-final (not shown) and 1,749 final.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78205-fig3-v2.tif"/></fig><p>The monkeys’ prolonged final fixations seem to contradict prior human studies, which typically report shorter final fixations. However, this discrepancy may be explained by the relatively larger number of fixations that humans make (mean ~3.7, computed from freely available data from the study described in <xref ref-type="bibr" rid="bib22">Krajbich et al., 2010</xref>), compared to the monkeys in the present study (mean ~2.5). Consequently, humans’ final fixations tend to have higher absolute order and are thus nearly always re-fixations onto a target—both factors that are associated with shorter fixation durations, as noted above. In contrast, the monkeys’ final fixations are overwhelmingly either the second or third in the trial. Thus, not only do their final fixations have a lower absolute order, but they are also comprised of a larger fraction of longer initial fixations onto a target (i.e. when the final fixation is the 2nd fixation).</p></sec><sec id="s2-6"><title>Fixation durations depend on target value</title><p>The average fixation duration was dependent on the value of the target being fixated, and upon the order in the trial in which the fixation occurred. For the first fixation in each trial, duration increased as a function of fixated target value (β<sub>fixValue</sub> = 0.003, CI = [0.0011,0.0040]; F(1, 28392)=11.37, p=0.0007, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, row 8). Intuitively, the effect size can be interpreted as the influence of a one-drop change in juice volume; for example, for initial fixations the regression estimate of 0.003 corresponds to a 3ms increase in fixation duration per drop of juice. For middle fixations, defined as any fixation that was neither first nor final, the average duration decreased as a function of the value of the fixated target (β<sub>fixValue</sub> = -0.0076, CI = [-0.0086,–0.0067]; F(1, 17580)=231.37, p&lt;1e-10; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, row 9). At the same time, middle fixation durations were also influenced by the value of the target not currently being viewed (β<sub>nofixValue</sub> = -0.035, CI=[-0.040,–0.030]; F(1, 17580)=197.78, p&lt;1e-10; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, row 10); because most middle fixations were the second fixations in the trial, this means that the value of the first-viewed item ‘carried over’ into the second fixation to influence its duration. Combining the influences of fixated and non-fixated values into a single variable, middle fixation durations increased as a function of relative value, defined here as the value of the fixated target minus the value of the non-fixated target (β<sub>relativeFixVal</sub> = 0.016, CI = [0.012, 0.021]; F(1, 17580)=48.55, p&lt;1e-10; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, row 11). Finally, we find that durations decreased as function of the absolute difference in target values (β<sub>difficulty</sub> = -0.019, CI = [-0.025,–0.0154]; F(1, 17580)=47.17, p&lt;1e-10; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, row 12). Together, these results support the notion that fixation behavior and valuation are not independent processes, and that the effects of value on viewing durations must be accounted for when computing and interpreting gaze biases (see <italic>Cumulative Gaze-Time Bias</italic>, below).</p></sec><sec id="s2-7"><title>Gaze-related choice biases</title><p>Humans show three core choice biases related to gaze behavior. First, they tend to choose the item that they viewed first (initial fixation bias). Second, they tend to choose the item they spend more time looking at over the course of the trial (cumulative gaze-time bias). Third, they tend to choose in favor of the target they are viewing at the moment they indicate their choice. All three of these biases are evident in our monkey subjects, as detailed below. However, before these biases can be quantified, we must first address the question of whether monkeys show evidence for a two-stage form of gaze bias recently identified in human behavior.</p></sec><sec id="s2-8"><title>Evidence for a two-stage model of gaze biases</title><p>To explain how these gaze biases come about, prior human studies have used modified sequential sampling models (aSSMs) that provide an increase in evidence accumulation for a given item while it is being viewed (see <xref ref-type="bibr" rid="bib24">Krajbich, 2019</xref>, for review). This bias generally takes one of two forms: Some studies suggest that the gaze amplifies the value of attended targets so that the increase in evidence accumulation is proportional to the attended value (i.e. is multiplicative). Others suggest that gaze acts by providing a fixed increase in evidence accumulation to attended targets, independent of target values (i.e. is additive). Studies directly comparing these two mechanisms are not fully conclusive (<xref ref-type="bibr" rid="bib57">Smith and Krajbich, 2019</xref>; <xref ref-type="bibr" rid="bib62">Thomas et al., 2019</xref>), perpetuating debates as to which is ‘correct’. A third, hybrid modeling framework offers a compromise (<xref ref-type="bibr" rid="bib32">Manohar and Husain, 2013</xref>; <xref ref-type="bibr" rid="bib66">Westbrook et al., 2020</xref>). Under this framework, a trial can be divided into two stages. The first stage is the decision stage, where evidence accrues for the decision offers, and gaze increases evidence accumulation for attended items in multiplicative fashion. The second stage begins once an internal decision boundary is reached (a choice is made) and continues until the choice is physically reported. During this interval the gaze becomes drawn to the to-be-chosen item, which is captured with an additive gaze bias. In other words, in the first stage, gaze influences choice, whereas in the second stage, the latent choice influences gaze.</p><p>Two patterns in the monkeys’ fixation data are consistent with this two-stage model. First, as shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, final fixation durations are longer than non-final fixations even after accounting for absolute order in the trial, suggesting some process that occurs at the end of each trial that prolongs fixation durations. Second, the monkeys’ final fixations were on the to-be-chosen target on nearly every trial (~97% of trials), consistent with gaze being drawn to the to-be-chosen items.</p><p>To more directly test the hybrid, two-stage, model in our data, we reasoned that the relative fit of a model with an additive gaze bias compared to one with a multiplicative gaze bias would indicate whether choice reflects one mechanism alone, or a mixture of both. Using the Gaze-Weighted Linear Accumulator Model (GLAM, <xref ref-type="bibr" rid="bib62">Thomas et al., 2019</xref>), we found that while the multiplicative model provided a slightly better fit than the additive model, the difference was not significant (<xref ref-type="fig" rid="fig4">Figure 4</xref>, 0 on the x-axis, <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). This suggests that our data likely reflect a mixture of additive and multiplicative gaze biases, consistent with the hybrid model.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Relative fit of multiplicative and additive models across NDT truncation values.</title><p>Using an aSSM modeling framework, we computed the posterior probability (y-axis) of a model with either a multiplicative gaze bias parameter (black circles) or an additive parameter (white circles) being more likely to have generated the data. Probabilities (x-axis) were computed using data in which the final portion of the trial was truncated by 0ms (N = 30,046 trials), 50ms (N = 30,045 trials), 100ms (N = 30,045 trials), 150ms (N = 30,044 trials), 200ms (N = 30,044 trials), 300ms (N = 30,011 trials), and 400ms (N = 29,771 trials). See <italic>Estimating and truncating the terminal non-decision time</italic> for details.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78205-fig4-v2.tif"/></fig><p>Next, we tested the assumption that the influence of gaze switches between the two biasing mechanisms near the end of the trial, at the moment a decision threshold is met. We reasoned that if this were the case, that the relative fit of one model over the other would increase if we removed increasing amounts of data from the end of the trial. Alternatively, if the multiplicative and additive influences were mixed throughout the trial, this truncation would have little effect on the relative fit of the two model variants. We found that the difference in fit between the two models became significant for truncation values of 100ms and higher, with the multiplicative model providing the better fit (<xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). Further, at 100ms the probability that data were generated by a multiplicative, and not an additive, model reaches 100% (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Together, this supports the two-stage model put forth by Westbrook, where gaze acts multiplicatively during the first stage before transitioning to additive approximately 100ms before the RT.</p></sec><sec id="s2-9"><title>Estimating and truncating the terminal non-decision time</title><p>Under the two-stage framework, the transition point between the multiplicative and additive stages of the decision process occurs when accumulated evidence reaches the decision boundary. In other words, the additive stage occurs <italic>after</italic> a decision has been made but before the RT is detected. In standard sequential sampling models, this type of post-decision/pre-report interval is referred to as terminal non-decision time (NDT; <xref ref-type="bibr" rid="bib47">Ratcliff and McKoon, 2008</xref>; <xref ref-type="bibr" rid="bib48">Resulaj et al., 2009</xref>). As noted above, we estimated that in our data, this transition occurs approximately 100ms prior to the RT. Because the focus of this study was to characterize the influence of gaze on the decision process itself (i.e. the putative first stage), the remaining analyses were conducted using data that were truncated by 100ms, in order to minimize the influence of post-decision gaze effects. However, as this is merely an estimate of the NDT interval, we repeated each analysis using data truncated over a range of NDT values from 0 to 400ms. The results, presented in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>, indicate that the gaze biases described in the following sections are robust to NDT truncation values of up to 200ms.</p></sec><sec id="s2-10"><title>Initial fixation bias</title><p>As is the case in humans, the monkeys were more likely to choose the target that they looked at first in a given trial. To quantify this bias, we used a logit mixed effects regression, which gives an estimate of the effect of an initial leftward fixation on the log odds of a leftward choice: β<sub>first-is-left</sub>=0.66, CI = [0.32; 1.00]; F(1,30043)=14.36, p=0.00015, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, 13. The fact that β<sub>first-is-left</sub> is significantly greater than zero indicates that an initial leftward fixation is associated with a greater likelihood of a leftward choice. The effect of target value differences computed in this same model was β<sub>value-difference</sub> = 1.40 (CI = [1.32, 1.43]; F (1,30043)=5349.40, p&lt;1e-10). The ratio between these two regression estimates (β<sub>first-is-left</sub> / β<sub>value-difference</sub>), gives the relative influence the initial fixation on choice as compared to the value differences of the targets: 0.66/1.40=0.47. In other words, the monkeys chose as if the first-fixated item was, on average, worth 0.47 drops of juice more than its nominal value. Critically, because the targets were obscured until the monkeys initiated their first saccade (see <italic>Methods</italic>), their initial fixations were unbiased with regard to target value. Thus, the choice bias in favor of the initially fixated item cannot be attributed to a tendency for the monkeys to look at higher value targets first.</p><p>This suggests that the initial fixation direction, whether exogenously or endogenously driven, may have a causal influence in choice. To test this causal role, we conducted additional experimental sessions in which we manipulated initial fixation by staggering the onset of the two targets (Monkey C: 16 sessions, n=6931 trials; Monkey K: 8 sessions, n=5433 trials). Staggered target onsets – either left first or right first – occurred randomly in 30% of the trials in these sessions (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), with concurrent onsets in the other 70% of trials (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). In the staggered-onset trials, the monkeys almost always directed their initial fixations to the target that appeared first (100% of staggered trials for Monkey C, 94.1% for Monkey K). Trials in which Monkey K did not fixate on the first target (n=99) were removed from analysis. As in the sessions without gaze manipulation described above, we found a main effect of initial fixation on choice (logit mixed effects regression: β<sub>first-is-left</sub>=0.70, CI = [0.41; 1.00]; F(1,12359)=21.83, p=3.02e-06; β<sub>first-is-left</sub> / β<sub>value-difference</sub> = 0.57 drops of juice; see <xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, row 14). Importantly, we found no main effect of trial-type (staggered vs. standard; β<sub>trial-type</sub> = -0.07, CI = [–0.33; 0.18]; F(1,12359)=0.32, p=0.57) nor was there a difference in the magnitude of the initial fixation effect across the two trial-types β<sub>first-is-left:trial-type</sub> = 1.48e-3, CI = [–0.23; 0.24]; F(1,12359)=1.52e-4, p=0.57. Thus, not only could we induce an initial fixation bias by manipulating where the monkey looked first, but the magnitude of this bias is comparable to the bias observed without a manipulation.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Initial gaze bias for gaze-manipulation sessions.</title><p>(<bold>A</bold>) Depiction of gaze manipulation procedure. In 30% of trials, only a single target was initially presented, randomly assigned to the left or right of the display (left panel). Once a saccade was detected (or 250ms elapsed, whichever happened first), the second target appeared (right panel). (<bold>B</bold>) Depiction of standard trials with masked targets appearing simultaneously on both the left <italic>and</italic> right of the display (left panel). Masks disappeared when the initial saccade was detected (right panel). (<bold>C–F</bold>) Probability of choosing left as a function of the difference between the value of the item on the left and the value of the item on the right. Data were split according to the direction of the initial fixation (cyan and magenta) Circles show the mean probabilities of left choice, error bars show the standard error of the mean over 16 sessions for Monkey C and 8 sessions for Monkey K. Lines show logistic fits from the mixed effects model (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, row 13). (<bold>C &amp; E</bold>) show data from trials where the onset of the first target was staggered (N = 3449). (<bold>D &amp; F</bold>) show trials where the targets appeared simultaneously (N = 8915). (<bold>C–D</bold>) shows data from Monkey C; (<bold>E–F</bold>) shows data from Monkey K.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78205-fig5-v2.tif"/></fig></sec><sec id="s2-11"><title>Cumulative gaze-time bias</title><p>Here, we asked whether monkeys were more likely to choose items that were fixated longer in a trial. We first quantified the time spent looking at the two choice targets in every trial. Then, using a logit mixed effects regression, we asked whether choice outcomes were dependent on the relative time spent looking at the left item as compared to the right item (‘Time Advantage Left’). Consistent with human choice patterns, monkeys were more likely to choose the left item as the relative time advantage for the left item increased (<xref ref-type="fig" rid="fig6">Figure 6B and C</xref>; β<sub>time-advantage</sub> = 18.71, CI = [15.07; 22.34]; F(1,28393)=101.76, p&lt;1 e- 10, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, row 15). Because fixation durations are themselves dependent upon the target values (see <italic>Fixation Durations Depend on Target Value</italic>, above), we repeated this analysis using choice probabilities that were corrected to account for the target values (see <italic>Methods</italic>). Using these corrected choice probabilities, the positive relationship between relative viewing times and choice was maintained. (<xref ref-type="fig" rid="fig6">Figure 6D and C</xref>; linear mixed effects regression: β<sub>time-advantage</sub> = 1.07, CI = [0.92; 0.1.21]; F(1, 28393)=208.90, p&lt;1 e- 10, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, row 16). Intuitively, this effect size would predict a change in choice probability of ~5% given a change in relative viewing time advantage from –0.1s to 0.1s. Thus, the monkeys were more likely to choose items that were fixated longer, independent of the values of the targets in each trial. The corrected cumulative gaze time bias effects were significantly above zero for data truncated up to 200ms (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Cumulative gaze-time bias.</title><p>(<bold>A</bold>) Task schematic. The portion of the trial used to show the cumulative gaze-time bias is highlighted in red. (<bold>B–C</bold>) Probability of choosing left as a function of the binned final gaze-time advantage for the left item. (<bold>D–E</bold>) The same as B-C, but using choice probabilities corrected to account for the target values in each trial (see <italic>Methods</italic>). For visualization, these graphs exclude those trials with a final time advantage greater than +/-3 standard deviations from the mean (&lt;1% of trials for each monkey). The trials were not excluded from the logit mixed effects regression reported in the text. In B-E, the bin boundaries were 0, ±0.2, and ±∞. Error bars indicate SEM across sessions. The effect of NDT truncation on this bias is shown in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. In this figure, beta values in A and B refer to the estimate for variable time-advantage in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, rows 15 and 16 (respectively), with standard errors derived from the mixed-effects model. Panels B and D show data from Monkey C (N = 15,175 trials); panels C and E shows data from Monkey K (N = 13,220 trials).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78205-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>The effect of NDT truncation on the cumulative gaze bias.</title><p>Beta values in A and B refer to the estimate for variable time-advantage in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, rows 15 and 16 (respectively), with standard errors derived from the mixed-effects model. Trial count per level of NDT truncation: 0ms (N = 28,394 trials), 50ms (N = 28,394 trials), 100ms (N = 28,394 trials), 150ms (N = 28,394 trials), 200ms (N = 28,394 trials), 300ms (N = 28,381 trials), and 400ms (N = 28,316 trials).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78205-fig6-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-12"><title>Final fixation bias</title><p>A prediction of earlier aSSM models is that the item being fixated at the end of the trial should be chosen more often than the non-fixated item, due to the net increase in evidence accumulation for fixated items, and consequently the greater likelihood that evidence for a fixated item reaches a bound at any given moment (<xref ref-type="bibr" rid="bib22">Krajbich et al., 2010</xref>; <xref ref-type="bibr" rid="bib23">Krajbich and Rangel, 2011</xref>; <xref ref-type="bibr" rid="bib60">Tavares et al., 2017</xref>). In contrast, the two-stage model described by <xref ref-type="bibr" rid="bib66">Westbrook et al., 2020</xref>, suggests that the final fixation reflects, rather than influences, the result of the accumulation process. Therefore, we assessed the final fixation bias in our data in a manner similar to prior studies but with one important difference: as detailed above, the fixation data in each trial were truncated by 100ms to minimize the influence of late-stage gaze effects. In the truncated data, the location of the final fixation in each trial is defined as the target being viewed by the monkey at the <italic>virtual</italic> end of the trial, which in an aSSM framework can be interpreted as an estimate of when the decision boundary was reached (e.g. <xref ref-type="bibr" rid="bib48">Resulaj et al., 2009</xref>).</p><p>Using the truncated data, we found that the monkeys were more likely to choose the target being viewed at the virtual trial end, consistent with the final fixation bias effects observed in humans (<xref ref-type="fig" rid="fig7">Figure 7</xref>, logit mixed effects regression: β<sub>last-is-left</sub>=3.35, CI = [2.34; 4.37]; F(1,28392)=42.00, p&lt;1e-10, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, row 17). Note that this effect is present even after removing up to 200ms from the end of the trial (approximately 25% of the full trial duration, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). This means that final fixation biases can be explained at least in part by an effect of gaze on evidence accumulation – as suggested by prior studies (<xref ref-type="bibr" rid="bib22">Krajbich et al., 2010</xref>)– when assuming a terminal NDT epoch of up to 200ms.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Final fixation bias.</title><p>(<bold>A</bold>) Task schematic highlighting the portion of the trial used to show the final fixation bias (red box). (<bold>B–C</bold>) Probability of choosing left as a function of the difference in item values, split according to the location of gaze (left or right) at the end of the trial. Circles show mean probabilities of left choice, error bars show the standard error of the mean over 25 sessions for Monkey C and 29 sessions for Monkey K. Lines show logistic fits from the mixed effects model (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, row 17). Panel B shows data from Monkey C (N = 15,175 trials); panel C shows data from Monkey K (N = 13,220 trials). The effect of NDT truncation on this bias is shown in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>. In this figure, beta value refers to the estimate for variable last-is-left <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, row 17, with standard errors derived from the mixed-effects model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78205-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>The effect of NDT truncation on the final fixation bias.</title><p>Beta value refers to the estimate for variable last-is-left <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, row 17, with standard errors derived from the mixed-effects model. Trial count per level of NDT truncation: 0ms (N = 28,394 trials), 50ms (N = 28,394 trials), 100ms (N = 28,394 trials), 150ms (N = 28,394 trials), 200ms (N = 28,394 trials), 300ms (N = 28,381 trials), and 400ms (N = 28,316 trials).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78205-fig7-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-13"><title>Gaze-Weighted Linear Accumulator Model (GLAM)</title><p>The analyses above show broad consistency between NHP and human data with respect to the association between gaze and choice behavior. The question remains, however, as to whether these patterns emerge from a similar mechanism to those observed in humans. To this end, we asked whether an aSSM framework could explain the gaze and choice patterns we observed in NHPs. We chose the Gaze-weighted Linear Accumulator Model (GLAM; <xref ref-type="bibr" rid="bib35">Molter et al., 2019</xref>), which was previously used to explain gaze-related choice biases in humans across multiple independent decision studies (<xref ref-type="bibr" rid="bib62">Thomas et al., 2019</xref>). Using this model, we asked three questions with regards to our NHP data: First, does a model with a gaze bias mechanism fit our data better than a similar model without a gaze bias? Second, how do gaze biases estimated in our monkey subjects compare to prior human work? Third, can the model accurately predict the critical features of choice behavior, such as accuracy, RT, and the influence of gaze on choice?</p><p>To answer the first question, we fit two variants of the GLAM to each monkeys’ data. These variants differed only in their treatment of the gaze bias parameter, γ (see <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> for model specification and parameter definitions). Note that since the model was fit using the NDT-truncated data, we exclusively utilized the multiplicative version of the model (See <xref ref-type="fig" rid="fig4">Figure 4</xref> and associated text). In the first GLAM variant, the gaze bias parameter γ was left as a free parameter, allowing gaze to modify the rate of evidence accumulation for fixated items. The second model fixed γ at 1, assuming a priori that gaze has no effect on the decision process. The maximum a posteriori (MAP) and 95% highest posterior density intervals (HPD) for the parameters estimated in both model variants are given in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>. The relative fit of the two variants was assessed using the difference in WAIC score (dWAIC, <xref ref-type="bibr" rid="bib64">Vehtari et al., 2017</xref>), defined by subtracting the WAIC value for the model with γ fixed at 1 from the WAIC for the model where γ was a free parameter. The dWAIC for Monkey C was –3357.9, and for Monkey K was –8159.7, indicating that the model that allowed for the presence of gaze biases is the better fit of the two model variants. Further, this difference was significant as indicated by nonoverlapping WAIC values ±their standard error (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). In the gaze-bias model, γ values were as follows: Monkey C: MAP = 0.079, HPD = [0.045, 0.12]; Monkey K: MAP = –0.056, HPD = [-0.087,–0.018].</p><p>Since the GLAM had previously been used to explore gaze biases in four human decision-making datasets (<xref ref-type="bibr" rid="bib13">Folke et al., 2016</xref>; <xref ref-type="bibr" rid="bib22">Krajbich et al., 2010</xref>; <xref ref-type="bibr" rid="bib23">Krajbich and Rangel, 2011</xref>; <xref ref-type="bibr" rid="bib60">Tavares et al., 2017</xref>), we were able to directly compare the extent of the gaze bias by comparing the γ parameters for humans vs. our monkey subjects. Treating each session as an independent sample, we found that the distribution of γ in each of our monkey subjects fully overlapped with the γ distributions from human studies (<xref ref-type="fig" rid="fig8">Figure 8</xref>).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Distribution of gaze bias parameter (γ) across species.</title><p>X-axis shows the range of possible values of the gaze bias parameter, γ. The gray distribution combines the γ values estimated for each subject in the four human datasets previously fit using the GLAM (<xref ref-type="bibr" rid="bib62">Thomas et al., 2019</xref>). The means for each dataset are given as triangles above the distributions. The blue and red distributions show the session-wise distribution of γ parameters for Monkeys C, and K, respectively. Triangles in the corresponding colors show the means of the monkeys’ distributions. Total number of subjects for each dataset are as follows: <xref ref-type="bibr" rid="bib13">Folke et al., 2016</xref>: 24 subjects; <xref ref-type="bibr" rid="bib60">Tavares et al., 2017</xref>: 25; <xref ref-type="bibr" rid="bib23">Krajbich and Rangel, 2011</xref>: 30; <xref ref-type="bibr" rid="bib22">Krajbich et al., 2010</xref> Total number of sessions for the monkeys were 29 and 25 for Monkeys C and K, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78205-fig8-v2.tif"/></fig><p>Finally, we assessed the model’s predictive accuracy by performing out-of-sample simulations to predict core features of the behavioral data. To do this, the data were first split into even- and odd-numbered trials. Next, the models were estimated using only the even-numbered trials. Finally, these estimates were used to simulate each of the held-out trials 10 times, resulting in a set of simulated choice and reaction time measures that could be compared to the empirically observed behavior in the held out trials. As illustrated in <xref ref-type="fig" rid="fig9">Figure 9</xref>, the model predictions closely match the monkeys’ actual performance (see also <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, rows 18–20 and <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>). Note that repeating this procedure with γ fixed at 1 (no gaze bias) produces accurate predictions of choices and reaction times but fails to capture the effect of cumulative gaze-bias on choice (not illustrated, see <xref ref-type="bibr" rid="bib62">Thomas et al., 2019</xref>).</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Out-of-sample predictions of the GLAM model for choice and gaze behavior.</title><p>White bars show the empirically observed behavioral data from monkeys in odd-numbered trials. Blue dots and lines show GLAM model predictions for odd-numbered trials. Error bars show the standard error of the mean (SEM) across 25 sessions for Monkey C and 29 sessions for Monkey K. (<bold>A,D</bold>) Fraction of left choices as a function of the left minus right target value (in units of juice drops). (<bold>B,E</bold>) Reaction time decreases as a function of difficulty; note that reaction times are shorter here than in <xref ref-type="fig" rid="fig1">Figure 1E</xref>, due to the use of NDT-truncated data. (<bold>C,F</bold>) Corrected probability of choosing the left target as a function of the fraction of time spent looking at the left target minus fraction of time spent looking at the right target. The bin boundaries are 0,±0.33,±0.67, and ±1. Panels A-C correspond Monkey C (N = 15,612 trials); panels D-F correspond to Monkey K (N = 14,433 trials).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78205-fig9-v2.tif"/></fig><p>In summary, an aSSM model that includes gaze bias provides a better fit than a model without a gaze bias, and the magnitude of this bias is comparable to that estimated in human data using the same modeling framework. Finally, the model is able to make accurate out-of-sample predictions of choice and gaze behavior.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Over the last decade, there has been a growing appreciation for the active role that attention plays in a broad array of decision-making domains. However, the neural substrates of this relationship remain largely unknown due to the lack of a suitable animal model. The aim of the present study was to develop such a model using a novel value-based decision-making task for NHPs. We validated our animal model on two measures: behavioral control and the presence of human-comparable gaze biases. First, the monkeys’ performance indicated good behavioral control: both choices and RTs were a graded function of task difficulty (defined by the difference in target values), with fast and nearly optimal choices in the easiest trials, and slower and more variable choices in harder trials. Such signatures of behavioral control are crucial to the development of any animal model of complex cognitive processes, as they allow the confident attribution of an animal’s behavior to the construct of interest, and not to attentional lapses or guesses (<xref ref-type="bibr" rid="bib12">Fetsch, 2016</xref>; <xref ref-type="bibr" rid="bib26">Krakauer et al., 2017</xref>). For the present study, this suggests that the monkeys’ behavior is ascribable to a decision process driven primarily by the values of the targets in each trial. As to the second criterion, our results show that the monkeys exhibit similar gaze biases to those observed humans, both in terms of the core behavioral measures and also in their ability to be mechanistically described using an aSSM. Taken together, we conclude that this novel task is well suited for use in neural mechanistic studies, such as concurrent electrophysiological recordings or stimulation.</p><sec id="s3-1"><title>Comparison to prior human studies</title><p>While our findings are broadly consistent with those from humans, there are two notable differences between the monkeys’ gaze behavior in the present study, and that of humans reported in prior studies. First, monkeys made fewer fixations per trial than humans on average (~2.5 vs 3.7); second, the monkeys’ final fixations in each trial tended to be prolonged compared to non-final fixations—the opposite of what has been reported in many human studies. Critically, these two differences may be related. For both humans and monkeys, fixations made early in the trial are longer than those made later; thus, because the monkeys’ final fixations occur early in the trial (e.g. either 2nd or 3rd), they will be longer on average than final fixations in humans, which tend to occur later in the trial (e.g. 4th, 5th, etc.).</p><p>One significant similarity between the current study and prior studies in humans was the positive association between gaze and choices, which was well-explained using an aSSM. As noted in the results, our data are most consistent with a two-stage aSSM in which gaze initially biases the choice process, and then (after a decision boundary is reached) transitions to a mode in which gaze becomes drawn to the intended choice target (<xref ref-type="bibr" rid="bib32">Manohar and Husain, 2013</xref>; <xref ref-type="bibr" rid="bib66">Westbrook et al., 2020</xref>). The behavioral feature of this second stage is consistent with a motor-preparatory phenomenon known as a gaze-anchoring, in which gaze becomes ‘locked’ onto the reach target in order to increase the speed and efficiency of the response (<xref ref-type="bibr" rid="bib3">Battaglia-Mayer et al., 2001</xref>; <xref ref-type="bibr" rid="bib10">Dean et al., 2011</xref>; <xref ref-type="bibr" rid="bib40">Neggers and Bekkering, 2000</xref>).</p><p>In prior studies of gaze-anchoring in humans and NHPs, the latency between the eyes locking onto target of an upcoming response and the beginning of detected movement ranges between 64 and 100ms (<xref ref-type="bibr" rid="bib3">Battaglia-Mayer et al., 2001</xref>; <xref ref-type="bibr" rid="bib41">Neggers and Bekkering, 2001</xref>; <xref ref-type="bibr" rid="bib49">Reyes-Puerta et al., 2010</xref>; <xref ref-type="bibr" rid="bib58">Stuphorn et al., 2000</xref>). The terminal NDT estimate that we derived from the model comparison is consistent with these latencies, and with NDT estimates obtained from standard sequential sampling models (e.g. <xref ref-type="bibr" rid="bib48">Resulaj et al., 2009</xref>). Ultimately, the true duration of the post-decision epoch is difficult to estimate from behavioral data alone. Additional work will be necessary to either measure or infer the plausible range of post-decision/pre-report intervals in our task, in order to confirm the robustness of the gaze bias effects measured here.</p></sec><sec id="s3-2"><title>Comparison to prior NHP studies</title><p>Beyond similarities to prior human studies, our results are also consistent with the handful of NHP studies in which some degree of free eye movement was permitted. Among these, both <xref ref-type="bibr" rid="bib18">Hunt et al., 2018</xref> and <xref ref-type="bibr" rid="bib50">Rich and Wallis, 2016</xref> imposed some restrictions on eye movements, making it difficult to assess the relationship between fully natural gaze patterns and choices. <xref ref-type="bibr" rid="bib6">Cavanagh et al., 2019</xref> used a fully free-viewing design, and also observed a bias in favor of initially fixated items.</p><p>Our study confirms this result, and provides additional insights, due to novel elements of the study design. First, by manipulating initial fixation direction, we show that initial fixation has a causal effect on the decision process. Second, the results of the aSSM model-fitting provide a mechanistic explanation for our results and permits a direct comparison to model fitting results in human studies.</p><p>Finally, unlike many prior studies, the decision task requires the monkeys to sample the targets by fixating them directly, due to the use of a gaze contingent mask that completely obscures the stimuli at the beginning of the trial, and the use of visual crowders that closely surround each target. This subtle design element is crucial, because it permits accurate measurement of the time spent attending to each target, and of the relationship between relative viewing time and choice. In a forthcoming study using this same task, we show that OFC neurons do not begin to reflect the value of the second-fixated stimulus until well after it has been viewed by the monkey, indicating that value information is only available to the monkey once a target is fixated (<xref ref-type="bibr" rid="bib34">McGinty and Lupkin, 2021</xref>). In contrast, in tasks where value-associated targets are easily perceived in peripheral vision, even targets that are not fixated influence prefrontal value signals and decision behavior (e.g. <xref ref-type="bibr" rid="bib6">Cavanagh et al., 2019</xref>; <xref ref-type="bibr" rid="bib69">Xie et al., 2018</xref>). High-value peripheral targets are also implicated in value-based attentional capture, in which overt or covert attention becomes drawn to objects that are reliably associated with reward (<xref ref-type="bibr" rid="bib1">Anderson et al., 2011</xref>; <xref ref-type="bibr" rid="bib21">Kim et al., 2015</xref>). Thus, the task used in the present study minimizes value-based capture effects.</p></sec><sec id="s3-3"><title>Hypothesized neural substrates</title><p>The impetus for developing this animal model is to explore the neural underpinnings of the relationship between gaze and value-based choice. Given the fact that a sequential sampling model provides objectively good fits to the observed behavior, one potential approach for understanding neural mechanisms is to identify brain regions that correspond to different functions in a sequential sampling framework. Such an approach has been used in perceptual dot motion discrimination, where extrastriate cortical area MT is thought to provide the ‘input’ signal (encoding of visual motion) and neurons in parietal area LIP are thought to represent the accumulated evidence over time (see <xref ref-type="bibr" rid="bib15">Gold and Shadlen, 2007</xref>, for review). One note of caution, however, is that brain activity that outwardly appears to implement accumulator-like functions may not be causally involved in the choice (e.g. <xref ref-type="bibr" rid="bib20">Katz et al., 2016</xref>). Nonetheless, it is still useful to consider the neural origins of two core computations suggested by sequential sampling models: the representation of the target values (corresponding to the input signals), and neural signals that predict of the actions performed to obtain the reward (reflecting the decision output).</p><p>For the encoding of accumulated evidence, findings from NHP neurophysiology point to regions involved in the preparation of movement. In motion discrimination tasks that use eye movements to report decisions, accumulated evidence signals can be observed in oculomotor control regions such as area LIP, frontal eye fields, and the superior colliculus. Because the decision in our task is reported with a reach movement, potential sites for accumulator-like activity include motor cortical areas such as the arm representations within dorsal pre-motor cortex (<xref ref-type="bibr" rid="bib7">Chandrasekaran et al., 2017</xref>). In addition, human imaging studies have identified several other candidate regions with accumulator-like activity, including in the intraparietal sulcus, insula, caudate, and lateral prefrontal cortex (<xref ref-type="bibr" rid="bib14">Gluth et al., 2012</xref>; <xref ref-type="bibr" rid="bib17">Hare et al., 2011</xref>; <xref ref-type="bibr" rid="bib46">Pisauro et al., 2017</xref>; <xref ref-type="bibr" rid="bib52">Rodriguez et al., 2015</xref>). Interestingly, several studies find accumulator-like signals in cortical areas in the dorsal and medial frontal cortex, but these are not consistently localized. For example, whereas <xref ref-type="bibr" rid="bib46">Pisauro et al., 2017</xref> find accumulator-like activity in the supplementary motor area, <xref ref-type="bibr" rid="bib52">Rodriguez et al., 2015</xref> identify a more anterior region in the medial frontal cortex. Additional studies in both humans and NHPs will be necessary to understand the specializations of these regions with respect to encoding decision evidence.</p><p>Because decision behavior in this study was value-dependent, regions encoding economic value are strong candidates for signals that provide input to an accumulator. Value-related signals have been observed in numerous cortical and subcortical regions, including the amygdala (e.g. <xref ref-type="bibr" rid="bib45">Paton et al., 2006</xref>), the ventral striatum (<xref ref-type="bibr" rid="bib19">Kable and Glimcher, 2009</xref>; <xref ref-type="bibr" rid="bib29">Lim et al., 2011</xref>), and the ventromedial frontal lobe (VMF), including both the orbitofrontal and ventromedial prefrontal cortices (<xref ref-type="bibr" rid="bib43">Padoa-Schioppa and Assad, 2006</xref>; <xref ref-type="bibr" rid="bib50">Rich and Wallis, 2016</xref>; <xref ref-type="bibr" rid="bib63">Vaidya and Fellows, 2015</xref>). Of these regions, the VMF has received the most scrutiny with regard to gaze-biases. In humans, there is evidence of BOLD activity in this region related to attention-guided decision-making (<xref ref-type="bibr" rid="bib14">Gluth et al., 2012</xref>; <xref ref-type="bibr" rid="bib17">Hare et al., 2011</xref>; <xref ref-type="bibr" rid="bib28">Leong et al., 2017</xref>; <xref ref-type="bibr" rid="bib29">Lim et al., 2011</xref>; <xref ref-type="bibr" rid="bib46">Pisauro et al., 2017</xref>; <xref ref-type="bibr" rid="bib52">Rodriguez et al., 2015</xref>), and patients with lesions to this area show impairments in such tasks (<xref ref-type="bibr" rid="bib63">Vaidya and Fellows, 2015</xref>).</p><p>In NHPs, the orbitofrontal portion of the VMF (the OFC) has been demonstrated to contain value-signals that are dependent on whether gaze is directed towards a reward-associated visual target (<xref ref-type="bibr" rid="bib18">Hunt et al., 2018</xref>; <xref ref-type="bibr" rid="bib33">McGinty et al., 2016</xref>; <xref ref-type="bibr" rid="bib34">McGinty and Lupkin, 2021</xref>) a mechanism fully consistent with the effects of gaze posited by aSSM frameworks. However, the precise neural mechanisms explaining how gaze biases choice outcomes are still unknown. One hypothesis is that the value-coding regions inherit gaze-modulated information from visual cortical regions sensitive to shifts in visual attention. For instance, neurons in the anterior inferotemporal cortex, which projects directly to the OFC (<xref ref-type="bibr" rid="bib5">Carmichael and Price, 1995</xref>; <xref ref-type="bibr" rid="bib27">Kravitz et al., 2013</xref>), selectively encode features of attended objects to the exclusion of others in the same receptive field (<xref ref-type="bibr" rid="bib11">DiCarlo and Maunsell, 2000</xref>; <xref ref-type="bibr" rid="bib36">Moore and Armstrong, 2003</xref>; <xref ref-type="bibr" rid="bib38">Moran and Desimone, 1985</xref>; <xref ref-type="bibr" rid="bib51">Richmond et al., 1983</xref>; <xref ref-type="bibr" rid="bib54">Sheinberg and Logothetis, 2001</xref>). Therefore, the similar fixation-driven effects on OFC value-coding shown by McGinty and colleagues (<xref ref-type="bibr" rid="bib33">McGinty et al., 2016</xref>; <xref ref-type="bibr" rid="bib34">McGinty and Lupkin, 2021</xref>) may be inherited from this structure. Another candidate region for the top-down influences on gaze-dependent value-coding, is the frontal eye fields (FEF). In a recent study from <xref ref-type="bibr" rid="bib25">Krajbich et al., 2021</xref>, focal disruption of FEF leads to a reduction in the magnitude of gaze-biases. Prior work in both humans and NHPs has shown that the FEF can influence perceptual sensitivity through the top-down modulation of early visual areas (<xref ref-type="bibr" rid="bib36">Moore and Armstrong, 2003</xref>; <xref ref-type="bibr" rid="bib37">Moore and Fallah, 2004</xref>; <xref ref-type="bibr" rid="bib53">Ruff et al., 2006</xref>; <xref ref-type="bibr" rid="bib55">Silvanto et al., 2006</xref>; <xref ref-type="bibr" rid="bib61">Taylor et al., 2007</xref>), suggesting that it may exert similar influence on value-encoding regions.</p></sec><sec id="s3-4"><title>Conclusions</title><p>In real-world decision scenarios, both humans and NHPs depend heavily on active visual exploration to sample information in the environment and make optimal decisions. In this way, even simple decisions require dynamic sensory-motor coordination. The importance of these gaze dynamics is underscored by the growing number of human studies showing that gaze systematically biases the decision process. We have developed a novel value-based decision-making task for macaque monkeys and found that monkeys spontaneously exhibit gaze biases that are quantitatively similar to humans’, while also showing the high level of behavioral control necessary for neurophysiological study. This novel paradigm will therefore make it possible to identify the cell- and circuit-level mechanisms of these biases.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Subjects</title><p>The subjects were two adult male rhesus monkeys (<italic>Macaca Mulatta</italic>), referred to as Monkey C and Monkey K. Both subjects weighed between 13.5 and 15 kg during data acquisition, and both were 11 years old at the start of the experiment. They were implanted with an orthopedic head restraint device under full surgical anesthesia using aseptic techniques and instruments, with analgesics and antibiotics given pre-, intra-, and post-operatively as appropriate. Data from Monkey K were collected at Stanford University (25 sessions, 14,433 trials); data from Monkey C were collected at Rutgers University—Newark (29 sessions, 15,613 trials). All procedures were in accordance with the Guide for the Care and Use of Laboratory Animals (<xref ref-type="bibr" rid="bib39">National Research Council, 2011</xref>) and were approved by the Institutional Animal Care and Use Committees of both Stanford University and Rutgers University—Newark.</p></sec><sec id="s4-2"><title>Study design and apparatus</title><p>The subjects performed the task while head-restrained and seated in front of a fronto-parallel CRT monitor (120 Hz refresh rate, 1024x768 resolution), placed approximately 57 cm from the subjects’ eyes. Eye position was monitored at 250 Hz using a non-invasive infrared eye-tracker (Eyelink CL, SR Research, Mississauga, Canada). Three response levers (ENV- 612 M, Med Associates, Inc, St. Albans, VT) were placed in front of the subjects, within their reach. The ‘center’ lever was located approximately 17 cm below and 35 cm in front of the display center, and the center points of the two other levers were approximately 9 cm to the left and right of the center lever. The behavioral paradigm was run in Matlab (Mathworks, Inc, Natick, MA) using custom scripts using the Psychophysics Toolbox extensions (<xref ref-type="bibr" rid="bib4">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib8">Cornelissen et al., 2002</xref>) . Juice rewards were delivered through a sipper tube placed near the subjects’ mouths.</p></sec><sec id="s4-3"><title>Behavioral task</title><sec id="s4-3-1"><title>Task structure</title><p><xref ref-type="fig" rid="fig1">Figure 1A</xref> shows an abbreviated task sequence; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref> shows the full sequence as described here: Each trial began with a fixation point in the center of the screen. Upon appearance of the fixation point, the subjects were required to both depress the center lever and to maintain fixation within a radius of 3.5 degrees of visual angle from the center of the fixation point. The monkeys were required to use only one hand to perform the task (Monkey K used his right hand; Monkey C used his left). After the initial fixation and center lever press were both held for an interval between 1 and 1.5 s, the fixation point disappeared, and two target arrays were presented 7.5 degrees to the left and right of the fixation point. Each target array consisted of a choice target, an initial mask, and six non-relevant ‘visual crowders’. The net effect of the mask and crowders was to obscure the target until the monkey looked directly at the target with a saccadic eye movement. See <italic>Gaze Contingent Mask and Visual Crowders,</italic> below, for details. The two targets shown in each trial were selected randomly without replacement from a set of 12 (see <xref ref-type="fig" rid="fig1">Figure 1B</xref> &amp; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>) and placed randomly on the left or right side of the display, giving 132 unique trial conditions.</p><p>Once the target arrays appeared, the monkeys were free to move their gaze as they desired and were free to initiate a choice at any time. To initiate a choice, the monkeys lifted their hand off the center lever, at which point the target arrays were extinguished. After releasing the center lever, the monkeys had 500 or 400ms (for Monkey C and K, respectively) to choose their desired target by pressing the left or right lever. After a delay (1 s for Monkey K; 1 or 1.5 s for Monkey C), the subjects received the volume of juice reward associated with the chosen target (range 1–5 drops). If neither the left nor right lever was pressed within 400 or 500ms of the center lever lift, the trial ended in an error. Each trial was followed by an inter-trial interval randomly drawn from a uniform distribution between 2 and 4 s. Eye movements following the initial fixation were monitored but not enforced, and had no programmatic influence on the outcome of the trial. Reaction time (RT) was calculated as the time between target arrays onset and the release of the center lever.</p></sec><sec id="s4-3-2"><title>Choice targets</title><p>Choice targets were constructed as composites of two features, color and shape, and the reward value of a given target (range 1–5 drops) was determined by the sum of the reward values associated with its two features. (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>). Notably, this scheme allows for multiple visually distinct targets to share the same value. An example target set is shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>. For a given set, the colors were selected by randomly sampling four equally-spaced hues from a standardized color wheel based on the CIELUV color space, and shapes were quasi-randomly selected from a library of 28 equal-area shapes.</p><p>New stimulus sets were generated frequently to avoid over-training on a given stimulus set. For every new set, 1–2 training sessions (not used for analysis) were performed so that the monkeys could become familiar with the stimuli, followed by 1–3 sessions of data collection.</p></sec><sec id="s4-3-3"><title>Gaze-contingent initial mask and visual crowders</title><p>A key design goal of this task was to encourage the monkeys to fixate onto the choice targets directly using saccadic eye movements. To encourage this behavior, we used two methods to obscure the choice targets until they were fixated directly.</p><p>The first method was a gaze-contingent initial mask: when the target arrays first appeared in each trial (i.e. the first frame), the choice targets were not shown; rather, in their place were two non-relevant, non-informative masking stimuli-drawn from a pool of visual crowder stimuli (described below). The masking stimuli remained until the monkeys’ moved their eyes out of the initial fixation window by initiating a saccade, usually ~150ms after the onset of the arrays. Once the eyes left the initial window, the masking stimuli were replaced with the actual choice targets in the next display frame. Because the display frame rate (one frame every 8.25ms) was shorter than the typical time taken to complete a saccade (~30–40ms), the switch from the masks to the actual stimuli occurred while the monkeys’ eyes were moving, so that the choice targets appeared on the display before the end of the saccade (See <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref> for a step-by-step illustration). Therefore, information about target values was only available after the monkeys initiated a saccade towards one of the two target arrays.</p><p>The second method was the use of 6 visual crowders placed in close proximity in a hexagonal arrangement around each target (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). These crowders minimized the monkeys’ ability to use peripheral vision to identify the choice targets (<xref ref-type="bibr" rid="bib9">Crowder and Olson, 2015</xref>; <xref ref-type="bibr" rid="bib67">Whitney and Levi, 2011</xref>), further encouraging them to make fixations directly onto the targets.</p><p>The initial mask stimuli and the visual crowders were constructed using plus (+) or cross (x) shapes, colored using five hues randomly sampled from equally spaced points on a standardized color wheel. These stimuli convey no value information because the plus and cross shapes were never used to construct target stimulus sets, and because the use of multiple, random hues within each of the stimuli does not signify any one color that could indicate value.</p><p>The crowders and initial target masking were highly effective at encouraging the monkeys to fixate both targets before initiating a choice: both targets were viewed in 97.2% of trials for Monkey C and 92.6% of trials for Monkey K. These methods also ensured that the first fixation following target onset was unbiased with respect to the target values: in trials where one target had a larger value than the other, the monkeys directed their first saccade towards the higher value target 49.9% of the time, which was not significantly different from chance (t (25559)=–0.45, p=0.65).</p></sec></sec><sec id="s4-4"><title>Gaze manipulation</title><p>To test the causal nature of the relationship between initial fixation and choice, we conducted additional sessions in which we temporally staggered the onset of the two targets (Monkey C: 16 sessions, n=6931 trials; Monkey K: 8 sessions, n=5433 trials). During these sessions, in 30% of the trials the first target array would appear randomly on either the left or right of the screen. The second target array appeared 250ms after the onset of the first, or when the monkeys began their initial saccade away from the central fixation point, whichever came first. The location of the first target array was randomized across trials. This manipulation was intended to encourage the monkeys to direct their attention towards the target that appeared first, as shown in prior human studies (<xref ref-type="bibr" rid="bib60">Tavares et al., 2017</xref>). As expected, the monkeys directed their initial fixation to the cued target on 96% of the staggered-onset trials.</p></sec><sec id="s4-5"><title>Behavioral analysis</title><p>With the exception of the computational modeling, analyses were performed in Matlab v. R2022a (Mathworks, Inc, Natick, MA). All graphs, except <xref ref-type="fig" rid="fig3">Figure 3</xref>, were generated within the Matlab environment. For <xref ref-type="fig" rid="fig3">Figure 3</xref>, the violin plots were generated using the Seaborn v. 0.12.2 library for Python (<xref ref-type="bibr" rid="bib65">Waskom, 2021</xref>). Unless otherwise noted, analyses were conducted using generalized mixed-effects regressions with random effects for monkey-specific slopes and intercepts. To fit these models, data were collapsed across sessions, unless otherwise noted. Full specifications for all models are given in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>. Regression estimates (β’s) and 95% confidence intervals (CI) are reported for each effect. Significance for each effect was determined using a post-hoc ANOVA in which the regression estimates were compared to 0 (<italic>F-</italic> and p- values, reported in the text).</p><p>The onset and duration of fixations, defined as periods of stable gaze upon one of the two targets, were recorded by the Eyelink CL software. For analysis purposes, a fixation was considered to be on a given target if it was located within 3–5 degrees of the target center. Two consecutive fixations made onto the same target were merged into one continuous fixation. Fixations that were not onto either target were discarded; however, these were extremely rare: in the vast majority of trials, the monkeys fixated exclusively upon the targets.</p><p>For analysis purposes, fixations were designated as ‘first’, ‘final’ or ‘middle’. First fixations were defined as the first target that was fixated after the onset of the target arrays; final fixations were defined as the target being fixated at the moment the monkey initiates a choice by lifting the center lever; and middle fixations were defined as all on-target fixations that were neither first nor final. Fixation durations were measured in the conventional manner, except for final fixations, which were considered to be terminated at the time of choice in each trial, consistent with previous human studies (<xref ref-type="bibr" rid="bib22">Krajbich et al., 2010</xref>; <xref ref-type="bibr" rid="bib23">Krajbich and Rangel, 2011</xref>; <xref ref-type="bibr" rid="bib60">Tavares et al., 2017</xref>; <xref ref-type="bibr" rid="bib66">Westbrook et al., 2020</xref>). When using truncated data (see <italic>Estimation and Truncation of Terminal Non-Decision Time (NDT), below</italic>) final fixations were terminated at the virtual trial end point.</p><p>For analyses of the final fixation bias and the cumulative gaze-time bias, trials containing only a single fixation were excluded (Monkey C: N=447 trials (2.8% of total), Monkey K: N=1217 trials (8.4% of total)), as they could artificially inflate either or both of these biases. For the latter analysis, we computed in every trial the time advantage for the left item by subtracting the total duration of fixations onto the right target from the total duration of fixations onto the left target (<xref ref-type="bibr" rid="bib22">Krajbich et al., 2010</xref>). Thus, for trials where the monkey allocated more gaze time to the left target, the time advantage takes a positive value, whereas it takes a negative value for trials where the monkey allocated more gaze time to the right.</p><p>Finally, for the cumulative gaze bias analyses shown in <xref ref-type="fig" rid="fig6">Figure 6D and E</xref>, choice proportions were corrected using the same procedure set forth in <xref ref-type="bibr" rid="bib22">Krajbich et al., 2010</xref>. For each trial, we coded the choice outcome as 0 for a right choice and 1 for a left choice. Then from each trial’s choice outcome we subtracted the average probability of choosing left for all trials in that same condition, where the condition was defined as the left minus right target values. This correction removes from the choice data any variance attributable to the differences in the target values; as a result, <xref ref-type="fig" rid="fig4">Figure 4C and E</xref> reflect the effect of cumulative gaze after accounting for the influence of the target values on gaze durations.</p></sec><sec id="s4-6"><title>Computational model</title><p>To gain insight into potential mechanisms underlying choice and gaze behavior in this task, we employed the Gaze Weighted Linear Accumulator Model (GLAM, <xref ref-type="bibr" rid="bib62">Thomas et al., 2019</xref>). The GLAM modeling was conducted in Python version 3.7 using the <italic>glambox</italic> toolbox (see <italic>Code Availability</italic> for details). The GLAM is an aSSM characterized by a series of parallel accumulators racing to a single bound. These accumulators are co-dependent, such that their combined probability of reaching the bound is equal to 1. For two choice options, this formulation is mathematically equivalent to a single accumulator drifting between two bounds (<xref ref-type="bibr" rid="bib62">Thomas et al., 2019</xref>; <xref ref-type="bibr" rid="bib23">Krajbich and Rangel, 2011</xref>). The effect of gaze on choice behaviors is implemented as follows: The GLAM posits that over the course of a trial the absolute evidence signal for each item, <italic>i</italic>, can have two states: a biased state during periods when an item is viewed, and an unbiased state when it is not. The average absolute evidence for each item, <italic>A<sub>i</sub>,</italic> is a linear combination of these two states throughout the trial. As noted in the results, there are two methods through which gaze could bias choice behavior: a multiplicative manner in which gaze amplifies the value of the attended target, and an additive manner in which gaze adds a fixed increase in evidence accumulation to the attended target, independent of its value.</p><p>Assuming a multiplicative bias, <italic>A<sub>i</sub></italic> is defined according to <xref ref-type="disp-formula" rid="equ1">Equation 1a</xref>.<disp-formula id="equ1"><label>(1a)</label><mml:math id="m1"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>γ</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Here, <italic>r<sub>i</sub></italic> is the veridical value of target <italic>i</italic> (number of drops of juice), <italic>g<sub>i</sub></italic> is the fraction of gaze time that the monkey devoted to that target during a given trial, and γ is the crucial gaze bias parameter, which determines the extent to which the non-fixated item is discounted. This parameter has an upper bound of 1, which corresponds to no gaze bias. A value of γ less than 1 indicates biased accumulation in favor of the fixated item.</p><p>Under an additive gaze-biasing mechanism, <italic>A<sub>i</sub></italic> is defined according to <xref ref-type="disp-formula" rid="equ2">Equation 1b</xref>.<disp-formula id="equ2"><label>(1b)</label><mml:math id="m2"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>γ</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>As in the multiplicative model, <italic>r<sub>i</sub></italic> is the veridical value of target <italic>i</italic> (number of drops of juice), <italic>g<sub>i</sub></italic> is the fraction of gaze time that the monkey devoted to that target during a given trial, and γ is the gaze bias parameter. However, in the additive model variant, a γ value of 0 corresponds to no gaze bias, and a nonzero γ value indicates biased accumulation in favor of the fixated item.</p><p>The gaze-adjusted absolute evidence signals for each item are then used as inputs to an evidence accumulation process, as follows: First, each absolute value signal (<italic>A<sub>i</sub></italic>) is converted to a relative evidence signal, <italic>R<sub>i</sub>,</italic> through a logistic transform (see <xref ref-type="bibr" rid="bib35">Molter et al., 2019</xref>; <xref ref-type="bibr" rid="bib62">Thomas et al., 2019</xref> for details and rationale). Then, evidence for each item (<italic>E<sub>i</sub></italic>) at time <italic>t</italic> is accumulated according to <xref ref-type="disp-formula" rid="equ3">Equation 2</xref>, where ν and σ indicate the speed of accumulation and the standard deviation of the noise, respectively, and <italic>R<sub>i</sub></italic> is the relative evidence signal. <italic>N</italic> indicates a univariate normal distribution. A choice is made when the evidence for one of the items reaches the threshold.<disp-formula id="equ3"><label>(2)</label><mml:math id="m3"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>v</mml:mi><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>;</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mn>0</mml:mn></mml:math></disp-formula></p><p>We fit several different variants of the GLAM model to each monkey’s data. To evaluate a hybrid two-stage modeling framework, we compared the relative fit of multiplicative and additive models to the data (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Given the overall better fit obtained from the multiplicative models, these were used for the remainder of the analyses. To evaluate the necessity of the gaze bias parameter γ for explaining behavior, we used two variants of the multiplicative model, one in which the gaze bias parameter was left as a free parameter, and another that assumed no gaze bias by setting <italic>γ</italic>=1 (results in reported in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>). Model fits were compared using the Widely Applicable Information Criterion (WAIC, <xref ref-type="bibr" rid="bib64">Vehtari et al., 2017</xref>) which accounts for the differences in the complexity of the two model variants. Specifically, we used both the signed difference of WAIC values and the WAIC weights for each model, which can be interpreted as the probability that each model is true, given the data (<xref ref-type="bibr" rid="bib64">Vehtari et al., 2017</xref>).</p><p>In addition, we assessed the model’s predictive accuracy by estimating the parameters for the free- γ model variant using only even numbered trials, and then using these parameter estimates to simulate the results of each of the held-out odd-numbered trials 10 times. We quantified the goodness-of-fit of the simulations by comparing three outcome metrics: choice, RT, and corrected probability of a leftward choice as a function of the cumulative gaze time advantage for the left item (as in <xref ref-type="fig" rid="fig4">Figure 4D and E</xref>). Note that since value is removed from the corrected cumulative gaze effect metric (see above), the only input to the model that could modulate it was the relative amount of gaze time devoted to each target. Thus, we can use this metric to assess the model’s ability to emulate the net effect of gaze alone on choice. For each metric we computed a mixed effects regression (logistic for choice, linear for RT and the corrected cumulative gaze effect), in which a vector including both the empirically observed data and the simulations was explained as a function of a binary variable indicating whether a data point came from the empirically observed data, or from the model simulations (i.e. <italic>Metric ~1 + isSimulated</italic>). If the fixed effect of this variable was not significantly different from 0, then the simulated data did not deviate from the empirically observed data. The mean deviations for each metric are given by the beta-weights and are reported in <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref> along with the confidence intervals. As with the other regression models used in this paper, ‘Monkey’ was included as both a random slope and intercept.</p><p>All GLAM parameter estimation procedures were identical to those set forth in <xref ref-type="bibr" rid="bib62">Thomas et al., 2019</xref>, with one exception: Thomas and colleagues assumed a fixed rate of random choice of 5%. This was changed to 1% to reflect the extremely low lapse rate for the easiest trials shown by both monkeys. The lapse rate for each monkey was estimated by finding the percentage of trials in which the monkey chose a 1-drop target when a 5-drop target (the highest possible value) was also available (mean lapse rate: Monkey C=0.64%, Monkey K=0.33%).</p></sec><sec id="s4-7"><title>Estimation and truncation of terminal non-decision time (NDT)</title><p>In sequential sampling models, the interval between when a decision is reached and when it is detectable to the experimenter is referred to as non-decision time (NDT). To account for and minimize the influence of gaze behavior in this interval, we truncated the gaze data using an estimate of this post-decision epoch. To obtain this estimate, we leveraged the key assumption of the two-stage aSSM, namely that gaze acts in a multiplicative manner prior to a boundary crossing (i.e. reaching a latent decision) and then additively between the boundary crossing and when the decision is reported (i.e. the RT, <xref ref-type="bibr" rid="bib32">Manohar and Husain, 2013</xref>; <xref ref-type="bibr" rid="bib66">Westbrook et al., 2020</xref>) Therefore, by removing gaze data from the end of the trial, the probability of the data being better explained by an additive model (and not a multiplicative model) should decrease as more data are removed. We collapsed data across the two monkeys and fit both an additive and a multiplicative variant of the GLAM for data truncated at 0ms, 50ms, 100ms, 150ms, 200ms, 300ms, and 400ms. We then compared the posterior probabilities of the model being correct given the data and the alternative model using the <italic>compare</italic> function from PyMC. We also compared the WAIC for the two variants, for each level of truncation (<xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>). The relative probability of the additive model generating the data decreases sharply between 0ms and 100ms of truncation, after which it reaches zero.</p><p>Accordingly, we selected 100ms as our NDT estimate, and for the data shown in the main figures we removed gaze data between center lever lift and 100ms before the lever lift in every trial. In other words, in every trial the gaze data were treated as if the trial had actually ended 100ms before the center lever lift; all calculations related to the gaze sequence and timing were calculated with respect to this virtual trial stopping point. For trials where the virtual stopping point occurred during an earlier fixation, we reassigned the final fixation location according to the location of the eyes at the virtual stopping point. In this way, the effects of late-trial gaze effects during the NDT were minimized. The truncated data were used to compute the gaze biases, except the initial fixation bias, and were also used as input to the computational model. Importantly, the main analyses were repeated using truncation values ranging from 0 to 400ms, with results shown in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>.</p></sec><sec id="s4-8"><title>Sample size determination</title><p>Because the behavioral task was novel, and because there were no published data from free-viewing NHP decision tasks at the time the study began, there were no effect size estimates available to facilitate a power analysis. We therefore began by collecting a pilot sample, and assessing the initial gaze bias and cumulative gaze bias effects as reported in humans by <xref ref-type="bibr" rid="bib22">Krajbich et al., 2010</xref>. To minimize confirmatory bias stemming from exploratory analyses, these initial analyses procedures were identical to those in Krajbich et al. The first pilot sample, in Monkey K, consisted of 13 sessions (approximately 8000 trials). Using planned analyses from Krajbich et al., we identified statistically significant initial fixation and corrected cumulative gaze biases. Based upon this initial sample, we then targeted a per-monkey sample size of approximately 10–15 sessions or 7000–15,000 trials for detecting behavioral effects of interest in subsequent experiments.</p><p>We then collected additional sessions from Monkey K for the purposes of electrophysiological recordings. These were combined with previous behavior-only sessions to create the Monkey K data used in this study (total = 25 sessions). From the second monkey we first collected 13 behavior-only sessions, and then additional 16 sessions for the purpose of electrophysiological recordings, resulting in the 29 sessions for Monkey C reported here.</p><p>We defined outlier sessions as those in which the fraction of suboptimal decisions exceeded ~1% for the easiest choices (+/-3 or 4 on the X-axis in <xref ref-type="fig" rid="fig1">Figure 1D</xref>), indicating that the monkeys had not yet sufficiently learned the target values; these sessions were excluded from analysis. No exclusions were made on the basis of gaze data, or upon gaze-related outcome measures.</p></sec><sec id="s4-9"><title>Code Accessibility</title><p>The data and code in this folder are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/hkgmn/">https://osf.io/hkgmn/</ext-link>. The <italic>glambox</italic> toolbox that was used for the computational modeling can be accessed at <ext-link ext-link-type="uri" xlink:href="https://github.com/glamlab/glambox">https://github.com/glamlab/glambox</ext-link> (<xref ref-type="bibr" rid="bib35">Molter et al., 2019</xref>). The associated documentation can be accessed at <ext-link ext-link-type="uri" xlink:href="https://glambox.readthedocs.io/en/latest/">https://glambox.readthedocs.io/en/latest/</ext-link>.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All procedures were in accordance with the Guide for the Care and Use of Laboratory Animals (2011)and were approved by the Institutional Animal Care and Use Committees of both Stanford University (APLAC Protocol #9720) and Rutgers University-Newark (PROTO999900861). Surgeries to implant orthopedic head restraints were conducted using full surgical anesthesia using aseptic techniques and instruments, and with analgesics and antibiotics given pre-, intra-, and post-operatively as appropriate.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Mixed effects model specifications.</title><p>All models included a monkey-specific slope and intercept for each effect. In Wilkinson notation, this is indicated by including (<italic>1+effect(s) | monkey</italic>). In line 14, the colon (:) indicates an interaction term.</p></caption><media xlink:href="elife-78205-supp1-v2.xlsx" mimetype="application" mime-subtype="xlsx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Parameter estimates from both GLAM variants.</title><p>Maximum a posteriori (MAP) and 97.5 highest posterior density (HPD) interval for each parameter obtained from fitting each variant to all data available for each monkey. Note that for “Bias Model”, <bold>γ</bold> was a free parameter in the model, while in the “No Bias Model”, <bold>γ</bold> was fixed at 1. <bold>γ</bold>, gaze bias parameter; <bold>σ</bold>, noise parameter; <bold>τ,</bold> logistic scaling parameter; <bold>ν</bold>, velocity parameter. <bold>WAIC,</bold> widely applicable information criteria. See Methods for further details on parameter definitions.</p></caption><media xlink:href="elife-78205-supp2-v2.xlsx" mimetype="application" mime-subtype="xlsx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Effect of NDT truncation on relative fit of the GLAM with either an additive or a multiplicative gaze term.</title><p>Table entries give the WAIC with standard errors in parentheses * indicates models where the standard errors for the two models do not overlap, indicating the fits are statistically different from each other.</p></caption><media xlink:href="elife-78205-supp3-v2.xlsx" mimetype="application" mime-subtype="xlsx"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>Comparison of GLAM simulations to held out trials.</title><p>Estimates and confidence intervals are the result of mixed effects models where each metric was regressed against a binary variable indicating whether the data were from the predicted trials or the observed trials. Confidence intervals including zero indicate no significant difference between predicted vs. observed, i.e. that the predicted data are a good fit to out of sample observations. The choice metric was assessed using a logistic regression. The others were linear. All models included “Monkey” as a random effect. p-values were obtained from an ANOVA on the output of the mixed effects regressions.</p></caption><media xlink:href="elife-78205-supp4-v2.xlsx" mimetype="application" mime-subtype="xlsx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-78205-transrepform1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data and code used for the analyses and figures included in the present manuscript have been uploaded as an Open Science Framework project (and a linked GitHub account). These files can be accessed at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/hkgmn/">https://osf.io/hkgmn/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Lupkin</surname><given-names>SM</given-names></name><name><surname>McGinty</surname><given-names>VB</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>NHP-Gaze-Bias</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/HKGMN</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>AW</given-names></name><name><surname>Molter</surname><given-names>F</given-names></name><name><surname>Krajbich</surname><given-names>I</given-names></name><name><surname>Heekeren</surname><given-names>HRH</given-names></name><name><surname>Mohr</surname><given-names>PNC</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Gaze Bias Differences Capture Individual Choice Behavior</data-title><source>GitHub</source><pub-id pub-id-type="accession" xlink:href="https://github.com/glamlab/gaze-bias-differences">gaze-bias-differences</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>WT Newsome for funding, material support, and thoughtful discussions; A Rangel, F Molter, G Rosenbaum, G Karpov, E Murray, D Sharma, &amp; A Thomas for thoughtful discussions and comments on the manuscript; J Brown, E Carson, S Fong, A McCormick, M Ortiz, J Powell, J Sanders, &amp; D Siegel for technical assistance. R Kiani for creation of scripts for Psychtoolbox used to run the behavioral task. F Molter and A Thomas for creation of the <italic>glambox</italic> toolbox. Funding: SML was supported by the Behavioral and Neural Sciences Graduate Program, the Rutgers University Academic Advancement Fund, and a Dean’s Dissertation Fellowship. VBM was supported by the Howard Hughes Medical Institute (WT Newsome), National Institutes of Health Grant K01-DA-036659-01 (VBM), the Busch Biomedical Foundation (VBM), and the Whitehall Foundation (VBM).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>BA</given-names></name><name><surname>Laurent</surname><given-names>PA</given-names></name><name><surname>Yantis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Value-driven attentional capture</article-title><source>PNAS</source><volume>108</volume><fpage>10367</fpage><lpage>10371</lpage><pub-id pub-id-type="doi">10.1073/pnas.1104047108</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balewski</surname><given-names>ZZ</given-names></name><name><surname>Knudsen</surname><given-names>EB</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Fast and slow contributions to decision-making in corticostriatal circuits</article-title><source>Neuron</source><volume>110</volume><fpage>2170</fpage><lpage>2182</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.04.005</pub-id><pub-id pub-id-type="pmid">35525242</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Battaglia-Mayer</surname><given-names>A</given-names></name><name><surname>Ferraina</surname><given-names>S</given-names></name><name><surname>Genovesio</surname><given-names>A</given-names></name><name><surname>Marconi</surname><given-names>B</given-names></name><name><surname>Squatrito</surname><given-names>S</given-names></name><name><surname>Molinari</surname><given-names>M</given-names></name><name><surname>Lacquaniti</surname><given-names>F</given-names></name><name><surname>Caminiti</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Eye–Hand Coordination during Reaching</article-title><source>Cerebral Cortex</source><volume>11</volume><fpage>528</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1093/cercor/11.6.528</pub-id><pub-id pub-id-type="pmid">11375914</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Psychophysics Toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carmichael</surname><given-names>ST</given-names></name><name><surname>Price</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Sensory and premotor connections of the orbital and medial prefrontal cortex of macaque monkeys</article-title><source>The Journal of Comparative Neurology</source><volume>363</volume><fpage>642</fpage><lpage>664</lpage><pub-id pub-id-type="doi">10.1002/cne.903630409</pub-id><pub-id pub-id-type="pmid">8847422</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname><given-names>SE</given-names></name><name><surname>Malalasekera</surname><given-names>WMN</given-names></name><name><surname>Miranda</surname><given-names>B</given-names></name><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Kennerley</surname><given-names>SW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Visual fixation patterns during economic choice reflect covert valuation processes that emerge with learning</article-title><source>PNAS</source><volume>116</volume><fpage>22795</fpage><lpage>22801</lpage><pub-id pub-id-type="doi">10.1073/pnas.1906662116</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname><given-names>C</given-names></name><name><surname>Peixoto</surname><given-names>D</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Laminar differences in decision-related neural activity in dorsal premotor cortex</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>614</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-00715-0</pub-id><pub-id pub-id-type="pmid">28931803</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cornelissen</surname><given-names>FW</given-names></name><name><surname>Peters</surname><given-names>EM</given-names></name><name><surname>Palmer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The Eyelink Toolbox: eye tracking with MATLAB and the Psychophysics Toolbox</article-title><source>Behavior Research Methods, Instruments, &amp; Computers</source><volume>34</volume><fpage>613</fpage><lpage>617</lpage><pub-id pub-id-type="doi">10.3758/bf03195489</pub-id><pub-id pub-id-type="pmid">12564564</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crowder</surname><given-names>EA</given-names></name><name><surname>Olson</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Macaque monkeys experience visual crowding</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/15.5.14</pub-id><pub-id pub-id-type="pmid">26067532</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dean</surname><given-names>HL</given-names></name><name><surname>Martí</surname><given-names>D</given-names></name><name><surname>Tsui</surname><given-names>E</given-names></name><name><surname>Rinzel</surname><given-names>J</given-names></name><name><surname>Pesaran</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reaction time correlations during eye-hand coordination: behavior and modeling</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>2399</fpage><lpage>2412</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4591-10.2011</pub-id><pub-id pub-id-type="pmid">21325507</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Form representation in monkey inferotemporal cortex is virtually unaltered by free viewing</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>814</fpage><lpage>821</lpage><pub-id pub-id-type="doi">10.1038/77722</pub-id><pub-id pub-id-type="pmid">10903575</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fetsch</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The importance of task design and behavioral control for understanding the neural basis of cognitive functions</article-title><source>Current Opinion in Neurobiology</source><volume>37</volume><fpage>16</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.12.002</pub-id><pub-id pub-id-type="pmid">26774692</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Folke</surname><given-names>T</given-names></name><name><surname>Jacobsen</surname><given-names>C</given-names></name><name><surname>Fleming</surname><given-names>SM</given-names></name><name><surname>De Martino</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Explicit representation of confidence informs future value-based decisions</article-title><source>Nature Human Behaviour</source><volume>1</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.1038/s41562-016-0002</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gluth</surname><given-names>S</given-names></name><name><surname>Rieskamp</surname><given-names>J</given-names></name><name><surname>Büchel</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Deciding when to decide: time-variant sequential sampling models explain the emergence of value-based decisions in the human brain</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>10686</fpage><lpage>10698</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0727-12.2012</pub-id><pub-id pub-id-type="pmid">22855817</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gwinn</surname><given-names>R</given-names></name><name><surname>Leber</surname><given-names>AB</given-names></name><name><surname>Krajbich</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The spillover effects of attentional learning on value-based choice</article-title><source>Cognition</source><volume>182</volume><fpage>294</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2018.10.012</pub-id><pub-id pub-id-type="pmid">30391643</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hare</surname><given-names>TA</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Camerer</surname><given-names>CF</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name><name><surname>Rangel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Transformation of stimulus value signals into motor commands during simple choice</article-title><source>PNAS</source><volume>108</volume><fpage>18120</fpage><lpage>18125</lpage><pub-id pub-id-type="doi">10.1073/pnas.1109322108</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Malalasekera</surname><given-names>WMN</given-names></name><name><surname>de Berker</surname><given-names>AO</given-names></name><name><surname>Miranda</surname><given-names>B</given-names></name><name><surname>Farmer</surname><given-names>SF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Kennerley</surname><given-names>SW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Triple dissociation of attention and decision computations across prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1471</fpage><lpage>1481</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0239-5</pub-id><pub-id pub-id-type="pmid">30258238</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kable</surname><given-names>JW</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The neurobiology of decision: consensus and controversy</article-title><source>Neuron</source><volume>63</volume><fpage>733</fpage><lpage>745</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.09.003</pub-id><pub-id pub-id-type="pmid">19778504</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katz</surname><given-names>LN</given-names></name><name><surname>Yates</surname><given-names>JL</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dissociated functional significance of decision-related activity in the primate dorsal stream</article-title><source>Nature</source><volume>535</volume><fpage>285</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1038/nature18617</pub-id><pub-id pub-id-type="pmid">27376476</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>HF</given-names></name><name><surname>Ghazizadeh</surname><given-names>A</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dopamine neurons encoding long-term memory of object value for habitual behavior</article-title><source>Cell</source><volume>163</volume><fpage>1165</fpage><lpage>1175</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.10.063</pub-id><pub-id pub-id-type="pmid">26590420</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krajbich</surname><given-names>I</given-names></name><name><surname>Armel</surname><given-names>C</given-names></name><name><surname>Rangel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Visual fixations and the computation and comparison of value in simple choice</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1292</fpage><lpage>1298</lpage><pub-id pub-id-type="doi">10.1038/nn.2635</pub-id><pub-id pub-id-type="pmid">20835253</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krajbich</surname><given-names>I</given-names></name><name><surname>Rangel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multialternative drift-diffusion model predicts the relationship between visual fixations and choice in value-based decisions</article-title><source>PNAS</source><volume>108</volume><fpage>13852</fpage><lpage>13857</lpage><pub-id pub-id-type="doi">10.1073/pnas.1101328108</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krajbich</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Accounting for attention in sequential sampling models of decision making</article-title><source>Current Opinion in Psychology</source><volume>29</volume><fpage>6</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.copsyc.2018.10.008</pub-id><pub-id pub-id-type="pmid">30368108</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krajbich</surname><given-names>I</given-names></name><name><surname>Mitsumasu</surname><given-names>A</given-names></name><name><surname>Polania</surname><given-names>R</given-names></name><name><surname>Ruff</surname><given-names>CC</given-names></name><name><surname>Fehr</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A causal role for the right frontal eye fields in value comparison</article-title><source>eLife</source><volume>10</volume><elocation-id>e67477</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67477</pub-id><pub-id pub-id-type="pmid">34779767</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Gomez-Marin</surname><given-names>A</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuroscience needs behavior: correcting a reductionist bias</article-title><source>Neuron</source><volume>93</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.041</pub-id><pub-id pub-id-type="pmid">28182904</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Saleem</surname><given-names>KS</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The ventral visual pathway: an expanded neural framework for the processing of object quality</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>26</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.10.011</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leong</surname><given-names>YC</given-names></name><name><surname>Radulescu</surname><given-names>A</given-names></name><name><surname>Daniel</surname><given-names>R</given-names></name><name><surname>DeWoskin</surname><given-names>V</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamic interaction between reinforcement learning and attention in multidimensional environments</article-title><source>Neuron</source><volume>93</volume><fpage>451</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.040</pub-id><pub-id pub-id-type="pmid">28103483</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>S-L</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name><name><surname>Rangel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The decision value computations in the vmPFC and striatum use a relative value code that is guided by visual attention</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>13214</fpage><lpage>13223</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1246-11.2011</pub-id><pub-id pub-id-type="pmid">21917804</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>HZ</given-names></name><name><surname>Zhou</surname><given-names>YB</given-names></name><name><surname>Wei</surname><given-names>ZH</given-names></name><name><surname>Jiang</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The power of last fixation: Biasing simple choices by gaze-contingent manipulation</article-title><source>Acta Psychologica</source><volume>208</volume><elocation-id>103106</elocation-id><pub-id pub-id-type="doi">10.1016/j.actpsy.2020.103106</pub-id><pub-id pub-id-type="pmid">32512321</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>HZ</given-names></name><name><surname>Lyu</surname><given-names>XK</given-names></name><name><surname>Wei</surname><given-names>ZH</given-names></name><name><surname>Mo</surname><given-names>WL</given-names></name><name><surname>Luo</surname><given-names>JR</given-names></name><name><surname>Su</surname><given-names>XY</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Exploiting the dynamics of eye gaze to bias intertemporal choice</article-title><source>Journal of Behavioral Decision Making</source><volume>34</volume><fpage>419</fpage><lpage>431</lpage><pub-id pub-id-type="doi">10.1002/bdm.2219</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manohar</surname><given-names>SG</given-names></name><name><surname>Husain</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Attention as foraging for information and value</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><elocation-id>711</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00711</pub-id><pub-id pub-id-type="pmid">24204335</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGinty</surname><given-names>VB</given-names></name><name><surname>Rangel</surname><given-names>A</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Orbitofrontal cortex value signals depend on fixation location during free viewing</article-title><source>Neuron</source><volume>90</volume><fpage>1299</fpage><lpage>1311</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.04.045</pub-id><pub-id pub-id-type="pmid">27263972</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>McGinty</surname><given-names>VB</given-names></name><name><surname>Lupkin</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Value Signals in Orbitofrontal Cortex Predict Economic Decisions on a Trial-to-Trial Basis</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.03.11.434452</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molter</surname><given-names>F</given-names></name><name><surname>Thomas</surname><given-names>AW</given-names></name><name><surname>Heekeren</surname><given-names>HR</given-names></name><name><surname>Mohr</surname><given-names>PNC</given-names></name><name><surname>Smith</surname><given-names>DV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>GLAMbox: A Python toolbox for investigating the association between gaze allocation and decision behaviour</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0226428</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0226428</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>T</given-names></name><name><surname>Armstrong</surname><given-names>KM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Selective gating of visual signals by microstimulation of frontal cortex</article-title><source>Nature</source><volume>421</volume><fpage>370</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1038/nature01341</pub-id><pub-id pub-id-type="pmid">12540901</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>T</given-names></name><name><surname>Fallah</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Microstimulation of the frontal eye field and its effects on covert spatial attention</article-title><source>Journal of Neurophysiology</source><volume>91</volume><fpage>152</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1152/jn.00741.2002</pub-id><pub-id pub-id-type="pmid">13679398</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moran</surname><given-names>J</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Selective attention gates visual processing in the extrastriate cortex</article-title><source>Science</source><volume>229</volume><fpage>782</fpage><lpage>784</lpage><pub-id pub-id-type="doi">10.1126/science.4023713</pub-id><pub-id pub-id-type="pmid">4023713</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><collab>National Research Council</collab></person-group><year iso-8601-date="2011">2011</year><source>Guide for the Care and Use of Laboratory Animals</source><publisher-name>National Academies Press</publisher-name></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neggers</surname><given-names>SF</given-names></name><name><surname>Bekkering</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Ocular gaze is anchored to the target of an ongoing pointing movement</article-title><source>Journal of Neurophysiology</source><volume>83</volume><fpage>639</fpage><lpage>651</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.83.2.639</pub-id><pub-id pub-id-type="pmid">10669480</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neggers</surname><given-names>SFW</given-names></name><name><surname>Bekkering</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Gaze anchoring to a pointing target is present during the entire pointing movement and is driven by a non-visual signal</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>961</fpage><lpage>970</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.2.961</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ongür</surname><given-names>D</given-names></name><name><surname>Price</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The organization of networks within the orbital and medial prefrontal cortex of rats, monkeys and humans</article-title><source>Cerebral Cortex</source><volume>10</volume><fpage>206</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1093/cercor/10.3.206</pub-id><pub-id pub-id-type="pmid">10731217</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name><name><surname>Assad</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neurons in the orbitofrontal cortex encode economic value</article-title><source>Nature</source><volume>441</volume><fpage>223</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1038/nature04676</pub-id><pub-id pub-id-type="pmid">16633341</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pärnamets</surname><given-names>P</given-names></name><name><surname>Johansson</surname><given-names>P</given-names></name><name><surname>Hall</surname><given-names>L</given-names></name><name><surname>Balkenius</surname><given-names>C</given-names></name><name><surname>Spivey</surname><given-names>MJ</given-names></name><name><surname>Richardson</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Biasing moral decisions by exploiting the dynamics of eye gaze</article-title><source>PNAS</source><volume>112</volume><fpage>4170</fpage><lpage>4175</lpage><pub-id pub-id-type="doi">10.1073/pnas.1415250112</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paton</surname><given-names>JJ</given-names></name><name><surname>Belova</surname><given-names>MA</given-names></name><name><surname>Morrison</surname><given-names>SE</given-names></name><name><surname>Salzman</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The primate amygdala represents the positive and negative value of visual stimuli during learning</article-title><source>Nature</source><volume>439</volume><fpage>865</fpage><lpage>870</lpage><pub-id pub-id-type="doi">10.1038/nature04490</pub-id><pub-id pub-id-type="pmid">16482160</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pisauro</surname><given-names>MA</given-names></name><name><surname>Fouragnan</surname><given-names>E</given-names></name><name><surname>Retzler</surname><given-names>C</given-names></name><name><surname>Philiastides</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural correlates of evidence accumulation during value-based decisions revealed via simultaneous EEG-fMRI</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15808</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15808</pub-id><pub-id pub-id-type="pmid">28598432</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>McKoon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The diffusion decision model: theory and data for two-choice decision tasks</article-title><source>Neural Computation</source><volume>20</volume><fpage>873</fpage><lpage>922</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.12-06-420</pub-id><pub-id pub-id-type="pmid">18085991</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Resulaj</surname><given-names>A</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Changes of mind in decision-making</article-title><source>Nature</source><volume>461</volume><fpage>263</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1038/nature08275</pub-id><pub-id pub-id-type="pmid">19693010</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reyes-Puerta</surname><given-names>V</given-names></name><name><surname>Philipp</surname><given-names>R</given-names></name><name><surname>Lindner</surname><given-names>W</given-names></name><name><surname>Hoffmann</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Role of the rostral superior colliculus in gaze anchoring during reach movements</article-title><source>Journal of Neurophysiology</source><volume>103</volume><fpage>3153</fpage><lpage>3166</lpage><pub-id pub-id-type="doi">10.1152/jn.00989.2009</pub-id><pub-id pub-id-type="pmid">20357074</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rich</surname><given-names>EL</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Decoding subjective decisions from orbitofrontal cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>973</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1038/nn.4320</pub-id><pub-id pub-id-type="pmid">27273768</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richmond</surname><given-names>BJ</given-names></name><name><surname>Wurtz</surname><given-names>RH</given-names></name><name><surname>Sato</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Visual responses of inferior temporal neurons in awake rhesus monkey</article-title><source>Journal of Neurophysiology</source><volume>50</volume><fpage>1415</fpage><lpage>1432</lpage><pub-id pub-id-type="doi">10.1152/jn.1983.50.6.1415</pub-id><pub-id pub-id-type="pmid">6663335</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodriguez</surname><given-names>CA</given-names></name><name><surname>Turner</surname><given-names>BM</given-names></name><name><surname>Van Zandt</surname><given-names>T</given-names></name><name><surname>McClure</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The neural basis of value accumulation in intertemporal choice</article-title><source>The European Journal of Neuroscience</source><volume>42</volume><fpage>2179</fpage><lpage>2189</lpage><pub-id pub-id-type="doi">10.1111/ejn.12997</pub-id><pub-id pub-id-type="pmid">26179826</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruff</surname><given-names>CC</given-names></name><name><surname>Blankenburg</surname><given-names>F</given-names></name><name><surname>Bjoertomt</surname><given-names>O</given-names></name><name><surname>Bestmann</surname><given-names>S</given-names></name><name><surname>Freeman</surname><given-names>E</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name><name><surname>Josephs</surname><given-names>O</given-names></name><name><surname>Deichmann</surname><given-names>R</given-names></name><name><surname>Driver</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Concurrent TMS-fMRI and psychophysics reveal frontal influences on human retinotopic visual cortex</article-title><source>Current Biology</source><volume>16</volume><fpage>1479</fpage><lpage>1488</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.06.057</pub-id><pub-id pub-id-type="pmid">16890523</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheinberg</surname><given-names>DL</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Noticing familiar objects in real world scenes: the role of temporal cortical neurons in natural vision</article-title><source>The Journal of Neuroscience</source><volume>21</volume><fpage>1340</fpage><lpage>1350</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.21-04-01340.2001</pub-id><pub-id pub-id-type="pmid">11160405</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silvanto</surname><given-names>J</given-names></name><name><surname>Lavie</surname><given-names>N</given-names></name><name><surname>Walsh</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Stimulation of the human frontal eye fields modulates sensitivity of extrastriate visual cortex</article-title><source>Journal of Neurophysiology</source><volume>96</volume><fpage>941</fpage><lpage>945</lpage><pub-id pub-id-type="doi">10.1152/jn.00015.2006</pub-id><pub-id pub-id-type="pmid">16624999</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Krajbich</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Attention and choice across domains</article-title><source>Journal of Experimental Psychology. General</source><volume>147</volume><fpage>1810</fpage><lpage>1826</lpage><pub-id pub-id-type="doi">10.1037/xge0000482</pub-id><pub-id pub-id-type="pmid">30247061</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Krajbich</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Gaze amplifies value in decision making</article-title><source>Psychological Science</source><volume>30</volume><fpage>116</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1177/0956797618810521</pub-id><pub-id pub-id-type="pmid">30526339</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stuphorn</surname><given-names>V</given-names></name><name><surname>Bauswein</surname><given-names>E</given-names></name><name><surname>Hoffmann</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Neurons in the primate superior colliculus coding for arm movements in gaze-related coordinates</article-title><source>Journal of Neurophysiology</source><volume>83</volume><fpage>1283</fpage><lpage>1299</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.83.3.1283</pub-id><pub-id pub-id-type="pmid">10712456</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sui</surname><given-names>XY</given-names></name><name><surname>Liu</surname><given-names>HZ</given-names></name><name><surname>Rao</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The timing of gaze-contingent decision prompts influences risky choice</article-title><source>Cognition</source><volume>195</volume><elocation-id>104077</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2019.104077</pub-id><pub-id pub-id-type="pmid">31770670</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavares</surname><given-names>G</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Rangel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The attentional drift diffusion model of simple perceptual decision-making</article-title><source>Frontiers in Neuroscience</source><volume>11</volume><elocation-id>468</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2017.00468</pub-id><pub-id pub-id-type="pmid">28894413</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>PCJ</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>FEF TMS affects visual cortical activity</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>391</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhj156</pub-id><pub-id pub-id-type="pmid">16525126</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>AW</given-names></name><name><surname>Molter</surname><given-names>F</given-names></name><name><surname>Krajbich</surname><given-names>I</given-names></name><name><surname>Heekeren</surname><given-names>HR</given-names></name><name><surname>Mohr</surname><given-names>PNC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Gaze bias differences capture individual choice behaviour</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>625</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0584-8</pub-id><pub-id pub-id-type="pmid">30988476</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaidya</surname><given-names>AR</given-names></name><name><surname>Fellows</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Ventromedial frontal cortex is critical for guiding attention to reward-predictive visual features in humans</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>12813</fpage><lpage>12823</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1607-15.2015</pub-id><pub-id pub-id-type="pmid">26377468</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Gabry</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC</article-title><source>Statistics and Computing</source><volume>27</volume><fpage>1413</fpage><lpage>1432</lpage><pub-id pub-id-type="doi">10.1007/s11222-016-9696-4</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waskom</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>seaborn: statistical data visualization</article-title><source>Journal of Open Source Software</source><volume>6</volume><elocation-id>3021</elocation-id><pub-id pub-id-type="doi">10.21105/joss.03021</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Westbrook</surname><given-names>A</given-names></name><name><surname>van den Bosch</surname><given-names>R</given-names></name><name><surname>Määttä</surname><given-names>JI</given-names></name><name><surname>Hofmans</surname><given-names>L</given-names></name><name><surname>Papadopetraki</surname><given-names>D</given-names></name><name><surname>Cools</surname><given-names>R</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dopamine promotes cognitive effort by biasing the benefits versus costs of cognitive work</article-title><source>Science</source><volume>367</volume><fpage>1362</fpage><lpage>1366</lpage><pub-id pub-id-type="doi">10.1126/science.aaz5891</pub-id><pub-id pub-id-type="pmid">32193325</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitney</surname><given-names>D</given-names></name><name><surname>Levi</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual crowding: A fundamental limit on conscious perception and object recognition</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>160</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.02.005</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wise</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Forward frontal fields: phylogeny and fundamental function</article-title><source>Trends in Neurosciences</source><volume>31</volume><fpage>599</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2008.08.008</pub-id><pub-id pub-id-type="pmid">18835649</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Y</given-names></name><name><surname>Nie</surname><given-names>C</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Covert shift of attention modulates the value encoding in the orbitofrontal cortex</article-title><source>eLife</source><volume>7</volume><elocation-id>e31507</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31507</pub-id><pub-id pub-id-type="pmid">29533184</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78205.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rich</surname><given-names>Erin L</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04a9tmd77</institution-id><institution>Icahn School of Medicine at Mount Sinai</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.02.24.481847" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.02.24.481847"/></front-stub><body><p>This study analyzed viewing behavior in monkeys during value-based decision-making to determine whether relationships between gaze patterns and choices previously described in humans are also present in monkeys. The study used a clever task design and sophisticated modeling approaches to reveal robust evidence for similarities to extant human data. This is important to the field because it suggests common neural mechanisms linking viewing behavior and decision-making, which can now be further explored across species.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78205.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rich</surname><given-names>Erin L</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04a9tmd77</institution-id><institution>Icahn School of Medicine at Mount Sinai</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.02.24.481847">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.02.24.481847v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Monkeys exhibit human-like gaze biases in economic decisions&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Michael Frank as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>Below are three specific points, distilled from the attached reviews, considered essential revisions for the authors to address. The individual reviews appended below can be consulted for further elaboration on these points and additional suggestions to improve the manuscript.</p><p>1. A general concern is that the value associated with an item can potentially attract gaze independent of choice, and reviewers felt that these relationships should be considered in more detail (addressed primarily by R1 and R3). On one hand, given a limited set of values, value difference is highly correlated with both the options and the chosen value, and it is suggested that the authors attempt to disentangle these in their results. Similarly, effects on fixation duration should be explored in more detail, including whether they relate to only one of the option values or both, and how they vary when there are 2 versus 3 fixations in a choice. Finally, it was felt that there should be additional discussion of the complex interplay in which a valuable item attracts attention and attention to an item may also increase its subjective value, and the extent to which these possibilities are disentangled or confounded in the current data. (Note that this final concern may be somewhat addressed by the analyses suggested in point 3 below).</p><p>2. A second concern is that the approach to &quot;non-decision time&quot; was viewed as arbitrary and perhaps not entirely valid (addressed primarily by R2 and R1). The current approach assumes that final and non-final fixations should be the same length, although other work has shown that final fixations are generally shorter. It also assumes that motor preparation time is equal across all chosen values, although higher values typically motivate faster motor responses. Point 3 below makes a suggestion about how to consider NDT in alternative choice models. Other potential approaches to address this concern within the present model include testing different estimates for NDTs and demonstrating the robustness of the overall results.</p><p>3. Finally, reviewers felt that the study would be strengthened by additional exploration of their data in light of studies that assess whether gaze might directly increase drift rate (i.e., gaze is additive to value), rather than (or in addition to) multiplicative effects, where gaze amplifies the attended value. (See Cavanagh et al. 2014 JEP General, Gluth et al. 2018 <italic>eLife</italic>, and Smith and Krajbich 2019 Psych Sci.)</p><p>Specifically, Westbrook et al. (Science, 2020) suggested a hybrid two stage model whereby gaze is multiplicative on value early during choice and additive later, with a rationale similar to that discussed here (&quot;post-decisional gaze anchoring&quot; – see also recent work by Callaway et al. 2021 PLoS CB for a related but more continuous model). The central assumption of GLAM is that gaze weights the impact of value on evidence accumulation, but given the issues regarding NDT, considering other models in one way or another is important, and the authors have the trial counts to do so quantitatively. Thus, the authors should address the issue of whether gaze effects in monkeys are multiplicative as assumed by GLAM, or whether they might be additive when applied to the data. There was extensive discussion on this topic in consultation, and indeed there is evidence for both in the human literature. Clearly, publication decisions will not depend on the conclusions of this exploration per se, but a strength of the current data set is that it may be able to speak to this unresolved issue.</p><p>One consideration in undertaking such an alternative model analysis is that it could be done with and without truncating NDT (so that NDT would be captured by the second phase of a 2-phase model) and/or by better justifying the choice of NDT. Moreover, it was felt that a model/parameter recovery analysis is a critical component of these explorations. In other words, if Model A &quot;wins&quot;, simulated data from model A should be able to reproduce key features of the data that Model B cannot capture, and vice versa and parameter estimates should be recoverable when fitting to those data (see Wilson and Collins 2019 <italic>eLife</italic>).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Critique 1: The authors should consider the possibility that unique option values can explain putative effects of value difference. One approach might be to subselect trials and analyze data sets where chosen value varies but value difference is constant and vice versa.</p><p>Critique 2: The concern about whether effect first fixated values carry over into behavior during the second fixation could be addressed similarly to critique 1 or with model comparison approaches.</p><p>Critique 3: The authors should consider whether a more flexible definition of NDTs could better fit behavior. This concern might also be addressed in approaches to critique 4.</p><p>Critique 4: The concern in this critique is that value and choice can have influence on gaze behavior (and drift rates) that is not accounted for by the present models. Addressing this would involve more extensive re-analysis than the previous points, however given that the contribution of this paper is showing that monkey behavior recapitulates what has been reported in humans, I think it is warranted to explore more complex interactions between gaze and choice that have been found in humans subjects. To address this concern, the authors should consider comparing the present results to models that allow the γ parameter to vary with specific task parameters (for instance as in Westbrook, 2020).</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>It seems a little silly to expend all this effort generating bootstrapped distributions of final and non-final fixation differences, only to then arbitrarily choose the 95th percentile of that distribution as the terminal non decision time. Why not instead either simply get the best estimate you can from the literature, and/or demonstrate the robustness of the results to different non-decision time corrections (e.g. 0, 200, 400 ms)?</p><p>Regarding the manipulation where onsets were staggered: the analysis here seemed convoluted. Why not directly test whether subjects were more likely to choose the initially presented option? Figure 3 helps to address this, but leaves some ambiguity for Monkey K, who doesn't always initially fixate on the first presented alternative.</p><p>It may be worth noting that the effects in Figure 5 appear to be smaller than in most human data. This might be an argument for 'not' cutting the last 200ms of each trial?</p><p>On page 28 the authors discuss Krajbich et al. 2021 and argue that their findings are consistent with LIP, frontal eye fields, and the superior colliculus accumulating evidence in studies where eyes are used to report decisions. However, that study was like this one; subjects were free to look around but they chose using the keyboard.</p><p>I thought the authors missed an opportunity to discuss some of the human neuroimaging work on value-based SSM, given the focus of their Discussion section. There are a number of well-known articles that are relevant, for example:</p><p>Hare et al. 2011 (PNAS); Gluth et al. 2012 (J Neuro); Rodriguez et al. 2015 (European J Neurosci); Pisauro et al. 2017 (Nature Comm).</p><p>Why did the authors use different delays for the two monkeys?</p><p>How long were the delays between releasing the center lever and pressing left or right?</p><p>Was there some criterion for passing the training stage?</p><p>p.33-34 – these numbers should go in the Results section, not the Methods.</p><p>I think it might make sense to include session-level random effects in the models, given that each session used a different set of stimuli. I doubt this will change much, but it might further help clean up some variance.</p><p>p.38 – This argument about how early gaze might be used to explore and evaluate targets while late gaze is used to focus on the to-be-chosen target comes nearly straight out of Westbrook et al. 2020 (Science).</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I would have liked to see a more extensive discussion of what might underly the observed effects. The sequential sampling models are merely descriptive. One would like to know how fixation influences the neural signals that compute the value of the option. When a human/monkey is looking at one option, information about a previously viewed option must be maintained in working memory and the neural signals might be expected to be noisier, leading to a bias towards the most recently viewed option. Similarly, the first viewed option might have a stronger signal than the second viewed item by virtue of having less interference. This is obviously speculative but it would be nice to see what evidence there is for something like this – indeed the McGinty et al. 2016 paper seems to provide evidence of this kind. One could argue that it shouldn't be too surprising that computations performed at the fixation point are higher S/N than ones off fixation. (In the context of the sequential sampling model this would correspond to adding some noise to the item not currently fixated.)</p><p>Related to this, the discussion of how the paradigm might be used for further exploration is a bit vague. The example given of MT and LIP is not particularly well chosen, given the recent evidence from the Huk lab (Katz et al., Nature 2016) against LIP as the site of the perceptual decision.</p><p>Another general concern is the absence of any discussion of all the work showing that value associated with an item serves to attract gaze and indeed is a central aspect of the mechanisms of gaze control and learning where to attend. One closely related example is the work of Hikosaka (eg Kim et al., Cell, 2015) showing that cells in the caudate tail coded the value of previously rewarded fractal patterns and attracted gaze. This makes the question of causality a bit difficult to disentangle. It's such a central issue that it needs to be explicitly discussed. Having the stimuli not resolvable in the peripheral retina helps with this issue.</p><p>I would also like to see more of the actual fixation time data. If there are only two fixations, having a bias towards the first one viewed is in conflict with a bias towards the last one viewed. Thus it would be nice to see the fixation durations broken down into cases where there are 2 fixations and cases where there are 3 fixations. Just as the calculation that the initial-view bias is equal to about half a drop of juice, it would also be useful to know how fixation duration translates to choice bias. If I am understanding Figure 4 correctly it looks like an extra 200 msec viewing time translates to about a 10% increase in choice probability. More concrete description of the fixations would be helpful. For example, how do the β values on p8-9 translate to fixation durations?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78205.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Reviewer #1 (Recommendations for the authors):</p><p>Critique 1: The authors should consider the possibility that unique option values can explain putative effects of value difference. One approach might be to subselect trials and analyze data sets where chosen value varies but value difference is constant and vice versa.</p></disp-quote><p>To address this question, we have adapted the methods of Balewski and colleagues (<italic>Neuron</italic>, 2022) to isolate the unique contributions of chosen value and trial difficulty to reaction time and the number of fixations in a given trial (the two behaviors modulated by difficulty in the original paper). This new analysis reveals a double dissociation in which reaction time decreases as a function of chosen value but not difficulty, while the number of fixations in a trial shows the opposite pattern. Our interpretation is that reaction time largely reflects reward anticipation, whereas the number of fixations largely reflects the amount of information required to render a decision (i.e., choice difficulty). See lines 144-167 and Figure 2.</p><disp-quote content-type="editor-comment"><p>Critique 2: The concern about whether effect first fixated values carry over into behavior during the second fixation could be addressed similarly to critique 1 or with model comparison approaches.</p></disp-quote><p>This is a valid interpretation of the results. To test this directly, we now include an analysis of middle fixation duration as a function of the not-currently viewed target. Note that the vast majority of middle fixations are the second fixation in the trial, and therefore the value of the unattended target is typically the one that was viewed first. The analysis showed a negative correlation between middle fixation duration and the value of the unattended target which is consistent with the first fixated value carrying over to the second fixation. See lines 243-246.</p><disp-quote content-type="editor-comment"><p>Critique 3: The authors should consider whether a more flexible definition of NDTs could better fit behavior. This concern might also be addressed in approaches to critique 4.</p></disp-quote><p>In all sequential sampling model formulations we are aware of, nondecision time is considered to be fixed across trial types. Examples can be found for perceptual decisions (e.g., Resulaj et al., 2009) and in the “bifurcation point” approach used in the recent value-based decision study by Westbrook et al. (2020).</p><p>To further investigate this issue, we asked whether other post-decision processes were sensitive to chosen value in our paradigm. To do so, we measured the interval between the center lever lift and the left or right lever press, corresponding to the time taken to perform the reach movement in each trial (reach latency). We then fit a mixed effects model explaining reach latency as a function of chosen value. While the results showed significantly faster reach latencies with higher chosen values, the effect size was very small, showing on average a ~3ms decrease per drop of juice. In other words, between the highest and lowest levels of chosen value (5 vs. 1), there is only a difference of approximately 12ms. In contrast, the main RT measure used in the study (the interval between target onset and center lever lift) is an order of magnitude more sensitive to chosen value, decreasing ~40ms per drop of juice. These results are shown in <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78205-sa2-fig1-v2.tif"/></fig><p>This suggests that post-decision processes (NDT in standard models and the additive stage in the Westbrook paper) vary only minimally as a function of chosen value. We are happy to include this analysis as a supplemental figure upon request.</p><disp-quote content-type="editor-comment"><p>Critique 4: The concern in this critique is that value and choice can have influence on gaze behavior (and drift rates) that is not accounted for by the present models. Addressing this would involve more extensive re-analysis than the previous points, however given that the contribution of this paper is showing that monkey behavior recapitulates what has been reported in humans, I think it is warranted to explore more complex interactions between gaze and choice that have been found in humans subjects. To address this concern, the authors should consider comparing the present results to models that allow the γ parameter to vary with specific task parameters (for instance as in Westbrook, 2020).</p></disp-quote><p>The two-stage model of gaze effects put forth by Westbrook et al. (2020) is consistent with other observations of gaze behavior and choice (i.e., Thomas et al., 2019, Smith et al., 2018, Manohar and Husain, 2013). In this model, gaze effects early in the trial are best described by a multiplicative relationship between gaze and value, whereas gaze effects later in the trial are best described with an additive model term. To test the two-stage hypothesis, Westbrook and colleagues determined a ‘bifurcation point’ for each subject that represented the time at which gaze effects transitioned from multiplicative to additive. In our data, trial durations were typically very short (&lt;1s), making it difficult to divide trials and fit separate models to them. We therefore took at different approach: We reasoned that if gaze effects transition from multiplicative to additive at the end of the trial, then the transition point could be estimated by removing data from the end of each trial and assessing the relative fit of a multiplicative vs. additive model. If the early gaze effects are predominantly multiplicative and late gaze effects are additive, the relative goodness of fit for an additive model should decrease as more data are removed from the end of the trial. To test this idea, we compared the relative model fit of an additive vs. multiplicative models in the raw data, and for data in which successively larger epochs were removed from the end of the trial (50, 100, 150, 200, 300, and 400ms). The relative fit was assessed by computing the relative probability that each model accurately reflects the data. In addition, to identify significant differences in goodness of fit, we compared the WAIC values and their standard errors for each model (Supplementary file 3). As shown in Figure 4, the relative fit probability for both models is nonzero in the raw data 0 truncation, indicating that neither model provides a definitive best fit, potentially reflecting a mixture of the two processes. However, the relative fit of the additive model decreases sharply as data is removed, reaching zero at 100ms truncation. 100ms is also the point at which multiplicative models provide a significantly better fit, indicated by non-overlapping standard error intervals for the two models (Supplementary file 3). Together, this suggested that the transition between early- and late-stage gaze effects likely occurs approximately 100ms before the RT.</p><p>To minimize the influence of post-decision gaze effects, the main results use data truncated by 100ms. However, because 100ms is only an estimate, we repeated the main analyses over truncation values between 0 and 400ms, reported in Figure 6 —figure supplement 1 and Figure 7 —figure supplement 1. These show significant gaze duration biases and final gaze biases in data truncated by up to 200ms.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>It seems a little silly to expend all this effort generating bootstrapped distributions of final and non-final fixation differences, only to then arbitrarily choose the 95th percentile of that distribution as the terminal non decision time. Why not instead either simply get the best estimate you can from the literature, and/or demonstrate the robustness of the results to different non-decision time corrections (e.g. 0, 200, 400 ms)?</p></disp-quote><p>Thanks for the opportunity to clarify these points. There are three related issues:</p><p>First, with regards to fixation durations, in the updated Figure 3 we now show durations as a function of both the absolute order in the trial (first, second, third, fourth, etc.) and the relative order (final/nonfinal). We find that durations decrease as a function of absolute order in the trial, an effect also seen in humans (see Manohar and Husain, 2013). At the same time, while holding absolute order constant, final fixations are longer than non-final fixations. To explain the discrepancy with human final fixation durations, we note that monkeys make many fewer fixations per trial (~2.5) than humans do (~3.7, computed from publicly available data from Krajbich et al., 2010.) This means that compared to humans, monkeys’ final fixations occur earlier in the trial (e.g., second or third), and are therefore comparatively longer in duration. Note that studies with humans have not independently measured fixation durations by absolute and relative order, and therefore would not have detected the potential interaction between the two effects.</p><p>Second, the comment suggests that the final 200ms before lever lift is not spent planning the left/right movement, given that the monkeys have time <italic>after</italic> the lever lift in which to execute the movement (400 or 500ms, depending on the monkey). The presumption appears to be that 400/500ms should be sufficient to plan a left/right reach. However, we think that these two suggestions are unlikely, and that our original interpretation is the most plausible. First, the 400/500ms deadline between lift and left/right press was set to encourage the monkeys to complete the reach as fast as possible, to minimize deliberations or changes of mind after lifting the lever. More specifically, these deadlines were designed so that on ~0.5% of trials, the monkeys actually fail to complete the reach within the deadline and fail to obtain a reward. This manipulation was effective at motivating fast reaches, as the average reach latency (time between lift and press) was 165 SEM 20ms for Monkey K, and 290 SEM 100ms for Monkey C.</p><p>Therefore, given the time pressure imposed by the task, it is very unlikely that significant reach planning occurs after the lever lift. In addition to these empirical considerations, the idea that the final moments before the RT are used for motor planning is a standard assumption in many theoretical models of choice (including sequential sampling models, see Ratcliff and McKoon 2008, for review), and is also well-supported by studies of motor control and motor system neurophysiology.</p><p>Based on these, we think the assumption of some form of terminal NDT is warranted.</p><p>Third, we have changed our method for estimating the NDT interval. In brief we sweep through a range of NDT truncation values (0-400ms) and identify the smallest interval (100ms) that minimizes the contribution of “additive” gaze effects, which are thought to reflect late-stage, post-decision gaze processes. See the response to Point 4 for Reviewer 1, Figure 4 and lines 267-325 in the main text. In addition, we report all of the major study results over a range of truncation values between 0 and 400ms.</p><disp-quote content-type="editor-comment"><p>Regarding the manipulation where onsets were staggered: the analysis here seemed convoluted. Why not directly test whether subjects were more likely to choose the initially presented option? Figure 3 helps to address this, but leaves some ambiguity for Monkey K, who doesn't always initially fixate on the first presented alternative.</p></disp-quote><p>First, to eliminate ambiguity in the interpretation of the initial fixation bias for the manipulation sessions, we have removed the 99 trials (1.82%) in Monkey K’s data where he did not look at the suggested first target. We have changed the intext statistics and the corresponding figure, accordingly (now Figure 6). Removing these trials did not change any of the conclusions.</p><p>Second, we would like to take the opportunity to explain the rationale for the analysis of the gaze manipulation sessions. In the primary dataset (i.e., sessions without manipulation), we found that the monkeys were more likely to choose the first target that they looked at – evident by simply dividing the trials as the comment suggests.</p><p>The purpose of the manipulation sessions was to test the <italic>causal</italic> nature of the initial fixation bias by staggering the onset of the targets on a fraction of trials. Additionally, we also wanted to assess whether the size of the bias on the manipulation trials differed from the size of the bias in the non-manipulation trials collected in the same sessions.</p><p>To that end, for the data from the manipulation sessions we fit a single linear model that explains the fraction of left choices as a function of: (1) the value difference between the left and right targets; (2) the direction of initial fixation (left/right); (3) a Boolean variable indicating whether or not a given trial contained gaze manipulation; and (4) the interaction between the trial type and the location of the initial fixation. In Wilkinson notation, this can be written as:</p><disp-quote content-type="editor-comment"><p>P(choose-left) ~ value-difference + first-is-left + trial-type + (first-is-left:trial-type).</p></disp-quote><p>The first-is-left regressor indicates whether there was an overall effect of initial fixation location on the fraction of left choices (the main effect of interest). The trialtype regressor indicates whether the overall fraction of left choices differed between trials with or without gaze manipulation. And the interaction term indicates whether the effect of the initial fixation differed according to the trial-type – i.e., depended on whether the location of the first fixation was dictated to the monkey or if he was able to choose where to deploy his first fixation.</p><p>As detailed in lines 349-361, this analysis identified a significant main effect of firstis-left, and no significant effects for trial-type or the interaction, indicating that the bias due to initial gaze location was no different in trials with vs. without gaze manipulation. This analysis provides a parsimonious explanation of the effects of initial gaze during the manipulation sessions.</p><p>Note that, except for the trial-type regressor and the interaction term, this is the same model used to describe the initial fixation bias in the sessions without the any gaze manipulation, as well as the one used to assess this bias in humans (Krajbich et al., 2010).</p><disp-quote content-type="editor-comment"><p>It may be worth noting that the effects in Figure 5 appear to be smaller than in most human data. This might be an argument for 'not' cutting the last 200ms of each trial?</p></disp-quote><p>The reviewer’s intuition is correct that there is a relationship between the NDT truncation and the final fixation bias. As shown in Figure 7 —figure supplement 1, we find that the final fixation bias decreases as more time is truncated from the end of the trial.</p><p>The responses to Reviewer 1 (Points 3 and 4) and Reviewer 2 (Point 1), articulate the motivation for estimating terminal NDT and removing time from the end of each trial. In a nutshell, terminal non-decision time is standard assumption in many sequential sampling models, and is supported by evidence from motor control and motor system neurophysiology. In addition, our data appear consistent with a two stage process in which gaze becomes drawn to the to-be-chosen target within in the final 100ms of a trial Figure 4 and lines 267-325 Because our goal is to observe the early-stage gaze effects, we truncate the data to minimize the influence of late-stage gaze effects.</p><p>The revised manuscript uses an NDT estimate of 100ms for main figures (Figures 59) and analyses, and also reports main results for data truncated over a range from 0 to 400ms (Figure 6 —figure supplement 1 and Figure 7 —figure supplement 1.). The magnitude of gaze bias effects using 100ms truncation is similar to those reported in human studies.</p><disp-quote content-type="editor-comment"><p>On page 28 the authors discuss Krajbich et al. 2021 and argue that their findings are consistent with LIP, frontal eye fields, and the superior colliculus accumulating evidence in studies where eyes are used to report decisions. However, that study was like this one; subjects were free to look around but they chose using the keyboard.</p><p>I thought the authors missed an opportunity to discuss some of the human neuroimaging work on value-based SSM, given the focus of their Discussion section. There are a number of well-known articles that are relevant, for example:</p><p>Hare et al. 2011 (PNAS); Gluth et al. 2012 (J Neuro); Rodriguez et al. 2015 (European J Neurosci); Pisauro et al. 2017 (Nature Comm).</p></disp-quote><p>Thank you for the suggestions. We did not intend to suggest that participants in the Krajbich 2021 study indicated choice with their eyes. The relevant part of the discussion has been updated, as follows:</p><p>“For the encoding of accumulated evidence, findings from NHP neurophysiology point to regions involved in the preparation of movement. In motion discrimination tasks that use eye movements to report decisions, accumulated evidence signals can be observed in oculomotor control regions such as area LIP, frontal eye fields, and the superior colliculus. Because the decision in our task is reported with a reach movement, potential sites for accumulator-like activity include motor cortical areas such as the arm representations within dorsal pre-motor cortex (Chandrasekaran et al., 2017). In addition, human imaging studies have identified several other candidate regions with accumulator-like activity, including in the intraparietal sulcus, insula, caudate, and lateral prefrontal cortex (Gluth et al., 2012; Hare et al., 2011; Pisauro et al., 2017; Rodriguez et al., 2015). Interestingly, several studies find accumulator-like signals in cortical areas in the dorsal and medial frontal cortex, but these are not consistently localized. For example, whereas Pisauro et al. (2017) find accumulator like activity in the supplementary motor area, Rodriguez et al. (2015) identify a more anterior region in the medial frontal cortex. Additional studies in both humans and NHPs will be necessary to understand the specializations of these regions with respect to encoding decision evidence.”</p><disp-quote content-type="editor-comment"><p>Why did the authors use different delays for the two monkeys?</p></disp-quote><p>This comment seems to refer to the delay between the left/right lever press and the juice delivery, which was either 1 or 1.5s depending on the monkey. In our experience, introducing a delay between response and reward slows the overall reward rate that the animal experiences, which encourages slower reaction times and more accurate choices. The exact duration of this delay was tuned for each monkey during training – hence the different delays used for monkeys K and C. Because this delay occurs after the response lever is pressed, it is inconsequential for the study results.</p><disp-quote content-type="editor-comment"><p>How long were the delays between releasing the center lever and pressing left or right?</p></disp-quote><p>After lifting the center lever, Monkey C had up to 500ms and Monkey K had up to 400ms to press the left or right lever. These deadlines were determined individually for each monkey during their training and were set so that on a very small fraction of trials (~0.5%) the monkey failed to press the lever in time. This ensures that the monkey completes the responses quickly and minimizes opportunity for deliberation or changes of mind. On average, Monkey C’s lift-to-press latency was 290ms SEM 100ms and Monkey’s K’s average was 165ms SEM 20ms.</p><disp-quote content-type="editor-comment"><p>Was there some criterion for passing the training stage?</p></disp-quote><p>The criteria were qualitative. When the monkeys were first learning the fundamentals of the task, they were trained until the monkey could reliably learn stimulus-reward associations for a new stimulus set within several sessions, evident in their choice and reaction time curves (as in Figure 1D/E). At that point, we continued to introduce a new stimulus set every 2-5 sessions. When a new set was introduced, we considered the set to be “learned” when across the most recent ~200 trials the choices for the easiest stimuli (+/-4) were virtually 100% correct, choices for the next easiest (+/- 3) were ~80% correct or above, and the RT curves had a clearly visible negative slope (as in Figure1E). In fully trained animals, this threshold was usually met after 1-2 sessions. Data collection for each new set only began after this threshold was met, and each set was used for 1-3 data collection sessions.</p><disp-quote content-type="editor-comment"><p>p.33-34 – these numbers should go in the Results section, not the Methods.</p></disp-quote><p>These statistics are now also found on line 121.</p><disp-quote content-type="editor-comment"><p>I think it might make sense to include session-level random effects in the models, given that each session used a different set of stimuli. I doubt this will change much, but it might further help clean up some variance.</p></disp-quote><p>First, we’d like to clarify that each stimulus set was used for 1-3 consecutive sessions (preceded by 1-2 training sessions not used for analysis).</p><p>We re-ran the key analyses using either session, or the session-by-monkey interaction as a random effect. For both of these alternative models, the results were very similar to the model with Monkey as a random effect, and did not suggest any changes in the interpretation of the findings (not shown).</p><p>The Akaike Information Criterion (AIC) for each of the three models is presented in <xref ref-type="table" rid="sa2table1">Author response table 1</xref>, with the ‘winning’ model (lowest AIC) for each analysis marked with’*’. On the whole, the data were better fit by the monkey-only random effect model; this is likely due to the low intersession variability that results from extensive training. Consequently, the additional terms required by the session or session-by-monkey random effect model variants (54 levels of random effects per regressor vs. 2 levels) is disproportionately larger than the amount of additional variance explained—leading to higher AIC values. For this reason, the monkey-specific random effects models are the most appropriate, and we report these results in the manuscript. The results of alternative models can be provided as supplements if requested.</p><table-wrap id="sa2table1" position="float"><label>Author response table 1.</label><table frame="hsides" rules="groups"><thead><tr><th>Analysis</th><th>Session RE</th><th>Monkey:Session RE</th><th>Monkey RE</th></tr></thead><tbody><tr><td align="left" valign="top">Choice</td><td align="left" valign="top">1.68E+05</td><td align="left" valign="top">1.67E+05</td><td align="left" valign="top">1.64E+05*</td></tr><tr><td align="left" valign="top">RT</td><td align="left" valign="top">-<break/>4.06E+04</td><td align="left" valign="top">-4.06E+04</td><td align="left" valign="top">-3.68E+04*</td></tr><tr><td align="left" valign="top">Num. Fix</td><td align="left" valign="top">3.32E+04*</td><td align="left" valign="top">3.32E+04*</td><td align="left" valign="top">3.33E+04</td></tr><tr><td align="left" valign="top">Initial Fixation Bias</td><td align="left" valign="top">1.70E+05</td><td align="left" valign="top">1.70E+05</td><td align="left" valign="top">1.66E+05*</td></tr><tr><td align="left" valign="top">Final Fixation Bias</td><td align="left" valign="top">1.60E+05</td><td align="left" valign="top">1.60E+05</td><td align="left" valign="top">1.57E+05*</td></tr><tr><td align="left" valign="top">Cumulative Gaze Time-Bias</td><td align="left" valign="top">1.25E+05</td><td align="left" valign="top">1.25E+05</td><td align="left" valign="top">1.24E+05*</td></tr><tr><td align="left" valign="top">Cumulative Gaze-Time Bias :Corrected</td><td align="left" valign="top">1.87E+04*</td><td align="left" valign="top">1.87E+04*</td><td align="left" valign="top">1.90E+04</td></tr></tbody></table></table-wrap><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>I would have liked to see a more extensive discussion of what might underly the observed effects. The sequential sampling models are merely descriptive. One would like to know how fixation influences the neural signals that compute the value of the option. When a human/monkey is looking at one option, information about a previously viewed option must be maintained in working memory and the neural signals might be expected to be noisier, leading to a bias towards the most recently viewed option. Similarly, the first viewed option might have a stronger signal than the second viewed item by virtue of having less interference. This is obviously speculative but it would be nice to see what evidence there is for something like this – indeed the McGinty et al. 2016 paper seems to provide evidence of this kind. One could argue that it shouldn't be too surprising that computations performed at the fixation point are higher S/N than ones off fixation. (In the context of the sequential sampling model this would correspond to adding some noise to the item not currently fixated.)</p></disp-quote><p>Thank you for the suggestion. The following has been added to the discussion.:</p><p>“In NHPs, the orbitofrontal portion of the VMF (the OFC) has been demonstrated to contain value-signals that are dependent on whether gaze is directed towards a reward-associated visual target (Hunt et al., 2018; McGinty et al., 2016; McGinty and Lupkin, 2021) a mechanism fully consistent with the effects of gaze posited by aSSM frameworks. However, the precise neural mechanisms explaining how gaze biases choice outcomes are still unknown. One hypothesis is that the value-coding regions inherit gaze-modulated information from visual cortical regions sensitive to shifts in visual attention. For instance, neurons in the anterior inferotemporal cortex, which projects directly to the OFC (Carmichael and Price, 1995; Kravitz et al., 2013), selectively encode features of attended objects to the exclusion of others in the same receptive field (DiCarlo and Maunsell, 2000; Moore et al., 2003; Moran and Desimone, 1985; Richmond et al., 1983; Sheinberg and Logothetis, 2001). Therefore, the similar fixation-driven effects on OFC value-coding shown by McGinty and colleagues (McGinty et al., 2016; McGinty and Lupkin, 2021) may be inherited from this structure. Another candidate region for the top-down influences on gaze dependent value-coding, is the frontal eye fields (FEF). In a recent study from Krajbich et al. (2021), focal disruption of FEF leads to a reduction in the magnitude of gaze-biases. Prior work in both humans and NHPs has shown that the FEF can influence perceptual sensitivity through the top-down modulation of early visual areas (Moore and Armstrong, 2003; Moore and Fallah, 2004; Ruff et al., 2006; Silvanto et al., 2006; Taylor et al., 2007), suggesting that it may exert similar influence on value encoding regions.”</p><disp-quote content-type="editor-comment"><p>Related to this, the discussion of how the paradigm might be used for further exploration is a bit vague. The example given of MT and LIP is not particularly well chosen, given the recent evidence from the Huk lab (Katz et al., Nature 2016) against LIP as the site of the perceptual decision.</p></disp-quote><p>Thank you for the suggestion. The discussion has been updated as follows:</p><p>“The impetus for developing this animal model is to explore the neural underpinnings of the relationship between gaze and value-based choice. Given the fact that a sequential sampling model provides objectively good fits to the observed behavior, one potential approach for understanding neural mechanisms is to identify brain regions that correspond to different functions in a sequential sampling framework. Such an approach has been used in perceptual dot motion discrimination, where extrastriate cortical area MT is thought to provide the “input” signal (encoding of visual motion) and neurons in parietal area LIP are thought to represent the accumulated evidence over time (see Gold and Shadlen, 2007, for review). One note of caution, however, is that brain activity that outwardly appears to implement accumulator-like functions may not be causally involved in the choice (e.g. Katz et al., 2016). Nonetheless, it is still useful to consider the neural origins of two core computations suggested by sequential sampling models: the representation of the target values (corresponding to the input signals), and neural signals that predict of the actions performed to obtain the reward (reflecting the decision output).”</p><disp-quote content-type="editor-comment"><p>Another general concern is the absence of any discussion of all the work showing that value associated with an item serves to attract gaze and indeed is a central aspect of the mechanisms of gaze control and learning where to attend. One closely related example is the work of Hikosaka (eg Kim et al., Cell, 2015) showing that cells in the caudate tail coded the value of previously rewarded fractal patterns and attracted gaze. This makes the question of causality a bit difficult to disentangle. It's such a central issue that it needs to be explicitly discussed. Having the stimuli not resolvable in the peripheral retina helps with this issue.</p></disp-quote><p>The discussion has been updated as follows:</p><p>“Our study confirms this result, and provides additional insights, due to novel elements of the study design. First, by manipulating initial fixation direction, we show that initial fixation has a causal effect on the decision process. Second, the results of the aSSM model-fitting provide a mechanistic explanation for our results and permits a direct comparison to model fitting results in human studies.</p><p>Finally, unlike many prior studies, the decision task requires the monkeys to sample the targets by fixating them directly, due to the use of a gaze contingent mask that completely obscures the stimuli at the beginning of the trial, and the use of visual crowders that closely surround each target. This subtle design element is crucial, because it permits accurate measurement of the time spent attending to each target, and of the relationship between relative viewing time and choice. In a forthcoming study using this same task, we show that OFC neurons do not begin to reflect the value of the second-fixated stimulus until well after it has been viewed by the monkey, indicating that value information is only available to the monkey once a target is fixated (McGinty and Lupkin, 2021). In contrast, in tasks where value associated targets are easily perceived in peripheral vision, even targets that are not fixated influence prefrontal value signals and decision behavior (e.g., Cavanagh et al., 2019; Xie et al., n.d.). High-value peripheral targets are also implicated in valuebased attentional capture, in which overt or covert attention becomes drawn to objects that are reliably associated with reward (Anderson et al., 2011; Kim et al., 2015). Thus, the task used in the present study minimizes value-based capture effects.”</p><disp-quote content-type="editor-comment"><p>I would also like to see more of the actual fixation time data. If there are only two fixations, having a bias towards the first one viewed is in conflict with a bias towards the last one viewed. Thus it would be nice to see the fixation durations broken down into cases where there are 2 fixations and cases where there are 3 fixations. Just as the calculation that the initial-view bias is equal to about half a drop of juice, it would also be useful to know how fixation duration translates to choice bias. If I am understanding Figure 4 correctly it looks like an extra 200 msec viewing time translates to about a 10% increase in choice probability. More concrete description of the fixations would be helpful. For example, how do the β values on p8-9 translate to fixation durations?</p></disp-quote><p>A detailed description of fixation durations as a function of absolute trial order and relative trial order (final vs. non-final fixations) is shown in Figure 3 and described in lines 196-233. Intuitive descriptions of the choice bias effects, and fixation duration effects have been added on lines 386-388 and 238-243, respectively.</p></body></sub-article></article>