<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">60433</article-id><article-id pub-id-type="doi">10.7554/eLife.60433</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Cortical encoding of acoustic and linguistic rhythms in spoken narratives</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-199469"><name><surname>Luo</surname><given-names>Cheng</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-51085"><name><surname>Ding</surname><given-names>Nai</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3428-2723</contrib-id><email>ding_nai@zju.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Key Laboratory for Biomedical Engineering of Ministry of Education, College of Biomedical Engineering and Instrument Sciences, Zhejiang University</institution><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution>Research Center for Advanced Artificial Intelligence Theory, Zhejiang Lab</institution><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>van Wassenhove</surname><given-names>Virginie</given-names></name><role>Reviewing Editor</role><aff><institution>CEA, DRF/I2BM, NeuroSpin; INSERM, U992, Cognitive Neuroimaging Unit</institution><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>21</day><month>12</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e60433</elocation-id><history><date date-type="received" iso-8601-date="2020-06-26"><day>26</day><month>06</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-12-20"><day>20</day><month>12</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Luo and Ding</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Luo and Ding</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-60433-v2.pdf"/><abstract><p>Speech contains rich acoustic and linguistic information. Using highly controlled speech materials, previous studies have demonstrated that cortical activity is synchronous to the rhythms of perceived linguistic units, for example, words and phrases, on top of basic acoustic features, for example, the speech envelope. When listening to natural speech, it remains unclear, however, how cortical activity jointly encodes acoustic and linguistic information. Here we investigate the neural encoding of words using electroencephalography and observe neural activity synchronous to multi-syllabic words when participants naturally listen to narratives. An amplitude modulation (AM) cue for word rhythm enhances the word-level response, but the effect is only observed during passive listening. Furthermore, words and the AM cue are encoded by spatially separable neural responses that are differentially modulated by attention. These results suggest that bottom-up acoustic cues and top-down linguistic knowledge separately contribute to cortical encoding of linguistic units in spoken narratives.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>speech envelope</kwd><kwd>language</kwd><kwd>attention</kwd><kwd>rhythm</kwd><kwd>frequency tagging</kwd><kwd>spoken narratives</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>31771248</award-id><principal-award-recipient><name><surname>Ding</surname><given-names>Nai</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>MajorScientific Research Project of Zhejiang Lab</institution></institution-wrap></funding-source><award-id>2019KB0AC02</award-id><principal-award-recipient><name><surname>Ding</surname><given-names>Nai</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004731</institution-id><institution>Zhejiang Provincial Natural Science Foundation</institution></institution-wrap></funding-source><award-id>LGF19H090020</award-id><principal-award-recipient><name><surname>Luo</surname><given-names>Cheng</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Fundamental Research Funds for the Central Universities</institution></institution-wrap></funding-source><award-id>2020FZZX001-05</award-id><principal-award-recipient><name><surname>Ding</surname><given-names>Nai</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>National Key R&amp;D Program Of China</institution></institution-wrap></funding-source><award-id>2019YFC0118200</award-id><principal-award-recipient><name><surname>Ding</surname><given-names>Nai</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>In the processing of spoken narratives, bottom-up acoustic cues and top-down linguistic knowledge separately contribute to neural construction of linguistic units.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>When listening to speech, low-frequency cortical activity in the delta (&lt;4 Hz) and theta (4–8 Hz) bands is synchronous to speech (<xref ref-type="bibr" rid="bib36">Keitel et al., 2018</xref>; <xref ref-type="bibr" rid="bib51">Luo and Poeppel, 2007</xref>). However, it remains debated what speech features are encoded in the low-frequency cortical response. A large number of studies have demonstrated that the low-frequency cortical response tracks low-level acoustic features in speech, for example, the speech envelope (<xref ref-type="bibr" rid="bib11">Destoky et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="bib42">Koskinen and Seppä, 2014</xref>; <xref ref-type="bibr" rid="bib12">Di Liberto et al., 2015</xref>; <xref ref-type="bibr" rid="bib57">Nourski et al., 2009</xref>; <xref ref-type="bibr" rid="bib62">Peelle et al., 2013</xref>). Since the theta-band speech envelope provides an important acoustic cue for syllable boundaries, it has been hypothesized that neural tracking of the theta-band speech envelope is a mechanism to segment continuous speech into discrete units of syllables (<xref ref-type="bibr" rid="bib30">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib64">Poeppel and Assaneo, 2020</xref>). In other words, the theta-band envelope-tracking response reflects an intermediate neural representation linking auditory representation of acoustic speech features and phonological representation of syllables. Consistent with this hypothesis, it has been found that neural tracking of speech envelope is related to both low-level speech features (<xref ref-type="bibr" rid="bib17">Doelling et al., 2014</xref>) and perception. On one hand, it can occur when speech recognition fails (<xref ref-type="bibr" rid="bib21">Etard and Reichenbach, 2019</xref>; <xref ref-type="bibr" rid="bib33">Howard and Poeppel, 2010</xref>; <xref ref-type="bibr" rid="bib63">Peña and Melloni, 2012</xref>; <xref ref-type="bibr" rid="bib79">Zoefel and VanRullen, 2016</xref>; <xref ref-type="bibr" rid="bib80">Zou et al., 2019</xref>). On the other hand, it is strongly modulated by attention (<xref ref-type="bibr" rid="bib78">Zion Golumbic et al., 2013</xref>; <xref ref-type="bibr" rid="bib37">Kerlin et al., 2010</xref>) and may be a prerequisite for successful speech recognition (<xref ref-type="bibr" rid="bib74">Vanthornhout et al., 2018</xref>).</p><p>Speech comprehension, however, requires more than syllabic-level processing. Previous studies suggest that low-frequency cortical activity can also reflect neural processing of higher-level linguistic units, for example, words and phrases (<xref ref-type="bibr" rid="bib8">Buiatti et al., 2009</xref>; <xref ref-type="bibr" rid="bib13">Ding et al., 2016a</xref>; <xref ref-type="bibr" rid="bib36">Keitel et al., 2018</xref>), and the prosodic cues related to these linguistic units, for example, delta-band speech envelope and pitch contour (<xref ref-type="bibr" rid="bib4">Bourguignon et al., 2013</xref>; <xref ref-type="bibr" rid="bib50">Li and Yang, 2009</xref>; <xref ref-type="bibr" rid="bib72">Steinhauer et al., 1999</xref>). One line of research demonstrates that, when listening to natural speech, cortical responses can encode word onsets (<xref ref-type="bibr" rid="bib6">Brodbeck et al., 2018</xref>) and capture semantic similarity between words (<xref ref-type="bibr" rid="bib7">Broderick et al., 2018</xref>). It remains to be investigated, however, how bottom-up prosodic cues and top-down linguistic knowledge separately contribute to the generation of these word-related responses. Another line of research selectively focuses on top-down processing driven by linguistic knowledge. These studies demonstrate that cortical responses are synchronous to perceived linguistic units, for example, words, phrases, and sentences, even when acoustic correlates of these linguistic units are not available (<xref ref-type="bibr" rid="bib13">Ding et al., 2016a</xref>; <xref ref-type="bibr" rid="bib15">Ding et al., 2018</xref>; <xref ref-type="bibr" rid="bib34">Jin et al., 2018</xref>; <xref ref-type="bibr" rid="bib53">Makov et al., 2017</xref>). Based on these results, it has been hypothesized that low-frequency cortical activity can reflect linguistic-level neural representations that are constructed based on internal linguistic knowledge instead of acoustic cues (<xref ref-type="bibr" rid="bib13">Ding et al., 2016a</xref>; <xref ref-type="bibr" rid="bib15">Ding et al., 2018</xref>; <xref ref-type="bibr" rid="bib55">Meyer et al., 2020</xref>). Nevertheless, to dissociate linguistic units with the related acoustic cues, most of these studies present speech as an isochronous sequence of synthesized syllables, which organizes into a sequence of unrelated words and sentences. Therefore, it remains unclear whether cortical activity can synchronize to linguistic units in natural spoken narratives, and how it is influenced by bottom-up acoustic cues and top-down linguistic knowledge.</p><p>Here we first asked whether cortical activity could reflect the rhythm of disyllabic words in semantically coherent stories. The story was either naturally read or synthesized as an isochronous sequence of syllables to remove acoustic cues for word boundaries (<xref ref-type="bibr" rid="bib13">Ding et al., 2016a</xref>). We then asked how the neural response to disyllabic words was influenced by acoustic cues for words. To address this question, we amplitude modulated isochronous speech at the word rate and tested how this word-synchronized acoustic cue modulated the word response. Finally, since previous studies have shown that cortical tracking of speech strongly depended on the listeners’ task (<xref ref-type="bibr" rid="bib16">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="bib78">Zion Golumbic et al., 2013</xref>; <xref ref-type="bibr" rid="bib59">O'Sullivan et al., 2015</xref>), we designed two tasks during which the neural responses were recorded. One task required attentive listening to speech and answering of comprehension questions afterwards, while in the other task participants were engaged in watching a silent movie while passively listening to speech.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Neural encoding of words in isochronously presented narratives</title><p>We first presented semantically coherent stories that were synthesized as an isochronous sequence of syllables (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, left). To produce a metrical structure in stories, every other syllable was designed to be a word onset. More specifically, the odd terms in the metrical syllable sequence always corresponded to the initial syllable of a word, that is, word onset, while the even terms corresponded to either the second syllable of a disyllabic word (73% probability) or a monosyllabic word (23% probability). In the following, the odd terms of the syllable sequence were referred to as σ1, and the even terms as σ2. Since syllables were presented at a constant rate of 4 Hz, the neural response to syllables was frequency tagged at 4 Hz. Furthermore, since every other syllable in the sequence was the onset of a word, neural activity synchronous to word onsets was expected to show a regular rhythm at half of the syllabic rate, that is, 2 Hz (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, right).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Stimulus.</title><p>(<bold>A and B</bold>) Two types of stories are constructed: metrical stories and nonmetrical stories. (<bold>A</bold>) Metrical stories are composed of disyllabic words and pairs of monosyllabic words, so that the odd terms in the syllable sequence (referred to as σ1) must be the onset of a word. Here the onset syllable of each word is shown in bold. All syllables are presented at a constant rate of 4 Hz. A 500 ms gap is inserted at the position of any punctuation. The red curve illustrates cortical activity synchronous to word onsets, and it shows a 2-Hz rhythm, which can be clearly observed in the spectrum shown on the right. The stories are in Chinese and English examples are shown for illustrative purposes. (<bold>B</bold>) In the nonmetrical stories, word onsets are not regularly positioned, and activity that is synchronous to word onsets does not show 2-Hz rhythmicity. (<bold>C</bold>) Natural speech. The stories are naturally read by a human speaker and the duration of syllables is not controlled. (<bold>D</bold>) Amplitude-modulated isochronous speech is constructed by amplifying either σ1 or σ2 by a factor of 4, creating a 2-Hz amplitude modulation. The red and blue curves illustrate responses that are synchronous to word onsets and amplified syllables, respectively. The response synchronous to word onsets is identical for σ1- and σ2-amplified speech, that is, the phase difference was 0° at 2 Hz. In contrast, the response synchronous to amplified syllables is offset by 250 ms between conditions, that is, the phase difference was 180° at 2 Hz.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60433-fig1-v2.tif"/></fig><p>As a control condition, we also presented stories with a nonmetrical structure (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, left). These stories were referred to as the nonmetrical stories in the following. In these stories, the word duration was not controlled and σ1 was not always a word onset. Given that the word onsets in these stories did not show rhythmicity at 2 Hz, neural activity synchronous to the word onsets was not frequency tagged at 2 Hz (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, right).</p><p>When listening to the stories, one group of participants was asked to attend to the stories and answer comprehension questions presented at the end of each story. The task was referred to as story comprehension task, and the participants correctly answered 96 ± 9% and 94 ± 9% questions for metrical and nonmetrical stories, respectively. Another group of participants, however, were asked to watch a silent movie while passively listening to the same set of stories as those used in the story comprehension task. The silent movie was not related to the auditorily presented stories, and the participants did not have to answer any speech comprehension question. The task was referred to as movie watching task. The electroencephalogram (EEG) responses to isochronously presented stories are shown in <xref ref-type="fig" rid="fig2">Figure 2A and C</xref>. The response spectrum was averaged over participants and EEG electrodes.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Electroencephalogram (EEG) response spectrum.</title><p>Response spectrum is averaged over participants and EEG electrodes. The shaded area indicates one standard error of the mean (SEM) across participants. Stars indicate significantly higher power at 2 or 4 Hz than the power averaged over four neighboring frequency bins (two on each side). *p&lt;0.05, **p&lt;0.001 (bootstrap, false discovery rate [FDR] corrected). The color of the star is the same as the color of the spectrum. The topography on the top of each plot shows the distribution of response power at 2 Hz and 4 Hz. The five black dots in the topography indicate the position of electrodes FCz, Fz, Cz, FC3, and FC4. (<bold>A–D</bold>) Response spectrum for isochronous speech and amplitude-modulated speech during two tasks. To facilitate the comparison between stimuli, the red curves in panels <bold>A</bold> and <bold>C</bold> are repeated in panels <bold>B</bold> and <bold>D</bold>, respectively. (<bold>E</bold>) Response spectrum when the participants listen to natural speech. In this analysis, the response to natural speech is time warped to simulate the response to isochronous speech, and then transformed into the frequency-domain.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Preprocessed electroencephalogram (EEG) data recorded in Experiments 1–3.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-60433-fig2-data1-v2.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60433-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>2-Hz response power in individual electroencephalogram (EEG) electrodes and individual participants.</title><p>The x-axis is the EEG electrodes and the electrode index, from 1 to 64, which goes from left to right. The approximate scalp position of each electrode is shown at the bottom right corner. Individual results are shown as cyan dots. For each EEG electrode, the 2-Hz power averaged over participants is shown by a red or black dot, and the 95% confidence interval across participants is shown by a vertical bar. The dot and bar are red if the 2-Hz power is significantly stronger than the power averaged over four neighboring frequency bins (p&lt;0.05, bootstrap, false discovery rate [FDR] corrected) and black otherwise.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60433-fig2-figsupp1-v2.tif"/></fig></fig-group><p><xref ref-type="fig" rid="fig2">Figure 2A</xref> shows the responses from the participants attentively listening to the stories in story comprehension task. For metrical stories, two peaks were observed in the EEG spectrum, one at 4 Hz, that is, the syllable rate (p=0.0001, bootstrap, false discovery rate [FDR] corrected) and the other at 2 Hz, that is, the rate of disyllabic words (p=0.0001, bootstrap, FDR corrected). For nonmetrical stories, however, a single response peak was observed at 4 Hz (p=0.0001, bootstrap, FDR corrected), while no significant response peak was observed at 2 Hz (p=0.27, bootstrap, FDR corrected). A comparison of the responses to metrical and nonmetrical stories was performed, and a significant difference was observed at 2 Hz (p=0.0005, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig3">Figure 3A</xref>) but not at 4 Hz (p=0.40, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig3">Figure 3B</xref>). The response topography showed a centro-frontal distribution.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Response power and phase.</title><p>(<bold>A and B</bold>) Response power at 2 and 4 Hz. Color of the bars indicates the stimulus. Black stars indicate significant differences between different types of speech stimuli while red stars indicate significant differences between tasks. *p&lt;0.05, **p&lt;0.01 (bootstrap, false discovery rate [FDR] corrected). Throughout the manuscript, in all bar graphs of response power, the response power at a target frequency is subtracted by the power averaged over four neighboring frequency bins (two on each side) to reduce the influence of background neural activity. (<bold>C</bold>) The difference in 2-Hz response phase between the σ1- and σ2-amplified conditions at 2 Hz. The phase difference is averaged across participants, and the polar histogram shows the distribution of phase difference across 64 electrodes.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Preprocessed EEG data recorded in Experiment 1-3.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-60433-fig3-data1-v2.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60433-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>2-Hz response phase in individual electroencephalogram (EEG) electrodes.</title><p>The x-axis is the EEG electrode index, from 1 to 64, which goes from left to right. The approximate scalp position of each electrode is shown at the central right corner. Individual results are shown as cyan dots. For each EEG electrode, the 2-Hz phase averaged over participants is shown by a red or black dot, and the 95% confidence interval across participants is shown by a vertical bar. The dot and bar are red if the inter-participant phase coherence is significantly higher than chance (p&lt;0.05, see Materials and methods, false discovery rate [FDR] corrected) and black otherwise. To facilitate the comparison between the σ1- and σ2-amplified conditions, the mean phase in the σ1-amplified condition is repeated in the plot for the σ2-amplified condition as blue dots. (<bold>A–D</bold>) 2-Hz response phase for Experiments 1–4. The inter-participant phase coherence shows different patterns between the σ1- and σ2-amplified conditions during the story comprehension task. (<bold>E</bold>) Topographical distribution of the 2-Hz phase difference between the σ1- and σ2-amplified conditions. The phase difference is calculated for each participant and each electrode, and then averaged over participants. The black dots indicate the electrodes showing a significant phase difference between the σ1- and σ2-amplified conditions (p&lt;0.05, bootstrap, false discovery rate [FDR] corrected).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60433-fig3-figsupp1-v2.tif"/></fig></fig-group><p>When participants watched a silent movie during story listening, however, a single response peak was observed at 4 Hz for both metrical (p=0.0002, bootstrap, FDR corrected) and nonmetrical stories (p=0.0002, bootstrap, FDR corrected) (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). The response peak at 2 Hz was not significant for either kind of stories (p&gt;0.074, bootstrap, FDR corrected). A comparison of the responses to metrical and nonmetrical stories did not find significant difference at either 2 Hz (p=0.22, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig3">Figure 3A</xref>) or 4 Hz (p=0.39, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig3">Figure 3B</xref>). Furthermore, the 2-Hz response was significantly stronger in the story comprehension task than in the movie watching task (p=0.0004, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig3">Figure 3A</xref>). These results showed that cortical activity was synchronous to the word rhythm during attentive speech comprehension. When attention was diverted, however, the word-rate response was no longer detected.</p></sec><sec id="s2-2"><title>Neural encoding of words in natural spoken narratives</title><p>Next, we asked whether cortical activity was synchronous to disyllabic words in natural speech processing. The same set of stories used in the isochronous speech condition was read in a natural manner by a human speaker and presented to participants. The participants correctly answered 95 ± 4% and 97 ± 6% comprehension questions for metrical and nonmetrical stories, respectively. In natural speech, syllables were not produced at a constant rate (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), and therefore the syllable and word responses were not frequency tagged. Nevertheless, we time warped the response to natural speech and made the syllable and word responses periodic. Specifically, the neural response to each syllable in natural speech was extracted and realigned to a constant 4-Hz rhythm using a convolution-based procedure (<xref ref-type="bibr" rid="bib34">Jin et al., 2018</xref>; see Materials and methods for details). After time-warping analysis, cortical activity synchronous to the word onsets was expected to show a 2-Hz rhythm, the same as the response to the isochronous speech (<xref ref-type="fig" rid="fig2">Figure 2E</xref>).</p><p>The spectrum of the time-warped response was averaged over participants and EEG electrodes (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). For metrical stories, two peaks were observed in spectrum of the time-warped response, one at 4 Hz (p=0.0002, bootstrap, FDR corrected) and the other at 2 Hz (p=0.0007, bootstrap, FDR corrected). For nonmetrical stories, however, a single response peak was observed at 4 Hz (p=0.0002, bootstrap, FDR corrected), while no significant response peak was observed at 2 Hz (p=0.37, bootstrap, FDR corrected). A comparison of the responses to metrical and nonmetrical stories was performed and found a significant difference at 2 Hz (p=0.036, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig3">Figure 3A</xref>) but not at 4 Hz (p=0.09, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig3">Figure 3B</xref>). These results demonstrated that cortical activity was synchronous to the word rhythm in natural spoken narratives during attentive speech comprehension.</p></sec><sec id="s2-3"><title>Neural responses to amplitude-modulated speech</title><p>To investigate potential interactions between neural encoding of linguistic units and relevant acoustic features, we amplitude modulated the isochronous speech at the word rate, that is, 2 Hz. The amplitude modulation (AM) was achieved by amplifying either σ1 or σ2 by a factor of 4 (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), creating two conditions: σ1-amplified condition and σ2-amplified condition. Such 2-Hz AM provided an acoustic cue for the word rhythm. The speech was referred to as amplitude-modulated speech in the following. When listening to amplitude-modulated speech, the participants correctly answered 94 ± 12% and 97 ± 8% comprehension questions in the σ1- and σ2-amplified conditions, respectively.</p><p>The EEG responses to amplitude-modulated speech are shown in <xref ref-type="fig" rid="fig2">Figure 2B and D</xref>. When participants attended to the speech, significant response peaks were observed at 2 Hz (σ1-amplified: p=0.0001, and σ2-amplified: p=0.0001, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig2">Figure 2D</xref>). A comparison between the responses to isochronous speech and amplitude-modulated speech found that AM did not significantly influence the power of the 2-Hz neural response (σ1-amplified vs. σ2-amplified: p=0.56; σ1-amplified vs. isochronous: p=0.38, σ2-amplified vs. isochronous: p=0.44, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig3">Figure 3A</xref>). These results showed that the 2-Hz response power was not significantly influenced by the 2- Hz AM during attentive speech comprehension.</p><p>Another group of participants passively listened to amplitude-modulated speech while attending to a silent movie. In their EEG responses, significant response peaks were also observed at 2 Hz (σ1-amplified: p=0.0001, and σ2-amplified: p=0.0001, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig2">Figure 2D</xref>). A comparison between the responses to isochronous speech and amplitude-modulated speech showed stronger 2-Hz response in the processing of amplitude-modulated speech than isochronous speech (σ1-amplified vs. isochronous: p=0.021, σ2-amplified vs. isochronous: p=0.0042, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig3">Figure 3A</xref>). No significant difference was found between responses to σ1-amplified and σ2-amplified speech (p=0.069, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Therefore, when attention was diverted, the 2-Hz response power was significantly increased by the 2-Hz AM.</p><p>The Fourier transform decomposes an arbitrary signal into sinusoids and each complex-valued Fourier coefficient captures the magnitude and phase of a sinusoid. The power spectrum reflects the response magnitude but ignores the response phase. The phase difference between the 2-Hz responses in the σ1- and σ2-amplified conditions, however, carried important information about whether the neural response was synchronous to the word onsets or amplified syllables: Neural activity synchronous to amplified syllables showed a 250 ms time lag between the σ1- and σ2-amplified conditions (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), which corresponded to a 180° phase difference between conditions at 2 Hz. Neural activity synchronous to the word onsets, however, should be identical in the σ1- and σ2-amplified conditions.</p><p>The 2-Hz response phase difference between the σ1- and σ2-amplified conditions is shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref>. The phase difference, when averaged across participants and EEG electrodes, was 41° (95% confidence interval: −25–91°) during the story comprehension task and increased to 132° (95% confidence interval: 102–164°) during the movie watching task (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for response phase in individual conditions).</p></sec><sec id="s2-4"><title>Separate the responses to words and word-rate AM</title><p>In the previous section, we analyzed the difference in 2-Hz response phase difference between the σ1- and σ2-amplified conditions. A 0° phase difference indicated that the 2-Hz response was only synchronous to word onsets, while a 180° phase difference indicated that the 2-Hz response was only synchronous to amplified syllables. A phase difference between 0° and 180° indicated that neural response synchronous to word onsets and neural response synchronous to amplified syllables both exist, but the phase analysis could not reveal the strength of the two response components. Therefore, in the following, we extracted the neural responses to words and the 2-Hz AM by averaging the responses across the σ1- and σ2-amplified conditions in different manners.</p><p>To extract the neural response to words, we averaged the neural response waveforms across the σ1- and σ2-amplified conditions. The word onsets were aligned in these two conditions and therefore neural activity synchronous to the word onsets was preserved in the average. In contrast, cortical activity synchronous to the amplified syllables exhibited a 250 ms time lag between the σ1- and σ2-amplified conditions. Therefore, the 2-Hz response synchronous to the amplified syllables was 180° out of phase between the two conditions, and got canceled in the average across conditions. In sum, the average over the σ1- and σ2-amplified conditions preserved the response to words but canceled the response to amplified syllables (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). This average was referred to as the word response in the following.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Word and amplitude modulation (AM) responses to amplitude-modulated speech.</title><p>(<bold>A–D</bold>) Neural responses in σ1- and σ2-amplified conditions are aligned based on either word onsets (AB) or amplified syllables (CD), and averaged to extract the 2-Hz response synchronous to words or AM, respectively. Panels <bold>A</bold> and <bold>C</bold> illustrate the procedure. The red and blue curves illustrate the response components synchronous to word onsets and the AM respectively, which are mixed in the electroencephalogram (EEG) measurement and shown separately for illustrative purposes. The spectrum and topography in panels <bold>B</bold> and <bold>D</bold> are shown the same way as they are shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. *p&lt;0.05, **p&lt;0.001 (bootstrap, false discovery rate [FDR] corrected). (<bold>E</bold>) Response power at 2 Hz. Black stars indicate significant differences between the word and AM responses, while red stars indicate a significant difference between tasks. *p&lt;0.05, **p&lt;0.01 (bootstrap, FDR corrected). (<bold>F</bold>) The left panel shows the power difference between the word and AM responses in single electrodes. The right panel shows the difference in normalized topography, that is, topography divided by its maximal value. Black dots indicate electrodes showing a significant difference between the word and AM responses (p&lt;0.05, bootstrap, FDR corrected).</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Preprocessed electroencephalogram (EEG) data recorded in Experiments 1–3.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-60433-fig4-data1-v2.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60433-fig4-v2.tif"/></fig><p>For the word response, a significant response peak was observed at 2 Hz during both story comprehension task (p=0.0001, bootstrap, FDR corrected) and movie watching task (p=0.011, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig4">Figure 4B</xref>). The response power was significantly stronger in the story comprehension task than that in the movie watching task (p=0.0008, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig4">Figure 4E</xref>). During the movie watching task, a significant 2-Hz word response was observed when amplitude-modulated speech was presented (p=0.011, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig4">Figure 4B</xref>), while no significant 2-Hz word response was observed when isochronous speech was presented (p=0.074, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig2">Figure 2C</xref>). This finding indicated that the 2-Hz AM facilitated word processing during passive listening.</p><p>Furthermore, during the story comprehension task, the power of the 2-Hz word response did not significantly differ between amplitude-modulated speech and isochronous speech (p=0.69, bootstrap, FDR corrected), suggesting that the 2-Hz AM did not significantly modulate the 2-Hz word response during attentive speech comprehension.</p><p>To extract the neural response to 2-Hz AM, we first aligned the responses in the σ1- and σ2-amplified conditions by adding a delay of 250 ms to the response in the σ1-amplified condition. We then averaged the response waveforms across conditions (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). After the delay was added to the response to σ1-amplified speech, the 2-Hz AM was identical between the σ1- and σ2-amplified conditions. Therefore, the average across conditions preserved the response to 2-Hz AM while canceling the response to words. The averaged response was referred to as the AM response in the following.</p><p>For the AM response, significant 2-Hz response peaks were observed during both the story comprehension task (p=0.0021, bootstrap, FDR corrected) and the movie watching task (p=0.0001, bootstrap, FDR corrected). The 2-Hz response power did not significantly differ between tasks (p=0.39, bootstrap, FDR corrected), suggesting that the 2-Hz AM response was not strongly enhanced when participants attended to speech.</p><p>During the story comprehension task, the 2-Hz response power averaged across electrodes was significantly stronger for the word response than the AM response (p=0.021, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig4">Figure 4E</xref>). During the movie watching task, however, the reverse was true, that is, the 2-Hz word response was significantly weaker than the 2-Hz AM response (p=0.013, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig4">Figure 4E</xref>).</p><p>An analysis of individual electrodes showed the 2-Hz word response was significantly stronger than the 2-Hz AM response in centro-frontal electrodes during the story comprehension task (p&lt;0.05, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig4">Figure 4F</xref>, left). To further compare the spatial distribution of 2-Hz word and AM responses, on top of their power difference, we normalized the response topography by dividing its maximum value. Significant difference was found in the normalized topography between the 2-Hz word and AM responses in temporal electrodes (p&lt;0.05, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig4">Figure 4F</xref>, right). These results suggested that the 2-Hz word and AM responses had distinct neural sources.</p></sec><sec id="s2-5"><title>Neural responses to amplitude-modulated speech: a replication</title><p>The responses to amplitude-modulated speech suggested that the brain encoded words and acoustic cues related to words in a different manner. Furthermore, the word-related acoustic cues seemed to facilitate word processing during passive listening, but not significantly enhance the word response during attentive listening. To further validate these findings, we conducted a replication experiment to measure the neural response to amplitude-modulated speech in a separate group of participants. These participants first performed the movie watching task and then the story comprehension task. During the story comprehension task, the participants correctly answered 96 ± 8% and 95 ± 9% questions in the σ1-amplified and σ2-amplified conditions, respectively.</p><p>We first analyzed the spectrum of response to σ1- and σ2-amplified speech. Consistent with previous results, a significant 2-Hz response peak was observed whether the participants attended to the speech (σ1-amplified: p=0.0001, and σ2-amplified: p=0.0001, bootstrap, FDR corrected) or movie (σ1-amplified: p=0.0001, and σ2-amplified: p=0.0004, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A and B</xref>). When averaged across participants and electrodes, the 2-Hz response phase difference between the σ1- and σ2-amplified conditions was 7° (95% confidence interval: −26–47°) during the story comprehension task and 96° (95% confidence interval: 55–143°) during the movie watching task (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C and D</xref>).</p><p>We then separately extracted the word and AM responses following the procedures described in <xref ref-type="fig" rid="fig4">Figure 4A and C</xref>. Similar to previous results, in the spectrum (<xref ref-type="fig" rid="fig5">Figure 5A and B</xref>), a significant 2-Hz word response was observed during both tasks (story comprehension task: p=0.0001, bootstrap; movie watching task: p=0.0003, bootstrap, FDR corrected), and a significant 2-Hz AM response was also observed during both tasks (story comprehension task: p=0.0001, bootstrap; movie watching task: p=0.0001, bootstrap, FDR corrected). Furthermore, the 2-Hz word response exhibited significantly stronger power than the 2-Hz AM response during the story comprehension task (p=0.0036, bootstrap, FDR corrected, <xref ref-type="fig" rid="fig5">Figure 5C</xref>), and the two responses showed distinct topographical distribution (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). In sum, the results obtained in the replication experiment were consistent with those from the original experiment. A follow-up comparison of the results from the original and replication experiments suggested that the 2-Hz word response exhibited significantly stronger power during the movie watching task in the replication experiment than the original experiment (p=0.0008, bootstrap, FDR corrected).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Replication of the neural response to amplitude-modulated speech.</title><p>(<bold>A and B</bold>) Spectrum and topography for the word response (<bold>A</bold>) and amplitude modulation (AM) response (<bold>B</bold>). Colored stars indicate frequency bins with stronger power than the power averaged over four neighboring frequency bins (two on each side). *p&lt;0.05, **p&lt;0.001 (bootstrap, false discovery rate [FDR] corrected). The topography on the top of each plot shows the distribution of response power at 2 Hz and 4 Hz. (<bold>C</bold>) Response power at 2 Hz. Black stars indicate significant differences between the word and (AM) responses, while red stars indicate a significant difference between tasks. *p&lt;0.05, **p&lt;0.01 (bootstrap, FDR corrected). (<bold>D</bold>) Power difference between the word and AM responses in individual electrodes are shown in the left panel. To further illustrate the difference in topographical distribution instead of the response power, each response topography is normalized by dividing its maximum value. The difference in the normalized topography is shown in the right panel. Black dots indicate electrodes showing a significant difference between the word and AM responses (p&lt;0.05, bootstrap, FDR corrected).</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Preprocessed electroencephalogram (EEG) data recorded in Experiment 4.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-60433-fig5-data1-v2.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60433-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Electroencephalogram (EEG) response spectrum and 2-Hz phase difference in the replication experiment.</title><p>(<bold>A and B</bold>) Response spectrum averaged over participants and EEG electrodes. The shaded area indicates one standard error of the mean (SEM) across participants. Stars indicate significantly higher power at 2 Hz or 4 Hz than the power averaged over four neighboring frequency bins (two on each side). The color of the star is the same as the color of the spectrum. The topography on the top of each plot shows the distribution of response power at 2 Hz and 4 Hz. The five black dots in the topography indicate the position of electrodes FCz, Fz, Cz, FC3, and FC4. (<bold>C and D</bold>) Phase difference between the σ1- and σ2-amplified conditions at 2 Hz. The phase difference is averaged across participants, and the polar histogram shows the distribution of phase difference from 64 electrodes.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60433-fig5-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-6"><title>Time course of EEG responses to words</title><p>The event-related potential (ERP) responses evoked by σ1 and σ2 are separately shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>. The ERP analysis was restricted to disyllabic words so that the responses to σ1 and σ2 represented the responses to the first and second syllables of disyllabic words respectively. When participants attended to speech, the ERP responses to σ1 and σ2 showed significant differences for both isochronous (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) and natural speech (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). When participants watched a silent movie, a smaller difference was also observed between the ERP responses to σ1 and σ2 (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). The topography of the ERP difference showed a centro-frontal distribution. For isochronous speech, the ERP latency could not be unambiguously interpreted for isochronous speech, given that the stimulus was strictly periodic. For natural speech, the ERP responses to σ1 and σ2 differed in a time window around 300–500 ms.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Event-related potential (ERP) responses evoked by disyllabic words.</title><p>The ERP responses evoked by σ1 and σ2 are shown in red and black, respectively. The ERP response is averaged over participants and electrodes. The shaded area indicates 1 SEM across participants. The gray lines on top denote the time intervals in which the two responses are significantly different from each other (p&lt;0.05, cluster-based permutation test). The topography on top is averaged over all time intervals showing a significant difference between the two responses in each plot. Time 0 indicates syllable onset.</p><p><supplementary-material id="fig6sdata1"><label>Figure 6—source data 1.</label><caption><title>Preprocessed electroencephalogram (EEG) data recorded in Experiments 1–3.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-60433-fig6-data1-v2.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60433-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Event-related potential (ERP) responses evoked by disyllabic words in the replication experiment.</title><p>The ERP responses evoked by σ1 and σ2 are shown in red and black, respectively. The ERP response is averaged over participants and electrodes. The shaded area indicates 1 SEM across participants. The gray lines on top denote the time intervals in which the two responses are significantly different from each other (p&lt;0.05, cluster-based permutation test). The topography on top is averaged over all time intervals showing a significant difference between the two responses in each plot. Time 0 indicates syllable onset.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60433-fig6-figsupp1-v2.tif"/></fig></fig-group><p>The ERP results for amplitude-modulated speech are shown in <xref ref-type="fig" rid="fig6">Figure 6C and D</xref>. When participants attended to speech, a difference between the ERP responses to σ1 and σ2 was observed in both σ1- and σ2-amplified conditions (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). During passive listening, however, a significant ERP difference was observed near the onset of the amplified syllable. These results were consistent with the results in the replication experiment (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Speech comprehension is a complex process involving multiple stages, for example, encoding of acoustic features, extraction of phonetic features, and processing of higher-level linguistic units such as words, phrases, and sentences. Here, we investigate how low-frequency cortical activity encodes linguistic units and related acoustic features. When participants naturally listen to spoken narratives, we observe that cortical activity is synchronous to the rhythm of spoken words. The word synchronous response is observed whether participants listen to natural speech or synthesized isochronous speech that removes word-related acoustic cues. Furthermore, when introducing an AM cue to the word rhythm, neural responses to words and the AM cue are both observed and they show different spatial distribution. In addition, when participants are engaged in a story comprehension task, the word response exhibits stronger power than the AM response. The AM cue does not clearly modulate the word response during story comprehension task (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), but can facilitate word processing when attention is diverted: The word response is not detected for isochronous speech (<xref ref-type="fig" rid="fig2">Figure 2C</xref>), but is detected for amplitude-modulated speech during passive listening (<xref ref-type="fig" rid="fig4">Figures 4B</xref> and <xref ref-type="fig" rid="fig5">5A</xref>). In sum, these results show that both top-down linguistic knowledge and bottom-up acoustic cues separately contribute to word synchronous neural responses.</p><sec id="s3-1"><title>Neural encoding of linguistic units in natural speech</title><p>In speech, linguistic information is organized through a hierarchy of units, including phonemes, syllables, morphemes, words, phrases, sentences, and discourses. These units span a broad range of time scales, from tens of milliseconds for phonemes to a couple of seconds for sentences, and even longer for discourses. It is a challenging question to understand how the brain represents the hierarchy of linguistic units, and it is an appealing hypothesis that each level of linguistic unit is encoded by cortical activity on the relevant time scale (<xref ref-type="bibr" rid="bib13">Ding et al., 2016a</xref>; <xref ref-type="bibr" rid="bib18">Doumas and Martin, 2016</xref>; <xref ref-type="bibr" rid="bib30">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib31">Goswami, 2019</xref>; <xref ref-type="bibr" rid="bib36">Keitel et al., 2018</xref>; <xref ref-type="bibr" rid="bib38">Kiebel et al., 2008</xref>; <xref ref-type="bibr" rid="bib56">Meyer and Gumbert, 2018</xref>). Previous fMRI studies have suggested that neural processing at different levels, for example, syllables, words, and sentences, engages different cortical networks (<xref ref-type="bibr" rid="bib3">Blank and Fedorenko, 2020</xref>; <xref ref-type="bibr" rid="bib32">Hasson et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Lerner et al., 2011</xref>). Magnetoencephalography (MEG)/Electroencephalogram (EEG) studies have found reliable delta- and theta-band neural responses that are synchronous to speech (<xref ref-type="bibr" rid="bib16">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="bib51">Luo and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib62">Peelle et al., 2013</xref>), and the time scales of such activity are consistent with the time scales of syllables and larger linguistic units.</p><p>Nevertheless, it remains unclear whether these MEG/EEG responses directly reflect neural encoding of hierarchical linguistic units, or simply encode acoustic features associated with these units (<xref ref-type="bibr" rid="bib10">Daube et al., 2019</xref>; <xref ref-type="bibr" rid="bib41">Kösem and van Wassenhove, 2017</xref>). On one hand, neural tracking of sound envelope is reliably observed in the absence of speech comprehension, for example, when participants listen to unintelligible speech (<xref ref-type="bibr" rid="bib33">Howard and Poeppel, 2010</xref>; <xref ref-type="bibr" rid="bib79">Zoefel and VanRullen, 2016</xref>) and non-speech sound (<xref ref-type="bibr" rid="bib47">Lalor et al., 2009</xref>; <xref ref-type="bibr" rid="bib75">Wang et al., 2012</xref>). The envelope tracking response is even weaker for sentences composed of real words than sentences composed of pseudowords (<xref ref-type="bibr" rid="bib52">Mai et al., 2016</xref>), and is weaker for the native language than an unfamiliar language (<xref ref-type="bibr" rid="bib80">Zou et al., 2019</xref>). Neural tracking of speech envelope can also be observed in animal primary auditory cortex (<xref ref-type="bibr" rid="bib14">Ding et al., 2016b</xref>). Furthermore, a recent study shows that low-frequency cortical activity cannot reflect the perception of an ambiguous syllable sequence, for example, whether repetitions of a syllable is perceived as ‘flyflyfly’ or ‘lifelifelife’ (<xref ref-type="bibr" rid="bib40">Kösem et al., 2016</xref>).</p><p>On the other hand, cortical activity synchronous to linguistic units, such as words and phrases, can be observed using well-controlled synthesized speech that removes relevant acoustic cues (<xref ref-type="bibr" rid="bib13">Ding et al., 2016a</xref>; <xref ref-type="bibr" rid="bib34">Jin et al., 2018</xref>; <xref ref-type="bibr" rid="bib53">Makov et al., 2017</xref>). These studies, however, usually present semantically unrelated words or sentences at a constant pace, which creates a salient rhythm easily noticeable for listeners. In contrast, in the current study, we presented semantically coherent stories. It was found that for both synthesized isochronous speech and natural speech, cortical activity was synchronous to multi-syllabic words in metrical stories when participants were engaged in a story comprehension task. Furthermore, few listeners reported noticing any difference between the metrical and nonmetrical stories (see Supplementary materials for details), suggesting that the word rhythm was not salient and was barely noticeable to the listeners. Therefore, the word response is likely to reflect implicit word processing instead of the perception of an explicit rhythm.</p><p>A comparison of the responses to natural speech and isochronous speech showed that responses to word and syllable were weaker for natural speech, suggesting that strict periodicity in the stimulus could indeed boost rhythmic neural entrainment. Although the current study and previous studies (<xref ref-type="bibr" rid="bib15">Ding et al., 2018</xref>; <xref ref-type="bibr" rid="bib53">Makov et al., 2017</xref>) observe a word-rate neural response, the study conducted by <xref ref-type="bibr" rid="bib40">Kösem et al., 2016</xref> does not report observable neural activity synchronous to perceived word rhythm. A potential explanation for the mixed results is that <xref ref-type="bibr" rid="bib40">Kösem et al., 2016</xref> repeat the same word in each trial while the other studies present a large variety of words with no immediate repetitions in the stimuli. Therefore, it is possible that low-frequency word-rate neural response more strongly reflects neural processing of novel words, instead of the perception of a steady rhythm (see also <xref ref-type="bibr" rid="bib61">Ostarek et al., 2020</xref>).</p></sec><sec id="s3-2"><title>Mental processes reflected in neural activity synchronous to linguistic units</title><p>It remains elusive what kind of mental representations are reflected by cortical responses synchronous to linguistic units. For example, the response may reflect the phonological, syntactic, or semantic aspect of a perceived linguistic unit and it is difficult to tease apart these factors. Even if a sequence of sentences is constructed with independently synthesized monosyllabic words, the sequence does not sound like a stream of individual syllables delivered at a constant pace. Instead, listeners can clearly perceive each sentence as a prosodic unit. In this case, mental construction of sentences is driven by syntactic processing. Nevertheless, as long as the mental representation of a sentence is formed, it also has an associated phonological aspect. Previous psycholinguistic studies have already demonstrated that syntax has a significant impact on prosody perception (<xref ref-type="bibr" rid="bib9">Buxó-Lugo and Watson, 2016</xref>; <xref ref-type="bibr" rid="bib27">Garrett et al., 1966</xref>).</p><p>It is also possible that neural activity synchronous to linguistic units reflect more general cognitive processes that are engaged during linguistic processing. For example, within a word, later syllables are more predictable that earlier syllables. Therefore, neural processing associated with temporal prediction (<xref ref-type="bibr" rid="bib5">Breska and Deouell, 2017</xref>; <xref ref-type="bibr" rid="bib46">Lakatos et al., 2013</xref>; <xref ref-type="bibr" rid="bib71">Stefanics et al., 2010</xref>) may appear to be synchronous to the perceived linguistic units. However, it has been demonstrated that when the predictability of syllables is controlled as a constant, cortical activity can still synchronize to perceived artificial words, suggesting that temporal prediction is not the only factor driving low-frequency neural activity either (<xref ref-type="bibr" rid="bib13">Ding et al., 2016a</xref>). Nevertheless, in natural speech processing, temporal prediction may inevitably influence the low-frequency response. Similarly, temporal attention is known to affect low-frequency activity and attention certainly varies during speech perception (<xref ref-type="bibr" rid="bib1">Astheimer and Sanders, 2009</xref>; <xref ref-type="bibr" rid="bib34">Jin et al., 2018</xref>; <xref ref-type="bibr" rid="bib67">Sanders and Neville, 2003</xref>). In addition, low-frequency neural activity has also been suggested to reflect the perception of high-level rhythms (<xref ref-type="bibr" rid="bib58">Nozaradan et al., 2011</xref>) and general sequence chunking (<xref ref-type="bibr" rid="bib35">Jin et al., 2020</xref>).</p><p>Since multiple factors can drive low-frequency neural responses, the low-frequency response to natural speech is likely to be a mixture of multiple components, including, for example, auditory responses to acoustic prosodic features, neural activity related to temporal prediction and temporal attention, and neural activity encoding phonological, syntactic, or semantic information. These processes are closely coupled and one process can trigger other processes. Here we do not think the word-rate response exclusively reflects a single process. It may well consist of a mixture of multiple response components, or provide a reference signal to bind together mental representations of multiple dimensions of the same word. Similar to the binding of color and shape information in the perception of a visual object (<xref ref-type="bibr" rid="bib73">Treisman, 1998</xref>), the perception of a word requires the binding of, for example, phonological, syntactic, and semantic representations. It has been suggested that temporal coherence between neural responses to different features provides a strong cue to bind these features into a perceived object (<xref ref-type="bibr" rid="bib68">Shamma et al., 2011</xref>). It is possible that the word-rate response reflects the temporal coherence between distinct mental representations of each word and functionally relates to the binding of these representations.</p><p>In speech processing, multiple factors contribute to the word response and these factors interact. For example, the current study suggested that prosodic cues such as AM had a facilitative effect on word processing: It was observed that, during passive listening, a word response was observed for amplitude-modulated speech but not for isochronous speech. In addition, the word response to amplitude-modulated speech during passive listening exhibited stronger power in the replication experiment that only presented amplitude-modulated speech, compared with the results obtained in the original experiment that presented amplitude-modulated speech with isochronous speech in a mixed manner. Consistent with this finding, a larger number of participants reported the noticing of stories during passive listening in the replication experiment (see Materials and methods). All these results suggest that the 2-Hz AM, which provides an acoustic cue for the word rhythm, facilitates word processing during passive listening. This result is consistent with the idea that prosodic cues have a facilitative effect on speech comprehension (<xref ref-type="bibr" rid="bib24">Frazier et al., 2006</xref>; <xref ref-type="bibr" rid="bib28">Ghitza, 2017</xref>; <xref ref-type="bibr" rid="bib29">Ghitza, 2020</xref>; <xref ref-type="bibr" rid="bib30">Giraud and Poeppel, 2012</xref>).</p><p>Finally, it should be mentioned that we employed AM to manipulate the speech envelope, given that the speech envelope is one of the strongest cues to drive stimulus-synchronous cortical response. However, the AM is only a weak prosodic cue compared with other variables such as timing and pitch contour (<xref ref-type="bibr" rid="bib69">Shen, 1993</xref>). Furthermore, stress is strongly modulated by context and does not affect word recognition in Chinese (<xref ref-type="bibr" rid="bib19">Duanmu, 2001</xref>). Future studies are needed to characterize the modulation of language processing by different prosodic cues and investigate the modulatory effect across different languages.</p></sec><sec id="s3-3"><title>Attention modulation of cortical speech responses</title><p>It has been widely reported that cortical tracking of speech is strongly modulated by attention. Most previous studies demonstrate that in a complex auditory scene consisting of two speakers, attention can selectively enhance neural tracking of attended speech (<xref ref-type="bibr" rid="bib16">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="bib78">Zion Golumbic et al., 2013</xref>). These results strongly suggest that the auditory cortex can parse a complex auditory scene into auditory objects, for example, speakers, and separately represent each auditory object (<xref ref-type="bibr" rid="bib68">Shamma et al., 2011</xref>; <xref ref-type="bibr" rid="bib70">Shinn-Cunningham, 2008</xref>). When only one speech stream was presented, cross-modal attention can also modulate neural tracking of the speech envelope, but the effect is much weaker (<xref ref-type="bibr" rid="bib15">Ding et al., 2018</xref>; <xref ref-type="bibr" rid="bib39">Kong et al., 2014</xref>).</p><p>Consistent with previous findings, in the current study, the 4-Hz syllable response was also enhanced by cross-modal attention (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). The 2-Hz AM response power, however, was not significantly modulated by cross-modal attention (<xref ref-type="fig" rid="fig4">Figures 4D</xref> and <xref ref-type="fig" rid="fig5">5B</xref>), suggesting that attention did not uniformly enhance the processing of all features within the same speech stream. Given that the 2-Hz AM does not carry linguistic information, the result suggests that attention selectively enhances speech features relevant to speech comprehension. This result extends previous findings by showing that attention can differentially modulate different features within a speech stream.</p></sec><sec id="s3-4"><title>Time course of the neural response to words</title><p>The neurophysiological processes underlying speech perception has been extensively studied using ERPs (<xref ref-type="bibr" rid="bib25">Friederici, 2002</xref>). Early ERP components, such as the N1, mostly reflect auditory encoding of acoustic features, while late components can reflect higher-level processing of lexical, semantic, or syntactic processing (<xref ref-type="bibr" rid="bib25">Friederici, 2002</xref>; <xref ref-type="bibr" rid="bib26">Friederici, 2012</xref>; <xref ref-type="bibr" rid="bib67">Sanders and Neville, 2003</xref>). In the current study, for isochronous speech, response latency cannot be uniquely determined: Given that syllables are presented at a constant rate of 4 Hz, a response with latency T cannot be distinguished from responses with latency T ± 250 ms. The periodic pattern in the stimulus design enables accurate prediction of its timing in the brain, and therefore the responses observed could turn out to be predictive instead of reactive.</p><p>In natural speech, the responses to the two syllables in a disyllabic word differ in a late latency window of about 400 ms. This component is consistent with the latency of the N400 response, which can be observed when listening to either individual words or continuous speech (<xref ref-type="bibr" rid="bib7">Broderick et al., 2018</xref>; <xref ref-type="bibr" rid="bib44">Kutas and Federmeier, 2011</xref>; <xref ref-type="bibr" rid="bib45">Kutas and Hillyard, 1980</xref>; <xref ref-type="bibr" rid="bib66">Pylkkänen and Marantz, 2003</xref>; <xref ref-type="bibr" rid="bib65">Pylkkänen et al., 2002</xref>). A previous study on the neural responses to naturally spoken sentences has also shown that the initial syllable of an English word elicits larger N1 and N200–300 components than the word-medial syllable (<xref ref-type="bibr" rid="bib67">Sanders and Neville, 2003</xref>). A recent study also suggests that the word onset in natural speech elicits a response at ~100 ms latency (<xref ref-type="bibr" rid="bib6">Brodbeck et al., 2018</xref>). The current study, however, does not observe this early effect, and language difference might offer a potential explanation: In Chinese, a syllable generally equals a morpheme, while in English single syllables do not carry meaning in most cases. The 400 ms latency response observed in the current study is consistent with the hypothesis that the N400 is related to lexical processing (<xref ref-type="bibr" rid="bib25">Friederici, 2002</xref>; <xref ref-type="bibr" rid="bib44">Kutas and Federmeier, 2011</xref>). Besides, it is also possible that the second syllable in a disyllabic word elicits weaker N400 since it is more predictable than the first syllable (<xref ref-type="bibr" rid="bib43">Kuperberg et al., 2020</xref>; <xref ref-type="bibr" rid="bib48">Lau et al., 2008</xref>).</p><p>The difference between the ERPs evoked by the first and second syllables in disyllabic words was amplified by attention (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Furthermore, the ERP difference remained significant when participants’ attention was diverted away from the speech in the movie watching task, while the 2-Hz response in the power spectrum was no longer significant (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). These results are similar to the findings of a previous study (<xref ref-type="bibr" rid="bib15">Ding et al., 2018</xref>), in which no word-rate response peak in the EEG spectrum is observed in contrast to coherent word-rate response phase across participants in unattended listening task. Taken together, these results suggest that word-level processing occurs during the movie watching task, but the word-tracking response is rather weak.</p><p>In sum, the current study suggests that bottom-up acoustic cues and top-down linguistic knowledge separately contribute to neural construction of linguistic units in the processing of spoken narratives.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Sixty-eight participants (20–29 years old, mean age, 22.6 years; 37 females) took part in the EEG experiments. Thirty-four participants (19–26 years old, mean age, 22.5 years; 17 females) took part in a behavioral test to assess the naturalness of the stimuli. All participants were right-handed native Mandarin speakers, with no self-reported hearing loss or neurological disorders. The experimental procedures were approved by the Research Ethics Committee of the College of Medicine, Zhejiang University (2019–047). All participants provided written informed consent prior to the experiment and were paid.</p></sec><sec id="s4-2"><title>Stories</title><p>Twenty-eight short stories were constructed for the study. The stories were unrelated in terms of content and ranged from 81 to 143 in word count (107 words on average). In 21 stories, word onset was metrically organized and every other syllable was a word onset, and these stories were referred to as metrical stories (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In the other seven stories, word onset was not metrically organized and these stories were referred to as nonmetrical stories (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). In an ideal design, metrical stories should be constructed solely with disyllabic words to form a constant disyllabic word rhythm. Nevertheless, since it was difficult to construct such materials, the stories were constructed with disyllabic words and pairs of monosyllabic words. In other words, in between two disyllabic words, there must be an even number of monosyllabic words. After the stories were composed, the word boundaries within the stories were further parsed based on a Natural Language Processing (NLP) algorithm (<xref ref-type="bibr" rid="bib76">Zhang and Shang, 2019</xref>). The parsing result confirmed that every other syllable in the story (referred to as σ1 in <xref ref-type="fig" rid="fig1">Figure 1A</xref>) was the onset of a word. For the other syllables (referred to as σ2 in <xref ref-type="fig" rid="fig1">Figure 1A</xref>), 77% was the second syllable of a disyllabic word while 23% was a monosyllabic word.</p><p>Similarly, nonmetrical stories used as a control condition were composed with sentences with an even number of syllables. Nevertheless, no specific constraint was applied to word duration, and the odd terms in the syllable sequence were not always a word onset. Furthermore, in each sentence, it was required that at least one odd term was not a word onset.</p></sec><sec id="s4-3"><title>Speech</title><p>Each story was either synthesized as an isochronous sequence of syllables or naturally read by a human speaker.</p><sec id="s4-3-1"><title>Isochronous speech</title><p>All syllables were synthesized independently using the Neospeech synthesizer (<ext-link ext-link-type="uri" xlink:href="http://www.neospeech.com/">http://www.neospeech.com/</ext-link>, the male voice, Liang). The synthesized syllables were 75–354 ms in duration (mean duration 224 ms). All syllables were adjusted to 250 ms by truncation or padding silence at the end, following the procedure in <xref ref-type="bibr" rid="bib13">Ding et al., 2016a</xref>. The last 25 ms of each syllable were smoothed by a cosine window and all syllables were equalized in intensity. In this way, the syllables were presented at a constant rate of 4 Hz (<xref ref-type="fig" rid="fig1">Figure 1A and B</xref>). In addition, a silence gap lasting 500 ms, that is, the duration of two syllables, was inserted at the position of any punctuation, to facilitate story comprehension.</p></sec><sec id="s4-3-2"><title>Natural speech</title><p>The stories were read in a natural manner by a female speaker, who was not aware of the purpose of the study. In natural speech, syllables were not produced at a constant rate and the boundaries between syllables were labeled by professionals (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). The total duration of speech was 1122 s for the 21 metrical stories and 372 s for the seven nonmetrical stories. A behavioral test showed that most participants did not perceive any difference between the metrical and nonmetrical stories (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p></sec><sec id="s4-3-3"><title>Amplitude-modulated speech</title><p>Amplitude modulation (AM) was applied to isochronous speech to create a word-rate acoustic rhythm (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). In σ1-amplified condition, all σ1 syllables were amplified by a factor of 4. In σ2-amplified condition, all σ2 syllables were amplified by a factor of 4. Such 2-Hz AM was clearly perceivable but did not affect speech intelligibility, since sound intensity is a weak cue for stress (<xref ref-type="bibr" rid="bib77">Zhong et al., 2001</xref>) and stress does not affect word recognition in Mandarin Chinese (<xref ref-type="bibr" rid="bib19">Duanmu, 2001</xref>). A behavioral test suggested that when listening to amplitude-modulated speech, a larger number of participants perceived σ1-amplified speech as more natural than σ2-amplified speech (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p></sec></sec><sec id="s4-4"><title>Experimental procedures and tasks</title><sec id="s4-4-1"><title>Behavioral test</title><p>A behavioral test was conducted to assess the naturalness of the stimuli. The test was divided into two blocks. In block 1, the participants listened to a metrical story and a nonmetrical story read by a human speaker, which were presented in a pseudorandom order. The stories were randomly selected from the story set. Each story ranged from 53 to 66 s in duration. After listening to each story, the participants were asked to write a sentence to summarize the story and fill out a questionnaire. Block 2 was of the same procedure as block 1, except that the metrical and nonmetrical stories were replaced with σ1- and σ2-amplified speech.</p><p>In block 1, the first question in the questionnaire asked whether the two types of stories, a metrical and a nonmetrical story, showed any noticeable difference regardless of their content. Thirty-one participants (91%) reported no difference perceived, and the other three participants (9%) were asked to elaborate on the differences they detected. Two of them said the metrical story showed larger pitch variations, and the other said they were read with a different tone. Therefore, the vast majority of the participants did not notice any difference between the two types of stories read by a human speaker. A few participants noticed some differences in the intonation pattern but no participants reported the difference in word rhythm.</p><p>The second question was to check the naturalness of the stories read. Twenty-four participants (71%) reported that both the two types of stories were naturally read. Three participants (9%) commented that metrical story was not naturally read, and all of them attributed it to intonation. The rest seven participants (20%) thought neither types of stories were naturally read. The reasons reported included (1) exaggerated intonation (<italic>N</italic> = 2); (2) the speed and intonation pattern seemed uniform (<italic>N</italic> = 2); (3) lack of emotion (<italic>N</italic> = 2); and (4) the pitch went up at the end of each sentence (<italic>N</italic> = 1). In sum, most participants thought the stories were naturally read and only two participants (6%) commented on the uniformity of pace.</p><p>In block 2, only one question was asked. Participants had to compare the naturalness and accessibility of σ1-amplified speech or σ2-amplified speech. Fifteen participants (44%) perceived σ1-amplified speech as being more natural, two participants (6%) perceived the σ2-amplified speech as being more natural, and the rest 17 participants (50%) thought there was no difference in naturalness between the two conditions. In sum, relatively more participants thought the σ1-amplified speech was more natural.</p></sec></sec><sec id="s4-5"><title>EEG experiment</title><p>The study consisted of four EEG experiments. Experiments 1–3 involved 16 participants respectively, and Experiment 4 involved 20 participants. Experiments 1 and 4 both consisted of two blocks with stories presented in a randomized order within each block. In Experiments 2 and 3, all stories were presented in a randomized order in a single block.</p><sec id="s4-5-1"><title>Experiment 1</title><p>Synthesized speech was presented in Experiment 1. The experiment was divided into two blocks. In block 1, participants listened to isochronous speech, including seven metrical stories and seven nonmetrical stories. In block 2, the participants listened to amplitude-modulated speech, including 7 σ1-amplified stories and 7 σ2-amplified stories. All 14 stories presented in block 2 were metrical stories and did not overlap with the stories used in block 1. Participants were asked to keep their eyes closed while listening to the stories. After listening to each story, participants were required to answer three comprehension questions by giving oral responses. An experimenter recorded the responses and then pressed a key to continue the experiment. The next story was presented after an interval randomized between 1 and 2 s (uniform distribution) after the key press. The participants took a break between blocks.</p></sec><sec id="s4-5-2"><title>Experiment 2</title><p>Speech stimuli used in Experiment 2 were the same as those used in Experiment 1, while the task was different. The participants were asked to watch a silent movie (The Little Prince) with subtitles and ignored any sound during the experiment. The stories were presented ~5 min after the movie started to make sure that participants were already engaged in the movie watching task. The interval between stories was randomized between 1 and 2 s (uniform distribution). The movie was stopped after all 28 stories were presented. The experiment was followed up with questions on the awareness of stories being presented during the movie watching task, and 87.5% participants (<italic>N</italic> = 14) reported that they did not notice any story.</p></sec><sec id="s4-5-3"><title>Experiment 3</title><p>Experiment 3 used the same set of stories as those used in Experiment 1, but the stories were naturally read by a human speaker. The task was of the same design as that in Experiment 1. Participants listened to 21 metrical stories and seven nonmetrical stories. Participants took a break after every 14 stories.</p></sec><sec id="s4-5-4"><title>Experiment 4</title><p>Experiment 4 was designed to test whether the results based on amplitude-modulated speech was replicable in a different group of participants. All stories used in Experiment 4 were metrical stories and each story was presented once. In block 1, participants were asked to watch a silent movie (The Little Prince) with subtitles and ignored any sound during the task. Amplitude-modulated speech (5 σ1-amplified and 5 σ2-amplified stories) were presented ~5 min after the movie started. The interval between stories was randomized between 1 and 2 s (uniform distribution). Block 1 was followed up with questions on the awareness of stories being presented during the movie watching task, and 15% participants (<italic>N</italic> = 3) reported that they did not notice any story during the task. Note that the percentage of participants reporting no awareness of the presentation of stories was much lower in Experiment 4 than that in Experiment 2. A potential explanation was that Experiment 4 only presented amplitude-modulated speech and the consistent presence of word-rate acoustic cues facilitated word recognition. In Experiment 2, however, as amplitude-modulated speech was mixed with isochronous speech, the lack of consistent presence of AM cue diminished its effect. After block 1 was finished, the participants took a break.</p><p>In block 2, participants listened to amplitude-modulated speech (5 σ1-amplified stories and 5 σ2-amplified stories) with their eyes closed. At the end of each story, three comprehension questions were presented, and answers were to be given with oral responses. The experimenter recorded the responses and then pressed a key to continue the experiment. The next story was presented after an interval randomized between 1 and 2 s (uniform distribution) after the key press.</p></sec></sec><sec id="s4-6"><title>Data recording and preprocessing</title><p>Electroencephalogram (EEG) and electrooculogram (EOG) were recorded using a Biosemi ActiveTwo system. Sixty-four EEG electrodes were recorded. Two additional electrodes were placed at the left and right temples to record the horizontal EOG (right minus left), and two electrodes were placed above and below the right eye to record the vertical EOG (upper minus lower). Two additional electrodes were placed at the left and right mastoids and their average was used as the reference for EEG (<xref ref-type="bibr" rid="bib15">Ding et al., 2018</xref>). The EEG/EOG recordings were low-pass filtered below 400 Hz and sampled at 2048 Hz.</p><p>All preprocessing and analysis in this study were performed using MATLAB (The MathWorks, Natick, MA). The EEG recordings were down-sampled to 128 Hz, referenced to the average of mastoid recordings, and band-pass filtered between 0.8 Hz and 30 Hz using a linear-phase finite impulse response (FIR) filter (6 s Hamming window, −6 dB attenuation at the cut-off frequencies). A linear-phase FIR filter causes a constant time delay to the input. The delay equals to <italic>N</italic>/2, where <italic>N</italic> was the window length of the filter (<xref ref-type="bibr" rid="bib60">Oppenheim et al., 1997</xref>). The delay was compensated by removing the first <italic>N</italic>/2 samples in the filter output. To remove ocular artifacts in EEG, the horizontal and vertical EOG were regressed out using the least-squares method (<xref ref-type="bibr" rid="bib15">Ding et al., 2018</xref>). Occasional large artifacts in EEG/EOG, that is, samples with magnitude &gt;1 mV, were removed from the analysis (<xref ref-type="bibr" rid="bib34">Jin et al., 2018</xref>).</p></sec><sec id="s4-7"><title>Data analysis</title><sec id="s4-7-1"><title>Frequency-domain analysis</title><p>The EEG responses during the gap between sentences and the responses to the first two syllables of each sentence were removed from analysis to avoid the responses to sound onsets. The EEG responses to the rest of the sentence, that is, from the third syllable to the last syllable of each sentence, were then concatenated.</p><p>To further remove potential artifacts, the EEG responses were divided into 7 s trials (a total of 148 trials for Experiments 1–3, and 104 trials for Experiment 4), and we visually inspected all the trials and removed trials with identifiable artifacts. On average, 8.45 ± 3.20% trials were rejected in Experiment 1, 15.20% ± 3.97% trials were rejected in Experiment 2, 10.35% ± 1.53% trials were rejected in Experiment 3, and 12.9 ± 4.46% trials were rejected in Experiment 4.</p><p>Then, the response averaged over trials was transformed into the frequency-domain using Discrete Fourier transform (DFT) without any additional smoothing window. Therefore, the frequency resolution of the DFT analysis was 1/7 Hz. The response power, that is, the square of the magnitude of the Fourier coefficients, was grand averaged over EEG electrodes and participants. The phase of the Fourier coefficients were averaged using the circular mean (<xref ref-type="bibr" rid="bib23">Fisher, 1995</xref>). The 2-Hz phase difference between the σ1- and σ2-amplified conditions was averaged over participants in each electrode.</p></sec><sec id="s4-7-2"><title>Time-warping analysis</title><p>In natural speech used in Experiment 3, syllables were not produced at a constant rate, and therefore the responses to syllables and words were not frequency tagged. However, the neural response to natural speech could be time warped to simulate the response to isochronous speech (<xref ref-type="bibr" rid="bib34">Jin et al., 2018</xref>). In the time-warping analysis, we first extracted the ERP response to each syllable (from 0 to 750 ms), and simulated the response to 4-Hz isochronous speech using the following convolution procedure: <italic>s</italic>(<italic>t</italic>) = Σ<italic><sub>j</sub> h<sub>j</sub></italic>(<italic>t</italic>)*<italic>δ</italic>(<italic>t</italic> –0.25<italic>j</italic>), where <italic>s</italic>(<italic>t</italic>) was the time-warped response, <italic>δ</italic>(<italic>t</italic>) was the Dirac delta function, and <italic>h<sub>j</sub></italic>(<italic>t</italic>) was the ERP evoked by the <italic>j<sup>th</sup></italic> syllable. The word index <italic>j</italic> ranged from one to the total number of syllables in a story.</p><p>In the time-warping procedure, it was assumed that the syllable response was time-locked to the syllable onsets and the word response was time-locked to word onsets. The frequency-domain analysis was subsequently applied to the time-warped response, following the same procedure as adopted in the analysis of the response to isochronous speech.</p></sec><sec id="s4-7-3"><title>Time-domain analysis</title><p>Time-domain analysis was only applied to the responses to disyllabic words, and the responses to monosyllabic words were not analyzed. The ERP responses to the first and second syllables of each disyllabic word were separately extracted and averaged across all disyllabic words. The ERP response to each syllable was baseline correlated by subtracting the mean response in a 100 ms window before the syllable onset.</p></sec></sec><sec id="s4-8"><title>Statistical test</title><p>In the frequency-domain analysis, statistical tests were performed using bias-corrected and accelerated bootstrap (<xref ref-type="bibr" rid="bib20">Efron and Tibshirani, 1994</xref>). In the bootstrap procedure, data of all participants were resampled with replacement 10,000 times. To test the significance of the 2-Hz and 4-Hz peaks in the response spectrum (<xref ref-type="fig" rid="fig2">Figures 2A–E</xref>, <xref ref-type="fig" rid="fig3">3B and D</xref>, and <xref ref-type="fig" rid="fig4">4A and B</xref>), the response amplitude at the peak frequency was compared with the mean power of the neighboring four frequency bins (two bin on each side, one-sided comparison). If the response power at 2 Hz or 4 Hz was stronger than the mean power of the neighboring bins <italic>N</italic> times in the resampled data, the significance level could be calculated as (<italic>N</italic> + 1)/10,001 (<xref ref-type="bibr" rid="bib34">Jin et al., 2018</xref>).</p><p>When comparing the response power between conditions, the response power was always subtracted by the power averaged over four neighboring frequency bins (two on each side) to reduce the influence of background neural activity. A two-sided test was used to test the power difference between conditions within an experiment (solid black lines in <xref ref-type="fig" rid="fig3">Figure 3A and B</xref> and <xref ref-type="fig" rid="fig4">Figures 4E</xref> and <xref ref-type="fig" rid="fig5">5C</xref>; topography in <xref ref-type="fig" rid="fig4">Figures 4F</xref> and <xref ref-type="fig" rid="fig5">5D</xref>). If the response power was greater in one condition <italic>N</italic> times in the resampled data, the significance level could be calculated as (2<italic>N</italic> +1)/10,001. As to the power difference between experiments (dotted red lines in <xref ref-type="fig" rid="fig3">Figures 3A and B</xref> and <xref ref-type="fig" rid="fig4">Figure 4E</xref>), the significance level was <italic>v</italic> if the sample mean in one experiment exceeded the 100 <italic>v</italic>/2 percentile (or fell below the <italic>v</italic>/2 percentile) of the distribution of the sample mean in the other experiment (<xref ref-type="bibr" rid="bib15">Ding et al., 2018</xref>).</p><p>To test the phase difference between conditions, the <italic>V</italic>% confidence interval of the phase difference was measured by the smallest angle that could cover <italic>V</italic>% of the 10,000 resampled phase difference (<xref ref-type="bibr" rid="bib34">Jin et al., 2018</xref>). In the inter-participant phase coherence test (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), 10,000 phase coherence values were generated based on the null distribution, that is, a uniform distribution. If the actual phase coherence was smaller than <italic>N</italic> of the 10,000 phase coherence values generated based on the null distribution, its significance level was (<italic>N</italic> + 1)/10,001 (<xref ref-type="bibr" rid="bib15">Ding et al., 2018</xref>).</p><p>In the time-domain analysis (<xref ref-type="fig" rid="fig6">Figure 6</xref>), the significance of ERP difference between conditions was determined by means of the cluster-based permutation test (<xref ref-type="bibr" rid="bib54">Maris and Oostenveld, 2007</xref>). The test was performed with the following steps given below: (1) The ERP for each participant in two conditions were pooled into the same set. (2) The set was randomly partitioned into two equally sized subsets. (3) At each time point, the responses were compared between the two subsets using a paired t-test. (4) The significantly different data points in the responses were clustered based on temporal adjacency. (5) The cluster-level statistics were calculated by taking the sum over the t-values within each cluster. (6) Steps 2–5 were repeated 2000 times. The p-value was estimated as the proportion of partitions that resulted in a higher cluster-level statistic than the actual two conditions.</p><p>When multiple comparisons were performed, the p-value was adjusted using the false discovery rate (FDR) correction (<xref ref-type="bibr" rid="bib2">Benjamini and Hochberg, 1995</xref>).</p></sec><sec id="s4-9"><title>Post-hoc effect size calculation</title><p>On top of showing the 2-Hz response power from individual participants and individual electrodes in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, an effect size analysis was applied to validate that the sample size was appropriate to observe the 2-Hz response. To simplify the analysis, we calculated the effect size based on a paired t-test to compare the power at 2 Hz and the power averaged over four neighboring frequencies. Since the response power was not subject to a normal distribution, such a t-test had lower power than, for example, the bootstrap test. However, based on the t-test, the 2-Hz response remained significantly stronger than the mean response averaged over neighboring frequency bins in all conditions shown in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>. The effect size of the t-test was calculated using the G*Power software (version 3.1) (<xref ref-type="bibr" rid="bib22">Faul et al., 2007</xref>). We calculated <italic>d</italic> and power based on the mean and standard deviation of the 2-Hz response (reported in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>). The power was above 0.8 for all conditions, suggesting that the sample size was big enough even for the more conservative t-test.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Dr. Virginie van Wassenhove and three anonymous reviewers for their constructive comments. We thank Dr. Xunyi Pan, Dr. Lang Qin, Jiajie Zou, and Yuhan Lu for thoughtful comments on previous versions of the manuscript. The research was supported by National Natural Science Foundation of China 31771248 (ND), Major Scientific Research Project of Zhejiang Lab 2019KB0AC02 (ND), National Key R and D Program of China 2019YFC0118200 (ND), Zhejiang Provincial Natural Science Foundation of China LGF19H090020 (CL), and Fundamental Research Funds for the Central Universities 2020FZZX001-05 (ND).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Supervision, Validation, Visualization, Methodology, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The experimental procedures were approved by the Research Ethics Committee of the College of Medicine, Zhejiang University (2019-047). All participants provided written informed consent prior to the experiment and were paid.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="scode1"><label>Source code 1.</label><caption><title>The MATLAB code to process data in Experiments 1–3 and plot the results as displayed in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-60433-code1-v2.zip"/></supplementary-material><supplementary-material id="scode2"><label>Source code 2.</label><caption><title>The MATLAB code to process data in Experiments 1–3 and plot the results as displayed in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-60433-code2-v2.zip"/></supplementary-material><supplementary-material id="scode3"><label>Source code 3.</label><caption><title>The MATLAB code to process data in Experiments 1–3 and plot the results as displayed in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-60433-code3-v2.zip"/></supplementary-material><supplementary-material id="scode4"><label>Source code 4.</label><caption><title>The MATLAB code to process data in Experiment 4 and plot the results as displayed in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-60433-code4-v2.zip"/></supplementary-material><supplementary-material id="scode5"><label>Source code 5.</label><caption><title>The MATLAB code to process data in Experiments 1–3 and plot the results as displayed in <xref ref-type="fig" rid="fig6">Figure 6</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-60433-code5-v2.zip"/></supplementary-material><supplementary-material id="scode6"><label>Source code 6.</label><caption><title>MATLAB functions used in other source codes.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-60433-code6-v2.rar"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Assessment of the stimulus.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-60433-supp1-v2.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Post-hoc effect size calculation.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-60433-supp2-v2.docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Samples of the stimulus including isochronous speech, natural speech, σ1-amplified speech, and σ2-amplified speech.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-60433-supp3-v2.rar"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-60433-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The EEG data and analysis code (in MatLab) were uploaded as source data files.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Astheimer</surname> <given-names>LB</given-names></name><name><surname>Sanders</surname> <given-names>LD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Listeners modulate temporally selective attention during natural speech processing</article-title><source>Biological Psychology</source><volume>80</volume><fpage>23</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1016/j.biopsycho.2008.01.015</pub-id><pub-id pub-id-type="pmid">18395316</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname> <given-names>Y</given-names></name><name><surname>Hochberg</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blank</surname> <given-names>IA</given-names></name><name><surname>Fedorenko</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>No evidence for differences among language regions in their temporal receptive windows</article-title><source>NeuroImage</source><volume>219</volume><elocation-id>116925</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116925</pub-id><pub-id pub-id-type="pmid">32407994</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourguignon</surname> <given-names>M</given-names></name><name><surname>De Tiège</surname> <given-names>X</given-names></name><name><surname>de Beeck</surname> <given-names>MO</given-names></name><name><surname>Ligot</surname> <given-names>N</given-names></name><name><surname>Paquier</surname> <given-names>P</given-names></name><name><surname>Van Bogaert</surname> <given-names>P</given-names></name><name><surname>Goldman</surname> <given-names>S</given-names></name><name><surname>Hari</surname> <given-names>R</given-names></name><name><surname>Jousmäki</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The pace of prosodic phrasing couples the listener's cortex to the reader's voice</article-title><source>Human Brain Mapping</source><volume>34</volume><fpage>314</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1002/hbm.21442</pub-id><pub-id pub-id-type="pmid">22392861</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breska</surname> <given-names>A</given-names></name><name><surname>Deouell</surname> <given-names>LY</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural mechanisms of rhythm-based temporal prediction: Delta phase-locking reflects temporal predictability but not rhythmic entrainment</article-title><source>PLOS Biology</source><volume>15</volume><elocation-id>e2001665</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2001665</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname> <given-names>C</given-names></name><name><surname>Hong</surname> <given-names>LE</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rapid transformation from auditory to linguistic representations of continuous speech</article-title><source>Current Biology</source><volume>28</volume><fpage>3976</fpage><lpage>3983</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.10.042</pub-id><pub-id pub-id-type="pmid">30503620</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broderick</surname> <given-names>MP</given-names></name><name><surname>Anderson</surname> <given-names>AJ</given-names></name><name><surname>Di Liberto</surname> <given-names>GM</given-names></name><name><surname>Crosse</surname> <given-names>MJ</given-names></name><name><surname>Lalor</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Electrophysiological correlates of semantic dissimilarity reflect the comprehension of natural, narrative speech</article-title><source>Current Biology</source><volume>28</volume><fpage>803</fpage><lpage>809</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.01.080</pub-id><pub-id pub-id-type="pmid">29478856</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buiatti</surname> <given-names>M</given-names></name><name><surname>Pena</surname> <given-names>M</given-names></name><name><surname>Dehaenelambertz</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Investigating the neural correlates of continuous speech computation with frequency-tagged neuroelectric responses</article-title><source>NeuroImage</source><volume>44</volume><fpage>509</fpage><lpage>519</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.09.015</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buxó-Lugo</surname> <given-names>A</given-names></name><name><surname>Watson</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Evidence for the influence of syntax on prosodic parsing</article-title><source>Journal of Memory and Language</source><volume>90</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2016.03.001</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daube</surname> <given-names>C</given-names></name><name><surname>Ince</surname> <given-names>RAA</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Simple acoustic features can explain Phoneme-Based predictions of cortical responses to speech</article-title><source>Current Biology</source><volume>29</volume><fpage>1924</fpage><lpage>1937</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.04.067</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Destoky</surname> <given-names>F</given-names></name><name><surname>Philippe</surname> <given-names>M</given-names></name><name><surname>Bertels</surname> <given-names>J</given-names></name><name><surname>Verhasselt</surname> <given-names>M</given-names></name><name><surname>Coquelet</surname> <given-names>N</given-names></name><name><surname>Vander Ghinst</surname> <given-names>M</given-names></name><name><surname>Wens</surname> <given-names>V</given-names></name><name><surname>De Tiège</surname> <given-names>X</given-names></name><name><surname>Bourguignon</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Comparing the potential of MEG and EEG to uncover brain tracking of speech temporal envelope</article-title><source>NeuroImage</source><volume>184</volume><fpage>201</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.09.006</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname> <given-names>GM</given-names></name><name><surname>O'Sullivan</surname> <given-names>JA</given-names></name><name><surname>Lalor</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Low-Frequency cortical entrainment to speech reflects Phoneme-Level processing</article-title><source>Current Biology</source><volume>25</volume><fpage>2457</fpage><lpage>2465</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.030</pub-id><pub-id pub-id-type="pmid">26412129</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Melloni</surname> <given-names>L</given-names></name><name><surname>Zhang</surname> <given-names>H</given-names></name><name><surname>Tian</surname> <given-names>X</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>158</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1038/nn.4186</pub-id><pub-id pub-id-type="pmid">26642090</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name><name><surname>David</surname> <given-names>SV</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Encoding of natural sounds by variance of the cortical local field potential</article-title><source>Journal of Neurophysiology</source><volume>115</volume><fpage>2389</fpage><lpage>2398</lpage><pub-id pub-id-type="doi">10.1152/jn.00652.2015</pub-id><pub-id pub-id-type="pmid">26912594</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Pan</surname> <given-names>X</given-names></name><name><surname>Luo</surname> <given-names>C</given-names></name><name><surname>Su</surname> <given-names>N</given-names></name><name><surname>Zhang</surname> <given-names>W</given-names></name><name><surname>Zhang</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Attention is required for Knowledge-Based sequential grouping: insights from the integration of syllables into words</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>1178</fpage><lpage>1188</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2606-17.2017</pub-id><pub-id pub-id-type="pmid">29255005</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title><source>PNAS</source><volume>109</volume><fpage>11854</fpage><lpage>11859</lpage><pub-id pub-id-type="doi">10.1073/pnas.1205381109</pub-id><pub-id pub-id-type="pmid">22753470</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname> <given-names>KB</given-names></name><name><surname>Arnal</surname> <given-names>LH</given-names></name><name><surname>Ghitza</surname> <given-names>O</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual parsing</article-title><source>NeuroImage</source><volume>85</volume><fpage>761</fpage><lpage>768</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.06.035</pub-id><pub-id pub-id-type="pmid">23791839</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Doumas</surname> <given-names>LAA</given-names></name><name><surname>Martin</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Abstraction in time: finding hierarchical linguistic structure in a model of relational processing</article-title><conf-name>Conference Cognitive Science</conf-name><fpage>2279</fpage><lpage>2284</lpage></element-citation></ref><ref id="bib19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Duanmu</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Stress in Chinese</source><publisher-name>University of Michigan</publisher-name></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Efron</surname> <given-names>B</given-names></name><name><surname>Tibshirani</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>An Introduction to the Bootstrap</source><publisher-name>CRC press</publisher-name><pub-id pub-id-type="doi">10.1201/9780429246593</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Etard</surname> <given-names>O</given-names></name><name><surname>Reichenbach</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural speech tracking in the theta and in the Delta frequency band differentially encode clarity and comprehension of speech in noise</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>5750</fpage><lpage>5759</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1828-18.2019</pub-id><pub-id pub-id-type="pmid">31109963</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faul</surname> <given-names>F</given-names></name><name><surname>Erdfelder</surname> <given-names>E</given-names></name><name><surname>Lang</surname> <given-names>AG</given-names></name><name><surname>Buchner</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>G*power 3: a flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title><source>Behavior Research Methods</source><volume>39</volume><fpage>175</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.3758/BF03193146</pub-id><pub-id pub-id-type="pmid">17695343</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fisher</surname> <given-names>NI</given-names></name></person-group><year iso-8601-date="1995">1995</year><source>Statistical Analysis of Circular Data</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511564345</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frazier</surname> <given-names>L</given-names></name><name><surname>Carlson</surname> <given-names>K</given-names></name><name><surname>Clifton</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Prosodic phrasing is central to language comprehension</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>244</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.04.002</pub-id><pub-id pub-id-type="pmid">16651019</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friederici</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Towards a neural basis of auditory sentence processing</article-title><source>Trends in Cognitive Sciences</source><volume>6</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01839-8</pub-id><pub-id pub-id-type="pmid">15866191</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friederici</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The cortical language circuit: from auditory perception to sentence comprehension</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>262</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.04.001</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrett</surname> <given-names>M</given-names></name><name><surname>Bever</surname> <given-names>T</given-names></name><name><surname>Fodor</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>The active use of grammar in speech perception</article-title><source>Perception &amp; Psychophysics</source><volume>1</volume><fpage>30</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.3758/BF03207817</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghitza</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Acoustic-driven delta rhythms as prosodic markers</article-title><source>Language, Cognition and Neuroscience</source><volume>32</volume><fpage>545</fpage><lpage>561</lpage><pub-id pub-id-type="doi">10.1080/23273798.2016.1232419</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghitza</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>“Acoustic-driven oscillators as cortical pacemaker”: a commentary on Meyer, Sun &amp; Martin (2019)</article-title><source>Language, Cognition and Neuroscience</source><volume>35</volume><fpage>1100</fpage><lpage>1105</lpage><pub-id pub-id-type="doi">10.1080/23273798.2020.1737720</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname> <given-names>A-L</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>511</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goswami</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Speech rhythm and language acquisition: an amplitude modulation phase hierarchy perspective</article-title><source>Annals of the New York Academy of Sciences</source><volume>1453</volume><fpage>67</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1111/nyas.14137</pub-id><pub-id pub-id-type="pmid">31237357</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Yang</surname> <given-names>E</given-names></name><name><surname>Vallines</surname> <given-names>I</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name><name><surname>Rubin</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A hierarchy of temporal receptive windows in human cortex</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>2539</fpage><lpage>2550</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5487-07.2008</pub-id><pub-id pub-id-type="pmid">18322098</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname> <given-names>MF</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Discrimination of speech stimuli based on neuronal response phase patterns depends on acoustics but not comprehension</article-title><source>Journal of Neurophysiology</source><volume>104</volume><fpage>2500</fpage><lpage>2511</lpage><pub-id pub-id-type="doi">10.1152/jn.00251.2010</pub-id><pub-id pub-id-type="pmid">20484530</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname> <given-names>P</given-names></name><name><surname>Zou</surname> <given-names>J</given-names></name><name><surname>Zhou</surname> <given-names>T</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Eye activity tracks task-relevant structures during speech and auditory sequence perception</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>5374</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-07773-y</pub-id><pub-id pub-id-type="pmid">30560906</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname> <given-names>P</given-names></name><name><surname>Lu</surname> <given-names>Y</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Low-frequency neural activity reflects rule-based chunking during speech listening</article-title><source>eLife</source><volume>9</volume><elocation-id>e55613</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.55613</pub-id><pub-id pub-id-type="pmid">32310082</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname> <given-names>A</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Perceptually relevant speech tracking in auditory and motor cortex reflects distinct linguistic features</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2004473</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004473</pub-id><pub-id pub-id-type="pmid">29529019</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kerlin</surname> <given-names>JR</given-names></name><name><surname>Shahin</surname> <given-names>AJ</given-names></name><name><surname>Miller</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Attentional gain control of ongoing cortical speech representations in a &quot;cocktail party&quot;</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>620</fpage><lpage>628</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3631-09.2010</pub-id><pub-id pub-id-type="pmid">20071526</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiebel</surname> <given-names>SJ</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A hierarchy of time-scales and the brain</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000209</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000209</pub-id><pub-id pub-id-type="pmid">19008936</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kong</surname> <given-names>Y-Y</given-names></name><name><surname>Mullangi</surname> <given-names>A</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Differential modulation of auditory responses to attended and unattended speech in different listening conditions</article-title><source>Hearing Research</source><volume>316</volume><fpage>73</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2014.07.009</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kösem</surname> <given-names>A</given-names></name><name><surname>Basirat</surname> <given-names>A</given-names></name><name><surname>Azizi</surname> <given-names>L</given-names></name><name><surname>van Wassenhove</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>High-frequency neural activity predicts word parsing in ambiguous speech streams</article-title><source>Journal of Neurophysiology</source><volume>116</volume><fpage>2497</fpage><lpage>2512</lpage><pub-id pub-id-type="doi">10.1152/jn.00074.2016</pub-id><pub-id pub-id-type="pmid">27605528</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kösem</surname> <given-names>A</given-names></name><name><surname>van Wassenhove</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Distinct contributions of low- and high-frequency neural oscillations to speech comprehension</article-title><source>Language, Cognition and Neuroscience</source><volume>32</volume><fpage>536</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1080/23273798.2016.1238495</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koskinen</surname> <given-names>M</given-names></name><name><surname>Seppä</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Uncovering cortical MEG responses to listened audiobook stories</article-title><source>NeuroImage</source><volume>100</volume><fpage>263</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.06.018</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuperberg</surname> <given-names>GR</given-names></name><name><surname>Brothers</surname> <given-names>T</given-names></name><name><surname>Wlotko</surname> <given-names>EW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A tale of two positivities and the N400: distinct neural signatures are evoked by confirmed and violated predictions at different levels of representation</article-title><source>Journal of Cognitive Neuroscience</source><volume>32</volume><fpage>12</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01465</pub-id><pub-id pub-id-type="pmid">31479347</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname> <given-names>M</given-names></name><name><surname>Federmeier</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (ERP)</article-title><source>Annual Review of Psychology</source><volume>62</volume><fpage>621</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id><pub-id pub-id-type="pmid">20809790</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname> <given-names>M</given-names></name><name><surname>Hillyard</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Reading senseless sentences: brain potentials reflect semantic incongruity</article-title><source>Science</source><volume>207</volume><fpage>203</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1126/science.7350657</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname> <given-names>P</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name><name><surname>Leitman</surname> <given-names>DI</given-names></name><name><surname>Javitt</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Predictive suppression of cortical excitability and its deficit in schizophrenia</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>11692</fpage><lpage>11702</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0010-13.2013</pub-id><pub-id pub-id-type="pmid">23843536</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lalor</surname> <given-names>EC</given-names></name><name><surname>Power</surname> <given-names>AJ</given-names></name><name><surname>Reilly</surname> <given-names>RB</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Resolving precise temporal processing properties of the auditory system using continuous stimuli</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>349</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1152/jn.90896.2008</pub-id><pub-id pub-id-type="pmid">19439675</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname> <given-names>EF</given-names></name><name><surname>Phillips</surname> <given-names>C</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A cortical network for semantics: (de)constructing the N400</article-title><source>Nature Reviews Neuroscience</source><volume>9</volume><fpage>920</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1038/nrn2532</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lerner</surname> <given-names>Y</given-names></name><name><surname>Honey</surname> <given-names>CJ</given-names></name><name><surname>Silbert</surname> <given-names>LJ</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Topographic mapping of a hierarchy of temporal receptive windows using a narrated story</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>2906</fpage><lpage>2915</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3684-10.2011</pub-id><pub-id pub-id-type="pmid">21414912</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>W</given-names></name><name><surname>Yang</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Perception of prosodic hierarchical boundaries in Mandarin Chinese sentences</article-title><source>Neuroscience</source><volume>158</volume><fpage>1416</fpage><lpage>1425</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2008.10.065</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>H</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex</article-title><source>Neuron</source><volume>54</volume><fpage>1001</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.004</pub-id><pub-id pub-id-type="pmid">17582338</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mai</surname> <given-names>G</given-names></name><name><surname>Minett</surname> <given-names>JW</given-names></name><name><surname>Wang</surname> <given-names>WS-Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Delta, theta, beta, and gamma brain oscillations index levels of auditory sentence processing</article-title><source>NeuroImage</source><volume>133</volume><fpage>516</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.02.064</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makov</surname> <given-names>S</given-names></name><name><surname>Sharon</surname> <given-names>O</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Ben-Shachar</surname> <given-names>M</given-names></name><name><surname>Nir</surname> <given-names>Y</given-names></name><name><surname>Zion Golumbic</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sleep disrupts High-Level speech parsing despite significant basic auditory processing</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>7772</fpage><lpage>7781</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0168-17.2017</pub-id><pub-id pub-id-type="pmid">28626013</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname> <given-names>L</given-names></name><name><surname>Sun</surname> <given-names>Y</given-names></name><name><surname>Martin</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Synchronous, but not entrained: exogenous and endogenous cortical rhythms of speech and language processing</article-title><source>Language, Cognition and Neuroscience</source><volume>35</volume><fpage>1089</fpage><lpage>1099</lpage><pub-id pub-id-type="doi">10.1080/23273798.2019.1693050</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname> <given-names>L</given-names></name><name><surname>Gumbert</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Synchronization of electrophysiological responses with speech benefits syntactic information processing</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>1066</fpage><lpage>1074</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01236</pub-id><pub-id pub-id-type="pmid">29324074</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nourski</surname> <given-names>KV</given-names></name><name><surname>Reale</surname> <given-names>RA</given-names></name><name><surname>Oya</surname> <given-names>H</given-names></name><name><surname>Kawasaki</surname> <given-names>H</given-names></name><name><surname>Kovach</surname> <given-names>CK</given-names></name><name><surname>Chen</surname> <given-names>H</given-names></name><name><surname>Howard</surname> <given-names>MA</given-names></name><name><surname>Brugge</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Temporal envelope of time-compressed speech represented in the human auditory cortex</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>15564</fpage><lpage>15574</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3065-09.2009</pub-id><pub-id pub-id-type="pmid">20007480</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname> <given-names>S</given-names></name><name><surname>Peretz</surname> <given-names>I</given-names></name><name><surname>Missal</surname> <given-names>M</given-names></name><name><surname>Mouraux</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Tagging the neuronal entrainment to beat and meter</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>10234</fpage><lpage>10240</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0411-11.2011</pub-id><pub-id pub-id-type="pmid">21753000</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Sullivan</surname> <given-names>JA</given-names></name><name><surname>Power</surname> <given-names>AJ</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Rajaram</surname> <given-names>S</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name><name><surname>Slaney</surname> <given-names>M</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name><name><surname>Lalor</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attentional selection in a cocktail party environment can be decoded from Single-Trial EEG</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>1697</fpage><lpage>1706</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht355</pub-id><pub-id pub-id-type="pmid">24429136</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Oppenheim</surname> <given-names>AV</given-names></name><name><surname>Willsky</surname> <given-names>AS</given-names></name><name><surname>Nawab</surname> <given-names>SH</given-names></name></person-group><year iso-8601-date="1997">1997</year><source>Signals and Systems</source><publisher-name>Prentice Hall</publisher-name></element-citation></ref><ref id="bib61"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ostarek</surname> <given-names>M</given-names></name><name><surname>Alday</surname> <given-names>PM</given-names></name><name><surname>Gawel</surname> <given-names>O</given-names></name><name><surname>Wolfgruber</surname> <given-names>J</given-names></name><name><surname>Knudsen</surname> <given-names>B</given-names></name><name><surname>Mantegna</surname> <given-names>F</given-names></name><name><surname>Huettig</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Oscillatory responses to generated and perceived rhythms</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.11.19.390062</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname> <given-names>JE</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Phase-locked responses to speech in human auditory cortex are enhanced during comprehension</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>1378</fpage><lpage>1387</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs118</pub-id><pub-id pub-id-type="pmid">22610394</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peña</surname> <given-names>M</given-names></name><name><surname>Melloni</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Brain oscillations during spoken sentence processing</article-title><source>Journal of Cognitive Neuroscience</source><volume>24</volume><fpage>1149</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00144</pub-id><pub-id pub-id-type="pmid">21981666</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Assaneo</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Speech rhythms and their neural foundations</article-title><source>Nature Reviews Neuroscience</source><volume>21</volume><fpage>322</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0304-4</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pylkkänen</surname> <given-names>L</given-names></name><name><surname>Stringfellow</surname> <given-names>A</given-names></name><name><surname>Marantz</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Neuromagnetic evidence for the timing of lexical activation: an MEG component sensitive to phonotactic probability but not to neighborhood density</article-title><source>Brain and Language</source><volume>81</volume><fpage>666</fpage><lpage>678</lpage><pub-id pub-id-type="doi">10.1006/brln.2001.2555</pub-id><pub-id pub-id-type="pmid">12081430</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pylkkänen</surname> <given-names>L</given-names></name><name><surname>Marantz</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Tracking the time course of word recognition with MEG</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>187</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(03)00092-5</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanders</surname> <given-names>LD</given-names></name><name><surname>Neville</surname> <given-names>HJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>An ERP study of continuous speech processing. I. segmentation, semantics, and syntax in native speakers</article-title><source>Brain Research. Cognitive Brain Research</source><volume>15</volume><fpage>228</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1016/s0926-6410(02)00195-7</pub-id><pub-id pub-id-type="pmid">12527097</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamma</surname> <given-names>SA</given-names></name><name><surname>Elhilali</surname> <given-names>M</given-names></name><name><surname>Micheyl</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Temporal coherence and attention in auditory scene analysis</article-title><source>Trends in Neurosciences</source><volume>34</volume><fpage>114</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2010.11.002</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname> <given-names>XS</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Relative duration as a perceptual cue to stress in mandarin</article-title><source>Language and Speech</source><volume>36</volume><fpage>415</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1177/002383099303600404</pub-id><pub-id pub-id-type="pmid">8072347</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Object-based auditory and visual attention</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>182</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.02.003</pub-id><pub-id pub-id-type="pmid">18396091</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stefanics</surname> <given-names>G</given-names></name><name><surname>Hangya</surname> <given-names>B</given-names></name><name><surname>Hernádi</surname> <given-names>I</given-names></name><name><surname>Winkler</surname> <given-names>I</given-names></name><name><surname>Lakatos</surname> <given-names>P</given-names></name><name><surname>Ulbert</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Phase entrainment of human Delta oscillations can mediate the effects of expectation on reaction speed</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>13578</fpage><lpage>13585</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0703-10.2010</pub-id><pub-id pub-id-type="pmid">20943899</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinhauer</surname> <given-names>K</given-names></name><name><surname>Alter</surname> <given-names>K</given-names></name><name><surname>Friederici</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Brain potentials indicate immediate use of prosodic cues in natural speech processing</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>191</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1038/5757</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treisman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Feature binding, attention and object perception</article-title><source>Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</source><volume>353</volume><fpage>1295</fpage><lpage>1306</lpage><pub-id pub-id-type="doi">10.1098/rstb.1998.0284</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanthornhout</surname> <given-names>J</given-names></name><name><surname>Decruy</surname> <given-names>L</given-names></name><name><surname>Wouters</surname> <given-names>J</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name><name><surname>Francart</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Speech intelligibility predicted from neural entrainment of the speech envelope</article-title><source>Journal of the Association for Research in Otolaryngology</source><volume>19</volume><fpage>181</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1007/s10162-018-0654-z</pub-id><pub-id pub-id-type="pmid">29464412</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>Y</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Ahmar</surname> <given-names>N</given-names></name><name><surname>Xiang</surname> <given-names>J</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Sensitivity to temporal modulation rate and spectral bandwidth in the human auditory system: MEG evidence</article-title><source>Journal of Neurophysiology</source><volume>107</volume><fpage>2033</fpage><lpage>2041</lpage><pub-id pub-id-type="doi">10.1152/jn.00310.2011</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>H</given-names></name><name><surname>Shang</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>NLPIR-Parser: an intelligent semantic analysis toolkit for big data</article-title><source>Corpus Linguistics</source><volume>6</volume><fpage>87</fpage><lpage>104</lpage></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhong</surname> <given-names>X</given-names></name><name><surname>Wang</surname> <given-names>B</given-names></name><name><surname>Yang</surname> <given-names>Y</given-names></name><name><surname>Lv</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The perception of prosodic word stress in standard Chinese</article-title><source>Acta Psychologica Sinica</source><volume>033</volume><fpage>481</fpage><lpage>488</lpage></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zion Golumbic</surname> <given-names>EM</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Bickel</surname> <given-names>S</given-names></name><name><surname>Lakatos</surname> <given-names>P</given-names></name><name><surname>Schevon</surname> <given-names>CA</given-names></name><name><surname>McKhann</surname> <given-names>GM</given-names></name><name><surname>Goodman</surname> <given-names>RR</given-names></name><name><surname>Emerson</surname> <given-names>R</given-names></name><name><surname>Mehta</surname> <given-names>AD</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Mechanisms underlying selective neuronal tracking of attended speech at a &quot;cocktail party&quot;</article-title><source>Neuron</source><volume>77</volume><fpage>980</fpage><lpage>991</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.12.037</pub-id><pub-id pub-id-type="pmid">23473326</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoefel</surname> <given-names>B</given-names></name><name><surname>VanRullen</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>EEG oscillations entrain their phase to high-level features of speech sound</article-title><source>NeuroImage</source><volume>124</volume><fpage>16</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.08.054</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zou</surname> <given-names>J</given-names></name><name><surname>Feng</surname> <given-names>J</given-names></name><name><surname>Xu</surname> <given-names>T</given-names></name><name><surname>Jin</surname> <given-names>P</given-names></name><name><surname>Luo</surname> <given-names>C</given-names></name><name><surname>Zhang</surname> <given-names>J</given-names></name><name><surname>Pan</surname> <given-names>X</given-names></name><name><surname>Chen</surname> <given-names>F</given-names></name><name><surname>Zheng</surname> <given-names>J</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Auditory and language contributions to neural encoding of speech features in noisy environments</article-title><source>NeuroImage</source><volume>192</volume><fpage>66</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.02.047</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.60433.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>van Wassenhove</surname><given-names>Virginie</given-names></name><role>Reviewing Editor</role><aff><institution>CEA, DRF/I2BM, NeuroSpin; INSERM, U992, Cognitive Neuroimaging Unit</institution><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p>Thank you for submitting your article &quot;Delta-band Cortical Tracking of Acoustic and Linguistic Features in Natural Spoken Narratives&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Andrew King as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, when editors judge that a submitted work as a whole belongs in <italic>eLife</italic> but that some conclusions require a modest amount of additional new data, as they do with your paper, we are asking that the manuscript be revised to either limit claims to those supported by data in hand, or to explicitly state that the relevant conclusions require additional supporting data.</p><p>Our expectation is that the authors will eventually carry out the additional experiments and report on how they affect the relevant conclusions either in a preprint on bioRxiv or medRxiv, or if appropriate, as a Research Advance in <italic>eLife</italic>, either of which would be linked to the original paper.</p><p>Summary:</p><p>The acoustics of human speech convey complex linguistic hierarchical structures. While several studies have demonstrated that cortical activity shows speech-following responses, the extent to which these responses track acoustic features or linguistic units remains highly debated. Delta-band activity has been reported to track prosody, syntactic phrasing as well as temporal predictions. In this new study comprised of three EEG experiments, Luo and colleagues tested the cortical responses to the presentation of spoken stories which could be metrically organized or not, and read via synthesized isochronous or delta (~2Hz) amplitude modulated isochronous speech, or by a natural reader. The authors argue that delta synchronizes to multisyllabic compound words, favoring a word-based rather than a syllabic-based speech tracking response.</p><p>Revisions for this paper:</p><p>1) The Materials and methods section and the figures need to be clearly documented and annotated. Reviewer 1 raises some concerns about the loose use of &quot;phase&quot; and some of the methods that are not clearly incorporated in the Results to help the more general audience follow the subtleties of the analyses. I also leave the full set of reviews to guide such revision – as well as edit the English when needed.</p><p>2) Tempering the claim that delta uniquely tracks word rhythm.</p><p>All reviewers provide detailed comments pointing out to additional literature that could be considered and which can feed the discussion regarding syntactic vs. semantic vs. prosodic dissociations…</p><p>Clarifying the specificities reported in the literature should also help the authors address serious comments raised by reviewer 1 and 3, who are particularly hesitant regarding the strong claim on word-specificity of the delta response – including the lack of changes in the phase of delta oscillations. Reviewer 2 also stresses this point. The legitimacy of calling the low-frequency response delta is questioned as opposed to a cautionary possible mixture of network activity as suggested for instance by reviewer 3.</p><p>All three reviewers further question the description of delta phase considering the quantifications (e.g. well-known effect of the reference) and the observable topographical changes in Figure 3.</p><p>As a result of these major considerations, the authors should temper the strong claims that are currently being put forward.</p><p>Revisions expected in follow-up work:</p><p>3) While the study provides an interesting means for parametric manipulation of amplitude, reviewer 2 raised an interesting point regarding the effect of prosody in that frequency range. As expressed by reviewer 2: &quot;no only the amplitude of pitch changes is critical to the perception of prosodic events, but also the timing and rise/fall slopes. All this is not linearly affected by increasing the volume at 2 Hz. To show that delta-band oscillations are not affected by stimulus prosody would require a more systematic dedication to the different linguistic dimension of prosody, amplitude being only one.&quot;</p><p><italic>Reviewer #1:</italic></p><p>In a series of three EEG experiments, Luo and colleagues tested whether the low-frequency following response to auditory speech was elicited by linguistic or acoustic features. The authors modulated the task (story comprehension vs. passive video watching) and the acoustic structure of the speech signals using (non-)metrical, isochronous, and 2Hz amplitude modulated speech, or natural speech. The authors mainly used spectral quantification methods of the following responses and some phase analyses. The authors summarize their findings as illustrating that the delta band synchronizes to multisyllabic words, favoring a word-based following response as compared to a syllabic-based speech tracking response.</p><p>My main concerns about the study stem from the methods and a difficulty to straightforwardly understand the procedure.</p><p>For instance:</p><p>– Can the authors clarify the choice of band-pass filtering and the compensation that was applied as briefly stated &quot;and the delay caused by the filter was compensated.&quot;</p><p>– How did the authors insure that concatenated responses did not introduce low-frequency noise in their analysis?</p><p>– Subsection “Frequency domain analysis”: can the authors report the number of participants excluded by the analysis?</p><p>I also have substantial difficulties regarding the following methodological aspects:</p><p>– Bootstrap analyses artificially increase the number of samples while maintaining constant the variance of the samples. How did the authors take this into account in their final statistical comparisons?</p><p>– Figure 1D and predictions on the (absolute? relative? Instantaneous?) phase-locked activity to word onset and syllable need to be better and more precisely explained. For instance, there is no a priori reason to believe that 0° and 180° owe to be the actual expected values as clearly illustrated in Figure 3?</p><p>– While a time-warping analysis is being performed for data collected in Experiment 3, no mention of this approach is being explained in more details (as to possible interpretational limits) in the actual Results section.</p><p>I am here particularly concerned about the phase analysis which is not being sufficiently reported in details to clearly understand the results and the claims of the authors.</p><p>In line with this, I highlight two additional issues:</p><p>– Subsection “Response phase at 2 Hz”, Figure 4: the authors argue that attention may explain the lack of phase differences. Additionally, the authors seem to assume in their working hypothesis that the phase response should be comparable across participants.</p><p>In a recent study (Kösem et al., 2016), no evidence for phase-related linguistic parsing in low-frequency activity was found when participants actively or passively listened to a stationary acoustic speech sequence whose perceptual interpretation could alternate. Are these findings compatible with those illustrated in Figure 4? Furthermore, some important inter-individual differences were reported, which could severely impact the primary assumption of phase-consistency across individuals.</p><p>– Have the authors considered the possibility that the choice of EEG reference may largely affect the reported mean instantaneous phase responses and if so, could they control for it?</p><p><italic>Reviewer #2:</italic></p><p>The authors report an n = 48 EEG study aiming at dissociating the previously proposed roles of delta-band oscillations in the processing of speech acoustics and symbolic linguistic information. The authors report that delta-band activity is related more strongly to linguistic rather than acoustic information.</p><p>This is a welcome study, given that by now, the literature contains claims of delta-band oscillations being involved in (1) prosody tracking, (2) generation of linguistic structures / syntactic phrasing, (3) processing of linguistic information content, (4) temporal prediction in- and outside of the linguistic domain.</p><p>The study is well-written and I am optimistic that it will provide a good contribution to the existing literature.</p><p>I find the framing and discussion a bit gappy and I missed multiple lines of related research. For instance, the literature on the relationship between delta-band oscillations and prosody is not mentioned in the Introduction; likewise, the literature on temporal prediction versus linguistic prediction is not discussed well. In addition, I also think that this study is <italic>not</italic> as definitive as the authors claim, resulting from the choice of unnatural (i.e., isochronous) stimuli that, strangely enough, the authors claim to be natural.</p><p>Below, I given some comments for the authors to further improve their interesting work.</p><p>1) False claim of studying natural speech: The authors artificially induce unnatural stimulus rhythms as a means to frequency-tag their stimuli and perform frequency-domain analysis. While this is an interesting approach, this results in stimuli that do not have ecological validity. Using unnatural stimuli with artificial periodicity to propose periodic electrophysiological processing of words is not scientifically sound. In natural speech, words do <italic>not</italic> occur periodically. Why assume then that they are processed by a periodic oscillatory mechanism? While the frequency-tagging approach inherently provides good SNR, it cannot claim ecological validity for anything above phonemes or syllables, as for these, there are no linguistic corpus studies out there that show periodicity of the constructs to start with. Specific comment:</p><p>Subsection “Neural tracking of words in natural spoken narratives”: &quot;natural&quot; The authors artificially make natural speech unnatural by making it isochronous. It is bizarre to still call the resulting stimulus &quot;natural&quot;. Isochronous word sequences are certainly not natural speech. Hence, the authors can also not claim that they demonstrate cortical tracking of natural speech. To show this, they would have to show that there is speech-brain synchronicity for the non-isochronous condition. Certainly, to do so, the authors cannot use their standard spectral-peak analysis, but would have to show coherence or phase-locking against a different baseline (e.g., scrambled, autocorrelation-preserving speech). I strongly encourage the authors to do this analysis instead of pretending that isochronous word sequences are <italic>natural</italic>. The more general problem with this is that it adds to literature claiming that oscillations track words in human speech processing, which cannot possibly be true in this simplistic version: words are simply too variable in duration and timing.</p><p>2) Depth of framing and discussion: (a) Introduction is not specific enough to speech prosody versus the different types of results on linguistic information (e.g., syntactic structure / phrasing, syntactic and lexical entropy), including reference to prior work; also, relevant references that suggested delta-band oscillations to serve both bottom-up (i.e., prosody) and top-down (i.e., syntax, prediction) functionalities are not discussed in the Introduction; (b) interpretation of the results in terms of the different proposed functionalities of the delta-band lacks depth and differentiation.</p><p>Introduction: What is missing here is mentioning that delta-band oscillations have also been implied in prosody (e.g., Bourguignon et al., 2013). Relatedly, the idea that delta-band oscillations are related both to slow-fluctuating linguistic representations (e.g., phrases; Meyer and Gumbert, 2018; Meyer et al., 2016; Weissbart et al., 2019) and prosody has been put forward various times (e.g., Ghitza, 2020; Meyer et al., 2016). I think that mentioning this would help the framing of the current manuscript: (1) delta = symbolic linguistic units, (2) delta = prosody, (3) current study: Is delta reflecting symbolic processing or prosody?</p><p>Subsection “Time course of ERP response to words”: As to prediction and the delta-band, please consider discussing Weissbart et al., 2019, Meyer and Gumbert, 2018, Breska and Deouell, 2017, Lakatos et al., 2013, Roehm et al., 2009; in particular the relationship between linguistic predictions and the delta-band should be discussed shortly here. In general, there is need in the field to characterize the relationship between temporal prediction, prosody, linguistic units, and the delta-band. Just noting all this would be fine here.</p><p>&quot;semantic processing&quot; Why semantics? See my above comment on the relationship between the delta-band and temporal predictions (e.g., Roehm et al., 2009; Stefanics et al., 2010). In addition, please consider the recent reconceptualisation of the N400-P600 complex in terms of predictive coding (e.g., Kuperberg et al., 2019).</p><p>3) EEG preprocessing / report thereof does not accord to standards in the field. Apparently, the EEG data have not been cleaned from artifacts except for calculating the residual between the EEG and the EOG. Hence, I cannot assume that the data are clean enough to allow for statistical analysis. Specific point:</p><p>Materials and methods: The EEG-cleaning section is very sparse. With standard EEG and standard cleaning, rejection rates for adult EEG are in the other of 25-35 %. It am highly doubtful that these EEG data have been cleaned in an appropriate way. There is no mentioning of artifact removal, amount of removed data segments, time- and frequency-domain aspects that were considered for artifact definition. What toolboxes were used, what algorithms?</p><p>4) Terminology: Like many people in the field, the authors speak of “tracking” of linguistic information. To be clear: While it is fine that the field understands the problems of the term “entrainment”, now opting for “tracking”, it does not make much sense to use the combination of “tracking” and “linguistic units”. This is because linguistic units are not in the stimulus; instead they are only present in the brain as such. They cannot be tracked; instead, they need to be “associated”, “inferred”, or even “generated”. If one were to assume that the brain “tracks” linguistic units, this would mean that the brain tracks itself. Specific comments:</p><p>&quot;cortical activity tracking of higher-level linguistic units&quot; Linguistic units, such as &quot;words and phrases&quot; cannot be tracked, as they are not present in the stimulus. They are purely symbolic and they must be inferred. They cannot be sensed. Instead, they are present only in the mind and electrophysiology of the listener. Linguistic units are activated in memory or generated in some form. They are internal representations or states. If one were to say that the brain tracks these in some way, one would say that the brain tracks its own internal representations / states. Try and rephrase, please.</p><p>&quot;linguistic information&quot; cannot be extracted from acoustic features. Instead, linguistic information (e.g., distinctive features in phonology, semantic meaning, part-of-speech labels) must be associated with a given acoustic stimulus by an associative / inferential cognitive process. When we hear some acoustic stimulus, we associate with something linguistic. Just as in my above comment on the loose use of the term “tracking”, I suggest the authors to change this wording as well. A suggestion for a better phrasing would be &quot;Linguistic information is retrieved / constructed / generated when specific acoustic information is encountered / recognized…&quot;. Or &quot;Speech comprehension involves the active inference of linguistic structure from speech acoustics&quot;.</p><p>Discussion: &quot;tracks&quot; see my above comment.</p><p>5) Potential differences between the σ1 and 2 conditions: There are apparent differences in the number of sensors that show sensitivity to the amplitude modulation. This is not captured in the current statistical analysis. Still, given these drastic differences, the claim that acoustics of speech prosody do not affect delta-band amplitude is certainly much too strong and may just result from a lack of analysis (i.e., overall phase / ERP rather than sensor-by-sensor analysis). See specific comment next:</p><p>&quot;or the phase of the 2-Hz amplitude modulation&quot; I disagree with excitement and a happy suggestion that the authors must consider. Look, the plots (Figure 3, σ1/2) may not show differences in the phase angle. Yet, what they may point to is a dimension of the data that the authors have not assessed yet: topographical spread. From simply eyeballing it is clear that the number of significant electrodes differs <italic>drastically</italic> between the σ1 and σ2 conditions. This pattern is interesting and should be examined more closely. Is word-initial stress more natural in Chinese? If so, this could be interpreted in terms of some influence of native prosodic structure: Amplitude modulation does appear affect the amount / breadth of EEG signal (i.e. number of sensors), but only if it conforms to the native stress pattern? Not only words / phrases are part of a speakers linguistic knowledge, but also the stress pattern of their language. I urge the authors to look into this more closely.</p><p>&quot;The response&quot; see my above comment on the differences in the number significant sensors in the σ1 and 2 conditions.</p><p><italic>Reviewer #3:</italic></p><p>This manuscript presents research aimed at examining how cortical activity tracks with the acoustic and linguistic features of speech. In particular, the authors are interested in how rhythms in cortical data track with the timing of the acoustic envelope of speech and how that might reflect and, indeed, dissociate, from the tracking of the linguistic content. The authors collect EEG from subjects as they listen to speech that is presented in a number of different ways. This includes conditions where: the speech is rhythmic and the first syllable of every word appears at regular intervals; the speech is rhythmic but the first syllable of every word does not appear at regular intervals; the speech appears with a more natural rhythm with some approximate regularity to the timing of syllables (that allows for an analysis based on time warping); and some rhythmic speech that is amplitude modulated to either emphasize the first or second syllables. And they also present these stimuli when subjects are tasked with attending to the speech and when subjects are engaged in watching a silent video. The authors analyze the EEG from these different conditions in the frequency domain (using time-warping where needed), in terms of the phase of the rhythmic EEG responses; and in the time domain. Ultimately, they conclude that, when subjects are attending to the speech, the EEG primarily tracks the linguistic content of speech. And, when subjects are not attending to the speech, the EEG primarily tracks the acoustics.</p><p>I enjoyed reading this manuscript, which tackles an interesting and topical question. And I would like to commend the authors on a very nice set of experiments – it's a lovely design.</p><p>However, I also had a number of fairly substantial concerns – primarily about the conclusions being drawn from the data.</p><p>1) My main concern with the manuscript is that it seems – a priori – very committed to discussing the results in terms of tracking by activity in a specific (i.e., delta) cortical band. I understand the reason to want to do this – there is a lot of literature on speech and language processing that argues for the central role of cortical oscillations in tracking units of speech/language with different timescales. However, in the present work, I think this framework is causing confusion. To argue that delta tracking is a unitary thing that might wax and wane like some kind of push/pull mechanism to track acoustics sometimes and linguistic features other times seems unlikely to be true and likely to cause confusion in the interpretation of a nice set of results. My personal bias here – and I think it could be added for balance to the manuscript – is that speech processing is carried out by a hierarchically organized, interconnected network with earlier stages/lower levels operating on acoustic features and later stages/higher levels operating on linguistic features (with lots of interaction between the stages/levels). In that framing, the data here would be interpreted as containing separable simultaneous evoked contributions from processing at different hierarchical levels that are time locked to the relevant features in the stimulus. Of course, because of the nature of the experiments here, these contributions are all coming at 2 Hz. But to then jump to saying a single &quot;delta oscillation&quot; is preferentially tracking different features of the speech given different tasks, seems to me to be very unlikely. Indeed, the authors seem sensitive to the idea of different evoked contributions as they include some discussion of the N400 later in their manuscript. But then they still hew to the idea that what they are seeing is a shift in what &quot;delta-band&quot; activity is doing. I am afraid I don't buy it. I think if you want to make this case – it would only be fair to discuss the alternatives and more clearly argue why you think this is the best way to interpret the data.</p><p>2) Following my previous point, I have to admit that I am struggling to figure out how, precisely, you are determining that &quot;cortical activity primarily tracks the word rhythm during speech comprehension&quot;. There is no question that the cortical activity is tracking the word rhythm. But how are you arguing that it is primarily word rhythm. I am guessing it has something to do with Figure 3? But I am afraid I am not convinced of some of the claims you are making in Figure 3. You seem to want to embrace the idea that there is no difference in phase between the σ1-amplified and σ2-amplified conditions and that, therefore, the signal is primarily tracking the word rhythm. But I think there is an important flaw in this analysis – that stems from my concern in point number 1 above. In particular, I think it is pretty clear that there are differences in phase at some more frontal channel locations, and no difference in more posterior channels (Figure 3A, rightmost panel). So, going back to my issue in point number 1, I think it is very likely that word rhythm is being tracked posteriorly (maybe driven by N400 like evoked activity) – and that phase is similar (obviously). But it also seems likely that the 2 Hz rhythm over frontal channels (which likely reflect evoked activity based on acoustic features) are at a different phase. Now, cleanly disentangling these things is difficult because they are both at 2Hz and will contaminate each other. And I think when subjects are watching the silent video, the posterior (N400 like) contribution disappears and all you have left is the acoustic one. So, again, I think two separable things are going on here – not just one delta rhythm that selectively tracks different features.</p><p>3) I'm afraid I must take issue with the statements that &quot;Nevertheless, to dissociate linguistic units with the related acoustic cues, the studies showing linguistic tracking responses mostly employ well-controlled synthesized speech that is presented as an isochronous sequence of syllables… Therefore, it remains unclear whether neural activity can track linguistic units in natural speech, which is semantically coherent but not periodic, containing both acoustic and linguistic information in the delta band.&quot; I know the (very nice) studies you are talking about. But you have just also cited several studies that try to disentangle linguistic and acoustic features using more natural speech (e.g., Brodbeck; Broderick). So I think your statement is just too strong.</p><p>4) I was confused with the presentation of the data in Figure 2F and G. Why are some bars plotted up and others down? They are all positive measures of power, and they would be much easier to compare if they were all pointed in the same direction. Unless I am missing the value of plotting them this way?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.60433.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Revisions for this paper:</p><p>1) The Materials and methods section and the figures need to be clearly documented and annotated. Reviewer 1 raises some concerns about the loose use of &quot;phase&quot; and some of the methods that are not clearly incorporated in the Results to help the more general audience follow the subtleties of the analyses. I also leave the full set of reviews to guide such revision – as well as edit the English when needed.</p></disp-quote><p>Based on the very constructive comments from the reviewers, we have clarified our analysis procedures in the revised manuscript. The response phase in the manuscript refers to the phase of the complex-valued Fourier coefficients, which we have now clearly defined. Furthermore, we have also added a new analysis that is easier to interpret and moved detailed results of phase analysis to Figure 3—figure supplement 1 (please see the reply to the next question for details).</p><disp-quote content-type="editor-comment"><p>2) Tempering the claim that delta uniquely tracks word rhythm.</p><p>All reviewers provide detailed comments pointing out to additional literature that could be considered and which can feed the discussion regarding syntactic vs. semantic vs. prosodic dissociations…</p></disp-quote><p>We have added the suggested references to the manuscript and expanded the Introduction and Discussion. Please see our point-to-point reply for details.</p><disp-quote content-type="editor-comment"><p>Clarifying the specificities reported in the literature should also help the authors address serious comments raised by reviewer 1 and 3, who are particularly hesitant regarding the strong claim on word-specificity of the delta response – including the lack of changes in the phase of delta oscillations. Reviewer 2 also stresses this point. The legitimacy of calling the low-frequency response delta is questioned as opposed to a cautionary possible mixture of network activity as suggested for instance by reviewer 3.</p></disp-quote><p>In the previous manuscript, we referred to the 2-Hz neural response as a “delta-band” neural response to distinguish it from the 4-Hz response to speech envelope which was usually referred to as a “theta-band” envelope-tracking response. We now realized that this terminology was not appropriate since it could mislead the readers to believe that the 2-Hz response must relate to spontaneous delta oscillations. To avoid this potential confusion, we have now used amplitude modulation (AM) and speech envelope to refer to the 2-Hz and 4-Hz acoustic rhythms respectively in the amplitude-modulated speech.</p><disp-quote content-type="editor-comment"><p>All three reviewers further question the description of delta phase considering the quantifications (e.g. well-known effect of the reference) and the observable topographical changes in Figure 3.</p><p>As a result of these major considerations, the authors should temper the strong claims that are currently being put forward.</p></disp-quote><p>The original conclusion that “delta-band activity primarily tracks the word rhythm” was made on the basis of response phase analysis, which showed that the response phase was barely influenced by the 2-Hz amplitude modulation (AM). The three reviewers, however, all raised concerns about the conclusion and/or the relevant analysis procedure. Following the constructive comments from the reviewers, we have now collected more data and performed new analyses to further investigate how acoustic and linguistic information is jointly encoded in cortical activity. We have updated our conclusions on the grounds of the new results, which are summarized in the following.</p><p>Phase difference analysis</p><p>Reviewers 2 and 3 both pointed out that the response phase was affected by the 2-Hz AM even during attentive story comprehension. To further investigate this important observation, we have now calculated the phase difference between σ1- and σ2-amplified conditions for each participant and each EEG electrode, and tested whether the phase difference was significantly different from 0º in any electrode. The results indeed revealed a significant phase difference between conditions in some EEG electrodes, even during the story comprehension task (<xref ref-type="fig" rid="sa2fig1">Author response image 1A</xref>, B), confirming the reviewers’ observation.</p><fig id="sa2fig1"><label>Author response image 1.</label><caption><title>Topographical distribution of the 2-Hz phase difference between the σ1- and σ2-amplified conditions.</title><p>The phase difference is calculated for each participant and each electrode, and then averaged over participants. The black dots indicate electrodes showing a significant phase difference between the σ1- and σ2-amplified conditions (P &lt; 0.05, bootstrap, FDR corrected). (AB) The 2-Hz phase difference in the original experiment. (CD) The 2-Hz phase difference pooled over the original experiment and the replication experiment.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60433-resp-fig1-v2.tif"/></fig><p>A replication experiment</p><p>To further validate the results of phase difference analysis, we have now replicated the experiment with another group of participants. In this replication experiment, participants only listened to amplitude-modulated speech and they participated in the video watching task before the story comprehension task. The replication experiment confirmed our original observation, finding larger 2-Hz response phase difference between the σ1- and σ2-amplified conditions in the video watching task than in the story comprehension task. Nevertheless, the new observation illustrated in <xref ref-type="fig" rid="sa2fig1">Author response image 1A</xref> was not replicated. After the data was pooled over the original experiment and the replication experiment, no EEG electrode showed any significant phase difference between conditions during the story comprehension task (<xref ref-type="fig" rid="sa2fig1">Author response image 1C</xref>).</p><p>Therefore, the 2-Hz AM reliably modulated the response phase during passive listening (<xref ref-type="fig" rid="sa2fig1">Author response image 1B</xref>, D), but not during active listening (<xref ref-type="fig" rid="sa2fig1">Author response image 1A</xref>, C). Although the 2-Hz AM did not significantly modulate the response phase during active listening, the following new analysis showed that this null result was potentially attributable to the low statistical power of the response phase analysis.</p><p>Time-domain separation of AM and word responses</p><p>For amplitude modulated speech, the neural response synchronous to the 2Hz AM (referred to as the AM response) and the response synchronous to word onsets (referred to as the word response) can be dissociated based on response timing (Figure 1D). In the previous manuscript, we mainly employed the phase analysis to characterize response timing (previous Figure 3). Reviewer 1, however, raised the concern that the previous analysis was built on the assumption that the response phase was consistent across participants, which was neither a necessary nor a well-supported assumption. Therefore, we have now employed a different method to extract the AM and word responses based on response timing. This analysis was briefly reported in Supplementary Figure 2 of the previous manuscript. We have now expanded it and moved it to the main text.</p><p>In this new analysis, AM and word responses were extracted by averaging the response over the σ1- and σ2-amplified conditions in different ways, which was illustrated in Figure 4A and C. The spectra of the AM and word responses were calculated in the same way as the spectra were calculated for, e.g., the response to isochronous speech. In this new analysis, the power spectrum was calculated for each participant and then averaged. Therefore it did not assume phase consistency across participants. More importantly, this new method could estimate the strength of the AM and word responses, while the previous phase analysis could only prove the existence of the AM response.</p><p>The new analysis clearly revealed that the AM response was statistically significant during both attentive and passive listening, suggesting that the cortical response contained a component that reflected low-level auditory encoding. The new analysis confirmed that the word response was stronger than the AM response. Nevertheless, it also showed that the word response was also statistically significant during passive listening for amplitude-modulated speech, but not for isochronous speech, suggesting that the AM cue could facilitate word processing during passive listening. Additionally, the AM response was not significantly modulated by attention, while the word response was. The AM and word responses also showed distinguishable spatial distribution, suggesting different neural sources. These key findings were all replicated in the replication experiment (Figure 5), demonstrating that the new spectral analysis method was robust.</p><p>Conclusions</p><p>With findings from the new analysis, we have now updated the conclusions in the Abstract as follows.</p><p>“Our results indicate that an amplitude modulation (AM) cue for word rhythm enhances the word-level response, but the effect is only observed during passive listening. […] These results suggest that bottom-up acoustic cues and top-down linguistic knowledge separately contribute to cortical encoding of linguistic units in spoken narratives.”</p><disp-quote content-type="editor-comment"><p>Revisions expected in follow-up work:</p><p>3) While the study provides an interesting means for parametric manipulation of amplitude, reviewer 2 raised an interesting point regarding the effect of prosody in that frequency range. As expressed by reviewer 2: &quot;no only the amplitude of pitch changes is critical to the perception of prosodic events, but also the timing and rise/fall slopes. All this is not linearly affected by increasing the volume at 2 Hz. To show that delta-band oscillations are not affected by stimulus prosody would require a more systematic dedication to the different linguistic dimension of prosody, amplitude being only one.&quot;</p></disp-quote><p>Reviewer 2 raised a very important point, which was not discussed in the previous manuscript. We certainly agree that the 2-Hz word response in the current study could reflect prosodic processing. Furthermore, we would like to distinguish the perceived prosody and the acoustic cues for prosody. Previous literature has shown that high-level linguistic information can modulate prosodic and even auditory processing (e.g., Buxó-Lugo and Watson, 2016; Garrett et al., 1966). Therefore, even when the prosodic cues in speech are removed, e.g., in the isochronous speech sequence, listeners may still mentally recover the prosodic structure. In fact, the experiment design is not to rule out the possibility that prosody is related to low-frequency neural activity. This important point was not discussed in the previous manuscript and caused confusion.</p><p>Furthermore, as the reviewer pointed out, the amplitude envelope is just one kind of prosodic cues. Furthermore, in most languages, the amplitude envelope contributes less to prosodic processing than other cues, e.g., the pitch contour, rise/fall slopes, and timing. Therefore, the purpose of manipulating the speech envelope is not to modulate prosodic processing. Instead, we manipulate the amplitude envelope since it is one of the acoustic features that can most effectively drive cortical responses. Even amplitude modulated noise, which has no pitch contour or linguistic content, can strongly drive an envelope-tracking response. Therefore, we would like to test how auditory envelope tracking and linguistic processing, which includes prosodic processing, separately contribute to speech synchronous neural activity.</p><p>We have now added a new section of discussion about prosodic processing (see point-to-point reply for details) and revised the Discussion to explain why we manipulated the speech envelope.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>[…] My main concerns about the study stem from the methods and a difficulty to straightforwardly understand the procedure.</p></disp-quote><p>Thank you for pointing out these issues. We have thoroughly modified the Materials and methods and Results, and we believe there is now enough information to understand the procedure. Furthermore, we will make the data and analysis code publicly available after the manuscript is accepted.</p><disp-quote content-type="editor-comment"><p>For instance:</p><p>– Can the authors clarify the choice of band-pass filtering and the compensation that was applied as briefly stated &quot;and the delay caused by the filter was compensated.&quot;</p></disp-quote><p>We have now clarified the filter we used and how the delay was compensated.</p><p>“The EEG recordings were down-sampled to 128 Hz, referenced to the average of mastoid recordings, and band-pass filtered between 0.8 Hz and 30 Hz using a linear-phase finite impulse response (FIR) filter (6 s Hamming window, -6 dB attenuation at the cut-off frequencies). […] The delay was compensated by removing the first N/2 samples in the filter output…”</p><disp-quote content-type="editor-comment"><p>– How did the authors insure that concatenated responses did not introduce low-frequency noise in their analysis?</p></disp-quote><p>The EEG responses to different sentences were concatenated in the frequency-domain analysis. Duration of the response to each sentence ranged from 1.5 to 7.5 s (mean duration: 2.5 s). Therefore, concatenation of the responses could only generate low-frequency noises below 1 Hz, which could barely interfere with the 2-Hz and 4-Hz responses that we analyzed.</p><p>To confirm that the 2-Hz and 4-Hz responses were not caused by data concatenation, we have now also analyzed the EEG response averaged over sentences. Considering the variation in sentence duration, this analysis was restricted to sentences that consisted of at least 10 syllables. Moreover, only the responses to the first 10 syllables in these sentences were averaged. In a procedure similar to that applied in the original spectral analysis, the responses during the first 0.5 s were removed to avoid the onset response and the remaining 2-second response was transformed into the frequency domain using the DFT. The results was consistent with the results obtained from the original analysis and were illustrated in <xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>. The advantage of this new analysis was that it did not involve data concatenation, while the disadvantage was that it threw away a lot of data to ensure equal duration in the response to each sentence.</p><fig id="sa2fig2"><label>Author response image 2.</label><caption><title>Spectrum of the EEG response averaged over sentences.</title><p>This analysis is restricted to sentences that had at least 10 syllables, and only the response to the first 10 syllables is analyzed. The response during the first two syllables is removed to avoid the onset response and the rest 2 seconds of response is averaged over sentences. The averaged response is transformed into the frequency-domain using the DFT. Response spectrum averaged over participants and EEG electrodes. The shaded area indicates 1 standard error of the mean (SEM) across participants. Stars indicate significantly higher power at 2 Hz or 4 Hz than the power averaged over 4 neighboring frequency bins (2 on each side). The color of the star is the same as the color of the spectrum **P &lt; 0.01(bootstrap, FDR corrected).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60433-resp-fig2-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>– Subsection “Frequency domain analysis”: can the authors report the number of participants excluded by the analysis?</p></disp-quote><p>In the previous phase analysis, for each EEG electrode, participants not showing significant inter-trial phase coherence (P &gt; 0.1) were excluded. Therefore, a different number of participants were excluded for each electrode. To simplify the procedure, we have now kept all participants in the analysis and the updated results were shown in Figure 3—figure supplement 1.</p><disp-quote content-type="editor-comment"><p>I also have substantial difficulties regarding the following methodological aspects:</p><p>– Bootstrap analyses artificially increase the number of samples while maintaining constant the variance of the samples. How did the authors take this into account in their final statistical comparisons?</p></disp-quote><p>In the bootstrap procedure, we estimated the distribution of the sample mean by resampling the data. The p-value was determined based on the null distribution. We used a standard bootstrap procedure and its details were specified in the reference (Efron and Tibshirani, 1994), and the code was publicly available (MATLAB scripts: bootstrap_for_vector.m, https://staticcontent.springer.com/esm/art%3A10.1038%2Fs41467-018-07773y/MediaObjects/41467_2018_7773_MOESM4_ESM.zip). The bootstrap procedure is well established and has been used in our previous studies (e.g., Jin et al., 2018) and in other studies (e.g., Bagherzadeh et al., 2020, and Norman-Haignere et al., 2019).</p><p>Bagherzadeh, Y., Baldauf, D., Pantazis, D., Desimone, R., 2020. Alpha synchrony and the neurofeedback control of spatial attention. Neuron 105, 577-587. e575.</p><p>Norman-Haignere, S.V., Kanwisher, N., McDermott, J.H., Conway, B.R., 2019. Divergence in the functional organization of human and macaque auditory cortex revealed by fMRI responses to harmonic tones. Nature Neuroscience 22, 1057-1060.</p><disp-quote content-type="editor-comment"><p>– Figure 1D and predictions on the (absolute? relative? Instantaneous?) phase-locked activity to word onset and syllable need to be better and more precisely explained. For instance, there is no a priori reason to believe that 0° and 180° owe to be the actual expected values as clearly illustrated in Figure 3?</p></disp-quote><p>Thank you for pointing out this issue. We have now modified the illustration.</p><p>Since response phase can be defined in different ways, we no longer illustrate the response phase in Figure 1. Instead, we now show the time lag between neural responses in Figure 1D. Correspondingly, in the main analysis, we have now separately extracted the word and AM responses by time shifting and averaging the responses across σ1- and σ2-amplified conditions (please see Figure 4).</p><disp-quote content-type="editor-comment"><p>– While a time-warping analysis is being performed for data collected in Experiment 3, no mention of this approach is being explained in more details (as to possible interpretational limits) in the actual Results section.</p></disp-quote><p>Thank you for pointing out this issue. We have now detailed the time-warping procedure in Materials and methods.</p><p>“Time-Warping Analysis: In natural speech used in Experiment 3, syllables were not produced at a constant rate, and therefore the responses to syllables and words were not frequency tagged. […] The word index j ranged from 1 to the total number of syllables in a story.”</p><p>We have also mentioned the assumption underlying the time-warping procedure.</p><p>“In the time-warping procedure, it was assumed that the syllable response was time-locked to the syllable onsets and the word response was time-locked to word onsets. The frequency-domain analysis was subsequently applied to the time-warped response, following the same procedure as adopted in the analysis of the response to isochronous speech.”</p><p>In the time-domain analysis (please see Figure 6), which does not involve time warping, it is further confirmed that there is a word response synchronous to the word onset.</p><disp-quote content-type="editor-comment"><p>I am here particularly concerned about the phase analysis which is not being sufficiently reported in details to clearly understand the results and the claims of the authors.</p></disp-quote><p>The previous manuscript did not mention how the response phase was calculated. It was the phase of the complex-valued Fourier coefficient. Most of the phase results are now moved to Figure 3—figure supplement 1, and we have clearly defined the method for phase calculation. Since we analyzed the steady-state response, we did not calculate, e.g., the instantaneous phase.</p><p>In the Results, it is mentioned:</p><p>“The Fourier transform decomposes an arbitrary signal into sinusoids and each complex-valued Fourier coefficient captures the magnitude and phase of a sinusoid. […] Nevertheless, the response phase difference between the σ1- and σ2-amplified conditions carried important information about whether the neural response was synchronous to the word onsets or amplified syllables…”</p><p>In Materials and methods, it is mentioned:</p><p>“Then, the response averaged over trials was transformed into the frequency-domain using Discrete Fourier transform (DFT) without any additional smoothing window. […] The 2-Hz phase difference between the σ1- and σ2-amplified conditions was averaged over participants in each electrode.”</p><disp-quote content-type="editor-comment"><p>In line with this, I highlight two additional issues:</p><p>– Subsection “Response phase at 2 Hz”, Figure 4: the authors argue that attention may explain the lack of phase differences. Additionally, the authors seem to assume in their working hypothesis that the phase response should be comparable across participants.</p><p>In a recent study (Kösem et al., 2016), no evidence for phase-related linguistic parsing in low-frequency activity was found when participants actively or passively listened to a stationary acoustic speech sequence whose perceptual interpretation could alternate. Are these findings compatible with those illustrated in Figure 4? Furthermore, some important inter-individual differences were reported, which could severely impact the primary assumption of phase-consistency across individuals.</p></disp-quote><p>The reviewer raised two very important questions here. One was about the phase consistency across participants, and the other was about how the current results were related to the results in Kösem et al., 2016. In the following, we answered the two questions separately.</p><p>Inter-participant phase consistency</p><p>In terms of the experiment design, we do not assume that the response phase is consistent across individuals. We only hypothesize that there are two potential neural response components, which are separately synchronous to the speech envelope and the word rhythm. In the previous analysis, however, we did implicitly assume consistent response phase across individuals, since the response phase appeared to be consistent across individuals in most conditions (previous Figure 3). However, as the reviewer pointed out, this assumption was not necessary to test our hypothesis. Therefore, we added a new analysis that did not assume inter-participant phase consistency. This analysis calculated the phase difference between the σ1- and σ2-amplified conditions for individuals and tested whether the phase difference significantly deviated from 0º. This analysis was illustrated in Figure 3—figure supplement 1E.</p><p>Relation to the results in Kösem et al., 2016</p><p>We actually have our own pilot data acquired using a paradigm very similar to the paradigm adopted in Kösem et al., 2016. In the pilot study, a single word was repeated in the stimuli and we did not observe a word-related response either. It seems like a robust word synchronous response is only observable when the sequence presents different words instead of repeating the same word. There are several potential reasons for this difference. First, repeating the same word generates an acoustic rhythm of the same rate with the word rhythm. Neural tracking of the acoustic rhythm may potentially interact with the word response. Secondly, repetition of the same word may lead to neural adaptation and phenomena such as semantic satiation, which attenuates the neural response to words.</p><p>We have now added a discussion about the issue in <italic>Discussion</italic>.</p><p>“Furthermore, a recent study shows that low-frequency cortical activity cannot reflect the perception of an ambiguous syllable sequence, e.g., whether repetitions of a syllable is perceived as “flyflyfly” or “lifelifelife” (Kösem et al., 2016).”</p><p>“Although the current study and previous studies (Ding et al., 2018; Makov et al., 2017) observe a word-rate neural response, the study conducted by Kösem et al., 2016, does not report observable neural activity synchronous to perceived word rhythm. […] Therefore, it is possible that low-frequency word-rate neural response more strongly reflects neural processing of novel words, instead of the perception of a steady rhythm (see also Ostarek et al., 2020).”</p><disp-quote content-type="editor-comment"><p>– Have the authors considered the possibility that the choice of EEG reference may largely affect the reported mean instantaneous phase responses and if so, could they control for it?</p></disp-quote><p>We agree that the choice of EEG reference can influence the absolute phase. However, the hypothesis we would like to test is about the phase difference across conditions, which should not be sensitive to the choice of EEG reference.</p><p>To confirm that the phase difference between conditions is not strongly influenced by the choice of EEG electrodes, we have also analyzed the results using the average of sixty-four electrodes as the reference. The phase difference results are illustrated in <xref ref-type="fig" rid="sa2fig3">Author response image 3</xref>, which is consistent with the results obtained using the average of the two mastoid electrodes as the reference (Figure 3 —figure supplement 1E).</p><fig id="sa2fig3"><label>Author response image 3.</label><caption><title>Topographical distribution of the 2-Hz phase difference between the σ1- and σ2-amplified conditions using the average of sixty-four electrodes as the reference.</title><p>The phase difference is calculated for each participant and each electrode, and then averaged over participants. The black dots indicate the electrodes showing a significant phase difference between the σ1- and σ2-amplified conditions (P &lt; 0.05, bootstrap, FDR corrected).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60433-resp-fig3-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>[…] I find the framing and discussion a bit gappy and I missed multiple lines of related research. For instance, the literature on the relationship between delta-band oscillations and prosody is not mentioned in the Introduction; likewise, the literature on temporal prediction versus linguistic prediction is not discussed well. In addition, I also think that this study is not as definitive as the authors claim, resulting from the choice of unnatural (i.e., isochronous) stimuli that, strangely enough, the authors claim to be natural.</p></disp-quote><p>Thank you for pointing out these issues. We have now made substantial modifications to the Introduction and Discussion sections, and have modified the conclusions based on new data and new analyses. Please see our responses in the following for details.</p><disp-quote content-type="editor-comment"><p>Below, I given some comments for the authors to further improve their interesting work.</p><p>1) False claim of studying natural speech: The authors artificially induce unnatural stimulus rhythms as a means to frequency-tag their stimuli and perform frequency-domain analysis. While this is an interesting approach, this results in stimuli that do not have ecological validity. Using unnatural stimuli with artificial periodicity to propose periodic electrophysiological processing of words is not scientifically sound. In natural speech, words do not occur periodically. Why assume then that they are processed by a periodic oscillatory mechanism? While the frequency-tagging approach inherently provides good SNR, it cannot claim ecological validity for anything above phonemes or syllables, as for these, there are no linguistic corpus studies out there that show periodicity of the constructs to start with. Specific comment:</p></disp-quote><p>Thank you for raising these important issues. First, we want to clarify that we did use natural speech in the “natural speech condition”. We time warp the neural response instead of the speech sound. The confusion is caused by the previous Figure 1C, which shows time-warped speech for illustrative purposes. We have now modified the figure. The updated Figure 1 only displays the stimuli and Figure 2E illustrates time-warped neural response. We have also included samples of the stimulus as Supplementary file 3.</p><p>We fully agree that words are not of equal duration in natural speech. Syllables and phonemes are not of equal duration either. The motivation of including a natural speech condition is to test whether neural activity is synchronous to words when the word rhythm is not constant. Therefore, we never used the word “oscillation” in the manuscript. We used frequency-tagging as a high SNR analysis method to extract the word-related response, but did not assume that words were encoded by periodic neural oscillations. We did refer to the word response as a “delta-band” response. However, we used the term to distinguish it from the “theta-band” syllabic-level response. In the current manuscript, we have avoided using the word “delta-band” to denote the word response.</p><disp-quote content-type="editor-comment"><p>Subsection “Neural tracking of words in natural spoken narratives”: &quot;natural&quot; The authors artificially make natural speech unnatural by making it isochronous. It is bizarre to still call the resulting stimulus &quot;natural&quot;. Isochronous word sequences are certainly not natural speech. Hence, the authors can also not claim that they demonstrate cortical tracking of natural speech. To show this, they would have to show that there is speech-brain synchronicity for the non-isochronous condition. Certainly, to do so, the authors cannot use their standard spectral-peak analysis, but would have to show coherence or phase-locking against a different baseline (e.g., scrambled, autocorrelation-preserving speech). I strongly encourage the authors to do this analysis instead of pretending that isochronous word sequences are natural. The more general problem with this is that it adds to literature claiming that oscillations track words in human speech processing, which cannot possibly be true in this simplistic version: words are simply too variable in duration and timing.</p></disp-quote><p>The “natural speech” used in the study was not time-warped, which was explained in the response to the previous question. Nevertheless, metrical stories had an implicit disyllabic word-level rhythm, which was unnatural. The speaker reading the stories, however, were unaware of the purpose of the study, and the listeners were not told that some stories had a metrical word rhythm. We now conducted a behavioral experiment to test if these stories sounded natural and whether listeners could easily hear the metrical word rhythm. The results were reported in Supplementary file 1 (see Materials and methods for details). In short, most participants did not detect any difference between the metrical and nonmetrical stories and perceived the speech materials as natural.</p><p>“Thirty-four participants (19-26 years old, mean age, 22.5 years; 17 females) took part in a behavioral test to assess the naturalness of the stimuli…”</p><p>“…The test was divided into 2 blocks. In block 1, the participants listened to a metrical story and a nonmetrical story read by a human speaker, which were presented in a pseudorandom order. The stories were randomly selected from the story set. Each story ranged from 53 to 66 second in duration. After listening to each story, the participants were asked to write a sentence to summarize the story and fill out a questionnaire…”</p><p>“…In block 1, the first question in the questionnaire asked whether the two types of stories, a metrical and a nonmetrical story, showed any noticeable difference regardless of their content. […] The reasons reported included (1) exaggerated intonation (N = 2); (2) the speed and intonation pattern seemed uniform (N = 2); (3) lack of emotion (N = 2); (4) the pitch went up at the end of each sentence (N = 1). In sum, most participants thought the stories were naturally read and only two participants (6%) commented on the uniformity of pace.”</p><disp-quote content-type="editor-comment"><p>2) Depth of framing and discussion: (a) Introduction is not specific enough to speech prosody versus the different types of results on linguistic information (e.g., syntactic structure / phrasing, syntactic and lexical entropy), including reference to prior work; also, relevant references that suggested delta-band oscillations to serve both bottom-up (i.e., prosody) and top-down (i.e., syntax, prediction) functionalities are not discussed in the Introduction; (b) interpretation of the results in terms of the different proposed functionalities of the delta-band lacks depth and differentiation.</p></disp-quote><p>Thank you for the useful suggestions, we have updated the Introduction and Discussion. Please see, e.g., the response to the following question.</p><disp-quote content-type="editor-comment"><p>Introduction: What is missing here is mentioning that delta-band oscillations have also been implied in prosody (e.g., Bourguignon et al., 2013). Relatedly, the idea that delta-band oscillations are related both to slow-fluctuating linguistic representations (e.g., phrases; Meyer and Gumbert, 2018; Meyer et al., 2016; Weissbart et al., 2019) and prosody has been put forward various times (e.g., Ghitza, 2020; Meyer et al., 2016). I think that mentioning this would help the framing of the current manuscript: (1) delta = symbolic linguistic units, (2) delta = prosody, (3) current study: Is delta reflecting symbolic processing or prosody?</p></disp-quote><p>The reviewer made a very good point here and we have added the following into Introduction.</p><p>“Speech comprehension, however, requires more than syllabic-level processing. […] Therefore, it remains unclear whether cortical activity can synchronize to linguistic units in natural spoken narratives, and how it is influenced by bottom-up acoustic cues and top-down linguistic knowledge”.</p><p>Furthermore, we have added the following into the Discussion.</p><p>“It remains elusive what kind of mental representations are reflected by cortical responses synchronous to linguistic units. [...] Previous psycholinguistic studies have already demonstrated that syntax has a significant impact on prosody perception (Buxó-Lugo and Watson, 2016; Garrett et al., 1966).”</p><p>“In speech processing, multiple factors contribute to the word response and these factors interact. […] This result is consistent with the idea that prosodic cues have a facilitative effect on speech comprehension (Frazier et al., 2006; Ghitza, 2017, 2020; Giraud and Poeppel, 2012).”</p><disp-quote content-type="editor-comment"><p>Subsection “Time course of ERP response to words”: As to prediction and the delta-band, please consider discussing Weissbart et al., 2019, Meyer and Gumbert, 2018, Breska and Deouell, 2017, Lakatos et al., 2013, Roehm et al., 2009; in particular the relationship between linguistic predictions and the delta-band should be discussed shortly here. In general, there is need in the field to characterize the relationship between temporal prediction, prosody, linguistic units, and the delta-band. Just noting all this would be fine here.</p></disp-quote><p>Thank you for the useful suggestions and we have added the following to Discussion.</p><p>“It is also possible that neural activity synchronous to linguistic units reflect more general cognitive processes that are engaged during linguistic processing. […] In addition, low-frequency neural activity has also been suggested to reflect the perception of high-level rhythms (Nozaradan et al., 2011) and general sequence chunking (Jin et al., 2020).”</p><disp-quote content-type="editor-comment"><p>&quot;semantic processing&quot; Why semantics? See my above comment on the relationship between the delta-band and temporal predictions (e.g., Roehm et al., 2009; Stefanics et al., 2010). In addition, please consider the recent reconceptualisation of the N400-P600 complex in terms of predictive coding (e.g., Kuperberg et al., 2019).</p></disp-quote><p>We have replaced the sentence about “semantic processing” with the following sentence:</p><p>“This component is consistent with the latency of the N400 response, which can be observed when listening to either individual words or continuous speech (Broderick et al., 2018; Kutas and Federmeier, 2011; Kutas and Hillyard, 1980; Pylkkänen and Marantz, 2003; Pylkkänen et al., 2002).”</p><p>We have also added the following into the Discussion.</p><p>“The 400-ms latency response observed in the current study is consistent with the hypothesis that the N400 is related to lexical processing (Friederici, 2002; Kutas and Federmeier, 2011). Besides, it is also possible that the second syllable in a disyllabic word elicits weaker N400 since it is more predictable than the first syllable (Kuperberg et al., 2020; Lau et al., 2008).”</p><disp-quote content-type="editor-comment"><p>3) EEG preprocessing / report thereof does not accord to standards in the field. Apparently, the EEG data have not been cleaned from artifacts except for calculating the residual between the EEG and the EOG. Hence, I cannot assume that the data are clean enough to allow for statistical analysis. Specific point:</p><p>Materials and methods: The EEG-cleaning section is very sparse. With standard EEG and standard cleaning, rejection rates for adult EEG are in the other of 25-35 %. It am highly doubtful that these EEG data have been cleaned in an appropriate way. There is no mentioning of artifact removal, amount of removed data segments, time- and frequency-domain aspects that were considered for artifact definition. What toolboxes were used, what algorithms?</p></disp-quote><p>Thank you for pointing out this issue. We have now mentioned the software and added more details about the pre-processing procedures. We only used basic Matlab functions without any particular toolbox.</p><p>“All preprocessing and analysis in this study were performed using Matlab</p><p>(The MathWorks, Natick, MA). […] Occasional large artifacts in EEG/EOG, i.e., samples with magnitude &gt; 1 mV, were removed from the analysis (Jin et al., 2018).”</p><p>We have now removed the trials with obvious muscle artifacts after the preprocessing.</p><p>“To further remove potential artifacts, the EEG responses were divided into 7-s trials (a total of 148 trials for Experiments 1-3, and 104 trials for Experiment 4), and we visually inspected all the trials and removed trials with identifiable artifacts. On average, 8.45% ± 3.20% trials were rejected in Experiment 1, 15.20% ± 3.97% trials were rejected in Experiment 2, 10.35% ± 1.53% trials were rejected in Experiment 3, and 12.9% ± 4.46% trials were rejected in Experiment 4.”</p><disp-quote content-type="editor-comment"><p>4) Terminology: Like many people in the field, the authors speak of “tracking” of linguistic information. To be clear: While it is fine that the field understands the problems of the term “entrainment”, now opting for “tracking”, it does not make much sense to use the combination of “tracking” and “linguistic units”. This is because linguistic units are not in the stimulus; instead they are only present in the brain as such. They cannot be tracked; instead, they need to be “associated”, “inferred”, or even “generated”. If one were to assume that the brain “tracks” linguistic units, this would mean that the brain tracks itself. Specific comments:</p><p>&quot;cortical activity tracking of higher-level linguistic units&quot; Linguistic units, such as &quot;words and phrases&quot; cannot be tracked, as they are not present in the stimulus. They are purely symbolic and they must be inferred. They cannot be sensed. Instead, they are present only in the mind and electrophysiology of the listener. Linguistic units are activated in memory or generated in some form. They are internal representations or states. If one were to say that the brain tracks these in some way, one would say that the brain tracks its own internal representations / states. Try and rephrase, please.</p></disp-quote><p>Thank you for pointing out this important terminology issue. We certainly agree that the word-related responses are internally constructed. The word “tracking” was used loosely in the previous manuscript. Now, we avoided calling the response a “word-tracking response”, and referred to the response as a word response or a neural response synchronous to the word rhythm.</p><disp-quote content-type="editor-comment"><p>&quot;linguistic information&quot; cannot be extracted from acoustic features. Instead, linguistic information (e.g., distinctive features in phonology, semantic meaning, part-of-speech labels) must be associated with a given acoustic stimulus by an associative / inferential cognitive process. When we hear some acoustic stimulus, we associate with something linguistic. Just as in my above comment on the loose use of the term “tracking”, I suggest the authors to change this wording as well. A suggestion for a better phrasing would be &quot;Linguistic information is retrieved / constructed / generated when specific acoustic information is encountered / recognized…&quot;. Or &quot;Speech comprehension involves the active inference of linguistic structure from speech acoustics&quot;.</p></disp-quote><p>We have removed the description, and we no longer use the word “extract” in similar situations.</p><disp-quote content-type="editor-comment"><p>Discussion: &quot;tracks&quot; see my above comment.</p></disp-quote><p>Please see our reply to the previous question.</p><disp-quote content-type="editor-comment"><p>5) Potential differences between the σ1 and 2 conditions: There are apparent differences in the number of sensors that show sensitivity to the amplitude modulation. This is not captured in the current statistical analysis. Still, given these drastic differences, the claim that acoustics of speech prosody do not affect delta-band amplitude is certainly much too strong and may just result from a lack of analysis (i.e., overall phase / ERP rather than sensor-by-sensor analysis). See specific comment next:</p></disp-quote><p>We have updated our conclusions based on a new analysis. Please see our reply to the editorial comments.</p><disp-quote content-type="editor-comment"><p>&quot;or the phase of the 2-Hz amplitude modulation&quot; I disagree with excitement and a happy suggestion that the authors must consider. Look, the plots (Figure 3, σ1/2) may not show differences in the phase angle. Yet, what they may point to is a dimension of the data that the authors have not assessed yet: topographical spread. From simply eyeballing it is clear that the number of significant electrodes differs drastically between the σ1 and σ2 conditions. This pattern is interesting and should be examined more closely. Is word-initial stress more natural in Chinese? If so, this could be interpreted in terms of some influence of native prosodic structure: Amplitude modulation does appear affect the amount / breadth of EEG signal (i.e. number of sensors), but only if it conforms to the native stress pattern? Not only words / phrases are part of a speakers linguistic knowledge, but also the stress pattern of their language. I urge the authors to look into this more closely.</p></disp-quote><p>The reviewer raised a very important point about the stress pattern in Chinese. According to the classic study by Yuen Ren Chao (Mandarin Primer, Harvard University Press, 1948), the stress of disyllabic Chinese words usually falls on the second syllable. Later studies more or less confirm that slightly more than 50% of the disyllabic words tend to be stressed on the second syllable. However, it is well established that, in Chinese, stress is highly dependent on the context and does not affect word recognition. For example, Shen, 1993, noted that “However, unlike English where lexical stress is fixed and can be predicted by the phonology, lexical stress in Mandarin varies socio-linguistically and idiosyncratically. Some disyllabic words can be uttered iambically or trochaically”.</p><p>Since linguistic analysis does not generate a strong prediction about whether σ1-amplified or σ2-amplified speech should sound more natural, we now have asked a group of participants (<italic>N</italic> = 34) to rate the naturalness of the speech designed for these two conditions (see Materials and methods for details). The results showed that half of the participants commented that speech in the two conditions were equally natural (<italic>N</italic> = 17). For the other half of the participants, most thought σ1-amplified speech sounded more natural (<italic>N</italic> = 15) while the others thought σ2-amplified speech was more natural (<italic>N</italic> = 2).</p><p>In sum, while previous linguistic studies tend to suggest that σ2-amplified speech is slightly more natural, our behavioral assessment suggests that σ1-amplified speech is slightly more natural. Therefore, it is difficult to draw a solid conclusion about which condition is more consistent with natural speech.</p><p>Additionally, the response power does not differ between the σ1-amplified or σ2-amplified conditions, and only the inter-participant phase coherence differ between conditions, as mentioned by the reviewer. This phenomenon, however, can potentially be explained without making any assumption about the interaction between the AM and word responses: If the AM and word responses are independent, the measured neural response is the sum of the two responses and its phase is influenced by both components. If the phase of the AM response is more consistent with the phase of the word response in the σ1-amplified condition, the inter-participant phase coherence will be higher in the σ1-amplified condition on the assumption that the strength of the AM and word response varies across participants (illustrated in <xref ref-type="fig" rid="sa2fig4">Author response image 4</xref>).</p><fig id="sa2fig4"><label>Author response image 4.</label><caption><title>Illustration of the response phase for individuals.</title><p>The red and blue arrows indicate the phase of word response and AM response, which are assumed to be consistent across individuals. The AM response is 180° out of phase between the σ1 and σ2-amplified conditions, while the word response phase is the same in both conditions. The measured response is the vector sum of the AM and word responses. The purple arrows indicate the phase of the measured response for individual participants. If the phase of the AM response is more consistent with the phase of the word response in the σ1-amplified condition, the inter-participants phase coherence is higher for the σ1-amplified condition than the σ2-amplified condition.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60433-resp-fig4-v2.tif"/></fig><p>Therefore, in the current study, we integrate the σ1- and σ2-amplified conditions to separate the AM and word responses, and future studies are needed to establish whether the neural response is modulated by the naturalness of speech prosodic cues. We have now added discussions about this issue.</p><p>“Finally, it should be mentioned that we employed amplitude modulation to manipulate the speech envelope, given that the speech envelope is one of the strongest cues to drive stimulus-synchronous cortical response. […] Future studies are needed to characterize the modulation of language processing by different prosodic cues and investigate the modulatory effect across different languages.”</p><disp-quote content-type="editor-comment"><p>&quot;The response&quot; see my above comment on the differences in the number significant sensors in the σ1 and 2 conditions.</p></disp-quote><p>The sentence is replaced as the following sentence:</p><p>“Consistent with previous findings, in the current study, the 4-Hz syllable response was also enhanced by cross-modal attention (Figure 3B). The 2-Hz AM response power, however, was not significantly modulated by cross-modal attention (Figure 4D, and Figure 5B), suggesting that attention did not uniformly enhance the processing of all features within the same speech stream…”</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>[…] I enjoyed reading this manuscript, which tackles an interesting and topical question. And I would like to commend the authors on a very nice set of experiments – it's a lovely design.</p><p>However, I also had a number of fairly substantial concerns – primarily about the conclusions being drawn from the data.</p><p>1) My main concern with the manuscript is that it seems – a priori – very committed to discussing the results in terms of tracking by activity in a specific (i.e., delta) cortical band. I understand the reason to want to do this – there is a lot of literature on speech and language processing that argues for the central role of cortical oscillations in tracking units of speech/language with different timescales. However, in the present work, I think this framework is causing confusion. To argue that delta tracking is a unitary thing that might wax and wane like some kind of push/pull mechanism to track acoustics sometimes and linguistic features other times seems unlikely to be true and likely to cause confusion in the interpretation of a nice set of results. My personal bias here – and I think it could be added for balance to the manuscript – is that speech processing is carried out by a hierarchically organized, interconnected network with earlier stages/lower levels operating on acoustic features and later stages/higher levels operating on linguistic features (with lots of interaction between the stages/levels). In that framing, the data here would be interpreted as containing separable simultaneous evoked contributions from processing at different hierarchical levels that are time locked to the relevant features in the stimulus. Of course, because of the nature of the experiments here, these contributions are all coming at 2 Hz. But to then jump to saying a single &quot;delta oscillation&quot; is preferentially tracking different features of the speech given different tasks, seems to me to be very unlikely. Indeed, the authors seem sensitive to the idea of different evoked contributions as they include some discussion of the N400 later in their manuscript. But then they still hew to the idea that what they are seeing is a shift in what &quot;delta-band&quot; activity is doing. I am afraid I don't buy it. I think if you want to make this case – it would only be fair to discuss the alternatives and more clearly argue why you think this is the best way to interpret the data.</p></disp-quote><p>The reviewer raised a very important point about how to interpret the results. We actually agree with the reviewer’s interpretation. It might be our writing that has caused confusions. We do not think there is a unitary delta oscillation and the term “oscillation” is never used in the manuscript. In the current manuscript, we no longer refer to the results as “delta-band responses”, and we have revised the Introduction and Discussion sections to make it clear that responses from multiple processing stages can contribute to the measured EEG response.</p><p>Furthermore, a new analysis now separates the measured response into a word response and an AM response , which is also based on the assumption that multiple response components coexist in the measured response. Please see our reply to the editorial comments for details.</p><disp-quote content-type="editor-comment"><p>2) Following my previous point, I have to admit that I am struggling to figure out how, precisely, you are determining that &quot;cortical activity primarily tracks the word rhythm during speech comprehension&quot;. There is no question that the cortical activity is tracking the word rhythm. But how are you arguing that it is primarily word rhythm. I am guessing it has something to do with Figure 3? But I am afraid I am not convinced of some of the claims you are making in Figure 3. You seem to want to embrace the idea that there is no difference in phase between the σ1-amplified and σ2-amplified conditions and that, therefore, the signal is primarily tracking the word rhythm. But I think there is an important flaw in this analysis – that stems from my concern in point number 1 above. In particular, I think it is pretty clear that there are differences in phase at some more frontal channel locations, and no difference in more posterior channels (Figure 3A, rightmost panel). So, going back to my issue in point number 1, I think it is very likely that word rhythm is being tracked posteriorly (maybe driven by N400 like evoked activity) – and that phase is similar (obviously). But it also seems likely that the 2 Hz rhythm over frontal channels (which likely reflect evoked activity based on acoustic features) are at a different phase. Now, cleanly disentangling these things is difficult because they are both at 2Hz and will contaminate each other. And I think when subjects are watching the silent video, the posterior (N400 like) contribution disappears and all you have left is the acoustic one. So, again, I think two separable things are going on here – not just one delta rhythm that selectively tracks different features.</p></disp-quote><p>Thank you for the insightful comments and please see our reply to the editorial comments.</p><disp-quote content-type="editor-comment"><p>3) I'm afraid I must take issue with the statements that &quot;Nevertheless, to dissociate linguistic units with the related acoustic cues, the studies showing linguistic tracking responses mostly employ well-controlled synthesized speech that is presented as an isochronous sequence of syllables… Therefore, it remains unclear whether neural activity can track linguistic units in natural speech, which is semantically coherent but not periodic, containing both acoustic and linguistic information in the delta-band.&quot; I know the (very nice) studies you are talking about. But you have just also cited several studies that try to disentangle linguistic and acoustic features using more natural speech (e.g., Brodbeck; Broderick). So I think your statement is just too strong.</p></disp-quote><p>Thank you for pointing out this issue. We have now revised the Introduction to make it more precise.</p><p>“Previous studies suggest that low-frequency cortical activity can also reflect neural processing of higher-level linguistic units, e.g., words and phrases (Buiatti et al., 2009; Ding et al., 2016a; Keitel et al., 2018), and the prosodic cues related to these linguistic units, e.g., delta-band speech envelope and pitch contour (Bourguignon et al., 2013; Li and Yang, 2009; Steinhauer et al., 1999). […] It remains to be investigated, however, how bottom-up prosodic cues and top-down linguistic knowledge separately contribute to the generation of these word-related responses”.</p><disp-quote content-type="editor-comment"><p>4) I was confused with the presentation of the data in Figure 2F and G. Why are some bars plotted up and others down? They are all positive measures of power, and they would be much easier to compare if they were all pointed in the same direction. Unless I am missing the value of plotting them this way?</p></disp-quote><p>In the previous manuscript, we thought it was easier to make comparisons if the bars were presented as mirror images. However, it was indeed confusing to have downward bars. We have now made all bars pointed in the same direction (Figure 3A, B).</p></body></sub-article></article>