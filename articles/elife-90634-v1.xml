<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">90634</article-id><article-id pub-id-type="doi">10.7554/eLife.90634</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.90634.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Statistical learning shapes pain perception and prediction independently of external cues</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-325624"><name><surname>Onysk</surname><given-names>Jakub</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0876-5465</contrib-id><email>jakub.onysk.22@ucl.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-325632"><name><surname>Gregory</surname><given-names>Nicholas</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-325633"><name><surname>Whitefield</surname><given-names>Mia</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-325634"><name><surname>Jain</surname><given-names>Maeghal</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-325635"><name><surname>Turner</surname><given-names>Georgia</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-47629"><name><surname>Seymour</surname><given-names>Ben</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1724-5832</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-27604"><name><surname>Mancini</surname><given-names>Flavia</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8441-9236</contrib-id><email>fm456@cam.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>Computational and Biological Learning Unit, Department of Engineering, University of Cambridge</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Applied Computational Psychiatry Lab, Max Planck Centre for Computational Psychiatry and Ageing Research, Queen Square Institute of Neurology and Mental Health Neuroscience Department, Division of Psychiatry, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>MRC Cognition and Brain Sciences Unit, University of Cambridge</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0080acb59</institution-id><institution>Wellcome Centre for Integrative Neuroimaging, John Radcliﬀe Hospital, Headington</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff5"><label>5</label><institution>Center for Information and Neural Networks (CiNet)</institution><addr-line><named-content content-type="city">Osaka</named-content></addr-line><country>Japan</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ploner</surname><given-names>Markus</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02kkvpp62</institution-id><institution>Technische Universität München</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Büchel</surname><given-names>Christian</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zgy1s35</institution-id><institution>University Medical Center Hamburg-Eppendorf</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>10</day><month>07</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP90634</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-07-19"><day>19</day><month>07</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-07-04"><day>04</day><month>07</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.23.23287656"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-10-04"><day>04</day><month>10</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.90634.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-05-28"><day>28</day><month>05</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.90634.2"/></event></pub-history><permissions><copyright-statement>© 2023, Onysk et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Onysk et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-90634-v1.pdf"/><abstract><p>The placebo and nocebo effects highlight the importance of expectations in modulating pain perception, but in everyday life we don’t need an external source of information to form expectations about pain. The brain can learn to predict pain in a more fundamental way, simply by experiencing ﬂuctuating, non-random streams of noxious inputs, and extracting their temporal regularities. This process is called statistical learning. Here, we address a key open question: does statistical learning modulate pain perception? We asked 27 participants to both rate and predict pain intensity levels in sequences of ﬂuctuating heat pain. Using a computational approach, we show that probabilistic expectations and confidence were used to weigh pain perception and prediction. As such, this study goes beyond well-established conditioning paradigms associating non-pain cues with pain outcomes, and shows that statistical learning itself shapes pain experience. This finding opens a new path of research into the brain mechanisms of pain regulation, with relevance to chronic pain where it may be dysfunctional.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>pain</kwd><kwd>learning</kwd><kwd>placebo</kwd><kwd>Bayesian inference</kwd><kwd>reinforcement learning</kwd><kwd>endogenous pain regulation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MR/T010614/1</award-id><principal-award-recipient><name><surname>Mancini</surname><given-names>Flavia</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MR/W027593/1</award-id><principal-award-recipient><name><surname>Seymour</surname><given-names>Ben</given-names></name><name><surname>Mancini</surname><given-names>Flavia</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.35802/214251</award-id><principal-award-recipient><name><surname>Seymour</surname><given-names>Ben</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100012041</institution-id><institution>Versus Arthritis</institution></institution-wrap></funding-source><award-id>21537</award-id><principal-award-recipient><name><surname>Seymour</surname><given-names>Ben</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>IITP</institution></institution-wrap></funding-source><award-id>MSIT 2019-0-01371</award-id><principal-award-recipient><name><surname>Seymour</surname><given-names>Ben</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000735</institution-id><institution>University of Cambridge Research Computing Service</institution></institution-wrap></funding-source><award-id>EP/T022159/1</award-id><principal-award-recipient><name><surname>Mancini</surname><given-names>Flavia</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100013373</institution-id><institution>NIHR Oxford Health BRC</institution></institution-wrap></funding-source><award-id>NIHR203316</award-id><principal-award-recipient><name><surname>Seymour</surname><given-names>Ben</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. The views expressed are those of the author(s) and not necessarily those of the NIHR or the Department of Health and Social Care or other funders. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Statistical learning shapes pain perception by allowing the brain to predict and modulate pain intensity based on temporal patterns, providing insights into pain regulation and their relevance to chronic pain.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Clinical pain typically varies over time; in most pain states, the brain receives a stream of volatile and noisy noxious signals, which are also auto-correlated in time. The temporal structure of these signals is important, because the human brain has evolved the exceptional ability to extract regularities from streams of auto-correlated sensory signals, a process called statistical learning (<xref ref-type="bibr" rid="bib14">Dehaene et al., 2015</xref>; <xref ref-type="bibr" rid="bib43">Schapiro and Turk-Browne, 2015</xref>; <xref ref-type="bibr" rid="bib18">Fiser and Lengyel, 2019</xref>; <xref ref-type="bibr" rid="bib37">Meyniel et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">Kourtzi and Welchman, 2019</xref>; <xref ref-type="bibr" rid="bib46">Sherman et al., 2020</xref>; <xref ref-type="bibr" rid="bib56">Turk-Browne et al., 2009</xref>). In the context of pain, statistical learning can allow the brain to predict future pain, which is crucial for orienting behaviour and maximising well-being (<xref ref-type="bibr" rid="bib34">Mancini et al., 2022</xref>; <xref ref-type="bibr" rid="bib39">Mulders et al., 2023</xref>). Statistical learning might also be fundamental to the ability of the nervous system to endogenously regulate pain. Indeed, statistical learning generates predictions about forthcoming pain. We already know that pain expectations can modulate pain levels by gating the reciprocal transmission of neural signals between the brain and spinal cord, as shown by previous work on placebo and nocebo effects (<xref ref-type="bibr" rid="bib54">Tracey, 2010</xref>; <xref ref-type="bibr" rid="bib53">Tinnermann et al., 2017</xref>; <xref ref-type="bibr" rid="bib15">Eippert et al., 2009</xref>; <xref ref-type="bibr" rid="bib20">Geuter and Büchel, 2013</xref>; <xref ref-type="bibr" rid="bib17">Fields, 2018</xref>).</p><p>By using temporal sequences of noxious inputs, we have previously shown that the pain system supports the statistical learning of the basic rate of getting pain by engaging both somatosensory and supramodal cortical regions (<xref ref-type="bibr" rid="bib34">Mancini et al., 2022</xref>). Specifically, both sensorimotor cortical regions and the ventral striatum encode probabilistic predictions about pain intensity, which are updated as a function of learning by engaging parietal and prefrontal regions. According to a Bayesian inference framework, both the predictive inference and its conﬁdence should, in principle, modulate the neural response to noxious inputs and affect perception, as a function of learning. In support of this conjecture, there is evidence that the confidence of probabilistic pain predictions modulates the cortical response to pain (<xref ref-type="bibr" rid="bib39">Mulders et al., 2023</xref>). The relationship is inverse: the lower the confidence, the higher is the early cortical response to noxious inputs (and vice versa), as measured by EEG. This is expected based on Bayesian inference theory: when confidence is low, the brain relies less on his prior beliefs and more on sensory evidence to respond to the input. Bayesian inference theory also predicts that prior expectations and their confidence scale perception (<xref ref-type="bibr" rid="bib30">Knill and Richards, 1996</xref>). Thus, we hypothesise that the predictions generated by learning the statistics of noxious inputs in dynamically evolving sequences of stimuli modulate the perception of forthcoming inputs.</p><p>Previously, it was found that pain perception is strongly inﬂuenced by probabilistic expectations as defined by a cue that predicts high or low pain (<xref ref-type="bibr" rid="bib26">Jepma et al., 2018</xref>). In contrast to such cue paradigm, the primary aim of our experiment was to determine whether the expectations participants hold about the sequence itself inform their perceptual beliefs about the intensity of the stimuli. To that end, we recruited 27 healthy participants to complete a psycho-physical experiment where we delivered four different, 80-trial-long sequences of evolving thermal stimuli, with four levels of temporal regularity. On each trial, a 2 s thermal stimulus was applied, following which participants were asked to either rate their perception of the intensity (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) or to predict the intensity of the next stimulus in the sequence (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Participants also reported their response confidence.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Task design.</title><p>On each trial, each participant received a thermal stimulus lasting 2s from a sequence of intensities. This was followed by a perception (<bold>A</bold>) or a prediction (<bold>B</bold>) input screen, where the <italic>y</italic>-axis indicates the level of perceived/predicted intensity (0–100) centred around participant’s pain threshold, and the <italic>x</italic>-axis indicates the level of confidence in one’s perception (0–1). The inter-stimulus interval (ISI; black screen) lasted 2.5s (trial example in <bold>C</bold>). (<bold>D</bold>) Example intensity sequences are plotted in green, participant’s perception and prediction responses are in red and black, respectively. (<bold>E</bold>) Participant’s confidence rating for perception (red) and prediction (black) trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-fig1-v1.tif"/></fig><p>We contrasted four models of statistical learning, which varied according to the inference strategy used (i.e. optimal Bayesian inference or a heuristic) and the role of expectations on perception. All models used confidence ratings to weigh the inference. We anticipate that probabilistic learning weighted by confidence and expectations modulates pain perception. This provides behavioural evidence for a link between learning and endogenous pain regulation. One reason why this is important is that it might help understand individual differences in the ability to endogenously regulate pain. This is particularly relevant for chronic pain, given that endogenous pain regulation can be dysfunctional in several chronic pain conditions (<xref ref-type="bibr" rid="bib12">Bushnell et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Bruehl et al., 1999</xref>; <xref ref-type="bibr" rid="bib63">Yarnitsky, 2015</xref>; <xref ref-type="bibr" rid="bib28">King et al., 2009</xref>; <xref ref-type="bibr" rid="bib6">Bannister and Dickenson, 2017</xref>), even before chronic pain develops (<xref ref-type="bibr" rid="bib55">Tracey, 2016</xref>). Although there is ample evidence for changes in the functional anatomy and connectivity of endogenous pain modulatory systems in chronic pain, their computational mechanisms are poorly understood.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Model-naive performance</title><p>Prior to modelling, we first checked whether participant’s performance in the task was affected by the level of temporal regularity, i.e., the sequence condition. We varied the level of volatility and stochasticity across blocks (i.e. conditions), whilst we fixed their overall level within each block; the level of volatility was defined by the number of trials until the mean intensity level changes. The stochasticity is the additional noise that is added on each trial to the underlying mean, often referred to as the observation noise. The changes were often subtle and participants were not informed when they happened. We set two levels (low/high) of each type of uncertainty, achieving a 2×2 factorial design, with the order of conditions randomised across participants. A set of four example sequences of thermal intensities delivered to one of the participants can be found in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, alongside their ratings of perception and predictions. Additionally, example confidence ratings for each type of response are plotted in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. <xref ref-type="fig" rid="app1fig2">Appendix 1—figures 2</xref> and <xref ref-type="fig" rid="app1fig3">3</xref> show the plots of each participant’s responses superimposed onto the sequences of noxious inputs.</p><p>As a measure of performance, we calculated the root mean square error (RMSE) of participants responses (ratings and predictions) compared to the normative noxious input for each condition as in <xref ref-type="fig" rid="fig2">Figure 2</xref> (see also Materials and methods). The lower the RMSE, the more accurate participants’ responses are. Performance in diﬀerent conditions was analysed with a repeated measures ANOVA, whose results are reported in full in <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>. Although volatility did not affect rating accuracy (<inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>26</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.96</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.336</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.036</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), we found a two-way interaction between the level of stochasticity of the sequence (low, high) and the type of rating provided (perceived intensity vs. prediction) (<inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>26</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>29.842</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.534</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). We followed up this interaction in post hoc comparisons, as reported in <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>. The performance score differences between all the pairs of stochasticity and response type interactions were significant, apart from the perception ratings in the stochastic environment as compared with perception and prediction performance in the low stochastic setting. Intuitively, the RMSE score analysis revealed an overall trend of participants performing worse on the prediction task, in particular when the level of stochasticity is high.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Participant’s model-naive performance in the task.</title><p>Violin plots of participant root mean square error (RMSE) for each condition for <bold>A: </bold>rating and <bold>B: </bold>prediction responses as compared with the input. Lower and upper hinges correspond to the first and third quartiles of partipants’ errors (the upper/lower whisker extends from the hinge to the largest/smallest value no further than 1.5 * ”Interquartile range” from the hinge); the line in the box corresponds to the median. Each condition has N=27 particpants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-fig2-v1.tif"/></fig></sec><sec id="s2-2"><title>Modelling strategy</title><p>Our models were selected a priori, following the modelling strategy from <xref ref-type="bibr" rid="bib26">Jepma et al., 2018</xref>, and hence considered the same set of core models for clear extension of the analysis to our non-cue paradigm. The key question for us was whether expectations were used to weigh the behavioural estimates during sequence learning. Therefore, we compared Bayesian and non-Bayesian models of sequential learning that weighted their ratings based on prior expectations versus two corresponding models that assumed perfect perception (i.e. not weighted by prior beliefs). As a baseline, we included a random response model (please see Materials and methods for a formal treatment of the computational models).</p><p>According to an optimal Bayesian inference strategy, on each trial, participants update their beliefs about the feature of interest (thermal stimuli) based on probabilistic inference, maintaining a full posterior distribution over its values (<xref ref-type="bibr" rid="bib26">Jepma et al., 2018</xref>; <xref ref-type="bibr" rid="bib42">Särkkä, 2013</xref>). Operating within a Bayesian paradigm, participants are assumed to track and, following new information, update both the mean of the sequence of interest and the uncertainty around it (<xref ref-type="bibr" rid="bib25">Hoskin et al., 2019</xref>). In most cases, such inference makes an assumption about environmental dynamics. For example, a common assumption is that the underlying mean (a hidden/latent state) evolves linearly according to a Gaussian random walk, with the rate of this evolution defined by the the variance of this Gaussian walk (volatility). The observed value is then drawn from another Gaussian with that mean, which has some observation noise (stochasticity). In this case, the observer can infer the latent states through the process of Bayesian filtering (<xref ref-type="bibr" rid="bib42">Särkkä, 2013</xref>), using the Kalman filter (KF) algorithm (<xref ref-type="bibr" rid="bib27">Kalman, 1960</xref>; <xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Expectation weighted models.</title><p>Computational models used in the main analysis to capture participants’ pain perception (<inline-formula><mml:math id="inf3"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>) and prediction (<inline-formula><mml:math id="inf4"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>E</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>) ratings. Both types of ratings are affected by confidence rating (<inline-formula><mml:math id="inf5"><mml:semantics><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>) on each trial. (<bold>A</bold>) In the reinforcement learning model, participant’s pain perception (<inline-formula><mml:math id="inf6"><mml:semantics><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>) is taken to be weighted sum of the current noxious input (<inline-formula><mml:math id="inf7"><mml:semantics><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>) and their current pain expectation (<inline-formula><mml:math id="inf8"><mml:semantics><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>). Following the noxious input, participant updates their pain expectation (<inline-formula><mml:math id="inf9"><mml:semantics><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>). (<bold>B</bold>) In the Kalman filter model, a generative model of the environment is assumed (yellow background) - where the mean pain level (<inline-formula><mml:math id="inf10"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>) evolves according to a Gaussian random walk (volatility <inline-formula><mml:math id="inf11"><mml:semantics><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>). The true pain level on each trial (<inline-formula><mml:math id="inf12"><mml:semantics><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>) is then drawn from a Gaussian (stochasticity <inline-formula><mml:math id="inf13"><mml:semantics><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>). Lastly, the noxious input, <inline-formula><mml:math id="inf14"><mml:semantics><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, is assumed an imperfect indicator of the true pain level (subjective noise <inline-formula><mml:math id="inf15"><mml:semantics><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>). Inference and prediction steps are depicted in a blue box. Participant’s perceived pain is a weighted sum of expectation about the pain level (<inline-formula><mml:math id="inf16"><mml:semantics><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>) and current noxious input (<inline-formula><mml:math id="inf17"><mml:semantics><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>). Following each observation, <inline-formula><mml:math id="inf18"><mml:semantics><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, participant updates their expectation about the pain level (<inline-formula><mml:math id="inf19"><mml:semantics><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-fig3-v1.tif"/></fig><p>Sequence learning can also be captured by a heuristic to the Bayesian inference, i.e., a simple reinforcement learning (RL) rule. Here, participants maintain and update a point estimate of the expected value of the sequence in an adaptive manner, within a non-stationary environment. RL explicitly involves correcting the tracked mean of the sequence proportionally to a trial-by-trial prediction error - a difference between the expected and actual value of the sequence (<xref ref-type="bibr" rid="bib50">Sutton and Barto, 2018</xref>; <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Importantly, RL agents do not assume any specific dynamics of the environment and hence are considered model-free.</p><p>Both models perform a form of error correction about the underlying sequence. The rate at which this occurs is captured by the learning rate <inline-formula><mml:math id="inf20"><mml:semantics><mml:mrow><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> element. The higher the learning rate, the faster participants update their beliefs about the sequence after each observation. For the RL model, the learning rate α is a free parameter that is constant across the trials. On the other hand, the learning rate in the KF model <inline-formula><mml:math id="inf21"><mml:semantics><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (known as the Kalman gain) is calculated on every trial. It depends on participants’ trial-wise belief uncertainty as well as their overall estimation of the inherent noise in the environment (stochasticity, <inline-formula><mml:math id="inf22"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula>). The belief uncertainty is updated after each observation and depends on participants’ sense of volatility (<inline-formula><mml:math id="inf23"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula>) and stochasticity (<inline-formula><mml:math id="inf24"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula>) in the environment.</p><p>Crucially, we also used participants’ trial-by-trial confidence ratings to measure to what extent conﬁdence plays a role in learning. This is captured by the confidence scaling factor <inline-formula><mml:math id="inf25"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula>, which defines the extent to which confidence aﬀects response (un-)certainty. Intuitively, the higher the confidence scaling factor <inline-formula><mml:math id="inf26"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula>, the less important role confidence plays in participant’s response. With relatively low values of <inline-formula><mml:math id="inf27"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula>, when the confidence is low, participants’ responses are more noisy, i.e., less certain. We demonstrate this in <xref ref-type="fig" rid="fig4">Figure 4</xref> by plotting hypothetical responses (A–F) and the effect on the noise scaling (G–L) as a function of <inline-formula><mml:math id="inf28"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula> and confidence ratings.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Confidence scaling factor demonstration.</title><p>(<bold>A–F</bold>) For a range of values of the confidence scaling factor <inline-formula><mml:math id="inf29"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula>, we simulated a set of typical responses a participant would make for various levels of confidence ratings. The belief about the mean of the sequence is set at 50, while the response noise at 10. The confidence scaling factor <inline-formula><mml:math id="inf30"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula> effectively scales the response noise, adding or reducing response uncertainty. (<bold>G–L</bold>) The effect of different levels of parameter <inline-formula><mml:math id="inf31"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula> on noise scaling. As <inline-formula><mml:math id="inf32"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula> increases the effect of confidence is diminished.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-fig4-v1.tif"/></fig><p>To evaluate the effect of expectation on perceived intensity (on top of statistical learning modulating perception), we expanded the standard RL and KF models by adding a perceptual weighting element, <inline-formula><mml:math id="inf33"><mml:semantics><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> (similarly to <xref ref-type="bibr" rid="bib26">Jepma et al., 2018</xref>). Essentially, <inline-formula><mml:math id="inf34"><mml:semantics><mml:mi>γ</mml:mi></mml:semantics></mml:math></inline-formula> governs how much each participant relies on the normative input on each trial, and how much their expectation of the input inﬂuences their reported perception - i.e., they take a weighted average of the two. The higher the <inline-formula><mml:math id="inf35"><mml:semantics><mml:mi>γ</mml:mi></mml:semantics></mml:math></inline-formula>, the bigger the impact of the expectation on perception. Again, in the case of the RL model (eRL - expectation weighted RL), <inline-formula><mml:math id="inf36"><mml:semantics><mml:mi>γ</mml:mi></mml:semantics></mml:math></inline-formula> is a free parameter that is constant across trials, while in the KF model (eKF - expectation weighted KF), <inline-formula><mml:math id="inf37"><mml:semantics><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is calculated on every trial and depends on: (1) the participants’ trial-wise belief uncertainty, (2) their overall estimation of the inherent noise in the environment (stochasticity, <inline-formula><mml:math id="inf38"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula>), and (3) the participant’s subjective uncertainty about the level of intensity, <inline-formula><mml:math id="inf39"><mml:semantics><mml:mi>ϵ</mml:mi></mml:semantics></mml:math></inline-formula>.</p><p>Thus, in total we tested five models: RL and KF (perception not weighted by expectations), eRL and eKF (perception weighted by expectations), and a baseline random model. We then proceeded to fit these five computational models to participants’ responses. For parameter estimation, we used hierarchical Bayesian methods, where we obtained group- and individual-level estimates for each model parameter (see Materials and methods).</p></sec><sec id="s2-3"><title>Modelling results</title><p>We fit each model for each condition sequence. Example trial-by-trial model prediction plots from one participant can be found in <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>. To establish which of the models fitted the data best, we ran model comparison analysis based on the difference in expected log point-wise predictive density (ELPD) between models. The models are ranked according to the ELPD (with the largest providing the best fit). The ratio between the ELPD difference and the standard error around it provides a significance test proxy through the sigma effect. We considered at least a 2 sigma effect as indication of a significant diﬀerence. In each condition, the expectation weighted models (eKF and eRL) provided better fit than models without this element (KF and RL; approximately 2–4 sigma difference, as reported in <xref ref-type="fig" rid="fig5">Figure 5A–D</xref>) and <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>. This suggests that regardless of the levels of volatility and stochasticity, participants still weigh perception of the stimuli with their expectation. In particular, we found that the expectation weighted KF model offered a better fit than the eRL, although in conditions of high stochasticity this diﬀerence was short of significance against the eRL model. This suggests that in learning about temporal regularities in the sequences of thermal stimuli, participants’ expectations modulate the perception of the stimulus. Moreover, this process was best captured by a model that updates the observer’s belief about the mean and the uncertainty of the sequence in a Bayesian manner.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Model comparison for each sequence condition (<bold>A–D</bold>).</title><p>The dots indicate the expected log point-wise predictive density (ELPD) difference between the winning model (eKF - expectation weighted Kalman filter) and every other model. The line indicates the standard error (SE) of the difference. The non-winning models’ ELPD differences are annotated with the ratio between the ELPD difference and SE indicating the sigma effect, a significance heuristic.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-fig5-v1.tif"/></fig><p>We also found that as the confidence in the response decreases, the response uncertainty is scaled linearly with a negative slope ranging between 0.112 and 0.276 across conditions (<xref ref-type="fig" rid="fig6">Figure 6</xref>), confirming the intuition that less confidence leads to bigger uncertainty.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>The effect of the confidence scaling factor on noise scaling for each condition.</title><p>(<bold>A–D</bold>) Each coloured line corresponds to one participant, with the black line indicating the mean across all participants. The mean slope for each condition is annotated.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-fig6-v1.tif"/></fig><p>As an additional check, for each participant, condition and response type (perception and prediction), we plotted participants’ ratings against model predicted ratings and calculated a grand mean correlation in <xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>.</p><p>Next, we checked whether the parameters of the the winning eKF model differed across diﬀerent sequence conditions. Given that volatility was fixed within condition, we treated it as a single-context scenario from the point of view of modelling (<xref ref-type="bibr" rid="bib21">Heald et al., 2023</xref>), and we did not interpret its effect on the learning rate (<xref ref-type="bibr" rid="bib41">Piray and Daw, 2021</xref>). There were no differences for the group-level parameters; i.e., we did not detect significant diﬀerences between conditions in a hypothetical healthy participant group as generalised from our population of participants (<xref ref-type="fig" rid="app1fig12">Appendix 1—figure 12</xref>).</p><p>However, we found some differences at the individual level of parameters (i.e. within our specific population of recruited participants), which we detected by performing repeated measures ANOVAs (see <xref ref-type="fig" rid="app1fig13">Appendix 1—figure 13</xref> for visualisation). The stochasticity parameter <inline-formula><mml:math id="inf40"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula> was affected by the interaction between the levels of stochasticity and volatility (<inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>26</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>35.108</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.575</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), and was higher in highly stochastic and volatile conditions as compared to conditions where either volatility <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>7.735</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, stochasticity <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>9.396</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, or both were low <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>8.826</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. This suggests that, while participants’ performance was generally worse in highly stochastic environments, participants seem to have attributed this to only one source - stochasticity (<inline-formula><mml:math id="inf45"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula>), regardless of the source of higher uncertainty in the sequence (stochasticity or volatility).</p><p>The response noise <inline-formula><mml:math id="inf46"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula> was modulated by the level of volatility (<inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>26</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>5.079</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.033</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.163</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), where it was smaller in highly volatile conditions. Moreover, we detected a significant interaction between volatility and stochasticity on the confidence scaling factor <inline-formula><mml:math id="inf48"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula> (<inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>26</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>81.258</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.758</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), where the values <inline-formula><mml:math id="inf50"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula> were overall lower when either volatility <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>11.570</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, stochasticity <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>6.165</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, or both <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>4.575</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> were high as compared to the conditions where both levels of noise were low. This indicates there may have been some trade-off between <inline-formula><mml:math id="inf54"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math id="inf55"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula>, as lower values of <inline-formula><mml:math id="inf56"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula> introduce additional uncertainty when participant’s confidence is low.</p><p>Lastly, we found the initial uncertainty belief <inline-formula><mml:math id="inf57"><mml:semantics><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> was affected by the interaction between volatility and stochasticity (<inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>26</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>5275.367</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.995</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) without a consistent pattern. All the other effects were not significant.</p><p>In summary, we formalised the process behind pain perception and prediction in noxious time-series within the framework of sequential learning, where the best description of participants’ statistical learning was captured through Bayesian filtering, in particular using a confidence weighted KF model. Most importantly, we discovered that, in addition to weighing their responses with confidence, participants used their expectations about stimulus intensity levels to form a judgement as to what they perceived. This mechanism was present across various levels of uncertainty that defined the sequences (volatility and stochasticity).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Statistical learning allows the brain to extract regularities from streams of sensory inputs and is central to perception and cognitive function. Despite its fundamental role, it has often been overlooked in the field of pain research. Yet, chronic pain appears to ﬂuctuate over time. For instance <xref ref-type="bibr" rid="bib35">Mayr et al., 2022</xref>; <xref ref-type="bibr" rid="bib4">Baliki et al., 2012</xref>; <xref ref-type="bibr" rid="bib19">Foss et al., 2006</xref>, reported that chronic back pain ratings vary periodically, over several seconds-minutes and in absence of movements. This temporal aspect of pain is important because periodic temporal structures are easy to learn for the brain (<xref ref-type="bibr" rid="bib14">Dehaene et al., 2015</xref>; <xref ref-type="bibr" rid="bib34">Mancini et al., 2022</xref>). If the temporal evolution of pain is learned, it can be used by the brain to regulate its responses to forthcoming pain, effectively shaping how much pain it experiences. Indeed, we show that healthy participants extract temporal regularities from sequences of noxious stimuli and use this probabilistic knowledge to form confidence weighted judgements and predictions about the level of pain intensity they experience in the sequence. We formalised our results within a Bayesian inference framework, where the belief about the level of pain intensity is updated on each trial according to the amount of uncertainty participants ascribe to the stimuli and the environment. Importantly, their perception and prediction of pain were inﬂuenced by the expected level of intensity that participants held about the sequence before responding. When varying different levels of inherent uncertainty in the sequences of stimuli (stochasticity and volatility), the expectation and confidence weighted models fitted the data better than models weighted for confidence but not for expectations (<xref ref-type="fig" rid="fig5">Figure 5A–D</xref>). The expectation weighted Bayesian (eKF) model offered a better fit than the expectation weighted, model-free RL model, although in conditions of high stochasticity this difference was short of significance. Overall, this suggests that participants’ expectations play a significant role in the perception of sequences of noxious stimuli.</p><sec id="s3-1"><title>Statistical inference and learning in pain sequences</title><p>The first main contribution of our work is towards the understanding of the phenomenon of statistical learning in the context of pain. Statistical learning is an important function that the brain employs across the lifespan, with relevance to perception, cognition, and learning (<xref ref-type="bibr" rid="bib46">Sherman et al., 2020</xref>). The large majority of past research on statistical learning focused on visual and auditory perception (<xref ref-type="bibr" rid="bib14">Dehaene et al., 2015</xref>; <xref ref-type="bibr" rid="bib18">Fiser and Lengyel, 2019</xref>; <xref ref-type="bibr" rid="bib37">Meyniel et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Meyniel and Dehaene, 2017</xref>), with the nociceptive system receiving relatively little attention (<xref ref-type="bibr" rid="bib51">Tabor et al., 2017</xref>). Recently, we showed that the human brain can learn to predict a sequence of two pain levels (low and high) in a manner consistent with optimal Bayesian inference, by engaging sensorimotor regions, parietal, premotor regions, and dorsal striatum (<xref ref-type="bibr" rid="bib34">Mancini et al., 2022</xref>). We also found that the confidence of these probabilistic inferences modulates the cortical response to pain, as expected by hierarchical Bayesian inference theory (<xref ref-type="bibr" rid="bib39">Mulders et al., 2023</xref>). Here, we tested sequences with a much larger range of stimulus intensities to elucidate the effect of statistical learning and expectations on pain perception. As predicted by hierarchical Bayesian inference theory, we find that the pain intensity judgements are scaled by both expectations and confidence.</p><p>Hence, our work highlights the inferential nature of the nociceptive system (<xref ref-type="bibr" rid="bib26">Jepma et al., 2018</xref>; <xref ref-type="bibr" rid="bib51">Tabor et al., 2017</xref>; <xref ref-type="bibr" rid="bib16">Fardo et al., 2017</xref>; <xref ref-type="bibr" rid="bib45">Seymour and Mancini, 2020</xref>; <xref ref-type="bibr" rid="bib11">Büchel et al., 2014</xref>), where in addition to the sheer input received by the nociceptors, there is a wealth of a priori knowledge and beliefs the agent holds about themselves and the environment that need to be integrated to form a judgement about pain (<xref ref-type="bibr" rid="bib64">Yoshida et al., 2013</xref>; <xref ref-type="bibr" rid="bib2">Anchisi and Zanon, 2015</xref>; <xref ref-type="bibr" rid="bib61">Wiech, 2016</xref>; <xref ref-type="bibr" rid="bib52">Tabor and Burr, 2019</xref>). This has an immediate signiﬁcance for the real world, where weights need to be assigned to prior beliefs and/or stimuli to successfully protect the organism from further damage, but only to an extent to which it is beneficial.</p><p>Secondly, our results regarding the effect of expectation on pain perception relate to a much larger literature on this topic. The prime example would be placebo analgesia (i.e. the expectation of pain relief decreasing pain perception) and nocebo hyperalgesia (i.e. the expectation of high level of pain increasing its perception; <xref ref-type="bibr" rid="bib54">Tracey, 2010</xref>; <xref ref-type="bibr" rid="bib11">Büchel et al., 2014</xref>; <xref ref-type="bibr" rid="bib13">Colloca et al., 2008</xref>; <xref ref-type="bibr" rid="bib7">Blasini et al., 2017</xref>). Recent work attempted to capture such expectancy effects within the Bayesian inference framework. For example, <xref ref-type="bibr" rid="bib25">Hoskin et al., 2019</xref>, showed that in addition to expectation inﬂuencing perceived pain in general, higher level of uncertainty around that expectation attenuated its effect on perception. Similarly, <xref ref-type="bibr" rid="bib23">Hird et al., 2019</xref>, demonstrated that when the discrepancy between the expectation and outcome (prediction error) is unusually large, the role of expectation is significantly reduced and so the placebo and nocebo effects are not that strong. An unusually large prediction error could be thought of as contributing to increased uncertainty about the stimuli, which mirrors the results from <xref ref-type="bibr" rid="bib25">Hoskin et al., 2019</xref> Bayesian framework. Nevertheless, the types of stimuli used in the above studies (i.e. noxious stimuli cued by non-noxious stimuli) differed from the more ecologically valid sequences of pain that are reported by chronic pain patients (<xref ref-type="bibr" rid="bib35">Mayr et al., 2022</xref>), as we indicated above. Furthermore, <xref ref-type="bibr" rid="bib26">Jepma et al., 2018</xref>, used a conditioning paradigm and also found that expectations inﬂuence both perception and learning, in a self-reinforcing loop. Our work has followed a similar modelling strategy to <xref ref-type="bibr" rid="bib26">Jepma et al., 2018</xref>, but it goes beyond simple conditioning schedules or sequences of two-level discrete painful stimuli, showing expectancy effects even when the intensities are allowed to vary across a wider range of values and according to more complex statistical temporal structures. Additionally, given the reported role of confidence in the perception of pain (<xref ref-type="bibr" rid="bib39">Mulders et al., 2023</xref>; <xref ref-type="bibr" rid="bib9">Brown et al., 2008</xref>), we draw a more complete picture by including participants’ confidence ratings in our modelling analysis.</p><p>Future studies would need to determine whether statistical learning and its effect on pain is altered in chronic pain conditions. This is important because statistical learning could, in principle, inﬂuence how a pain state evolves. Once a pain state is initiated, how an individual learns and anticipates the ﬂuctuating pain signals may contribute to determine how well it can be regulated by the nervous system, thus affecting the severity and recurrence of pain ﬂares. This, in turn, would affect whether aversive associations with the instigating stimulus are extinguished or reinforced (<xref ref-type="bibr" rid="bib45">Seymour and Mancini, 2020</xref>). In chronic pain, dysfunctional learning may promote the amplification and maintenance of pain signals, contributing to the reinforcement of aversive associations with incident stimuli, as well as the persistence of pain (<xref ref-type="bibr" rid="bib44">Seymour, 2019</xref>; <xref ref-type="bibr" rid="bib5">Baliki and Apkarian, 2015</xref>; <xref ref-type="bibr" rid="bib59">Vlaeyen et al., 2016</xref>).</p><p>Our paper comes with open tools, which can be adapted in future studies on statistical learning in chronic pain. The key advantage of taking an hypothesis-driven, computational-neuroscience approach to quantify learning is that it allows to go beyond symptoms-mapping, identifying the quantifiable computational principles that mediate the link between symptoms and neural function.</p><p>In summary, we show that statistical expectations and confidence scale the judgement of pain in sequences of noxious stimuli as predicted by hierarchical Bayesian inference theory, opening a new avenue of research on the role of learning in pain.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Thirty-three (18 female) healthy adult participants were recruited for the experiment. The mean age of participants was 22.4±2.7 years of age (range: 18–35). Participants had no chronic condition and no infectious illnesses, as well as no skin conditions (e.g. eczema) at the site of stimulus delivery. Moreover, we only recruited participants that had not taken any anti-anxiety, anti-depressive medication, nor any illicit substances, alcohol and pain medication (including NSAIDs such as ibuprofen and paracetamol) in the 24 hr prior to the experiment. All participants gave informed written consent to take part in the study, which was approved by the local ethics committee (Department of Engineering, University of Cambridge Ethics Committee).</p></sec><sec id="s4-2"><title>Protocol</title><p>The experimental room’s temperature was maintained between 20°C and 23°C. Upon entry, an infrared thermometer was used to ensure participants’ temperature was above 36°C at the forehead and forearm of the non-dominant hand, to account for the known effects of temperature on pain perception (<xref ref-type="bibr" rid="bib49">Strigo et al., 2000</xref>). A series of slideshows were presented, which explained the premise of the experiment and demonstrated what the participant would be asked to carry out. Throughout this presentation, questions were asked to ensure participants understood the task. Participants were given multiple opportunities to ask questions throughout the presentation.</p><p>We used the Medoc Advanced Thermosensory Stimulator 2 (TSA2) (<xref ref-type="bibr" rid="bib36">Medoc Advanced Medical Systems, 2022</xref>) to deliver thermal stimuli using the CHEPS thermode. The CHEPS thermode allowed for rapid cooling (40°C/s) and heating (70°C/s) so transitions between the baseline and stimuli temperatures were minimal. The TSA2 was controlled externally, via Matlab (Mathworks).</p><p>We then established the pain threshold, using the method of limits (<xref ref-type="bibr" rid="bib33">Lue and Shih, 2017</xref>), in order to centre the range of temperature intensities used in the experiment. Each participant was provided with stimuli of increasing temperature, starting from 40°C going up in 0.5°C increments, using an inter-stimulus interval (ISI) of 2.5 s and a 2 s duration. The participant was asked to indicate when the stimuli went from warm to painful - this temperature was noted and the stimuli ended. The procedure was repeated three times, and the average was used as an estimate of the pain threshold.</p><p>During the experiment, four sequences of thermal stimuli were delivered. Due to the phenomenon of offset analgesia, where decreases in tonic pain result in a proportionally larger decrease in perceived pain (<xref ref-type="bibr" rid="bib22">Hermans et al., 2016</xref>), we chose phasic stimuli, with a duration of 2 s and an ISI 2.5 s. In order to account for individual differences, the temperatures which the levels refer to are based upon the participants’ threshold. The median intensity level was defined as threshold, giving a max temperature of 3°C above threshold, which was found to be acceptable by participants. Before the start of the experiment each participant was provided with the highest temperature stimuli that could be presented, given their measured threshold, to ensure they were comfortable with this. Two participants found the stimulus too painful - the temperature range was lowered by 1°C and this was found to be acceptable.</p><p>After every trial of each sequence, the participant was asked for either their perception of the previous stimulus or their prediction for the next stimulus through a 2D VAS (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), presented using PsychToolBox-3 (<xref ref-type="bibr" rid="bib29">Kleiner, 2007</xref>). The <italic>y</italic>-axis encodes the intensity of the stimulus either perceived or predicted, ranging from 0 (no heat detected/predicted) to 100 (worst pain imaginable perceived/predicted); on this scale, 50 represents the pain threshold. This was done as a given sequence was centred around the threshold. The <italic>x</italic>-axis encodes confidence in either perception or prediction, ranging from 0 - completely uncertain (‘unsure’) - to 1 - complete confidence in the rating (‘sure’). Differing background colours were chosen to ensure participants were aware of what was being asked, and throughout the experiment participants were reminded to take care in answering the right question. The mouse movement was limited to be inside of the coloured box, which defined the area of participants’ input. At the beginning of each input screen, the mouse location was uniformly randomised within the input box.</p><p>The sequence of response types was randomised so as to retain 40 prediction and 40 perception ratings for each of the four sequence conditions. For an 80-trial-long sequence, this gave 80 participant responses. Each sequence condition was separated by a 5 min break, during which the thermode’s probe was slightly moved around the area of skin on the forearm to reduce sensitisation (i.e. a gradual increase in perceived intensity with repetitive noxious stimuli) (<xref ref-type="bibr" rid="bib24">Hollins et al., 2011</xref>). In the middle of each sequence, there was a 3 min break. During the ISI, the temperature returned to a baseline of 38°C. One participant was unable to complete the sequence as their threshold was too low, and data from four participants was lost due to Medoc software issues (the remote control failed and the data of two out of four sessions were not saved). We excluded one participant’s whose ratings/predictions were inversely proportional to the noxious input. Thus, we analysed data from 27 participants.</p></sec><sec id="s4-3"><title>Generative process of the painful sequences</title><p>We manipulated two sources of uncertainty in the sequence: the stochasticity (<inline-formula><mml:math id="inf59"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula>) of the observation and the volatility (<inline-formula><mml:math id="inf60"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula>) of the underlying sequence (<xref ref-type="bibr" rid="bib41">Piray and Daw, 2021</xref>). Sequences were defined by two levels (high or low) of stochasticity and volatility, resulting in four different sequences conditions - creating a 2×2 factorial design. Each sequence was defined as a series of chunks, where the intensity for trial <inline-formula><mml:math id="inf61"><mml:semantics><mml:mi>t</mml:mi></mml:semantics></mml:math></inline-formula>, <inline-formula><mml:math id="inf62"><mml:semantics><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, was sampled from <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf64"><mml:semantics><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> indicates the level of stochasticity (<inline-formula><mml:math id="inf65"><mml:semantics><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1.75</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> for high level of stochasticity, <inline-formula><mml:math id="inf66"><mml:semantics><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> for low level of stochasticity). The mean of the chunk, <inline-formula><mml:math id="inf67"><mml:semantics><mml:mi>I</mml:mi></mml:semantics></mml:math></inline-formula>, was drawn from <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>3.5</mml:mn><mml:mo>,</mml:mo><mml:mn>10.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. To ensure a noticeable difference in chunk intensity to the participant, concurrent chunk means were constrained to be at least two intensity levels different. Volatility was implemented by defining the length, or number of trials, of a chunk (<inline-formula><mml:math id="inf69"><mml:semantics><mml:mi>l</mml:mi></mml:semantics></mml:math></inline-formula>) drawn from <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf71"><mml:semantics><mml:mi>L</mml:mi></mml:semantics></mml:math></inline-formula> is the mean o the chunk length (<inline-formula><mml:math id="inf72"><mml:semantics><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> for high volatility level, <inline-formula><mml:math id="inf73"><mml:semantics><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> for low volatility level). A jitter, <inline-formula><mml:math id="inf74"><mml:semantics><mml:mi>a</mml:mi></mml:semantics></mml:math></inline-formula>, was added around the mean to ensure the transition from one chunk to the next was not consistent or predictable. For both high and low volatility conditions, we set <inline-formula><mml:math id="inf75"><mml:semantics><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>. Sampled values were then discretised, where any intensities outside the valid intensity range <inline-formula><mml:math id="inf76"><mml:semantics><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>13</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> were discarded and re-sampled resulting in an 80-trial-long sequence for each condition. The mean of each sequence was centred around intensity level 7, i.e., the participants threshold. So defined, six sets of four sequences were sampled. Each participant received one set, with a randomised sequence order. See an example sequence (after subject-specific linear transformation) and one participant’s responses (including confidence ratings) in <xref ref-type="fig" rid="fig1">Figure 1C and D</xref>.</p></sec><sec id="s4-4"><title>Data pre-processing</title><p>Since the intensity values of the noxious input were discretised between 1 and 13, while the participant’s responses (perception and prediction) were given on a 0–100 scale, we applied a linear transformation of the input to map its values onto a common 0–100 range. For each participant, for a set of inputs at perception trials from the concatenated sequence (separate sequence conditions in the order as presented), we fit a linear least squares regression using Python’s scipy.stats.linregress function. On rare occasions, when the transformed input was negative, we refit the line using Python’s non-linear least squares function scipy.optimize.curve_fit, constraining the intercept above 0 (<xref ref-type="bibr" rid="bib58">Virtanen et al., 2020</xref>). We then extracted each participant’s optimised slope and intercept and applied the transformation to both the concatenated and condition-specific sequence of inputs. So transformed, the sequences were then used in all the analyses. Plots of each participant transformation can be found in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>. We superimposed participant’s responses onto the noxious input condition sequences in <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>.</p><p>To capture participant’s model-naive performance in the task, both for the concatenated and condition-specific sequence, we calculated RMSE of each participant’s perception (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) and prediction (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) responses as compared to the input. The lower the RMSE, the higher the response accuracy.<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mfrac></mml:msqrt></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mfrac></mml:msqrt></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf77"><mml:semantics><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is the number of perception trials, <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is participant’s perception response to the stimulus <inline-formula><mml:math id="inf79"><mml:semantics><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> at trial <inline-formula><mml:math id="inf80"><mml:semantics><mml:mi>t</mml:mi></mml:semantics></mml:math></inline-formula>, <inline-formula><mml:math id="inf81"><mml:semantics><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is the number of prediction trials, and <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is participant’s prediction of the next stimulus intensity <inline-formula><mml:math id="inf83"><mml:semantics><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> at trial <inline-formula><mml:math id="inf84"><mml:semantics><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>.</p></sec><sec id="s4-5"><title>Models</title><sec id="s4-5-1"><title>Reinforcement learning</title><sec id="s4-5-1-1"><title>RL</title><p>In RL models, learning is driven by discrepancies between the estimate of the expected value and observed values. Before any learning begins, at trial <inline-formula><mml:math id="inf85"><mml:semantics><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>, participants have an initial expectation, <inline-formula><mml:math id="inf86"><mml:semantics><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, which is a free parameter that we estimate.</p><p>On each trial, participants receive a thermal input <inline-formula><mml:math id="inf87"><mml:semantics><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>. We then calculate the prediction error <inline-formula><mml:math id="inf88"><mml:semantics><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, defined as the difference between the expectation <inline-formula><mml:math id="inf89"><mml:semantics><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and the input <inline-formula><mml:math id="inf90"><mml:semantics><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>).<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Participant is then assumed to update their expectation of the stimulus on the next trial as in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:semantics><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf91"><mml:semantics><mml:mi>α</mml:mi></mml:semantics></mml:math></inline-formula> is the learning rate (free parameter), which governs how fast participants assimilate new information to update their belief.</p><p>On trials when participants rate their perceived intensity, we assume no effects on their perception other than conﬁdence rating <inline-formula><mml:math id="inf92"><mml:semantics><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and response noise, so participants’ perception response <inline-formula><mml:math id="inf93"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is drawn from a Gaussian distribution, with the mean <inline-formula><mml:math id="inf94"><mml:semantics><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and a confidence-scaled response noise <inline-formula><mml:math id="inf95"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula> (free parameter), as in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf96"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula> is the confidence scaling factor (free parameter), which deﬁnes the extent to which conﬁdence affects response uncertainty. Please see <xref ref-type="fig" rid="fig4">Figure 4</xref> for an intuition behind confidence scaling.</p><p>On trials when participants are asked to predict the intensity of the next thermal stimulus, we use the updated expectation <inline-formula><mml:math id="inf97"><mml:semantics><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> to model participants’ prediction response <inline-formula><mml:math id="inf98"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>E</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>. This is similarly affected by confidence rating and response noise and is deﬁned as in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>.<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To recap, the RL model has four free parameters: the learning rate <inline-formula><mml:math id="inf99"><mml:semantics><mml:mi>α</mml:mi></mml:semantics></mml:math></inline-formula>, response noise <inline-formula><mml:math id="inf100"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula>, the initial expectation <inline-formula><mml:math id="inf101"><mml:semantics><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, and the confidence scaling factor <inline-formula><mml:math id="inf102"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula>.</p></sec><sec id="s4-5-1-2"><title>eRL</title><p>Additionally, where we investigate the effects of expectation on the perception of pain (<xref ref-type="bibr" rid="bib26">Jepma et al., 2018</xref>), we included an element that allows us to express the perception as a weighted sum of the input and expectation (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>):<disp-formula id="equ7"> <label>(7)</label><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf103"><mml:semantics><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> (free parameter) captures how much participants rely on the normative thermal input vs. their expectation. When <inline-formula><mml:math id="inf104"><mml:semantics><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>, the expectation plays no role and the model simplifies to that of the standard RL above. In total, the eRL model has five free parameters, with the other equations the same as in the RL model, with the exception of the prediction error, which now relies on the expectation weighted pain perception <inline-formula><mml:math id="inf105"><mml:semantics><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>).<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:semantics><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></disp-formula></p></sec></sec></sec><sec id="s4-6"><title>Kalman filter</title><sec id="s4-6-1"><title>KF</title><p>To capture sequential learning in a Bayesian manner, we used the KF model (<xref ref-type="bibr" rid="bib26">Jepma et al., 2018</xref>; <xref ref-type="bibr" rid="bib42">Särkkä, 2013</xref>; <xref ref-type="bibr" rid="bib27">Kalman, 1960</xref>). KF assumes a generative model of the environment where the latent state on trial <italic>t</italic>, <inline-formula><mml:math id="inf106"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (the mean of the sequences in the experiment), evolves according to a Gaussian random walk with a fixed drift rate, <inline-formula><mml:math id="inf107"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula> (volatility), as in <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>.<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The observation on trial <italic>t</italic>, <inline-formula><mml:math id="inf108"><mml:semantics><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, is then drawn from a Gaussian (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref>) with a fixed variance, which represents the observation uncertainty <inline-formula><mml:math id="inf109"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula> (stochasticity).<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>As such the KF assumes stable dynamics since the generative process has fixed volatility and stochasticity.</p><p>For ease of explanation, we refer to the thermal input at each trial as <inline-formula><mml:math id="inf110"><mml:semantics><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, we also use the <inline-formula><mml:math id="inf111"><mml:semantics><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> notation, which refers to a sequence of observations up to and including trial <inline-formula><mml:math id="inf112"><mml:semantics><mml:mi>t</mml:mi></mml:semantics></mml:math></inline-formula>. The model allows to obtain posterior beliefs about the latent state <inline-formula><mml:math id="inf113"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> given the observations. This is done by tracking an internal estimate of the mean <inline-formula><mml:math id="inf114"><mml:semantics><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and the uncertainty, <inline-formula><mml:math id="inf115"><mml:semantics><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, of the latent state <inline-formula><mml:math id="inf116"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>.</p><p>First, following standard KF results, on each trial, the participant is assumed to hold a prior belief (indicated with (–) superscript) about the latent state, <inline-formula><mml:math id="inf117"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ1">Equation 11</xref>).<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>On the first trial, before any observations, we set <inline-formula><mml:math id="inf118"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (free parameters). In light of the new observation, <inline-formula><mml:math id="inf119"><mml:semantics><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> on trial <inline-formula><mml:math id="inf120"><mml:semantics><mml:mi>t</mml:mi></mml:semantics></mml:math></inline-formula>, the tracked mean and uncertainty of the latent state are reweighed based on the new evidence <inline-formula><mml:math id="inf121"><mml:semantics><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and its associated observation uncertainty <inline-formula><mml:math id="inf122"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula> as in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>.<disp-formula id="equ12"> <label>(12)</label><mml:math id="m12"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We can then define the learning rate <inline-formula><mml:math id="inf123"><mml:semantics><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>),<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:semantics><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>to get the update rule for the new posterior beliefs (indicated with (+) superscript) about the mean (<xref ref-type="disp-formula" rid="equ14">Equation 14</xref>) and uncertainty (<xref ref-type="disp-formula" rid="equ15">Equation 15</xref>) of <inline-formula><mml:math id="inf124"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>.<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></disp-formula><disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>+</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Following this new belief, and the assumption about the environmental dynamics (volatility), the participant forms a new prior belief about the latent state <inline-formula><mml:math id="inf125"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> for the next trial <inline-formula><mml:math id="inf126"><mml:semantics><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> as in <xref ref-type="disp-formula" rid="equ16">Equation 16</xref>.<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:semantics></mml:math></disp-formula><disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>We can simplify the notation to make it comparable to the RL models. We let <inline-formula><mml:math id="inf127"><mml:semantics><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula>, and <inline-formula><mml:math id="inf128"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>. Following a new observation at trial <inline-formula><mml:math id="inf129"><mml:semantics><mml:mi>t</mml:mi></mml:semantics></mml:math></inline-formula>, we calculate the prediction error (<xref ref-type="disp-formula" rid="equ19">Equation 19</xref>) and learning rate (<xref ref-type="disp-formula" rid="equ20">Equation 20</xref>).<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:semantics><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></disp-formula><disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:semantics><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>We then update the belief about the mean (<xref ref-type="disp-formula" rid="equ21">Equation 21</xref>) and uncertainty (<xref ref-type="disp-formula" rid="equ22">Equation 22</xref>) of the latent state for the next trial.<disp-formula id="equ21"> <label>(21)</label><mml:math id="m21"><mml:semantics><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:semantics></mml:math></disp-formula><disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>Now, mapping this onto the experiment, the mean of the latent state is participants’ expectation <inline-formula><mml:math id="inf130"><mml:semantics><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, and so we have participant perception rating modelled as in <xref ref-type="disp-formula" rid="equ23">Equation 23</xref>.<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>and the prediction rating for the next trial as in <xref ref-type="disp-formula" rid="equ24">Equation 24</xref>.<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In total the model has six free parameters: <inline-formula><mml:math id="inf131"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula> (environmental stochasticity), <inline-formula><mml:math id="inf132"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula> (environmental volatility), <inline-formula><mml:math id="inf133"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula> (response noise), <inline-formula><mml:math id="inf134"><mml:semantics><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (initial belief about the mean), <inline-formula><mml:math id="inf135"><mml:semantics><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (initial belief about the uncertainty), and <inline-formula><mml:math id="inf136"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula> (confidence scaling factor).</p></sec><sec id="s4-6-2"><title>eKF</title><p>We can introduce the effect of expectation on the pain perception, by assuming that participants treat the thermal input as an imperfect indicator of the true level of pain (<xref ref-type="bibr" rid="bib26">Jepma et al., 2018</xref>). In this case, the input, <inline-formula><mml:math id="inf137"><mml:semantics><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, is modelled as in <xref ref-type="disp-formula" rid="equ25">Equation 25</xref>:<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>which forms an expression for the likelihood of the observation and adds an additional level to the inference, slightly modifying the KF assumptions such that:<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>However, we can apply the standard KF results and Bayes’ rule to arrive at simple update rules for the participants’ belief about the mean and uncertainty of the latent state <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula><italic><sub>t</sub></italic>. From this, we get a prior on the <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> defined in <xref ref-type="disp-formula" rid="equ27">Equation 27</xref>:<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>which, following a new input <inline-formula><mml:math id="inf140"><mml:semantics><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, gives us the posterior belief about <inline-formula><mml:math id="inf141"><mml:semantics><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> as in <xref ref-type="disp-formula" rid="equ28">Equation 28</xref>.<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Now, if we define <inline-formula><mml:math id="inf142"><mml:semantics><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> as in <xref ref-type="disp-formula" rid="equ29">Equation 29</xref>:<disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:semantics><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>We have that the posterior belief about the mean level of pain <inline-formula><mml:math id="inf143"><mml:semantics><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is calculated as:<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msubsup><mml:mi>m</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>which is a weighted sum of the input <inline-formula><mml:math id="inf144"><mml:semantics><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and participant expectation about the latent state <inline-formula><mml:math id="inf145"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, governed by the perceptual weight <inline-formula><mml:math id="inf146"><mml:semantics><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, analogously to the eRL model. Finally, the posterior belief about <inline-formula><mml:math id="inf147"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is obtained in <xref ref-type="disp-formula" rid="equ31">Equation 31</xref>.<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Now, setting the learning rate as in <xref ref-type="disp-formula" rid="equ32">Equation 32</xref>:<disp-formula id="equ32"><label>(32)</label><mml:math id="m32"><mml:semantics><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>we get:<disp-formula id="equ33"><label>(33)</label><mml:math id="m33"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></disp-formula><disp-formula id="equ34"><label>(34)</label><mml:math id="m34"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>Next, following the same notation simplification as before, we get the update rules for the prior belief about the mean (<xref ref-type="disp-formula" rid="equ35">Equation 35</xref>) and uncertainty (<xref ref-type="disp-formula" rid="equ36">Equation 36</xref>) of the latent state <inline-formula><mml:math id="inf148"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> for the next trial.<disp-formula id="equ35"><label>(35)</label><mml:math id="m35"><mml:semantics><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:semantics></mml:math></disp-formula><disp-formula id="equ36"><label>(36)</label><mml:math id="m36"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>as well as the expression for subjective perception, <inline-formula><mml:math id="inf149"><mml:semantics><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, at trial <inline-formula><mml:math id="inf150"><mml:semantics><mml:mi>t</mml:mi></mml:semantics></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ37">Equation 37</xref>).<disp-formula id="equ37"><label>(37)</label><mml:math id="m37"><mml:semantics><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>The perception and prediction responses are modelled analogously as the KF model. In total, the model has seven free parameters: <inline-formula><mml:math id="inf151"><mml:semantics><mml:mi>ϵ</mml:mi></mml:semantics></mml:math></inline-formula> (subjective noise), <inline-formula><mml:math id="inf152"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula> (environmental stochasticity), <inline-formula><mml:math id="inf153"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula> (environmental volatility), <inline-formula><mml:math id="inf154"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula> (response noise), <inline-formula><mml:math id="inf155"><mml:semantics><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (initial belief about the mean), <inline-formula><mml:math id="inf156"><mml:semantics><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (initial belief about the uncertainty), and <italic>C</italic> (confidence scaling factor).</p></sec></sec><sec id="s4-7"><title>Random model</title><p>As a baseline, we also included a model that performs a random guess. The perceptual/prediction ratings were modelled as in <xref ref-type="disp-formula" rid="equ38">Equation 38</xref>.<disp-formula id="equ38"><label>(38)</label><mml:math id="m38"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The model has three free parameters: <inline-formula><mml:math id="inf157"><mml:semantics><mml:mi>R</mml:mi></mml:semantics></mml:math></inline-formula>, <inline-formula><mml:math id="inf158"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula>, and <inline-formula><mml:math id="inf159"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula>, where <inline-formula><mml:math id="inf160"><mml:semantics><mml:mi>R</mml:mi></mml:semantics></mml:math></inline-formula> is a constant value that participants respond with.</p></sec><sec id="s4-8"><title>Model fitting</title><p>Model parameters were estimated using hierarchical Bayesian methods, performed with RStan package (v. 2.21.0) (<xref ref-type="bibr" rid="bib48">Stan Development Team, 2019</xref>) in R (v. 4.0.2) based on Markov Chain Monte Carlo techniques (No-U-Turn Hamiltonian Monte Carlo). For the individual-level parameters we used non-centred parametrisation (<xref ref-type="bibr" rid="bib40">Papaspiliopoulos et al., 2007</xref>). For the group-level parameters we used <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> priors for the mean, and the gamma-mixture representation of the Student’s-t(3,0,1) for the scale (<xref ref-type="bibr" rid="bib47">Stan Development, 2022</xref>). Parameters in the (0, 1) range were constrained using Phi_approx - a logistic approximation to the cumulative Normal distribution (<xref ref-type="bibr" rid="bib8">Bowling et al., 2009</xref>).</p><p>For each condition and each of the four chains, we ran 6000 samples (after discarding 6000 warm-up ones). For each condition, we examined R-hat values for each individual- (including the <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> error term from the non-centred parametrisation) and group-level parameters from each model to verify whether the Markov chains have converged. At the group-level and individual-level, all R-hat values had a value &lt;1.1, indicating convergence. In the random response model, 0.01–0.16% iterations saturated the maximum tree depth of 11.</p><sec id="s4-8-1"><title>Model comparison</title><p>For model comparison, we used R package loo, which provides eﬃcient approximate leave-one-out (LOO) cross-validation. The package allows to estimate the difference in models’ expected predictive accuracy through the difference in ELPD (<xref ref-type="bibr" rid="bib57">Vehtari et al., 2017</xref>). By looking at the ratio between the ELPD difference and the SE of the difference, we get the sigma effect - a heuristic for significance of such model differences. There’s no agreed-upon threshold of SEs that determines significance, but the higher the sigma difference, the more robust is the eﬀect. The closeness of fit can also be captured with LOO information criterion (LOOIC), where the lower LOOIC values indicate better fit.</p></sec><sec id="s4-8-2"><title>Parameter comparison</title><p>For the comparison of group-level parameters between conditions, we extracted 95% high-density intervals of the permuted and merged (across chains) posterior samples of each group-level parameter (<xref ref-type="bibr" rid="bib32">Kruschke, 2023</xref>). To assess significant differences between conditions, we calculated a difference between such defined intervals. In the Bayesian scenario, a significant difference is indicated by the interval not containing the value 0 (<xref ref-type="bibr" rid="bib3">Aylward et al., 2019</xref>; <xref ref-type="bibr" rid="bib1">Ahn et al., 2017</xref>).</p></sec><sec id="s4-8-3"><title>Parameter and model recovery</title><p>To asses the reliability of our modelling analysis (<xref ref-type="bibr" rid="bib62">Wilson and Collins, 2019</xref>), for each model we performed parameter recovery analysis, where we simulated participants’ responses using newly drawn individual-level parameters from the group-level distributions.</p><p>We repurposed existing sequences of noxious inputs in the [1, 13] range (pre-transformation). When then applied a linear transformation to the input sequences using sampled slope and intercept coeﬃcients from a Gaussian distribution of these coeﬃcients that we estimated based on our dataset using R’s fitdistrplus package. Furthermore, we simulated the confidence ratings based on lag-1 auto-correlation across a moving window of the transformed input sequence.</p><p>We then fit the same model to the simulated data and calculated Pearson correlation coeﬃcients <inline-formula><mml:math id="inf163"><mml:semantics><mml:mi>r</mml:mi></mml:semantics></mml:math></inline-formula> between the generated and estimated individual-level parameters. The higher the coeﬃcient <inline-formula><mml:math id="inf164"><mml:semantics><mml:mi>r</mml:mi></mml:semantics></mml:math></inline-formula>, the more reliable the estimates are, which can be categorised as: poor (if <italic>r</italic>&lt;0.5); fair (if 0.5&lt;<italic>r</italic>&lt;0.75); good (0.75&lt;<italic>r</italic>&lt;0.9); excellent (if <italic>r</italic>&gt;0.9) (<xref ref-type="bibr" rid="bib60">White et al., 2018</xref>). Results are reported in <xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref> and <xref ref-type="fig" rid="app1fig6">Appendix 1—figures 6</xref>–<xref ref-type="fig" rid="app1fig11">11</xref>.</p><p>We also performed model recovery analysis (<xref ref-type="bibr" rid="bib62">Wilson and Collins, 2019</xref>), where we first simulated responses using each model and then fit each model-specific dataset with each model. We then counted the number of times a model fit the simulated data best (according to the LOOIC rule), effectively creating an <italic>M</italic>×<italic>M</italic> confusion matrix, where <italic>M</italic> is the number of models. In the case where we have a diagonal matrix of ones, the models are perfectly recoverable and hence as reliable as possible. Results are reported in <xref ref-type="table" rid="app1table4">Appendix 1—table 4</xref>.</p><p>In <xref ref-type="table" rid="app1table6 app1table8 app1table7 app1table9">Appendix 1—Tables 6–9</xref> we report bulk and tail effective sample size (ESS) for each condition, for each model and parameter.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Formal analysis, Methodology</p></fn><fn fn-type="con" id="con4"><p>Formal analysis</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Methodology</p></fn><fn fn-type="con" id="con6"><p>Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All participants gave informed written consent to take part in the study, which was approved by the the ethics committee of the Department of Engineering, University of Cambridge.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-90634-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All code and data are openly available on Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.11394627">https://doi.org/10.5281/zenodo.11394627</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Onysk</surname><given-names>J</given-names></name><name><surname>Gregory</surname><given-names>N</given-names></name><name><surname>Whitefield</surname><given-names>M</given-names></name><name><surname>Jain</surname><given-names>M</given-names></name><name><surname>Turner</surname><given-names>G</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Mancini</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Statistical learning shapes pain perception and prediction independently of external cues</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.11394627</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>The study was funded by an MRC Career Development Award to FM (MR/T010614/1) and a UKRI Advanced Pain Discovery Platform grant to both FM and BS (MR/W027593/1). BS was also funded by Wellcome (214251/Z/18/Z), Versus Arthritis (21537), and IITP (MSIT 2019-0-01371). This work has been performed using resources provided by the Cambridge Tier-2 system operated by the University of Cambridge Research Computing Service (<ext-link ext-link-type="uri" xlink:href="https://www.hpc.cam.ac.uk/">https://www.hpc.cam.ac.uk/</ext-link>) funded by EPSRC Tier-2 capital grant (EP/T022159/1). HPC access was additionally funded by an EPSRC research infrastructure grant to FM. We are grateful to Prof. Máté Lengyel and Prof. Deborah Talmi for helpful discussions about the study. For the purpose of open access, the author has applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising from this submission.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahn</surname><given-names>WY</given-names></name><name><surname>Haines</surname><given-names>N</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Revealing neurocomputational mechanisms of reinforcement learning and decision-making with the hBayesDM package</article-title><source>Computational Psychiatry</source><volume>1</volume><fpage>24</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1162/CPSY_a_00002</pub-id><pub-id pub-id-type="pmid">29601060</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anchisi</surname><given-names>D</given-names></name><name><surname>Zanon</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A Bayesian perspective on sensory and cognitive integration in pain perception and placebo analgesia</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0117270</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0117270</pub-id><pub-id pub-id-type="pmid">25664586</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aylward</surname><given-names>J</given-names></name><name><surname>Valton</surname><given-names>V</given-names></name><name><surname>Ahn</surname><given-names>WY</given-names></name><name><surname>Bond</surname><given-names>RL</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Roiser</surname><given-names>JP</given-names></name><name><surname>Robinson</surname><given-names>OJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Altered learning under uncertainty in unmedicated mood and anxiety disorders</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>1116</fpage><lpage>1123</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0628-0</pub-id><pub-id pub-id-type="pmid">31209369</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baliki</surname><given-names>MN</given-names></name><name><surname>Petre</surname><given-names>B</given-names></name><name><surname>Torbey</surname><given-names>S</given-names></name><name><surname>Herrmann</surname><given-names>KM</given-names></name><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Schnitzer</surname><given-names>TJ</given-names></name><name><surname>Fields</surname><given-names>HL</given-names></name><name><surname>Apkarian</surname><given-names>AV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Corticostriatal functional connectivity predicts transition to chronic back pain</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1117</fpage><lpage>1119</lpage><pub-id pub-id-type="doi">10.1038/nn.3153</pub-id><pub-id pub-id-type="pmid">22751038</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baliki</surname><given-names>MN</given-names></name><name><surname>Apkarian</surname><given-names>AV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Nociception, pain, negative moods, and behavior selection</article-title><source>Neuron</source><volume>87</volume><fpage>474</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.06.005</pub-id><pub-id pub-id-type="pmid">26247858</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bannister</surname><given-names>K</given-names></name><name><surname>Dickenson</surname><given-names>AH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The plasticity of descending controls in pain: translational probing</article-title><source>The Journal of Physiology</source><volume>595</volume><fpage>4159</fpage><lpage>4166</lpage><pub-id pub-id-type="doi">10.1113/JP274165</pub-id><pub-id pub-id-type="pmid">28387936</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blasini</surname><given-names>M</given-names></name><name><surname>Corsi</surname><given-names>N</given-names></name><name><surname>Klinger</surname><given-names>R</given-names></name><name><surname>Colloca</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Nocebo and pain: an overview of the psychoneurobiological mechanisms</article-title><source>PAIN Reports</source><volume>2</volume><elocation-id>e585</elocation-id><pub-id pub-id-type="doi">10.1097/PR9.0000000000000585</pub-id><pub-id pub-id-type="pmid">28971165</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowling</surname><given-names>SR</given-names></name><name><surname>Khasawneh</surname><given-names>MT</given-names></name><name><surname>Kaewkuekool</surname><given-names>S</given-names></name><name><surname>Cho</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A logistic approximation to the cumulative normal distribution</article-title><source>Journal of Industrial Engineering and Management</source><volume>2</volume><fpage>114</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.3926/jiem.2009.v2n1.p114-127</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>CA</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>El-Deredy</surname><given-names>W</given-names></name><name><surname>Jones</surname><given-names>AKP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Confidence in beliefs about pain predicts expectancy effects on pain perception and anticipatory processing in right anterior insula</article-title><source>Pain</source><volume>139</volume><fpage>324</fpage><lpage>332</lpage><pub-id pub-id-type="doi">10.1016/j.pain.2008.04.028</pub-id><pub-id pub-id-type="pmid">18584963</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruehl</surname><given-names>S</given-names></name><name><surname>McCubbin</surname><given-names>JA</given-names></name><name><surname>Harden</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Theoretical review: altered pain regulatory systems in chronic pain</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>23</volume><fpage>877</fpage><lpage>890</lpage><pub-id pub-id-type="doi">10.1016/S0149-7634(99)00039-1</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Büchel</surname><given-names>C</given-names></name><name><surname>Geuter</surname><given-names>S</given-names></name><name><surname>Sprenger</surname><given-names>C</given-names></name><name><surname>Eippert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Placebo analgesia: A predictive coding perspective</article-title><source>Neuron</source><volume>81</volume><fpage>1223</fpage><lpage>1239</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.02.042</pub-id><pub-id pub-id-type="pmid">24656247</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bushnell</surname><given-names>MC</given-names></name><name><surname>Ceko</surname><given-names>M</given-names></name><name><surname>Low</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cognitive and emotional control of pain and its disruption in chronic pain</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>502</fpage><lpage>511</lpage><pub-id pub-id-type="doi">10.1038/nrn3516</pub-id><pub-id pub-id-type="pmid">23719569</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colloca</surname><given-names>L</given-names></name><name><surname>Sigaudo</surname><given-names>M</given-names></name><name><surname>Benedetti</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The role of learning in nocebo and placebo effects</article-title><source>Pain</source><volume>136</volume><fpage>211</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1016/j.pain.2008.02.006</pub-id><pub-id pub-id-type="pmid">18372113</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Meyniel</surname><given-names>F</given-names></name><name><surname>Wacongne</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Pallier</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The neural representation of sequences: from transition probabilities to algebraic patterns and linguistic trees</article-title><source>Neuron</source><volume>88</volume><fpage>2</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.019</pub-id><pub-id pub-id-type="pmid">26447569</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eippert</surname><given-names>F</given-names></name><name><surname>Finsterbusch</surname><given-names>J</given-names></name><name><surname>Bingel</surname><given-names>U</given-names></name><name><surname>Büchel</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Direct evidence for spinal cord involvement in placebo analgesia</article-title><source>Science</source><volume>326</volume><elocation-id>404</elocation-id><pub-id pub-id-type="doi">10.1126/science.1180142</pub-id><pub-id pub-id-type="pmid">19833962</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fardo</surname><given-names>F</given-names></name><name><surname>Auksztulewicz</surname><given-names>R</given-names></name><name><surname>Allen</surname><given-names>M</given-names></name><name><surname>Dietz</surname><given-names>MJ</given-names></name><name><surname>Roepstorff</surname><given-names>A</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Expectation violation and attention to pain jointly modulate neural gain in somatosensory cortex</article-title><source>NeuroImage</source><volume>153</volume><fpage>109</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.03.041</pub-id><pub-id pub-id-type="pmid">28341164</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fields</surname><given-names>HL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How expectations influence pain</article-title><source>Pain</source><volume>159 Suppl 1</volume><fpage>S3</fpage><lpage>S10</lpage><pub-id pub-id-type="doi">10.1097/j.pain.0000000000001272</pub-id><pub-id pub-id-type="pmid">30113941</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiser</surname><given-names>J</given-names></name><name><surname>Lengyel</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A common probabilistic framework for perceptual and statistical learning</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>218</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.09.007</pub-id><pub-id pub-id-type="pmid">31669722</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foss</surname><given-names>JM</given-names></name><name><surname>Apkarian</surname><given-names>AV</given-names></name><name><surname>Chialvo</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dynamics of pain: fractal dimension of temporal variability of spontaneous pain differentiates between pain States</article-title><source>Journal of Neurophysiology</source><volume>95</volume><fpage>730</fpage><lpage>736</lpage><pub-id pub-id-type="doi">10.1152/jn.00768.2005</pub-id><pub-id pub-id-type="pmid">16282201</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geuter</surname><given-names>S</given-names></name><name><surname>Büchel</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Facilitation of pain in the human spinal cord by nocebo treatment</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>13784</fpage><lpage>13790</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2191-13.2013</pub-id><pub-id pub-id-type="pmid">23966699</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heald</surname><given-names>JB</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Contextual inference in learning and memory</article-title><source>Trends in Cognitive Sciences</source><volume>27</volume><fpage>43</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2022.10.004</pub-id><pub-id pub-id-type="pmid">36435674</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hermans</surname><given-names>L</given-names></name><name><surname>Calders</surname><given-names>P</given-names></name><name><surname>Van Oosterwijck</surname><given-names>J</given-names></name><name><surname>Verschelde</surname><given-names>E</given-names></name><name><surname>Bertel</surname><given-names>E</given-names></name><name><surname>Meeus</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>An Overview of Offset Analgesia and the Comparison with Conditioned Pain Modulation: A Systematic Literature Review</article-title><source>Pain Physician</source><volume>19</volume><fpage>307</fpage><lpage>326</lpage><pub-id pub-id-type="pmid">27454261</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hird</surname><given-names>EJ</given-names></name><name><surname>Charalambous</surname><given-names>C</given-names></name><name><surname>El-Deredy</surname><given-names>W</given-names></name><name><surname>Jones</surname><given-names>AKP</given-names></name><name><surname>Talmi</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Boundary effects of expectation in human pain perception</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>9443</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-45811-x</pub-id><pub-id pub-id-type="pmid">31263144</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hollins</surname><given-names>M</given-names></name><name><surname>Harper</surname><given-names>D</given-names></name><name><surname>Maixner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Changes in pain from a repetitive thermal stimulus: the roles of adaptation and sensitization</article-title><source>Pain</source><volume>152</volume><fpage>1583</fpage><lpage>1590</lpage><pub-id pub-id-type="doi">10.1016/j.pain.2011.02.049</pub-id><pub-id pub-id-type="pmid">21454015</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoskin</surname><given-names>R</given-names></name><name><surname>Berzuini</surname><given-names>C</given-names></name><name><surname>Acosta-Kane</surname><given-names>D</given-names></name><name><surname>El-Deredy</surname><given-names>W</given-names></name><name><surname>Guo</surname><given-names>H</given-names></name><name><surname>Talmi</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sensitivity to pain expectations: a bayesian model of individual differences</article-title><source>Cognition</source><volume>182</volume><fpage>127</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2018.08.022</pub-id><pub-id pub-id-type="pmid">30243037</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jepma</surname><given-names>M</given-names></name><name><surname>Koban</surname><given-names>L</given-names></name><name><surname>van Doorn</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>M</given-names></name><name><surname>Wager</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Behavioural and neural evidence for self-reinforcing expectancy effects on pain</article-title><source>Nature Human Behaviour</source><volume>2</volume><fpage>838</fpage><lpage>855</lpage><pub-id pub-id-type="doi">10.1038/s41562-018-0455-8</pub-id><pub-id pub-id-type="pmid">31558818</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalman</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>A new approach to linear filtering and prediction problems</article-title><source>Journal of Basic Engineering</source><volume>82</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1115/1.3662552</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>CD</given-names></name><name><surname>Wong</surname><given-names>F</given-names></name><name><surname>Currie</surname><given-names>T</given-names></name><name><surname>Mauderli</surname><given-names>AP</given-names></name><name><surname>Fillingim</surname><given-names>RB</given-names></name><name><surname>Riley</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Deficiency in endogenous modulation of prolonged heat pain in patients with irritable bowel syndrome and temporomandibular disorder</article-title><source>Pain</source><volume>143</volume><fpage>172</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.pain.2008.12.027</pub-id><pub-id pub-id-type="pmid">19278784</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in psychtoolbox-3</article-title><source>Perception</source><volume>36</volume><fpage>1</fpage><lpage>16</lpage></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Knill</surname><given-names>DC</given-names></name><name><surname>Richards</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1996">1996</year><source>Perception as Bayesian Inference</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Welchman</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning predictive structure without a teacher: decision strategies and brain routes</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>130</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.09.014</pub-id><pub-id pub-id-type="pmid">31569060</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kruschke</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2023">2023</year><source>Doing Bayesian data analysis: a Tutorial</source><person-group person-group-type="editor"><name><surname>Jags</surname><given-names>R</given-names></name></person-group><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lue</surname><given-names>YF</given-names></name><name><surname>Shih</surname><given-names>YC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Method of limit and method of level for thermal and pain detection assessment</article-title><source>International Journal of Physical Therapy &amp; Rehabilitation</source><volume>3</volume><elocation-id>130</elocation-id><pub-id pub-id-type="doi">10.15344/2455-7498/2017/130</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mancini</surname><given-names>F</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Computational and neural mechanisms of statistical pain learning</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>6613</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-34283-9</pub-id><pub-id pub-id-type="pmid">36329014</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mayr</surname><given-names>A</given-names></name><name><surname>Jahn</surname><given-names>P</given-names></name><name><surname>Stankewitz</surname><given-names>A</given-names></name><name><surname>Deak</surname><given-names>B</given-names></name><name><surname>Winkler</surname><given-names>A</given-names></name><name><surname>Witkovsky</surname><given-names>V</given-names></name><name><surname>Eren</surname><given-names>O</given-names></name><name><surname>Straube</surname><given-names>A</given-names></name><name><surname>Schulz</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Patients with chronic pain exhibit individually unique cortical signatures of pain encoding</article-title><source>Human Brain Mapping</source><volume>43</volume><fpage>1676</fpage><lpage>1693</lpage><pub-id pub-id-type="doi">10.1002/hbm.25750</pub-id><pub-id pub-id-type="pmid">34921467</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Medoc Advanced Medical Systems</collab></person-group><year iso-8601-date="2022">2022</year><article-title>TSA 2 - Advanced thermosensory stimulator</article-title><ext-link ext-link-type="uri" xlink:href="https://www.medoc-web.com/tsa-2">https://www.medoc-web.com/tsa-2</ext-link><date-in-citation iso-8601-date="2022-08-15">August 15, 2022</date-in-citation></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyniel</surname><given-names>F</given-names></name><name><surname>Maheu</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Human inferences about sequences: a minimal transition probability model</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005260</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005260</pub-id><pub-id pub-id-type="pmid">28030543</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyniel</surname><given-names>F</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Brain networks for confidence weighting and hierarchical inference during probabilistic learning</article-title><source>PNAS</source><volume>114</volume><fpage>E3859</fpage><lpage>E3868</lpage><pub-id pub-id-type="doi">10.1073/pnas.1615773114</pub-id><pub-id pub-id-type="pmid">28439014</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mulders</surname><given-names>D</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Mouraux</surname><given-names>A</given-names></name><name><surname>Mancini</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Confidence of probabilistic predictions modulates the cortical response to pain</article-title><source>PNAS</source><volume>120</volume><elocation-id>e2212252120</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2212252120</pub-id><pub-id pub-id-type="pmid">36669115</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papaspiliopoulos</surname><given-names>O</given-names></name><name><surname>Roberts</surname><given-names>GO</given-names></name><name><surname>Sköld</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A general framework for the parametrization of hierarchical models</article-title><source>Statistical Science</source><volume>22</volume><fpage>59</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1214/088342307000000014</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piray</surname><given-names>P</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A model for learning based on the joint estimation of stochasticity and volatility</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>6587</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-26731-9</pub-id><pub-id pub-id-type="pmid">34782597</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Särkkä</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Bayesian FIltering and Smoothing</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9781139344203</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>A</given-names></name><name><surname>Turk-Browne</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Statistical learning</article-title><source>Brain Mapping</source><volume>3</volume><fpage>501</fpage><lpage>506</lpage></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seymour</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Pain: a precision signal for reinforcement learning and control</article-title><source>Neuron</source><volume>101</volume><fpage>1029</fpage><lpage>1041</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.01.055</pub-id><pub-id pub-id-type="pmid">30897355</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Mancini</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Hierarchical models of pain: Inference, information-seeking, and adaptive control</article-title><source>NeuroImage</source><volume>222</volume><elocation-id>117212</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117212</pub-id><pub-id pub-id-type="pmid">32739554</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherman</surname><given-names>BE</given-names></name><name><surname>Graves</surname><given-names>KN</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The prevalence and importance of statistical learning in human cognition and behavior</article-title><source>Current Opinion in Behavioral Sciences</source><volume>32</volume><fpage>15</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.01.015</pub-id><pub-id pub-id-type="pmid">32258249</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Stan Development</collab></person-group><year iso-8601-date="2022">2022</year><article-title>Reparameterization stan user’s guide</article-title><ext-link ext-link-type="uri" xlink:href="https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html">https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html</ext-link><date-in-citation iso-8601-date="2022-08-15">August 15, 2022</date-in-citation></element-citation></ref><ref id="bib48"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Stan Development Team</collab></person-group><year iso-8601-date="2019">2019</year><data-title>Stan</data-title><source>Stan</source><ext-link ext-link-type="uri" xlink:href="http://mc-stan.org/">http://mc-stan.org/</ext-link></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strigo</surname><given-names>IA</given-names></name><name><surname>Carli</surname><given-names>F</given-names></name><name><surname>Bushnell</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Effect of ambient temperature on human pain and temperature perception</article-title><source>Anesthesiology</source><volume>92</volume><fpage>699</fpage><lpage>707</lpage><pub-id pub-id-type="doi">10.1097/00000542-200003000-00014</pub-id><pub-id pub-id-type="pmid">10719949</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning: An Introduction. Adaptive Computation and Machine Learning Series</source><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabor</surname><given-names>A</given-names></name><name><surname>Thacker</surname><given-names>MA</given-names></name><name><surname>Moseley</surname><given-names>GL</given-names></name><name><surname>Körding</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Pain: a statistical account</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005142</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005142</pub-id><pub-id pub-id-type="pmid">28081134</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabor</surname><given-names>A</given-names></name><name><surname>Burr</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Bayesian learning models of pain: a call to action</article-title><source>Current Opinion in Behavioral Sciences</source><volume>26</volume><fpage>54</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2018.10.006</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tinnermann</surname><given-names>A</given-names></name><name><surname>Geuter</surname><given-names>S</given-names></name><name><surname>Sprenger</surname><given-names>C</given-names></name><name><surname>Finsterbusch</surname><given-names>J</given-names></name><name><surname>Büchel</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Interactions between brain and spinal cord mediate value effects in nocebo hyperalgesia</article-title><source>Science</source><volume>358</volume><fpage>105</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1126/science.aan1221</pub-id><pub-id pub-id-type="pmid">28983051</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tracey</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Getting the pain you expect: mechanisms of placebo, nocebo and reappraisal effects in humans</article-title><source>Nature Medicine</source><volume>16</volume><fpage>1277</fpage><lpage>1283</lpage><pub-id pub-id-type="doi">10.1038/nm.2229</pub-id><pub-id pub-id-type="pmid">20948533</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tracey</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A vulnerability to chronic pain and its interrelationship with resistance to analgesia</article-title><source>Brain</source><volume>139</volume><fpage>1869</fpage><lpage>1872</lpage><pub-id pub-id-type="doi">10.1093/brain/aww147</pub-id><pub-id pub-id-type="pmid">27343219</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Scholl</surname><given-names>BJ</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name><name><surname>Johnson</surname><given-names>MK</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neural evidence of statistical learning: efficient detection of visual regularities without awareness</article-title><source>Journal of Cognitive Neuroscience</source><volume>21</volume><fpage>1934</fpage><lpage>1945</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21131</pub-id><pub-id pub-id-type="pmid">18823241</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Gabry</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC</article-title><source>Statistics and Computing</source><volume>27</volume><fpage>1413</fpage><lpage>1432</lpage><pub-id pub-id-type="doi">10.1007/s11222-016-9696-4</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>İ</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vlaeyen</surname><given-names>JWS</given-names></name><name><surname>Crombez</surname><given-names>G</given-names></name><name><surname>Linton</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The fear-avoidance model of pain</article-title><source>Pain</source><volume>157</volume><fpage>1588</fpage><lpage>1589</lpage><pub-id pub-id-type="doi">10.1097/j.pain.0000000000000574</pub-id><pub-id pub-id-type="pmid">27428892</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>CN</given-names></name><name><surname>Servant</surname><given-names>M</given-names></name><name><surname>Logan</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Testing the validity of conflict drift-diffusion models for use in estimating cognitive processes: A parameter-recovery study</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>286</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.3758/s13423-017-1271-2</pub-id><pub-id pub-id-type="pmid">28357629</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiech</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deconstructing the sensation of pain: the influence of cognitive processes on pain perception</article-title><source>Science</source><volume>354</volume><fpage>584</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1126/science.aaf8934</pub-id><pub-id pub-id-type="pmid">27811269</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Collins</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ten simple rules for the computational modeling of behavioral data</article-title><source>eLife</source><volume>8</volume><elocation-id>e49547</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49547</pub-id><pub-id pub-id-type="pmid">31769410</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarnitsky</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Role of endogenous pain modulation in chronic pain mechanisms and treatment</article-title><source>Pain</source><volume>156</volume><fpage>S24</fpage><lpage>S31</lpage><pub-id pub-id-type="doi">10.1097/01.j.pain.0000460343.46847.58</pub-id><pub-id pub-id-type="pmid">25789433</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshida</surname><given-names>W</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Koltzenburg</surname><given-names>M</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Uncertainty increases pain: evidence for a novel mechanism of pain modulation involving the periaqueductal gray</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>5638</fpage><lpage>5646</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4984-12.2013</pub-id><pub-id pub-id-type="pmid">23536078</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Behavioural results</title><sec sec-type="appendix" id="s8-1"><title>Model-naive performance</title><p>For each sequence condition (volatility × stochasticity), we calculated the RMSE of participants’ responses (type: perception and prediction ratings) and compared to the normative noxious input, as a measure of performance in the task. We analysed the RMSEs with a repeated measures ANOVA, with the results reported in <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>.</p><p>Given the significant interaction between stochasticity and response type, we further ran a post hoc comparison tests for this effect, as reported in <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>.</p></sec></sec><sec sec-type="appendix" id="s9"><title>Noxious inputs and responses</title><sec sec-type="appendix" id="s9-1"><title>Input transformation</title><p>We linearly transformed participants’ responses to project them from the 1–13 range to 0–100 using a linear transformation we obtained from a regression of stimulus intensities onto pain ratings.</p><p>Plots of each participant’s transformation can be found in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>.</p><p>We superimposed participants’ responses (perception and prediction ratings) onto the noxious input condition sequences in <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>. The black line marks the start of a new sequence condition.</p><p>Finally, we plotted participants’ confidence ratings throughout the task in <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>.</p></sec></sec><sec sec-type="appendix" id="s10"><title>Model predictions</title><p>Following the model fitting procedure, in <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>, we plotted example model predicted ratings for both perception and prediction responses for each condition as compared with noxious input for one participant’s responses.</p><p>Lastly, for each condition and for each participant we plotted model responses (perception and prediction) against participant responses in <xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>. The grand mean correlation across participants for each condition and response type was calculated and included in the figure.</p></sec><sec sec-type="appendix" id="s11"><title>Parameter recovery</title><p>The results of parameter recovery analysis for each parameter for each model are reported in <xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref>. We recovered each individual (out of 27 participants) parameter ≈100 times and calculated the mean and SD of the correlation between the true and recovered parameter values.</p><p>Moreover, to assess the number of simulations needed, we calculated the average SD (and its error) of the correlation as a function of increasing number of simulation, as plotted in <xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6</xref>. The average was obtained from the 500 randomly chosen permutations of different simulations at each <inline-formula><mml:math id="inf165"><mml:semantics><mml:mi>n</mml:mi></mml:semantics></mml:math></inline-formula> (out of ≈100).</p><p>Next, we include scatter plots from the parameter recovery for each model and parameter in <xref ref-type="fig" rid="app1fig7">Appendix 1—figures 7</xref>–<xref ref-type="fig" rid="app1fig11">11</xref>.</p></sec><sec sec-type="appendix" id="s12"><title>Model recovery</title><p>We also ran model recovery analysis as described in the Materials and methods. We report the confusion matrix of our analysis based on approximately 100 simulations (per model pair) in <xref ref-type="table" rid="app1table4">Appendix 1—table 4</xref>.</p></sec><sec sec-type="appendix" id="s13"><title>Condition-wise model comparison</title><p>For each condition, we ran model comparison procedure as described in the Materials and methods. The results are reported in <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>. In each condition, the expectation weighted models provided significantly better fit than models without this element.</p></sec><sec sec-type="appendix" id="s14"><title>Model diagnostics</title><p>In <xref ref-type="table" rid="app1table6 app1table9 app1table7 app1table8">Appendix 1—tables 6–9</xref>, we report bulk and tail ESS for each condition, for each model and parameter.</p><p>While some of the ESS values are below the recommended threshold of 100, indicating potential issues with parameter inference. This may be due to a low participant sample size, as well as small number of trials per condition, hinting limited statistical power. Given that the Rhat values are all around 1, and that there are no divergent transitions, as well as a fairly good parameter recovery, we see this as a minor issue.</p><p>Lastly, in <xref ref-type="table" rid="app1table10">Appendix 1—table 10</xref> we model diagnostics for each condition, such as the estimated Bayesian fraction of missing information (E-BFMI), number of divergent transition, and E-BFMI values per chain.</p></sec><sec sec-type="appendix" id="s15"><title>Modelling results</title><sec sec-type="appendix" id="s15-1"><title>Group-level differences between each condition</title><p>We plotted the estimate posterior distributions for each parameter of the model (including the across-trial average and the final learning rate and perceptual weighting term) in <xref ref-type="fig" rid="app1fig12">Appendix 1—figure 12</xref>. We found no group-level differences between conditions for any of the posterior distribution of the parameters in the winning eKF model.</p></sec><sec sec-type="appendix" id="s15-2"><title>Individual-level differences between conditions</title><p>We estimated the individual-level parameters for each condition, and include their violin plots in <xref ref-type="fig" rid="app1fig13">Appendix 1—figure 13</xref>.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Linear transformation of the input at perception trials.</title><p>Blue dots indicate participant’s perception responses for a given level of stimulus intensity, black dots indicate transformed intensity values, a linear least squares regression was performed to achieve the best fitting line through participant’s responses as shown in red, the intercept was constrained&gt;0.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-app1-fig1-v1.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Participants’ responses (red - perception; green - prediction) to the noxious input (dotted line) sequences.</title><p>Vertical purple lines mark the end of each condition.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-app1-fig2-v1.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Participants’ confidence ratings (red - perception; green - prediction) during the task.</title><p>Vertical purple lines mark the end of each condition.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-app1-fig3-v1.tif"/></fig><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Example plot of the input sequences (black) for each condition, one participant’s responses (white) and the winning, expectation weighted Kalman filter (eKF), model predictions (blue) including 95% confidence intervals (shaded blue) for (<bold>A–D</bold>) perception and (<bold>E–H</bold>) prediction.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-app1-fig4-v1.tif"/></fig><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>Model responses against participants’ responses for each condition and each response type (<bold>A–D</bold>) perception and (<bold>E–H</bold>) prediction.</title><p>The annotated value is the grand mean correlation across subjects for each condition and response type.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-app1-fig5-v1.tif"/></fig><fig id="app1fig6" position="float"><label>Appendix 1—figure 6.</label><caption><title>Parameter recovery average SD for: (<bold>A</bold>) eRL; (<bold>B</bold>) RL; (<bold>C</bold>) eKF; (<bold>D</bold>) KF; (<bold>E</bold>) Random model.</title><p>The average SD is plotted as a function of simulation number averaged across 500 permutations of ≈100 simulations. The coloured shading corresponds to 1 SD around the average error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-app1-fig6-v1.tif"/></fig><fig id="app1fig7" position="float"><label>Appendix 1—figure 7.</label><caption><title>Parameter recovery scatter plot for expectation weighted reinforcement learning (eRL) model from ≈100 simulations for: (<bold>A</bold>) ɑ; (<bold>B</bold>) ɣ; (<bold>C</bold>) ξ; (<bold>D</bold>) E0; (<bold>E</bold>) C parameter.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-app1-fig7-v1.tif"/></fig><fig id="app1fig8" position="float"><label>Appendix 1—figure 8.</label><caption><title>Parameter recovery scatter plot for reinforcement learning (RL) model from ≈100 simulations for: (<bold>A</bold>) ɑ; (<bold>B</bold>) ξ; (<bold>C</bold>) E0; (<bold>D</bold>) C parameter.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-app1-fig8-v1.tif"/></fig><fig id="app1fig9" position="float"><label>Appendix 1—figure 9.</label><caption><title>Parameter recovery scatter plot for expectation weighted Kalman filter (eKF) model from ≈100 simulations for: (<bold>A</bold>) ε; (<bold>B</bold>) s; (<bold>C</bold>) v; (<bold>D</bold>) ξ; (<bold>E</bold>) E0; (<bold>F</bold>) w0; (<bold>G</bold>) C parameter.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-app1-fig9-v1.tif"/></fig><fig id="app1fig10" position="float"><label>Appendix 1—figure 10.</label><caption><title>Parameter recovery scatter plot for Kalman filter (KF) model from ≈100 simulations for: (<bold>A</bold>) s; (<bold>B</bold>) v; (<bold>C</bold>) ξ; (<bold>D</bold>) E0; (<bold>E</bold>) w0; (<bold>F</bold>) C parameter.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-app1-fig10-v1.tif"/></fig><fig id="app1fig11" position="float"><label>Appendix 1—figure 11.</label><caption><title>Parameter recovery scatter plot for random model from ≈100 simulations for: (<bold>A</bold>) ξ; (<bold>B</bold>) R; (<bold>C</bold>) C parameter.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-app1-fig11-v1.tif"/></fig><fig id="app1fig12" position="float"><label>Appendix 1—figure 12.</label><caption><title>Group-level distributions for parameters for each condition for the expectation weighted Kalman filter (eKF) model.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-app1-fig12-v1.tif"/></fig><fig id="app1fig13" position="float"><label>Appendix 1—figure 13.</label><caption><title>Violin plots (and box-plots) of individual-level parameters for each condition in the winning expectation weighted Kalman filter (eKF) model.</title><p>Lower and upper hinges correspond to the first and third quartiles of partipants’ errors (the upper/lower whisker extends from the hinge to the largest/smallest value no further than 1.5 * ”Interquartile range” from the hinge); the line in the box corresponds to the median. Each condition has N=27 particpants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-app1-fig13-v1.tif"/></fig><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Within-subjects effects from repeated measures ANOVA of participant’s RMSE scores with stochasticity, volatility, and response type factors.</title><p>SS - sum of squares, MS - mean square, RMSE - root mean square error</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Effect</th><th align="left" valign="bottom">SS</th><th align="left" valign="bottom">df</th><th align="left" valign="bottom">MS</th><th align="left" valign="bottom"><italic>F</italic></th><th align="left" valign="bottom">p</th><th align="left" valign="bottom"><inline-formula><mml:math id="inf166"><mml:semantics><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf167"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="left" valign="bottom">Volatility</td><td align="left" valign="bottom">10.714</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">10.714</td><td align="left" valign="bottom">0.960</td><td align="left" valign="bottom">0.336</td><td align="left" valign="bottom">0.007</td><td align="left" valign="bottom">0.036</td></tr><tr><td align="left" valign="bottom">Residuals</td><td align="left" valign="bottom">290.166</td><td align="left" valign="bottom">26</td><td align="left" valign="bottom">11.160</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Stochasticity</td><td align="left" valign="bottom">113.964</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">113.964</td><td align="left" valign="bottom">19.939</td><td align="left" valign="bottom">&lt;<bold>0.001*</bold></td><td align="left" valign="bottom">0.074</td><td align="left" valign="bottom">0.434</td></tr><tr><td align="left" valign="bottom">Residuals</td><td align="left" valign="bottom">148.603</td><td align="left" valign="bottom">26</td><td align="left" valign="bottom">5.715</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Type</td><td align="left" valign="bottom">365.000</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">365.000</td><td align="left" valign="bottom">85.109</td><td align="left" valign="bottom"><bold>&lt;0.001*</bold></td><td align="left" valign="bottom">0.237</td><td align="left" valign="bottom">0.766</td></tr><tr><td align="left" valign="bottom">Residuals</td><td align="left" valign="bottom">111.503</td><td align="left" valign="bottom">26</td><td align="left" valign="bottom">4.289</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Volatility × stochasticity</td><td align="left" valign="bottom">0.006</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">0.006</td><td align="left" valign="bottom">5.688e<sup>-4</sup></td><td align="left" valign="bottom">0.981</td><td align="left" valign="bottom">3.723e<sup>-6</sup></td><td align="left" valign="bottom">2.188e<sup>-5</sup></td></tr><tr><td align="left" valign="bottom">Residuals</td><td align="left" valign="bottom">261.912</td><td align="left" valign="bottom">26</td><td align="left" valign="bottom">10.074</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Volatility × type</td><td align="left" valign="bottom">7.313</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">7.313</td><td align="left" valign="bottom">3.196</td><td align="left" valign="bottom">0.085</td><td align="left" valign="bottom">0.005</td><td align="left" valign="bottom">0.109</td></tr><tr><td align="left" valign="bottom">Residuals</td><td align="left" valign="bottom">59.487</td><td align="left" valign="bottom">26</td><td align="left" valign="bottom">2.288</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Stochasticity × type</td><td align="left" valign="bottom">63.662</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">63.662</td><td align="left" valign="bottom">29.842</td><td align="left" valign="bottom"><bold>&lt;0.001*</bold></td><td align="left" valign="bottom">0.041</td><td align="left" valign="bottom">0.534</td></tr><tr><td align="left" valign="bottom">Residuals</td><td align="left" valign="bottom">55.466</td><td align="left" valign="bottom">26</td><td align="left" valign="bottom">2.133</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Volatility × stochasticity × type</td><td align="left" valign="bottom">1.356</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">1.356</td><td align="left" valign="bottom">0.704</td><td align="left" valign="bottom">0.409</td><td align="left" valign="bottom">8.807e<sup>-4</sup></td><td align="left" valign="bottom">0.026</td></tr><tr><td align="left" valign="bottom">Residuals</td><td align="left" valign="bottom">50.060</td><td align="left" valign="bottom">26</td><td align="left" valign="bottom">1.925</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr></tbody></table><table-wrap-foot><fn><p>* indicates statistical significance at 0.05 level.</p></fn></table-wrap-foot></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Post hoc comparisons for the repeated measures ANOVA’s interaction effect of stochasticity × type.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom"/><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="2">95%CI for mean diff.</th><th align="left" valign="bottom"/><th align="left" valign="bottom"/><th align="left" valign="bottom"/></tr></thead><tbody><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">Mean diff.</td><td align="left" valign="bottom">Lower</td><td align="left" valign="bottom">Upper</td><td align="left" valign="bottom">SE</td><td align="left" valign="bottom"><italic>t</italic></td><td align="left" valign="bottom">p<italic><sub>bonf</sub></italic></td></tr><tr><td align="left" valign="bottom">High, perception</td><td align="left" valign="bottom">Low, perception</td><td align="left" valign="bottom">0.367</td><td align="left" valign="bottom">−0.687</td><td align="left" valign="bottom">1.421</td><td align="left" valign="bottom">0.381</td><td align="left" valign="bottom">0.963</td><td align="left" valign="bottom">1.000</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">High, prediction</td><td align="left" valign="bottom">−3.686</td><td align="left" valign="bottom">−4.636</td><td align="left" valign="bottom">−2.735</td><td align="left" valign="bottom">0.345</td><td align="left" valign="bottom">−10.688</td><td align="left" valign="bottom">&lt;<bold>0.001*</bold></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Low, prediction</td><td align="left" valign="bottom">−1.147</td><td align="left" valign="bottom">−2.329</td><td align="left" valign="bottom">0.034</td><td align="left" valign="bottom">0.430</td><td align="left" valign="bottom">−2.665</td><td align="left" valign="bottom">0.062</td></tr><tr><td align="left" valign="bottom">Low, perception</td><td align="left" valign="bottom">High, prediction</td><td align="left" valign="bottom">−4.053</td><td align="left" valign="bottom">−5.234</td><td align="left" valign="bottom">−2.871</td><td align="left" valign="bottom">0.430</td><td align="left" valign="bottom">−9.415</td><td align="left" valign="bottom"><bold>&lt;0.001*</bold></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Low, prediction</td><td align="left" valign="bottom">−1.514</td><td align="left" valign="bottom">−2.464</td><td align="left" valign="bottom">−0.564</td><td align="left" valign="bottom">0.345</td><td align="left" valign="bottom">−4.390</td><td align="left" valign="bottom"><bold>&lt;0.001*</bold></td></tr><tr><td align="left" valign="bottom">High, prediction</td><td align="left" valign="bottom">Low, prediction</td><td align="left" valign="bottom">2.539</td><td align="left" valign="bottom">1.484</td><td align="left" valign="bottom">3.593</td><td align="left" valign="bottom">0.381</td><td align="left" valign="bottom">6.658</td><td align="left" valign="bottom"><bold>&lt;0.001*</bold></td></tr></tbody></table><table-wrap-foot><fn><p>* indicates statistical significance at 0.05 level.</p></fn></table-wrap-foot></table-wrap><table-wrap id="app1table3" position="float"><label>Appendix 1—table 3.</label><caption><title>Pearson correlation coeﬃcient <inline-formula><mml:math id="inf168"><mml:semantics><mml:mi>r</mml:mi></mml:semantics></mml:math></inline-formula> (SD) from the parameter recovery analysis for each model.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="7">eRL</th></tr></thead><tbody><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf169"><mml:semantics><mml:mi>α</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf170"><mml:semantics><mml:mi>γ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf171"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf172"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf173"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf174"><mml:semantics><mml:mi>r</mml:mi></mml:semantics></mml:math></inline-formula> (SD)</td><td align="char" char="." valign="bottom">0.685 (0.113)</td><td align="left" valign="bottom">0.92 (0.049)</td><td align="left" valign="bottom">0.993 (0.005)</td><td align="left" valign="bottom">0.723 (0.093)</td><td align="left" valign="bottom">0.481 (0.131)</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="7">RL</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf175"><mml:semantics><mml:mi>α</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf176"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf177"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf178"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf179"><mml:semantics><mml:mi>r</mml:mi></mml:semantics></mml:math></inline-formula> (SD)</td><td align="char" char="." valign="bottom">0.842 (0.081)</td><td align="left" valign="bottom">0.993 (0.004)</td><td align="left" valign="bottom">0.625 (0.107)</td><td align="left" valign="bottom">0.455 (0.133)</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="7">eKF</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf180"><mml:semantics><mml:mi>ϵ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf181"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf182"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf183"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf184"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf185"><mml:semantics><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf186"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf187"><mml:semantics><mml:mi>r</mml:mi></mml:semantics></mml:math></inline-formula> (SD)</td><td align="char" char="." valign="bottom">0.742 (0.1)</td><td align="left" valign="bottom">0.531 (0.13)</td><td align="left" valign="bottom">0.745 (0.09)</td><td align="left" valign="bottom">0.986 (0.075)</td><td align="left" valign="bottom">0.849 (0.118)</td><td align="char" char="." valign="bottom">0.309 (0.179)</td><td align="char" char="." valign="bottom">0.472 (0.123)</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="7">KF</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf188"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf189"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf190"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf191"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf192"><mml:semantics><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf193"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf194"><mml:semantics><mml:mi>r</mml:mi></mml:semantics></mml:math></inline-formula> (SD)</td><td align="char" char="." valign="bottom">0.605 (0.129)</td><td align="left" valign="bottom">0.589 (0.117)</td><td align="left" valign="bottom">0.993 (0.005)</td><td align="left" valign="bottom">0.585 (0.157)</td><td align="left" valign="bottom">0.298 (0.18)</td><td align="char" char="." valign="bottom">0.442 (0.146)</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="7">Random model</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf195"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf196"><mml:semantics><mml:mi>R</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf197"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf198"><mml:semantics><mml:mi>r</mml:mi></mml:semantics></mml:math></inline-formula> (SD)</td><td align="char" char="." valign="bottom">0.996 (0.004)</td><td align="left" valign="bottom">0.999 (0.001)</td><td align="left" valign="bottom">0.079 (0.206)</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr></tbody></table></table-wrap><table-wrap id="app1table4" position="float"><label>Appendix 1—table 4.</label><caption><title>Confusion matrix from the model recovery analysis based on ≈100 simulations.</title><p>The <italic>y</italic>-axis indicates which model simulated the dataset, while the <italic>x</italic>-axis indicates which model ﬁt the data based on leave-one-out information criterion (LOOIC).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom"/><th align="left" valign="bottom">eRL</th><th align="left" valign="bottom">RL</th><th align="left" valign="bottom">eKF</th><th align="left" valign="bottom">KF</th><th align="left" valign="bottom">Random</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="6">Simulated</td><td align="left" valign="bottom">eRL</td><td align="left" valign="bottom">0.327</td><td align="left" valign="bottom">0.173</td><td align="left" valign="bottom">0.404</td><td align="left" valign="bottom">0.096</td><td align="left" valign="bottom">0.000</td></tr><tr><td align="left" valign="bottom">RL</td><td align="left" valign="bottom">0.223</td><td align="left" valign="bottom">0.234</td><td align="left" valign="bottom">0.223</td><td align="left" valign="bottom">0.319</td><td align="left" valign="bottom">0.000</td></tr><tr><td align="left" valign="bottom">eKF</td><td align="left" valign="bottom">0.382</td><td align="left" valign="bottom">0.067</td><td align="left" valign="bottom">0.427</td><td align="left" valign="bottom">0.124</td><td align="left" valign="bottom">0.000</td></tr><tr><td align="left" valign="bottom">KF</td><td align="left" valign="bottom">0.229</td><td align="left" valign="bottom">0.281</td><td align="left" valign="bottom">0.281</td><td align="left" valign="bottom">0.208</td><td align="left" valign="bottom">0.000</td></tr><tr><td align="left" valign="bottom">Random</td><td align="left" valign="bottom">0.292</td><td align="left" valign="bottom">0.000</td><td align="left" valign="bottom">0.358</td><td align="left" valign="bottom">0.000</td><td align="left" valign="bottom">0.349</td></tr><tr><td align="left" valign="bottom" colspan="6">Fit</td></tr></tbody></table></table-wrap><table-wrap id="app1table5" position="float"><label>Appendix 1—table 5.</label><caption><title>Model comparison results for each condition.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Condition</th><th align="left" valign="bottom">Model name</th><th align="left" valign="bottom">ELPD diﬀerence</th><th align="left" valign="bottom">SE diﬀerence</th><th align="left" valign="bottom">Sigma eﬀect</th><th align="left" valign="bottom">LOOIC</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="5">Vol. high Stoch. high</td><td align="left" valign="bottom">eKF - expectation weighted</td><td align="left" valign="bottom">0.000</td><td align="left" valign="bottom">0.000</td><td align="left" valign="bottom"/><td align="left" valign="bottom">15748.389</td></tr><tr><td align="left" valign="bottom">eRL - expectation weighted</td><td align="left" valign="bottom">–9.560</td><td align="left" valign="bottom">5.071</td><td align="left" valign="bottom">1.885</td><td align="left" valign="bottom">15767.509</td></tr><tr><td align="left" valign="bottom">RL</td><td align="left" valign="bottom">–139.407</td><td align="left" valign="bottom">61.362</td><td align="left" valign="bottom">2.272</td><td align="left" valign="bottom">16027.202</td></tr><tr><td align="left" valign="bottom">KF</td><td align="left" valign="bottom">–161.444</td><td align="left" valign="bottom">77.335</td><td align="left" valign="bottom">2.088</td><td align="left" valign="bottom">16071.277</td></tr><tr><td align="left" valign="bottom">Random response</td><td align="left" valign="bottom">–730.600</td><td align="left" valign="bottom">77.009</td><td align="left" valign="bottom">9.487</td><td align="left" valign="bottom">17209.588</td></tr><tr><td align="left" valign="bottom" rowspan="5">Vol. high Stoch. low</td><td align="left" valign="bottom">eKF - expectation weighted</td><td align="left" valign="bottom">0.000</td><td align="left" valign="bottom">0.000</td><td align="left" valign="bottom"/><td align="left" valign="bottom">15682.115</td></tr><tr><td align="left" valign="bottom">eRL - expectation weighted</td><td align="left" valign="bottom">–17.439</td><td align="left" valign="bottom">5.896</td><td align="left" valign="bottom">2.958</td><td align="left" valign="bottom">15716.993</td></tr><tr><td align="left" valign="bottom">RL</td><td align="left" valign="bottom">–131.817</td><td align="left" valign="bottom">35.936</td><td align="left" valign="bottom">3.668</td><td align="left" valign="bottom">15945.749</td></tr><tr><td align="left" valign="bottom">KF</td><td align="left" valign="bottom">–133.464</td><td align="left" valign="bottom">37.171</td><td align="left" valign="bottom">3.591</td><td align="left" valign="bottom">15949.042</td></tr><tr><td align="left" valign="bottom">Random response</td><td align="left" valign="bottom">–824.346</td><td align="left" valign="bottom">79.148</td><td align="left" valign="bottom">10.415</td><td align="left" valign="bottom">17330.807</td></tr><tr><td align="left" valign="bottom" rowspan="5">Vol. low Stoch. high</td><td align="left" valign="bottom">eKF - expectation weighted</td><td align="left" valign="bottom">0.000</td><td align="left" valign="bottom">0.000</td><td align="left" valign="bottom"/><td align="left" valign="bottom">15990.114</td></tr><tr><td align="left" valign="bottom">eRL - expectation weighted</td><td align="left" valign="bottom">–12.027</td><td align="left" valign="bottom">7.029</td><td align="left" valign="bottom">1.711</td><td align="left" valign="bottom">16014.169</td></tr><tr><td align="left" valign="bottom">RL</td><td align="left" valign="bottom">–149.338</td><td align="left" valign="bottom">43.874</td><td align="left" valign="bottom">3.404</td><td align="left" valign="bottom">16288.789</td></tr><tr><td align="left" valign="bottom">KF</td><td align="left" valign="bottom">–159.738</td><td align="left" valign="bottom">46.485</td><td align="left" valign="bottom">3.436</td><td align="left" valign="bottom">16309.590</td></tr><tr><td align="left" valign="bottom">Random response</td><td align="left" valign="bottom">–831.096</td><td align="left" valign="bottom">84.549</td><td align="left" valign="bottom">9.830</td><td align="left" valign="bottom">17652.306</td></tr><tr><td align="left" valign="bottom" rowspan="5">Vol. low Stoch. low</td><td align="left" valign="bottom">eKF - expectation weighted</td><td align="left" valign="bottom">0.000</td><td align="left" valign="bottom">0.000</td><td align="left" valign="bottom"/><td align="left" valign="bottom">15904.936</td></tr><tr><td align="left" valign="bottom">eRL - expectation weighted</td><td align="left" valign="bottom">–11.068</td><td align="left" valign="bottom">4.309</td><td align="left" valign="bottom">2.569</td><td align="left" valign="bottom">15927.072</td></tr><tr><td align="left" valign="bottom">RL</td><td align="left" valign="bottom">–70.588</td><td align="left" valign="bottom">16.643</td><td align="left" valign="bottom">4.241</td><td align="left" valign="bottom">16046.111</td></tr><tr><td align="left" valign="bottom">KF</td><td align="left" valign="bottom">–74.031</td><td align="left" valign="bottom">20.972</td><td align="left" valign="bottom">3.530</td><td align="left" valign="bottom">16052.997</td></tr><tr><td align="left" valign="bottom">Random response</td><td align="left" valign="bottom">–901.792</td><td align="left" valign="bottom">107.244</td><td align="left" valign="bottom">8.409</td><td align="left" valign="bottom">17708.519</td></tr></tbody></table></table-wrap><table-wrap id="app1table6" position="float"><label>Appendix 1—table 6.</label><caption><title>Bulk and tail effective sample size (ESS) values for vol. high - stoch. high.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Model</th><th align="left" valign="bottom">Param.</th><th align="left" valign="bottom">ESS (bulk)</th><th align="left" valign="bottom">ESS (tail)</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="5">eRL</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf199"><mml:semantics><mml:mi>α</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">58.166</td><td align="left" valign="bottom">47.491</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf200"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">90.5</td><td align="left" valign="bottom">79.142</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf201"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">54.655</td><td align="left" valign="bottom">137.729</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf202"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">31.233</td><td align="left" valign="bottom">47.726</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf203"><mml:semantics><mml:mi>γ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">39.509</td><td align="left" valign="bottom">49.335</td></tr><tr><td align="left" valign="bottom" rowspan="4">RL</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf204"><mml:semantics><mml:mi>α</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">56.22</td><td align="left" valign="bottom">36.057</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf205"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">99.642</td><td align="left" valign="bottom">52.599</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf206"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">126.757</td><td align="left" valign="bottom">467.373</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf207"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">31.322</td><td align="left" valign="bottom">36.92</td></tr><tr><td align="left" valign="bottom" rowspan="7">eKF</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf208"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">89.281</td><td align="left" valign="bottom">83.274</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf209"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">37.723</td><td align="left" valign="bottom">103.977</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf210"><mml:semantics><mml:mi>ϵ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">94.203</td><td align="left" valign="bottom">429.332</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf211"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">53.099</td><td align="left" valign="bottom">41.511</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf212"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">1665.566</td><td align="left" valign="bottom">4593.161</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf213"><mml:semantics><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">616458.467</td><td align="left" valign="bottom">467603.626</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf214"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">31.322</td><td align="left" valign="bottom">47.1</td></tr><tr><td align="left" valign="bottom" rowspan="6">KF</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf215"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">101.584</td><td align="left" valign="bottom">55.345</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf216"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">122.76</td><td align="left" valign="bottom">512.134</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf217"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">114.644</td><td align="left" valign="bottom">54.015</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf218"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">438.028</td><td align="left" valign="bottom">730.579</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf219"><mml:semantics><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">904.643</td><td align="left" valign="bottom">6759.804</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf220"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">31.457</td><td align="left" valign="bottom">36.763</td></tr><tr><td align="left" valign="bottom" rowspan="3">Random</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf221"><mml:semantics><mml:mi>R</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">27.939</td><td align="left" valign="bottom">33.982</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf222"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">397.862</td><td align="left" valign="bottom">259.967</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf223"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">32.334</td><td align="left" valign="bottom">41.271</td></tr></tbody></table></table-wrap><table-wrap id="app1table7" position="float"><label>Appendix 1—table 7.</label><caption><title>Bulk and tail effective sample size (ESS) values for vol. high - stoch. low.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Model</th><th align="left" valign="bottom">Param.</th><th align="left" valign="bottom">ESS (bulk)</th><th align="left" valign="bottom">ESS (tail)</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="5">eRL</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf224"><mml:semantics><mml:mi>α</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">86.32</td><td align="left" valign="bottom">60.849</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf225"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">235.396</td><td align="left" valign="bottom">373.736</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf226"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">43.489</td><td align="left" valign="bottom">109.903</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf227"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">30.471</td><td align="left" valign="bottom">36.664</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf228"><mml:semantics><mml:mi>γ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">42.125</td><td align="left" valign="bottom">55.178</td></tr><tr><td align="left" valign="bottom" rowspan="4">RL</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf229"><mml:semantics><mml:mi>α</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">49.221</td><td align="left" valign="bottom">40.877</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf230"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">328.761</td><td align="left" valign="bottom">455.542</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf231"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">63.341</td><td align="left" valign="bottom">111.689</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf232"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">30.304</td><td align="left" valign="bottom">38.063</td></tr><tr><td align="left" valign="bottom" rowspan="7">eKF</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf233"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">227.813</td><td align="left" valign="bottom">363.944</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf234"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">33.393</td><td align="left" valign="bottom">104.395</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf235"><mml:semantics><mml:mi>ϵ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">376.691</td><td align="left" valign="bottom">1218.299</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf236"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">45.861</td><td align="left" valign="bottom">37.486</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf237"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">99526.69</td><td align="left" valign="bottom">148393.383</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf238"><mml:semantics><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">567627.288</td><td align="left" valign="bottom">634817.458</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf239"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">30.438</td><td align="left" valign="bottom">36.66</td></tr><tr><td align="left" valign="bottom" rowspan="6">KF</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf240"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">328.005</td><td align="left" valign="bottom">448.632</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf241"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">57.467</td><td align="left" valign="bottom">124.471</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf242"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">293.426</td><td align="left" valign="bottom">480.255</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf243"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">164.454</td><td align="left" valign="bottom">598.211</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf244"><mml:semantics><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">412979.973</td><td align="left" valign="bottom">354163.251</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf245"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">30.16</td><td align="left" valign="bottom">38.105</td></tr><tr><td align="left" valign="bottom" rowspan="3">Random</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf246"><mml:semantics><mml:mi>R</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">28.397</td><td align="left" valign="bottom">32.922</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf247"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">1794.614</td><td align="left" valign="bottom">1170.459</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf248"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">30.204</td><td align="left" valign="bottom">34.896</td></tr></tbody></table></table-wrap><table-wrap id="app1table8" position="float"><label>Appendix 1—table 8.</label><caption><title>Bulk and tail effective sample size (ESS) values for vol. low - stoch. high.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Model</th><th align="left" valign="bottom">Param.</th><th align="left" valign="bottom">ESS (bulk)</th><th align="left" valign="bottom">ESS (tail)</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="5">eRL</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf249"><mml:semantics><mml:mi>α</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">43.312</td><td align="left" valign="bottom">40.66</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf250"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">248.885</td><td align="left" valign="bottom">434.44</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf251"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">49.006</td><td align="left" valign="bottom">85.409</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf252"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">29.68</td><td align="left" valign="bottom">34.909</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf253"><mml:semantics><mml:mi>γ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">45.37</td><td align="left" valign="bottom">52.755</td></tr><tr><td align="left" valign="bottom" rowspan="4">RL</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf254"><mml:semantics><mml:mi>α</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">39.911</td><td align="left" valign="bottom">35.351</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf255"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">433.949</td><td align="left" valign="bottom">435.575</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf256"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">181.442</td><td align="left" valign="bottom">618.317</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf257"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">29.527</td><td align="left" valign="bottom">36.192</td></tr><tr><td align="left" valign="bottom" rowspan="7">eKF</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf258"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">248.848</td><td align="left" valign="bottom">418.003</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf259"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">35.363</td><td align="left" valign="bottom">51.728</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf260"><mml:semantics><mml:mi>ϵ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">1272.838</td><td align="left" valign="bottom">2427.211</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf261"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">41.144</td><td align="left" valign="bottom">40.915</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf262"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">2399.657</td><td align="left" valign="bottom">6854.212</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf263"><mml:semantics><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">612283.163</td><td align="left" valign="bottom">531588.25</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf264"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">29.699</td><td align="left" valign="bottom">34.762</td></tr><tr><td align="left" valign="bottom" rowspan="6">KF</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf265"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">423.339</td><td align="left" valign="bottom">417.747</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf266"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">88.749</td><td align="left" valign="bottom">302.863</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf267"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">58.795</td><td align="left" valign="bottom">47.015</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf268"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">206.969</td><td align="left" valign="bottom">672.666</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf269"><mml:semantics><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">499152.469</td><td align="left" valign="bottom">573964.793</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf270"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">29.511</td><td align="left" valign="bottom">36.341</td></tr><tr><td align="left" valign="bottom" rowspan="3">Random</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf271"><mml:semantics><mml:mi>R</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">27.892</td><td align="left" valign="bottom">32.919</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf272"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">269.239</td><td align="left" valign="bottom">106.139</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf273"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">29.69</td><td align="left" valign="bottom">44.38</td></tr></tbody></table></table-wrap><table-wrap id="app1table9" position="float"><label>Appendix 1—table 9.</label><caption><title>Bulk and tail effective sample size (ESS) values for vol. low - stoch. low.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Model</th><th align="left" valign="bottom">Param.</th><th align="left" valign="bottom">ESS (bulk)</th><th align="left" valign="bottom">ESS (tail)</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="5">eRL</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf274"><mml:semantics><mml:mi>α</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">57.116</td><td align="left" valign="bottom">40.932</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf275"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">162.472</td><td align="left" valign="bottom">129.413</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf276"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">43.707</td><td align="left" valign="bottom">117.295</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf277"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">29.632</td><td align="left" valign="bottom">34.486</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf278"><mml:semantics><mml:mi>γ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">65.497</td><td align="left" valign="bottom">151.548</td></tr><tr><td align="left" valign="bottom" rowspan="4">RL</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf279"><mml:semantics><mml:mi>α</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">45.892</td><td align="left" valign="bottom">37.244</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf280"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">158.681</td><td align="left" valign="bottom">98.898</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf281"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">80.406</td><td align="left" valign="bottom">441.719</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf282"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">29.558</td><td align="left" valign="bottom">35.077</td></tr><tr><td align="left" valign="bottom" rowspan="7">eKF</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf283"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">149.16</td><td align="left" valign="bottom">126.209</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf284"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">38.88</td><td align="left" valign="bottom">73.732</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf285"><mml:semantics><mml:mi>ϵ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">653.635</td><td align="left" valign="bottom">1473.554</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf286"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">48.883</td><td align="left" valign="bottom">43.445</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf287"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">2263.547</td><td align="left" valign="bottom">9318.066</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf288"><mml:semantics><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">635517.969</td><td align="left" valign="bottom">313426.188</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf289"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">29.699</td><td align="left" valign="bottom">34.721</td></tr><tr><td align="left" valign="bottom" rowspan="6">KF</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf290"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">158.729</td><td align="left" valign="bottom">105.929</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf291"><mml:semantics><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">71.438</td><td align="left" valign="bottom">457.431</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf292"><mml:semantics><mml:mi>v</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">91.988</td><td align="left" valign="bottom">69.957</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf293"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">287.835</td><td align="left" valign="bottom">895.249</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf294"><mml:semantics><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">527620.655</td><td align="left" valign="bottom">587092.529</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf295"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">29.527</td><td align="left" valign="bottom">35.147</td></tr><tr><td align="left" valign="bottom" rowspan="3">Random</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf296"><mml:semantics><mml:mi>R</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">28.474</td><td align="left" valign="bottom">38.123</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf297"><mml:semantics><mml:mi>C</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">2426.581</td><td align="left" valign="bottom">1279.66</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf298"><mml:semantics><mml:mi>ξ</mml:mi></mml:semantics></mml:math></inline-formula></td><td align="left" valign="bottom">29.532</td><td align="left" valign="bottom">34.731</td></tr></tbody></table></table-wrap><table-wrap id="app1table10" position="float"><label>Appendix 1—table 10.</label><caption><title>Model diagnostics for each condition - estimated Bayesian fraction of missing information (E-BFMI), number of divergent transition E-BFMI values per chain.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Condition</th><th align="left" valign="bottom">Model</th><th align="left" valign="bottom"># chains low E-BFMI</th><th align="left" valign="bottom"># div. transitions</th><th align="left" valign="bottom">E-BFMI values</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="5">HVHS</td><td align="left" valign="bottom">eRL</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.696 0.713 0.695 0.691</td></tr><tr><td align="left" valign="bottom">RL</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.76 0.748 0.771 0.806</td></tr><tr><td align="left" valign="bottom">eKF</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.755 0.767 0.771 0.759</td></tr><tr><td align="left" valign="bottom">KF</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.633 0.596 0.547 0.563</td></tr><tr><td align="left" valign="bottom">Random</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.842 0.851 0.843 0.835</td></tr><tr><td align="left" valign="bottom" rowspan="5">HVLS</td><td align="left" valign="bottom">eRL</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.689 0.76 0.69 0.689</td></tr><tr><td align="left" valign="bottom">RL</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.624 0.688 0.688 0.685</td></tr><tr><td align="left" valign="bottom">eKF</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.741 0.764 0.753 0.779</td></tr><tr><td align="left" valign="bottom">KF</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.654 0.734 0.689 0.674</td></tr><tr><td align="left" valign="bottom">Random</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.883 0.779 0.836 0.833</td></tr><tr><td align="left" valign="bottom" rowspan="5">LVHS</td><td align="left" valign="bottom">eRL</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.73 0.732 0.728 0.702</td></tr><tr><td align="left" valign="bottom">RL</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.719 0.714 0.742 0.7</td></tr><tr><td align="left" valign="bottom">eKF</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.753 0.755 0.792 0.766</td></tr><tr><td align="left" valign="bottom">KF</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.75 0.768 0.729 0.754</td></tr><tr><td align="left" valign="bottom">Random</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.864 0.849 0.883 0.845</td></tr><tr><td align="left" valign="bottom" rowspan="5">LVLS</td><td align="left" valign="bottom">eRL</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.764 0.762 0.75 0.764</td></tr><tr><td align="left" valign="bottom">RL</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.714 0.764 0.719 0.697</td></tr><tr><td align="left" valign="bottom">eKF</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.783 0.751 0.772 0.77</td></tr><tr><td align="left" valign="bottom">KF</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.705 0.695 0.702 0.726</td></tr><tr><td align="left" valign="bottom">Random</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.835 0.829 0.852 0.847</td></tr></tbody></table></table-wrap></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90634.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ploner</surname><given-names>Markus</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Technische Universität München</institution><country>Germany</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This study presents a <bold>valuable</bold> insight into a computational mechanism of pain perception. The evidence supporting the authors' claims is <bold>compelling</bold>. The work will be of interest to pain researchers working on computational models and cognitive mechanisms of pain in a Bayesian framework.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90634.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This study examined the role of statistical learning in pain perception, suggesting that individuals' expectations about a sequence of events influence their perception of pain intensity. They incorporated the components of volatility and stochasticity into their experimental design and asked participants (n = 27) to rate the pain intensity, their prediction, and their confidence level. They compared two different inference strategies: Bayesian inference vs. heuristic-employing Kalman filters and model-free reinforcement learning. They showed that the expectation-weighted Kalman filter best explained the temporal pattern of participants' ratings. These results provide evidence for a Bayesian inference perspective on pain, supported by a computational model that elucidates the underlying process.</p><p>Strengths:</p><p>- Their experimental design included a wide range of input intensities and the levels of volatility and stochasticity. With elaborated computational models, they provide solid evidence that statistical learning shapes pain.</p><p>Weaknesses:</p><p>- Relevance to clinical pain: While the authors underscore the relevance of their findings to chronic pain, they did not include data pertaining to clinical pain.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90634.3.sa2</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The study investigated how statistical aspects of temperature sequences, such as manipulations of stochasticity (i.e., randomness of a sequence) and volatility (i.e., speed at which a sequence unfolded) influenced pain perception. Using an innovative stimulation paradigm and computational modelling of perceptual variables, this study demonstrated that perception is weighted by expectations. Overall, the findings support the conclusion that pain perception is mediated by expectations in a Bayesian manner. The provision of additional details during the review process strengthens the reliability of this conclusion. The methods presented offer tools and frameworks for further research in pain perception and can be extended to investigations into chronic pain processes.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90634.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Onysk</surname><given-names>Jakub</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Gregory</surname><given-names>Nicholas</given-names></name><role specific-use="author">Author</role><aff><institution>University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Whitefield</surname><given-names>Mia</given-names></name><role specific-use="author">Author</role><aff><institution>University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Jain</surname><given-names>Maeghal</given-names></name><role specific-use="author">Author</role><aff><institution>University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Turner</surname><given-names>Georgia</given-names></name><role specific-use="author">Author</role><aff><institution>University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Seymour</surname><given-names>Ben</given-names></name><role specific-use="author">Author</role><aff><institution>John Radcliffe Hospital</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Mancini</surname><given-names>Flavia</given-names></name><role specific-use="author">Author</role><aff><institution>University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><p><bold>eLife assessment</bold></p><p>This study presents a valuable insight into a computational mechanism of pain perception. The evidence supporting the authors’ claims is solid, although the inclusion of (1) more diverse candidate computational models, (2) more systematic analysis of the temporal regularity effects on the model fit, and (3) tests on clinical samples would have strengthened the study. The work will be of interest to pain researchers working on computational models and cognitive mechanisms of pain in a Bayesian framework.</p><p>Thank you very much again for considering the manuscript and judging it as a valuable contribution to understanding mechanisms of pain perception. We recognise the above-mentioned points of improvement and elaborate on them in the initial response to the reviewers.</p><p>Response to the reviewers</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 1:</bold></p><p>Reviewer Comment 1.1 — Selection of candidate computational models: While the paper juxtaposes the simple model-free RL model against a Kalman Filter model in the context of pain perception, the rationale behind this choice remains ambiguous. It prompts the question: could other RL-based models, such as model-based RL or hierarchical RL, offer additional insights? A more detailed explanation of their computational model selection would provide greater clarity and depth to the study.</p></disp-quote><p>Initial reply: Thank you for this point. Our models were selected a-priori, following the modelling strategy from Jepma et al. (2018) and hence considered the same set of core models for clear extension of the analysis to our non-cue paradigm. The key question for us was whether expectations were used to weight the behavioural estimates, so our main interest was to compare expectation vs non-expectation weighted models.</p><p>Model-based and hierarchical RL are very broad terms that can be used to refer to many different models, and we are not clear about which specific models the reviewer is referring to. Our Bayesian models are generative models, i.e. they learn the generative statistics of the environment (which is characterised by inherent stochasticity and volatility) and hence operate model-based analyses of the stimulus dynamics. In our case, this happened hierarchically and it was combined with a simple RL rule.</p><p>Revised reply: We clarified our modelling choices in the ”Modelling strategy” subsection of the results section.</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 1.2 — Effects of varying levels of volatility and stochasticity: The study commendably integrates varying levels of volatility and stochasticity into its experimental design. However, the depth of analysis concerning the effects of these variables on model fit appears shallow. A looming concern is whether the superior performance of the expectation-weighted Kalman Filter model might be a natural outcome of the experimental design. While the non-significant difference between eKF and eRL for the high stochasticity condition somewhat alleviates this concern, it raises another query: Would a more granular analysis of volatility and stochasticity effects reveal fine-grained model fit patterns?</p></disp-quote><p>Initial reply: We are sorry that the reviewer finds shallow ”the depth of analysis concerning the effects of these variables on model fit”. We are not sure which analysis the reviewer has in mind when suggesting a ”more granular analysis of volatility and stochasticity effects” to ”reveal fine-grained model fit patterns”. Therefore, we find it difficult to improve our manuscript in this regard. We are happy to add analyses to our paper but we would be greatful for some specific pointers. We have already provided:</p><p>• Analysis of model-naive performance across different levels of stochasticity and volatility (section 2.3, figure 3, supplementary information section 1.1 and tables S1-2)</p><p>• Model fitting for each stochasticity/volatility condition (section 2.4.1, figure 4, supplementary table S5)</p><p>• Group-level and individual-level differences of each model parameter across stochasticity/volatility conditions (supplementary information section 7, figures S4-S5).</p><p>• Effect of confidence on scaling factor for each stochasticity/volatility condition (figure 5)</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 1.3 — Rating instruction: According to Fig. 1A, participants were prompted to rate their responses to the question, ”How much pain DID you just feel?” and to specify their confidence level regarding their pain. It is difficult for me to understand the meaning of confidence in this context, given that they were asked to report their *subjective* feelings. It might have been better to query participants about perceived stimulus intensity levels. This perspective is seemingly echoed in lines 100-101, ”the primary aim of the experiment was to determine whether the expectations participants hold about the sequence inform their perceptual beliefs about the intensity of the stimuli.”</p></disp-quote><p>Initial reply: Thank you for raising this question, which allows us to clarify our paradigm. On half of the trials, participants were asked to report the perceived intensity of the previous stimulus; on the remaining trials, participants were requested to predict the intensity of the next stimulus. Therefore, we did query ”participants about perceived stimulus intensity levels”, as described at lines 49-55, 296-303, and depicted in figure 1.</p><p>The confidence refers to the level of confidence that participants have regarding their rating - how sure they are. This is done in addition to their perceived stimulus intensity and it has been used in a large body of previous studies in any sensory modality.</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 1.4 — Relevance to clinical pain: While the authors underscore the relevance of their findings to chronic pain, they did not include data pertaining to clinical pain. Notably, their initial preprint seemed to encompass data from a clinical sample (<ext-link ext-link-type="uri" xlink:href="https://www.medrxiv.org">https://www.medrxiv.org</ext-link> /content/10.1101/2023.03.23.23287656v1), which, for reasons unexplained, has been omitted in the current version. Clarification on this discrepancy would be instrumental in discerning the true relevance of the study’s findings to clinical pain scenarios.</p></disp-quote><p>Initial reply: The preprint that the Reviewer is referring to was an older version of the manuscript in which we combined two different experiments, which were initially born as separate studies: the one that we submitted to eLife (done in the lab, with noxious stimuli in healthy participants) and an online study with a different statistical learning paradigm (without noxious stimuli, in chronic back pain participants). Unfortunately, the paradigms were different and not directly comparable. Indeed, following submission to a different journal, the manuscript was criticised for this reason. We therefore split the paper in two, and submitted the first study to eLife. We are now planning to perform the same lab-based experiment with noxious stimuli on chronic back pain participants. Progress on this front has been slowed down by the fact that I (Flavia Mancini) am on maternity leave, but it remains top priority once back to work.</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 1.5 — Paper organization: The paper’s organization appears a little bit weird, possibly due to the removal of significant content from their initial preprint. Sections 2.12.2 and 2.4 seem more suitable for the Methods section, while 2.3 and 2.4.1 are the only parts that present results. In addition, enhancing clarity through graphical diagrams, especially for the experimental design and computational models, would be quite beneficial. A reference point could be Fig. 1 and Fig. 5 from Jepma et al. (2018), which similarly explored RL and KF models.</p></disp-quote><p>Initial reply: Thank you for these suggestions. We will consider restructuring the paper in the revised version.</p><p>Revised reply: We restructured introduction, results and parts of the methods. We followed the reviewer’s suggestion regarding enhancing clarity through graphical diagrams. We have visualised the experimental design in Figure 1D. Furthemore, we have visualised the two main computational models (eRL and eKF) in Figure 2, following from Jepma et al. (2018). As a result, we have updated the notation in Section 4.4 to be clearer and consistent with the graphical representation (rename the variable referring to observed thermal input from <italic>Ot</italic> to <italic>Nt</italic>).</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 1.6 — In lines 99-100, the statement ”following the work by [23]” would be more helpful if it included a concise summary of the main concepts from the referenced work.</p><p>- It would be helpful to have descriptions of the conditions that Figure 1C is elaborating on.</p><p>- In line 364, the ”N {t}” in the sentence ”The observation on trial t, N {t}”, should be O {t}.</p></disp-quote><p>Initial reply: Thank you for spotting these and for providing the suggestions. We will include the correction in the revised version.</p><p>Revised reply: We have added the following regarding the lines 99-100:</p><p>”We build on the work by [23], who show that pain perception is strongly influenced by expectations as defined by a cue that predicts high or low pain. In contrast to the cue-paradigm from [23], the primary aim of our experiment was to determine whether the expectations participants hold about the sequence itself inform their perceptual beliefs about the intensity of the stimuli.”</p><p>See comment in the previous reply, regarding the notation change from <italic>Ot</italic> to <italic>Nt</italic>.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 2:</bold></p><p>Reviewer Comment 2.1 — This is a highly interesting and novel finding with potential implications for the understanding and treatment of chronic pain where pain regulation is deficient. The paradigm is clear, the analysis is state-of-the-art, the results are convincing, and the interpretation is adequate.</p></disp-quote><p>Initial reply: Thank you very much for these positive comments.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 3:</bold></p><p>Summary:</p><p>I am pleased to have had the opportunity to review this manuscript, which investigated the role of statistical learning in the modulation of pain perception. In short, the study showed that statistical aspects of temperature sequences, with respect to specific manipulations of stochasticity (i.e., randomness of a sequence) and volatility (i.e., speed at which a sequence unfolded) influenced pain perception. Computational modelling of perceptual variables (i.e., multi-dimensional ratings of perceived or predicted stimuli) indicated that models of perception weighted by expectations were the best explanation for the data. My comments below are not intended to undermine or question the quality of this research. Rather, they are offered with the intention of enhancing what is already a significant contribution to the pain neuroscience field. Below, I highlight the strengths and weaknesses of the manuscript and offer suggestions for incorporating additional methodological details.</p><p>Strengths:</p><p>The manuscript is articulate, coherent, and skilfully written, making it accessible and engaging.</p><p>- The innovative stimulation paradigm enables the exploration of expectancy effects on perception without depending on external cues, lending a unique angle to the research.</p><p>- By including participants’ ratings of both perceptual aspects and their confidence in what they perceived or predicted, the study provides an additional layer of information to the understanding of perceptual decision-making. This information was thoughtfully incorporated into the modelling, enabling the investigation of how confidence influences learning.</p><p>- The computational modelling techniques utilised here are methodologically robust. I commend the authors for their attention to model and parameter recovery, a facet often neglected in previous computational neuroscience studies.</p><p>- The well-chosen citations not only reflect a clear grasp of the current research landscape but also contribute thoughtfully to ongoing discussions within the field of pain neuroscience.</p></disp-quote><p>Initial reply: We are really grateful for reviewer’s insightful comments and for providing useful guidance regarding our methodology. We are also thankful for highlighting the strengths of our manuscript. Below we respond to individual weakness mentioned in the reviews report.</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 3.1 — In Figure 1, panel C, the authors illustrate the stimulation intensity, perceived intensity, and prediction intensity on the same scale, facilitating a more direct comparison. It appears that the stimulation intensity has been mathematically transformed to fit a scale from 0 to 100, aligning it with the intensity ratings corresponding to either past or future stimuli. Given that the pain threshold is specifically marked at 50 on this scale, one could logically infer that all ratings falling below this value should be deemed non-painful. However, I find myself uncertain about this interpretation, especially in relation to the term ”arbitrary units” used in the figure. I would greatly appreciate clarification on how to accurately interpret these units, as well as an explanation of the relationship between these values and the definition of pain threshold in this experiment.</p></disp-quote><p>Initial reply: Indeed, as detailed in the Methods section 4.3, the stimulation intensity was originally transformed from the 1-13 scale to 0-100 scale to match the scales in the participant response screens.</p><p>Following the method used to establish the pain threshold, we set the stimulus intensity of 7 as the threshold on the original 1-13 scale. However, during the rating part of the experiment, several of the participants never or very rarely selected a value above 50 (their individually defined pain threshold), despite previously indicating a moment during pain threshold procedure when a stimulus becomes painful. This then results in the re-scaled intensity values as well the perception rating, both on the same 0-100 scale of arbitrary units, to never go above the pain threshold. Please see all participant ratings and inputs in the Figure below. We see that it would be more illustrative to re-plot Figure 1 with a different exemplary participant, whose ratings go above the pain threshold, perhaps with an input intensity on the 1-13 scale on the additional right-hand-side y-axis. We will add this in the revised version as well as highlight the fact above.</p><p>Importantly, while values below 50 are deemed non-painful by participants, the thermal stimulation still activates C-fibres involved in nociception, and we would argue that the modelling framework and analysis still applies in this case.</p><p>Revised reply: We re-plotted Figure 1E-F with a different exemplary participant, whose rating go above the pain threshold. We also included all participant pain perception and prediction ratings, noxious input sequences and confidence ratings in the supplement in Figures S1-S3.</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 3.2 — The method of generating fluctuations in stimulation temperatures, along with the handling of perceptual uncertainty in modelling, requires further elucidation. The current models appear to presume that participants perceive each stimulus accurately, introducing noise only at the response stage. This assumption may fail to capture the inherent uncertainty in the perception of each stimulus intensity, especially when differences in consecutive temperatures are as minimal as 1°C.</p></disp-quote><p>Initial reply: We agree with the reviewer that there are multiple sources of uncertainty involved in the process of rating the intensity of thermal stimuli - including the perception uncertainty. In order to include an account of inaccurate perception, one would have to consider different sources that contribute to this, which there may be many. In our approach, we consider one, which is captured in the expectation weighted model, more clearly exemplified in the expectation-weighted Kalman-Filter model (eKF). The model assumes participants perception of input as an imperfect indicator of the true level of pain. In this case, it turns out that perception is corrupted as a result of the expectation participants hold about the upcoming stimuli. The extent of this effect is partly governed by a subjective level of noise <italic>ϵ</italic>, which may also subsume other sources of uncertainty beyond the expectation effect. Moreover, the response noise <italic>ξ</italic>, could also subsume any other unexplained sources of noise.</p><fig id="sa3fig1" position="float"><label>Author response image 1.</label><caption><title>Stimulis intensity transformation.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90634-sa3-fig1-v1.tif"/></fig><p>Revised reply: We clarified our modelling choices in the ”2.2 Modelling strategy” subsection.</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 3.3 — A key conclusion drawn is that eKF is a better model than eRL. However, a closer examination of the results reveals that the two models behave very similarly, and it is not clear that they can be readily distinguished based on model recovery and model comparison results.</p></disp-quote><p>Initial reply: While, the eKF appears to rank higher than the eRL in terms of LOOIC and sigma effects, we don’t wish to make make sweeping statements regarding significance of differences between eRL and eKF, but merely point to the trend in the data. We shall make this clearer in the revised version of the manuscript. However, the most important result is that the models involving expectation-weighing are arguably better capturing the data.</p><p>Revised reply: We elaborated on the significance statements in the ”Modelling Results” subsection:</p><p>• We considered at least a 2 sigma effect as indication of a significant difference. In each condition, the expectation weighted models (eKF and eRL) provided better fit than models without this element (KF and RL; approx. 2-4 sigma difference, as reported in Figure 5A-D). This suggests that regardless of the levels of volatility and stochasticity, participants still weigh perception of the stimuli with their expectation.</p><p>and in the first paragraph of the Discussion:</p><p>• When varying different levels of inherent uncertainty in the sequences of stimuli (stochasticity and volatility), the expectation and confidence weighted models fitted the data better than models weighted for confidence but not for expectations (Figure 5A-D). The expectation-weighted bayesian (KF) model offered a better fit than the expectation-weighted, model-free RL model, although in conditions of high stochasticity this difference was short of significance. Overall, this suggests that participants’ expectations play a significant role in the perception of sequences of noxious stimuli.</p><p>We are aware of the limitations and lack of clear guidance regarding using sigma effects to establish significance (as per reviewer’s suggestion: <ext-link ext-link-type="uri" xlink:href="https://discourse.mc-stan.org/t/loo-comparison-in-referenceto-standard-error/4009">see here</ext-link>). Here we decided to use the above-mentioned threshold of 2-sigma as an indication of significance, but note the potential limitations of the inferences - especially when distinguishing between eRL/eKF models.</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 3.4 — Regarding model recovery, the distinction between the eKF and eRL models seems blurred. When the simulation is based on the eKF, there is no ability to distinguish whether either eKF or eRL is better. When the simulation is based on the eRL, the eRL appears to be the best model, but the difference with eKF is small. This raises a few more questions. What is the range of the parameters used for the simulations?</p></disp-quote><p>Initial reply: We agree that the distinction between eKF and eRL in the model recovery is not that clean-cut, which may in turn point to the similarity between the two models. To simulate the data for the model and parameter recovery analysis, we used the group means and variances estimated on the participant data to sample individual parameter values.</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 3.5 — Is it possible that either eRL or eKF are best when different parameters are simulated? Additionally, increasing the number of simulations to at least 100 could provide more convincing model recovery results.</p></disp-quote><p>Initial reply: It could be a possibility, but would require further investigation and comparison of fits for different bins/ranges of parameters to see if there is any consistent advantage of one model over another is each bin. We will consider adding this analysis, and provide an additional 50 simulations to paint a more convincing picture.</p><p>Revised reply: We increased the number of simulations per model pair to ≈ 100 (after rejecting fits based on diagnostics criteria - E-BFMI and divergent transitions) and updated the confusion matrix (Table S4). Although the confusion between eRL and eKF remains, the model recovery shows good distinction between expectation weighted vs non-expectation weighted (and Random) models, which supports our main conclusion in the paper.</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 3.6 — Regarding model comparison, the authors reported that ”the expectation-weighted KF model offered a better fit than the eRL, although in conditions of high stochasticity, this difference was short of significance against the eRL model.” This interpretation is based on a significance test that hinges on the ratio between the ELPD and the surrounding standard error (SE). Unfortunately, there’s no agreed-upon threshold of SEs that determines significance, but a general guideline is to consider ”several SEs,” with a higher number typically viewed as more robust. However, the text lacks clarity regarding the specific number of SEs applied in this test. At a cursory glance, it appears that the authors may have employed 2 SEs in their interpretation, while only depicting 1 SE in Figure 4.</p></disp-quote><p>Initial reply: Indeed, we considered 2 sigma effect as a threshold, however we recognise that there is no agreed-upon threshold, and shall make this and our interpretation clearer regarding the trend in the data, in the revision.</p><p>Revised reply: We clarify this further, as per our revised response to Comment 3.3 above. We have also added the following statement in section 4.5.1 (Methods, Model comparison): ”There’s no agreed-upon threshold of SEs that determines significance, but the higher the sigma difference, the more robust is the effect.”</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 3.7 — With respect to parameter recovery, a few additional details could be included for completeness. Specifically, while the range of the learning rate is understandably confined between 0 and 1, the range of other simulated parameters, particularly those without clear boundaries, remains ambiguous. Including scatter plots with the simulated parameters on the xaxis and the recovered parameters on the y-axis would effectively convey this missing information.</p><p>Furthermore, it would be beneficial for the authors to clarify whether the same priors were used for both the modelling results presented in the main paper and the parameter recovery presented in the supplementary material.</p></disp-quote><p>Initial reply: Thanks for this comment and for the suggestions. To simulate the data for the model and parameter recovery analysis, we used the group means and variances estimated on the participant data to sample individual parameter values. The priors on the group and individual-level parameters in the recovery analysis where the same as in the fitting procedure. We will include the requested scatter plots in the next iteration of the manuscript.</p><p>Revised reply: We included parameter recovery scatter plots for each model and parameter in the Supplement Figures S7-S11.</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 3.8 — While the reliance on R-hat values for convergence in model fitting is standard, a more comprehensive assessment could include estimates of the effective sample size (bulk ESS and/or tail ESS) and the Estimated Bayesian Fraction of Missing Information (EBFMI), to show efficient sampling across the distribution. Consideration of divergences, if any, would further enhance the reliability of the results.</p></disp-quote><p>Initial reply: Thank you very much for this suggestion, we will aim to include these measures in the revised version.</p><p>Revised reply: We have considered the suggested diagnostics and include bulk and tail ESS values for each condition, model, parameter in the Supplement Tables S6-S9. We also report number of chain with low E-BFMI (0), number of divergent transitions (0) and the E-BFMI values per chain in Table S10.</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 3.9 — The authors write: ”Going beyond conditioning paradigms based in cuing of pain outcomes, our findings offer a more accurate description of endogenous pain regulation.” Unfortunately, this statement isn’t substantiated by the results. The authors did not engage in a direct comparison between conditioning and sequence-based paradigms. Moreover, even if such a comparison had been made, it remains unclear what would constitute the gold standard for quantifying ”endogenous pain regulation.”</p></disp-quote><p>Initial reply: This is valid point, indeed we do not compare paradigms in our study, and will remove this statement in the future version.</p><p>Revised reply: We have removed this statement from the revised version.</p><disp-quote content-type="editor-comment"><p>Reviewer Comment 3.10 — In relation to the comment on model comparison in my public review, I believe the following link may provide further insight and clarify the basis for my observation. It discusses the use of standard error in model comparison and may be useful for the authors in addressing this particular point:<ext-link ext-link-type="uri" xlink:href="https://discourse.mc-stan.org/t/loo-comparison-in-referenceto-standard-error/4009">see here</ext-link></p></disp-quote><p>Initial reply: Thank you for this suggestion, we will consider the forum discussion in our manuscript.</p></body></sub-article></article>